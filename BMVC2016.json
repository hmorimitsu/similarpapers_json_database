{
  "bmvc2016_main_localshapetransferforimageco-segmentation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Local Shape Transfer for Image Co-segmentation",
    "authors": [
      "Wei Teng",
      "Yu Zhang",
      "Xiaowu Chen",
      "Jia Li",
      "Zhiqiang He"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper003/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper003/paper003.pdf",
    "published": "2016-09",
    "summary": "Image co-segmentation is a challenging computer vision task that aims to segment all pixels of the common objects in an image set. In real-world cases, however, the common objects often vary greatly in poses, locations and scales, making their global shapes highly inconsistent across images and difficult to be segmented. To address this problem, this paper proposes a novel co-segmentation approach that transfers patch-level local object shapes, which appear more consistently across different images. In our approach, we first employ dense correspondences to construct a patch neighbourhood system, which is refined using Locally Linear Embedding. Based on the patch relationships, an efficient algorithm is developed to jointly segment the objects in each image while transferring their local shapes across different images. Experiments show that our approach performs comparably with or better than the state-of-the-arts on iCoseg dataset, while achieving more than 31% relative improvements on a challenging benchmark Fashionista."
  },
  "bmvc2016_main_smurfssuperpixelsfrommulti-scalerefinementofsuper-regions": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "SMURFS: Superpixels from Multi-scale Refinement of Super-regions",
    "authors": [
      "Imanol Luengo",
      "Mark Basham",
      "Andrew French"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper004/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper004/paper004.pdf",
    "published": "2016-09",
    "summary": "Recent applications in computer vision have come to rely on superpixel segmentation as a pre-processing step for higher level vision tasks, such as object recognition, scene labelling or image segmentation. Here, we present a new algorithm, Superpixels from MUlti-scale ReFinement of Super-regions (SMURFS), which not only obtains state-of-the-art superpixels, but can also be applied hierarchically to form what we call n-th order super-regions. In essence, starting from a uniformly distributed set of super-regions, the algorithm iteratively alternates graph-based split and merge optimization schemes which yield superpixels that better represent the image. The split step is performed over the pixel grid to separate large super-regions into different smaller superpixels. The merging process, conversely, is performed over the superpixel graph to create 2nd-order super-regions (super-segments). Iterative refinement over two scale of regions allows the algorithm to achieve better over-segmentation results than current state-of-the-art methods, as experimental results show on the public Berkeley Segmentation Dataset (BSD500)."
  },
  "bmvc2016_main_biologicallyplausibleboundarydetection": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Biologically plausible boundary detection",
    "authors": [
      "Arash Akbarinia",
      "Alejandro Parraga"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper005/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper005/paper005.pdf",
    "published": "2016-09",
    "summary": "Edges are key components of any visual scene to the extent that we can recognise objects merely by their silhouettes. The human visual system captures edge information through neurons in the visual cortex that are sensitive to both intensity discontinuities and particular orientations. The \"classical approach\" assumes that these cells are only responsive to the stimulus present within their receptive fields, however, recent studies demonstrate that surrounding regions and inter-areal feedback connections influence their responses significantly. In this work we propose a biologically-inspired edge detection model in which orientation selective neurons are represented through the first derivative of a Gaussian function resembling double-opponent cells in the primary visual cortex (V1). In our model we account for four kinds of surround, i.e. full, far, iso- and orthogonal-orientation, whose contributions are contrast-dependant. The output signal from V1 is pooled in its perpendicular direction by larger V2 neurons employing a contrast-variant centre-surround kernel. We further introduce a feedback connection from higher-level visual areas to the lower ones. The results of our model on two benchmark datasets show a big improvement compared to the current non-learning and biologically-inspired state-of-the-art algorithms while being competitive to the learning-based methods."
  },
  "bmvc2016_main_bio-inspiredcollisiondetectorwithenhancedselectivityforgroundroboticvisionsystem": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Bio-inspired Collision Detector with Enhanced Selectivity for Ground Robotic Vision System",
    "authors": [
      "Qinbing Fu",
      "Shigang Yue",
      "Cheng Hu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper006/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper006/paper006.pdf",
    "published": "2016-09",
    "summary": "There are many ways of building collision-detecting systems. In this paper, we propose a novel collision selective visual neural network inspired by LGMD2 neurons in the juvenile locusts. Such collision-sensitive neuron matures early in the first-aged or even hatching locusts, and is only selective to detect looming dark objects against bright background in depth, represents swooping predators, a situation which is similar to ground robots or vehicles. However, little has been done on modeling LGMD2, let alone its potential applications in robotics and other vision-based areas. Compared to other collision detectors, our major contributions are first, enhancing the collision selectivity in a bio-inspired way, via constructing a computing efficient visual sensor, and realizing the revealed specific characteristics of LGMD2. Second, we applied the neural network to help near range path navigation of an autonomous ground miniature robot in an arena. We also examined its neural properties through systematic experiments challenged against image streams from a visual sensor of the micro-robot."
  },
  "bmvc2016_main_adeepprimal-dualnetworkforguideddepthsuper-resolution": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "A Deep Primal-Dual Network for Guided Depth Super-Resolution",
    "authors": [
      "Gernot Riegler",
      "David Ferstl",
      "Matthias R\u00fcther",
      "Horst Bischof"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper007/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper007/paper007.pdf",
    "published": "2016-09",
    "summary": "In this paper we present a novel method to increase the spatial resolution of depth images. We combine a deep fully convolutional network with a non-local variational method in a \\textit{deep primal-dual network}. The joint network computes a noise-free, high-resolution estimate from a noisy, low-resolution input depth map. Additionally, a high-resolution intensity image is used to guide the reconstruction in the network. By unrolling the optimization steps of a first-order primal-dual algorithm and formulating it as a network, we can train our joint method end-to-end. This not only enables us to learn the weights of the fully convolutional network, but also to optimize all parameters of the variational method and its optimization procedure. The training of such a deep network requires a large dataset for supervision. Therefore, we generate high-quality depth maps and corresponding color images with a physically based renderer. In an exhaustive evaluation we show that our method outperforms the state-of-the-art on multiple benchmarks."
  },
  "bmvc2016_main_towardsdeepstyletransferacontent-awareperspective": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Towards Deep Style Transfer: A Content-Aware Perspective",
    "authors": [
      "Yi-Lei Chen",
      "Chiou-Ting Hsu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper008/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper008/paper008.pdf",
    "published": "2016-09",
    "summary": "Modern research has demonstrated that many eye-catching images can be generated by style transfer via deep neural network. There is, however, a dearth of research on content-aware style transfer. In this paper, we generalize the neural algorithm for style transfer from two perspectives: where to transfer and what to transfer. To specify where to transfer, we propose a simple yet effective strategy, named masking out, to constrain the transfer layout. To illustrate what to transfer, we define a new style feature by high-order statistics to better characterize content coherency. Without resorting to additional local matching or MRF models, the proposed method embeds the desired content information, either semantic-aware or saliency-aware, into the original framework seamlessly. Experimental results show that our method is applicable to various types of style transfers and can be extended to image inpainting."
  },
  "bmvc2016_main_real-timeintensity-imagereconstructionforeventcamerasusingmanifoldregularisation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Real-Time Intensity-Image Reconstruction for Event Cameras Using Manifold Regularisation",
    "authors": [
      "Christian Reinbacher",
      "Gottfried Graber",
      "Thomas Pock"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper009/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper009/paper009.pdf",
    "published": "2016-09",
    "summary": "Event cameras or neuromorphic cameras mimic the human perception system as they measure the per-pixel intensity change rather than the actual intensity level. In contrast to traditional cameras, such cameras capture new information about the scene at MHz frequency in the form of sparse events. The high temporal resolution comes at the cost of losing the familiar per-pixel intensity information. In this work we propose a variational model that accurately models the behaviour of event cameras, enabling reconstruction of intensity images with arbitrary frame rate in real-time. Our method is formulated on a per-event-basis, where we explicitly incorporate information about the asynchronous nature of events via an event manifold induced by the relative timestamps of events. In our experiments we verify that solving the variational model on the manifold produces high-quality images without explicitly estimating optical flow."
  },
  "bmvc2016_main_multi-viewmulti-illuminantintrinsicdataset": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Multi-view Multi-illuminant Intrinsic Dataset",
    "authors": [
      "Shida Beigpour",
      "Mai Lan Ha",
      "Sven Kunz",
      "Andreas Kolb",
      "Volker Blanz"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper010/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper010/paper010.pdf",
    "published": "2016-09",
    "summary": "This paper proposes a novel high-resolution multi-view dataset of complex multi-illuminant scenes with precise reflectance and shading ground-truth as well as raw depth and 3D point cloud. Our dataset challenges the intrinsic image methods by providing complex coloured cast shadows, highly textured and colourful surfaces, and specularity. This is the first publicly available multi-view real-photo dataset at such complexity with pixel-wise intrinsic ground-truth. In the effort to help evaluating different intrinsic image methods, we propose a new perception-inspired metric based on the reflectance consistency. We provide the evaluation of three intrinsic image methods using our dataset and metric."
  },
  "bmvc2016_main_accurateclosed-formestimationoflocalaffinetransformationsconsistentwiththeepipolargeometry": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Accurate Closed-form Estimation of Local Affine Transformations Consistent with the Epipolar Geometry",
    "authors": [
      "Daniel Barath",
      "Jiri Matas",
      "Levente Hajder"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper011/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper011/paper011.pdf",
    "published": "2016-09",
    "summary": "For a pair of images satisfying the epipolar constraint, a method for accurate estimation of local affine transformations is proposed. The method returns the local affine transformation consistent with the epipolar geometry that is closest in the least squares sense to the initial estimate provided by an affine-covariant detector. The minimized L2 norm of the affine matrix elements is found in closed-form. We show that the used norm has an intuitive geometric interpretation. The method, with negligible computational requirements, is validated on publicly available benchmarking datasets and on synthetic data. The accuracy of the local affine transformations is improved for all detectors and all image pairs. Implicitly, precision of the tested feature detectors was compared. The Hessian-Affine detector combined with ASIFT view synthesis was the most accurate."
  },
  "bmvc2016_main_recognitionoftransitionalactionforshort-termactionpredictionusingdiscriminativetemporalcnnfeature": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Recognition of Transitional Action for Short-Term Action Prediction using Discriminative Temporal CNN Feature",
    "authors": [
      "Hirokatsu Kataoka",
      "Yudai Miyashita",
      "Masaki Hayashi",
      "Kenji Iwata",
      "Yutaka Satoh"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper012/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper012/paper012.pdf",
    "published": "2016-09",
    "summary": "Herein, we address transitional actions class as a class between actions. Transitional actions should be useful for producing short-term action predictions while an action is transitive. However, transitional action recognition is difficult because actions and transitional actions partially overlap each other. To deal with this issue, we propose a subtle motion descriptor (SMD) that identifies the sensitive differences between actions and transitional actions. The two primary contributions in this paper are as follows: (i) defining transitional actions for short-term action predictions that permit earlier predictions than early action recognition, and (ii) utilizing convolutional neural network (CNN) based SMD to present a clear distinction between actions and transitional actions.Using three different datasets, we will show that our proposed approach produces better results than do other state-of-the-art models. The experimental results clearly show the recognition performance effectiveness of our proposed model, as well as its ability to comprehend temporal motion in transitional actions."
  },
  "bmvc2016_main_multi-hefficientrecoveryoftangentplanesinstereoimages": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Multi-H: Efficient recovery of tangent planes in stereo images",
    "authors": [
      "Daniel Barath",
      "Jiri Matas",
      "Levente Hajder"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper013/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper013/paper013.pdf",
    "published": "2016-09",
    "summary": "Multi-H _ an efficient method for the recovery of the tangent planes of a set of point correspondences satisfying the epipolar constraint is proposed. The problem is formulated as a search for a labeling minimizing an energy that includes a data and spatial regularization terms. The number of planes is controlled by a combination of Mean- Shift and $\\alpha$-expansion. Experiments on the fountain-P11 3D dataset show that Multi-H provides highly accurate tangent plane estimates. It also outperforms all state-of-the-art techniques for multihomography estimation on the publicly available AdelaideRMF dataset. Since Multi-H achieves nearly error-free performance, we introduce and make public a more challenging dataset for multi-plane fitting evaluation."
  },
  "bmvc2016_main_jointlylearningnon-negativeprojectionanddictionarywithdiscriminativegraphconstraintsforclassification": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Jointly Learning Non-negative Projection and Dictionary with Discriminative Graph Constraints for Classification",
    "authors": [
      "Weiyang Liu",
      "Zhiding Yu",
      "Yandong Wen",
      "Rongmei Lin",
      "Meng Yang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper014/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper014/paper014.pdf",
    "published": "2016-09",
    "summary": "Sparse coding with dictionary learning (DL) has shown excellent classification performance. Despite the considerable number of existing works, how to obtain features on top of which dictionaries can be better learned remains an open and interesting question. Many current prevailing DL methods directly adopt well-performing crafted features. While such strategy may empirically work well, it ignores certain intrinsic relationship between dictionaries and features. We propose a framework where features and dictionaries are jointly learned and optimized. The framework, named joint non-negative projection and dictionary learning (JNPDL), enables interaction between the input features and the dictionaries. The non-negative projection leads to discriminative parts-based object features while DL seeks a more suitable representation. Discriminative graph constraints are further imposed to simultaneously maximize intra-class compactness and inter-class separability. Experiments on both image and image set classification show the excellent performance of JNPDL by outperforming several state-of-the-art approaches."
  },
  "bmvc2016_main_amultipathnetworkforobjectdetection": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "A MultiPath Network for Object Detection",
    "authors": [
      "Sergey Zagoruyko",
      "Adam Lerer",
      "Tsung-Yi Lin",
      "Pedro O. Pinheiro",
      "Sam Gross",
      "Soumith Chintala",
      "Piotr Dollar"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper015/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper015/paper015.pdf",
    "published": "2016-09",
    "summary": "The recent COCO object detection dataset presents several new challenges for object detection. In particular, it contains objects at a broad range of scales, less prototypical images, and requires more precise localization. To address these challenges, we test three modifications to the standard Fast R-CNN object detector: (1) skip connections that give the detector access to features at multiple network layers, (2) a foveal structure to exploit object context at multiple object resolutions, and (3) an integral loss function and corresponding network adjustment that improve localization. The result of these modifications is that information can flow along multiple paths in our network, including through features from multiple network layers and from multiple object views. We refer to our modified classifier as a `MultiPath' network. We couple our MultiPath network with DeepMask object proposals, which are well suited for localization and small objects, and adapt our pipeline to predict segmentation masks in addition to bounding boxes. The combined system improves results over the baseline Fast R-CNN detector with Selective Search by 66 overall and by 4x on small objects. It placed second in both the COCO 2015 detection and segmentation challenges."
  },
  "bmvc2016_main_fasteigenmatching": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Fast Eigen Matching",
    "authors": [
      "Yusuke Sekikawa",
      "Koichiro Suzuki",
      "Yuichi Yoshida",
      "Kosuke Hara",
      "Ikuro Sato"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper016/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper016/paper016.pdf",
    "published": "2016-09",
    "summary": "Abstract We propose a method for accelerating the matching and learning processes of the eigenspace method for rotation invariant template matching (RITM). To achieve efficient matching using eigenimages, it is necessary to learn 2D-Fourier transform of eigenimages before matching. Little attentions has been paid to speeding up the learning process, which is important for applications in which a template changes frame by frame. We propose two key ideas: First, to further speedup the matching process using FFT, we decompose rotated templates to orthogonal fast-eigenimages using Fourier basis by utilizing the circularity of rotated templates. Second, to speedup the learning process, we compute 2D-Fourier transform of the fast-eigenimages in polar coordinates using Hankel transform. Proposed learning method is equivalent to but considerably faster than that existing method, i.e., rotated template generation, SVD and 2D-FFTs in Cartesian coordinates. Experiments revealed that the learning, matching and the total processes becomes respectively 120, 3, and 36 times faster while keeping comparable detection rate compared to existing method utilizing SVD in Cartesian coordinates. The algorithm was successfully applied to global localization of mobile robot where online learning is required."
  },
  "bmvc2016_main_recodingcolortransferasacolorhomography": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Recoding Color Transfer as A Color Homography",
    "authors": [
      "Han Gong",
      "Graham Finlayson",
      "Robert Fisher"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper017/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper017/paper017.pdf",
    "published": "2016-09",
    "summary": "Color transfer is an image editing process that adjusts the colors of a picture to match a target picture's color theme. A natural color transfer not only matches the color styles but also prevents after-transfer artifacts due to image compression, noise, and gradient smoothness change. The recently discovered color homography theorem proves that colors across a change in photometric viewing condition are related by a homography. In this paper, we propose a color-homography-based color transfer decomposition which encodes color transfer as a combination of chromaticity shift and shading adjustment. A powerful form of shading adjustment is shown to be a global shading curve by which the same shading homography can be applied elsewhere. Our experiments show that the proposed color transfer decomposition provides a very close approximation to many popular color transfer methods. The advantage of our approach is that the learned color transfer can be applied to many other images (\\eg other frames in a video), instead of a frame-to-frame basis. We demonstrate two applications for color transfer enhancement and video color grading re-application. This simple model of color transfer is also important for future color transfer algorithm design."
  },
  "bmvc2016_main_pose-robust3dfaciallandmarkestimationfromasingle2dimage": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Pose-Robust 3D Facial Landmark Estimation from a Single 2D Image",
    "authors": [
      "Brandon Smith",
      "Charles Dyer"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper018/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper018/paper018.pdf",
    "published": "2016-09",
    "summary": "An algorithm is presented that estimates 3D facial landmark coordinates and occlusion state from a single 2D image. Unlike previous approaches, we divide the 3D cascaded shape regression problem into a set of viewpoint domains, which helps avoid problems in the optimization, such as local minima at test time, and averaging conflicting gradient directions in the domain maps during training. These problems are especially important to address in the 3D case, where a wider range of head poses is expected. Parametric shape models are used and are shown to have several desirable qualities compared to the recent trend of modeling shape nonparametrically. Results show quantitatively that our approach is significantly more accurate than recent work."
  },
  "bmvc2016_main_bottom-upinstancesegmentationusingdeephigher-ordercrfs": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Bottom-up Instance Segmentation using Deep Higher-Order CRFs",
    "authors": [
      "Anurag Arnab",
      "Philip Torr"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper019/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper019/paper019.pdf",
    "published": "2016-09",
    "summary": "Traditional Scene Understanding problems such as Object Detection and Semantic Segmentation have made breakthroughs in recent years due to the adoption of deep learning. However, the former task is not able to localise objects at a pixel level, and the latter task has no notion of different instances of objects of the same class. We focus on the task of Instance Segmentation which recognises and localises objects down to a pixel level. Our model is based on a deep neural network trained for semantic segmentation. This network incorporates a Conditional Random Field with end-to-end trainable higher order potentials based on object detector outputs. This allows us to reason about instances from an initial, category-level semantic segmentation. Our simple method effectively leverages the great progress recently made in semantic segmentation and object detection. The accurate instance-level segmentations that our network produces is reflected by the considerable improvements obtained over previous work at high $AP^r$ IoU thresholds."
  },
  "bmvc2016_main_horizonlinesinthewild": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Horizon Lines in the Wild",
    "authors": [
      "Scott Workman",
      "Menghua Zhai",
      "Nathan Jacobs"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper020/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper020/paper020.pdf",
    "published": "2016-09",
    "summary": "The horizon line is an important contextual attribute for a wide variety of image understanding tasks. As such, many methods have been proposed to estimate its location from a single image. These methods typically require the image to contain specific cues, such as vanishing points, coplanar circles, and regular textures, thus limiting their real-world applicability. We introduce a large, realistic evaluation dataset, Horizon Lines in the Wild (HLW), containing natural images with labeled horizon lines. Using this dataset, we investigate the application of convolutional neural networks for directly estimating the horizon line, without requiring any explicit geometric constraints or other special cues. An extensive evaluation shows that using our CNNs, either in isolation or in conjunction with a previous geometric approach, we achieve state-of-the-art results on the challenging HLW dataset and two existing benchmark datasets."
  },
  "bmvc2016_main_anoctree-basedapproachtowardsefficientvariationalrangedatafusion": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "An Octree-Based Approach towards Efficient Variational Range Data Fusion",
    "authors": [
      "Wadim Kehl",
      "Tobias Holl",
      "Federico Tombari",
      "Slobodan Ilic",
      "Nassir Navab"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper021/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper021/paper021.pdf",
    "published": "2016-09",
    "summary": "Volume-based reconstruction is usually expensive both in terms of memory consumption and runtime. Especially for sparse geometric structures, volumetric representations produce a huge computational overhead. We present an efficient way to fuse range data via a variational Octree-based minimization approach by taking the actual range data geometry into account. We transform the data into Octree-based truncated signed distance fields and show how the optimization can be conducted on the newly created structures. The main challenge is to uphold speed and a low memory footprint without sacrificing the solutions' accuracy during optimization. We explain how to dynamically adjust the optimizer's geometric structure via joining/splitting of Octree nodes and how to define the operators. We evaluate on various datasets and outline the suitability in terms of performance and geometric accuracy."
  },
  "bmvc2016_main_finslergeodesicsevolutionmodelforregionbasedactivecontours": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Finsler Geodesics Evolution Model for Region based Active Contours",
    "authors": [
      "Da Chen",
      "Jean-Marie Mirebeau",
      "Laurent Cohen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper022/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper022/paper022.pdf",
    "published": "2016-09",
    "summary": "In this paper, we present a new deformable model for image segmentation by building the connection between the region based active contours energy and the geodesic energy via a Finsler metric. Basically, we solve the region based active contours energy minimization problem under a nonlinear Eikonal equation framework with respect to a Finsler metric. By sampling a set of control points from the closed active contour in clockwise order, the active contours evolution problem is turned to find a collection of minimal curves joining all the control points. These minimal curves can be obtained by solving the Finsler metric based Eikonal equation. Benefiting from the well established numerical solver of the Eikonal PDE, named fast marching algorithm, our model can obtain a fast numerical solution."
  },
  "bmvc2016_main_patchbasedconfidencepredictionfordensedisparitymap": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Patch Based Confidence Prediction for Dense Disparity Map",
    "authors": [
      "Akihito Seki",
      "Marc Pollefeys"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper023/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper023/paper023.pdf",
    "published": "2016-09",
    "summary": "In this paper, we propose a novel method to predict the correctness of stereo correspondences, which we call confidence, and a confidence fusion method for dense disparity estimation. The input of our method consists in a two channels local window (disparity patch) which is designed by taking into account ideas of conventional confidence features. 1st channel is coming from the idea that neighboring pixels which have consistent disparities are more likely to be correct matching. In 2nd channel, a disparity from another image is considered such that the matches from left to right image should be consistent with those from right to left. The disparity patches are used as inputs of Convolutional Neural Networks so that the features and classifiers are simultaneously trained unlike what is done by existing methods. Moreover, the confidence is incorporated into Semi-Global Matching(SGM) by adjusting its parameters directly. We show the prominent performance of both confidence prediction and dense disparity estimation on KITTI datasets which are real world scenery."
  },
  "bmvc2016_main_boostedconvolutionalneuralnetworks": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Boosted Convolutional Neural Networks",
    "authors": [
      "Mohammad Moghimi",
      "Serge Belongie",
      "Mohammad Saberian",
      "Jian Yang",
      "Nuno Vasconcelos",
      "Li-Jia Li"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper024/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper024/paper024.pdf",
    "published": "2016-09",
    "summary": "In this work, we propose a new algorithm for boosting Deep Convolutional Neural Networks (BoostCNN) to combine the merits of boosting and these networks. To learn this new model, we propose a novel algorithm to incorporate boosting weights into the deep learning architecture based on least square objective function. We also show that it is possible to use networks of different structures within the proposed boosting framework and BoostCNN is able to select the best network structure in each iteration. This not only results in superior performance but also reduces the required manual effort for finding the right network structure. Experiments show that the proposed method is able to achieve state-of-the-art performance on several fine-grained classification tasks such as bird, car, and aircraft classification."
  },
  "bmvc2016_main_material-specificchromaticitypriors": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Material-Specific Chromaticity Priors",
    "authors": [
      "Jeroen Put",
      "Nick Michiels"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper025/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper025/paper025.pdf",
    "published": "2016-09",
    "summary": "Recent advances in machine learning have enabled the recognition of high-level categories of materials with a reasonable accuracy. With these techniques, we can construct a per-pixel material labeling from a single image. We observe that groups of high-level material categories have distinct chromaticity distributions. This fact can be used to predict the range of the absolute chromaticity values of objects, provided the material is correctly labeled. We explore whether these constraints are useful in the context of the intrinsic images problem. This paper describes how to leverage material category identification to boost estimation results in state-of-the-art intrinsic images datasets."
  },
  "bmvc2016_main_playandlearnusingvideogamestotraincomputervisionmodels": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Play and Learn: Using Video Games to Train Computer Vision Models",
    "authors": [
      "Alireza Shafaei",
      "James Little",
      "Mark Schmidt"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper026/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper026/paper026.pdf",
    "published": "2016-09",
    "summary": "Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision."
  },
  "bmvc2016_main_sdf-tarparalleltrackingandrefinementinrgb-ddatausingvolumetricregistration": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "SDF-TAR: Parallel Tracking and Refinement in RGB-D Data using Volumetric Registration",
    "authors": [
      "Miroslava Slavcheva",
      "Slobodan Ilic"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper027/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper027/paper027.pdf",
    "published": "2016-09",
    "summary": "This paper introduces SDF-TAR: a real-time SLAM system based on volumetric registration in RGB-D data. While the camera is tracked online on the GPU, the most recently estimated poses are jointly refined on the CPU. We perform registration by aligning the data in limited-extent volumes anchored at salient 3D locations. This strategy permits efficient tracking on the GPU. Furthermore, the small memory load of the partial volumes allows for pose refinement to be done concurrently on the CPU. This refinement is performed over batches of a fixed number of frames, which are jointly optimized until the next batch becomes available. Thus drift is reduced during online operation, eliminating the need for any posterior processing. Evaluating on two public benchmarks, we demonstrate improved rotational motion estimation and higher reconstruction precision than related methods."
  },
  "bmvc2016_main_probabilisticsemi-supervisedmulti-modalhashing": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Probabilistic Semi-Supervised Multi-Modal Hashing",
    "authors": [
      "Behnam Gholami",
      "Abolfazl Hajisami"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper028/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper028/paper028.pdf",
    "published": "2016-09",
    "summary": "Learning hash functions for high dimensional multi-modal data is of great interest for many real-world retrieval applications in which data comes from diverse heterogeneous sources. In this paper, we propose a novel probabilistic semi-supervised multi-modal retrieval model, by which we can learn both the binary codes and their dimension from the available training data. We also develop a new Variational Bayes (VB) algorithm for learning the parameters of the proposed model. The experiments on two real-world data sets show the superiority of the proposed method over other state-of-the-art algorithms for learning binary codes."
  },
  "bmvc2016_main_learningtoinvertlocalbinarypatterns": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning to Invert Local Binary Patterns",
    "authors": [
      "Felix Juefei-Xu",
      "Marios Savvides"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper029/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper029/paper029.pdf",
    "published": "2016-09",
    "summary": "In this work, we have proposed to invert the local binary patterns (LBP) descriptor. The success of the inversion gives rise to two applications: face de-appearance and re-appearance. The de-appearance, based on image-LBP forward mapping, is thorough in the sense that not only the identity information but also the soft-biometric information of the subject is removed. The re-appearance yields face reconstruction with high fidelity and also enables secure application with a unique encryption key. The re-appearance is achieved by learning the inverse mapping of the LBP descriptors through an $\\ell_0$-constrained coupled dictionary learning scheme that jointly learns two overcomplete dictionaries in both the pixel and the LBP domains such that inverse mapping from the LBP domain to the pixel domain is made possible without knowing the mapping function explicitly. The procedure also comes naturally with high selectivity when reconstructing the faces with various LBP encryption keys. We have shown the effectiveness of our proposed approach on the FRGC ver 2.0 database which involves large-scale fidelity test and face verification experiments using the state-of-the-art commercial and academic face matchers."
  },
  "bmvc2016_main_probabilisticcompositionalactivebasismodelsforrobustpatternrecognition": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Probabilistic Compositional Active Basis Models for Robust Pattern Recognition",
    "authors": [
      "Adam Kortylewski",
      "Thomas Vetter"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper030/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper030/paper030.pdf",
    "published": "2016-09",
    "summary": "Hierarchical compositional models (HCMs) have shown impressive generalisation capabilities, especially compared to the small amounts of data needed for training. However, regarding occlusion and other non-linear pattern distortions, experimental setups have been controlled so far. In this work, we study the robustness of HCMs under such more challenging pattern recognition conditions. Our contribution is three-fold: First, we introduce a greedy EM-type algorithm to automatically infer the structure of compositional active basis models (CABMs). Second, we formulate the proposed representation and its learning process in a fully probabilistic manner. Finally, building on the statistical framework, we augment the CABM with an implicit geometric background model that reduces the models sensitivity to pattern occlusions and background clutter. In order to demonstrate the robustness of the proposed object representation, we evaluate it on a complex forensic image analysis task. We demonstrate that probabilistic CABMs are capable of recognising patterns under complex non-linear distortions that can hardly be represented by a finite set of training data. Experimental results show that the forensic image analysis task is processed with unprecedented quality."
  },
  "bmvc2016_main_logletsiftforpartdescriptionindeformablepartmodelsapplicationtofacealignment": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Loglet SIFT for Part Description in Deformable Part Models: Application to Face Alignment",
    "authors": [
      "Qiang Zhang",
      "Abhir Bhalerao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper031/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper031/paper031.pdf",
    "published": "2016-09",
    "summary": "We focus on a novel loglet-SIFT descriptor for the parts representation in the Deformable Part Models (DPM). We manipulate the feature scales in the Fourier domain and decompose the image into multi-scale oriented gradient components for computing SIFT. The scale selection is controlled explicitly by tiling Log-wavelet functions (loglets) on the spectrum. Then oriented gradients are obtained by adding imaginary odd parts to the loglets, converting them into differential filters. Coherent feature scales and domain sizes are further generated by spectrum cropping. Our loglet gradient filters are shown to compare favourably against spatial differential operators, and have a straightforward and efficient implementation. We present experiments to validate the performance of the loglet-SIFT descriptor which show it to improve the DPM using a supervised descent method by a significant margin."
  },
  "bmvc2016_main_dictionaryreplacementforsingleimagerestorationof3dscenes.": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Dictionary Replacement for Single Image Restoration of 3D Scenes.",
    "authors": [
      "Nimisha T M",
      "Arun Mathamkode",
      "Rajagopalan Ambasamudram"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper032/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper032/paper032.pdf",
    "published": "2016-09",
    "summary": "In this paper, we address the problem of jointly estimating the latent image and the depth/blur map from a single space-variantly blurred image using dictionary learning. The approach taken is based on the central idea of dictionary replacement viz. the sparse representation of a blurred image over a blurred dictionary is equivalent to that over a clean dictionary. While most of the dictionary-based deblurring methods consider planar scenes with space-invariant blur, we handle 3D scenes with space-variant blur caused by either camera motion or optical defocus. For a given blurred image, the dictionary blurred with the corresponding blur kernel provides the best representation with the least error. We formulate our problem of blur map and latent image estimation as a multi-label MRF and solve it using graph-cut. Experimental results on defocus as well as motion blur depict the effectiveness of our scheme."
  },
  "bmvc2016_main_poissonnoiseremovalforimagedemosaicing": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Poisson Noise Removal for Image Demosaicing",
    "authors": [
      "Sukanya Patil",
      "Ajit Rajwade"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper033/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper033/paper033.pdf",
    "published": "2016-09",
    "summary": "With increasing resolution of the sensors in camera detector arrays, acquired images are ever more susceptible to perturbations that appear as grainy artifacts called `noise'. In real acquisitions, the dominant noise model has been shown to follow the Poisson distribution, which is signal dependent. Most color image cameras today acquire only one out of the R, G, B values per pixel by means of a color filter array in the hardware, and in-built software routines have to undertake the task of obtaining the rest of the color information at each pixel through a process termed demosaicing. The presence of the Poisson noise can significantly degrade the output of a demosaicing algorithm. In this paper, we propose and compare two dictionary learning methods to remove the Poisson noise from the single channel images by directly solving a Poisson likelihood problem or performing a variance stabilizer transform prior to demosaicing. Experimental results on simulated noisy images as well as real camera acquisitions, show the advantage of these methods over approaches that remove noise subsequent to demosaicing."
  },
  "bmvc2016_main_factorizedbinarycodesforlarge-scalenearestneighborsearch": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Factorized Binary Codes for Large-Scale Nearest Neighbor Search",
    "authors": [
      "Frederick Tung",
      "James Little"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper034/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper034/paper034.pdf",
    "published": "2016-09",
    "summary": "Hashing algorithms for fast large-scale nearest neighbor search transform data points into compact binary codes by applying a set of learned or randomly generated hash functions. Retrieval accuracy generally increases with the number of hash functions, but increasing the number of hash functions also increases the storage requirements of the resulting binary codes. We present a novel factorized binary codes approach that uses an approximate matrix factorization of the binary codes to increase the number of hash functions while maintaining the original storage requirements. The proposed approach does not assume a particular algorithm for generating the hash functions, and requires only that we can discover and take advantage of correlations among the hash functions. Experiments on publicly available datasets suggest that factorized binary codes work particularly well for locality-sensitive hashing algorithms."
  },
  "bmvc2016_main_edgeenhanceddirectvisualodometry": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Edge Enhanced Direct Visual Odometry",
    "authors": [
      "Xin Wang",
      "Wei Dong",
      "Mingcai Zhou",
      "Renju Li",
      "Hongbin Zha"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper035/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper035/paper035.pdf",
    "published": "2016-09",
    "summary": "We propose an RGB-D visual odometry method that both minimizes the photometric error and aligns the edges between frames. The combination of the direct photometric information and the edge features leads to higher tracking accuracy and allows the approach to deal with challenging texture-less scenes. In contrast to traditional line feature based methods, we use all edges rather than only line segments, avoiding aperture problem and the uncertainty of endpoints. Instead of explicitly matching edge features, we design a dense representation of edges to align them, bridging the direct methods and the feature-based methods in tracking. Image alignment and feature matching are performed in a general framework, where not only pixels but also salient visual landmarks are aligned. Evaluations on real-world benchmark datasets show that our method achieves competitive results in indoor scenes, especially in texture-less scenes where it outperforms the state-of-the-art algorithms."
  },
  "bmvc2016_main_optimisedphotometricstereovianon-convexvariationalminimisation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Optimised photometric stereo via non-convex variational minimisation",
    "authors": [
      "Laurent Hoeltgen",
      "Yvain Queau",
      "Michael Breu\u00df",
      "Georg Radow"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper036/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper036/paper036.pdf",
    "published": "2016-09",
    "summary": "Estimating the shape and appearance of a three dimensional object from flat images is a challenging research topic that is still actively pursued. Among the various techniques available, Photometric Stereo is known to provide very accurate local shape recovery, in terms of surface normals. In this work, we propose to minimise non-convex variational models for Photometric Stereo that recover the depth information directly. We suggest an approach based on a novel optimisation scheme for non-convex cost functions. Experiments show that our strategy achieves more accurate results than competing approaches."
  },
  "bmvc2016_main_u-shapednetworksforshapefromlightfield": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "U-shaped Networks for Shape from Light Field",
    "authors": [
      "Stefan Heber",
      "Wei Yu",
      "Thomas Pock"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper037/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper037/paper037.pdf",
    "published": "2016-09",
    "summary": "This paper presents a novel technique for Shape from Light Field (SfLF), that utilizes deep learning strategies. Our model is based on a fully convolutional network, that involves two symmetric parts, an encoding and a decoding part, leading to a u-shaped network architecture. By leveraging a recently proposed Light Field (LF) dataset, we are able to effectively train our model using supervised training. To process an entire LF we split the LF data into the corresponding Epipolar Plane Image (EPI) representation and predict each EPI separately. This strategy provides good reconstruction results combined with a fast prediction time. In the experimental section we compare our method to the state of the art. The method performs well in terms of depth accuracy, and is able to outperform competing methods in terms of prediction time by a large margin."
  },
  "bmvc2016_main_usingshadinganda3dtemplatetoreconstructcomplexsurfacedeformations": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Using Shading and a 3D Template to Reconstruct Complex Surface Deformations",
    "authors": [
      "Mathias Gallardo",
      "Toby Collins",
      "Adrien Bartoli"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper038/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper038/paper038.pdf",
    "published": "2016-09",
    "summary": "The goal of Shape-from-Template (SfT) is to register and reconstruct the 3D shape of a deforming surface from a single image and a known deformable 3D template. Most SfT methods use only motion information and require well-textured surfaces which deform smoothly. Consequently they are unsuccessful for poorly-textured surfaces with complex deformations such as creases. We propose to combine shading and motion to handle these cases. There exist a few previous works which also exploit shading. However these do not provide an integrated solution, and they assume the surface and its deformations are globally smooth. Furthermore, most require a training phase and a priori photometric calibration for surface reflectance, camera response and/or light, which is often not possible. We propose an integrated solution without these shortcomings by jointly reconstructing the surface and performing photometric auto-calibration, considering an unknown light source which is constant and fixed in the camera coordinates. We evaluate with qualitative and quantitative results and show that it is possible to accurately register and reconstruct poorly-textured surfaces with complex deformations without any a priori photometric calibration."
  },
  "bmvc2016_main_physics101learningphysicalobjectpropertiesfromunlabeledvideos": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Physics 101: Learning Physical Object Properties from Unlabeled Videos",
    "authors": [
      "Jiajun Wu",
      "Joseph Lim",
      "Hongyi Zhang",
      "Joshua Tenenbaum",
      "William Freeman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper039/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper039/paper039.pdf",
    "published": "2016-09",
    "summary": "We study the problem of learning physical properties of objects from unlabeled videos. Humans can learn basic physical laws when they are very young, which suggests that such tasks may be important goals for computational vision systems. We consider various scenarios: objects sliding down an inclined surface and colliding; objects attached to a spring; objects falling onto various surfaces, etc. Many physical properties like mass, density, and coefficient of restitution influence the outcome of these scenarios, and our goal is to recover them automatically. We have collected 17,408 video clips containing 101 objects of various materials and appearances (shapes, colors, and sizes). Together, they form a dataset, named Physics 101, for studying object-centered physical properties. We propose an unsupervised representation learning model, which explicitly encodes basic physical laws into the structure and use them, with automatically discovered observations from videos, as supervision. Experiments demonstrate that our model can learn physical properties of objects from video. We also illustrate how its generative nature enables solving other tasks such as outcome prediction."
  },
  "bmvc2016_main_attributeembeddingwithvisual-semanticambiguityremovalforzero-shotlearning": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Attribute Embedding with Visual-Semantic Ambiguity Removal for Zero-shot Learning",
    "authors": [
      "Yang Long",
      "Li Liu",
      "Ling Shao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper040/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper040/paper040.pdf",
    "published": "2016-09",
    "summary": "Conventional zero-shot learning (ZSL) methods recognise an unseen instance by projecting its visual features to a semantic space that is shared by both seen and unseen categories. However, we observe that such a one-way paradigm suffers from the visual-semantic ambiguity problem. Namely, the semantic concepts (e.g. attributes) cannot explicitly correspond to visual patterns, and vice versa. Such a problem can lead to a huge variance in the visual features for each attribute. In this paper, we investigate how to remove such semantic ambiguity based on the observed visual appearances. In particular, we propose (1) a novel latent attribute space to mitigate the gap between visual appearances and semantic expressions; (2) a dual-graph regularised embedding algorithm called Visual-Semantic Ambiguity Removal (VSAR) that can simultaneously extract the shared components between visual and semantic information and mutually align the data distribution based on the intrinsic local structures of both spaces; (3) a new zero-shot recognition framework that can deal with both instance-level and category-level ZSL tasks. We validate our method on two popular zero-shot learning datasets, AwA and aPY. Extensive experiments demonstrate that our proposed approach significantly outperforms the state-of-the-art methods."
  },
  "bmvc2016_main_nrsfm-flowrecoveringnon-rigidsceneflowfrommonocularimagesequences": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "NRSfM-Flow: Recovering Non-Rigid Scene Flow from Monocular Image Sequences",
    "authors": [
      "Vladislav Golyanik",
      "Aman Shankar  Mathur",
      "Didier Stricker"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper041/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper041/paper041.pdf",
    "published": "2016-09",
    "summary": "Scene flow recovery from monocular image sequences is an emerging field in computer vision. While existing Monocular Scene Flow (MSF) methods extend the classical optical flow formulation to estimate depths/disparities and 3D motion, we propose a framework based on Non-Rigid Structure from Motion (NRSfM) technique \u201e NRSfM-Flow. Therefore, both problems are formulated in the continuous domain and relation between them is established. To cope with real data, we propose two preprocessing steps for image sequences \u201e redundancy removal and translation resolution \u201e which increase quality of reconstructions and speedup computations. In contrast to the existing MSF methods which can cope with non-rigid deformations, our solution makes no strong assumptions about a scene such as known camera motion or camera velocity constancy and can handle occlusions. NRSfM-Flow is qualitatively evaluated on challenging real-world data. Experiments provide evidence that the proposed approach achieves high accuracy and outperforms state of the art in terms of the ability to reconstruct MSF with less prior knowledge about a scene."
  },
  "bmvc2016_main_bettertogetherjointreasoningfornon-rigid3dreconstructionwithspecularitiesandshading": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Better Together: Joint Reasoning for Non-rigid 3D Reconstruction with Specularities and Shading",
    "authors": [
      "Qi Liu-Yin",
      "Rui Yu",
      "Lourdes Agapito",
      "Andrew Fitzgibbon",
      "Chris Russell"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper042/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper042/paper042.pdf",
    "published": "2016-09",
    "summary": "In this paper, we demonstrate the use of shape-from-shading (SFS) to improve both the quality and the robustness of 3D reconstruction of dynamic objects captured by a single camera. Unlike previous approaches that made use of SFS~as a post-processing step, we offer a principled integrated approach that solves dynamic object tracking and reconstruction and SFS~as a single unified cost function. Moving beyond Lambertian SFS, we propose a general approach that models both specularities and shading while simultaneously tracking and reconstructing general dynamic objects. Solving these problems jointly prevents the kinds of tracking failures which can not be recovered from by pipeline approaches. We show state-of-the-art results both qualitatively and quantitatively."
  },
  "bmvc2016_main_star-netaspatialattentionresiduenetworkforscenetextrecognition": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "STAR-Net: A SpaTial Attention Residue Network for Scene Text Recognition",
    "authors": [
      "Wei Liu",
      "Chaofeng Chen",
      "Kwan-Yee K. Wong",
      "Zhizhong Su",
      "Junyu Han"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper043/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper043/paper043.pdf",
    "published": "2016-09",
    "summary": "In this paper, we present a novel SpaTial Attention Residue Network (STAR-Net) for recognising scene texts. Our STAR-Net is equipped with a spatial attention mechanism which employs a spatial transformer to remove the distortions of texts in natural images. This allows the subsequent feature extractor to focus on the rectified text region without being sidetracked by the distortions. Our STAR-Net also exploits residue convolutional blocks to build a very deep feature extractor, which is essential to the successful extraction of discriminative text features for this fine grained recognition task. Combining the spatial attention mechanism with the residue convolutional blocks, our STAR-Net is the deepest end-to-end trainable neural network for scene text recognition. Experiments have been conducted on five public benchmark datasets. And the results show that our STAR-Net can achieve a performance comparable to state-of-the-art methods for scene texts with little distortions, and outperform these methods for scene texts with considerable distortions."
  },
  "bmvc2016_main_contextmattersrefiningobjectdetectioninvideowithrecurrentneuralnetworks": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Context Matters: Refining Object Detection in Video with Recurrent Neural Networks",
    "authors": [
      "Subarna Tripathi",
      "Zachary  Lipton",
      "Serge Belongie",
      "Truong Nguyen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper044/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper044/paper044.pdf",
    "published": "2016-09",
    "summary": "Given the vast amounts of video available online, and recent breakthroughs in object detection with static images, object detection in video offers a promising new frontier. However, motion blur and compression artifacts cause substantial frame-level variability, even in videos that appear smooth to the eye. Additionally, video datasets tend to have sparsely annotated frames. We present a new framework for improving object detection in videos that captures temporal context and encourages consistency of predictions. First, we train a pseudo-labeler, that is, a domain-adapted convolutional neural network for object detection. The pseudo-labeler is first trained individually on the subset of labeled frames, and then subsequently applied to all frames. Then we train a recurrent neural network that takes as input sequences of pseudo-labeled frames and optimizes an objective that encourages both accuracy on the target frame and consistency across consecutive frames. The approach incorporates strong supervision of target frames, weak-supervision on context frames, and regularization via a smoothness penalty. Our approach achieves mean Average Precision (mAP) of 68.73, a 7.1 improvement over the strongest image-based baselines for the Youtube-Video Objects dataset. Our experiments demonstrate that neighboring frames can provide valuable information, even absent labels."
  },
  "bmvc2016_main_outlierrejectionforabsoluteposeestimationwithknownorientation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Outlier Rejection for Absolute Pose Estimation with Known Orientation",
    "authors": [
      "Viktor Larsson",
      "Johan Fredriksson",
      "Carl Toft",
      "Fredrik Kahl"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper045/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper045/paper045.pdf",
    "published": "2016-09",
    "summary": "Estimating the pose of a camera is a core problem in many geometric vision applications. While there has been much progress in the last two decades, the main difficulty is still dealing with data contaminated by outliers. For many scenes, e.g. with poor lightning conditions or repetitive textures, it is common that most of the correspondences are outliers. For real applications it is therefore essential to have robust estimation methods. In this paper we present an outlier rejection method for absolute pose estimation. We focus on the special case when the orientation of the camera is known. The problem is solved by projecting to a lower dimensional subspace where we are able to efficiently compute upper bounds on the maximum number of inliers. The method guarantees that only correspondences which cannot belong to an optimal pose are removed. In a number of challenging experiments we evaluate our method on both real and synthetic data and show improved performance compared to competing methods."
  },
  "bmvc2016_main_learningfromscratchaconfidencemeasure": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning from scratch a confidence measure",
    "authors": [
      "Matteo Poggi",
      "Stefano Mattoccia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper046/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper046/paper046.pdf",
    "published": "2016-09",
    "summary": "Stereo vision is a popular technique to infer depth from two or more images. In this field, confidence measures, typically obtained from the analysis of the cost volume, aim at detecting uncertain disparity assignments. As recently proved, multiple confidence measures combined with hand-crafted features extracted from the cost volume can be used also for other purposes and in particular to improve the overall disparity accuracy leveraging on machine learning techniques. In this paper, starting from the observation that recurrent local patterns occurring in the disparity maps can tell a correct assignment from a wrong one, we follow a completely different methodology to infer a novel confidence measure from scratch. Specifically, leveraging on Convolutional Neural Networks, we pose the confidence formulation as a regression problem by analyzing the disparity map provided by a stereo vision system. Once trained on a subset of the KITTI 2012 dataset with the disparity maps provided by the simple block-matching algorithm, our confidence measure outperforms state-of-the-art with two datasets (KITTI 2015 and Middlebury 2014) as well as with two stereo algorithms. The experimental evaluation reported clearly highlights that our approach is capable to better generalize its behavior in different circumstances with respect to state-of-the-art. Finally, not being based on cost volume analysis, our proposal is also potentially suited for out-of-the-box depth generation devices which usually do not expose the cues required by top-performing approaches."
  },
  "bmvc2016_main_localizingperiodicityintimeseriesandvideos": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Localizing Periodicity in Time Series and Videos",
    "authors": [
      "Giorgos Karvounas",
      "Iason Oikonomidis",
      "Antonis Argyros"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper047/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper047/paper047.pdf",
    "published": "2016-09",
    "summary": "Periodicity detection is a problem that has received a lot of attention, thus several important tools exist to analyse purely periodic signals. However, in many real world scenarios (time series, videos of human activities, etc) periodic signals appear in the context of non-periodic ones. In this work we propose a method that, given a time series representing a periodic signal that has a non-periodic prefix and tail, estimates the start, the end and the period of the periodic part of the signal. We formulate this as an optimization problem that is solved based on evolutionary optimization techniques. Quantitative experiments on synthetic data demonstrate that the proposed method is successful in localizing the periodic part of a signal and exhibits robustness in the presence of noisy measurements. Also, it does so even when the periodic part of the signal is too short compared to its non-periodic prefix and tail. We also provide quantitative and qualitative results obtained from the application of the proposed method to the problem of unsupervised localization and segmentation of periodic activities in real world videos."
  },
  "bmvc2016_main_personre-identificationinappearanceimpairedscenarios": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Person Re-identification in Appearance Impaired Scenarios",
    "authors": [
      "Mengran Gou",
      "Xikang  Zhang",
      "Angels Rates-Borras",
      "Sadjad Asghari-Esfeden",
      "Octavia Camps",
      "Mario Sznaier"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper048/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper048/paper048.pdf",
    "published": "2016-09",
    "summary": "Person re-identification is critical in surveillance applications. Current approaches rely on appearance-based features extracted from a single or multiple shots of the target and candidate matches. These approaches are at a disadvantage when trying to distinguish between candidates dressed in similar colors or when targets change their clothing. In this paper we propose a dynamics-based feature to overcome this limitation. The main idea is to capture soft biometrics from gait and motion patterns by gathering dense short trajectories (tracklets) which are Fisher vector encoded. To illustrate the merits of the proposed features we introduce three new \u00f1appearance-impaired\u00ee datasets. Our experiments demonstrate the benefits of incorporating dynamics-based information into re-identification algorithms."
  },
  "bmvc2016_main_learningtodetectandmatchkeypointswithdeeparchitectures": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning to Detect and Match Keypoints with Deep Architectures",
    "authors": [
      "Hani Altwaijry",
      "Andreas Veit",
      "Serge Belongie"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper049/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper049/paper049.pdf",
    "published": "2016-09",
    "summary": "Feature detection and description is a pivotal step in many computer vision pipelines. Traditionally, human engineered features have been the main workhorse in this domain. In this paper, we present a novel approach for learning to detect and describe keypoints from images leveraging deep architectures. To allow for a learning based approach, we collect a large-scale dataset of patches with matching multiscale keypoints. The proposed model learns from this vast dataset to identify and describe meaningful keypoints. We evaluate our model for the effectiveness of its learned representations for detecting multiscale keypoints and describing their respective support regions."
  },
  "bmvc2016_main_supervisedincrementalhashing": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Supervised Incremental Hashing",
    "authors": [
      "Bahadir Ozdemir",
      "Mahyar Najibi",
      "Larry Davis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper050/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper050/paper050.pdf",
    "published": "2016-09",
    "summary": "We propose an incremental strategy for learning hash functions with kernels for large-scale image search. Our method is based on a two-stage classification framework that treats binary codes as intermediate variables between the feature space and the semantic space. In the first stage of classification, binary codes are considered as class labels by a set of binary SVMs; each corresponds to one bit. In the second stage, binary codes become the input space of a multi-class SVM. Hash functions are learned by an efficient algorithm where the NP-hard problem of finding optimal binary codes is solved via cyclic coordinate descent and SVMs are trained in a parallelized incremental manner. For modifications like adding images from a previously unseen class, we describe an incremental procedure for effective and efficient updates to the previous hash functions. Experiments on three large-scale image datasets demonstrate the effectiveness of the proposed hashing method, Supervised Incremental Hashing (SIH), over the state-of-the-art supervised hashing methods."
  },
  "bmvc2016_main_multi-scalefullyconvolutionalnetworkforfastfacedetection": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Multi-Scale Fully Convolutional Network for Fast Face Detection",
    "authors": [
      "Yancheng Bai",
      "Wenjing Ma",
      "Yucheng Li",
      "Liangliang Cao",
      "Wen Guo",
      "Luwei Yang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper051/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper051/paper051.pdf",
    "published": "2016-09",
    "summary": "Image pyramid is a common strategy in detecting objects with different scales in an image. The computation of features at every scale of a finely-sampled image pyramid is the computational bottleneck of many modern face detectors. To deal with this problem, we propose a multi-scale fully convolutional network framework for face detection. In our detector, face models at different scales are trained end-to-end and they share the same convolutional feature maps. During testing, only images at octave-spaced scale intervals need to be processed by our detector. And faces of different scales between two consecutive octaves can be detected by multi-scale models in our system. This makes our detector very efficient and can run about 100 FPS on a GPU for VGA images. Meanwhile, our detector shows superior performance over most of state-of-the-art ones on three challenging benchmarks, including FDDB, AFW, and PASCAL faces."
  },
  "bmvc2016_main_attentionnetworksforweaklysupervisedobjectlocalization": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Attention Networks for Weakly Supervised Object Localization",
    "authors": [
      "Eu Wern Teh",
      "Mrigank Rochan",
      "Yang Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper052/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper052/paper052.pdf",
    "published": "2016-09",
    "summary": "We consider the problem of weakly supervised learning for object localization. Given a collection of images with image-level annotation indicating the presence/absence of an object, our goal is to localize the object in each image. In this paper, we propose an deep network architecture called attention network for this problem. Given a set candidate regions in each image, the attention network first computes an attention score on each candidate region. Then these candidate regions are combined together with their attention scores to form a whole-image feature vector. This feature vector is used for classifying the image. The object localization is implicitly achieved via the attention scores on candidate regions. We demonstrate the our approach achieves performance comparable to or better than other state-of-the-art methods on several benchmark datasets."
  },
  "bmvc2016_main_imagecaptioningwithsentimenttermsviaweakly-supervisedsentimentdataset": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Image Captioning with Sentiment Terms via Weakly-Supervised Sentiment Dataset",
    "authors": [
      "Andrew Shin",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper053/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper053/paper053.pdf",
    "published": "2016-09",
    "summary": "Image captioning task has become a highly competitive research area with successful application of convolutional and recurrent neural networks, especially with the advent of long short-term memory (LSTM) architecture. However, its primary focus has been a factual description of the images, including the objects, movements, and their relations. While such focus has demonstrated competence, describing the images along with non-factual elements, namely sentiments of the images expressed via adjectives, has mostly been neglected. We attempt to address this issue by fine-tuning an additional convolutional neural network solely devoted to sentiments, where dataset on sentiment is built upon a data-driven, multi-label approach. Our experimental results show that our method can generate image captions with sentiment terms that are more compatible with the images than solely relying on features devoted to object classification, while capable of preserving the semantics."
  },
  "bmvc2016_main_learningofseparablefiltersbystackedfisherconvolutionalautoencoders": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning of Separable Filters by Stacked Fisher Convolutional Autoencoders",
    "authors": [
      "Arash Shahriari"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper054/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper054/paper054.pdf",
    "published": "2016-09",
    "summary": "Learning of convolutional filters in deep neural networks proves high efficiency to provide sparse representations for the purpose of image recognition. The computational cost of these networks can be alleviated by focusing on separable filters to reduce the number of learning parameters. Autoencoders are a family of powerful deep networks to build scalable generative models for automatic feature learning. Inspired by their stacked hierarchy, we introduce Fisher convolutional autoencoders to learn separable filters in a distributed architecture. These novel overcomplete autoencoders employ discriminant analysis to impose the highest possible distinction among texture classes whilst holds the minimum separation within each individual class. A distributed network of stacked Fisher autoencoders learns banks of separable filters in parallel and makes an ensemble of deep convolutional features with higher separability for a better classification. This network automatically adjusts depth of each stack with respect to the capability of its correspondent separable filter on extracting higher order convolutional features for the dataset under study. We conduct our experiments on several publicly available datasets varying in number of classes and quality of samples by using a standard implementation. Our results confirm the supremacy of our method on improving the precision of texture understanding in comparison with the recently published benchmarks."
  },
  "bmvc2016_main_deskewingbyspace-variantdeblurring": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Deskewing by space-variant deblurring",
    "authors": [
      "Karthik Seemakurthy",
      "Subeesh Vasu",
      "Rajagopalan Ambasamudram"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper055/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper055/paper055.pdf",
    "published": "2016-09",
    "summary": "Skew and motion blur are significant challenges when camera and scene of interest are in two different media. Skew occurs due to spatially varying refraction on a dynamic water surface, whereas motion blur results from multiple intensities impinging on the imaging sensor during camera exposure time due to time varying refraction. In this paper, we introduce the notion of virtual depth map which we assign to a planar scene when observed through a dynamic water surface and transform the deskewing problem into one of space-variant deblurring from a single image within an alternating minimization framework. Since the nature of the virtual depth map can change during exposure due to change in wind properties, we also propose a shot detection framework to identify segments of frames from the captured video which conform to a single virtual depth map. While the overall wave motion can be arbitrary, within each segment the nature of wave is modeled as an exponentially decaying periodic wave."
  },
  "bmvc2016_main_facesinplacescompoundqueryretrieval": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Faces in Places: compound query retrieval",
    "authors": [
      "Yujie Zhong",
      "Relja Arandjelovic",
      "Andrew Zisserman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper056/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper056/paper056.pdf",
    "published": "2016-09",
    "summary": "The goal of this work is to retrieve images containing both a target person and a target scene type from a large dataset of images. At run time this compound query is handled using a face classifier trained for the person, and an image classifier trained for the scene type. We make three contributions: first, we propose a hybrid convolutional neural network architecture that produces place-descriptors that are aware of faces and their corresponding descriptors. The network is trained to correctly classify a combination of face and scene classifier scores. Second, we propose an image synthesis system to render high quality fully-labelled face-and-place images, and train the network only from these synthetic images. Last, but not least, we collect and annotate a dataset of real images containing celebrities in different places, and use this dataset to evaluate the retrieval system. We demonstrate significantly improved retrieval performance for compound queries using the new face-aware place-descriptors."
  },
  "bmvc2016_main_semi-supervisedvideoobjectsegmentationusingmultiplerandomwalkers": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Semi-supervised Video Object Segmentation Using Multiple Random Walkers",
    "authors": [
      "Won-Dong Jang",
      "Chang-Su Kim"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper057/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper057/paper057.pdf",
    "published": "2016-09",
    "summary": "A semi-supervised video object segmentation algorithm using multiple random walkers (MRW) is proposed in this work. We develop an initial probability estimation scheme that minimizes an objective function to roughly separate the foreground from the background. Then, we simulate MRW by employing the foreground and background agents. During the MRW process, we update restart distributions using a hybrid of inference restart rule and interactive restart rule. By performing these processes from the second to the last frames, we obtain a segment track of the target object. Furthermore, we optionally refine the segment track by performing Markov random field optimization. Experimental results demonstrate that the proposed algorithm significantly outperforms the state-of-the-art conventional algorithms on the SegTrack v2 dataset."
  },
  "bmvc2016_main_deeplearningfordetectingmultiplespace-timeactiontubesinvideos": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos",
    "authors": [
      "Suman Saha",
      "Gurkirt Singh",
      "Michael Sapienza",
      "Philip Torr",
      "Fabio Cuzzolin"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper058/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper058/paper058.pdf",
    "published": "2016-09",
    "summary": "In this work, we propose an approach to the spatiotemporal localisation (detection) and classification of multiple concurrent actions within temporally untrimmed videos. Our framework is composed of three stages. In stage 1, appearance and motion detection networks are employed to localise and score actions from colour images and optical flow. In stage 2, the appearance network detections are boosted by combining them with the motion detection scores, in proportion to their respective spatial overlap. In stage 3, sequences of detection boxes most likely to be associated with a single action instance, called action tubes, are constructed by solving two energy maximisation problems via dynamic programming. While in the first pass, action paths spanning the whole video are built by linking detection boxes over time using their class-specific scores and their spatial overlap, in the second pass, temporal trimming is performed by ensuring label consistency for all constituting detection boxes. We demonstrate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new state-of-the-art results across the board and significantly increasing detection speed at test time."
  },
  "bmvc2016_main_exploitingrandomrgbandsparsefeaturesforcameraposeestimation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Exploiting Random RGB and Sparse Features for Camera Pose Estimation",
    "authors": [
      "Lili Meng",
      "Jianhui Chen",
      "Frederick Tung",
      "James Little",
      "Clarence Silva"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper059/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper059/paper059.pdf",
    "published": "2016-09",
    "summary": "We address the problem of estimating camera pose relative to a known scene, given a single RGB image. We extend recent advances in scene coordinate regression forests for camera relocalization in RGB-D images to use RGB features, enabling camera relocalization from a single RGB image. Furthermore, we integrate random RGB features and sparse feature matching in an efficient and accurate way, broadening the method for fast sports camera calibration in highly dynamic scenes. We evaluate our method on both static, small scale and dynamic, large scale datasets with challenging camera poses. The proposed method is compared with several strong baselines. Experiment results demonstrate the efficacy of our approach, showing superior or on-par performance with the state of the art."
  },
  "bmvc2016_main_fine-grainedrecognitioninthenoisywildsensitivityanalysisofconvolutionalneuralnetworksapproaches": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of Convolutional Neural Networks Approaches",
    "authors": [
      "Erik  Rodner",
      "Marcel Simon",
      "Robert Fisher",
      "Joachim Denzler"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper060/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper060/paper060.pdf",
    "published": "2016-09",
    "summary": "In this paper, we study the sensitivity of CNN outputs with respect to image transformations and noise in the area of fine-grained recognition. In particular, we answer the following questions (1) how sensitive are CNNs with respect to image transformations encountered during wild image capture?; (2) how can we predict CNN sensitivity?; and (3) can we increase the robustness of CNNs with respect to image degradations? To answer the first question, we provide an extensive empirical sensitivity analysis of commonly used CNN architectures (AlexNet, VGG19, GoogleNet) across various of types of image degradations. This allows for predicting CNN performance for new domains comprised by images of lower quality or captured from a different viewpoint. We also show how the sensitivity of CNN outputs can be predicted for single images. Furthermore, we demonstrate that input layer dropout or pre-filtering during test time only reduces CNN sensitivity for high levels of degradation. Experiments for fine-grained recognition tasks reveal that VGG19 is more robust to severe image degradations than AlexNet and GoogleNet. However, small intensity noise can lead to dramatic changes in CNN performance even for VGG19."
  },
  "bmvc2016_main_near-fieldphotometricstereoinambientlight": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Near-Field Photometric Stereo in Ambient Light",
    "authors": [
      "Fotios Logothetis",
      "Roberto Mecca",
      "Yvain Queau",
      "Roberto Cipolla"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper061/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper061/paper061.pdf",
    "published": "2016-09",
    "summary": "Shape recovery from shading information has recently regained importance due to the improvement towards making the Photometric Stereo technique more reliable in terms of appearance of reflective objects. However, although more advanced models have been lately proposed, 3D scanners based on this technology do not provide reliable reconstructions as long as the considered irradiance equation neglects any additive bias. Depending on the context, such bias assumes different physical meanings. For example, in murky water it is known as saturated backscattered effect or for acquisition in pure air medium it is known as ambient light. Although the theoretical part covers both cases, this work mostly focuses on the pure air acquisition case. Indeed, we present a new approach based on ratios of differences of images where an exhaustive set of physical features are tackled while dealing with Photometric Stereo acquisition with considerable importance for the ambient light. To the best of our knowledge, this is the first attempt to recover the shape from Photometric Stereo considering simultaneously perspective viewing geometry, non-linear light propagation, both specular and diffuse reflectance plus the additive bias of the ambient light. Proof of concept is provided by showing experimental results on synthetic and real data."
  },
  "bmvc2016_main_addingsynchronizationandrollingshutterinmulti-camerabundleadjustment": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Adding Synchronization and Rolling Shutter in Multi-Camera BundleAdjustment",
    "authors": [
      "Thanh-Tin Nguyen",
      "Maxime Lhuillier"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper062/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper062/paper062.pdf",
    "published": "2016-09",
    "summary": "Multi-cameras built by fixing together several consumer cameras become popular and are convenient for recent applications like 360 videos. However, their self-calibration is not easy since they are composed of several unsynchronized and rolling shutter cameras. This paper introduces a new bundle adjustment for these multi-cameras that estimates not only the usual parameters (camera poses and 3D points) but also the synchronization and the rolling shutter of the cameras. We experiment using videos taken by GoPro cameras mounted on a helmet, moving along trajectories of several hundreds of meters or kilometers, and compare our results to ground truth."
  },
  "bmvc2016_main_emvsevent-basedmulti-viewstereo": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "EMVS: Event-based Multi-View Stereo",
    "authors": [
      "Henri Rebecq",
      "Guillermo Gallego",
      "Davide Scaramuzza"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper063/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper063/paper063.pdf",
    "published": "2016-09",
    "summary": "Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the output is composed of a sequence of asynchronous events rather than actual intensity images, traditional vision algorithms cannot be applied, so that a paradigm shift is needed. We introduce the problem of Event-based Multi-View Stereo (EMVS) for event cameras and propose a solution to it. Unlike traditional MVS methods, which address the problem of estimating dense 3D structure from a set of known viewpoints, EMVS estimates semi-dense 3D structure from an event camera with known trajectory. Our EMVS solution elegantly exploits two inherent properties of an event camera: (i) its ability to respond to scene edges\u201ewhich naturally provide semidense geometric information without any pre-processing operation\u201eand (ii) the fact that it provides continuous measurements as the sensor moves. Despite its simplicity (it can be implemented in a few lines of code), our algorithm is able to produce accurate, semidense depth maps. We successfully validate our method on both synthetic and real data. Our method is computationally very efficient and runs in real-time on a CPU."
  },
  "bmvc2016_main_occlusion-aware3dmorphablefacemodels": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Occlusion-aware 3D Morphable Face Models",
    "authors": [
      "Bernhard Egger",
      "Andreas Schneider",
      "Clemens Blumer",
      "Andreas Forster",
      "Sandro Sch\u00f6nborn",
      "Thomas Vetter"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper064/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper064/paper064.pdf",
    "published": "2016-09",
    "summary": "We propose a probabilistic occlusion-aware 3D Morphable Face Model adaptation framework for face image analysis based on the Analysis-by-Synthesis setup. In natural images, parts of the face are often occluded by a variety of objects. Such occlusions are a challenge for face model adaptation. We propose to segment the image into face and non-face regions and model them separately. The segmentation and the face model parameters are not known in advance and have to be adapted to the target image. A good segmentation is necessary to obtain a good face model fit and vice-versa. Therefore, face model adaptation and segmentation are solved together using an EM-like procedure. We use a stochastic sampling strategy based on the Metropolis-Hastings algorithm for face model parameter adaptation and a modified Chan-Vese segmentation for face region segmentation. Previous robust methods are limited to homogeneous, controlled illumination settings and tend to fail for important regions such as the eyes or mouth. We propose a RANSAC-based robust illumination estimation technique to handle complex illumination conditions. We do not use any manual annotation and the algorithm is not optimised to any specific kind of occlusion or database. We evaluate our method on a controlled and an ``in the wild'' database."
  },
  "bmvc2016_main_next-beststereoextendingnext-bestviewoptimisationforcollaborativesensors": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Next-Best Stereo: Extending Next-Best View Optimisation For Collaborative Sensors",
    "authors": [
      "Oscar Mendez Maldonado",
      "Simon Hadfield",
      "Nicolas Pugeault",
      "Richard Bowden"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper065/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper065/paper065.pdf",
    "published": "2016-09",
    "summary": "Most 3D reconstruction approaches passively optimise over all data, exhaustively matching pairs, rather than actively selecting data to process. This is costly both in terms of time and computer resources, and quickly becomes intractable for large datasets. This work proposes an approach to intelligently filter large amounts of data for 3D reconstructions of unknown scenes using monocular cameras. Our contributions are two-fold: First, we present a novel approach to efficiently optimise the Next-Best View (NBV) in terms of accuracy and coverage using partial scene geometry. Second, we extend this to intelligently selecting stereo pairs by jointly optimising the baseline and vergence to find the NBV\u00cds best stereo pair to perform reconstruction. Both contributions are extremely efficient, taking 0.8ms and 0.3ms per pose, respectively. Experimental evaluation shows that the proposed method allows efficient selection of stereo pairs for reconstruction, such that a dense model can be obtained with only a small number of images. Once a complete model has been obtained, the remaining computational budget is used to intelligently refine areas of uncertainty, achieving results comparable to state-of-the-art batch approaches on the Middlebury dataset, using as little as 3.8% of the views."
  },
  "bmvc2016_main_shape-basedimagecorrespondence": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Shape-based Image Correspondence",
    "authors": [
      "Berk Sevilmis",
      "Benjamin Kimia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper066/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper066/paper066.pdf",
    "published": "2016-09",
    "summary": "Current state-of-the-art dense correspondence algorithms establish correspondences between pair of images by searching for a flow field that minimizes the distance between local signatures (e.g., color histogram, SIFT descriptor) of aligned pixels while preserving smoothness. Agnostic to the global signatures (e.g., object membership, category of object), these local signatures face difficulties in resolving alignment ambiguities when scene content undergoes type and configuration variation. In this paper, we investigate the effect of adding shape correspondence constraints either in the form of pair of corresponding contour fragments or pair of closed curves. We find the shape does not play a significant role in optical flow and stereo correspondence but it does play a significant role when scene content changes are large. We also explore using object proposals as a way of providing shape constraints with encouraging results."
  },
  "bmvc2016_main_reprojectionflowforimageregistrationacrossseasons": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Reprojection Flow for Image Registration Across Seasons",
    "authors": [
      "Shane Griffith",
      "Cedric Pradalier"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper067/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper067/paper067.pdf",
    "published": "2016-09",
    "summary": "We address the problem of robust visual data association across seasons and viewpoints. The predominant methods in this area are typically appearance-based, which lose representational power in outdoor and natural environments that have significant variation in appearance. After a natural environment is surveyed multiple times, we recover its 3D structure in a map, which provides the basis for robust data association. Our approach is called Reprojection Flow, which consists of using reprojected map points for appearance-invariant viewpoint selection and robust image registration. We evaluated this approach using a dataset of 24 surveys of a natural environment that span over a year. Experiments showed robustness to variation in appearance and viewpoint across seasons, a significant improvement over a state-of-the-art appearance-based technique for pairwise dense correspondence."
  },
  "bmvc2016_main_unsupervisedlearningofshape-motionpatternsforobjectsinurbanstreetscenes": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Unsupervised Learning of Shape-Motion Patterns for Objects in Urban Street Scenes",
    "authors": [
      "Dirk Klostermann",
      "Aljosa Osep",
      "J\u00f6rg St\u00fcckler",
      "Bastian Leibe"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper068/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper068/paper068.pdf",
    "published": "2016-09",
    "summary": "Tracking in urban street scenes is predominantly based on pretrained object-specific detectors and Kalman-filter based tracking. More recently, methods have been proposed that track objects by modelling their shape as well as ones that predict the motion of objects using learned trajectory models. In this paper, we combine these ideas and propose shape-motion patterns (SMPs) that incorporate shape as well as motion to model a variety of objects in an unsupervised way. By using shape, our method can acquire trajectory models that distinguish object categories with distinct behaviour. We develop methods to classify objects into SMPs and to predict future motion. In experiments, we analyse our learned categorization and demonstrate superior performance of our motion predictions compared to a Kalman-filter and a learned pure trajectory model. We also demonstrate how SMPs can indicate potentially harmful situations in traffic scenarios."
  },
  "bmvc2016_main_efficientlearningfordiscriminativesegmentationwithsupermodularlosses": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Efficient Learning for Discriminative Segmentation with Supermodular Losses",
    "authors": [
      "Jiaqian Yu",
      "Matthew Blaschko"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper070/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper070/paper070.pdf",
    "published": "2016-09",
    "summary": "Several supermodular losses have been shown to improve the perceptual quality of image segmentation in a discriminative framework such as a structured output support vector machine (SVM). These loss functions do not necessarily have the same structure as the segmentation inference algorithm, and in general, we may have to resort to generic submodular minimization algorithms for loss augmented inference. Although these come with polynomial time guarantees, they are not practical to apply to image scale data. Many supermodular losses come with strong optimization guarantees but are not readily incorporated in a loss augmented graph cuts procedure. This motivates our strategy of employing the alternating direction method of multipliers (ADMM) decomposition for loss augmented inference. In doing so, we create a new API for the structured SVM that separates the maximum a posteriori (MAP) inference of the model from the loss augmentation during training. In this way, we gain computational efficiency, making new choices of loss functions practical for the first time, while simultaneously making the inference algorithm employed during training closer to the test time procedure. We show improvement both in accuracy and computational performance on the Microsoft Research Grabcut database and a brain structure segmentation task, empirically validating the use of a supermodular loss during training, and the improved computational properties of the proposed ADMM approach over the Fujishige-Wolfe minimum norm point algorithm."
  },
  "bmvc2016_main_variationalweaklysupervisedgaussianprocesses": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Variational Weakly Supervised Gaussian Processes",
    "authors": [
      "Melih Kandemir",
      "Manuel Haussmann",
      "Ferran Diego",
      "Kumar Rajamani",
      "Jeroen Van Der Laak",
      "Fred Hamprecht"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper071/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper071/paper071.pdf",
    "published": "2016-09",
    "summary": "We introduce the first model to perform weakly supervised learning with Gaussian processes on up to millions of instances. The key ingredient to achieve this scalability is to replace the standard assumption of MIL that the bag-level prediction is the maximum of instance-level estimates with the accumulated evidence of instances within a bag. This enables us to devise a novel variational inference scheme that operates solely by closed-form updates. Keeping all its parameters but one fixed, our model updates the remaining parameter to the global optimum. This virtue leads to charmingly fast convergence, fitting perfectly to large-scale learning setups. Our model performs significantly better in two medical applications than adaptation of GPMIL to scalable inference and various scalable MIL algorithms. It also proves to be very competitive in object classification against state-of-the-art adaptations of deep learning to weakly supervised learning."
  },
  "bmvc2016_main_regionalgatingneuralnetworksformulti-labelimageclassification": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Regional Gating Neural Networks for Multi-label Image Classification",
    "authors": [
      "Rui-Wei Zhao",
      "Jianguo Li",
      "Yurong Chen",
      "Jia-Ming Liu",
      "Yu-Gang Jiang",
      "Xiangyang Xue"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper072/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper072/paper072.pdf",
    "published": "2016-09",
    "summary": "This paper proposes a novel deep learning framework for multi-label image classification, namely regional gating neural networks (RGNN). The motivation is two folds. First, global image features (including CNN based features) ignore the underlying context information among different objects in an image. Consequently, people attempt to use information from objectness regions. However, current objectness region proposal algorithms usually produce several thousand region candidates, including many classification irrelevant or even noisy regions. This leads to the second problem: how to select useful contextual regions for image classification. RGNN is an end-to-end deep learning framework that can automatically select contextual region features with specially designed gate units, which are then fused for classification. Because the gate units and the classifier are integrated in the same deep neural network pipeline, we can learn parameters of the network simultaneously. We evaluate the proposed method on PASCAL VOC 2007{\\slash}2012 and MS-COCO benchmarks, and results show that RGNN is superior to existing state-of-the-art methods."
  },
  "bmvc2016_main_multispectraldeepneuralnetworksforpedestriandetection": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Multispectral Deep Neural Networks for Pedestrian Detection",
    "authors": [
      "Jingjing Liu",
      "Shaoting Zhang",
      "Shu Wang",
      "Dimitris Metaxas"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper073/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper073/paper073.pdf",
    "published": "2016-09",
    "summary": "Multispectral pedestrian detection is essential for around-the-clock applications, e.g., surveillance and autonomous driving. We deeply analyze Faster R-CNN for multispectral pedestrian detection task and then model it into a convolutional network (ConvNet) fusion problem. Further, we discover that ConvNet-based pedestrian detectors trained by color or thermal images separately provide complementary information in discriminating human instances. Thus there is a large potential to improve pedestrian detection by using color and thermal images in DNNs simultaneously. We carefully design four ConvNet fusion architectures that integrate two-branch ConvNets on different DNNs stages, all of which yield better performance compared with the baseline detector. Our experimental results on KAIST pedestrian benchmark show that the Halfway Fusion model that performs fusion on the middle-level convolutional features outperforms the baseline method by 19% and yields a missing rate 7.5% lower than the other proposed architectures."
  },
  "bmvc2016_main_l1graphbasedsparsemodelforlabelde-noising": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "L1 Graph Based Sparse Model for Label De-noising",
    "authors": [
      "Xiaobin Chang",
      "Tao Xiang",
      "Timothy Hospedales"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper074/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper074/paper074.pdf",
    "published": "2016-09",
    "summary": "The abundant images and user-provided tags available on social media websites provide an intriguing opportunity to scale vision problems beyond the limits imposed by manual dataset collection and annotation. However, exploiting user-tagged data in practice is challenging since it contains many noisy (incorrect and missing) labels. In this work, we propose a novel robust graph-based approach for label de-noising. Specifically, the proposed model is built upon (i) label smoothing via a visual similarity graph in a form of $L_{1}$ graph regulariser, which is more robust against visual outliers than the conventional $L_{2}$ regulariser, and (ii) explicitly modelling the label noise pattern, which helps to further improve de-noising performance. An efficient algorithm is formulated to optimise the proposed model, which contains multiple robust $L_1$ terms in its objective function and is thus non-trivial to optimise. We demonstrate our model's superior de-noising performance across the spectrum of problems from multi-class with label noise to real social media data with more complex multi-label structured label noise patterns."
  },
  "bmvc2016_main_patchitself-supervisednetworkweightinitializationforfine-grainedrecognition": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "PatchIt: Self-Supervised Network Weight Initialization for Fine-grained Recognition",
    "authors": [
      "Patrick Sudowe",
      "Bastian Leibe"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper075/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper075/paper075.pdf",
    "published": "2016-09",
    "summary": "ConvNet training is highly sensitive to initialization of the weights. A widespread approach is to initialize the network with weights trained for a different task, an auxiliary task. The ImageNet-based ILSVRC classification task is a very popular choice for this, as it has shown to produce powerful feature representations applicable to a wide variety of tasks. However, this creates a significant entry barrier to exploring non-standard architectures. In this paper, we propose a self-supervised pretraining, the PatchTask, to obtain weight initializations for fine-grained recognition problems, such as person attribute recognition, pose estimation, or action recognition. Our pretraining allows us to leverage additional unlabeled data from the same source, which is often readily available, such as detection bounding boxes. We experimentally show that our method outperforms a standard random initialization by a considerable margin and closely matches the ImageNet-based initialization."
  },
  "bmvc2016_main_combiningshapefromshadingandstereoavariationalapproachforthejointestimationofdepth,illuminationandalbedo": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Combining Shape from Shading and Stereo: A Variational Approach for the Joint Estimation of Depth, Illumination and Albedo",
    "authors": [
      "Daniel Maurer",
      "Yong Chul Ju",
      "Michael Breu\u00df",
      "Andres Bruhn"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper076/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper076/paper076.pdf",
    "published": "2016-09",
    "summary": "Shape from shading (SfS) and stereo are two fundamentally different strategies for image-based 3-D reconstruction. While approaches for SfS infer the depth solely from pixel intensities, methods for stereo are based on a matching process that establishes correspondences across images. In this paper we propose a joint variational method that combines the advantages of both strategies. By integrating recent stereo and SfS models into a single minimisation framework, we obtain an approach that exploits shading information to improve upon the reconstruction quality of robust stereo methods. To this end, we fuse a Lambertian SfS approach with a robust stereo model and supplement the resulting energy functional with a detail-preserving anisotropic second-order smoothness term. Moreover, we extend the novel model in such a way that it jointly estimates depth, albedo and illumination. This in turn makes it applicable to objects with non-uniform albedo as well as to scenes with unknown illumination. Experiments for synthetic and real-world images show the advantages of our combined approach: While the stereo part overcomes the albedo-depth ambiguity inherent to all SfS methods, the SfS part improves the degree of details of the reconstruction compared to pure stereo methods."
  },
  "bmvc2016_main_solvingvisualmadlibswithmultiplecues": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Solving VIsual Madlibs with Multiple Cues",
    "authors": [
      "Tatiana Tommasi",
      "Arun Mallya",
      "Bryan Plummer",
      "Svetlana Lazebnik",
      "Alexander Berg",
      "Tamara Berg"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper077/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper077/paper077.pdf",
    "published": "2016-09",
    "summary": "This paper focuses on answering fill-in-the-blank style multiple choice questions from the Visual Madlibs dataset. Previous approaches to Visual Question Answering (VQA) have mainly used generic image features from networks trained on the ImageNet dataset, despite the wide scope of questions. In contrast, our approach employs features derived from networks trained for specialized tasks of scene classification, person activ- ity prediction, and person and object attribute prediction. We also present a method for selecting sub-regions of an image that are relevant for evaluating the appropriateness of a putative answer. Visual features are computed both from the whole image and from local regions, while sentences are mapped to a common space using a simple normalized canonical correlation analysis (CCA) model. Our results show a significant improvement over the previous state of the art, and indicate that answering different question types ben- efits from examining a variety of image cues and carefully choosing informative image sub-regions."
  },
  "bmvc2016_main_lstmforimageannotationwithrelativevisualimportance": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "LSTM for Image Annotation with Relative Visual Importance",
    "authors": [
      "Geng Yan",
      "Yang Wang",
      "Zicheng Liao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper078/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper078/paper078.pdf",
    "published": "2016-09",
    "summary": "We consider the problem of image annotations that takes into account of the relative importance of tags. Previous work usually consider the tags associated with an image as an unordered list of object names. In contrast, we exploit the implicit cues about the relative importance of objects mentioned by the tags. For example, important objects tend to be mentioned first in a list of tags. Our proposed a recurrent neural network with long-short term memory for this problem. Given an image, our model can produce a ranked list of tags, where tags for important objects appear earlier in the list. Experimental results demonstrate that our model achieves better performance on several benchmark datasets."
  },
  "bmvc2016_main_onionnetsharingfeaturesincascadeddeepclassi_ers": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "OnionNet: Sharing Features in Cascaded Deep Classi_ers",
    "authors": [
      "Martin Simonovsky",
      "Nikos Komodakis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper079/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper079/paper079.pdf",
    "published": "2016-09",
    "summary": "The focus of our work is speeding up evaluation of deep neural networks in retrieval scenarios, where conventional architectures may spend too much time on negative examples. We propose to replace a monolithic network with our novel cascade of feature-sharing deep classifiers, called OnionNet, where subsequent stages may add both new layers as well as new feature channels to the previous ones. Importantly, intermediate feature maps are shared among classifiers, preventing them from the necessity of being recomputed. To accomplish this, the model is trained end-to-end in a principled way under a joint loss. We validate our approach in theory and on a synthetic benchmark. As a result demonstrated in three applications (patch matching, object detection, and image retrieval), our cascade can operate significantly faster than both monolithic networks and traditional cascades without sharing at the cost of marginal decrease in precision."
  },
  "bmvc2016_main_linereconstructionusingpriorknowledgeinsinglenon-centralview": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Line reconstruction using prior knowledge in single non-central view",
    "authors": [
      "Jesus Bermudez-Cameo",
      "C\u00e9dric Demonceaux",
      "Gonzalo Lopez-Nicolas",
      "Jose Guerrero"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper080/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper080/paper080.pdf",
    "published": "2016-09",
    "summary": "Line projections in non-central systems contain more geometric information than in central systems. The four degrees of freedom of the 3D line are mapped to the line-image and the 3D line can be theoretically recovered from 4 projecting rays (i.e. line-image points) from a single non-central view. In practice, extraction of line-images is considerably more difficult and the resulting reconstruction is imprecise and sensitive to noise. In this paper we present a minimal solution to recover the geometry of the 3D line from only three line-image points when the line is parallel to a given plane. A second minimal solution allows to recover the 3D line from two points when the direction of the 3D line is known. Both cases can exploit the prior knowledge of the vertical direction in man-made environments to reduce the complexity in line-image extraction and line reconstruction. The formulation based on Plucker lines is integrated in a line-extraction pipeline, which is tested with synthetic and real non-central circular panoramas. In addition, we evaluate the performance of the robust extractor and the accuracy of the proposal in comparison with the unconstrained method."
  },
  "bmvc2016_main_attributerecognitionfromadaptiveparts": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Attribute Recognition from Adaptive Parts",
    "authors": [
      "Luwei Yang",
      "Ligeng Zhu",
      "Yichen Wei",
      "Shuang Liang",
      "Ping Tan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper081/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper081/paper081.pdf",
    "published": "2016-09",
    "summary": "Previous part-based attribute recognition approaches perform part detection and attribute recognition in separate steps. The parts are not optimized for attribute recognition and therefore could be sub-optimal. We present an end-to-end deep learning approach to overcome the limitation. It generates object parts from key points and perform attribute recognition accordingly, allowing adaptive spatial transform of the parts. Both key point estimation and attribute recognition are learnt jointly in a multi-task setting. Extensive experiments on two datasets verify the efficacy of the proposed end-to-end approach."
  },
  "bmvc2016_main_memory-basedgaitrecognition": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Memory-based Gait Recognition",
    "authors": [
      "Dan Liu",
      "Mao Ye",
      "Xudong Li",
      "Feng Zhang",
      "Lan Lin"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper082/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper082/paper082.pdf",
    "published": "2016-09",
    "summary": "Gait recognition is an interesting and challenging task aiming to classify the subjects based on the way they walk, which is subject to various covariates including carrying, clothing, surface and view angle. In this paper, we propose to utilize the memory mechanism to effectively alleviate the aforementioned problems. Specifically, we extract the 2D location information of human joints as the gait features via the migratory articulated human detection. Inspired by the mechanism of brain sequence processing, we input the gait feature sequence into the memory-based gait recognition (MGR) network, which achieves the process of memory and identification of the gait sequence. Our proposed MGR is robust to the noise that maybe exist in the gait sequence features. Besides, MGR is able to learn on the data with long range temporal dependencies. The experimental results on the CASIA A and CASIA B gait datasets verify the feasibility and effectiveness of the proposed method."
  },
  "bmvc2016_main_three-pointdirectstereovisualodometry": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Three-Point Direct Stereo Visual Odometry",
    "authors": [
      "Jeong-Kyun Lee",
      "Kuk-Jin Yoon"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper083/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper083/paper083.pdf",
    "published": "2016-09",
    "summary": "Stereo visual odometry estimates the ego-motion of a stereo camera given an image sequence. Previous methods generally estimate the ego-motion using a set of inlier features while filtering out outlier features. However, since the perfect classification of inlier and outlier features is practically impossible, the motion estimate is often contaminated by erroneous inliers. In this paper, we propose a novel three-point direct method for stereo visual odometry, which is more accurate and robust to outliers. To improve both accuracy and robustness, we consider two key points: sampling a minimum number of features, i.e., 3 points, and minimizing photometric errors in order to maximally reduce measurement errors. In addition, we utilize temporal information of features, i.e., feature tracks. Local features are updated by the feature tracks and the updated feature points improve the performance of the proposed pose estimation. We compare the proposed method with other state-of-the-art methods and demonstrate the superiority of the proposed method through experiments on the KITTI benchmark."
  },
  "bmvc2016_main_semanticsegmentationforreal-worlddatabyjointlyexploitingsupervisedandtransferrableknowledge": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Semantic Segmentation for Real-World Data by Jointly Exploiting Supervised and Transferrable Knowledge ",
    "authors": [
      "Li-Hsien Lu",
      "Chiou-Ting Hsu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper084/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper084/paper084.pdf",
    "published": "2016-09",
    "summary": "This paper addresses two major challenges in semantic segmentation for real-world data. First, with ever-increasing semantic labels, we need a more pragmatic approach other than existing fully-supervised methods. Second, semantic segmentation for very small or rarely-appeared objects are still very challenging for existing methods. In this paper, we propose to (1) fully utilize the predicted label information from an existing supervised model and to (2) infer newly generated labels via label transfer from a real-world dataset. We propose a \"content-adaptive\" and \"label-aware\" MRF framework to jointly exploiting both the supervised and label-transferrable knowledge. The proposed method needs no off-line training and can easily adapt to real-world data. Experimental results on SIFT Flow and LMSun datasets demonstrate the effectiveness of the proposed method, and show promising performance over state-of-the-art methods under the real-world scenario."
  },
  "bmvc2016_main_optimizedregressorforestforimagesuper-resolution": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Optimized Regressor Forest for Image Super-Resolution",
    "authors": [
      "Chia-Yang Chang",
      "Wei-Chih Tu",
      "Shao-Yi Chien"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper085/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper085/paper085.pdf",
    "published": "2016-09",
    "summary": "The goal of image super-resolution is to recover missing high frequency details of an image given single or multiple low-resolution images._It is a well-known ill-posed problem and requires mature prior knowledges or enough examples to restore high-quality high-resolution images._Recently, many methods formulate image super-resolution as a regression problem. Input image patches are classified into pre-trained clusters, and cluster-dependent mapping functions are employed to super-resolve input patches._In this paper, for further improving the reconstructed image quality,_an optimized regressor forest framework is proposed, which leverages the discriminative power of random forest._There are three major contributions of the proposed framework._(i) The proposed scheme overturns existing approaches by training the regressors first and learning the way to find the best regressor to avoid quality degradation introduced from the classification outliers._(ii) We propose to employ EM-algorithm to optimize regressors by jointly optimizing the clustering results as well as the regression functions._(iii) In order to find the most appropriate regressor for an input patch at the testing stage,_random forest is adopted to accurately classify patches into their best clusters (regressors)._The experimental results demonstrate that the proposed method generates high-quality high-resolution images and yields state-of-the-art results."
  },
  "bmvc2016_main_convolutionalaggregationoflocalevidenceforlargeposefacealignment": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Convolutional aggregation of local evidence for large pose face alignment",
    "authors": [
      "Adrian Bulat",
      "Yorgos Tzimiropoulos"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper086/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper086/paper086.pdf",
    "published": "2016-09",
    "summary": "Methods for unconstrained face alignment must satisfy two requirements: they must not rely on accurate initialisation/face detection and they should perform equally well for the whole spectrum of facial poses. To the best of our knowledge, there are no methods meeting these requirements to satisfactory extent, and in this paper, we propose Convolutional Aggregation of Local Evidence (CALE), a Convolutional Neural Network (CNN) architecture particularly designed for addressing both of them. In particular, to remove the requirement for accurate face detection, our system firstly performs facial part detection, providing confidence scores for the location of each of the facial landmarks (local evidence). Next, these score maps along with early CNN features are aggregated by our system through joint regression in order to refine the landmarks' location. Besides playing the role of a graphical model, CNN regression is a key feature of our system, guiding the network to rely on context for predicting the location of occluded landmarks, typically encountered in very large poses. The whole system is trained end-to-end with intermediate supervision. When applied to AFLW-PIFA, the most challenging human face alignment test set to date, our method provides more than 50% gain in localisation accuracy when compared to other recently published methods for large pose face alignment. Going beyond human faces, we also demonstrate that CALE is effective in dealing with very large changes in shape and appearance, typically encountered in animal faces."
  },
  "bmvc2016_main_wideresidualnetworks": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Wide Residual Networks",
    "authors": [
      "Sergey Zagoruyko",
      "Nikos Komodakis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper087/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper087/paper087.pdf",
    "published": "2016-09",
    "summary": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. Our code is available at https://github.com/szagoruyko/wide-residual-networks"
  },
  "bmvc2016_main_deeppart-basedgenerativeshapemodelwithlatentvariables": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Deep Part-Based Generative Shape Model with Latent Variables",
    "authors": [
      "Alexander Kirillov",
      "Mikhail Gavrikov",
      "Ekaterina Lobacheva",
      "Anton Osokin",
      "Dmitry Vetrov"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper088/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper088/paper088.pdf",
    "published": "2016-09",
    "summary": "The Shape Boltzmann Machine (SBM) and its multilabel version MSBM have been recently introduced as deep generative models that capture the variations of an object shape. While being more flexible MSBM requires datasets with labeled parts of the objects for training. In the paper we present an algorithm for training MSBM using binary masks of objects and the seeds which approximately correspond to the locations of objects parts. The latter can be obtained from part-based detectors in an unsupervised manner. We derive a latent variable model and an EM-like training procedure for adjusting the weights of MSBM using a deep learning framework. We show that the model trained by our method outperforms SBM in the tasks related to binary shapes and is very close to the original MSBM in terms of quality of multilabel shapes."
  },
  "bmvc2016_main_generalhumantraitsorientedgenericelasticmodelfor3dfacereconstruction": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "General Human Traits Oriented Generic Elastic Model for 3D Face Reconstruction",
    "authors": [
      "Joi San Tan",
      "Ibrahim Venkat",
      "Iman Yi Liao",
      "Philippe De Wilde"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper089/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper089/paper089.pdf",
    "published": "2016-09",
    "summary": "We propose a Simpli_ed Generic Elastic Model (S-GEM) which intends to construct a 3D face from a given 2D face image by making use of a set of general human traits viz., Gender, Ethnicity and Age (GEA). Different from the original GEM model which employs and deforms the mean depth value of 3D sample faces according to a speci_c 2D input face image, we hypothesise that the variations inherent on the depth information for individuals are signi_cantly mitigated by narrowing down the target information via a selection of speci_c GEA traits. This is achieved by representing the unknown 3D facial feature points of a 2D input as a Gaussian Mixture Model (GMM) of that of the samples of its own GEA type. It is then further incorporated into a Bayesian framework whereby the 3D face reconstruction is posed as estimating the PCA coef_cients of a statistical 3D face model, given the observation of 2D feature points, however, with their respective depth as hidden variables. By making the reasonable assumption that the support area of each component of GMM is small enough, the proposed method is reduced to choose the depth values of the features points of a sample face that is nearest to the 2D input face. Thus the 3D reconstruction is obtained with depth-value augmented feature points rather than the 2D ones in normal PCA statistical model based reconstruction. The proposed method has been tested with the USF 3D face database as well as the FRGC dataset. The experimental results show that the proposed S-GEM has achieved improved reconstruction accuracy, consistency, and the robustness over the conventional PCA based and the GEM (mean-face feature points) reconstruction, and also yields enhanced visual improvements on certain facial features."
  },
  "bmvc2016_main_attendrefinerepeatactiveboxproposalgenerationviain-outlocalization": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization",
    "authors": [
      "Spyridon Gidaris",
      "Nikos Komodakis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper090/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper090/paper090.pdf",
    "published": "2016-09",
    "summary": "The problem of computing category agnostic bounding box proposals is utilized as a core component in many computer vision tasks and thus has lately attracted a lot of attention. In this work we propose a new approach to tackle this problem that is based on an active strategy for generating box proposals that starts from a set of seed boxes, which are uniformly distributed on the image, and then progressively moves its attention on the promising image areas where it is more likely to discover well localized bounding box proposals. We call our approach AttractioNet and a core component of it is a CNN-based category agnostic object location refinement module that is capable of yielding accurate and robust bounding box predictions regardless of the object category. We extensively evaluate our AttractioNet approach on the COCO 2014 validation set as well as on the PASCAL VOC2007 test set, reporting for both of them state-of-the-art results that surpass the previous work in the field by a significant margin. Finally, we provide strong empirical evidence that our approach is capable to generalize to unseen categories. Project page:: https://github.com/gidariss/AttractioNet."
  },
  "bmvc2016_main_craftingamulti-taskcnnforviewpointestimation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Crafting a multi-task CNN for viewpoint estimation",
    "authors": [
      "Francisco Massa",
      "Renaud Marlet",
      "Mathieu Aubry"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper091/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper091/paper091.pdf",
    "published": "2016-09",
    "summary": "Convolutional Neural Networks (CNNs) were recently shown to provide state-of-the-art results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP."
  },
  "bmvc2016_main_improvingweakly-supervisedobjectlocalizationbymicro-annotation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Improving Weakly-Supervised Object Localization By Micro-Annotation",
    "authors": [
      "Alexander Kolesnikov",
      "Christoph Lampert"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper092/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper092/paper092.pdf",
    "published": "2016-09",
    "summary": "Weakly-supervised object localization methods tend to fail for object classes that consistently co-occur with the same background elements, e.g. trains on tracks. We propose a method to overcome these failures by adding a very small amount of model-specific additional annotation. The main idea is to cluster a deep network\u00cds mid-level representations and assign object or distractor labels to each cluster. Experiments show substantially improved localization results on the challenging ILSVRC 2014 dataset for bounding box detection and the PASCAL VOC 2012 dataset for semantic segmentation."
  },
  "bmvc2016_main_mbeststructm-bestdiversesamplingforstructuredtracker": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "MBest Struct: M-Best diverse sampling for structured tracker",
    "authors": [
      "Ivan Bogun",
      "Eraldo Ribeiro"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper093/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper093/paper093.pdf",
    "published": "2016-09",
    "summary": "We approach the problem of model-free visual tracking of objects in videos. Model-free tracking has its state-of-the-art in a class of methods called tracking-by-detection, as shown in recent benchmarks. Some top-performing methods use deep neural networks (i.e., convnets) to solve the learning-based steps of the tracking algorithm (e.g., bounding-box prediction and evaluation). Despite improving accuracy, convnets impose a high computational cost on trackers, limiting their real-time applications. In this paper, we propose to use deep features from a pre-learned deep-convolutional network in a computationally efficient way. Here, we use M-Best diverse-sampling to sample a small yet diverse set of bounding boxes that are likely to contain the tracked object. Given these bounding boxes, our method performs detection using deep features. The resulting tracker, named MBestStruck, uses a high-quality feature representation while being computationally efficient. Our tracking approach compares very well to the state-of-the-art, as shown by experiments done on popular benchmark datasets."
  },
  "bmvc2016_main_event-basedhoughtransforminaspikingneuralnetworkformultiplelinedetectionandtrackingusingadynamicvisionsensor": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Event-Based Hough Transform in a Spiking Neural Network for Multiple Line Detection and Tracking Using a Dynamic Vision Sensor",
    "authors": [
      "Sajjad Seifozzakerini",
      "Wei-Yun Yau",
      "Bo Zhao",
      "Kezhi Mao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper094/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper094/paper094.pdf",
    "published": "2016-09",
    "summary": "Hough Transform has been widely used to detect lines in images captured by conventional cameras. In this paper, we develop an event-based Hough transform and apply it to a new type of camera, namely Dynamic Vision Sensor (DVS). DVS outputs an asynchronous stream of binary events representing illumination change in the scene. We implement the proposed algorithm in a spiking neural network to detect lines on DVS output. Spikes (events) from the DVS sensor are first mapped to Hough transform parameter space and then sent to corresponding spiking neurons for accumulation. A spiking neuron will fire an output spike once it accumulates enough input contributions and then reset itself. The output spikes of the spiking neural network represent the parameters of detected lines. An event-based clustering algorithm is applied on the parameter space spikes to segment multiple lines and track them. In our spiking neural network, a lateral inhibition strategy is applied to suppress noise lines from being detected. This is achieved by resetting a neuron's neighbors in addition to itself once the neuron fires an output spike. The efficacy of the proposed algorithm is shown by extensive experiments on both artificially generated events and various real DVS outputs."
  },
  "bmvc2016_main_holisticallyconstrainedlocalmodelgoingbeyondfrontalposesforfaciallandmarkdetection": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Holistically Constrained Local Model: Going Beyond Frontal Poses for Facial Landmark Detection",
    "authors": [
      "Kanggeon Kim",
      "Tadas Baltru\u0161aitis",
      "Amirali Zadeh",
      "Louis-Philippe Morency",
      "G\u00e9rard Medioni"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper095/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper095/paper095.pdf",
    "published": "2016-09",
    "summary": "Facial landmark detection has received much attention in recent years, with two detection paradigms emerging: local approaches, where each facial landmark is modeled individually and with the help of a shape model; and holistic approaches, where the whole face appearance is jointly modeled for improved robustness. These approaches have shown great performance for facial landmark detection even under \"in-the-wild\" conditions of varying illumination, occlusion and image quality. However, their accuracy and robustness are very often reduced for profile faces where face alignment is more challenging (e.g., no more facial symmetry, less defined features and more variable background). In this paper, we present a new model, named Holistically Constrained Local Model (HCLM), which unifies local and holistic facial landmark detection by integrating head pose estimation, sparse-holistic landmark detection and dense-local landmark detection. We evaluate our new model on two publicly available datasets, 300-W and AFLW, as well as a newly introduced dataset, IJB-FL which includes a larger proportion of profile face poses. Our HCLM model shows state-of-the-art performance, especially with extreme head poses."
  },
  "bmvc2016_main_bagofsurrogatepartsoneinherentfeatureofdeepcnns": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Bag of Surrogate Parts: one inherent feature of deep CNNs",
    "authors": [
      "Yanming Guo",
      "Michael S. Lew"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper096/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper096/paper096.pdf",
    "published": "2016-09",
    "summary": "Convolutional Neural Networks (CNNs) have achieved promising performance in image classification tasks. In this paper, we develop a new feature from convolutional layers, called Bag of Surrogate Parts (BoSP), and its spatial variant, Spatial BoSP (S-BoSP). Specifically, we take the feature maps in convolutional layers as surrogate parts, and densely sample and assign the regions in input images to these surrogate parts by observing the activation values. To better handle the objects with different scales and deformations, and make more comprehensive predictions, we further propose a scale pooling technique for assigning the features, and global constrained augmentation for the final prediction. Compared with most existing methods that also utilize the activations from convolutional layers, the proposed method is efficient, has no tuning parameters, and could generate low-dimensional, highly discriminative features. The experiments on generic object, fine-grained object and scene datasets indicate that the proposed feature can not only produce superior results to fully-connected layer based features, but also get comparable, or in some cases considerably better performance than the state-of-the-art."
  },
  "bmvc2016_main_multi-scalecolorectaltumoursegmentationusinganovelcoarsetofinestrategy": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Multi-scale Colorectal Tumour Segmentation Using a Novel Coarse to Fine Strategy",
    "authors": [
      "Kun Zhang",
      "Danny Crookes",
      "Jim Diamond",
      "Minrui Fei",
      "Jianguo Wu",
      "Peijian Zhang",
      "Huiyu Zhou"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper097/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper097/paper097.pdf",
    "published": "2016-09",
    "summary": "This paper addresses the problem of colorectal tumour segmentation in complex real world imagery. For efficient segmentation, a multi-scale strategy is developed for extracting the potentially cancerous region of interest (ROI) based on colour histograms while searching for the best texture resolution. To achieve better segmentation accuracy, we apply a novel bag-of-visual-words method based on rotation invariant raw statistical features and random projection based l2-norm sparse representation to classify tumour areas in histopathology images. Experimental results on 20 real world digital slides demonstrate that the proposed algorithm results in better recognition accuracy than several state of the art segmentation techniques."
  },
  "bmvc2016_main_learningadditivekernelforfeaturetransformationanditsapplicationtocnnfeatures": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning Additive Kernel For Feature Transformation and Its Application to CNN Features",
    "authors": [
      "Takumi Kobayashi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper098/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper098/paper098.pdf",
    "published": "2016-09",
    "summary": "Feature transformation is an important process following feature extraction for improving classification performance. It has been frequently addressed in the kernel-based framework utilizing non-linear kernel functions, and the additive kernel equipped with explicit feature mapping works as efficient and effective (non-linear) feature transformation. The kernel functions, however, are defined in a top-down manner taking into account the inherent nature of the features, which makes it difficult to appropriately apply them to the features whose characteristics are not fully disclosed, such as CNN features. In this paper, we propose a method to learn an additive kernel of which explicit mapping serves feature transformation. By means of a bottom-up learning approach leveraging annotated data, the proposed method builds the kernel function of high generality and discriminative power even for the CNN features. The experiments on various datasets using various types of pre-trained CNN features show favorable performance improvement by the learned additive kernel (feature transformation) of which generality over the datasets and the CNN models is also demonstrated."
  },
  "bmvc2016_main_robust3dcarshapeestimationfromlandmarksinmonocularimage": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Robust 3D Car Shape Estimation from Landmarks in Monocular Image",
    "authors": [
      "Yanan Miao",
      "Xiaoming Tao",
      "Jianhua Lu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper099/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper099/paper099.pdf",
    "published": "2016-09",
    "summary": "The reconstruction of 3D object shape from monocular image is inherently an ill-posed problem. And it suffers significant performance degradation when large errors are present. In this paper, we propose a robust model to estimate 3D shape from 2D landmarks with unknown camera pose. The 3D shape of the object is assumed as a linear combination of predefined shape basis. To handle severely contaminated observations, we explicitly model the outliers as sparse noise. The objective function hence is non-convex and non-smooth constrained on Stiefel manifold, where the coupling of the unknown shape representation coefficients and camera pose makes it more difficult to solve. We then propose a numerical algorithm based on Alternative Direction Method of Multipliers to optimize it. We set the orthogonality constraints into the smooth sub-problem, which admits a closed-form solution. The proposed algorithm can achieve convergence rapidly. Experimental results both in controlled experiments and on real data show that, the proposed method outperforms the other methods."
  },
  "bmvc2016_main_projectiveunsupervisedflexibleembeddingwithoptimalgraph": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Projective Unsupervised Flexible Embedding with Optimal Graph",
    "authors": [
      "Wei Wang",
      "Yan Yan",
      "Feiping Nie",
      "Xavier Pineda",
      "Shuicheng Yan",
      "Nicu Sebe"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper100/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper100/paper100.pdf",
    "published": "2016-09",
    "summary": "Graph based dimensionality reduction techniques have been successfully applied to clustering and classification tasks. The fundamental basis of these algorithms is the constructed graph which dominates their performance. Usually, the graph is defined by the input affinity matrix. However, the affinity matrix is sub-optimal for dimension reduction as there is much noise in the data. To address this issue, we propose the projective unsupervised flexible embedding with optimal graph (PUFE-OG) model. We build an optimal graph by adjusting the affinity matrix. To tackle the out-of-sample problem, we employ a linear regression term to learn a projection matrix. The optimal graph and projection matrix are jointly learned by integrating the manifold regularizer and regression residual into a unified model. An efficient algorithm is derived to solve the challenging model. The experimental results on several public benchmark datasets demonstrate that the presented PUFE-OG outperforms other state-of-the-art methods."
  },
  "bmvc2016_main_towardsautomaticimageeditinglearningtoseeanotheryou": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Towards Automatic Image Editing: Learning to See another You",
    "authors": [
      "Xu Jia",
      "Amir Ghodrati",
      "Marco Pedersoli",
      "Tinne Tuytelaars"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper101/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper101/paper101.pdf",
    "published": "2016-09",
    "summary": "In this paper we propose a method that aims at automatically editing an image by altering its attributes. More specifically, given an image of a certain class (e.g. a human face), the method should generate a new image as similar as possible to the given one, but with an altered visual attribute (e.g. the same face with a new pose or a different illumination). To this end, we propose a solution following an encoder-decoder pipeline. The desired attribute and the input image are independently encoded into a convolutional network and fused at feature map level. A convolutional decoder is then used to generate the target image. The result is further refined with another convolutional encoder-decoder network with the initial result and the original image as inputs. We evaluate the proposed method on MultiPIE dataset for three sub-tasks, that is, rotating faces, changing illumination and image inpainting. We show that the method is able to generate realistic images for the three tasks in most of the evaluated samples."
  },
  "bmvc2016_main_denselabelingwithuserinteractionanexamplefordepth-of-fieldsimulation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Dense Labeling with User Interaction: an Example for Depth-Of-Field Simulation",
    "authors": [
      "Ana Cambra",
      "Adolfo Mu\u00f1oz",
      "Jose Guerrero",
      "Ana Murillo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper102/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper102/paper102.pdf",
    "published": "2016-09",
    "summary": "This work presents a method to achieve dense labeling in a very simple and efficient way, faster and with more flexibility than related approaches. We use an initial superpixel graph and reformulate its constraints as a sparse linear system of equations, which is efficiently solved as a linear least squares problem. We demonstrate our approach with an interactive application for depth-of-field simulated effects based on dense depth estimation from a single image. This kind of interactive applications requires to continuously obtain a dense labeling estimation, therefore it is necessary to solve the task with high efficiency. Our experiments show our method obtains comparable results for a dense depth estimation against related approaches, while providing a more generic and powerful representation of the problem. The proposed dense labeling system opens new opportunities to design interactive applications that require dense labeling estimation of any other image property."
  },
  "bmvc2016_main_mlboostrevisitedafastermetriclearningalgorithmforidentity-basedfaceretrieval": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "MLBoost Revisited: A Faster Metric Learning Algorithm for Identity-Based Face Retrieval",
    "authors": [
      "Romain Negrel",
      "Alexis Lechervy",
      "Frederic Jurie"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper103/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper103/paper103.pdf",
    "published": "2016-09",
    "summary": "This paper addresses the question of metric learning, i.e. the learning of a dissimilarity function from a set of similar/dissimilar example pairs. This domain plays an important role in many machine learning applications such as those related to face recognition or face retrieval. More specifically, this paper builds on the recent MLBoost method proposed by Negrel. MLBoost has been shown to perform very well for face retrieval tasks, but this algorithm relies on the computation of a weak metric which is very time consuming. This paper demonstrates how, by introducing sparsity into the weak projectors, the convergence time can be reduced up to a factor of 10 times compared to MLBoost, without any performance loss. The paper also introduces an explicit way to control the rank of the so-obtained metrics, allowing to fix in advance the dimension of the (projected) feature space. The proposed ideas are experimentally validated on a face retrieval task with three different signatures."
  },
  "bmvc2016_main_learningneuralnetworkarchitecturesusingbackpropagation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning Neural Network Architectures using Backpropagation",
    "authors": [
      "Suraj Srinivas",
      "Venkatesh Babu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper104/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper104/paper104.pdf",
    "published": "2016-09",
    "summary": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We start with a large neural network, and then learn which neurons to prune. To this end, we introduce a new trainable parameter called the Tri-State ReLU, which helps in pruning unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with considerably smaller number of parameters without affecting prediction accuracy."
  },
  "bmvc2016_main_enhancingposeestimationthroughefficientpatchsynthesis": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Enhancing pose estimation through efficient patch synthesis",
    "authors": [
      "Pierre Rolin",
      "Marie-Odile Berger",
      "Frederic Sur"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper105/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper105/paper105.pdf",
    "published": "2016-09",
    "summary": "Estimating the pose of a camera from a scene model is a challenging problem when the camera is in a position not covered by the views used to build the model, because feature matching is difficult. Several viewpoint simulation techniques have been recently proposed in this context. They generally come with a high computational cost, are limited to specific scenes such as urban environments or object-centred scenes, or need an initial pose guess. This paper presents a viewpoint simulation method well suited to most scenes and query views. Two major problems are addressed: the positioning of the virtual viewpoints with respect to the scene, and the synthesis of geometrically consistent patches. Experiments show that patch synthesis dramatically improves the accuracy of the pose in case of difficult registration, with a limited computational cost."
  },
  "bmvc2016_main_discoveringmotionhierarchiesviatree-structuredcodingoftrajectories": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Discovering motion hierarchies via tree-structured coding of trajectories",
    "authors": [
      "Juan-Manuel Perez-Rua",
      "Tomas Crivelli",
      "Patrick Perez",
      "Patrick Bouthemy"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper106/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper106/paper106.pdf",
    "published": "2016-09",
    "summary": "The dynamic content of physical scenes is largely compositional, that is, the movements of the objects and of their parts are hierarchically organized and relate through composition along this hierarchy. This structure also prevails in the apparent 2D motion that a video captures. Accessing this visual motion hierarchy is important to get a better understanding of dynamic scenes and is useful for video manipulation. We propose to capture it through learned, tree-structured sparse coding of point trajectories. We leverage this new representation within an unsupervised clustering scheme to partition hierarchically the trajectories into meaningful groups. We show through experiments on motion capture data that our model is able to extract moving segments along with their organization. We also present competitive results on the task of segmenting objects in video sequences from trajectories."
  },
  "bmvc2016_main_coplanarrepeatsbyenergyminimization": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Coplanar Repeats by Energy Minimization",
    "authors": [
      "James Pritts",
      "Denys Rozumnyi",
      "M. Pawan Kumar",
      "Ondrej Chum"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper107/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper107/paper107.pdf",
    "published": "2016-09",
    "summary": "This paper proposes an automated method to detect, group and rectify arbitrarily-arranged coplanar repeats via energy minimization. The proposed energy functional combines several features reflecting how planes with repeated elements are built and projected into images. The scene is modeled globally, so discriminating interactions between different coplanar repeat groups and scene planes can be captured. An inference framework based on a recent variant of _-expansion is described and fast convergence is demonstrated. The proposed method is evaluated using a new dataset of annotated images containing coplanar repeats in diverse arrangements. The dataset will be made publicly available. A significant improvement in rectifications estimated from detected coplanar repeats is reported with respect to two widely used geometric multi-model fitting methods."
  },
  "bmvc2016_main_two-streamsr-cnnsforactionrecognitioninvideos": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Two-Stream SR-CNNs for Action Recognition in Videos",
    "authors": [
      "Yifan Wang",
      "Jie Song",
      "Limin Wang",
      "Luc Van Gool",
      "Otmar Hilliges"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper108/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper108/paper108.pdf",
    "published": "2016-09",
    "summary": "Human action is a high-level concept in computer vision research and understanding it may benefit from different semantics, such as human pose, interacting objects, and scene context. In this paper, we explicitly exploit semantic cues with aid of existing object detectors for action recognition in videos, and thoroughly study their effect on the recognition performance for different types of actions. Specifically, we propose a new deep architecture by incorporating object/human detection results into the framework for action recognition, called two-stream semantic region based CNNs (SR-CNNs). Our proposed architecture not only shares great modeling capacity with two-stream input augmentation, but also exhibits the flexibility of leveraging semantic cues (e.g. scene, person, object) for action understanding. We perform experiments on UCF101 dataset and demonstrate its superior performance to the original two-stream CNNs. In addition, we systematically study the effect of incorporating semantic cues on the recognition performance for different types of action classes, and try to provide some insights for building more reasonable action benchmarks and robust recognition algorithms."
  },
  "bmvc2016_main_anefficientconvolutionalnetworkforhumanposeestimation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "An Efficient Convolutional Network for Human Pose Estimation",
    "authors": [
      "Umer Rafi",
      "Bastian Leibe",
      "Juergen Gall",
      "Ilya Kostrikov"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper109/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper109/paper109.pdf",
    "published": "2016-09",
    "summary": "In recent years, human pose estimation has greatly benefited from deep learning and huge gains in performance have been achieved. The trend to maximise the accuracy on benchmarks, however, resulted in computationally expensive deep network architectures that require expensive hardware and pre-training on large datasets. This makes it difficult to compare different methods and to reproduce existing results. In this paper, we therefore propose an efficient deep network architecture that can be efficiently trained on mid-range GPUs without the need of any pre-training. Despite the low computational requirements of our network, it is on par with much more complex models on popular benchmarks for human pose estimation."
  },
  "bmvc2016_main_adataaugmentationmethodologyfortrainingmachine/deeplearninggaitrecognitionalgorithms": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "A data augmentation methodology for training machine/deep learning gait recognition algorithms",
    "authors": [
      "Christoforos Charalambous",
      "Anil Bharath"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper110/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper110/paper110.pdf",
    "published": "2016-09",
    "summary": "There are several confounding factors that can reduce the accuracy of gait recognition systems. These factors can reduce the distinctiveness, or alter the features used to characterise gait; they include variations in clothing, lighting, pose and environment, such as the walking surface. Full invariance to all confounding factors is challenging in the absence of high-quality labelled training data. We introduce a simulation-based methodology and a subject-specific dataset which can be used for generating synthetic video frames and sequences for data augmentation. With this methodology, we generated a multi-modal dataset. In addition, we supply simulation files that provide the ability to simultaneously sample from several confounding variables. The basis of the data is real motion capture data of subjects walking and running on a treadmill at different speeds. Results from gait recognition experiments suggest that information about the identity of subjects is retained within synthetically generated examples. The dataset and methodology allow studies into fully-invariant identity recognition spanning a far greater number of observation conditions than would otherwise be possible."
  },
  "bmvc2016_main_meanboxpoolingarichimagerepresentationandoutputembeddingforthevisualmadlibstask": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding forthe Visual Madlibs Task",
    "authors": [
      "Mateusz Malinowski",
      "Ashkan Mokarian",
      "Mario Fritz"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper111/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper111/paper111.pdf",
    "published": "2016-09",
    "summary": "We present a method that pools over CNN representations of a large number, highly overlapping object proposals for the Visual Madlibs task. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on this task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by maximizing similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs."
  },
  "bmvc2016_main_practicalviewonfacepresentationattackdetection": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Practical View on Face Presentation Attack Detection",
    "authors": [
      "Naser Damer",
      "Kristiyan Dimitrov"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper112/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper112/paper112.pdf",
    "published": "2016-09",
    "summary": "Face recognition is one of the most socially accepted forms of biometric recognition. The recent availability of very accurate and efficient face recognition algorithms leaves the vulnerability to presentation attacks as the major challenge to face recognition solutions. Previous works have shown high preforming presentation attack detection PAD solutions under controlled evaluation scenarios. This work tried to analyze the practical use of PAD by investigating the more realistic scenario of cross-database evaluation and presenting a state-of-the-art performance comparison. The work also investigated the relation between the video duration and the PAD performance. This is done along with presenting an optical flow based approach that proves to outperform state-of-the-art solutions in most experiment settings."
  },
  "bmvc2016_main_fastfeature-lessquaternion-basedparticleswarmoptimizationforobjectposeestimationfromrgb-dimages": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Fast Feature-Less Quaternion-based Particle Swarm Optimization for ObjectPose Estimation From RGB-D Images",
    "authors": [
      "Stefano Rosa",
      "Giorgio Toscana"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper113/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper113/paper113.pdf",
    "published": "2016-09",
    "summary": "We present a novel quaternion-based formulation of Particle Swarm Optimization for pose estimation which, differently from other approaches, does not rely on image features or machine learning. The quaternion formulation avoids the gimbal lock problem, and the objective function is based on raw 2D depth information only, under the assumption that the object region is segmented from the background. This makes the algorithm suitable for pose estimation of objects with large variety in appearance, from lack of texture to strong textures, for the task of robotic grasping. We find candidate object regions using a graph-based image segmentation approach that integrates color and depth information, but the PSO is agnostic to the segmentation algorithm used. The algorithm is implemented on GPU, and the nature of the objective function allows high parallelization. We test the approach on different publicly available RGB-D object datasets, discuss the results and compare them with other existing methods."
  },
  "bmvc2016_main_graphconvolutionalneuralnetwork": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Graph Convolutional Neural Network",
    "authors": [
      "Michael Edwards",
      "Xianghua Xie"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper114/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper114/paper114.pdf",
    "published": "2016-09",
    "summary": "The benefit of localized features within the regular domain has given rise to the use of \\acp{CNN} in machine learning, with great proficiency in the image classification. The use of \\acp{CNN} becomes problematic within the irregular spatial domain due to design and convolution of a kernel filter being non-trivial. One solution to this problem is to utilize graph signal processing techniques and the convolution theorem to perform convolutions on the graph of the irregular domain to obtain feature map responses to learnt filters. We propose graph convolution and pooling operators analogous to those in the regular domain. We also provide gradient calculations on the input data and spectral filters, which allow for the deep learning of an irregular spatial domain problem. Signal filters take the form of spectral multipliers, applying convolution in the graph spectral domain. Applying smooth multipliers results in localized convolutions in the spatial domain, with smoother multipliers providing sharper feature maps. Algebraic Multigrid is presented as a graph pooling method, reducing the resolution of the graph through agglomeration of nodes between layers of the network. Evaluation of performance on the MNIST digit classification problem in both the regular and irregular domain is presented, with comparison drawn to standard \\ac{CNN}. The proposed graph \\ac{CNN} provides a deep learning method for the irregular domains present in the machine learning community, obtaining 94.23\\% on the regular grid, and 94.96\\% on a spatially irregular subsampled MNIST."
  },
  "bmvc2016_main_ihaveseenenoughtransferringpartsacrosscategories": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "I Have Seen Enough: Transferring Parts Across Categories",
    "authors": [
      "David Novotny",
      "Diane Larlus",
      "Andrea Vedaldi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper115/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper115/paper115.pdf",
    "published": "2016-09",
    "summary": "The recent successes of deep learning have been possible due to the availability of increasingly large quantities of annotated data. A natural question, however, is whether further progress can be indefinitely sustained by annotating more data, or whether there is a saturation point beyond which a problem is essentially solved, or the capacity of a model is saturated. In this paper we examine this question from the viewpoint of learning shareable semantic parts, a fundamental building block to generalize visual knowledge between object categories. We ask two research questions often neglected: whether semantic parts are also visually shareable between classes, and how many annotations are required to learn them. In order to answer such questions, we collect 15,000 images of 100 animal classes and annotate them with parts. We then thoroughly test active learning and domain adaptation techniques to generalize to unseen classes parts that are learned from a limited number of classes and example images. Our experiments show that, for a majority of the classes, part annotations transfer well, and that performance reaches 98% of the accuracy of the fully annotated scenario by providing only a few thousand examples."
  },
  "bmvc2016_main_impatientdnns-deepneuralnetworkswithdynamictimebudgets": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Impatient DNNs - Deep Neural Networks with Dynamic Time Budgets",
    "authors": [
      "Manuel Amthor",
      "Erik  Rodner",
      "Joachim Denzler"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper116/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper116/paper116.pdf",
    "published": "2016-09",
    "summary": "We propose Impatient Deep Neural Networks (DNNs) which deal with dynamic time budgets during application. They allow for individual budgets given a priori for each test example and for anytime prediction, i.e., a possible interruption at multiple stages during inference while still providing output estimates. Our approach can therefore tackle the computational costs and energy demands of DNNs in an adaptive manner, a property essential for real-time applications. Our Impatient DNNs are based on a new general framework of learning dynamic budget predictors using risk minimization, which can be applied to current DNN architectures by adding early prediction and additional loss layers. A key aspect of our method is that all of the intermediate predictors are learned jointly. In experiments, we evaluate our approach for different budget distributions, architectures, and datasets. Our results show a significant gain in expected accuracy compared to common baselines."
  },
  "bmvc2016_main_maximummarginlinearclassifiersinunionsofsubspaces": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Maximum Margin Linear Classifiers in Unions of Subspaces",
    "authors": [
      "Xinrui Lyu",
      "Joaquin Zepeda",
      "Patrick Perez"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper117/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper117/paper117.pdf",
    "published": "2016-09",
    "summary": "In this work, we propose a framework, dubbed Union-of-Subspaces SVM (US-SVM), to learn linear classifiers as sparse codes over a learned dictionary. In contrast to discriminative sparse coding with a learned dictionary, it is not the data but the classifiers that are sparsely encoded. Experiments in visual categorization demonstrate that, at training time, the joint learning of the classifiers and of the over-complete dictionary allows the discovery and sharing of mid-level attributes. The resulting classifiers further have a very compact representation in the learned dictionaries, offering substantial performance advantages over standard SVM classifiers for a fixed representation sparsity. This high degree of sparsity of our classifier also provides computational gains, especially in the presence of numerous classes. In addition, the learned atoms can help identify several intra-class modalities."
  },
  "bmvc2016_main_aerialimagegeolocalizationfromrecognitionandmatchingofroadsandintersections": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Aerial image geolocalization from recognition and matching of roads and intersections",
    "authors": [
      "Dragos Costea",
      "Marius Leordeanu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper118/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper118/paper118.pdf",
    "published": "2016-09",
    "summary": "Aerial image analysis at a semantic level is important in many applications with strong potential impact in industry and consumer use, such as automated mapping, urban planning, real estate, environment monitoring or disaster relief. The problem is enjoying a great interest in computer vision and remote sensing, due to the increasing computa- tional power and improvements in automated image understanding algorithms. In this paper, we address the task of automatic geolocalization of aerial images from recognition and matching of roads and intersections. Our proposed method is a novel contribution in the literature that could enable many applications of aerial image analysis where GPS data is not available. We offer a complete pipeline for geolocalization, from the detec- tion of roads and intersections, to the identification of the enclosing geographic region by matching detected intersections to previously learned manually labeled ones. This step is followed by accurate geometric alignment between the detected roads and the manually labeled maps. We test on a novel dataset with aerial images of two European cities and use the publicly available OpenStreetMap project for collecting ground truth roads annotations. We show in extensive experiments that our approach produces highly accurate localizations in the challenging case when we train on images from one city and test on the other, with relatively poor quality of the aerial images. We also show that the alignment between detected roads and pre-stored manual annotations can be effectively used for improving the quality of road detection."
  },
  "bmvc2016_main_learninglocalfeaturedescriptorswithtripletsandshallowconvolutionalneuralnetworks": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning local feature descriptors with triplets and shallow convolutional neural networks",
    "authors": [
      "Vassileios Balntas",
      "Edgar Riba",
      "Daniel Ponsa",
      "Krystian  Mikolajczyk"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper119/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper119/paper119.pdf",
    "published": "2016-09",
    "summary": "It has recently been demonstrated that local feature descriptors based on convolutional neural networks (CNN) can significantly improve the matching performance. Previous work on learning such descriptors has focused on exploiting pairs of positive and negative patches to learn discriminative CNN representations. In this work, we propose to utilize triplets of training samples, together with in-triplet mining of hard negatives. We show that our method achieves state of the art results, without the computational overhead typically associated with mining of negatives and with lower complexity of the network architecture. We compare our approach to recently introduced convolutional local feature descriptors, and demonstrate the advantages of the proposed methods in terms of performance and speed. We also examine different loss functions associated with triplets."
  },
  "bmvc2016_main_onlinefeatureselectionforvisualtracking": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Online Feature Selection for Visual Tracking",
    "authors": [
      "Giorgio Roffo",
      "Simone Melzi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper120/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper120/paper120.pdf",
    "published": "2016-09",
    "summary": "Object tracking is one of the most important tasks in many applications of computer vision. Many tracking methods use a fixed set of features ignoring that appearance of a target object may change drastically due to intrinsic and extrinsic factors. The ability to dynamically identify discriminative features would help in handling the appearance variability by improving tracking performance. The contribution of this work is threefold. Firstly, this paper presents a collection of several modern feature selection approaches selected among filter, embedded, and wrapper methods. Secondly, we provide extensive tests regarding the classification task intended to explore the strengths and weaknesses of the proposed methods with the goal to identify the right candidates for online tracking. Finally, we show how feature selection mechanisms can be successfully employed for ranking the features used by a tracking system, maintaining high frame rates. In particular, feature selection mounted on the Adaptive Color Tracking (ACT) system operates at over 110 FPS. This work demonstrates the importance of feature selection in online and realtime applications, resulted in what is clearly a very impressive performance, our solutions improve by 3% up to 7% the baseline ACT while providing superior results compared to 29 state-of-the-art tracking methods."
  },
  "bmvc2016_main_deepaggregationoflocal3dgeometricfeaturesfor3dmodelretrieval": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Deep Aggregation of Local 3D Geometric Features for 3D Model Retrieval",
    "authors": [
      "Takahiko Furuya",
      "Ryutarou Ohbuchi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper121/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper121/paper121.pdf",
    "published": "2016-09",
    "summary": "Aggregation of local features is a well-studied approach for image as well as 3D model retrieval (3DMR). A carefully designed local 3D geometric feature is able to describe detailed local geometry of 3D model, often with invariance to geometric transformations that include 3D rotation of local 3D regions. For efficient 3DMR, these local features are aggregated into a feature per 3D model. A recent alternative, end-to-end 3D Deep Convolutional Neural Network (3D-DCNN), has achieved accuracy superior to the abovementioned aggregation-of-local-features approach. However, current 3D-DCNN based methods have weaknesses; they lack invariance against 3D rotation, and they often miss detailed geometrical features due to their quantization of shapes into coarse voxels in applying 3D-DCNN. In this paper, we propose a novel deep neural network for 3DMR called Deep Local feature Aggregation Network (DLAN) that combines extraction of rotation-invariant 3D local features and their aggregation in a single deep architecture. The DLAN describes local 3D regions of a 3D model by using a set of 3D geometric features invariant to local rotation. The DLAN then aggregates the set of features into a (global) rotation-invariant and compact feature per 3D model. Experimental evaluation shows that the DLAN outperforms the existing deep learning-based 3DMR algorithms."
  },
  "bmvc2016_main_learninggrimacesbywatchingtv": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning Grimaces by Watching TV",
    "authors": [
      "Samuel Albanie",
      "Andrea Vedaldi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper122/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper122/paper122.pdf",
    "published": "2016-09",
    "summary": "Differently from computer vision systems which require explicit supervision, humans can learn facial expressions by simply observing other humans in their environment. In this paper, we look at the problem of developing similar capabilities in machine vision. As a starting point, we consider the problem of relating facial expressions to objectively measurable events occurring in videos. In particular, we consider a gameshow in which contestants play to win significant sums of money. We extract events affecting the game and corresponding facial expressions objectively and automatically from the videos, obtaining large quantities of labelled data for our study. We also develop, using benchmarks such as FER and SFEW 2.0, state-of-the-art deep neural networks for facial expression recognition, showing that pre-training on face verification data can be highly beneficial for this task. Then, we extend these models to use facial expressions to predict events in videos and learn nameable expressions from them. The dataset and emotion recognition models are available at http://www.robots.ox.ac.uk/~vgg/data/facevalue."
  },
  "bmvc2016_main_accurateandrobustfacerecognitionfromrgb-dimageswithadeeplearningapproach": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Accurate and robust face recognition from RGB-D images with a deep learning approach",
    "authors": [
      "Yuancheng Lee",
      "Jiancong Chen",
      "Ching Wei Tseng",
      "Shang-Hong Lai"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper123/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper123/paper123.pdf",
    "published": "2016-09",
    "summary": "Face recognition from RGB-D images utilizes 2 complementary types of image data, i.e. colour and depth images, to achieve more accurate recognition. In this paper, we propose a face recognition system based on deep learning, which can be used to verify and identify a subject from the colour and depth face images captured with a consumer-level RGB-D camera. To recognize faces with colour and depth information, our system contains 3 parts: depth image recovery, deep learning for feature extraction, and joint classification. To alleviate the problem of the limited size of available RGB-D data for deep learning, our deep network is firstly trained with colour and grayscale images from CASIA-WebFace dataset, and later fine-tuned on depth images for transfer learning. Our experiments on some public and our own RGB-D face datasets show that the proposed face recognition system provides very accurate face recognition results and it is robust against variations in head rotation and environmental illumination."
  },
  "bmvc2016_main_globaldeconvolutionalnetworksforsemanticsegmentation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Global Deconvolutional Networks for Semantic Segmentation",
    "authors": [
      "Vladimir Nekrasov",
      "Janghoon Ju",
      "Jaesik Choi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper124/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper124/paper124.pdf",
    "published": "2016-09",
    "summary": "Semantic image segmentation is a principal problem in computer vision, where the aim is to correctly classify each individual pixel of an image into a semantic label. Its widespread use in many areas, including medical imaging and autonomous driving, has fostered extensive research in recent years. Empirical improvements in tackling this task have primarily been motivated by successful exploitation of Convolutional Neural Networks (CNNs) pre-trained for image classification and object recognition. However, the pixel-wise labelling with CNNs has its own unique challenges: (1) an accurate deconvolution, or upsampling, of low-resolution output into a higher-resolution segmentation mask and (2) an inclusion of global information, or context, within locally extracted features. To address these issues, we propose a novel architecture to conduct the equivalent of the deconvolution operation globally and acquire dense predictions. We demonstrate that it leads to improved performance of state-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark, reaching 74.0% mean IU accuracy on the test set."
  },
  "bmvc2016_main_convolutionalsparsecoding-basedimagedecomposition": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Convolutional Sparse Coding-based Image Decomposition",
    "authors": [
      "He Zhang",
      "Vishal Patel"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper125/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper125/paper125.pdf",
    "published": "2016-09",
    "summary": "We propose a novel sparsity-based method for cartoon and texture decomposition based on Convolutional Sparse Coding (CSC). Our method first learns a set of generic filters that can sparsely represent cartoon and texture type images. Then using these learned filters, we propose a sparsity-based optimization framework to decompose a given image into cartoon and texture components. By working directly on the whole image, the proposed image separation algorithm does not need to divide the image into overlapping patches for leaning local dictionaries. Extensive experiments indicate the proposed method performs favorably compared to state-of-the-art image separation methods."
  },
  "bmvc2016_main_domainadaptivesubspaceclustering": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Domain Adaptive Subspace Clustering",
    "authors": [
      "Mahdi Abavisani",
      "Vishal Patel"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper126/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper126/paper126.pdf",
    "published": "2016-09",
    "summary": "We propose domain adaptive extensions of the recently introduced sparse subspace clustering and low-rank representation-based subspace clustering algorithms for clustering data lying in a union of subspaces. We propose a general method that learns the projections of data in a space where the sparsity or low-rankness of data is maintained. We propose an efficient iterative procedure for solving the proposed optimization problems. Various experiments on face, object and handwritten digits datasets show that the proposed methods can perform better than many competitive subspace clustering methods."
  },
  "bmvc2016_main_filtering3dkeypointsusinggistforaccurateimage-basedlocalization": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Filtering 3D Keypoints Using GIST For Accurate Image-Based Localization",
    "authors": [
      "Charbel Azzi",
      "Daniel Asmar",
      "Adel Fakih",
      "John Zelek"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper127/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper127/paper127.pdf",
    "published": "2016-09",
    "summary": "Image-Based Localization (IBL) is the problem of estimating the 3D pose of a camera with respect to a 3D representation of the scene. IBL is quite challenging in large-scale environments spanning a wide variety of viewpoints, illumination, and areas where matching a query image against hundreds of thousands of 3D points becomes prone to a large number of outliers and ambiguous situations. The current state of the art IBL solutions attempted to address the problem using paradigms such as bag-of-words, features co-occurrence, deep learning and others, with varying degrees of success. This paper presents GIST-based Search Space Reduction (GSSR) for indoor and large scale Image-Based Localization applications such as relocalization, loop closure and location recognition. GSSR explores the use of global descriptors, in particular GIST, to introduce a new similarity measure for keyframes that combines the GIST descriptor scores of all neighboring frames to qualify a limited number of 3D points for the matching process, hence reducing the problem to its small size counterpart. Our results on standard datasets show that our system can achieve better localization accuracy and speed than the main state of the art. It obtains approximately 0.24m and 0.3deg in less then 0.1 seconds."
  },
  "bmvc2016_main_probabilisticobstaclepartitioningofmonocularvideoforautonomousvehicles": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Probabilistic Obstacle Partitioning of Monocular Video for Autonomous Vehicles",
    "authors": [
      "Ryan Wolcott",
      "Ryan Eustice"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper128/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper128/paper128.pdf",
    "published": "2016-09",
    "summary": "This paper reports on visual obstacle detection from a monocular camera for autonomous vehicles. By leveraging a textured prior map, we propose a probabilistic formulation for finding the optimal image partition that separates obstacles from ground-plane. Our key insight is the use of a prior map that enables ground appearance models conditioned on prior map texture and a probabilistic optical flow vector formulation derived from known scene structure and camera egomotion. We evaluate our methods on a challenging urban setting using data collected on our autonomous platform and we demonstrate that a notion of obstacles in the camera frame can improve visual localization quality."
  },
  "bmvc2016_main_trackfacialpointsinunconstrainedvideos": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Track Facial Points in Unconstrained Videos",
    "authors": [
      "Xi Peng",
      "Qiong Hu",
      "Junzhou Huang",
      "Dimitris Metaxas"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper129/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper129/paper129.pdf",
    "published": "2016-09",
    "summary": "Tracking Facial Points in unconstrained videos is challenging due to the non-rigid deformation that changes over time. In this paper, we propose to exploit incremental learning for person-specific alignment in wild conditions. Our approach takes advantage of part-based representation and cascade regression for robust and efficient alignment on each frame. Unlike existing methods that usually rely on models trained offline, we incrementally update the representation subspace and the cascade of regressors in a unified framework to achieve personalized modeling on the fly. To alleviate the drifting issue, the fitting results are evaluated using a deep neural network, where well-aligned faces are picked out to incrementally update the representation and fitting models. Both image and video datasets are employed to valid the proposed method. The results demonstrate the superior performance of our approach compared with existing approaches in terms of fitting accuracy and efficiency."
  },
  "bmvc2016_main_structuredpredictionof3dhumanposewithdeepneuralnetworks": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Structured Prediction of 3D Human Pose with Deep Neural Networks",
    "authors": [
      "Bugra Tekin",
      "Isinsu Katircioglu",
      "Mathieu Salzmann",
      "Vincent Lepetit",
      "Pascal Fua"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper130/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper130/paper130.pdf",
    "published": "2016-09",
    "summary": "Most recent approaches to monocular 3D pose estimation rely on Deep Learning. They either train a Convolutional Neural Network to directly regress from image to 3D pose, which ignores the dependencies between human joints, or model these dependencies via a max-margin structured learning framework, which involves a high computational cost at inference time. In this paper, we introduce a Deep Learning regression architecture for structured prediction of 3D human pose from monocular images that relies on an overcomplete autoencoder to learn a high-dimensional latent pose representation and account for joint dependencies. We demonstrate that our approach outperforms state-of-the-art ones both in terms of structure preservation and prediction accuracy."
  },
  "bmvc2016_main_multi-taskrelativeattributepredictionbyincorporatinglocalcontextandglobalstyleinformation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Multi-task Relative Attribute Prediction by Incorporating Local Context and Global Style Information",
    "authors": [
      "Yuhang He",
      "Long Chen",
      "Jianda Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper131/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper131/paper131.pdf",
    "published": "2016-09",
    "summary": "Relative attribute represents the correlation degree of one attribute between an image pair\\,(e.g. one car image has more seat number than the other car image). While appearance highly and directly correlated relative attribute is easy to predict, fine-grained or appearance insensitive relative attribute prediction still remains as a challenging task. To address this challenge, we propose a multi-task trainable deep neural networks by incorporating an object's both local context and global style information to infer the relative attribute. In particular, we leverage convolutional neural networks (CNNs) to extract feature, followed by a ranking network to score the image pair. In CNNs, we treat features arising from intermediate convolution layers and full connection layers in CNNs as local context and global style information, respectively. Our intuition is that local context corresponds to bottom-to-top localised visual difference and global style information records high-level global subtle difference from a top-to-bottom scope between an image pair. We concatenate them together to escalate overall performance of multi-task relative attribute prediction. Finally, experimental results on 5 publicly available datasets demonstrate that our proposed approach outperforms several other state of the art methods and further achieves comparable results when comparing to very deep networks, like 152-ResNet and inception-v3."
  },
  "bmvc2016_main_deepmulti-taskattribute-drivenrankingforfine-grainedsketch-basedimageretrieval": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Deep Multi-task Attribute-driven Ranking for Fine-grained Sketch-based Image Retrieval",
    "authors": [
      "Jifei Song",
      "Yi-Zhe Song",
      "Tao Xiang",
      "Timothy Hospedales",
      "Xiang Ruan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper132/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper132/paper132.pdf",
    "published": "2016-09",
    "summary": "Fine-grained sketch-based image retrieval (SBIR) aims to go beyond conventional SBIR to perform instance-level cross-domain retrieval: finding the specific photo that matches an input sketch. Existing methods focus on designing/learning good features for cross-domain matching and/or learning cross-domain matching functions. However, they neglect the semantic aspect of retrieval, i.e., what meaningful object properties does a user try encode in her/his sketch? We propose a fine-grained SBIR model that exploits semantic attributes and deep feature learning in a complementary way. Specifically, we perform multi-task deep learning with three objectives, including: retrieval by fine-grained ranking on a learned representation, attribute prediction, and attribute-level ranking. Simultaneously predicting semantic attributes and using such predictions in the ranking procedure help retrieval results to be more semantically relevant. Importantly, the introduction of semantic attribute learning in the model allows for the elimination of the otherwise prohibitive cost of human annotations required for training a fine-grained deep ranking model. Experimental results demonstrate that our method outperforms the state-of-the-art on challenging fine-grained SBIR benchmarks while requiring less annotation."
  },
  "bmvc2016_main_theroleofcontextselectioninobjectdetection": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "The Role of Context Selection in Object Detection",
    "authors": [
      "Ruichi Yu",
      "Xi Chen",
      "Vlad Morariu",
      "Larry Davis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper133/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper133/paper133.pdf",
    "published": "2016-09",
    "summary": "We investigate the reasons why context in object detection has limited utility by isolating and evaluating the predictive power of different context cues under ideal conditions in which context provided by an oracle. Based on this study, we propose a region-based context re-scoring method with dynamic context selection to remove noise and emphasize informative context. We introduce latent indicator variables to select (or ignore) potential contextual regions, and learn the selection strategy with latent-SVM. We conduct experiments to evaluate the performance of the proposed context selection method on the SUN RGB-D dataset. The method achieves a significant improvement in terms of mean average precision (mAP), compared with both appearance based detectors and a conventional context model without the selection scheme."
  },
  "bmvc2016_main_highlyefficientregressionforscalablepersonre-identification": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Highly Efficient Regression for Scalable Person Re-Identification",
    "authors": [
      "Hanxiao Wang",
      "Shaogang Gong",
      "Tao Xiang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper134/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper134/paper134.pdf",
    "published": "2016-09",
    "summary": "Existing person re-identification models are poor for scaling up to large data required in real-world applications due to: (1) Complexity: They employ complex models for optimal performance resulting in high computational cost for training at a large scale; (2) Inadaptability: Once trained, they are unsuitable for incremental update to incorporate any new data available. This work proposes a truly scalable solution to re-id by addressing both problems. Specifically, a Highly Efficient Regression (HER) model is formulated by embedding the Fisher's criterion to a ridge regression model for very fast re-id model learning with scalable memory/storage usage. Importantly, this new HER model supports faster than real-time incremental model updates therefore making real-time active learning feasible in re-id with human-in-the-loop. Extensive experiments show that such a simple and fast model not only outperforms notably the state-of-the-art re-id methods, but also is more scalable to large data with additional benefits to active learning for reducing human labelling effort in re-id deployment."
  },
  "bmvc2016_main_reflectiveregressionof2d-3dfaceshapeacrosslargepose": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Reflective Regression of 2D-3D Face Shape Across Large Pose",
    "authors": [
      "Xuhui Jia",
      "Heng Yang",
      "Xiaolong Zhu",
      "Zhanghui Kuang",
      "Yifeng Niu",
      "Kwok-Ping Chan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper135/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper135/paper135.pdf",
    "published": "2016-09",
    "summary": "In this paper we present a novel reflective method to estimate 2D-3D face shape across large pose. We include the knowledge that a face is a 3D object into learning pipeline, and formulate face alignment as a 3DMM fitting problem, where the camera projection matrix and 3D shape parameters are learned by a extended cascaded pose regression framework. In order to improve algorithm robustness to difficult pose, we introduce a reflective invariant metric for failure alert. We investigate the relation between reflective variance and face misalignment error, and find there is strong correlation between them, consequently this finding is exploited to provide feedback to our algorithm. For the sample predicted as failure, we restart them with $better$ initialisation based on explicitly head pose estimation, which enhance the possibility of convergence. Extensive experiments on the challenging $AFLW$ and $AFW$ datasets demonstrate that our approach achieves superior performance than the state-of-the-art methods."
  },
  "bmvc2016_main_deepsignhybridcnn-hmmforcontinuoussignlanguagerecognition": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Deep Sign: Hybrid CNN-HMM for Continuous Sign Language Recognition",
    "authors": [
      "Oscar Koller",
      "Sepehr Zargaran",
      "Hermann Ney",
      "Richard Bowden"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper136/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper136/paper136.pdf",
    "published": "2016-09",
    "summary": "This paper introduces the end-to-end embedding of a CNN into a HMM, while interpreting the outputs of the CNN in a Bayesian fashion. The hybrid CNN-HMM combines the strong discriminative abilities of CNNs with the sequence modelling capabilities of HMMs. Most current approaches in the field of gesture and sign language recognition disregard the necessity of dealing with sequence data both for training and evaluation. With our presented end-to-end embedding we are able to improve over the state-of-the-art on three challenging benchmark continuous sign language recognition tasks by between 15% and 38% relative and up to 13.3% absolute."
  },
  "bmvc2016_main_measuringtheeffectofnuisancevariablesonclassifiers": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Measuring the effect of nuisance variables on classifiers",
    "authors": [
      "Alhussein Fawzi",
      "Pascal Frossard"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper137/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper137/paper137.pdf",
    "published": "2016-09",
    "summary": "In real-world classification problems, nuisance variables can cause wild variability in the data. Nuisance corresponds for example to geometric distortions of the image, occlusions, illumination changes or any other deformations that do not alter the ground truth label of the image. It is therefore crucial that designed classifiers are robust to nuisance variables, especially when these are deployed in real and possibly hostile environments. We propose in this paper a probabilistic framework for efficiently estimating the robustness of state-of-the-art classifiers and sampling problematic samples from the nuisance space. This allows us to visualize and understand the regions of the nuisance space that cause misclassification, in the perspective of improving robustness. Our probabilistic framework is applicable to arbitrary classifiers and potentially high-dimensional and complex nuisance spaces. We illustrate the proposed approach on several classification problems and compare classifiers in terms of their robustness to nuisances. Moreover, using our sampling technique, we visualize problematic regions in the nuisance space and infer insights into the weaknesses of classifiers as well as the features used in classification (e.g., in face recognition). We believe the proposed analysis tools represent an important step towards understanding large modern classification architectures and building architectures with better robustness to nuisance."
  },
  "bmvc2016_main_learningrobustgraphregularisationforsubspaceclustering": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Learning Robust Graph Regularisation for Subspace Clustering",
    "authors": [
      "Elyor Kodirov",
      "Tao Xiang",
      "Zhenyong Fu",
      "Shaogang Gong"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper138/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper138/paper138.pdf",
    "published": "2016-09",
    "summary": "Various subspace clustering methods have benefited from introducing a graph regularisation term in their objective functions. In this work, we identify two critical limitations of the graph regularisation term employed in existing subspace clustering models and provide solutions for both of them. First, the squared $l_2$-norm used in the existing term is replaced by a $l_1$-norm term to make the regularisation term more robust against outlying data samples and noise. Solving $l_1$ optimisation problems is notoriously expensive and a new formulation and an efficient algorithm are provided to make our model tractable. Second, instead of assuming that the graph topology and weights are known a priori and fixed during learning, we propose to learn the graph and integrate the graph learning into the proposed $l_1$-norm graph regularised optimisation problem. Extensive experiments conducted on five benchmark datasets show that the proposed robust subspace clustering method significantly outperforms the state-of-the-art."
  },
  "bmvc2016_main_solvingjigsawpuzzleswithlinearprogramming": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Solving Jigsaw Puzzles with Linear Programming",
    "authors": [
      "Rui Yu",
      "Chris Russell",
      "Lourdes Agapito"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper139/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper139/paper139.pdf",
    "published": "2016-09",
    "summary": "We propose a novel Linear Program (LP) based formulation for solving jigsaw puzzles. We formulate jigsaw solving as a set of successive global convex relaxations of the standard NP-hard formulation, that can describe both jigsaws with pieces of unknown position and puzzles of unknown position and orientation. The main contribution and strength of our approach comes from the LP assembly strategy. In contrast to existing greedy methods, our LP solver exploits all the pairwise matches simultaneously, and computes the position of each piece/component globally. The main advantages of our LP approach include: (i) a reduced sensitivity to local minima compared to greedy approaches, since our successive approximations are global and convex and (ii) an increased robustness to the presence of mismatches in the pairwise matches due to the use of a weighted L1 penalty. To demonstrate the effectiveness of our approach, we test our algorithm on public jigsaw datasets and show that it outperforms state-of-the-art methods."
  },
  "bmvc2016_main_detectingtrackingerrorsviaforecasting": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Detecting tracking errors via forecasting",
    "authors": [
      "Obaidullah Khalid",
      "Andrea Cavallaro",
      "Bernhard Rinner"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper140/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper140/paper140.pdf",
    "published": "2016-09",
    "summary": "We propose a tracker-independent framework to determine time instants when a video tracker fails. The framework is divided into two steps. First, we determine tracking quality by comparing the distributions of the tracker state and a region around the state. We generate the distributions using Distribution Fields and compute a tracking quality score by comparing the distributions using the L1 distance. Then, we model this score as a time series and employ the Auto Regressive Moving Average method to forecast future values of the quality score. A difference between the original and forecast returns an error signal that we use to detect a tracker failure. We validate the proposed approach over different datasets and demonstrate its flexibility with tracking results and sequences from the Visual Object Tracking (VOT) challenge."
  },
  "bmvc2016_main_oracleperformanceforvisualcaptioning": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Oracle Performance for Visual Captioning",
    "authors": [
      "Li Yao",
      "Nicolas  Ballas",
      "Kyunghyun Cho",
      "John Smith",
      "Yoshua Bengio"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper141/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper141/paper141.pdf",
    "published": "2016-09",
    "summary": "The task of associating images and videos with a natural language description has attracted a great amount of attention recently. The state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates performances that an oracle can obtain. In order to disentangle the contribution from visual model from the language model, our oracle assumes that high-quality visual concept extractor is available and focuses only on the language part. We demonstrate the construction of such oracles on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite the simplicity of the model and the training procedure, we show that current state-of-the-art models fall short when being compared with the learned oracle. Furthermore, it suggests the inability of current models in capturing important visual concepts in captioning tasks."
  },
  "bmvc2016_main_beyondactionrecognitionactioncompletioninrgb-ddata": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Beyond Action Recognition: Action Completion in RGB-D Data",
    "authors": [
      "Farnoosh Heidarivincheh",
      "Majid Mirmehdi",
      "Dima Damen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper142/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper142/paper142.pdf",
    "published": "2016-09",
    "summary": "An action is completed when its goal has been successfully achieved. Using current state-of-the-art depth features, designed primarily for action recognition, an incomplete sequence may still be classified as its complete counterpart due to the overlap in evidence. In this work we show that while features can perform comparably for action recognition, they vary in their ability to recognise incompletion. Experimenting on a novel dataset of 414 complete/incomplete object interaction sequences, spanning six actions and captured using an RGB-D camera, we test for completion using binary classification on labelled data. Results show that by selecting the suitable feature per action, we achieve 95.7% accuracy for recognising action completion."
  },
  "bmvc2016_main_videostreamretrievalofunseenqueriesusingsemanticmemory": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Video Stream Retrieval of Unseen Queries using Semantic Memory",
    "authors": [
      "Spencer Cappallo",
      "Thomas Mensink",
      "Cees Snoek"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper143/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper143/paper143.pdf",
    "published": "2016-09",
    "summary": "Retrieval of live, user-broadcast video streams is an under-addressed and increasingly relevant challenge. The on-line nature of the problem requires temporal evaluation and the unforeseeable scope of potential queries motivates an approach which can accommodate arbitrary search queries. To account for the breadth of possible queries, we adopt a no-example approach to query retrieval, which uses a query's semantic relatedness to pre-trained concept classifiers. To adapt to shifting video content, we propose memory pooling and memory welling methods that favor recent information over long past content. We identify two stream retrieval tasks, instantaneous retrieval at any particular time and continuous retrieval over a prolonged duration, and propose means for evaluating them. Three large scale video datasets are adapted to the challenge of stream retrieval. We report results for our search methods on the new stream retrieval tasks, as well as demonstrate their efficacy in a traditional, non-streaming video task."
  },
  "bmvc2016_main_mappingauto-contextdecisionforeststodeepconvnetsforsemanticsegmentation": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Mapping Auto-context Decision Forests to Deep ConvNets for Semantic Segmentation",
    "authors": [
      "David Richmond",
      "Dagmar Kainmueller",
      "Michael Yang",
      "Eugene Myers",
      "Carsten Rother"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper144/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper144/paper144.pdf",
    "published": "2016-09",
    "summary": "We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Random Forests (RF) and Neural Networks (NN). In this work, we explore the relationship between two special forms of these techniques: stacked RFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our main contribution is to show that Auto-context can be mapped to a deep ConvNet with novel architecture, and thereby trained end-to-end. This mapping can be viewed as an intelligent initialization of a deep ConvNet, enabling training even in the face of very limited amounts of training data. We also demonstrate an approximate mapping back from the refined ConvNet to a second stacked RF, with improved performance over the original. We experimentally verify that these mappings outperform stacked RFs for two different applications in computer vision and biology: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish."
  },
  "bmvc2016_main_fully-trainabledeepmatching": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Fully-trainable deep matching",
    "authors": [
      "James Thewlis",
      "Shuai Zheng",
      "Philip Torr",
      "Andrea Vedaldi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper145/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper145/paper145.pdf",
    "published": "2016-09",
    "summary": "Deep Matching (DM) is a popular method for quasi-dense image matching due to the quality of the correspondences that it can establish. However, DM, as originally proposed, is not a deep neural network and cannot be trained end-to-end via backpropagation. In this paper, we remove this limitation by rewriting the complete DM algorithm as a convolutional neural network. This results in a novel deep architecture for image matching that involves a number of new layer types and that, similar to recent networks for image segmentation, has a $U$-topology. We demonstrate the utility of the approach by improving the performance of DM by learning it end-to-end on an image matching task."
  },
  "bmvc2016_main_detectionoffastincomingobjectswithamovingcamera": {
    "conf_id": "BMVC2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2016",
    "title": "Detection of fast incoming objects with a moving camera",
    "authors": [
      "Fabio Poiesi",
      "Andrea Cavallaro"
    ],
    "page_url": "http://www.bmva.org/bmvc/2016/papers/paper146/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2016/papers/paper146/paper146.pdf",
    "published": "2016-09",
    "summary": "Using a monocular camera for early collision detection in cluttered scenes to elude fast incoming objects is a desirable but challenging functionality for mobile robots, such as small drones. We present a novel moving object detection and avoidance algorithm for an uncalibrated camera that uses only the optical flow to predict collisions. First, we estimate the optical flow and compensate the global camera motion. Then we detect incoming objects while removing the noise caused by dynamic textures, nearby terrain and lens distortion by means of an adaptively learnt background-motion model. Next, we estimate the time to contact, namely the expected time for an incoming object to cross the infinite plane defined by the extension of the image plane. Finally, we combine the time to contact and the compensated motion in a Bayesian framework to identify an object-free region the robot can move towards to avoid the collision. We demonstrate and evaluate the proposed algorithm using footage of flying robots that observe fast incoming objects such as birds, balls and other drones."
  }
}