{
  "iclr2017_workshop_atheoreticalframeworkforrobustnessof(deep)classifiersagainstadversarialsamples": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples",
    "authors": [
      "Beilun Wang",
      "Ji Gao",
      "Yanjun Qi"
    ],
    "page_url": "https://openreview.net/forum?id=HkcM7yVKl",
    "pdf_url": "https://openreview.net/pdf?id=HkcM7yVKl",
    "published": "2017-05",
    "summary": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes.The goal of this paper is not to introduce a single method, but to make theoretical steps toward fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis.By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong robust. TL;DR: We propose a theoretical framework to explain and measure model robustness and harden DNN model against adversarial attacks.",
    "code_link": "https://github.com/facebook/fb.resnet.torch"
  },
  "iclr2017_workshop_semi-superviseddeeplearningbymetricembedding": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Semi-supervised deep learning by metric embedding",
    "authors": [
      "Elad Hoffer",
      "Nir Ailon"
    ],
    "page_url": "https://openreview.net/forum?id=rJoZ1i3_l",
    "pdf_url": "https://openreview.net/pdf?id=rJoZ1i3_l",
    "published": "2017-05",
    "summary": "Deep networks are successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples.",
    "code_link": ""
  },
  "iclr2017_workshop_adaptivefeatureabstractionfortranslatingvideotolanguage": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Adaptive Feature Abstraction for Translating Video to Language",
    "authors": [
      "Yunchen Pu",
      "Martin Renqiang Min",
      "Zhe Gan",
      "Lawrence Carin"
    ],
    "page_url": "https://openreview.net/forum?id=BJ0K82lKg",
    "pdf_url": "https://openreview.net/pdf?id=BJ0K82lKg",
    "published": "2017-05",
    "summary": "A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. A novel attention mechanism with spatiotemporal alignment is employed to adaptively and sequentially focus on different layers of CNN features (levels of feature abstraction), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on the YouTube2Text benchmark. Experimental results demonstrate quantitatively the effectiveness of our proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantic structures.",
    "code_link": ""
  },
  "iclr2017_workshop_programmingwithadifferentiableforthinterpreter": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Programming With a Differentiable Forth Interpreter",
    "authors": [
      "Matko Bo\u0161njak",
      "Tim Rockt\u00e4schel",
      "Jason Naradowsky",
      "Sebastian Riedel"
    ],
    "page_url": "https://openreview.net/forum?id=SJAM_pVte",
    "pdf_url": "https://openreview.net/pdf?id=SJAM_pVte",
    "published": "2017-05",
    "summary": "There are families of neural networks that can learn to compute any function, provided sufficient training data. However, given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. Here we consider the case of prior procedural knowledge, such as knowing the overall recursive structure of a sequence transduction program or the fact that a program will likely use arithmetic operations on real numbers to solve a task. To this end we present a differentiable interpreter for the programming language Forth. Through a neural implementation of the dual stack machine that underlies Forth, programmers can write program sketches with slots that can be filled with behaviour trained from program input-output data. As the program interpreter is end-to-end differentiable, we can optimize this behaviour directly through gradient descent techniques on user specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex transduction tasks such as sequence sorting or addition with substantially less data and better generalisation over problem sizes. In addition, we introduce neural program optimisations based on symbolic computation and parallel branching that lead to significant speed improvements. ",
    "code_link": ""
  },
  "iclr2017_workshop_extrapolationandlearningequations": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Extrapolation and learning equations",
    "authors": [
      "Georg Martius",
      "Christoph H. Lampert"
    ],
    "page_url": "https://openreview.net/forum?id=BkgRp0FYe",
    "pdf_url": "https://openreview.net/pdf?id=BkgRp0FYe",
    "published": "2017-05",
    "summary": "In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.",
    "code_link": ""
  },
  "iclr2017_workshop_towardsanautomaticturingtestlearningtoevaluatedialogueresponses": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Towards an automatic Turing test: Learning to evaluate dialogue responses",
    "authors": [
      "Ryan Lowe",
      "Michael Noseworthy",
      "Iulian V. Serban",
      "Nicholas Angelard-Gontier",
      "Yoshua Bengio",
      "Joelle Pineau"
    ],
    "page_url": "https://openreview.net/forum?id=Sk7c3yVYg",
    "pdf_url": "https://openreview.net/pdf?id=Sk7c3yVYg",
    "published": "2017-05",
    "summary": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately,existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model\u2019s predictions correlate significantly, and at level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level.We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.",
    "code_link": ""
  },
  "iclr2017_workshop_fastchirplettransforminjectspriorsindeeplearningofanimalcallsandspeech": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Fast Chirplet Transform Injects Priors in Deep Learning of Animal Calls and Speech",
    "authors": [
      "Herv\u00e9 Glotin",
      "Julien Ricard",
      "Randall Balestriero"
    ],
    "page_url": "https://openreview.net/forum?id=HJWjTANFx",
    "pdf_url": "https://openreview.net/pdf?id=HJWjTANFx",
    "published": "2017-05",
    "summary": "Bioacoustic data set analyses require substantial baseline training data in order to accurately recognize and characterize specific kernels. Current approaches using the scattering framework and/or Convolutional Neural Nets (CNN) often require substantial dedicated computer time to achieve desired results. We propose a trade-off between these two approaches using a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrain the CNN. First we implement a Chirplet bioinspired auditory representation. Second we implement the first algorithm (and code) for a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of the FCT on selected large environmental databases: including months of Orca recordings and 1000 Birds species from the LifeClef challenge. Fourth, we validate the FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN by twenty eight percent for birds classification, and by twenty six percent for vowel classification. Scores are also enhanced by FCT pretraining, with a relative gain of 7.8\\% of Mean Average Precision on birds, and 2.3\\% of vowel accuracy against raw audio CNN. We conclude on with perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.",
    "code_link": ""
  },
  "iclr2017_workshop_shortanddeepsketchingandneuralnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Short and Deep: Sketching and Neural Networks",
    "authors": [
      "Amit Daniely",
      "Nevena Lazic",
      "Yoram Singer",
      "Kunal Talwar"
    ],
    "page_url": "https://openreview.net/forum?id=S1hsDCNFx",
    "pdf_url": "https://openreview.net/pdf?id=S1hsDCNFx",
    "published": "2017-05",
    "summary": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.Despite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree. A practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.",
    "code_link": ""
  },
  "iclr2017_workshop_symmetry-breakingconvergenceanalysisofcertaintwo-layeredneuralnetworkswithrelunonlinearity": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity",
    "authors": [
      "Yuandong Tian"
    ],
    "page_url": "https://openreview.net/forum?id=r1lVgRNtx",
    "pdf_url": "https://openreview.net/pdf?id=r1lVgRNtx",
    "published": "2017-05",
    "summary": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(\\vx; \\vw) = \\sum_{j=1}^K \\sigma(\\vw_j\\trans\\vx)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $\\vx$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $\\vw\\opt$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $\\vw\\opt$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice~\\cite{xavier, PReLU,lecun2012efficient}. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{\\vw\\opt_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $\\vw\\opt$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.",
    "code_link": ""
  },
  "iclr2017_workshop_unsupervisedperceptualrewardsforimitationlearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Unsupervised Perceptual Rewards for Imitation Learning",
    "authors": [
      "Pierre Sermanet",
      "Kelvin Xu",
      "Sergey Levine"
    ],
    "page_url": "https://openreview.net/forum?id=Byf3mmNFl",
    "pdf_url": "https://openreview.net/pdf?id=Byf3mmNFl",
    "published": "2017-05",
    "summary": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions, which are dense and smooth, can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task.",
    "code_link": ""
  },
  "iclr2017_workshop_datasetaugmentationinfeaturespace": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Dataset Augmentation in Feature Space",
    "authors": [
      "Terrance DeVries",
      "Graham W. Taylor"
    ],
    "page_url": "https://openreview.net/forum?id=HyaF53XYx",
    "pdf_url": "https://openreview.net/pdf?id=HyaF53XYx",
    "published": "2017-05",
    "summary": "Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.",
    "code_link": ""
  },
  "iclr2017_workshop_neuralfunctionalprogramming": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Neural Functional Programming",
    "authors": [
      "John K. Feser",
      "Marc Brockschmidt",
      "Alexander L. Gaunt",
      "Daniel Tarlow"
    ],
    "page_url": "https://openreview.net/forum?id=Byp_ccVte",
    "pdf_url": "https://openreview.net/pdf?id=Byp_ccVte",
    "published": "2017-05",
    "summary": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.",
    "code_link": ""
  },
  "iclr2017_workshop_lifelongperceptualprogrammingbyexample": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Lifelong Perceptual Programming By Example",
    "authors": [
      "Alexander L. Gaunt",
      "Marc Brockschmidt",
      "Nate Kushman",
      "Daniel Tarlow"
    ],
    "page_url": "https://openreview.net/forum?id=rJNulIVtx",
    "pdf_url": "https://openreview.net/pdf?id=rJNulIVtx",
    "published": "2017-05",
    "summary": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.",
    "code_link": ""
  },
  "iclr2017_workshop_onrobustconceptsandsmallneuralnets": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "On Robust Concepts and Small Neural Nets",
    "authors": [
      "Amit Deshpande",
      "Sushrut Karmalkar"
    ],
    "page_url": "https://openreview.net/forum?id=ByUg_SNte",
    "pdf_url": "https://openreview.net/pdf?id=ByUg_SNte",
    "published": "2017-05",
    "summary": "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights. However, robust concepts often have small neural networks in practice. We show an efficient analog of the universal approximation theorem on the boolean hypercube in this context.We prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, that depend only on the noise-stability and approximation parameters, and are independent of n. We also give a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. We also show weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.",
    "code_link": ""
  },
  "iclr2017_workshop_exponentialmachines": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Exponential Machines",
    "authors": [
      "Alexander Novikov",
      "Mikhail Trofimov",
      "Ivan Oseledets"
    ],
    "page_url": "https://openreview.net/forum?id=rkm1sE4tg",
    "pdf_url": "https://openreview.net/pdf?id=rkm1sE4tg",
    "published": "2017-05",
    "summary": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.",
    "code_link": ""
  },
  "iclr2017_workshop_bit-pragmaticdeepneuralnetworkcomputing": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Bit-Pragmatic Deep Neural Network Computing",
    "authors": [
      "Jorge Albericio",
      "Patrick Judd",
      "Alberto Delm\u00e1s",
      "Sayeh Sharify",
      "Andreas Moshovos"
    ],
    "page_url": "https://openreview.net/forum?id=ryeF7mVFl",
    "pdf_url": "https://openreview.net/pdf?id=ryeF7mVFl",
    "published": "2017-05",
    "summary": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency.The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, Pragmatic calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, Pragmatic improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDiannao was reported to be 300x faster than commodity graphics processors. ",
    "code_link": "https://github.com/google/gemmlowp"
  },
  "iclr2017_workshop_onlinestructurelearningforsum-productnetworkswithgaussianleaves": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves",
    "authors": [
      "Wilson Hsu",
      "Agastya Kalra",
      "Pascal Poupart"
    ],
    "page_url": "https://openreview.net/forum?id=By7LxZNFe",
    "pdf_url": "https://openreview.net/pdf?id=By7LxZNFe",
    "published": "2017-05",
    "summary": "Sum-product networks (SPNs) have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique. ",
    "code_link": ""
  },
  "iclr2017_workshop_deeplearningwithsetsandpointclouds": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Deep Learning with Sets and Point Clouds",
    "authors": [
      "Siamak Ravanbakhsh",
      "Jeff Schneider",
      "Barnabas Poczos"
    ],
    "page_url": "https://openreview.net/forum?id=Bkj2v6XYl",
    "pdf_url": "https://openreview.net/pdf?id=Bkj2v6XYl",
    "published": "2017-05",
    "summary": "We introduce a simple permutation equivariant layer for deep learning with set structure. This type of layer, obtained by parameter-sharing, has a simple implementation and linear-time complexity in the size of each set. We use deep permutation-invariant networks to perform point-could classification and MNIST-digit summation, where in both cases the output is invariant to permutations of the input. In a semi-supervised setting, where the goal is make predictions for each instance within a set, we demonstrate the usefulness of this type of layer in set-outlier detection as well as semi-supervised learning with clustering side-information.",
    "code_link": ""
  },
  "iclr2017_workshop_discoveringobjectsandtheirrelationsfromentangledscenerepresentations": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Discovering objects and their relations from entangled scene representations",
    "authors": [
      "D. Raposo",
      "A. Santoro",
      "D.G.T. Barrett",
      "R. Pascanu",
      "T. Lillicrap",
      "P. Battaglia"
    ],
    "page_url": "https://openreview.net/forum?id=rkrjrvmKl",
    "pdf_url": "https://openreview.net/pdf?id=rkrjrvmKl",
    "published": "2017-05",
    "summary": "Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) - a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning.",
    "code_link": ""
  },
  "iclr2017_workshop_multiplicativelstmforsequencemodelling": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Multiplicative LSTM for sequence modelling",
    "authors": [
      "Ben Krause",
      "Iain Murray",
      "Steve Renals",
      "Liang Lu"
    ],
    "page_url": "https://openreview.net/forum?id=SJCS5rXFl",
    "pdf_url": "https://openreview.net/pdf?id=SJCS5rXFl",
    "published": "2017-05",
    "summary": "We introduce multiplicative LSTM (mLSTM), a novel recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. mLSTM is characterised by its ability to have different recurrent transition functions for each possible input, which we argue makes it more expressive for autoregressive density estimation. We demonstrate empirically that mLSTM outperforms standard LSTM and its deep variants for a range of character level modelling tasks, and that this improvement increases with the complexity of the task. This model achieves a test error of 1.19 bits/character on the last 4 million characters of the Hutter prize dataset when combined with dynamic evaluation.",
    "code_link": "https://github.com/benkrause/mLSTM"
  },
  "iclr2017_workshop_learningtodiscoversparsegraphicalmodels": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Learning to Discover Sparse Graphical Models",
    "authors": [
      "Eugene Belilovsky",
      "Kyle Kastner",
      "Gael Varoquaux",
      "Matthew B. Blaschko"
    ],
    "page_url": "https://openreview.net/forum?id=H1GvR47Ye",
    "pdf_url": "https://openreview.net/pdf?id=H1GvR47Ye",
    "published": "2017-05",
    "summary": "We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a maximum likelihood objective with a penalization on the precision matrix. Adapting this estimator to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is an indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function mapping from empirical covariance matrices to estimated graph structures.Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. We apply this framework to several real-world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional structure of the problem. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive(and generally superior) performance, compared with analytical methods. ",
    "code_link": ""
  },
  "iclr2017_workshop_adifferentiablephysicsenginefordeeplearninginrobotics": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "A Differentiable Physics Engine for Deep Learning in Robotics",
    "authors": [
      "Jonas Degrave",
      "Michiel Hermans",
      "Joni Dambre",
      "Francis wyffels"
    ],
    "page_url": "https://openreview.net/forum?id=HkrB8XXte",
    "pdf_url": "https://openreview.net/pdf?id=HkrB8XXte",
    "published": "2017-05",
    "summary": "One of the most important fields in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose an implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.",
    "code_link": ""
  },
  "iclr2017_workshop_automatedgenerationofmultilingualclustersfortheevaluationofdistributedrepresentations": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations",
    "authors": [
      "Philip Blair",
      "Yuval Merhav",
      "Joel Barry"
    ],
    "page_url": "https://openreview.net/forum?id=SyA0t9fFe",
    "pdf_url": "https://openreview.net/pdf?id=SyA0t9fFe",
    "published": "2017-05",
    "summary": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of outlier elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.",
    "code_link": "https://github.com/belph/wiki-sem-500"
  },
  "iclr2017_workshop_recurrentnormalizationpropagation": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Recurrent Normalization Propagation",
    "authors": [
      "C\u00e9sar Laurent",
      "Nicolas Ballas",
      "Pascal Vincent"
    ],
    "page_url": "https://openreview.net/forum?id=r1hNW-MYg",
    "pdf_url": "https://openreview.net/pdf?id=r1hNW-MYg",
    "published": "2017-05",
    "summary": "We propose an LSTM parametrizationthat preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows andpresent a way of initializing the weights accordingly.We evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.",
    "code_link": "https://github.com/jbornschein/draw"
  },
  "iclr2017_workshop_rendergangeneratingrealisticlabeleddata": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "RenderGAN: Generating Realistic Labeled Data",
    "authors": [
      "Leon Sixt",
      "Benjamin Wild",
      "Tim Landgraf"
    ],
    "page_url": "https://openreview.net/forum?id=Hk-bBcWYe",
    "pdf_url": "https://openreview.net/pdf?id=Hk-bBcWYe",
    "published": "2017-05",
    "summary": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ",
    "code_link": "https://github.com/berleon/deepdecoder"
  },
  "iclr2017_workshop_tuningrecurrentneuralnetworkswithreinforcementlearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Tuning Recurrent Neural Networks with Reinforcement Learning",
    "authors": [
      "Natasha Jaques",
      "Shixiang Gu",
      "Richard E. Turner",
      "Douglas Eck"
    ],
    "page_url": "https://openreview.net/forum?id=Syyv2e-Kx",
    "pdf_url": "https://openreview.net/pdf?id=Syyv2e-Kx",
    "published": "2017-05",
    "summary": "The approach of training sequence models using supervised learning and next-step predictionsuffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, butsignificantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.",
    "code_link": ""
  },
  "iclr2017_workshop_generalizablefeaturesfromunsupervisedlearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Generalizable Features From Unsupervised Learning",
    "authors": [
      "Mehdi Mirza",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=BynzZolYg",
    "pdf_url": "https://openreview.net/pdf?id=BynzZolYg",
    "published": "2017-05",
    "summary": "Humans learn a predictive model of the world and use this model to reason about future events and the consequences of actions. In contrast to most machine predictors, we exhibit an impressive ability to generalize to unseen scenarios and reason intelligently in these settings.One important aspect of this ability is physical intuition(Lake et al., 2016). In this work, we explore the potential of unsupervised learning to find features that promote better generalization to settings outside the supervised training distribution.Our task is predicting the stability of towers of square blocks. We demonstrate that an unsupervised model, trained to predict future frames of a video sequence of stable and unstable block configurations, can yield features that support extrapolating stability prediction to blocks configurations outside the training set distribution",
    "code_link": ""
  },
  "iclr2017_workshop_perceptionupdatingnetworksonarchitecturalconstraintsforinterpretablevideogenerativemodels": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Perception Updating Networks: On architectural constraints for interpretable video generative models",
    "authors": [
      "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network"
    ],
    "page_url": "https://openreview.net/forum?id=H1JBMVpdx",
    "pdf_url": "https://openreview.net/pdf?id=H1JBMVpdx",
    "published": "2017-05",
    "summary": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents sprites or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.",
    "code_link": "https://github.com/cnel/PUN"
  },
  "iclr2017_workshop_adversarialexamplesinthephysicalworld": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Adversarial examples in the physical world",
    "authors": [
      "Alexey Kurakin",
      "Ian J. Goodfellow",
      "Samy Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=HJGU3Rodl",
    "pdf_url": "https://openreview.net/pdf?id=HJGU3Rodl",
    "published": "2017-05",
    "summary": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
    "code_link": ""
  },
  "iclr2017_workshop_developmentofjavascript-baseddeeplearningplatformandapplicationtodistributedtraining": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Development of JavaScript-based deep learning platform and application to distributed training",
    "authors": [
      "Masatoshi Hidaka",
      "Ken Miura",
      "Tatsuya Harada"
    ],
    "page_url": "https://openreview.net/forum?id=HyFq3EF_e",
    "pdf_url": "https://openreview.net/pdf?id=HyFq3EF_e",
    "published": "2017-05",
    "summary": "Deep learning is increasingly attracting attention for processing big data. Existing frameworks for deep learning must be set up to specialized computer systems. Gaining sufficient computing resources therefore entails high costs of deployment and maintenance. In this work, we implement a matrix library and deep learning framework that uses JavaScript. It can run on web browsers operating on ordinary personal computers and smartphones. Using JavaScript, deep learning can be accomplished in widely diverse environments without the necessity for software installation. Using GPGPU from WebCL framework, our framework can train large scale convolutional neural networks such as VGGNet and ResNet. In the experiments, we demonstrate their practicality by training VGGNet in a distributed manner using web browsers as the client.",
    "code_link": ""
  },
  "iclr2017_workshop_songfrompiamusicallyplausiblenetworkforpopmusicgeneration": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Song From PI: A Musically Plausible Network for Pop Music Generation",
    "authors": [
      "Hang Chu",
      "Raquel Urtasun",
      "Sanja Fidler"
    ],
    "page_url": "https://openreview.net/forum?id=BJ2p6a_dg",
    "pdf_url": "https://openreview.net/pdf?id=BJ2p6a_dg",
    "published": "2017-05",
    "summary": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.",
    "code_link": ""
  },
  "iclr2017_workshop_gatedmultimodalunitsforinformationfusion": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Gated Multimodal Units for Information Fusion",
    "authors": [
      "John Arevalo",
      "Thamar Solorio",
      "Manuel Montes-y-G\u00f3mez",
      "Fabio A. Gonz\u00e1lez"
    ],
    "page_url": "https://openreview.net/forum?id=S12_nquOe",
    "pdf_url": "https://openreview.net/pdf?id=S12_nquOe",
    "published": "2017-05",
    "summary": "This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.",
    "code_link": "https://github.com/johnarevalo/gmu-mmimdb"
  },
  "iclr2017_workshop_chargedpointnormalizationanefficientsolutiontothesaddlepointproblem": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Charged Point Normalization: An Efficient Solution to the Saddle Point Problem",
    "authors": [
      "Armen Aghajanyan"
    ],
    "page_url": "https://openreview.net/forum?id=BJdqLbDde",
    "pdf_url": "https://openreview.net/pdf?id=BJdqLbDde",
    "published": "2017-05",
    "summary": "Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks.",
    "code_link": "https://github.com/fchollet/keras"
  },
  "iclr2017_workshop_compositionalkernelmachines": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Compositional Kernel Machines",
    "authors": [
      "Robert Gens",
      "Pedro Domingos"
    ],
    "page_url": "https://openreview.net/forum?id=rk34W-DOl",
    "pdf_url": "https://openreview.net/pdf?id=rk34W-DOl",
    "published": "2017-05",
    "summary": "Convolutional neural networks (convnets) have achieved impressive results on recent computer vision benchmarks. While they benefit from multiple layers that encode nonlinear decision boundaries and a degree of translation invariance, training convnets is a lengthy procedure fraught with local optima. Alternatively, a kernel method that incorporates the compositionality and symmetry of convnets could learn similar nonlinear concepts yet with easier training and architecture selection. We propose compositional kernel machines (CKMs), which effectively create an exponential number of virtual training instances by composing transformed sub-regions of the original ones. Despite this, CKM discriminant functions can be computed efficiently using ideas from sum-product networks. The ability to compose virtual instances in this way gives CKMs invariance to translations and other symmetries, and combats the curse of dimensionality. Just as support vector machines (SVMs) provided a compelling alternative to multilayer perceptrons when they were introduced, CKMs could become an attractive approach for object recognition and other vision problems. In this paper we define CKMs, explore their properties, and present promising results on NORB datasets. Experiments show that CKMs can outperform SVMs and be competitive with convnets in a number of dimensions, by learning symmetries and compositional concepts from fewer samples without data augmentation.",
    "code_link": ""
  },
  "iclr2017_workshop_theeffectivenessoftransferlearninginelectronichealthrecordsdata": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "The Effectiveness of Transfer Learning in Electronic Health Records Data",
    "authors": [
      "Sebastien Dubois",
      "Nathanael Romano",
      "Kenneth Jung",
      "Nigam Shah",
      "and David C. Kale"
    ],
    "page_url": "https://openreview.net/forum?id=B1_E8xrKe",
    "pdf_url": "https://openreview.net/pdf?id=B1_E8xrKe",
    "published": "2017-05",
    "summary": "The application of machine learning to clinical data from Electronic Health Records is limited by the scarcity of meaningful labels.Here we present initial results on the application of transfer learning to this problem.We explore the transfer of knowledge from source tasks in which training labels are plentiful but of limited clinical value to more meaningful target tasks that have few labels.",
    "code_link": ""
  },
  "iclr2017_workshop_forcedtolearndiscoveringdisentangledrepresentationswithoutexhaustivelabels": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels",
    "authors": [
      "Alexey Romanov",
      "Anna Rumshisky"
    ],
    "page_url": "https://openreview.net/forum?id=SkCmfeSFg",
    "pdf_url": "https://openreview.net/pdf?id=SkCmfeSFg",
    "published": "2017-05",
    "summary": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used forclustering and introduce a novel loss component that substantially improves the quality of produced clusters, is simple to apply to an arbitrary cost function, and does not require a complicated training procedure. ",
    "code_link": "https://github.com/edwin-de-jong/mnist-digits-stroke-sequence-data"
  },
  "iclr2017_workshop_deepnetsdontlearnviamemorization": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Deep Nets Don't Learn via Memorization",
    "authors": [
      "David Krueger*"
    ],
    "page_url": "https://openreview.net/forum?id=rJv6ZgHYg",
    "pdf_url": "https://openreview.net/pdf?id=rJv6ZgHYg",
    "published": "2017-05",
    "summary": "We use empirical methods to argue that deep neural networks (DNNs) do not achieve their performance by \\textit{memorizing} training data, in spite of overly-expressive model architectures.Instead, they learn a simple available hypothesis that fits the finite data samples. In support of this view, we establish that there are qualitative differences when learning noise vs.~natural datasets, showing that: (1) more capacity is needed to fit noise, (2) time to convergence is longer for random labels, but \\emph{shorter} for random inputs, and (3) DNNs trained on real data examples learn simpler functions than when trained with noise data, as measured by the sharpness of the loss function at convergence. Finally, we demonstrate that for appropriately tuned explicit regularization, e.g.~dropout, we can degrade DNN training performance on noise datasets without compromising generalization on real data. ",
    "code_link": ""
  },
  "iclr2017_workshop_intelligentsynapsesformulti-taskandtransferlearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Intelligent synapses for multi-task and transfer learning",
    "authors": [
      "Ben Poole*",
      "Friedemann Zenke*",
      "Surya Ganguli"
    ],
    "page_url": "https://openreview.net/forum?id=rJzabxSFg",
    "pdf_url": "https://openreview.net/pdf?id=rJzabxSFg",
    "published": "2017-05",
    "summary": "Deep learning has led to remarkable advances when applied to problems in which the data distribution does not change over the course of learning. In stark contrast, biological neural networks exhibit continual learning, solve a diversity of tasks simultaneously, and have no clear separation between training and evaluation phase. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery that enable non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce intelligent synapses that are capable of accumulating information over time, and exploiting this information to efficiently protect old memories from being overwritten as new problems are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.",
    "code_link": ""
  },
  "iclr2017_workshop_learningalgorithmsforactivelearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Learning Algorithms for Active Learning",
    "authors": [
      "Philip Bachman",
      "Alessandro Sordoni",
      "Adam Trischler"
    ],
    "page_url": "https://openreview.net/forum?id=rJj2ZxHtl",
    "pdf_url": "https://openreview.net/pdf?id=rJj2ZxHtl",
    "published": "2017-05",
    "summary": "We present a model that learns active learning algorithms via metalearning. For each metatask, our model jointly learns: a data representation, an item selection heuristic, and a one-shot classifier. Our model uses the item selection heuristic to construct a labeled support set for the one-shot classifier. Using metatasks based on the Omniglot and MovieLens datasets, we show that our model performs well in synthetic and practical settings.",
    "code_link": ""
  },
  "iclr2017_workshop_reinterpretingimportance-weightedautoencoders": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Reinterpreting Importance-Weighted Autoencoders",
    "authors": [
      "Chris Cremer",
      "Quaid Morris",
      "David Duvenaud"
    ],
    "page_url": "https://openreview.net/forum?id=Syw2ZgrFx",
    "pdf_url": "https://openreview.net/pdf?id=Syw2ZgrFx",
    "published": "2017-05",
    "summary": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution.We formally derive this result, and visualize the implicit importance-weighted approximate posterior.",
    "code_link": ""
  },
  "iclr2017_workshop_unsupervisedandscalablealgorithmforlearningnoderepresentations": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Unsupervised and Scalable Algorithm for Learning Node Representations",
    "authors": [
      "Tiago Pimentel",
      "Adriano Veloso",
      "Nivio Ziviani"
    ],
    "page_url": "https://openreview.net/forum?id=S1-6egSFl",
    "pdf_url": "https://openreview.net/pdf?id=S1-6egSFl",
    "published": "2017-05",
    "summary": "Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning fields, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efficient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes' neighborhoods as contexts to generate representations. NBNE achieves results comparable or better to the state-of-the-art in three different datasets.",
    "code_link": ""
  },
  "iclr2017_workshop_acceleratingsgdfordistributeddeep-learningusinganapproximtedhessianmatrix": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Accelerating SGD for Distributed Deep-Learning Using an Approximted Hessian Matrix",
    "authors": [
      "Sebastien Arnold",
      "Chunming Wang"
    ],
    "page_url": "https://openreview.net/forum?id=B1lpelBYl",
    "pdf_url": "https://openreview.net/pdf?id=B1lpelBYl",
    "published": "2017-05",
    "summary": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime. By leveraging the differences in gradients and parameters of multiple Workers, we are able to efficiently implement a distributed approximation of the Newton-Raphson method. We also present preliminary results which underline advantages and challenges of second-order methods for large stochastic optimization problems. In particular, our work suggests that novel strategies for combining gradients will provide further information on the loss surface.",
    "code_link": ""
  },
  "iclr2017_workshop_acceleratingeulerianfluidsimulationwithconvolutionalnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Accelerating Eulerian Fluid Simulation With Convolutional Networks",
    "authors": [
      "Jonathan Tompson",
      "Kristofer Schlachter",
      "Pablo Sprechmann",
      "Ken Perlin"
    ],
    "page_url": "https://openreview.net/forum?id=ByH2gxrKl",
    "pdf_url": "https://openreview.net/pdf?id=ByH2gxrKl",
    "published": "2017-05",
    "summary": "Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large linear system with many free-parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.",
    "code_link": ""
  },
  "iclr2017_workshop_robustnesstoadversarialexamplesthroughanensembleofspecialists": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Robustness to Adversarial Examples through an Ensemble of Specialists",
    "authors": [
      "Mahdieh Abbasi",
      "Christian Gagne"
    ],
    "page_url": "https://openreview.net/forum?id=S1cYxlSFx",
    "pdf_url": "https://openreview.net/pdf?id=S1cYxlSFx",
    "published": "2017-05",
    "summary": "We are proposing to use an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.",
    "code_link": ""
  },
  "iclr2017_workshop_neuralcombinatorialoptimizationwithreinforcementlearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Neural Combinatorial Optimization with Reinforcement Learning",
    "authors": [
      "Irwan Bello",
      "Hieu Pham",
      "Quoc Le",
      "Mohammad Norouzi",
      "Samy Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=Bk9mxlSFx",
    "pdf_url": "https://openreview.net/pdf?id=Bk9mxlSFx",
    "published": "2017-05",
    "summary": "We present a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city \\mbox{coordinates}, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. Without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to $100$ nodes. These results, albeit still quite far from state-of-the-art, give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems.",
    "code_link": ""
  },
  "iclr2017_workshop_neuralexpectationmaximization": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Neural Expectation Maximization",
    "authors": [
      "Klaus Greff",
      "Sjoerd van Steenkiste",
      "J\u00fcrgen Schmidhuber"
    ],
    "page_url": "https://openreview.net/forum?id=BJMO1grtl",
    "pdf_url": "https://openreview.net/pdf?id=BJMO1grtl",
    "published": "2017-05",
    "summary": "We introduce a novel framework for clustering that combines generalized EM with neural networks and can be implemented as an end-to-end differentiable recurrent neural network. It learns its statistical model directly from the data and can represent complex non-linear dependencies between inputs. We apply our framework to a perceptual grouping task and empirically verify that it yields the intended behavior as a proof of concept.",
    "code_link": ""
  },
  "iclr2017_workshop_onhyperparameteroptimizationinlearningsystems": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "On Hyperparameter Optimization in Learning Systems",
    "authors": [
      "Luca Franceschi",
      "Michele Donini",
      "Paolo Frasconi",
      "Massimiliano Pontil"
    ],
    "page_url": "https://openreview.net/forum?id=r1mQ01SYl",
    "pdf_url": "https://openreview.net/pdf?id=r1mQ01SYl",
    "published": "2017-05",
    "summary": "We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparametersof any iterative learning algorithm. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offsin terms of running time and space requirements. The reverse-mode procedure extends previous work by (Maclaurin et al. 2015) and offers the opportunity to insert constraints on the hyperparameters in a natural way. The forward-mode procedure is suitable for stochastic hyperparameter updates, which may significantly speedup the overall hyperparameter optimization procedure. ",
    "code_link": ""
  },
  "iclr2017_workshop_adversarialattacksonneuralnetworkpolicies": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Adversarial Attacks on Neural Network Policies",
    "authors": [
      "Sandy Huang",
      "Nicolas Papernot",
      "Ian Goodfellow",
      "Yan Duan",
      "Pieter Abbeel"
    ],
    "page_url": "https://openreview.net/forum?id=ryvlRyBKl",
    "pdf_url": "https://openreview.net/pdf?id=ryvlRyBKl",
    "published": "2017-05",
    "summary": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass ofadversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe asignificant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .",
    "code_link": ""
  },
  "iclr2017_workshop_audiosuper-resolutionusingneuralnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Audio Super-Resolution using Neural Networks",
    "authors": [
      "Volodymyr Kuleshov",
      "S. Zayd Enam",
      "Stefano Ermon"
    ],
    "page_url": "https://openreview.net/forum?id=S1gNakBFx",
    "pdf_url": "https://openreview.net/pdf?id=S1gNakBFx",
    "published": "2017-05",
    "summary": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.",
    "code_link": ""
  },
  "iclr2017_workshop_meanteachersarebetterrolemodelsweight-averagedconsistencytargetsimprovesemi-superviseddeeplearningresults": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
    "authors": [
      "Antti Tarvainen",
      "Harri Valpola"
    ],
    "page_url": "https://openreview.net/forum?id=ry8u21rtl",
    "pdf_url": "https://openreview.net/pdf?id=ry8u21rtl",
    "published": "2017-05",
    "summary": "The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Mean Teacher achieves error rate 4.35% on SVHN with 250 labels, better than Temporal Ensembling does with 1000 labels.",
    "code_link": "https://github.com/pytorch/pytorch"
  },
  "iclr2017_workshop_semanticembeddingsforprogrambehaviourpatterns": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Semantic embeddings for program behaviour patterns",
    "authors": [
      "Alexander Chistyakov",
      "Ekaterina Lobacheva",
      "Arseny Kuznetsov",
      "Alexey Romanenko"
    ],
    "page_url": "https://openreview.net/forum?id=BJ_X2yHFe",
    "pdf_url": "https://openreview.net/pdf?id=BJ_X2yHFe",
    "published": "2017-05",
    "summary": "In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behaviour graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.",
    "code_link": ""
  },
  "iclr2017_workshop_jointembeddingsofscenegraphsandimages": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Joint Embeddings of Scene Graphs and Images",
    "authors": [
      "Eugene Belilovsky",
      "Matthew Blaschko",
      "Jamie Ryan Kiros",
      "Raquel Urtasun",
      "Richard Zemel"
    ],
    "page_url": "https://openreview.net/forum?id=BkyScySKl",
    "pdf_url": "https://openreview.net/pdf?id=BkyScySKl",
    "published": "2017-05",
    "summary": "Multimodal representations of text and images have become popular in recent years. Text however has inherent ambiguities when describing visual scenes, leading to the recent development of datasets with detailed graphical descriptions in the form of scene graphs. We consider the task of joint representation of semantically precise scene graphs and images. We propose models for representing scene graphs and aligning them with images.We investigate methods based on bag-of-words, subpath representations, as well as neural networks. Our investigation proposes and contrasts several models which can address this task and highlights some unique challenges in both designing models and evaluation.",
    "code_link": ""
  },
  "iclr2017_workshop_tacticsofadversarialattackondeepreinforcementlearningagents": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents",
    "authors": [
      "Yen-Chen Lin",
      "Zhang-Wei Hong",
      "Yuan-Hong Liao",
      "Meng-Li Shih",
      "Ming-Yu Liu",
      "Min Sun"
    ],
    "page_url": "https://openreview.net/forum?id=r1Cy5yrKx",
    "pdf_url": "https://openreview.net/pdf?id=r1Cy5yrKx",
    "published": "2017-05",
    "summary": "We introduce two novel tactics for adversarial attack on deep reinforcement learning (RL) agents: strategically-timed and enchanting attack. For strategically- timed attack, our method selectively forces the deep RL agent to take the least likely action. For enchanting attack, our method lures the agent to a target state by staging a sequence of adversarial attacks. We show that both DQN and A3C agents are vulnerable to our proposed tactics of adversarial attack.",
    "code_link": ""
  },
  "iclr2017_workshop_jointtrainingofratingsandreviewswithrecurrentrecommendernetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Joint Training of Ratings and Reviews with Recurrent Recommender Networks",
    "authors": [
      "Chao-Yuan Wu",
      "Amr Ahmed",
      "Alex Beutel",
      "Alexander J. Smola"
    ],
    "page_url": "https://openreview.net/forum?id=Bkv9FyHYx",
    "pdf_url": "https://openreview.net/pdf?id=Bkv9FyHYx",
    "published": "2017-05",
    "summary": "Accurate modeling of ratings and text reviews is at the core of successful recommender systems. While neural networks have been remarkably successful in modeling images and natural language, they have been largely unexplored in recommender system research. In this paper, we provide aneural network model that combines ratings, reviews, and temporal patterns to learn highly accurate recommendations. We co-train for prediction on both numerical ratings and natural language reviews, as well as using a recurrent architecture to capture the dynamic components of users' and items' states. We demonstrate that incorporating text reviews and temporal dynamic gives state-of-the-art results over the IMDb dataset.",
    "code_link": ""
  },
  "iclr2017_workshop_particlevaluefunctions": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Particle Value Functions",
    "authors": [
      "Chris J. Maddison",
      "Dieterich Lawson",
      "George Tucker",
      "Nicolas Heess",
      "Arnaud Doucet",
      "Andriy Mnih",
      "Yee Whye Teh"
    ],
    "page_url": "https://openreview.net/forum?id=BJyBKyHKg",
    "pdf_url": "https://openreview.net/pdf?id=BJyBKyHKg",
    "published": "2017-05",
    "summary": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld. ",
    "code_link": ""
  },
  "iclr2017_workshop_naturallanguagegenerationindialogueusinglexicalizedanddelexicalizeddata": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data",
    "authors": [
      "Shikhar Sharma",
      "Jing He",
      "Kaheer Suleman",
      "Hannes Schulz",
      "Philip Bachman"
    ],
    "page_url": "https://openreview.net/forum?id=B1naD1rFx",
    "pdf_url": "https://openreview.net/pdf?id=B1naD1rFx",
    "published": "2017-05",
    "summary": "Natural language generation plays a critical role in spoken dialogue systems. We present a new approach to natural language generation for task-oriented dialogue using recurrent neural networks in an encoder-decoder framework. In contrast to previous work, our model uses both lexicalized and delexicalized components i.e. slot-value pairs for dialogue acts, with slots and corresponding values aligned together. This allows our model to learn from all available data including the slot-value pairing, rather than being restricted to delexicalized slots. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by transferring weights learnt from a pretrained sentence auto-encoder. Human evaluation of our best-performing model indicates that it generates sentences which users find more appealing.",
    "code_link": "https://github.com/tylin/coco-caption"
  },
  "iclr2017_workshop_lossisitsownrewardself-supervisionforreinforcementlearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Loss is its own Reward: Self-Supervision for Reinforcement Learning",
    "authors": [
      "Evan Shelhamer",
      "Parsa Mahmoudieh",
      "Max Argus",
      "Trevor Darrell"
    ],
    "page_url": "https://openreview.net/forum?id=S15PPJStl",
    "pdf_url": "https://openreview.net/pdf?id=S15PPJStl",
    "published": "2017-05",
    "summary": "Reinforcement learning, driven by reward, addresses tasks by optimizing policies for expected return. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, so we argue that reward alone is a noisy and impoverished signal for end-to-end optimization. To augment reward, we consider self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. Self-supervised pre-training improves the data efficiency and returns of end-to-end reinforcement learning on Atari.",
    "code_link": ""
  },
  "iclr2017_workshop_denovodrugdesignwithdeepgenerativemodelsanempiricalstudy": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "De novo drug design with deep generative models : an empirical study",
    "authors": [
      "Mehdi cherti",
      "Balazs Kegl",
      "Akin kazakci"
    ],
    "page_url": "https://openreview.net/forum?id=SkkC41HYl",
    "pdf_url": "https://openreview.net/pdf?id=SkkC41HYl",
    "published": "2017-05",
    "summary": "We present an empirical study about the usage of RNN generative models for stochastic optimization in the context of de novo drug design. We study different kinds of architectures and we find models that can generate molecules with higher values than ones seen in the training set. Our results suggest that we can improve traditional stochastic optimizers, that rely on random perturbations or random sampling by using generative models trained on unlabeled data, to perform knowledge-driven optimization. ",
    "code_link": "https://github.com/fchollet/keras"
  },
  "iclr2017_workshop_out-of-classnoveltygenerationanexperimentalfoundation": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Out-of-class novelty generation: an experimental foundation",
    "authors": [
      "Mehdi Cherti",
      "Bal\u00e1zs K\u00e9gl",
      "Ak\u0131n Kazak\u00e7\u0131"
    ],
    "page_url": "https://openreview.net/forum?id=r1QXQkSYg",
    "pdf_url": "https://openreview.net/pdf?id=r1QXQkSYg",
    "published": "2017-05",
    "summary": "Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess severalmetrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.",
    "code_link": ""
  },
  "iclr2017_workshop_memorymatchingnetworksforgenomicsequenceclassification": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Memory Matching Networks for Genomic Sequence Classification",
    "authors": [
      "Jack Lanchantin",
      "Ritambhara Singh",
      "Yanjun Qi"
    ],
    "page_url": "https://openreview.net/forum?id=ryh9ZySFg",
    "pdf_url": "https://openreview.net/pdf?id=ryh9ZySFg",
    "published": "2017-05",
    "summary": "When analyzing the genome, researchers have discovered that proteins bind to DNA based on certain patterns on the DNA sequence known as motifs. However, it is difficult to manually construct motifs for protein binding location prediction due to their complexity. Recently, external learned memory models have proven to be effective methods for reasoning over inputs and supporting sets. In this work, we present memory matching networks (MMN) for classifying DNA sequences as protein binding sites. Our model learns a memory bank of encoded motifs, which are dynamic memory modules, and then matches a new test sequence to each of the motifs to classify the sequence as a binding or non-binding site. ",
    "code_link": ""
  },
  "iclr2017_workshop_unseenstyletransferbasedonaconditionalfaststyletransfernetwork": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Unseen Style Transfer Based on a Conditional Fast Style Transfer Network",
    "authors": [
      "Keiji Yanai"
    ],
    "page_url": "https://openreview.net/forum?id=H1Y7-1HYg",
    "pdf_url": "https://openreview.net/pdf?id=H1Y7-1HYg",
    "published": "2017-05",
    "summary": "In this paper, we propose a feed-forward neural style transfer network which can transfer unseen arbitrary styles. To do that, first, we extend the fast neural style transfer network proposed by Johnson et al. (2016) so that the network can learn multiple styles at the same time by adding a conditional input. We call this as \u201ca conditional style transfer network\u201d. Next, we add a style condition network which generates a conditional signal from a style image directly, and train \u201ca conditional style transfer network with a style condition network\u201d in an end-to-end manner. The proposed network can generate a stylized image from a content image and a style image in one-time feed-forward computation instantly.",
    "code_link": ""
  },
  "iclr2017_workshop_adversarialdiscriminativedomainadaptation(workshopextendedabstract)": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Adversarial Discriminative Domain Adaptation (workshop extended abstract)",
    "authors": [
      "Eric Tzeng",
      "Judy Hoffman",
      "Kate Saenko",
      "Trevor Darrell"
    ],
    "page_url": "https://openreview.net/forum?id=B1Vjl1Stl",
    "pdf_url": "https://openreview.net/pdf?id=B1Vjl1Stl",
    "published": "2017-05",
    "summary": "Domain adversarial approaches have been at the core of many recent unsupervised domain adaptation algorithms. However, each new algorithm is presented independently with limited or no connections mentioned across the works. Instead, in this work we propose a unified view of adversarial adaptation methods. We show how to describe a variety of state-of-the-art adaptation methods within our framework and furthermore use our generalized view in order to better understand the similarities and differences between these recent approaches. In turn, this framework facilitates the development of new adaptation methods through modeling choices that combine the desirable properties of multiple existing methods. In this way, we propose a novel adversarial adaptation method that is effective yet considerably simpler than other competing methods. We demonstrate the promise of our approach by achieving state-of-the-art unsupervised adaptation results on the standard Office dataset. ",
    "code_link": ""
  },
  "iclr2017_workshop_changingmodelbehaviorattest-timeusingreinforcementlearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Changing Model Behavior at Test-time Using Reinforcement Learning",
    "authors": [
      "Augustus Odena",
      "Dieterich Lawson",
      "Christopher Olah"
    ],
    "page_url": "https://openreview.net/forum?id=Hk8-lkHKe",
    "pdf_url": "https://openreview.net/pdf?id=Hk8-lkHKe",
    "published": "2017-05",
    "summary": "Machine learning models are often used at test-time subject to constraints and trade-offs not present at training-time. For example, a computer vision model operating on an embedded device may need to perform real-time inference, or a translation model operating on a cell phone may wish to bound its average compute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change itstest-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.",
    "code_link": ""
  },
  "iclr2017_workshop_efficientsparse-winogradconvolutionalneuralnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Efficient Sparse-Winograd Convolutional Neural Networks",
    "authors": [
      "Xingyu Liu",
      "Song Han",
      "Huizi Mao",
      "William J. Dally"
    ],
    "page_url": "https://openreview.net/forum?id=r1rqJyHKg",
    "pdf_url": "https://openreview.net/pdf?id=r1rqJyHKg",
    "published": "2017-05",
    "summary": "Convolutional Neural Networks (CNNs) are compute intensive which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin (2015)) and network pruning (Han et al. (2015)) reduce the operation count. Unfortunately, these two methods cannot be combined\u2014because applying theWinograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we prune the weights in the \u201dWinograd domain\u201d (after the transform) to exploit static weight sparsity. Second, we move the ReLU operation into the \u201dWinograd domain\u201d to improve the sparsity of the transformed activations. On CIFAR-10, our method reduces the number of multiplications in the VGG-nagadomi model by 10.2x with no loss of accuracy.",
    "code_link": ""
  },
  "iclr2017_workshop_char2wavend-to-endspeechsynthesis": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Char2Wav: End-to-End Speech Synthesis",
    "authors": [
      "Jose Sotelo",
      "Soroush Mehri",
      "Kundan Kumar",
      "Joao Felipe Santos",
      "Kyle Kastner",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=B1VWyySKx",
    "pdf_url": "https://openreview.net/pdf?id=B1VWyySKx",
    "published": "2017-05",
    "summary": "We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoder-decoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.",
    "code_link": "https://github.com/sotelo/parrot"
  },
  "iclr2017_workshop_performanceguaranteesfortransferringrepresentations": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Performance guarantees for transferring representations",
    "authors": [
      "Daniel McNamara",
      "Maria-Florina Balcan"
    ],
    "page_url": "https://openreview.net/forum?id=rJNa3C4Yg",
    "pdf_url": "https://openreview.net/pdf?id=rJNa3C4Yg",
    "published": "2017-05",
    "summary": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. Our work proposes novel and general sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight sharing, which we validate with experiments.",
    "code_link": ""
  },
  "iclr2017_workshop_generativeadversariallearningofmarkovchains": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Generative Adversarial Learning of Markov Chains",
    "authors": [
      "Jiaming Song",
      "Shengjia Zhao",
      "Stefano Ermon"
    ],
    "page_url": "https://openreview.net/forum?id=S1L-hCNtl",
    "pdf_url": "https://openreview.net/pdf?id=S1L-hCNtl",
    "published": "2017-05",
    "summary": "We investigate generative adversarial training methods to learn a transition operator for a Markov chain, where the goal is to match its stationary distribution to a target data distribution. We propose a novel training procedure that avoids sampling directly from the stationary distribution, while still capable of reaching the target distribution asymptotically. The model can start from random noise, is likelihood free, and is able to generate multiple distinct samples during a single run. Preliminary experiment results show the chain can generate high quality samples when it approaches its stationary, even with smaller architectures traditionally considered for Generative Adversarial Nets.",
    "code_link": ""
  },
  "iclr2017_workshop_understandingintermediatelayersusinglinearclassifierprobes": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Understanding intermediate layers using linear classifier probes",
    "authors": [
      "Guillaume Alain",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=HJ4-rAVtl",
    "pdf_url": "https://openreview.net/pdf?id=HJ4-rAVtl",
    "published": "2017-05",
    "summary": "Neural network models have a reputation for being black boxes. We propose a new method to better understand the roles and dynamics of the intermediate layers.Our method uses linear classifiers, referred to as probes, where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training.We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems.",
    "code_link": ""
  },
  "iclr2017_workshop_deepcloakmaskingdeepneuralnetworkmodelsforrobustnessagainstadversarialsamples": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples",
    "authors": [
      "Ji Gao",
      "Beilun Wang",
      "Zeming Lin",
      "Weilin Xu"
    ],
    "page_url": "https://openreview.net/forum?id=r1X_kR4Yl",
    "pdf_url": "https://openreview.net/pdf?id=r1X_kR4Yl",
    "published": "2017-05",
    "summary": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task.To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.",
    "code_link": ""
  },
  "iclr2017_workshop_explainingthelearningdynamicsofdirectfeedbackalignment": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Explaining the Learning Dynamics of Direct Feedback Alignment",
    "authors": [
      "Justin Gilmer",
      "Colin Raffel",
      "Samuel S. Schoenholz",
      "Maithra Raghu",
      "and Jascha Sohl-Dickstein"
    ],
    "page_url": "https://openreview.net/forum?id=HkXKUTVFl",
    "pdf_url": "https://openreview.net/pdf?id=HkXKUTVFl",
    "published": "2017-05",
    "summary": "Two recently developed methods, Feedback Alignment (FA) and Direct Feedback Alignment (DFA), have been shown to obtain surprising performance on vision tasks by replacing the traditional backpropagation update with a random feedback update. However, it is still not clear what mechanisms allow learning to happen with these random updates.In this work we argue that DFA can be viewed as a noisy variant of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). We support this connection theoretically by comparing the update rules for the two methods.We additionally empirically verify that the random update matrices used in DFA work effectively as readout matrices, and that strong correlations exist between the error vectors used in the DFA and LAFS updates. With this new connection between DFA and LAFS we are able to explain why the alignment happens in DFA. ",
    "code_link": ""
  },
  "iclr2017_workshop_trainingasubsamplingmechanisminexpectation": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Training a Subsampling Mechanism in Expectation",
    "authors": [
      "Colin Raffel",
      "Dieterich Lawson"
    ],
    "page_url": "https://openreview.net/forum?id=BJBkkaNYe",
    "pdf_url": "https://openreview.net/pdf?id=BJBkkaNYe",
    "published": "2017-05",
    "summary": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation.We test this approach on a simple toy problem and discuss its shortcomings.",
    "code_link": "https://github.com/craffel/subsampling_in_expectation"
  },
  "iclr2017_workshop_deepkernelmachinesviathekernelreparametrizationtrick": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Deep Kernel Machines via the Kernel Reparametrization Trick",
    "authors": [
      "Jovana Mitrovic",
      "Dino Sejdinovic",
      "Yee Whye Teh"
    ],
    "page_url": "https://openreview.net/forum?id=Bkiqt3Ntg",
    "pdf_url": "https://openreview.net/pdf?id=Bkiqt3Ntg",
    "published": "2017-05",
    "summary": "While deep neural networks have achieved state-of-the-art performance on many tasks across varied domains, they still remain black boxes whose inner workings are hard to interpret and understand. In this paper, we develop a novel method for efficiently capturing the behaviour of deep neural networks using kernels. In particular, we construct a hierarchy of increasingly complex kernels that encode individual hidden layers of the network. Furthermore, we discuss how our framework motivates a novel supervised weight initialization method that discovers highly discriminative features already at initialization. ",
    "code_link": ""
  },
  "iclr2017_workshop_encodinganddecodingrepresentationswithsum-andmax-productnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Encoding and Decoding Representations with Sum- and Max-Product Networks",
    "authors": [
      "Antonio Vergari",
      "Robert Peharz",
      "Nicola Di Mauro",
      "Floriana Esposito"
    ],
    "page_url": "https://openreview.net/forum?id=rkndY2VYx",
    "pdf_url": "https://openreview.net/pdf?id=rkndY2VYx",
    "published": "2017-05",
    "summary": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation Learning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN encoding and decoding schemes prove highly competitive for Multi-Label Classification tasks.",
    "code_link": ""
  },
  "iclr2017_workshop_restrictedboltzmannmachinesprovideanaccuratemetricforretinalresponsestovisualstimuli": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Restricted Boltzmann Machines provide an accurate metric for retinal responses to visual stimuli",
    "authors": [
      "Christophe Gardella",
      "Olivier Marre",
      "Thierry Mora"
    ],
    "page_url": "https://openreview.net/forum?id=Sk1OOnNFx",
    "pdf_url": "https://openreview.net/pdf?id=Sk1OOnNFx",
    "published": "2017-05",
    "summary": "How to discriminate visual stimuli based on the activity they evoke in sensory neurons is still an open challenge. To measure discriminability power, we search for a neural metric that preserves distances in stimulus space, so that responses to different stimuli are far apart and responses to the same stimulus are close. Here, we show that Restricted Boltzmann Machines (RBMs) provide such a distance-preserving neural metric. Even when learned in a unsupervised way, RBM-based metric can discriminate stimuli with higher resolution than classical metrics.",
    "code_link": ""
  },
  "iclr2017_workshop_embracingdataabundance": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Embracing Data Abundance",
    "authors": [
      "Ondrej Bajgar",
      "Rudolf Kadlec and Jan Kleindienst"
    ],
    "page_url": "https://openreview.net/forum?id=H1U4mhVFe",
    "pdf_url": "https://openreview.net/pdf?id=H1U4mhVFe",
    "published": "2017-05",
    "summary": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and is offering the BookTest dataset as a step in that direction.",
    "code_link": ""
  },
  "iclr2017_workshop_playingsnesintheretrolearningenvironment": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Playing SNES in the Retro Learning Environment",
    "authors": [
      "Nadav Bhonker",
      "Shai Rozenberg",
      "Itay Hubara"
    ],
    "page_url": "https://openreview.net/forum?id=rJV7l2VFg",
    "pdf_url": "https://openreview.net/pdf?id=rJV7l2VFg",
    "published": "2017-05",
    "summary": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) has become a commonly used benchmark environment allowing algorithms to trainon various Atari 2600 games.In many games the state-of-the-art algorithms out-performhumans. Inthispaperweintroduceanewlearningenvironment,the Retro Learning Environment \u2014 RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles.The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining a simple unified interface. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. A more extensive paper describing our work is available on arXiv",
    "code_link": ""
  },
  "iclr2017_workshop_variationalintrinsiccontrol": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Variational Intrinsic Control",
    "authors": [
      "Karol Gregor",
      "Danilo Jimenez Rezende",
      "Daan Wierstra"
    ],
    "page_url": "https://openreview.net/forum?id=Skc-Fo4Yg",
    "pdf_url": "https://openreview.net/pdf?id=Skc-Fo4Yg",
    "published": "2017-05",
    "summary": "We introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. Both algorithms also yield a tractable and explicit empowerment measure, which is useful for empowerment maximizing agents. Furthermore, they scale well with function approximation and we demonstrate their applicability on a range of tasks.",
    "code_link": ""
  },
  "iclr2017_workshop_unsupervisedfeaturelearningforaudioanalysis": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Unsupervised Feature Learning for Audio Analysis",
    "authors": [
      "Matthias Meyer",
      "Jan Beutel",
      "Lothar Thiele"
    ],
    "page_url": "https://openreview.net/forum?id=rJeYrsEYg",
    "pdf_url": "https://openreview.net/pdf?id=rJeYrsEYg",
    "published": "2017-05",
    "summary": "Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 % better results when used with a classifier and 36 % better results when used for clustering.",
    "code_link": "https://github.com/fchollet/keras"
  },
  "iclr2017_workshop_fastadaptationingenerativemodelswithgenerativematchingnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Fast Adaptation in Generative Models with Generative Matching Networks",
    "authors": [
      "Sergey Bartunov",
      "Dmitry P. Vetrov"
    ],
    "page_url": "https://openreview.net/forum?id=r1IvyjVYl",
    "pdf_url": "https://openreview.net/pdf?id=r1IvyjVYl",
    "published": "2017-05",
    "summary": "We develop a new class of deep generative model called generative matching networks (GMNs) which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent.Our experiments on the Omniglot dataset demonstrate that GMNs can significantly improve predictive performance on the fly as more additional data is available and generate examples of previously unseen handwritten characters once only a few images of them are provided.",
    "code_link": ""
  },
  "iclr2017_workshop_efficientvariationalbayesianneuralnetworkensemblesforoutlierdetection": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Efficient variational Bayesian neural network ensembles for outlier detection",
    "authors": [
      "Nick Pawlowski",
      "Miguel Jaques",
      "Ben Glocker"
    ],
    "page_url": "https://openreview.net/forum?id=Hy-po5NFx",
    "pdf_url": "https://openreview.net/pdf?id=Hy-po5NFx",
    "published": "2017-05",
    "summary": "In this work we perform outlier detection using ensembles of neural networks obtained by variational approximation of the posterior in a Bayesian neural network setting. The variational parameters are obtained by sampling from the true posterior by gradient descent. We show our outlier detection results are comparable to those obtained using other efficient ensembling methods.",
    "code_link": "https://github.com/pawni/sgld_online_approximation"
  },
  "iclr2017_workshop_emergenceoflanguagewithmulti-agentgameslearningtocommunicatewithsequencesofsymbols": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols",
    "authors": [
      "Serhii Havrylov",
      "Ivan Titov"
    ],
    "page_url": "https://openreview.net/forum?id=SkaxnKEYg",
    "pdf_url": "https://openreview.net/pdf?id=SkaxnKEYg",
    "published": "2017-05",
    "summary": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.",
    "code_link": ""
  },
  "iclr2017_workshop_asmoothoptimisationperspectiveontrainingfeedforwardneuralnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "A Smooth Optimisation Perspective on Training Feedforward Neural Networks",
    "authors": [
      "Hao Shen"
    ],
    "page_url": "https://openreview.net/forum?id=S1nFVFNYx",
    "pdf_url": "https://openreview.net/pdf?id=S1nFVFNYx",
    "published": "2017-05",
    "summary": "We present a smooth optimisation perspective on training multilayer Feedforward Neural Networks (FNNs) in the supervised learning setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local optima of the cost function. By studying the Hessian structure of the cost function at the global minima, we develop an approximate Newton FNN algorithm, which demonstrates promising convergence properties.",
    "code_link": ""
  },
  "iclr2017_workshop_syntheticgradientmethodswithvirtualforward-backwardnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Synthetic Gradient Methods with Virtual Forward-Backward Networks",
    "authors": [
      "Takeru Miyato",
      "Daisuke Okanohara",
      "Shin-ichi Maeda",
      "Masanori Koyama"
    ],
    "page_url": "https://openreview.net/forum?id=H1hLmF4Fx",
    "pdf_url": "https://openreview.net/pdf?id=H1hLmF4Fx",
    "published": "2017-05",
    "summary": "The concept of synthetic gradient introduced by Jaderberg et al. (2016) provides an avant-garde framework for asynchronous learning of neural network. Their model, however, has a weakness in its construction, because the structure of their synthetic gradient has little relation to the objective function of the target task. In this paper we introduce virtual forward-backward networks (VFBN).VFBN is a model that produces synthetic gradient whose structure is analogous to the actual gradient of the objective function. VFBN is the first of its kind that succeeds in decoupling deep networks like ResNet-110 (He et al., 2016) without compromising its performance.",
    "code_link": "https://github.com/mitmul/chainer-cifar10"
  },
  "iclr2017_workshop_thepreimageofrectifiernetworkactivities": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "THE PREIMAGE OF RECTIFIER NETWORK ACTIVITIES",
    "authors": [
      "Stefan Carlsson",
      "Hossein Azizpour",
      "Ali Razavian",
      "Josephine Sullivan",
      "Kevin Smith"
    ],
    "page_url": "https://openreview.net/forum?id=rk7YG_4Yg",
    "pdf_url": "https://openreview.net/pdf?id=rk7YG_4Yg",
    "published": "2017-05",
    "summary": "We give a procedure for explicitely computing the complete preimage of activities of a layer in a rectifier network with fully connected layers, from knowledge of the weights in the network. The most general characterization of preimages is as piecewise linear manifolds in the input space with possibly multiple branches. This work therefore complements previous demonstrations of preimages obtained by heuristic optimization and regularization algorithms Mahendran & Vedaldi (2015; 2016) We are presently empirically evaluating the procedure and it\u2019s ability to extract complete preimages as well as the general structure of preimage manifolds. ICLR 2017 CONFRENCE TRACK SUBMISSION: https://openreview.net/forum?id=HJcLcw9xg&noteId=HJcLcw9xg",
    "code_link": ""
  },
  "iclr2017_workshop_jointmultimodallearningwithdeepgenerativemodels": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Joint Multimodal Learning with Deep Generative Models",
    "authors": [
      "Masahiro Suzuki",
      "Kotaro Nakayama",
      "Yutaka Matsuo"
    ],
    "page_url": "https://openreview.net/forum?id=BkL7bONFe",
    "pdf_url": "https://openreview.net/pdf?id=BkL7bONFe",
    "published": "2017-05",
    "summary": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models. However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that JMVAE can generate multiple modalities bi-directionally.",
    "code_link": ""
  },
  "iclr2017_workshop_trainingtripletnetworkswithgan": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Training Triplet Networks with GAN",
    "authors": [
      "Maciej Zieba",
      "Lei Wang"
    ],
    "page_url": "https://openreview.net/forum?id=BJiMcB4Kl",
    "pdf_url": "https://openreview.net/pdf?id=BJiMcB4Kl",
    "published": "2017-05",
    "summary": "Triplet networks are widely used models that are characterized by good performance in classification and retrieval tasks. In this work we propose to train a triplet network by putting it as the discriminator in Generative Adversarial Nets (GANs). We make use of the good capability of representation learning of the discriminator to increase the predictive quality of the model. We evaluated our approach on Cifar10 and MNIST datasets and observed significant improvement on the classification performance using the simple k-nn method. ",
    "code_link": ""
  },
  "iclr2017_workshop_neu0": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Neu0",
    "authors": [
      "Karthik R",
      "Aman Achpal",
      "Vinayshekhar BK",
      "Anantharaman Palacode Narayana Iyer",
      "Channa Bankapur"
    ],
    "page_url": "https://openreview.net/forum?id=SkbJrSVFe",
    "pdf_url": "https://openreview.net/pdf?id=SkbJrSVFe",
    "published": "2017-05",
    "summary": "MU0 is a deterministic computer that can store data in memory, manipulate it using programs, enabling decision making. Neu0 is a neural computational core modeled around the same principles. We create an ensemble of Neural Networks capable of executing ARM code, and discuss generalizations of our framework. We showcase the advantage of our technique by correctly executing malformed instructions, and discuss efficient memory management techniques.",
    "code_link": "https://github.com/Neu0/neu0"
  },
  "iclr2017_workshop_dancedanceconvolution": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Dance Dance Convolution",
    "authors": [
      "Chris Donahue",
      "Zachary C. Lipton",
      "Julian McAuley"
    ],
    "page_url": "https://openreview.net/forum?id=HyzsgBEtg",
    "pdf_url": "https://openreview.net/pdf?id=HyzsgBEtg",
    "published": "2017-05",
    "summary": "Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks:deciding when to place steps and deciding which steps to select. We demonstrate deep learning solutions for both tasks and establish strong benchmarks for future work.",
    "code_link": ""
  },
  "iclr2017_workshop_pl@ntnetappintheeraofdeeplearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Pl@ntNet app in the era of deep learning",
    "authors": [
      "Antoine Affouard",
      "Herv\u00e9 Goeau",
      "Pierre Bonnet",
      "Jean-Christophe Lombardo",
      "Alexis Joly"
    ],
    "page_url": "https://openreview.net/forum?id=HJVJpENFg",
    "pdf_url": "https://openreview.net/pdf?id=HJVJpENFg",
    "published": "2017-05",
    "summary": "Pl@ntNet is a large-scale participatory platform and information system dedicated to the production of botanical data through image-based plant identification. In June 2015, Pl@ntNet mobile front-ends moved from classical hand-crafted visual features to deep-learning based image representations. This paper gives an overview of today's Pl@ntNet architecture and discusses how the introduction of convolutional neural networks did improve the whole workflow along the years.",
    "code_link": ""
  },
  "iclr2017_workshop_onlinemulti-tasklearningusingactivesampling": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Online Multi-Task Learning Using Active Sampling",
    "authors": [
      "Sahil Sharma",
      "Balaraman Ravindran"
    ],
    "page_url": "https://openreview.net/forum?id=H1XLbXEtg",
    "pdf_url": "https://openreview.net/pdf?id=H1XLbXEtg",
    "published": "2017-05",
    "summary": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learningforgoal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which requireextensive training. We propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online oractive learning setup without the need for expert supervision. ",
    "code_link": ""
  },
  "iclr2017_workshop_onimprovingthenumericalstabilityofwinogradconvolutions": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "On Improving the Numerical Stability of Winograd Convolutions",
    "authors": [
      "Kevin Vincent",
      "Kevin Stephano",
      "Michael Frumkin",
      "Boris Ginsburg",
      "Julien Demouth"
    ],
    "page_url": "https://openreview.net/forum?id=H1ZaRZVKg",
    "pdf_url": "https://openreview.net/pdf?id=H1ZaRZVKg",
    "published": "2017-05",
    "summary": "Deep convolutional neural networks rely on heavily optimized convolution algorithms. Winograd convolutions provide an efficient approach to performing such convolutions. Using larger Winograd convolution tiles, the convolution will become more efficient but less numerically accurate. Here we provide some approaches to mitigating this numerical inaccuracy. We will exemplify these approaches by working on a tile much larger than any previously documented: F(9x9, 5x5). Using these approaches, we will show that such a tile can be used to train modern networks and provide performance benefits.",
    "code_link": ""
  },
  "iclr2017_workshop_fastgenerationforconvolutionalautoregressivemodels": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Fast Generation for Convolutional Autoregressive Models",
    "authors": [
      "Prajit Ramachandran",
      "Tom Le Paine",
      "Pooya Khorrami",
      "Mohammad Babaeizadeh",
      "Shiyu Chang",
      "Yang Zhang",
      "Mark A. Hasegawa-Johnson",
      "Roy H. Campbell",
      "Thomas S. Huang"
    ],
    "page_url": "https://openreview.net/forum?id=rkdF0ZNKl",
    "pdf_url": "https://openreview.net/pdf?id=rkdF0ZNKl",
    "published": "2017-05",
    "summary": "Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a naive fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to 21x and 183x speedups respectively. ",
    "code_link": ""
  },
  "iclr2017_workshop_factorizationtricksforlstmnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Factorization tricks for LSTM networks",
    "authors": [
      "Oleksii Kuchaiev",
      "Boris Ginsburg"
    ],
    "page_url": "https://openreview.net/forum?id=ByxWXyNFg",
    "pdf_url": "https://openreview.net/pdf?id=ByxWXyNFg",
    "published": "2017-05",
    "summary": "Large Long Short-Term Memory (LSTM) networks have tens of millions of parameters and they are very expensive to train. We present two simple ways of reducing the number of parameters in LSTM network: the first one is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29.",
    "code_link": "https://github.com/okuchaiev/f-lm"
  },
  "iclr2017_workshop_shake-shakeregularizationof3-branchresidualnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Shake-Shake regularization of 3-branch residual networks",
    "authors": [
      "Xavier Gastaldi"
    ],
    "page_url": "https://openreview.net/forum?id=HkO-PCmYl",
    "pdf_url": "https://openreview.net/pdf?id=HkO-PCmYl",
    "published": "2017-05",
    "summary": "The method introduced in this paper aims at helping computer vision practitioners faced with an overfit problem. The idea is to replace, in a 3-branch ResNet, the standard summation of residual branches by a stochastic affine combination. The largest tested model improves on the best single shot published result on CIFAR-10 by reaching 2.86% test error. Code is available at https://github.com/xgastaldi/shake-shake",
    "code_link": "https://github.com/facebook/fb.resnet.torch"
  },
  "iclr2017_workshop_tracenormregulariseddeepmulti-tasklearning": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Trace Norm Regularised Deep Multi-Task Learning",
    "authors": [
      "Yongxin Yang",
      "Timothy M. Hospedales"
    ],
    "page_url": "https://openreview.net/forum?id=rknkNR7Ke",
    "pdf_url": "https://openreview.net/pdf?id=rknkNR7Ke",
    "published": "2017-05",
    "summary": "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way. ",
    "code_link": "https://github.com/wOOL/TNRDMTL"
  },
  "iclr2017_workshop_neurogenesis-inspireddictionarylearningonlinemodeladaptioninachangingworld": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD",
    "authors": [
      "Sahil Garg",
      "Irina Rish",
      "Guillermo Cecchi",
      "Aurelie Lozano"
    ],
    "page_url": "https://openreview.net/forum?id=S1dJ1smFg",
    "pdf_url": "https://openreview.net/pdf?id=S1dJ1smFg",
    "published": "2017-05",
    "summary": "We address the problem of online model adaptation when learning representations from non-stationary data streams. For now, we focus on single hidden-layer sparse linear autoencoders (i.e. sparse dictionary learning), although in the future, the proposed approach can be extended naturally to general multi-layer autoencoders and supervised models. We propose a simple but effective online model-selection, based on alternating-minimization scheme, which involves \u201cbirth\u201d (addition of new elements) and \u201cdeath\u201d (removal, via l1/l2 group sparsity) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on both real-life and synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of Mairal et al. (2009) in the presence of non-stationary data, especially when dictionaries are sparse. ",
    "code_link": "https://github.com/sgarg87/neurogenesis_inspired_dictionary_learning"
  },
  "iclr2017_workshop_delvingintoadversarialattacksondeeppolicies": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Delving into adversarial attacks on deep policies",
    "authors": [
      "Jernej Kos",
      "Dawn Song"
    ],
    "page_url": "https://openreview.net/forum?id=BJcib5mFe",
    "pdf_url": "https://openreview.net/pdf?id=BJcib5mFe",
    "published": "2017-05",
    "summary": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
    "code_link": ""
  },
  "iclr2017_workshop_thehigh-dimensionalgeometryofbinaryneuralnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "The High-Dimensional Geometry of Binary Neural Networks",
    "authors": [
      "Alexander G. Anderson",
      "Cory P. Berg"
    ],
    "page_url": "https://openreview.net/forum?id=SkvQFOmtg",
    "pdf_url": "https://openreview.net/pdf?id=SkvQFOmtg",
    "published": "2017-05",
    "summary": "Traditionally, researchers thought that high-precision weights were crucial for training neural networks with gradient descent. However, recent research has obtained a finer understanding of the role of precision in neural network weights. One can train a NN with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the Courbariaux, Hubara et al. (2016) method work because of the high-dimensional geometry of binary vectors. In particular, the continuous vectors that extract out features in these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the geometry of high-dimensional binary vectors. Our theory serves as a foundation for understanding not only BNNs but networks that make use of low precision weights and activations. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.",
    "code_link": ""
  },
  "iclr2017_workshop_revisitingbatchnormalizationforpracticaldomainadaptation": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Revisiting Batch Normalization For Practical Domain Adaptation",
    "authors": [
      "Yanghao Li",
      "Naiyan Wang",
      "Jianping Shi",
      "Jiaying Liu",
      "Xiaodi Hou"
    ],
    "page_url": "https://openreview.net/forum?id=Hk6dkJQFx",
    "pdf_url": "https://openreview.net/pdf?id=Hk6dkJQFx",
    "published": "2017-05",
    "summary": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods.Combining AdaBN with existing domain adaptation treatments may further improve model performance.",
    "code_link": ""
  },
  "iclr2017_workshop_couplingdistributedandsymbolicexecutionfornaturallanguagequeries": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries",
    "authors": [
      "Lili Mou",
      "Zhengdong Lu",
      "Hang Li",
      "Zhi Jin"
    ],
    "page_url": "https://openreview.net/forum?id=Hkx-gCfYl",
    "pdf_url": "https://openreview.net/pdf?id=Hkx-gCfYl",
    "published": "2017-05",
    "summary": "In this paper, we propose to combine neural execution and symbolic execution to query a table with natural languages. Our approach makes use the differentiability of neural networks and transfers (imperfect) knowledge to the symbolic executor before reinforcement learning. Experiments show our approach achieves high learning efficiency, high execution efficiency, high interpretability, as well as high performance.",
    "code_link": ""
  },
  "iclr2017_workshop_transferringknowledgetosmallernetworkwithclass-distanceloss": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss",
    "authors": [
      "Seung Wook Kim",
      "Hyo-Eun Kim"
    ],
    "page_url": "https://openreview.net/forum?id=ByXrfaGFe",
    "pdf_url": "https://openreview.net/pdf?id=ByXrfaGFe",
    "published": "2017-05",
    "summary": "Training a network with small capacity that can perform as well as a larger capacity network is an important problem that needs to be solved in real life applications which require fast inference time and small memory requirement. Previous approaches that transfer knowledge from a bigger network to a smaller network show little benefit when applied to state-of-the-art convolutional neural network architectures such as Residual Network trained with batch normalization. We propose class-distance loss that helps teacher networks to form densely clustered vector space to make it easy for the student network to learn from it. We show that a small network with half the size of the original network trained with the proposed strategy can perform close to the original network on CIFAR-10 dataset.",
    "code_link": ""
  },
  "iclr2017_workshop_regularizingneuralnetworksbypenalizingconfidentoutputdistributions": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Regularizing Neural Networks by Penalizing Confident Output Distributions",
    "authors": [
      "Gabriel Pereyra",
      "George Tucker",
      "Jan Chorowski",
      "Lukasz Kaiser",
      "Geoffrey Hinton"
    ],
    "page_url": "https://openreview.net/forum?id=HyhbYrGYe",
    "pdf_url": "https://openreview.net/pdf?id=HyhbYrGYe",
    "published": "2017-05",
    "summary": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.",
    "code_link": ""
  },
  "iclr2017_workshop_preciserecoveryoflatentvectorsfromgenerativeadversarialnetworks": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Precise Recovery of Latent Vectors from Generative Adversarial Networks",
    "authors": [
      "Zachary C. Lipton",
      "Subarna Tripathi"
    ],
    "page_url": "https://openreview.net/forum?id=HJC88BzFl",
    "pdf_url": "https://openreview.net/pdf?id=HJC88BzFl",
    "published": "2017-05",
    "summary": "Generative adversarial networks (GANs) transform latent vectors into visually plausible images.It is generally thought that the original GAN formulation gives no out-of-the-box method to reverse the mapping, projecting images back into latent space. We introduce a simple, gradient-based technique called stochastic clipping. In experiments, for images generated by the GAN, we exactly recover their latent vector pre-images 100% of the time. Additional experiments demonstrate that this method is robust to noise. Finally, we show that even for unseen images, our method appears to recover unique encodings.",
    "code_link": ""
  },
  "iclr2017_workshop_arbitrarystyletransferinreal-timewithadaptiveinstancenormalization": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
    "authors": [
      "Xun Huang",
      "Serge Belongie"
    ],
    "page_url": "https://openreview.net/forum?id=B1fUVMzKg",
    "pdf_url": "https://openreview.net/pdf?id=B1fUVMzKg",
    "published": "2017-05",
    "summary": "Gatys et al. (2015) recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called \\emph{style transfer}. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations withfeed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization~(AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles.",
    "code_link": ""
  },
  "iclr2017_workshop_adversarialexamplesforsemanticimagesegmentation": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Adversarial Examples for Semantic Image Segmentation",
    "authors": [
      "Volker Fischer",
      "Mummadi Chaithanya Kumar",
      "Jan Hendrik Metzen",
      "Thomas Brox"
    ],
    "page_url": "https://openreview.net/forum?id=S1SED1MYe",
    "pdf_url": "https://openreview.net/pdf?id=S1SED1MYe",
    "published": "2017-05",
    "summary": "Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.",
    "code_link": ""
  },
  "iclr2017_workshop_compactembeddingofbinary-codedinputsandoutputsusingbloomfilters": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters",
    "authors": [
      "Joan Serr\u00e0",
      "Alexandros Karatzoglou"
    ],
    "page_url": "https://openreview.net/forum?id=rySCp-1Yg",
    "pdf_url": "https://openreview.net/pdf?id=rySCp-1Yg",
    "published": "2017-05",
    "summary": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results.",
    "code_link": ""
  },
  "iclr2017_workshop_rebarlow-variance,unbiasedgradientestimatesfordiscretelatentvariablemodels": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models",
    "authors": [
      "George Tucker",
      "Andriy Mnih",
      "Chris J. Maddison",
      "Jascha Sohl-Dickstein"
    ],
    "page_url": "https://openreview.net/forum?id=ryBDyehOl",
    "pdf_url": "https://openreview.net/pdf?id=ryBDyehOl",
    "published": "2017-05",
    "summary": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, unbiased gradient estimates. We present encouraging preliminary results on a toy problem and on learning sigmoid belief networks. ",
    "code_link": ""
  },
  "iclr2017_workshop_variationalreferencepriors": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Variational Reference Priors",
    "authors": [
      "Eric Nalisnick",
      "Padhraic Smyth"
    ],
    "page_url": "https://openreview.net/forum?id=rJnjwsYde",
    "pdf_url": "https://openreview.net/pdf?id=rJnjwsYde",
    "published": "2017-05",
    "summary": "In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.However, informative priors are often costly to elicit, and in consequence, flat priors are chosen with the hopes that they are reasonably uninformative.Yet, objective priors such as the Jeffreys and Reference would often be preferred over flat priors if deriving them was generally tractable.We overcome this problem by proposing a black-box learning algorithm for Reference prior approximations.We derive a lower bound on the mutual information between data and parameters and describe how its optimization can be made derivation-free and scalable via differentiable Monte Carlo expectations.We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's Reference prior. ",
    "code_link": ""
  },
  "iclr2017_workshop_earlymethodsfordetectingadversarialimages": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Early Methods for Detecting Adversarial Images",
    "authors": [
      "Dan Hendrycks",
      "Kevin Gimpel"
    ],
    "page_url": "https://openreview.net/forum?id=B1dexpDug",
    "pdf_url": "https://openreview.net/pdf?id=B1dexpDug",
    "published": "2017-05",
    "summary": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
    "code_link": "https://github.com/hendrycks/fooling"
  },
  "iclr2017_workshop_towardsalphachemchemicalsynthesisplanningwithtreesearchanddeepneuralnetworkpolicies": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "Towards AlphaChem: Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies",
    "authors": [
      "Marwin Segler",
      "Mike Preuss",
      "Mark P. Waller"
    ],
    "page_url": "https://openreview.net/forum?id=SJ5JPtg_x",
    "pdf_url": "https://openreview.net/pdf?id=SJ5JPtg_x",
    "published": "2017-05",
    "summary": "Retrosynthesis is a technique to plan the chemical synthesis of organic molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a search tree is built by analysing molecules recursively and dissecting them into simpler molecular building blocks until one obtains a set of known building blocks. The search space is intractably large, and it is difficult to determine the value of retrosynthetic positions. Here, we propose to model retrosynthesis as a Markov Decision Process. In combination with a Deep Neural Network policy trained on 5.5 million reactions, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In exploratory studies, we demonstrate that MCTS with neural network policies outperforms the traditionally used best-first search with hand-coded heuristics.",
    "code_link": ""
  },
  "iclr2017_workshop_commaievaluatingthefirststepstowardsausefulgeneralai": {
    "conf_id": "ICLR2017",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2017_workshop",
    "title": "CommAI: Evaluating the first steps towards a useful general AI",
    "authors": [
      "Marco Baroni",
      "Armand Joulin",
      "Allan Jabri",
      "Germ\u00e0n Kruszewski",
      "Angeliki Lazaridou",
      "Klemen Simonic",
      "Tomas Mikolov"
    ],
    "page_url": "https://openreview.net/forum?id=Syh_o0pPx",
    "pdf_url": "https://openreview.net/pdf?id=Syh_o0pPx",
    "published": "2017-05",
    "summary": "With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.",
    "code_link": ""
  }
}