{
  "wacv2021_hbu_pose-basedsignlanguagerecognitionusinggcnandbert": {
    "conf_id": "WACV2021",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Human Behavior Understanding",
    "title": "Pose-Based Sign Language Recognition Using GCN and BERT",
    "authors": [
      "Anirudh Tunga",
      "Sai Vidyaranya Nuthalapati",
      "Juan Wachs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/html/Tunga_Pose-Based_Sign_Language_Recognition_Using_GCN_and_BERT_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/papers/Tunga_Pose-Based_Sign_Language_Recognition_Using_GCN_and_BERT_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Sign language recognition (SLR) plays a crucial role in bridging the communication gap between the hearing and vocally impaired community and the rest of the society. Word-level sign language recognition (WSLR) is the first important step towards understanding and interpreting sign language. However, recognizing signs from videos is a challenging task as the meaning of a word depends on a combination of subtle body motions, hand configurations, and other movements. Recent pose-based architectures for WSLR either model both the spatial and temporal dependencies among the poses in different frames simultaneously or only model the temporal information without fully utilizing the spatial information. We tackle the problem of WSLR using a novel pose-based approach, which captures spatial and temporal information separately and performs late fusion. Our proposed architecture explicitly captures the spatial interactions in the video using a Graph Convolutional Network (GCN). The temporal dependencies between the frames are captured using Bidirectional Encoder Representations from Transformers (BERT). Experimental results on WLASL, a standard word-level sign language recognition dataset show that our model significantly outperforms the state-of-the-art on pose-based methods by achieving an improvement in the prediction accuracy by up to 5%. "
  },
  "wacv2021_hbu_context-awarepersonalityinferenceindyadicscenariosintroducingtheudivadataset": {
    "conf_id": "WACV2021",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Human Behavior Understanding",
    "title": "Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset",
    "authors": [
      "Cristina Palmero",
      "Javier Selva",
      "Sorina Smeureanu",
      "Julio C. S. Jacques Junior",
      "Albert Clapes",
      "Alexa Mosegui",
      "Zejian Zhang",
      "David Gallardo",
      "Georgina Guilera",
      "David Leiva",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/html/Palmero_Context-Aware_Personality_Inference_in_Dyadic_Scenarios_Introducing_the_UDIVA_Dataset_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/papers/Palmero_Context-Aware_Personality_Inference_in_Dyadic_Scenarios_Introducing_the_UDIVA_Dataset_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " This paper introduces UDIVA, a new non-acted dataset of face-to-face dyadic interactions, where interlocutors perform competitive and collaborative tasks with different behavior elicitation and cognitive workload. The dataset consists of 90.5 hours of dyadic interactions among 147 participants distributed in 188 sessions, recorded using multiple audiovisual and physiological sensors. Currently, it includes sociodemographic, self- and peer-reported personality, internal state, and relationship profiling from participants. As an initial analysis on UDIVA, we propose a transformer-based method for self-reported personality inference in dyadic scenarios, which uses audiovisual data and different sources of context from both interlocutors to regress a target person's personality traits. Preliminary results from an incremental study show consistent improvements when using all available context information. "
  },
  "wacv2021_hbu_personperceptionbiasesexposedrevisitingthefirstimpressionsdataset": {
    "conf_id": "WACV2021",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Human Behavior Understanding",
    "title": "Person Perception Biases Exposed: Revisiting the First Impressions Dataset",
    "authors": [
      "Julio C. S. Jacques Junior",
      "Agata Lapedriza",
      "Cristina Palmero",
      "Xavier Baro",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/html/Jacques_Person_Perception_Biases_Exposed_Revisiting_the_First_Impressions_Dataset_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/papers/Jacques_Person_Perception_Biases_Exposed_Revisiting_the_First_Impressions_Dataset_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " This work revisits the ChaLearn First Impressions database, annotated for personality perception using pairwise comparisons via crowdsourcing. We analyse for the first time the original pairwise annotations, and reveal existing person perception biases associated to perceived attributes like gender, ethnicity, age and face attractiveness. We show how person perception bias can influence data labelling of a subjective task, which has received little attention from the computer vision and machine learning communities by now. We further show that the mechanism used to convert pairwise annotations to continuous values may magnify the biases if no special treatment is considered. The findings of this study are relevant for the computer vision community that is still creating new datasets on subjective tasks, and using them for practical applications, ignoring these perceptual biases. "
  },
  "wacv2021_hbu_geeksandguestsestimatingplayerslevelofexperiencefromboardgamebehaviors": {
    "conf_id": "WACV2021",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Human Behavior Understanding",
    "title": "Geeks and Guests: Estimating Player's Level of Experience From Board Game Behaviors",
    "authors": [
      "Feyisayo Olalere",
      "Metehan Doyran",
      "Ronald Poppe",
      "Albert Ali Salah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/html/Olalere_Geeks_and_Guests_Estimating_Players_Level_of_Experience_From_Board_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/HBU/papers/Olalere_Geeks_and_Guests_Estimating_Players_Level_of_Experience_From_Board_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Board games have become promising tools for observing and studying social behaviors in multi-person settings. While traditional methods such as self-report questionnaires are used to analyze game-induced behaviors, there is a growing desire to automate such analyses. In this paper, we focus on estimating the levels of board game experience by analyzing a player's confidence and anxiety from visual cues. We use a board game setting to induce relevant interactions. We investigate facial expressions in the interactions between players during such critical game events. For our analysis, we annotated the critical game events in a multiplayer cooperative board game, using the publicly available MUMBAI board game corpus. Using off-the-shelf tools, we encoded facial behavior in dyadic interactions and built classifiers to predict each player's level of experience. Our results show that considering the experience level of both parties involved in the interaction simultaneously improves the prediction results. "
  },
  "wacv2021_hadcv_per-vispersonretrievalinvideosurveillanceusingsemanticdescription": {
    "conf_id": "WACV2021",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "PeR-ViS: Person Retrieval in Video Surveillance Using Semantic Description",
    "authors": [
      "Parshwa Shah",
      "Arpit Garg",
      "Vandit Gajjar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/HADCV/html/Shah_PeR-ViS_Person_Retrieval_in_Video_Surveillance_Using_Semantic_Description_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/HADCV/papers/Shah_PeR-ViS_Person_Retrieval_in_Video_Surveillance_Using_Semantic_Description_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " A person is usually characterized by descriptors like age, gender, height, cloth type, pattern, color, etc. Such descriptors are known as attributes and/or soft-biometrics. They link the semantic gap between a person's description and retrieval in video surveillance. Retrieving a specific person with the query of semantic description has an important application in video surveillance. Using computer vision to fully automate the person retrieval task has been gathering interest within the research community. However, the Current, trend mainly focuses on retrieving persons with image-based queries, which have major limitations for practical usage. Instead of using an image query, in this paper, we study the problem of person retrieval in video surveillance with a semantic description. To solve this problem, we develop a deep learning-based cascade filtering approach (PeR-ViS), which uses Mask R-CNN [14] (person detection and instance segmentation) and DenseNet-161 [16] (soft-biometric classification). On the standard person retrieval dataset of SoftBioSearch [6], we achieve 0.566 Average IoU and 0.792 %w IoU > 0.4, surpassing the current state-of-the-art by a large margin. We hope our simple, reproducible, and effective approach will help ease future research in the domain of person retrieval in video surveillance. The source code will be released after the paper is accepted for publication with baseline and pretrained weights. The source code and pretrained weights available at https://parshwa1999.github.io/PeR-ViS/. "
  },
  "wacv2021_hadcv_2020sequestereddataevaluationforknownactivitiesinextendedvideosummaryandresults": {
    "conf_id": "WACV2021",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "2020 Sequestered Data Evaluation for Known Activities in Extended Video: Summary and Results",
    "authors": [
      "Afzal Godil",
      "Yooyoung Lee",
      "Jon Fiscus",
      "Andrew Delgado",
      "Eliot Godard",
      "Baptiste Chocot",
      "Lukas Diduch",
      "Jim Golden",
      "Jesse Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/HADCV/html/Godil_2020_Sequestered_Data_Evaluation_for_Known_Activities_in_Extended_Video_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/HADCV/papers/Godil_2020_Sequestered_Data_Evaluation_for_Known_Activities_in_Extended_Video_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " This paper presents a summary and results for the ActEV'20 SDL (Activities in Extended Video Sequestered Data Leaderboard) challenge that was held under the CVPR'20 ActivityNet workshop. The primary goal of the challenge was to provide an impetus for advancing research and capabilities in the field of human activity detection in untrimmed multi-camera videos. Advancements in activity detection will help with a wide range of public safety applications. The challenge was administered by the National Institute of Standards and Technology (NIST), where anyone could submit their system which run on sequestered data with the resulting score posted to a public leaderboard. Ten teams submitted their systems for the ActEV'20 SDL competition on the Multiview Extended Video with Activities (MEVA) test set with 37 target activities. The performance metric for the leaderboard ranking is the partial, normalized Area Under the Detection Error Tradeoff (DET) curve (nAUDC). The top rank on activity detection was by UCF at 37%, followed by CMU at 39% and OPPO at 41%. "
  },
  "wacv2021_xai4b_symbolicaiforxaievaluatinglfitinductiveprogrammingforfairandexplainableautomaticrecruitment": {
    "conf_id": "WACV2021",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Symbolic AI for XAI: Evaluating LFIT Inductive Programming for Fair and Explainable Automatic Recruitment",
    "authors": [
      "Alfonso Ortega",
      "Julian Fierrez",
      "Aythami Morales",
      "Zilong Wang",
      "Tony Ribeiro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/html/Ortega_Symbolic_AI_for_XAI_Evaluating_LFIT_Inductive_Programming_for_Fair_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/papers/Ortega_Symbolic_AI_for_XAI_Evaluating_LFIT_Inductive_Programming_for_Fair_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Machine learning methods are growing in relevance for biometrics and personal information processing in domains such as forensics, e health, recruitment, and e learning. In these domains, white box (human readable) explanations of systems built on machine learning methods can become crucial. Inductive Logic Programming (ILP) is a subfield of symbolic AI aimed to automatically learn declarative theories about the process of data. Learning from Interpretation Transition (LFIT) is an ILP technique that can learn a propositional logic theory equivalent to a given black box system (under certain conditions). The present work takes a first step to a general methodology to incorporate accurate declarative explanations to classic machine learning by checking the viability of LFIT in a specific AI application scenario: fair recruitment based on an automatic tool generated with machine learning methods for ranking Curricula Vitae that incorporates soft biometric information (gender and ethnicity). We show the expressiveness of LFIT for this specific problem and propose a scheme that can be applicable to other domains. "
  },
  "wacv2021_xai4b_interpretablesecurityanalysisofcancellablebiometricsusingconstrained-optimizedsimilarity-basedattack": {
    "conf_id": "WACV2021",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Interpretable Security Analysis of Cancellable Biometrics Using Constrained-Optimized Similarity-Based Attack",
    "authors": [
      "Hanrui Wang",
      "Xingbo Dong",
      "Zhe Jin",
      "Andrew Beng Jin Teoh",
      "Massimo Tistarelli"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/html/Wang_Interpretable_Security_Analysis_of_Cancellable_Biometrics_Using_Constrained-Optimized_Similarity-Based_Attack_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/papers/Wang_Interpretable_Security_Analysis_of_Cancellable_Biometrics_Using_Constrained-Optimized_Similarity-Based_Attack_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " In cancellable biometrics (CB) schemes, template security is achieved by applying, mainly non-linear, transformations to the biometric template. The transformation is designed to preserve the template distance/similarity in the transformed domain. Despite its effectiveness, the security issues attributed to similarity preservation property of CB are underestimated. Dong et al. [BTAS'19], exploited the similarity preservation trait of CB and proposed a similarity-based attack with high successful attack rate. The similarity-based attack utilizes preimage that are generated from the protected biometric template for impersonation and perform cross matching. In this paper, we propose a constrained optimization similarity-based attack (CSA), which is improved upon Dong's genetic algorithm enabled similarity-based attack (GASA). The CSA applies algorithm-specific equality or inequality relations as constraints, to optimize preimage generation. We interpret the effectiveness of CSA from the supervised learning perspective. We identify such constraints then conduct extensive experiments to demonstrate CSA against CB with LFW face dataset. The results suggest that CSA is effective to breach IoM hashing and BioHashing security, and outperforms GASA significantly. Inferring from the above results, we further remark that, other than IoM and BioHashing, CSA is critical to other CB schemes as far as the constraints can be formulated. Furthermore, we reveal the correlation of hash code size and the attack performance of CSA. "
  },
  "wacv2021_xai4b_anexplainableattention-guidedirispresentationattackdetector": {
    "conf_id": "WACV2021",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "An Explainable Attention-Guided Iris Presentation Attack Detector",
    "authors": [
      "Cunjian Chen",
      "Arun Ross"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/html/Chen_An_Explainable_Attention-Guided_Iris_Presentation_Attack_Detector_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/papers/Chen_An_Explainable_Attention-Guided_Iris_Presentation_Attack_Detector_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Convolutional Neural Networks (CNNs) are being increasingly used to address the problem of iris presentation attack detection. In this work, we propose an explainable attention-guided iris presentation attack detector (AG-PAD) to augment CNNs with attention mechanisms and to provide visual explanations of model predictions. Two types of attention modules are independently placed on top of the last convolutional layer of the backbone network. Specifically, the channel attention module is used to model the inter-channel relationship between features, while the position attention module is used to model inter-spatial relationship between features. An element-wise sum is employed to fuse these two attention modules. Further, a novel hierarchical attention mechanism is introduced. Experiments involving both a JHU-APL proprietary dataset and the benchmark LivDet-Iris-2017 dataset suggest that the proposed method achieves promising detection results while explaining occurrences of salient regions for discriminative feature learning. To the best of our knowledge, this is the first work that exploits the use of attention mechanisms in iris presentation attack detection. "
  },
  "wacv2021_xai4b_focusedlrpexplainableaiforfacemorphingattackdetection": {
    "conf_id": "WACV2021",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Focused LRP: Explainable AI for Face Morphing Attack Detection",
    "authors": [
      "Clemens Seibold",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/html/Seibold_Focused_LRP_Explainable_AI_for_Face_Morphing_Attack_Detection_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/papers/Seibold_Focused_LRP_Explainable_AI_for_Face_Morphing_Attack_Detection_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " The task of detecting morphed face images has become highly relevant in recent years to ensure the security of automatic verification systems based on facial images, e.g. automated border control gates. Detection methods based on Deep Neural Networks (DNN) have been shown to be very suitable to this end. However, they do not provide transparency in the decision making and it is not clear how they distinguish between genuine and morphed face images. This is particularly relevant for systems intended to assist a human operator, who should be able to understand the reasoning. In this paper, we tackle this problem and present Focused Layer-wise Relevance Propagation (FLRP). This framework explains to a human inspector on a precise pixel level, which image regions are used by a Deep Neural Network to distinguish between a genuine and a morphed face image. Additionally, we propose another framework to objectively analyze the quality of our method and compare FLRP to other DNN interpretability methods. This evaluation framework is based on removing detected artifacts and analyzing the influence of these changes on the decision of the DNN. Especially, if the DNN is uncertain in its decision or even incorrect, FLRP performs much better in highlighting visible artifacts compared to other methods. "
  },
  "wacv2021_xai4b_explainablefingerprintroisegmentationusingmontecarlodropout": {
    "conf_id": "WACV2021",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Explainable Fingerprint ROI Segmentation Using Monte Carlo Dropout",
    "authors": [
      "Indu Joshi",
      "Riya Kothari",
      "Ayush Utkarsh",
      "Vinod K. Kurmi",
      "Antitza Dantcheva",
      "Sumantra Dutta Roy",
      "Prem Kumar Kalra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/html/Joshi_Explainable_Fingerprint_ROI_Segmentation_Using_Monte_Carlo_Dropout_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/XAI4B/papers/Joshi_Explainable_Fingerprint_ROI_Segmentation_Using_Monte_Carlo_Dropout_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " A fingerprint ROI segmentation module is one of the most crucial component in the fingerprint pre-processing pipeline. It separates the foreground fingerprint and background region due to which feature extraction and matching is restricted to ROI instead of entire fingerprint image. However, state-of-the-art segmentation algorithms act like a black box and do not indicate model confidence. In this direction, we propose an explainable fingerprint ROI segmentation model which indicates the pixels on which the model is uncertain. Towards this, we benchmark four state-of-the-art models for semantic segmentation on fingerprint ROI segmentation. Furthermore, we demonstrate the effectiveness of model uncertainty as an attention mechanism to improve the segmentation performance of the best performing model. Experiments on publicly available Fingerprint Verification Challenge (FVC) databases showcase the effectiveness of the proposed model. "
  },
  "wacv2021_avv_reliabilityofgangenerateddatatotrainandvalidateperceptionsystemsforautonomousvehicles": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Reliability of GAN Generated Data to Train and Validate Perception Systems for Autonomous Vehicles",
    "authors": [
      "Weihuang Xu",
      "Nasim Souly",
      "Pratik Prabhanjan Brahma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Xu_Reliability_of_GAN_Generated_Data_to_Train_and_Validate_Perception_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Xu_Reliability_of_GAN_Generated_Data_to_Train_and_Validate_Perception_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Autonomous systems deployed in the real world have to deal with potential problem causing situations that they have never seen during their training phases. Due to the long tail nature of events, it is difficult to collect large amount of data for such corner cases. While simulation is one plausible solution, recent developments in the field of Generative Adversarial Networks (GANs) have shown how they can be used to generate and augment realistic data without exhibiting a domain shift from actual real data. In this manuscript, we empirically analyze and propose novel solutions for the trust that we can place on GAN generated data for training and validation of vision based perception modules like object detection and scenario classification. "
  },
  "wacv2021_avv_driveguardrobustificationofautomateddrivingsystemswithdeepspatio-temporalconvolutionalautoencoder": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "DriveGuard: Robustification of Automated Driving Systems With Deep Spatio-Temporal Convolutional Autoencoder",
    "authors": [
      "Andreas Papachristodoulou",
      "Christos Kyrkou",
      "Theocharis Theocharides"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Papachristodoulou_DriveGuard_Robustification_of_Automated_Driving_Systems_With_Deep_Spatio-Temporal_Convolutional_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Papachristodoulou_DriveGuard_Robustification_of_Automated_Driving_Systems_With_Deep_Spatio-Temporal_Convolutional_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Autonomous vehicles increasingly rely on cameras to provide the input for perception and scene understanding and the ability of these models to classify their environment and objects, under adverse conditions and image noise is crucial. When the input is, either unintentionally or through targeted attacks, deteriorated, the reliability of autonomous vehicle is compromised. In order to mitigate such phenomena, we propose DriveGuard, a lightweight spatio-temporal autoencoder, as a solution to robustify the image segmentation process for autonomous vehicles. By first processing camera images with DriveGuard, we offer a more universal solution than having to re-train each perception model with noisy input. We explore the space of different autoencoder architectures and evaluate them on a diverse dataset created with real and synthetic images demonstrating that by exploiting spatio-temporal information combined with multi-component loss we significantly increase robustness against adverse image effects reaching within 5-6% of that of the original model on clean images. "
  },
  "wacv2021_avv_neuralvision-basedsemantic3dworldmodeling": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Neural Vision-Based Semantic 3D World Modeling",
    "authors": [
      "Sotirios Papadopoulos",
      "Ioannis Mademlis",
      "Ioannis Pitas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Papadopoulos_Neural_Vision-Based_Semantic_3D_World_Modeling_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Papadopoulos_Neural_Vision-Based_Semantic_3D_World_Modeling_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Scene geometry estimation and semantic segmentation using image/video data are two active machine learning/computer vision research topics. Given monocular or stereoscopic 3D images, depicted scene/object geometry in the form of depth maps can be successfully estimated, while modern Deep Neural Network (DNN) architectures can accurately predict semantic masks on an image. In several scenarios, both tasks are required at once, leading to a need for combined semantic 3D world mapping methods. In the wake of modern autonomous systems, DNNs that simultaneously handle both tasks have arisen, exploiting machine/deep learning to save up considerably on computational resources and enhance performance, as these tasks can mutually benefit from each other. A great application area is 3D road scene modeling and semantic segmentation, e.g., for an autonomous car to identify and localize in 3D space visible pavement regions (marked as \"road\") that are essential for autonomous car driving. Due to the significance of this field, this paper surveys the state-of-the-art DNN-based methods for scene geometry estimation, image semantic segmentation and joint inference of both. "
  },
  "wacv2021_avv_multi-scalevoxelclassbalancedasppforlidarpointcloudsemanticsegmentation": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Multi-Scale Voxel Class Balanced ASPP for LIDAR Pointcloud Semantic Segmentation",
    "authors": [
      "K. S. Chidanand",
      "Samir Al-stouhi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Chidanand_Multi-Scale_Voxel_Class_Balanced_ASPP_for_LIDAR_Pointcloud_Semantic_Segmentation_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Chidanand_Multi-Scale_Voxel_Class_Balanced_ASPP_for_LIDAR_Pointcloud_Semantic_Segmentation_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " This paper explores efficient techniques to improve PolarNet model performance to address the real-time semantic segmentation of LiDAR point clouds. The core framework consists of an encoder network, Atrous spatial pyramid pooling(ASPP)/Dense Atrous spatial pyramid pooling(DenseASPP) followed by a decoder network. Encoder extracts multi-scale voxel information in a top-down manner while decoder fuses multiple feature maps from various scales in a bottom-up manner. In between encoder and decoder block, an ASPP/DenseASPP block is inserted to enlarge receptive fields in a very dense manner. In contrast to PolarNet model, we use weighted cross entropy in conjunction with Lovasz-softmax loss to improve segmentation accuracy. Also this paper accelerates training mechanism of PolarNet model by incorporating learning-rate schedulers in conjunction with Adam optimizer for faster convergence with fewer epochs without degrading accuracy. Extensive experiments conducted on challenging SemanticKITTI dataset shows that our high-resolution-grid model obtains competitive state-of-art result of 60.6 mIOU @21fps whereas our low-resolution-grid model obtains 54.01 mIOU @35fps thereby balancing accuracy/speed trade-off. "
  },
  "wacv2021_avv_automaticvirtual3dcitygenerationforsyntheticdatacollection": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Automatic Virtual 3D City Generation for Synthetic Data Collection",
    "authors": [
      "Bingyu Shen",
      "Boyang Li",
      "Walter J. Scheirer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Shen_Automatic_Virtual_3D_City_Generation_for_Synthetic_Data_Collection_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Shen_Automatic_Virtual_3D_City_Generation_for_Synthetic_Data_Collection_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Computer vision has achieved superior results with the rapid development of new techniques in deep neural networks. Object detection in the wild is a core task in computer vision, and already has many successful applications in the real world. However, deep neural networks for object detection usually consist of hundreds, and sometimes even thousands, of layers. Training such networks is challenging, and training data has a fundamental impact on model performance. Because data collection and annotation are expensive and labor-intensive, lots of data augmentation methods have been proposed to generate synthetic data for neural network training. Most of those methods focus on manipulating 2D images. In contrast to that, in this paper, we leverage the realistic visual effects of 3D environments and propose a new way of generating synthetic data for computer vision tasks related to city scenes. Specifically, we describe a pipeline that can generate a 3D city model from an input of a 2D image that portrays the layout design of a city. This pipeline also takes optional parameters to further customize the output 3D city model. Using our pipeline, a virtual 3D city model with high-quality textures can be generated within seconds, and the output is an object ready to render. The model generated will assist people with limited 3D development knowledge to create high quality city scenes for different needs. As examples, we show the use of generated 3D city models as the synthetic data source for a scene text detection task and a traffic sign detection task. Both qualitative and quantitative results show that the generated virtual city is a good match to real-world data and potentially can benefit other computer vision tasks with similar contexts. "
  },
  "wacv2021_avv_weaklysupervisedmulti-objecttrackingandsegmentation": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Weakly Supervised Multi-Object Tracking and Segmentation",
    "authors": [
      "Idoia Ruiz",
      "Lorenzo Porzi",
      "Samuel Rota Bulo",
      "Peter Kontschieder",
      "Joan Serrat"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Ruiz_Weakly_Supervised_Multi-Object_Tracking_and_Segmentation_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Ruiz_Weakly_Supervised_Multi-Object_Tracking_and_Segmentation_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " We introduce the problem of weakly supervised Multi-Object Tracking and Segmentation, i.e. joint weakly supervised instance segmentation and multi-object tracking, in which we do not provide any kind of mask annotation. To address it, we design a novel synergistic training strategy by taking advantage of multi-task learning, i.e. classification and tracking tasks guide the training of the unsupervised instance segmentation. For that purpose, we extract weak foreground localization information, provided by Grad-CAM heatmaps, to generate a partial ground truth to learn from. Additionally, RGB image level information is employed to refine the mask prediction at the edges of the objects. We evaluate our method on KITTI MOTS, the most representative benchmark for this task, reducing the performance gap on the MOTSP metric between the fully supervised and weakly supervised approach to just 12% and 12.7% for cars and pedestrians, respectively. "
  },
  "wacv2021_avv_domainadaptiveknowledgedistillationfordrivingscenesemanticsegmentation": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Domain Adaptive Knowledge Distillation for Driving Scene Semantic Segmentation",
    "authors": [
      "Divya Kothandaraman",
      "Athira Nambiar",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Kothandaraman_Domain_Adaptive_Knowledge_Distillation_for_Driving_Scene_Semantic_Segmentation_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Kothandaraman_Domain_Adaptive_Knowledge_Distillation_for_Driving_Scene_Semantic_Segmentation_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Practical autonomous driving systems face two crucial challenges: memory constraints and domain gap issues. In this paper, we present a novel approach to learn domain adaptive knowledge in models with limited memory, thus bestowing the model with the ability to deal with these issues in a comprehensive manner. We term this as \"Domain Adaptive Knowledge Distillation\" and address the same in the context of unsupervised domain-adaptive semantic segmentation by proposing a multi-level distillation strategy to effectively distil knowledge at different levels. Further, we introduce a novel cross entropy loss that leverages pseudo labels from the teacher. These pseudo teacher labels play a multifaceted role towards: (i) knowledge distillation from the teacher network to the student network & (ii) serving as a proxy for the ground truth for target domain images, where the problem is completely unsupervised. We introduce four paradigms for distilling domain adaptive knowledge and carry out extensive experiments and ablation studies on real-to-real as well as synthetic-to-real scenarios. Our experiments demonstrate the profound success of our proposed method. "
  },
  "wacv2021_avv_usingsemanticinformationtoimprovegeneralizationofreinforcementlearningpoliciesforautonomousdriving": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Using Semantic Information to Improve Generalization of Reinforcement Learning Policies for Autonomous Driving",
    "authors": [
      "Florence Carton",
      "David Filliat",
      "Jaonary Rabarisoa",
      "Quoc Cuong Pham"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Carton_Using_Semantic_Information_to_Improve_Generalization_of_Reinforcement_Learning_Policies_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Carton_Using_Semantic_Information_to_Improve_Generalization_of_Reinforcement_Learning_Policies_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " The problem of generalization of reinforcement learning policies to new environments is seldom addressed but essential in practical applications. We focus on this problem in an autonomous driving context using the CARLA simulator and first show that semantic information is the key to a good generalization for this task. We then explore and compare different classical ways to improve generalization in an unseen environment without finetuning, showing that using semantic segmentation as an auxiliary task is the most efficient approach. "
  },
  "wacv2021_avv_per-framemappredictionforcontinuousperformancemonitoringofobjectdetectionduringdeployment": {
    "conf_id": "WACV2021",
    "conf_sub_id": "AVV",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Autonomous Vehicle Vision",
    "title": "Per-Frame mAP Prediction for Continuous Performance Monitoring of Object Detection During Deployment",
    "authors": [
      "Quazi Marufur Rahman",
      "Niko Sunderhauf",
      "Feras Dayoub"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/html/Rahman_Per-Frame_mAP_Prediction_for_Continuous_Performance_Monitoring_of_Object_Detection_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Rahman_Per-Frame_mAP_Prediction_for_Continuous_Performance_Monitoring_of_Object_Detection_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Performance monitoring of object detection is crucial for safety-critical applications such as autonomous vehicles that operate under varying and complex environmental conditions. Currently, object detectors are evaluated using summary metrics based on a single dataset that is assumed to be representative of all future deployment conditions. In practice, this assumption does not hold, and the performance fluctuates as a function of the deployment conditions. To address this issue, we propose an introspection approach to performance monitoring during deployment without the need for ground truth data. We do so by predicting when the per-frame mean average precision drops below a critical threshold using the detector's internal features. We quantitatively evaluate and demonstrate our method's ability to reduce risk by trading off making an incorrect decision by raising the alarm and absenting from detection. "
  },
  "wacv2021_ghb_shineonilluminatingdesignchoicesforpracticalvideo-basedvirtualclothingtry-on": {
    "conf_id": "WACV2021",
    "conf_sub_id": "GHB",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Generation of Human Behavior",
    "title": "ShineOn: Illuminating Design Choices for Practical Video-Based Virtual Clothing Try-On",
    "authors": [
      "Gaurav Kuppa",
      "Andrew Jong",
      "Xin Liu",
      "Ziwei Liu",
      "Teng-Sheng Moh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/GHB/html/Kuppa_ShineOn_Illuminating_Design_Choices_for_Practical_Video-Based_Virtual_Clothing_Try-On_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/GHB/papers/Kuppa_ShineOn_Illuminating_Design_Choices_for_Practical_Video-Based_Virtual_Clothing_Try-On_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Virtual try-on has garnered interest as a neural rendering benchmark task to evaluate complex object transfer and scene composition. Recent works in virtual clothing try-on feature a plethora of possible architectural and data representation choices. However, they present little clarity on quantifying the isolated visual effect of each choice, nor do they specify the hyperparameter details that are key to experimental reproduction. Our work, ShineOn, approaches the try-on task from a bottom-up approach and aims to shine light on the visual and quantitative effects of each experiment. We build a series of scientific experiments to isolate effective design choices in video synthesis for virtual clothing try-on. Specifically, we investigate the effect of different pose annotations, self-attention layer placement, and activation functions on the quantitative and qualitative performance of video virtual try-on. We find that DensePose annotations not only enhance face details but also decrease memory usage and training time. Next, we find that attention layers improve face and neck quality. Finally, we show that GELU and ReLU activation functions are the most effective in our experiments despite the appeal of newer activations such as Swish and Sine. We will release a well-organized code base, hyperparameters, and model checkpoints to support the reproducibility of our results. We expect our extensive experiments and code to greatly inform future design choices in video virtual try-on. Our code may be accessed at https://github.com/andrewjong/ShineOn-Virtual-Tryon. "
  },
  "wacv2021_ghb_facialexpressionneutralizationwithstoicnet": {
    "conf_id": "WACV2021",
    "conf_sub_id": "GHB",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Generation of Human Behavior",
    "title": "Facial Expression Neutralization With StoicNet",
    "authors": [
      "William Carver",
      "Ifeoma Nwogu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/GHB/html/Carver_Facial_Expression_Neutralization_With_StoicNet_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/GHB/papers/Carver_Facial_Expression_Neutralization_With_StoicNet_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " Expression neutralization is the process of synthetically altering an image of a face so as to remove any facial expression from it without changing the face's identity. Facial expression neutralization could have a variety of applications, particularly in the realms of facial recognition, in action unit analysis, or even improving the quality of identification pictures for various types of documents. Our proposed model, StoicNet, combines the robust encoding capacity of variational autoencoders, the generative power of generative adversarial networks, and the enhancing capabilities of super resolution networks with a learned encoding transformation to achieve compelling expression neutralization, while preserving the identity of the input face. Objective experiments demonstrate that StoicNet successfully generates realistic, identity-preserved faces with neutral expressions, regardless of the emotion or expression intensity of the input face. "
  },
  "wacv2021_ghb_alog-likelihoodregularizedkldivergenceforvideopredictionwitha3dconvolutionalvariationalrecurrentnetwork": {
    "conf_id": "WACV2021",
    "conf_sub_id": "GHB",
    "is_workshop": true,
    "conf_name": "WACV2021_workshops - Generation of Human Behavior",
    "title": "A Log-Likelihood Regularized KL Divergence for Video Prediction With a 3D Convolutional Variational Recurrent Network",
    "authors": [
      "Haziq Razali",
      "Basura Fernando"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2021W/GHB/html/Razali_A_Log-Likelihood_Regularized_KL_Divergence_for_Video_Prediction_With_a_WACVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2021W/GHB/papers/Razali_A_Log-Likelihood_Regularized_KL_Divergence_for_Video_Prediction_With_a_WACVW_2021_paper.pdf",
    "published": "2021-01",
    "summary": " The use of latent variable models has shown to be a powerful tool for modeling probability distributions over sequences. In this paper, we introduce a new variational model that extends the recurrent network in two ways for the task of video frame prediction. First, we introduce 3D convolutions inside all modules including the recurrent model for future frame prediction, inputting and outputting a sequence of video frames at each timestep. This enables us to better exploit spatiotemporal information inside the variational recurrent model, allowing us to generate high-quality predictions. Second, we enhance the latent loss of the variational model by introducing a maximum likelihood estimate in addition to the KL divergence that is commonly used in variational models. This simple extension acts as a stronger regularizer in the variational autoencoder loss function and lets us obtain better results and generalizability. Experiments show that our model outperforms existing video prediction methods on several benchmarks while requiring fewer parameters. "
  }
}