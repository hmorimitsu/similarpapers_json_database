{
  "accv2022_main_imageretrievalwithwell-separatedsemantichashcenters": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Image Retrieval with Well-Separated Semantic Hash Centers",
    "authors": [
      "Liangdao Wang",
      "Yan Pan",
      "Hanjiang Lai",
      "Jian Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Image_Retrieval_with_Well-Separated_Semantic_Hash_Centers_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Image_Retrieval_with_Well-Separated_Semantic_Hash_Centers_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, some point-wise hash learning methods such as CSQ [1] and DPN [2] adapted \"hash centers\" as the global similarity label for each category and force the hash codes of the images with the same category to get closed to their corresponding hash centers. Although they outperformed other pairwise/triplet hashing methods, they assign hash centers to each class randomly and result in a sub-optimal performance because of ignoring the semantic relationship between categories, which means that they ignore the fact that the Hamming distance between the hash centers corresponding to two semantically similar classes should be smaller than the Hamming distance between the hash centers corresponding to two semantically dissimilar classes. To solve the above problem and generate well-separated and semantic hash centers, in this paper, we propose an optimization approach which aims at generating hash centers not only with semantic category information but also distinguished from each other. Specifically, we adopt the weight of last fully-connected layer in ResNet-50 model as category features to help inject semantic information into the generation of hash centers and try to maximize the expectation of the Hamming distance between each two hash centers. With the hash centers corresponding to each image category, we propose two effective loss functions to learn deep hashing function. Importantly, extensive experiments show that our proposed hash centers and training method outperform the state-of-the-art hash models on three image retrieval datasets."
  },
  "accv2022_main_adifferentiabledistanceapproximationforfairerimageclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Differentiable Distance Approximation for Fairer Image Classification",
    "authors": [
      "Nicholas E Rosa",
      "Tom Drummond",
      "Mehrtash Harandi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Rosa_A_Differentiable_Distance_Approximation_for_Fairer_Image_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Rosa_A_Differentiable_Distance_Approximation_for_Fairer_Image_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Naively trained AI models can be heavily biased. This can be particularly problematic when the biases involve legally or morally protected attributes such as ethnic background, age or gender. Existing solutions to this problem come at the cost of extra computation, unstable adversarial optimisation or have losses on the feature space structure that are disconnected from fairness measures and only loosely generalise to fairness. In this work we propose a differentiable approximation of the variance of demographics, a metric that can be used to measure the bias, or unfairness, in an AI model. Our approximation can be optimised alongside the regular training objective which eliminates the need for any extra models during training and directly improves the fairness of the regularised models. We demonstrate that our approach improves the fairness of AI models in varied task and dataset scenarios, whilst still maintaining a high level of classification accuracy. Code is available at https://bitbucket.org/nelliottrosa/base_fairness."
  },
  "accv2022_main_3dshapetemporalaggregationforvideo-basedclothing-changepersonre-identification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "3D Shape Temporal Aggregation for Video-Based Clothing-Change Person Re-identification",
    "authors": [
      "Ke Han",
      "Yan Huang",
      "Shaogang Gong",
      "Yan Huang",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Han_3D_Shape_Temporal_Aggregation_for_Video-Based_Clothing-Change_Person_Re-identification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Han_3D_Shape_Temporal_Aggregation_for_Video-Based_Clothing-Change_Person_Re-identification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "3D shape of human body can be both discriminative and clothing-independent information in video-based clothing-change person re-identification (Re-ID). However, existing Re-ID methods usually generate 3D body shapes without considering identity modelling, which severely weakens the discriminability of 3D human shapes. In addition, different video frames provide highly similar 3D shapes, but existing methods cannot capture the differences among 3D shapes over time. They are thus insensitive to the unique and discriminative 3D shape information of each frame and ineffectively aggregate many redundant framewise shapes in a videowise representation for Re-ID. To address these problems, we propose a 3D Shape Temporal Aggregation (3STA) model for video-based clothing-change Re-ID. To generate the discriminative 3D shape for each frame, we first introduce an identity-aware 3D shape generation module. It embeds the identity information into the generation of 3D shapes by the joint learning of shape estimation and identity recognition. Second, a difference-aware shape aggregation module is designed to measure inter-frame 3D human shape differences and automatically select the unique 3D shape information of each frame. This helps minimise redundancy and maximise complementarity in temporal shape aggregation. We further construct a Video-based Clothing-Change Re-ID (VCCR) dataset to address the lack of publicly available datasets for video-based clothing-change Re-ID. Extensive experiments on the VCCR dataset demonstrate the effectiveness of the proposed 3STA model. The dataset is available at https://vhank.github.io/vccr.github.io."
  },
  "accv2022_main_explainingdeepneuralnetworksforpointcloudsusinggradient-basedvisualisations": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Explaining Deep Neural Networks for Point Clouds using Gradient-based Visualisations",
    "authors": [
      "Jawad Tayyub",
      "Muhammad Sarmad",
      "Nicolas Sch\u00f6nborn"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tayyub_Explaining_Deep_Neural_Networks_for_Point_Clouds_using_Gradient-based_Visualisations_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tayyub_Explaining_Deep_Neural_Networks_for_Point_Clouds_using_Gradient-based_Visualisations_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Explaining decisions made by deep neural networks is a rapidly advancing research topic. In recent years, several approaches have attempted to provide visual explanations of decisions made by neural networks designed for structured 2D image input data. In this paper, we propose a novel approach to generate coarse visual explanations of networks designed to classify unstructured 3D data, namely point clouds. Our method uses gradients flowing back to the final feature map layers and maps these values as contributions of the corresponding points in the input point cloud. Due to dimensionality disagreement and lack of spatial consistency between input points and final feature maps, our approach combines gradients with points dropping to compute explanations of different parts of the point cloud iteratively. The generality of our approach is tested on various point cloud classification networks, including 'single object' networks PointNet, PointNet++, DGCNN, and a 'scene' network VoteNet. Our method generates symmetric explanation maps that highlight important regions and provide insight into the decision-making process of network architectures. We perform an exhaustive evaluation of trust and interpretability of our explanation method against comparative approaches using quantitative, quantitative and human studies. All our code is implemented in PyTorch and will be made publicly available."
  },
  "accv2022_main_tecm-cliptext-basedcontrollablemulti-attributefaceimagemanipulation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "TeCM-CLIP: Text-based Controllable Multi-attribute Face Image Manipulation",
    "authors": [
      "Xudong Lou",
      "Yiguang Liu",
      "Xuwei Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lou_TeCM-CLIP_Text-based_Controllable_Multi-attribute_Face_Image_Manipulation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lou_TeCM-CLIP_Text-based_Controllable_Multi-attribute_Face_Image_Manipulation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In recent years, various studies have demonstrated that utilizing the prior information of StyleGAN can effectively manipulate and generate realistic images. However, the latent code of StyleGAN is designed to control global styles, and it is arduous to precisely manipulate the property to achieve fine-grained control over synthesized images. In this work, we leverage a recently proposed Contrastive Language Image Pretraining (CLIP) model to manipulate latent code with text to control image generation. We encode image and text prompts in shared embedding space, leveraging powerful image-text representation capabilities pretrained on contrastive language images to manipulate partial style codes in the latent code. For multiple fine-grained attribute manipulations, we propose multiple attribute manipulation frameworks. Compared with previous CLIP-driven methods, our method can perform high-quality attribute editing much faster with less coupling between attributes. Extensive experimental illustrate the effectiveness of our approach."
  },
  "accv2022_main_enhancingfairnessofvisualattributepredictors": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Enhancing Fairness of Visual Attribute Predictors",
    "authors": [
      "Tobias H\u00e4nel",
      "Nishant Kumar",
      "Dmitrij Schlesinger",
      "Mengze Li",
      "Erdem \u00dcnal",
      "Abouzar Eslami",
      "Stefan Gumhold"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hanel_Enhancing_Fairness_of_Visual_Attribute_Predictors_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hanel_Enhancing_Fairness_of_Visual_Attribute_Predictors_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The performance of deep neural networks for image recognition tasks such as predicting a smiling face is known to degrade with under-represented classes of sensitive attributes. We address this problem by introducing fairness-aware regularization losses based on batch estimates of Demographic Parity, Equalized Odds, and a novel Intersection-over-Union measure. The experiments performed on facial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma classification challenge show the effectiveness of our proposed fairness losses for bias mitigation as they improve model fairness while maintaining high classification performance. To the best of our knowledge, our work is the first attempt to incorporate these types of losses in an end-to-end training scheme for mitigating biases of visual attribute predictors."
  },
  "accv2022_main_blindimagesuper-resolutionwithdegradation-awareadaptation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Blind Image Super-Resolution with Degradation-Aware Adaptation",
    "authors": [
      "Yue Wang",
      "Jiawen Ming",
      "Xu Jia",
      "James H. Elder",
      "Huchuan Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Blind_Image_Super-Resolution_with_Degradation-Aware_Adaptation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Blind_Image_Super-Resolution_with_Degradation-Aware_Adaptation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Most existing super-resolution (SR) methods are designed to restore high resolution (HR) images from certain low resolution (LR) images with a simple degradation, e.g. bicubic downsampling. Their generalization capability to real-world degradation is limited because it often couples several degradation factors such as noise and blur. To solve this problem, existing blind SR methods rely on either explicit degradation estimation or translation to bicubicly downsampled LR images, where inaccurate estimation or translation would severely deteriorate the SR performance. In this paper, we propose a plug-and-play module, which could be applied to any existing image super-resolution model for feature-level adaptation to improve the generalization ability to real-world degraded images. Specifically, the degradation encoder computes an implicit degradation representation which is supervised by a ranking loss based on the degradation level. The degradation representation then works as a kind of condition and is applied to the existing image super-resolution model pretrained on bicubicly downsampled LR images through the proposed region-aware modulation. With the proposed method, the base super-resolution model could be fine-tuned to adapt to the condition of degradation representation for further improvement. Experimental results on both synthetic and real-world datasets show that the proposed image SR method with compact model size performs favorably against state-of-the-art methods."
  },
  "accv2022_main_causal-setrasegmentationtransformervariantbasedoncausalintervention": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Causal-SETR: A SEgmentation TRansformer Variant Based on Causal Intervention",
    "authors": [
      "Wei Li",
      "Zhixin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Causal-SETR_A_SEgmentation_TRansformer_Variant_Based_on_Causal_Intervention_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Causal-SETR_A_SEgmentation_TRansformer_Variant_Based_on_Causal_Intervention_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present a novel SEgmentaion TRansformer variant based on causal intervention. It serves as an improved vision encoder for semantic segmentation. Many studies have proved that vision transformers (ViT) can achieve a competitive benchmark on these downstream tasks, which shows that they can learn feature representations well. In other words, it is good at observing the instance from the image. However, in the human visual system, to recognize the objects in the scene, it is necessary to observe the objects themselves and introduce some prior knowledge for producing higher confidence results. Inspired by this, we introduced a structural causal model(SCM) to model images, category labels, and context. Beyond observing, we propose a causal intervention method by removing the confounding bias of global context and plugging it in the ViT encoder. Unlike other sequence-to-sequence prediction tasks, we use causal intervention instead of likelihood. Besides, the proxy training objective of the framework is to predict the contextual objects of a region. Finally, we combine this encoder with the segmentation decoder. Experiments show that our proposed method is flexible and effective."
  },
  "accv2022_main_three-stagebidirectionalinteractionnetworkforefficientrgb-dsalientobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Three-Stage Bidirectional Interaction Network for Efficient RGB-D Salient Object Detection",
    "authors": [
      "Yang Wang",
      "Yanqing Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Three-Stage_Bidirectional_Interaction_Network_for_Efficient_RGB-D_Salient_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Three-Stage_Bidirectional_Interaction_Network_for_Efficient_RGB-D_Salient_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The addition of depth maps improves the performance of salient object detection (SOD). However, most existing RGB-D SOD methods are inefficient. We observe that existing models take into account the respective advantages of the two modalities but do not fully explore the roles of cross-modality features of various levels. To this end, we remodel the relationship between RGB features and depth features from a new perspective of the feature encoding stage and propose a three-stage bidirectional interaction network (TBINet). Specifically, to obtain robust feature representations, we propose three interaction strategies: bidirectional attention guidance (BAG), bidirectional feature supplement (BFS), and shared network, and use them for the three stages of feature encoder, respectively. In addition, we propose a cross-modality feature aggregation (CFA) module for feature aggregation and refinement. Our model is lightweight (3.7 M parameters) and fast (329 ms on CPU). Experiments on six benchmark datasets show that TBINet outperforms other SOTA methods. Our model achieves the best performance and efficiency trade-off."
  },
  "accv2022_main_apaunetaxisprojectionattentionunetforsmalltargetin3dmedicalsegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "APAUNet: Axis Projection Attention UNet for Small Target in 3D Medical Segmentation",
    "authors": [
      "Yuncheng Jiang",
      "Zixun Zhang",
      "Shixi Qin",
      "Yao Guo",
      "Zhen Li",
      "Shuguang Cui"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Jiang_APAUNet_Axis_Projection_Attention_UNet_for_Small_Target_in_3D_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Jiang_APAUNet_Axis_Projection_Attention_UNet_for_Small_Target_in_3D_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In 3D medical image segmentation, small targets segmentation is crucial for diagnosis but still faces challenges. In this paper, we propose the Axis Projection Attention UNet, named APAUNet, for 3D medical image segmentation, especially for small targets. Considering the large proportion of the background in the 3D feature space, we introduce a projection strategy to project the 3D features into three orthogonal 2D planes to capture the contextual attention from different views. In this way, we can filter out the redundant feature information and mitigate the loss of critical information for small lesions in 3D scans. Then we utilize a dimension hybridization strategy to fuse the 3D features with attention from different axes and merge them by a weighted summation to adaptively learn the importance of different perspectives. Finally, in the APA Decoder, we concatenate both high and low resolution features in the 2D projection process, thereby obtaining more precise multi-scale information, which is vital for small lesion segmentation. Quantitative and qualitative experimental results on two public datasets (BTCV and MSD) demonstrate that our proposed APAUNet outperforms the other methods. Concretely, our APAUNet achieves an average dice score of 87.84 on BTCV, 84.48 on MSD-Liver and 69.13 on MSD-Pancreas, and significantly surpass the previous SOTA methods on small targets."
  },
  "accv2022_main_modulardegradationsimulationandrestorationforunder-displaycamera": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Modular Degradation Simulation and Restoration for Under-Display Camera",
    "authors": [
      "Yang Zhou",
      "Yuda Song",
      "Xin Du"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhou_Modular_Degradation_Simulation_and_Restoration_for_Under-Display_Camera_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhou_Modular_Degradation_Simulation_and_Restoration_for_Under-Display_Camera_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Under-display camera (UDC) provides an elegant solution for full-screen smartphones. However, UDC captured images suffer from severe degradation since sensors lie under the display. Although this issue can be tackled by image restoration networks, these networks require large-scale image pairs for training. To this end, we propose a modular network dubbed MPGNet trained using the generative adversarial network (GAN) framework for simulating UDC imaging. Specifically, we note that the UDC imaging degradation process contains brightness attenuation, blurring, and noise corruption. Thus we model each degradation with a characteristic-related modular network, and all modular networks are cascaded to form the generator. Together with a pixel-wise discriminator and supervised loss, we can train the generator to simulate the UDC imaging degradation process. Furthermore, we present a Transformer-style network named DWFormer for UDC image restoration. For practical purposes, we use depth-wise convolution instead of the multi-head self-attention to aggregate local spatial information. Moreover, we propose a novel channel attention module to aggregate global information, which is critical for brightness recovery. We conduct evaluations on the UDC benchmark, and our method surpasses the previous state-of-the-art models by 1.23 dB on the P-OLED track and 0.71 dB on the T-OLED track, respectively."
  },
  "accv2022_main_domaingeneralizedrppgnetworkdisentangledfeaturelearningwithdomainpermutationanddomainaugmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Domain Generalized RPPG Network: Disentangled Feature Learning with Domain Permutation and Domain Augmentation",
    "authors": [
      "Wei-Hao Chung",
      "Cheng-Ju Hsieh",
      "Sheng-Hung Liu",
      "Chiou-Ting Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chung_Domain_Generalized_RPPG_Network_Disentangled_Feature_Learning_with_Domain_Permutation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chung_Domain_Generalized_RPPG_Network_Disentangled_Feature_Learning_with_Domain_Permutation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Remote photoplethysmography (rPPG) offers a contactless method for monitoring physiological signals from facial videos. Existing learning-based methods, although work effectively on intra-dataset scenarios, degrade severely on cross-dataset testing. In this paper, we address the cross-dataset testing as a domain generalization problem and propose a novel DG-rPPGNet to learn a domain generalized rPPG estimator. To this end, we develop a feature disentangled learning framework to disentangle rPPG, identity, and domain features from input facial videos. Next, we propose a domain permutation strategy to further constrain the disentangled rPPG features to be invariant to different domains. Finally, we design a novel adversarial domain augmentation strategy to enlarge the domain sphere of DG-rPPGNet. Our experimental results show that DG-rPPGNet outperforms other rPPG estimation methods in many cross-domain settings on UBFC-rPPG, PURE, COHFACE, and VIPL-HR datasets."
  },
  "accv2022_main_adaptiverangeguidedmulti-viewdepthestimationwithnormalrankingloss": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Adaptive Range guided Multi-view Depth Estimation with Normal Ranking Loss",
    "authors": [
      "Yikang Ding",
      "Zhenyang Li",
      "Dihe Huang",
      "Kai Zhang",
      "Zhiheng Li",
      "Wensen Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ding_Adaptive_Range_guided_Multi-view_Depth_Estimation_with_Normal_Ranking_Loss_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ding_Adaptive_Range_guided_Multi-view_Depth_Estimation_with_Normal_Ranking_Loss_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep learning algorithms for Multi-view Stereo (MVS) have surpassed traditional MVS methods in recent years, due to enhanced reconstruction quality and runtime. Learning-based methods, on the other side, continue to generate overly smoothed depths, resulting in poor reconstruction. In this paper, we aim to boost depth estimation (BDE) for MVS and present an approach for reconstructing high-quality point clouds with precise depth prediction. This method is termed as BDE-MVSNet. We present a non-linear technique that derives an adaptive depth range (ADR) from the estimated probability, motivated by distinctive differences in estimated probability between foreground and background pixels. ADR offers accurate estimation while processing same-resolution depth maps in only two stages since the depth range is well-adapted for each pixel. ADR also tends to decrease fuzzy boundaries via upsampling low-resolution depth maps between stages. Additionally, we provide a novel structure-guided normal ranking (SGNR) loss that imposes geometrical consistency in boundary areas by using the surface normal vector. Extensive experiments on DTU dataset, Tanks and Temples benchmark, and BlendedMVS dataset demonstrate that our method outperforms known methods and achieves state-of-the-art performance."
  },
  "accv2022_main_gatedcrossword-visualattention-drivengenerativeadversarialnetworksfortext-to-imagesynthesis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Gated cross word-visual attention-driven generative adversarial networks for text-to-image synthesis",
    "authors": [
      "Borun Lai",
      "Lihong Ma",
      "Jing Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lai_Gated_cross_word-visual_attention-driven_generative_adversarial_networks_for_text-to-image_synthesis_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lai_Gated_cross_word-visual_attention-driven_generative_adversarial_networks_for_text-to-image_synthesis_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The main objective of text-to-image (Txt2Img) synthesis is to generate realistic images from text descriptions. We propose to insert a gated cross word-visual attention unit (GCAU) into the conventional multiple-stage generative adversarial network Txt2Img framework. Our GCAU consists of two key components. First, a cross word-visual attention mechanism is proposed to draw fine-grained details at different subregions of the image by focusing on the relevant words (via the visual-to-word attention), and select important words by paying attention to the relevant synthesized subregions of the image (via the word-to-visual attention). Second, a gated refinement mechanism is proposed to dynamically select important word information for refining the generated image. Extensive experiments are conducted to demonstrate the superior image generation performance of the proposed approach on CUB and MS-COCO benchmark datasets."
  },
  "accv2022_main_pbcstereoacompressedstereonetworkwithpurebinaryconvolutionaloperations": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PBCStereo: A Compressed Stereo Network with Pure Binary Convolutional Operations",
    "authors": [
      "Jiaxuan Cai",
      "ZHI QI",
      "Keqi Fu",
      "Xulong Shi",
      "Zan Li",
      "Xuanyu Liu",
      "Hao Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cai_PBCStereo_A_Compressed_Stereo_Network_with_Pure_Binary_Convolutional_Operations_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cai_PBCStereo_A_Compressed_Stereo_Network_with_Pure_Binary_Convolutional_Operations_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Although end-to-end stereo matching networks achieve great performance for disparity estimation, most of them require far too many floating-point operations to deploying on resource-constrained devices. To solve this problem, we propose PBCStereo, the first lightweight stereo network using pure binarized convolutional operations. The degradation of feature diversity, which is aggravated by binary deconvolution, is alleviated via our novel upsampling module (IBC). Furthermore, we propose an effective coding method, named BIL, for the insufficient binarization of the input layer. Based on IBC modules and BIL coding, all convolutional operations become binary in our stereo matching pipeline. PBCStereo gets 39x saving in OPs while achieving comparable accuracy on SceneFlow and KITTI datasets."
  },
  "accv2022_main_hicohierarchicalcontrastivelearningforultrasoundvideomodelpretraining": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "HiCo: Hierarchical Contrastive  Learning for Ultrasound Video Model Pretraining",
    "authors": [
      "Chunhui Zhang",
      "Yixiong Chen",
      "Li Liu",
      "Qiong Liu",
      "Xi Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_HiCo_Hierarchical_Contrastive__Learning_for_Ultrasound_Video_Model_Pretraining_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_HiCo_Hierarchical_Contrastive__Learning_for_Ultrasound_Video_Model_Pretraining_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The self-supervised ultrasound (US) video model pretraining can use a small amount of labeled data to achieve one of the most promising results on US diagnosis. However, it does not take full advantage of multi-level knowledge for learning deep neural networks (DNNs), and thus is difficult to learn transferable feature representations. This work proposes a hierarchical contrastive learning (HiCo) method to improve the transferability for the US video model pretraining. HiCo introduces both peer-level semantic alignment and cross-level semantic alignment to facilitate the interaction between different semantic levels, which can effectively accelerate the convergence speed, leading to better generalization and adaptation of the learned model. Additionally, a softened objective function is implemented by smoothing the hard labels, which can alleviate the negative effect caused by local similarities of images between different classes. Experiments with HiCo on five datasets demonstrate its favorable results over state-of-the-art approaches. The source code of this work is publicly available at https://github.com/983632847/HiCo."
  },
  "accv2022_main_symmnerflearningtoexploresymmetrypriorforsingle-viewviewsynthesis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SymmNeRF: Learning to Explore Symmetry Prior for Single-View View Synthesis",
    "authors": [
      "Xingyi Li",
      "Chaoyi Hong",
      "Yiran Wang",
      "Zhiguo Cao",
      "Ke Xian",
      "Guosheng Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_SymmNeRF_Learning_to_Explore_Symmetry_Prior_for_Single-View_View_Synthesis_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_SymmNeRF_Learning_to_Explore_Symmetry_Prior_for_Single-View_View_Synthesis_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We study the problem of novel view synthesis of objects from a single image. Existing methods have demonstrated the potential in single-view view synthesis. However, they still fail to recover the fine appearance details, especially in self-occluded areas. This is because a single view only provides limited information. We observe that manmade objects usually exhibit symmetric appearances, which introduce additional prior knowledge. Motivated by this, we investigate the potential performance gains of explicitly embedding symmetry into the scene representation. In this paper, we propose SymmNeRF, a neural radiance field (NeRF) based framework that combines local and global conditioning under the introduction of symmetry priors. In particular, SymmNeRF takes the pixel-aligned image features and the corresponding symmetric features as extra inputs to the NeRF, whose parameters are generated by a hypernetwork. As the parameters are conditioned on the image-encoded latent codes, SymmNeRF is thus scene-independent and can generalize to new scenes. Experiments on synthetic and real-world datasets show that SymmNeRF synthesizes novel views with more details regardless of the pose transformation, and demonstrates good generalization when applied to unseen objects. Code is available at: https://github.com/xingyi-li/SymmNeRF."
  },
  "accv2022_main_decouplingidentityandvisualqualityforimageandvideoanonymization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Decoupling identity and visual quality for image and video anonymization",
    "authors": [
      "Maxim Maximov",
      "Ismail Elezi",
      "Laura Leal-Taix\u00e9"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Maximov_Decoupling_identity_and_visual_quality_for_image_and_video_anonymization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Maximov_Decoupling_identity_and_visual_quality_for_image_and_video_anonymization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The widespread usage of computer vision applications in the public domain has opened the delicate question of image data privacy. In recent years, computer vision researchers have proposed technological solutions to anonymize image and video data so that computer vision systems can still be used without compromising data privacy. While promising, these methods come with a range of limitations, including low diversity of outputs, low-resolution generation quality, the appearance of artifacts when handling extreme poses, and non-smooth temporal consistency. In this work, we propose a novel network based on generative adversarial networks (GANs) for face anonymization in images and videos. The key insight of our approach is to decouple the problems of image generation and image blending. This allows us to reach significant improvements in image quality, diversity, and temporal consistency while making possible to train the network in different tasks and datasets. Furthermore, we show that our framework is able to anonymize faces containing extreme poses, a long-standing problem in the field."
  },
  "accv2022_main_slice-maskbased3dcardiacshapereconstructionfromctvolume": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Slice-mask based 3D Cardiac Shape Reconstruction from CT volume",
    "authors": [
      "Xiaohan Yuan",
      "Cong Liu",
      "Fu Feng",
      "Yinsu Zhu",
      "Yangang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yuan_Slice-mask_based_3D_Cardiac_Shape_Reconstruction_from_CT_volume_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yuan_Slice-mask_based_3D_Cardiac_Shape_Reconstruction_from_CT_volume_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "An accurate 3D ventricular model is essential for diagnosing and analyzing cardiovascular disease. It is challenging to obtain accurate patient-specific models on scarce data via widely accepted deep-learning methods. To fully use the characteristics of medical volume-based images, we present a slice-mask representation to better regress the parameters of the 3D model. A data synthesis strategy is proposed to alleviate the lack of training data by sampling in the constructed statistical shape model space and obtaining the corresponding slice-masks. We train the end-to-end structure by combining the segmentation and parametric regression modules. Furthermore, we establish a larger left ventricular CT dataset than before, which fills the gap in relevant data of the healthy population. Our method is evaluated on both synthetic data and real cardiac scans. Experiments demonstrate that our method can achieve advanced results in shape reconstruction and segmentation tasks. Code is publicly available at https://github.com/yuan-xiaohan/Slice-mask-based-3D-Cardiac-Shape-Reconstruction."
  },
  "accv2022_main_colliderarobusttrainingframeworkforbackdoordata": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "COLLIDER: A Robust Training Framework for Backdoor Data",
    "authors": [
      "Hadi Mohaghegh Dolatabadi",
      "Sarah Erfani",
      "Christopher Leckie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Dolatabadi_COLLIDER_A_Robust_Training_Framework_for_Backdoor_Data_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Dolatabadi_COLLIDER_A_Robust_Training_Framework_for_Backdoor_Data_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An adversary poisons some of the training data in such attacks by installing a trigger. The goal is to make the trained DNN output the attacker's desired class whenever the trigger is activated while performing as usual for clean data. Various approaches have recently been proposed to detect malicious backdoored DNNs. However, a robust, end-to-end training approach, like adversarial training, is yet to be discovered for backdoor poisoned data. In this paper, we take the first step toward such methods by developing a robust training framework, COLLIDER, that selects the most prominent samples by exploiting the underlying geometric structures of the data. Specifically, we effectively filter out candidate poisoned data at each training epoch by solving a geometrical coreset selection objective. We first argue how clean data samples exhibit (1) gradients similar to the clean majority of data and (2) low local intrinsic dimensionality (LID). Based on these criteria, we define a novel coreset selection objective to find such samples, which are used for training a DNN. We show the effectiveness of the proposed method for robust training of DNNs on various poisoned datasets, reducing the backdoor success rate significantly."
  },
  "accv2022_main_ageneraldivergencemodelingstrategyforsalientobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A General Divergence Modeling Strategy for Salient Object Detection",
    "authors": [
      "Xinyu Tian",
      "Jing Zhang",
      "Yuchao Dai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tian_A_General_Divergence_Modeling_Strategy_for_Salient_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tian_A_General_Divergence_Modeling_Strategy_for_Salient_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Salient object detection is subjective in nature, which implies that multiple estimations should be related to the same input image. Most existing salient object detection models are deterministic following a point to point estimation learning pipeline, making them incapable of estimating the predictive distribution. Although latent variable model based stochastic prediction networks exist to model the prediction variants, the latent space based on the single clean saliency annotation is less reliable in exploring the subjective nature of saliency, leading to less effective saliency \"divergence modeling\". Given multiple saliency annotations, we introduce a general divergence modeling strategy via random sampling, and apply our strategy to an ensemble based framework and three latent variable model based solutions to explore the \"subjective nature\" of saliency. Experimental results prove the superior performance of our general divergence modeling strategy."
  },
  "accv2022_main_causesofcatastrophicforgettinginclass-incrementalsemanticsegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Causes of Catastrophic Forgetting in Class-Incremental Semantic Segmentation",
    "authors": [
      "Tobias Kalb",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kalb_Causes_of_Catastrophic_Forgetting_in_Class-Incremental_Semantic_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kalb_Causes_of_Catastrophic_Forgetting_in_Class-Incremental_Semantic_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Class-incremental learning for semantic segmentation (CiSS) is presently a highly researched field which aims at updating a semantic segmentation model by sequentially learning new semantic classes. A major challenge in CiSS is overcoming the effects of catastrophic forgetting, which describes the sudden drop of accuracy on previously learned classes after the model is trained on a new set of classes.Despite latest advances in mitigating catastrophic forgetting, the underlying causes of forgetting specifically in CiSS are not well understood. Therefore, in a set of experiments and representational analyses, we demonstrate that the semantic shift of the background class and a bias towards new classes are the major causes of forgetting in CiSS. Furthermore, we show that both causes mostly manifest themselves in deeper classification layers of the network, while the early layers of the model are not affected. Finally, we demonstrate how both causes are effectively mitigated utilizing the information contained in the background, with the help of knowledge distillation and an unbiased cross-entropy loss."
  },
  "accv2022_main_cross-viewself-fusionforself-supervised3dhumanposeestimationinthewild": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Cross-View Self-Fusion for Self-Supervised 3D Human Pose Estimation in the Wild",
    "authors": [
      "Hyun-Woo Kim",
      "Gun-Hee Lee",
      "Myeong-Seok Oh",
      "Seong-Whan Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kim_Cross-View_Self-Fusion_for_Self-Supervised_3D_Human_Pose_Estimation_in_the_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kim_Cross-View_Self-Fusion_for_Self-Supervised_3D_Human_Pose_Estimation_in_the_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Human pose estimation methods have recently shown remarkable results with supervised learning that requires large amounts of labeled training data. However, such training data for various human activities does not exist since 3D annotations are acquired with traditional motion capture systems that usually require a controlled indoor environment. To address this issue, we propose a self-supervised approach that learns a monocular 3D human pose estimator from unlabeled multi-view images by using multi-view consistency constraints. Furthermore, we refine inaccurate 2D poses, which adversely affect 3D pose predictions, using the property of canonical space without relying on camera calibration. Since we do not require camera calibrations to leverage the multi-view information, we can train a network from in-the-wild environments. The key idea is to fuse the 2D observations across views and combine predictions from the observations to satisfy the multi-view consistency during training. We outperform state-of-the-art methods in self-supervised learning on the two benchmark datasets Human3.6M and MPI-INF-3DHP as well as on the in-the-wild dataset SkiPose."
  },
  "accv2022_main_compressedvisionforefficientvideounderstanding": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Compressed Vision for Efficient Video Understanding",
    "authors": [
      "Olivia Wiles",
      "Joao Carreira",
      "Iain Barr",
      "Andrew Zisserman",
      "Mateusz Malinowski"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wiles_Compressed_Vision_for_Efficient_Video_Understanding_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wiles_Compressed_Vision_for_Efficient_Video_Understanding_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Experience and reasoning occur across multiple temporal scales: milliseconds, seconds, hours or days. The vast majority of computer vision research, however, still focuses on individual images or short videos lasting only a few seconds. This is because handling longer videos require more scalable approaches even to process them. In this work, we propose a framework enabling research on hour-long videos with the same hardware that can now process second-long videos. We replace standard video compression, e.g. JPEG, with neural compression and show that we can directly feedcompressed videos as inputs to regular video networks. Operating on compressed videos improves efficiency at all pipeline levels -- data transfer, speed and memory -- making it possible to train models faster and on much longer videos. Processing compressed signals has, however, the downside of precluding standard augmentation techniques if done naively. We address that by introducing a small network that can apply transformations to latent codes corresponding to commonly used augmentations in the original video space. We demonstrate that with our compressed vision pipeline, we can train video models more efficiently on popular benchmarks such as Kinetics600 and COIN. We also perform proof-of-concept experiments with new tasks defined over hour-long videos at standard frame rates. Processing such long videos is impossible without using compressed representation."
  },
  "accv2022_main_flaretransformersolarflarepredictionusingmagnetogramsandsunspotphysicalfeatures": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Flare Transformer: Solar Flare Prediction using Magnetograms and Sunspot Physical Features",
    "authors": [
      "Kanta Kaneda",
      "Yuiga Wada",
      "Tsumugi Iida",
      "Naoto Nishizuka",
      "Y\u00fbki Kubo",
      "Komei Sugiura"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kaneda_Flare_Transformer_Solar_Flare_Prediction_using_Magnetograms_and_Sunspot_Physical_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kaneda_Flare_Transformer_Solar_Flare_Prediction_using_Magnetograms_and_Sunspot_Physical_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The prediction of solar flares is essential for reducing the potential damage to social infrastructures that are vital to society. However, predicting solar flares accurately is a very challenging task. Existing methods predict flares using either physical features or images, but the main bottleneck is that they sometimes incorrectly predict a class that is smaller than the actual solar flare. In this paper, we propose the Flare Transformer, a solar flare prediction model that handles both images and physical features through the Magnetogram Module and the Sunspot Feature Module. The transformer attention mechanism is introduced to model the temporal relationships between input features. We also introduce a new differentiable loss function to balance the two major metrics of the Gandin--Murphy--Gerrity score and Brier skill score. We validate our model on a publicly available dataset. The results show that the Flare Transformer outperformed the baseline methods in terms of the Gandin--Murphy--Gerrity score and true skill statistic, and achieved better performance than those given by human experts."
  },
  "accv2022_main_semi-supervisedsemanticsegmentationwithuncertainty-guidedselfcrosssupervision": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Semi-Supervised Semantic Segmentation with Uncertainty-guided Self Cross Supervision",
    "authors": [
      "Yunyang Zhang",
      "Zhiqiang Gong",
      "Xiaoyu Zhao",
      "Xiaohu Zheng",
      "Wen Yao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Semi-Supervised_Semantic_Segmentation_with_Uncertainty-guided_Self_Cross_Supervision_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Semi-Supervised_Semantic_Segmentation_with_Uncertainty-guided_Self_Cross_Supervision_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "As a powerful way of realizing semi-supervised segmentation, the cross supervision method learns cross consistency based on independent ensemble models using abundant unlabeled images.In this work, we propose a novel cross supervision method, namely uncertainty-guided self cross supervision (USCS).To avoid multiplying the cost of computation resources caused by ensemble models, we first design a multi-input multi-output (MIMO) segmentation model which can generate multiple outputs with the shared model. The self cross supervision is imposed over the results from one MIMO model, heavily saving the cost of parameters and calculations. On the other hand, to further alleviate the large noise in pseudo labels caused by insufficient representation ability of the MIMO model, we employ uncertainty as guided information to encourage the model to focus on the high confident regions of pseudo labels and mitigate the effects of wrong pseudo labeling in self cross supervision, improving the performance of the segmentation model. Extensive experiments show that our method achieves state-of-the-art performance while saving 40.5% and 49.1% cost on parameters and calculations."
  },
  "accv2022_main_gb-cosfacerethinkingsoftmax-basedfacerecognitionfromtheperspectiveofopensetclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "GB-CosFace: Rethinking Softmax-based Face Recognition from the Perspective of Open Set Classification",
    "authors": [
      "Mingqiang Chen",
      "Lizhe Liu",
      "Xiaohao Chen",
      "Siyu Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_GB-CosFace_Rethinking_Softmax-based_Face_Recognition_from_the_Perspective_of_Open_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_GB-CosFace_Rethinking_Softmax-based_Face_Recognition_from_the_Perspective_of_Open_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "State-of-the-art face recognition methods typically take the multi-classification pipeline and adopt the softmax-based loss for optimization. Although these methods have achieved great success, the softmax-based loss has its limitation from the perspective of open set classification: the multi-classification objective in the training phase does not strictly match the objective of open set classification testing. In this paper, we derive a new loss named global boundary CosFace (GB-CosFace). Our GB-CosFace introduces an adaptive global boundary to determine whether two face samples belong to the same identity so that the optimization objective is aligned with the testing process from the perspective of open set classification. Meanwhile, since the loss formulation is derived from the softmax-based loss, our GB-CosFace retains the excellent properties of the softmax-based loss, and CosFace is proved to be a special case of the proposed loss. We analyze and explain the proposed GB-CosFace geometrically. Comprehensive experiments on multiple face recognition benchmarks indicate that the proposed GB-CosFace outperforms current state-of-the-art face recognition losses in mainstream face recognition tasks. Compared to CosFace, our GB-CosFace improves 5.30%, 0.70%, and 0.36% at TAR@FAR=1e-6, 1e-5, 1e-4 on IJB-C benchmark."
  },
  "accv2022_main_dynamicfeatureaggregationforefficientvideoobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Dynamic Feature Aggregation for Efficient Video Object Detection",
    "authors": [
      "Yiming Cui"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cui_Dynamic_Feature_Aggregation_for_Efficient_Video_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cui_Dynamic_Feature_Aggregation_for_Efficient_Video_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Video object detection is a fundamental yet challenging task in computer vision. One practical solution is to take advantage of temporal information from the video and apply feature aggregation to enhance the object features in each frame. Though effective, those existing methods always suffer from low inference speeds because they use a fixed number of frames for feature aggregation regardless of the input frame. Therefore, this paper aims to improve the inference speed of the current feature aggregation-based video object detectors while maintaining their performance. To achieve this goal, we propose a vanilla dynamic aggregation module that adaptively selects the frames for feature enhancement. Then, we extend the vanilla dynamic aggregation module to a more effective and reconfigurable deformable version. Finally, we introduce inplace distillation loss to improve the representations of objects aggregated with fewer frames. Extensive experimental results validate the effectiveness and efficiency of our proposed methods: On the ImageNet VID benchmark, integrated with our proposed methods, FGFA and SELSA can improve the inference speed by 31% and 76% respectively while getting comparable performance on accuracy."
  },
  "accv2022_main_shapepriorisnotallyouneeddiscoveringbalancebetweentextureandshapebiasincnn": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Shape Prior is Not All You Need: Discovering Balance between Texture and Shape bias in CNN",
    "authors": [
      "Hyunhee Chung",
      "Kyung Ho Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chung_Shape_Prior_is_Not_All_You_Need_Discovering_Balance_between_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chung_Shape_Prior_is_Not_All_You_Need_Discovering_Balance_between_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "As Convolutional Neural Network (CNN) trained under ImageNet is known to be biased in image texture rather than object shapes, recent works proposed that elevating shape awareness of the CNNs makes them similar to human visual recognition. However, beyond the ImageNet-trained CNN, how can we make CNNs similar to human vision in the wild? In this paper, we present a series of analyses to answer this question. First, we propose AdaBA, a novel method of quantitatively illustrating CNN's shape and texture bias by resolving several limits of the prior method. With the proposed AdaBA, we focused on fine-tuned CNN's bias landscape which previous studies have not dealt with. We discover that fine-tuned CNNs are also biased to texture, but their bias strengths differ along with the downstream dataset; thus, we presume a data distribution is a root cause of texture bias exists. To tackle this root cause, we propose a granular labeling scheme, a simple but effective solution that redesigns the label space to pursue a balance between texture and shape biases. We empirically examine that the proposed scheme escalates CNN's classification and OOD detection performance. We expect key findings and proposed methods in the study to elevate understanding of the CNN and yield an effective solution to mitigate this texture bias."
  },
  "accv2022_main_ajointframeworktowardsclass-awareandclass-agnosticalignmentforfew-shotsegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Joint Framework Towards Class-aware and Class-agnostic Alignment for Few-shot Segmentation",
    "authors": [
      "Kai Huang",
      "Mingfei Cheng",
      "Yang Wang",
      "Bochen Wang",
      "Ye Xi",
      "Feigege Wang",
      "Peng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Huang_A_Joint_Framework_Towards_Class-aware_and_Class-agnostic_Alignment_for_Few-shot_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Huang_A_Joint_Framework_Towards_Class-aware_and_Class-agnostic_Alignment_for_Few-shot_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Few-shot segmentation (FSS) aims to segment objects of unseen classes given only a few annotated support images. Most existing methods simply stitch query features with independent support prototypes and segment the query image by feeding the mixed features to a decoder. Although significant improvements have been achieved, existing methods are still face class biases due to class variants and background confusion. In this paper, we propose a joint framework that combines more valuable class-aware and class-agnostic alignment guidance to facilitate the segmentation. Specifically, we design a hybrid alignment module which establishes multi-scale query-support correspondences to mine the most relevant class-aware information for each query image from the corresponding support features. In addition, we explore utilizing base-classes knowledge to generate class-agnostic prior mask which makes a distinction between real background and foreground by highlighting all object regions, especially those of unseen classes. By jointly aggregating class-aware and class-agnostic alignment guidance, better segmentation performances are obtained on query images. Extensive experiments on PASCAL-5i and COCO-20i datasets demonstrate that our proposed joint framework performs better, especially on the 1-shot setting."
  },
  "accv2022_main_continuousself-studyscenegraphgenerationwithself-knowledgedistillationandspatialaugmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Continuous Self-Study: Scene Graph Generation with Self-Knowledge Distillation and Spatial Augmentation",
    "authors": [
      "Yuan Lv",
      "Yajing Xu",
      "Shusen Wang",
      "Yingjian Ma",
      "DengKe Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lv_Continuous_Self-Study_Scene_Graph_Generation_with_Self-Knowledge_Distillation_and_Spatial_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lv_Continuous_Self-Study_Scene_Graph_Generation_with_Self-Knowledge_Distillation_and_Spatial_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "As an extension of visual detection tasks, scene graph generation (SGG) has drawn increasing attention with the achievement of complex image understanding. However, it still faces two challenges: one is the distinguishing of objects with high visual similarity, the other is the discriminating of relationships with long-tailed bias. In this paper, we propose a Continuous Self-Study model (CSS) with self-knowledge distillation and spatial augmentation to refine the detection of hard samples. We design a long-term memory structure for CSS to learn its own behavior with the context feature, which can perceive the hard sample of itself and focus more on similar targets in different scenes. Meanwhile, a fine-grained relative position encoding method is adopted to augment spatial features and supplement relationship information. On the Visual Genome benchmark, experiments show that the proposed CSS achieves obvious improvements over the previous state-of-the-art methods. Our code is available at https://github.com/LINYE1998/Continuous_Self_Study."
  },
  "accv2022_main_temporal-awaresiamesetrackerintegratetemporalcontextfor3dobjecttracking": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Temporal-aware Siamese Tracker: Integrate Temporal Context for 3D Object Tracking",
    "authors": [
      "Kaihao Lan",
      "Haobo Jiang",
      "Jin Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lan_Temporal-aware_Siamese_Tracker_Integrate_Temporal_Context_for_3D_Object_Tracking_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lan_Temporal-aware_Siamese_Tracker_Integrate_Temporal_Context_for_3D_Object_Tracking_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Learning discriminative target-specific feature representation for object localization is the core of the 3D Siamese object tracking algorithms.Current Siamese trackers focus on aggregating the target information from the latest template into the search area for target-specific feature construction, which presents the limited performance in the case of object occlusion or object missing.To this end, in this paper, we propose a novel temporal-aware Siamese tracking framework, where the rich target clue lying in a set of historical templates is integrated into the search area for reliable target-specific feature aggregation.Specifically, our method consists of three modules, including a template set sampling module, a temporal feature enhancement module and a temporal-aware feature aggregation module.In the template set sampling module, an effective scoring network is proposed to evaluate the tracking quality of the template so that the high-quality templates are collected to form the historical template set.Then, with the initial feature embeddings of the historical templates, the temporal feature enhancement module concatenates all template embeddings as a whole and then feeds them into a linear attention module for cross-template feature enhancement.Furthermore, the temporal-aware feature aggregation module aggregates the target clue lying in each template into the search area to construct multiple historical target-specific search-area features.Particularly, we follow the collection orders of the templates to fuse all generated target-specific features via an RNN-based module so that the fusion weight of the previous template information can be discounted to better fit the current tracking state.Finally, we feed the temporal fused target-specific feature into a modified CenterPoint detection head for target position regression. Extensive experiments on KITTI, NuScenes and waymo open datasets show the effectiveness of our proposed method."
  },
  "accv2022_main_consistentsemanticattacksonopticalflow": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Consistent Semantic Attacks on Optical Flow",
    "authors": [
      "Tom Koren",
      "Lior Talker",
      "Michael Dinerstein",
      "Ran Vitek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Koren_Consistent_Semantic_Attacks_on_Optical_Flow_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Koren_Consistent_Semantic_Attacks_on_Optical_Flow_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present a novel approach for semantically targeted adversarial attacks on Optical Flow. In such attacks the goal is to corrupt the flow predictions of a specific object category or instance. Usually, an attacker seeks to hide the adversarial perturbations in the input. However, a quick scan of the output reveals the attack. In contrast, our method helps to hide the attacker's intent in the output flow as well. We achieve this thanks to a regularization term that encourages off-target consistency. We perform extensive tests on leading optical flow models to demonstrate the benefits of our approach in both white-box and blackbox settings. Also, we demonstrate the effectiveness of our attack on subsequent tasks that depend on the optical flow."
  },
  "accv2022_main_neuralplenopticsamplinglearninglight-fieldfromthousandsofimaginaryeyes": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Neural Plenoptic Sampling: Learning Light-field from Thousands of Imaginary Eyes",
    "authors": [
      "Junxuan Li",
      "Yujiao Shi",
      "Hongdong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Neural_Plenoptic_Sampling_Learning_Light-field_from_Thousands_of_Imaginary_Eyes_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Neural_Plenoptic_Sampling_Learning_Light-field_from_Thousands_of_Imaginary_Eyes_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The Plenoptic function describes light rays observed from any given position in every viewing direction. It is often parameterized as a 5-D function L(x, y, z, \\theta, \\phi) for a static scene. Capturing all the plenoptic functions in the space of interest is paramount for Image-Based Rendering (IBR) and Novel View Synthesis (NVS). It encodes a complete light-field (i.e., lumigraph) therefore allows one to freely roam in the space and view the scene from any location in any direction. However, achieving this goal by conventional light-field capture technique is expensive, requiring densely sampling the ray space using arrays of cameras or lenses. This paper proposes a much simpler solution to address this challenge by using only a small number of sparsely configured camera views as input. Specifically, we adopt a simple Multi-Layer Perceptron (MLP) network as a universal function approximator to learn the plenoptic function at every position in the space of interest. By placing virtual viewpoints (dubbed `imaginary eyes') at thousands of randomly sampled locations and leveraging multi-view geometric relationship, we train the MLP to regress the plenoptic function for the space. Our network is trained on a per-scene basis, and the training time is relatively short (in the order of tens of minutes). When the model is converged, we can freely render novel images. Extensive experiments demonstrate that our method well approximates the complete plenoptic function and generates high-quality results."
  },
  "accv2022_main_causalpropertybasedanti-conflictmodelingwithhybriddataaugmentationforunbiasedscenegraphgeneration": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Causal Property based Anti-Conflict Modeling with Hybrid Data Augmentation for Unbiased Scene Graph Generation",
    "authors": [
      "Ruonan Zhang",
      "Gaoyun An"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Causal_Property_based_Anti-Conflict_Modeling_with_Hybrid_Data_Augmentation_for_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Causal_Property_based_Anti-Conflict_Modeling_with_Hybrid_Data_Augmentation_for_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Scene Graph Generation(SGG) aims to detect visual triplets of pairwise objects based on object detection. There are three key factors being explored to determine a scene graph: visual information, local and global context, and prior knowledge. However, conventional methods balancing losses among these factors lead to conflict, causing ambiguity, inaccuracy, and inconsistency. In this work, to apply evidence theory to scene graph generation, a novel plug-and-play Causal Property based Anti-conflict Modeling (CPAM) module is proposed, which models key factors by Dempster-Shafer evidence theory, and integrates quantitative information effectively. Compared with the existing methods, the proposed CPAM makes the training process interpretable, and also manages to cover more fine-grained relationships after inconsistencies reduction. Furthermore, we propose a Hybrid Data Augmentation (HDA) method, which facilitates data transfer as well as conventional debiasing methods to enhance the dataset. By combining CPAM with HDA, significant improvement has been achieved over the previous state-of-the-art methods. And extensive ablation studies have also been conducted to demonstrate the effectiveness of our method."
  },
  "accv2022_main_sac-ganfaceimageinpaintingwithspatial-awareattributecontrollablegan": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SAC-GAN : Face Image Inpainting with Spatial-aware Attribute Controllable GAN",
    "authors": [
      "Dongmin Cha",
      "Taehun Kim",
      "Joonyeong Lee",
      "Daijin Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cha_SAC-GAN__Face_Image_Inpainting_with_Spatial-aware_Attribute_Controllable_GAN_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cha_SAC-GAN__Face_Image_Inpainting_with_Spatial-aware_Attribute_Controllable_GAN_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The objective of image inpainting is refilling the masked area with semantically appropriate pixels and producing visually realistic images as an output. After the introduction of generative adversarial networks (GAN), many inpainting approaches are showing promising development. Several attempts have been recently made to control reconstructed output with the desired attribute on face images using exemplar images and style vectors. Nevertheless, conventional style vector has the limitation that to project style attribute representation onto linear vector without preserving dimensional information. We introduce spatial-aware attribute controllable GAN (SAC-GAN) for face image inpainting, which is effective for reconstructing masked images with desired controllable facial attributes with advantage of utilizing style tensors as spatial forms. Various experiments to control over facial characteristics demonstrate the superiority of our method compared with previous image inpainting methods."
  },
  "accv2022_main_revisitingunsuperviseddomainadaptationmodelsasmoothnessperspective": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Revisiting Unsupervised Domain Adaptation Models: a Smoothness Perspective",
    "authors": [
      "Xiaodong Wang",
      "Junbao Zhuo",
      "Mengru Zhang",
      "Shuhui Wang",
      "Yuejian Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Revisiting_Unsupervised_Domain_Adaptation_Models_a_Smoothness_Perspective_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Revisiting_Unsupervised_Domain_Adaptation_Models_a_Smoothness_Perspective_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Unsupervised Domain Adaptation (UDA) aims to leverage the labeled source data and unlabeled target data to generalize better in the target domain. UDA methods utilize better domain alignment or carefully-designed regularizations to increase the discriminability of target features. However, most methods focus on directly increasing the distance between cluster centers of target features, i.e., enlarging inter-class variance, which intuitively increases the discriminability of target features and is easy to implement. However, due to intra-class variance optimization being under-explored, there are still some samples of the same class are prone to be classified into several classes. To handle this problem, we aim to equip UDA methods with the high smoothness constraint. We first define the model's smoothness as the predictions similarity within each class, and propose a simple yet effective technique LeCo (impLicit smoothness Constraint) to promote the smoothness. We construct the weak and strong \"views\" of each target sample and enforce the model predictions of these two views to be consistent. Besides, a new uncertainty measure named Instance Class Confusion conditions the consistency is proposed to guarantee the transferability. LeCo implicitly reduces the model sensitivity to perturbations for target samples and guarantees smaller intra-class variance. Extensive experiments show that the proposed technique improves various baseline approaches by a large margin, and helps yield comparable results to the state-of-the-arts on four public datasets. Our codes are publicly available at https://github.com/Wang-Xiaodong1899/LeCo_UDA."
  },
  "accv2022_main_notend-to-endexploremulti-stagearchitectureforonlinesurgicalphaserecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition",
    "authors": [
      "Fangqiu Yi",
      "Yanfeng Yang",
      "Tingting Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yi_Not_End-to-End_Explore_Multi-Stage_Architecture_for_Online_Surgical_Phase_Recognition_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yi_Not_End-to-End_Explore_Multi-Stage_Architecture_for_Online_Surgical_Phase_Recognition_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Surgical phase recognition is of particular interest to computer assisted surgery systems, in which the goal is to predict what phase is occurring at each frame for a surgery video. Networks with multi-stage architecture have been widely applied in many computer vision tasks with rich patterns, where a predictor stage first outputs initial predictions and an additional refinement stage operates on the initial predictions to perform further refinement. Existing works show that surgical video contents are well ordered and contain rich temporal patterns, making the multi-stage architecture well suited for the surgical phase recognition task. However, we observe that when simply applying the multi-stage architecture to the surgical phase recognition task, the end-to-end training manner will make the refinement ability fall short of its wishes. To address the problem, we propose a new non end-to-end training strategy and explore different designs of multi-stage architecture for surgical phase recognition task. For the non end-to-end training strategy, the refinement stage is trained separately with proposed two types of disturbed sequences. Meanwhile, we evaluate three different choices of refinement models to show that our analysis and solution are robust to the choices of specific multi-stage models. We conduct experiments on two public benchmarks, the M2CAI16 Workflow Challenge and the Cholec80 dataset. The SOTA comparable results show that the multi-stage architecture holds the great potential to boost the performance of existing single-stage models. Code is available at https://github.com/ChinaYi/NETE."
  },
  "accv2022_main_clustercontrastforunsupervisedpersonre-identification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Cluster Contrast for Unsupervised Person Re-Identification",
    "authors": [
      "Zuozhuo Dai",
      "Guangyuan Wang",
      "Weihao Yuan",
      "Siyu Zhu",
      "Ping Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Dai_Cluster_Contrast_for_Unsupervised_Person_Re-Identification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Dai_Cluster_Contrast_for_Unsupervised_Person_Re-Identification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Thanks to the recent research development in contrastive learning, the gap of visual representation learning between supervised and unsupervised approaches has been gradually closed in the tasks of computer vision. In this paper, we focus on the downstream task of unsupervised person re-identification (re-ID). State-of-the-art unsupervised re-ID methods train the neural networks using a dictionary-based non-parametric softmax loss. They store the pre-computed instance feature vectors inside the dictionary, assign pseudo labels to them using clustering algorithm, and compare the query instances to the cluster using a form of contrastive loss. To enforce a consistent dictionary, that is the features in the dictionary are computed by a similar or the same encoder network,we present Cluster Contrast which stores feature vectors and computes contrastive loss at the cluster level. Moreover, the momentum update is introduced to reinforce the cluster-level feature consistency in the sequential space. Despite the straightforward design, experiments on four representative re-ID benchmarks demonstrate the effective performance of our method."
  },
  "accv2022_main_asimplestrategytoprovableinvarianceviaorbitmapping": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Simple Strategy to Provable Invariance via Orbit Mapping",
    "authors": [
      "Kanchana Vaishnavi Gandikota",
      "Jonas Geiping",
      "Zorah L\u00a8ahner",
      "Adam Czaplin \u0301ski",
      "Michael M \u0308oller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gandikota_A_Simple_Strategy_to_Provable_Invariance_via_Orbit_Mapping_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gandikota_A_Simple_Strategy_to_Provable_Invariance_via_Orbit_Mapping_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Many applications require robustness, or ideally invariance, of a neural network to certain transformations of input data. Most commonly, this requirement is addressed by either augmenting the training data, using adversarial training, or defining network architectures that include the desired invariance by design. In this work, we propose a method to make network architectures provably invariant with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. We analyze properties of such approaches, and demonstrate their advantages in terms of robustness and computational efficiency in several numerical examples. In particular, we investigate the robustness with respect to rotations of images (which can hold up to discretization artifacts) as well as the provable orientation and scaling invariance of 3D point cloud classification."
  },
  "accv2022_main_fromsparsetodensesemanticgraphevolutionaryhashingforunsupervisedcross-modalretrieval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "From Sparse to Dense: Semantic Graph Evolutionary Hashing for Unsupervised Cross-Modal Retrieval",
    "authors": [
      "Yang Zhao",
      "Jiaguo Yu",
      "Shengbin Liao",
      "Zheng Zhang",
      "Haofeng Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_From_Sparse_to_Dense_Semantic_Graph_Evolutionary_Hashing_for_Unsupervised_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_From_Sparse_to_Dense_Semantic_Graph_Evolutionary_Hashing_for_Unsupervised_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In recent years, cross-modal hashing has attracted an increasing attention due to its fast retrieval speed and low storage requirements. However, labeled datasets are limited in real application, and existing unsupervised cross-modal hashing algorithms usually employ heuristic geometric prior as semantics, which introduces serious deviations as the similarity score from original features cannot reasonably represent the relationships among instances. In this paper, we study the unsupervised deep cross-modal hash retrieval method and propose a novel Semantic Graph Evolutionary Hashing (SGEH) to solve the above problem. The key novelty of SGEH is its evolutionary affinity graph construction method. To be concrete, we explore the sparse similarity graph with clustering results, which evolve from fusing the affinity information from code-driven graph on intrinsic data and subsequently extends to dense hybrid semantic graph which restricts the process of hash code learning to learn more discriminative results. Moreover, the batch-inputs are chosen from edge set rather than vertexes for better exploring the original spatial information in the sparse graph. Experiments on four benchmark datasets demonstrate the superiority of our framework over the state-of-the-art unsupervised cross-modal retrieval methods."
  },
  "accv2022_main_lhdrhdrreconstructionforlegacycontentusingalightweightdnn": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "LHDR: HDR Reconstruction for Legacy Content using a Lightweight DNN",
    "authors": [
      "Cheng Guo",
      "Xiuhua Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Guo_LHDR_HDR_Reconstruction_for_Legacy_Content_using_a_Lightweight_DNN_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Guo_LHDR_HDR_Reconstruction_for_Legacy_Content_using_a_Lightweight_DNN_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "High dynamic range (HDR) image is widely-used in graphics and photography due to the rich information it contains. Recently the community has started using deep neural network (DNN) to reconstruct standard dynamic range (SDR) images into HDR. Albeit the superiority of current DNN-based methods, their application scenario is still limited: (1) heavy model impedes real-time processing, and (2) inapplicable to legacy SDR content with more degradation types. Therefore, we propose a lightweight DNN-based method trained to tackle legacy SDR. For better design, we reform the problem modeling and emphasize degradation model. Experiments show that our method reached appealing performance with minimal computational cost compared with others."
  },
  "accv2022_main_augmentingsoftmaxinformationforselectiveclassificationwithout-of-distributiondata": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data",
    "authors": [
      "Guoxuan Xia",
      "Christos-Savvas Bouganis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Combination (SIRC), that augments softmax-based confidence scores with feature-agnostic information such that their ability to identify OOD samples is improved without sacrificing separation between correct and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale datasets and convolutional neural network architectures show that SIRC is able to consistently match or outperform the baseline for SCOD, whilst existing OOD detection methods fail to do so. Code is available at https://github.com/Guoxoug/SIRC."
  },
  "accv2022_main_imagedenoisingusingconvolutionalsparsecodingnetworkwithdryfriction": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Image Denoising using Convolutional Sparse Coding Network with Dry Friction",
    "authors": [
      "Yali Zhang",
      "Xiaofan Wang",
      "Fengpin Wang",
      "Jinjia Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Image_Denoising_using_Convolutional_Sparse_Coding_Network_with_Dry_Friction_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Image_Denoising_using_Convolutional_Sparse_Coding_Network_with_Dry_Friction_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Convolutional sparse coding model has been successfully used in some tasks such as signal or image processing and classification. The recently proposedsupervised convolutional sparse coding network (CSCNet) model based on the Minimum Mean Square Error (MMSE) approximation shows the similar PSNR value for image denoising problem with state of the art methods while using much fewer parameters. The CSCNet uses the learning convolutional iterative shrinkage-thresholding algorithms (LISTA) based on the convolutional dictionary setting. However, LISTA methods are known to converge to local minima. In this paper we proposed one novel algorithm based on LISTA with dry friction, named LISTDFA. The dry friction enters the LISTDFA algorithm through proximal mapping. Due to the nature of dry friction, the LISTDFA algorithm is proven to converge in a finite time. The corresponding iterative neural network preserves the computational simplicity of the original CSCNet, and can reach a better local minima practically."
  },
  "accv2022_main_gaitstripgaitrecognitionviaeffectivestrip-basedfeaturerepresentationsandmulti-levelframework": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "GaitStrip: Gait Recognition via Effective Strip-based Feature Representations and Multi-Level Framework",
    "authors": [
      "Ming Wang",
      "Beibei Lin",
      "Xianda Guo",
      "Lincheng Li",
      "Zheng Zhu",
      "Jiande Sun",
      "Shunli Zhang",
      "Yu Liu",
      "Xin Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_GaitStrip_Gait_Recognition_via_Effective_Strip-based_Feature_Representations_and_Multi-Level_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_GaitStrip_Gait_Recognition_via_Effective_Strip-based_Feature_Representations_and_Multi-Level_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Many gait recognition methods first partition the human gait into N-parts and then combine them to establish part-based feature representations. Their gait recognition performance is often affected by partitioning strategies, which are empirically chosen in different datasets. However, we observe that strips as the basic component of parts are agnostic against different partitioning strategies. Motivated by this observation, we present a strip-based multi-level gait recognition network, named GaitStrip, to extract comprehensive gait information at different levels. To be specific, our high-level branch explores the context of gait sequences and our low-level one focuses on detailed posture changes. We introduce a novel StriP-Based feature extractor (SPB) to learn the strip-based feature representations by directly taking each strip of the human body as the basic unit.Moreover, we propose a novel multi-branch structure, called Enhanced Convolution Module (ECM), to extract different representations of gaits. ECM consists of the Spatial-Temporal feature extractor (ST), the Frame-Level feature extractor (FL) and SPB, and has two obvious advantages:First, each branch focuses on a specific representation, which can be used to improve the robustness of the network. Specifically, ST aims to extract spatial-temporal features of gait sequences, while FL is used to generate the feature representation of each frame. Second, the parameters of the ECM can be reduced in test by introducing a structural re-parameterization technique. Extensive experimental results demonstrate that our GaitStrip achieves state-of-the-art performance in both normal walking and complex conditions. The source code is published at https://github.com/M-Candy77/GaitStrip."
  },
  "accv2022_main_activedomainadaptationwithmulti-levelcontrastiveunitsforsemanticsegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Active Domain Adaptation with Multi-level Contrastive Units for Semantic Segmentation",
    "authors": [
      "Hao Zhang",
      "Ruimao Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Active_Domain_Adaptation_with_Multi-level_Contrastive_Units_for_Semantic_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Active_Domain_Adaptation_with_Multi-level_Contrastive_Units_for_Semantic_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "To further reduce the cost of semi-supervised domain adaptation (SSDA) labeling, a more effective way is to use active learning (AL) to annotate a selected subset with specific properties. However, DA tasks are always addressed in two interactive aspects: domain transfer and the enhancement of discrimination, which requires the selected data to be both uncertain under the model and diverse in feature space. Contrary to AL in classification tasks, it is usually challenging to select pixels that contain both the above properties in segmentation tasks, leading to the complex design of pixel selection strategy. To address such an issue, we propose a novel Active Domain Adaptation scheme with Multi-level Contrastive Units (ADA-MCU) for semantic segmentation. A simple pixel selection strategy followed with the construction of multi-level contrastive units is introduced to optimize the model for both domain adaptation and active supervised learning. In practice, MCUs are constructed from intra-image, cross-image, and cross-domain levels by using both labeled and unlabeled pixels. At each level, we define contrastive losses from center-to-center and pixel-to-pixel manners, with the aim of jointly aligning the category centers and reducing outliers near the decision boundaries. In addition, we also introduce a categories correlation matrix to implicitly describe the relationship between categories, which are used to adjust the weights of the losses for MCUs. Extensive experimental results show that the proposed method achieves competitive performance against state-of-the-art SSDA methods with 50% fewer labeled pixels and significantly outperforms state-of-the-art with a large margin by using the same level of annotation cost."
  },
  "accv2022_main_classconcentrationwithtwinvariationalautoencodersforunsupervisedcross-modalhashing": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Class Concentration with Twin Variational Autoencoders for Unsupervised Cross-modal Hashing",
    "authors": [
      "Yang Zhao",
      "Yazhou Zhu",
      "Shengbin Liao",
      "Qiaolin Ye",
      "Haofeng Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_Class_Concentration_with_Twin_Variational_Autoencoders_for_Unsupervised_Cross-modal_Hashing_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_Class_Concentration_with_Twin_Variational_Autoencoders_for_Unsupervised_Cross-modal_Hashing_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Multi-modal deep hash learning is arguably one of the most commonly used unsupervised methods in cross-modal retrieval tasks. Most existing deep hashing methods focus on maintaining similarity information in the hash code learning step. Although accurate and compact binary representations are learned, these methods fail to encourage discriminative learning of features. In this paper, we propose a new method called Class Concentrated Variational auto-encoder (CCTV) to learn discriminative hash codes. The novelty of CCTV lies in two aspects. First, the proposed method focuses on the concentration of the mean vector of latent features. Based on the assumption that the features in the shared latent space produce multivariate Gaussian, CCTV updates the mean vectors and the cluster centroids of the latent features at the same time by minimizing the class concentration loss, so as to narrow the distance between the cluster centroids and the mean vectors, and further make the concentration More compact. Secondly, under the constraints of raw similarity information, CCTV is different from previous works, it uses the mean vector of latent features as the representation of the images to reduce the influence of variance, and then embeds them in the Hamming space. Our experimental evaluation of four multimedia benchmarks shows a significant improvement over the state-of-the-art methods."
  },
  "accv2022_main_robustizingobjectdetectionnetworksusingaugmentedfeaturepooling": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Robustizing Object Detection Networks Using Augmented Feature Pooling",
    "authors": [
      "Takashi Shibata",
      "Masayuki Tanaka",
      "Masatoshi Okutomi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Shibata_Robustizing_Object_Detection_Networks_Using_Augmented_Feature_Pooling_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Shibata_Robustizing_Object_Detection_Networks_Using_Augmented_Feature_Pooling_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper presents a framework to robustize object detection networks against large geometric transformation. Deep neural networks rapidly and dramatically have improved object detection performance. Nevertheless, modern detection algorithms are still sensitive to large geometric transformation. Aiming at improving the robustness of the modern detection algorithms against the large geometric transformation, we propose a new feature extraction called augmented feature pooling. The key is to integrate the augmented feature maps obtained from the transformed images before feeding it to the detection head without changing the original network architecture. In this paper, we focus on rotation as a simple-yet-influential case of geometric transformation, while our framework is applicable to any geometric transformations. It is noteworthy that, with only adding a few lines of code from the original implementation of the modern object detection algorithms and applying simple fine-tuning, we can improve the rotation robustness of these original detection algorithms while inheriting modern network architectures' strengths. Our framework overwhelmingly outperforms typical geometric data augmentation and its variants used to improve robustness against appearance changes due to rotation. We construct a dataset based on MS COCO to evaluate the robustness of the rotation, called COCO-Rot. Extensive experiments on three datasets, including our COCO-Rot, demonstrate that our method can improve the rotation robustness of state-of-the-art algorithms."
  },
  "accv2022_main_learninginter-superpointaffinityforweaklysupervised3dinstancesegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation",
    "authors": [
      "Linghua Tang",
      "Le Hui",
      "Jin Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tang_Learning_Inter-Superpoint_Affinity_for_Weakly_Supervised_3D_Instance_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tang_Learning_Inter-Superpoint_Affinity_for_Weakly_Supervised_3D_Instance_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Weakly supervised 3D instance segmentation on point clouds has been rarely studied in recent years. Due to the few annotated labels of 3D point clouds, how to learn discriminative features of point clouds to segment object instances is a challenging problem. In this paper, we propose a simple but effective instance segmentation framework that can achieve striking performance by annotating only one point for each instance. Specifically, to tackle extremely few labels, we first oversegment the point cloud into superpoints in an unsupervised manner and extend the point-level annotations to the superpoint level. Then, based on the superpoint graph, we propose an inter-superpoint affinity mining module that considers the semantic and spatial relations to adaptively learns inter-superpoint affinity to generate high-quality pseudo labels via random walk. Finally, we propose a volume-aware instance refinement module to segment high-quality instances by applying volume constraints of objects in clustering on the superpoint graph. Extensive experiments on the ScanNet-v2 and S3DIS datasets demonstrate that our method achieves state-of-the-art performance in the weakly supervised point cloud instance segmentation task, and even outperforms some fully supervised methods."
  },
  "accv2022_main_pathtrcontext-awarememorytransformerfortumorlocalizationingigapixelpathologyimages": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PathTR: Context-Aware Memory Transformer for Tumor Localization in Gigapixel Pathology Images",
    "authors": [
      "Wenkang Qin",
      "Rui Xu",
      "Shan Jiang",
      "Tingting Jiang",
      "Lin Luo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Qin_PathTR_Context-Aware_Memory_Transformer_for_Tumor_Localization_in_Gigapixel_Pathology_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Qin_PathTR_Context-Aware_Memory_Transformer_for_Tumor_Localization_in_Gigapixel_Pathology_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "With the development of deep learning and computation pathology, whole-slide images (WSIs) are wildly used in clinical diagnosis. The WSI, which refers to the scanning of conventional glass slides in order to produce digital slides, usually has gigapixels. Most existing methods in computer vision process WSIs as many patches. The model infers patch by patch to get the results on WSI, which loses the global context of WSI. In this paper, we developed PATHology TRansformer (PathTR), which fully uses the global information of WSI. In PathTR, the local context is aggregated by the self-attention mechanism. We further design a recursive mechanism to encode the global context in extra states. In tumor detection of metastases of lymph node sections for breast cancer,we got the FROC score of 87.68% which outperforms the baseline and NCRF method with +8.99% and +7.08%, respectively. We highlight that we also achieve a significant 94.25% sensitivity at 8 false positives per image."
  },
  "accv2022_main_semi-supervisedbreastlesionsegmentationusinglocalcrosstripletlossforultrafastdynamiccontrast-enhancedmri": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Semi-supervised Breast Lesion  Segmentation using Local Cross Triplet Loss for Ultrafast Dynamic Contrast-Enhanced MRI",
    "authors": [
      "YoungTack Oh",
      "Eun Sook Ko",
      "Hyunjin Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Oh_Semi-supervised_Breast_Lesion__Segmentation_using_Local_Cross_Triplet_Loss_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Oh_Semi-supervised_Breast_Lesion__Segmentation_using_Local_Cross_Triplet_Loss_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) and its fast variant, ultrafast DCE-MRI, are useful for the management of breast cancer. Segmentation of breast lesions is necessary for automatic clinical decision support. Despite the advantage of acquisition time, existing segmentation studies on ultrafast DCE-MRI are scarce, and they are mostly fully supervised studies with high annotation costs. Herein, we propose a semi-supervised segmentation approach that can be trained with small amounts of annotations for ultrafast DCE-MRI. A time difference map is proposed to incorporate the distinct time-varying enhancement pattern of the lesion. Furthermore, we present a novel loss function that efficiently distinguishes breast lesions from non-lesions based on triple loss. This loss reduces the potential false positives induced by the time difference map. Our approach is compared to that of five competing methods using the dice similarity coefficient and two boundary-based metrics. Compared to other models, our approach achieves better segmentation results using small amounts of annotations, especially for boundary-based metrics relevant to spatially continuous breast lesions. An ablation study demonstrates the incremental effects of our study. Our code is available on GitHub (https://github.com/yt- oh96/SSL-CTL)."
  },
  "accv2022_main_qs-craftlearningtoquantize,scrabbleandcraftforconditionalhumanmotionanimation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "QS-Craft: Learning to Quantize, Scrabble and Craft for  Conditional Human Motion Animation",
    "authors": [
      "Yuxin Hong",
      "Xuelin Qian",
      "Simian Luo",
      "Guodong Guo",
      "Xiangyang Xue",
      "Yanwei Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hong_QS-Craft_Learning_to_Quantize_Scrabble_and_Craft_for__Conditional_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hong_QS-Craft_Learning_to_Quantize_Scrabble_and_Craft_for__Conditional_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper studies the task of conditional Human Motion Animation (cHMA). Given a source image and a driving video, the model should animate the new frame sequence, in which the person in the source image should perform a similar motion as the pose sequence from the driving video. Despite the success of Generative Adversarial Network (GANs) methods in image and video synthesis,it is still very challenging to conduct cHMA due to the difficulty in efficiently utilizing the conditional guided information such as images or poses, and generating images of good visual quality. To this end, this paper proposes a novelmodel of learning to Quantize, Scrabble, and Craft (QS-Craft) for conditional human motion animation. The key novelties come from the newly introduced three key steps:quantize, scrabble and craft. Particularly, our QS-Craft employs transformer in its structure to utilize theattention architectures. The guidedinformation is represented as a pose coordinate sequence extracted from the driving videos. Extensive experiments on human motion datasets validate the efficacy of our model."
  },
  "accv2022_main_patchflowatwo-stagepatch-basedapproachforlightweightopticalflowestimation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PatchFlow: A Two-Stage Patch-Based Approach for Lightweight Optical Flow Estimation",
    "authors": [
      "Ahmed Hammad Alhawwary",
      "Janne Mustaniemi",
      "Janne Heikkila"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Alhawwary_PatchFlow_A_Two-Stage_Patch-Based_Approach_for_Lightweight_Optical_Flow_Estimation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Alhawwary_PatchFlow_A_Two-Stage_Patch-Based_Approach_for_Lightweight_Optical_Flow_Estimation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The deep learning-based optical flow methods have shown noticeable advancements in flow estimation. The dense optical flow map offers high flexibility and quality for aligning neighbouring video frames. However, they are computationally expensive, and the memory requirements for processing high-resolution images such as 2K, 4K and 8K on resources-limited devices such as mobile phones can be prohibitive. We propose a patch-based approach for optical flow estimation. We redistribute the regular CNN-based optical flow regression into a two-stage pipeline, where the first stage estimates an optical flow for a low-resolution image version. The pre-flow is input to the second stage, where the high-resolution image is partitioned into small patches for optical flow refinement. With such a strategy, it becomes possible to process high-resolution images when the memory requirements are not sufficient. On the other hand, this solution also offers the ability to parallelize the optical flow estimation when possible. Furthermore, we show that such a pipeline can additionally allow for utilizing a lighter and shallower model in the two stages. It can perform on par with FastFlowNet (FFN) while being 1.7x faster computationally and with almost a half of the parameters. Against the state-of-the-art optical flow methods, the proposed solution can show a reasonable accuracy trade-off for running time and memory requirements."
  },
  "accv2022_main_multi-granularitytransformerforimagesuper-resolution": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-granularity Transformer for Image Super-resolution",
    "authors": [
      "Yunzhi Zhuge",
      "Xu Jia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhuge_Multi-granularity_Transformer_for_Image_Super-resolution_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhuge_Multi-granularity_Transformer_for_Image_Super-resolution_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, transformers have made great success in computer vision. Thus far, most of those works focus on high-level tasks, e.g., image classification and object detection, and fewer attempts were made to solve low-level problems. In this work, we tackle image super-resolution. Specifically, transformer architectures with multi-granularity transformer groups are explored for complementary information interaction, to improve the accuracy of super-resolution. We exploit three transformer patterns, i.e., the window transformers, dilated transformers and global transformers. We further investigate the combination of them and propose a Multi-granularity Transformer (MugFormer). Specifically, the window transformer layer is aggregated with other transformer layers to compose three transformer groups, namely, Local Transformer Group, Dilated Transformer Group and Global Transformer Group, which efficiently aggregate both local and global information for accurate reconstruction. Extensive experiments on five benchmark datasets demonstrate that our MugFormer performs favorably against state-of-the-art methods in terms of both quantitative and qualitative against state-of-the-art methods in terms of both quantitative and qualitative results."
  },
  "accv2022_main_multi-scaleresidualinteractionforrgb-dsalientobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-scale Residual Interaction for RGB-D Salient Object Detection",
    "authors": [
      "Mingjun Hu",
      "Xiaoqin Zhang",
      "Li Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hu_Multi-scale_Residual_Interaction_for_RGB-D_Salient_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hu_Multi-scale_Residual_Interaction_for_RGB-D_Salient_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "RGB-D salient object detection (SOD) is used to detect the most attractive object in the scene. There is a problem in front of the existing RGB-D SOD task: how to integrate the different context information between the RGB and depth map effectively. In this work, we propose the Siamese Residual Interactive Refinement Network (SiamRIR) equipped with the encoder and decoder to handle the above problem. Concretely, we adopt the Siamese Network shared parameters to encode two modalities and fuse them during decoding phase. Then, we design the Multi-scale Residual Interavtive Refinement Block (RIRB) which contains Residual Interactive Module (RIM) and Residual Refinement Module (RRM). This block utilizes the multi-type cues to fuse and refine features, where RIM takes interaction between modalities to integrate the complementary regions with residual manner, and RRM refines features during fusion phase by incorporating spatial detail context with multi-scale manner. Extensive experiments on five benchmarks demonstrate that our method outperforms the state-of-the-art RGB-D SOD methods both quantitatively and qualitatively."
  },
  "accv2022_main_fine-grainedimagestyletransferwithvisualtransformers": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Fine-Grained Image Style Transfer with Visual Transformers",
    "authors": [
      "Jianbo Wang",
      "Huan Yang",
      "Jianlong Fu",
      "Toshihiko Yamasaki",
      "Baining Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Fine-Grained_Image_Style_Transfer_with_Visual_Transformers_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Fine-Grained_Image_Style_Transfer_with_Visual_Transformers_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "With the development of the convolutional neural network, image style transfer has drawn increasing attention. However, most existing approaches adopt a global feature transformation to transfer style patterns into content images (e.g., AdaIN and WCT). Such a design usually destroys the spatial information of the input images and fails to transfer fine-grained style patterns into style transfer results. To solve this problem, we propose a novel STyle TRansformer (STTR) network which breaks both content and style images into visual tokens to achieve a fine-grained style transformation. Specifically, two attention mechanisms are adopted in our STTR. We first propose to use self-attention to encode content and style tokens such that similar tokens can be grouped and learned together. We then adopt cross-attention between content and style tokens that encourages fine-grained style transformations. To compare STTR with existing approaches, we conduct user studies on Amazon Mechanical Turk (AMT), which are carried out with 50 human subjects with 1,000 votes in total. Extensive evaluations demonstrate the effectiveness and efficiency of the proposed STTR in generating visually pleasing style transfer results."
  },
  "accv2022_main_rgbroadscenematerialsegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "RGB Road Scene Material Segmentation",
    "authors": [
      "Sudong Cai",
      "Ryosuke Wakaki",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cai_RGB_Road_Scene_Material_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cai_RGB_Road_Scene_Material_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We address RGB road scene material segmentation, i.e., per-pixel segmentation of materials in real-world driving views with pure RGB images, by building a new tailored benchmark dataset and model for it. Our new dataset, KITTI-Materials, based on the well-established KITTI dataset, consists of 1000 frames covering 24 different road scenes of urban/suburban landscapes, annotated with one of 20 material categories for every pixel in high quality. It is the first dataset tailored to RGB material segmentation in realistic driving scenes which allows us to train and test any RGB material segmentation model. Based on an analysis on KITTI-Materials, we identify the extraction and fusion of texture and context as the key to robust road scene material appearance. We introduce Road scene Material Segmentation Network (RMSNet), a new Transformer-based framework which will serve as a baseline for this challenging task. RMSNet encodes multi-scale hierarchical features with self-attention. We construct the decoder of RMSNet based on a novel lightweight self-attention model, which we refer to as SAMixer. SAMixer achieves adaptive fusion of informative texture and context cues across multiple feature levels. It also significantly accelerates self-attention for feature fusion with a balanced query-key similarity measure. We also introduce a built-in bottleneck of local statistics to achieve further efficiency and accuracy. Extensive experiments on KITTI-Materials validate the effectiveness of our RMSNet. We believe our work lays a solid foundation for further studies on RGB road scene material segmentation."
  },
  "accv2022_main_uhdunderwaterimageenhancementviafrequency-spatialdomainawarenetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "UHD Underwater Image Enhancement via Frequency-Spatial Domain Aware Network",
    "authors": [
      "Yiwen Wei",
      "Zhuoran Zheng",
      "Xiuyi Jia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wei_UHD_Underwater_Image_Enhancement_via_Frequency-Spatial_Domain_Aware_Network_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wei_UHD_Underwater_Image_Enhancement_via_Frequency-Spatial_Domain_Aware_Network_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Currently, carrying ultra high definition (UHD) imaging equipment to record rich environmental conditions in deep water has become a hot issue in underwater exploration. However, due to the poor light transmission in deep water spaces and the large number of impurity particles, UHD underwater imaging is often plagued by low contrast and blur. To overcome these challenges, we propose an efficient two-path model (UHD-SFNet) that recovers the color and the texture of an underwater blurred image in the frequency and the spatial domains. Specifically, the method consists of two branches: in the first branch, we use a bilateral enhancement pipeline that extracts the frequency domain information of a degraded image to reconstruct clear textures. In the pipeline, we embed 1D convolutional layers in the MLP-based framework to capture the local characteristics of the token sequence. In the second branch, we develop U-RSGNet to capture the color features of the image after Gaussian blurring to generate a feature map rich in color information. Finally, the extracted texture features are fused with the color features to produce a clear underwater image. In addition, to construct paired high-quality underwater image enhancement dataset, we propose UHD-CycleGAN with the help of domain adaptation to produce more realistic UHD synthetic images. Experimental results show that our algorithm outperforms existing methods significantly in underwater image enhancement on a single GPU with 24G RAM. Codes are available at https://github.com/wyw0112/UHD-SFNet."
  },
  "accv2022_main_basslboundary-awareself-supervisedlearningforvideoscenesegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "BaSSL: Boundary-aware Self-Supervised Learning for Video Scene Segmentation",
    "authors": [
      "Jonghwan Mun",
      "Minchul Shin",
      "Gunsoo Han",
      "Sangho Lee",
      "Seongsu Ha",
      "Joonseok Lee",
      "Eun-Sol Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Mun_BaSSL_Boundary-aware_Self-Supervised_Learning_for_Video_Scene_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Mun_BaSSL_Boundary-aware_Self-Supervised_Learning_for_Video_Scene_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Self-supervised learning has drawn attention through its effectiveness in learning in-domain representations with no ground-truth annotations; in particular, it is shown that properly designed pretext tasks bring significant performance gains for downstream tasks. Inspired from this, we tackle video scene segmentation, which is a task of temporally localizing scene boundaries in a long video, with a self-supervised learning framework where we mainly focus on designing effective pretext tasks. In our framework, given a long video, we adopt a sliding window scheme; from a sequence of shots in each window, we discover a moment with a maximum semantic transition and leverage it as pseudo-boundary to facilitate the pre-training. Specifically, we introduce three novel boundary-aware pretext tasks: 1) Shot-Scene Matching (SSM), 2) Contextual Group Matching (CGM) and 3) Pseudo-boundary Prediction (PP); SSM and CGM guide the model to maximize intra-scene similarity and inter-scene discrimination by capturing contextual relation between shots while PP encourages the model to identify transitional moments. We perform an extensive analysis to validate effectiveness of our method and achieve the new state-of-the-art on the MovieNet-SSeg benchmark. The code is available at https://github.com/kakaobrain/bassl"
  },
  "accv2022_main_coil-agnosticattention-basednetworkforparallelmrireconstruction": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Coil-Agnostic Attention-Based Network for Parallel MRI Reconstruction",
    "authors": [
      "Jingshuai Liu",
      "Chen Qin",
      "Mehrdad Yaghoobi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Liu_Coil-Agnostic_Attention-Based_Network_for_Parallel_MRI_Reconstruction_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Liu_Coil-Agnostic_Attention-Based_Network_for_Parallel_MRI_Reconstruction_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Magnetic resonance imaging (MRI) is widely used in clinical diagnosis. However, as a slow imaging modality, the long scan time hinders its development in time-critical applications. The acquisition process can be accelerated by types of under-sampling strategies in k-space and reconstructing images from a few measurements. To reconstruct the image, many parallel imaging methods use the coil sensitivity maps to fold multiple coil images with model-based or deep learning-based estimation methods. However, they can potentially suffer from the inaccuracy of sensitivity estimation. In this work, we propose a novel coil-agnostic attention-based framework for multi-coil MRI reconstruction which completely avoids the sensitivity estimation and performs data consistency (DC) via a sensitivity-agnostic data aggregation consistency block (DACB). Experiments were performed on the FastMRI knee dataset and show that the proposed DACB and attention module-integrated framework outperforms other deep learning-based algorithms in terms of image quality and reconstruction accuracy. Ablation studies also indicate the superiority of DACB over conventional DC methods."
  },
  "accv2022_main_multi-viewcoupledself-attentionnetworkforpulmonarynodulesclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-View Coupled Self-Attention Network for Pulmonary Nodules Classification",
    "authors": [
      "Qikui Zhu",
      "Yanqing Wang",
      "Xiangpeng Chu",
      "Xiongwen Yang",
      "Wenzhao Zhong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhu_Multi-View_Coupled_Self-Attention_Network_for_Pulmonary_Nodules_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhu_Multi-View_Coupled_Self-Attention_Network_for_Pulmonary_Nodules_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Evaluation of the malignant degree of pulmonary nodules plays an important role in early detecting lung cancer.Deep learning-based methods have obtained promising results in this domain with their effectiveness in learning feature representation. Both local and global features are crucial for medical image classification tasks, particularly for 3D medical image data, however, the receptive field of convolution kernel limits the global feature learning. Although self-attention mechanism can success fully model long-range dependencies by directly flattening the input image to a sequence, which has high computational complexity. Additionally, which unable to model the image local context information across spatial and depth dimensions. To address the above challenges, in this paper,we carefully design a Multi-View Coupled Self-Attention Module (MVCS). Specifically, a novel self-attention module is proposed to model spatial and dimensional correlations sequentially for learning global spatial contexts and further improving the identification accuracy. Compared withvanilla self-attention, which have three-fold advances: 1) uses fewer memory consumption and computational complexity than the existing self-attention methods; 2) except for exploiting the correlations along the spatial and channel dimension, the dimension correlations are also exploited; 3) the proposed self-attention module can be easily integrated with other frameworks. By adding the proposed module into 3D ResNet50, we build a classification network for lung nodules' malignancy evaluation. The nodule classification network was validated on a public dataset from LIDC-IDRI. Extensive experimental results demonstrate that our proposed model has performance comparable to state-of-the-art approaches."
  },
  "accv2022_main_affinity-awarerelationnetworkfororientedobjectdetectioninaerialimages": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Affinity-Aware Relation Network for Oriented Object Detection in Aerial Images",
    "authors": [
      "Tingting Fang",
      "Bin Liu",
      "Zhiwei Zhao",
      "Qi Chu",
      "Nenghai Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Fang_Affinity-Aware_Relation_Network_for_Oriented_Object_Detection_in_Aerial_Images_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Fang_Affinity-Aware_Relation_Network_for_Oriented_Object_Detection_in_Aerial_Images_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Object detection in aerial images is a challenging task due to the oriented and densely packed objects. However, densely packed objects constitute a significant characteristic of aerial images: objects are not randomly scattered around in images but in groups sharing similar orientations. Such a recurring pattern of object arrangement could enhance the rotated features and improve the detection performance. This paper proposes a novel and flexible Affinity-Aware Relation Network based on two-stage detectors. Specifically, an affinity-graph construction module is adopted to measure the affinity among objects and to select bounding boxes sharing high similarity with the reference box. Furthermore, we design a dynamic enhancement module, which uses the attention to learn neighbourhood message and dynamically determines weights for feature enhancement. Finally, we conduct experiments on several public benchmarks and achieve notable AP improvements as well as state-of-the-art performances on DOTA, HRSC2016 and UCAS-AOD datasets."
  },
  "accv2022_main_spatial-temporaladaptivegraphconvolutionalnetworkforskeleton-basedactionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Spatial-Temporal Adaptive Graph Convolutional Network for Skeleton-based Action Recognition",
    "authors": [
      "Rui Hang",
      "Minxian Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hang_Spatial-Temporal_Adaptive_Graph_Convolutional_Network_for_Skeleton-based_Action_Recognition_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hang_Spatial-Temporal_Adaptive_Graph_Convolutional_Network_for_Skeleton-based_Action_Recognition_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Skeleton-based action recognition approaches usually construct the skeleton sequence as spatial-temporal graphs and perform graph convolution on these graphs to extract discriminative features. However, due to the fixed topology shared among different poses and the lack of direct long-range temporal dependencies, it is not trivial to learn the robust spatial-temporal feature. Therefore, we present a spatial-temporal adaptive graph convolutional network (STA-GCN) to learn adaptive spatial and temporal topologies and effectively aggregate features for skeletonbased action recognition. The proposed network is composed of spatial adaptive graph convolution (SA-GC) and temporal adaptive graph convolution (TA-GC) with an adaptive topology encoder. The SA-GC can extract the spatial feature for each pose with the spatial adaptive topology, while the TA-GC can learn the temporal feature by modeling adaptively the direct long-range temporal dependencies. On three large-scale skeleton action recognition datasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton, the STA-GCN outperforms the existing stateof-the-art methods."
  },
  "accv2022_main_actionrepresentingbyconstrainedconditionalmutualinformation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Action Representing by Constrained Conditional Mutual Information",
    "authors": [
      "Haoyuan Gao",
      "Yifan Zhang",
      "Linhui Sun",
      "Jian Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_Action_Representing_by_Constrained_Conditional_Mutual_Information_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_Action_Representing_by_Constrained_Conditional_Mutual_Information_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Contrastive learning achieves a remarkable performance for representation learning by constructing the InfoNCE loss function. It enables learned representations to describe the invariance in data transformation without labels. Contrastive learning also been employed in self-supervised learning of action recognition. However, this kind of method fails to introduce assumptions according to human knowledge about the prior distribution of representations in the training process.For solving this problem, this paper proposes a self-supervised learning framework, which can achieve different self-supervised learning methods by choosing different assumptions about the prior distribution of representations, while still learning the description of invariance in data transformation as contrastive learning. This framework minimizes the CCMI (Constrained Conditional Mutual Information) loss function, which represents the conditional mutual information between input augmented samples of the same sample and the output representations of the encoder while the prior distribution of representations is constrained. By theoretical analysis of the framework, it is proved that traditional contrastive learning by InfoNCE is a special case without human knowledge constraint of this framework. The Gaussian Mixture Model on Unit Hyper-sphere is chosen as the representation prior distribution to achieve the self-supervised method called CoMInG. Compared with the existing methods, the performance of the learned representation by this method in the downstream task of action recognition is significantly improved."
  },
  "accv2022_main_adeladaptivedistributioneffective-matchingmethodforguidinggeneratorsofgans": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "ADEL: Adaptive Distribution Effective-matching Method for Guiding Generators of GANs",
    "authors": [
      "Jungeun Kim",
      "Jeongeun Park",
      "Ha Young Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kim_ADEL_Adaptive_Distribution_Effective-matching_Method_for_Guiding_Generators_of_GANs_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kim_ADEL_Adaptive_Distribution_Effective-matching_Method_for_Guiding_Generators_of_GANs_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Research on creating high-quality, realistic fake images has engendered immense improvement in GANs. However, GAN training is still subject to mode collapse or vanishing gradient problems. To address these issues, we propose an adaptive distribution effective-matching method (ADEL) that sustains the stability of training and enables high performance by ensuring that the training abilities of the generator and discriminator are maintained in balance without bias in either direction. ADEL can help the generator's training by matching the difference between the distribution of real and fake images. As training is ideal when the discriminator and generator are in a balanced state, ADEL works when it is out of a certain optimal range based on the loss value. Through this, ADEL plays an important role in guiding the generator to create images similar to real images in the early stage when training is difficult. As training progresses, it naturally decays and gives model more freedom to generate a variety of images. ADEL can be applied to a variety of loss functions such as Kullback-Liebler divergence loss, Wasserstein loss, and Least-squares loss. Through extensive experiments, we show that ADEL improves the performance of diverse models such as DCGAN, WGAN, WGAN-GP, LSGAN, and StyleGANv2 upon five datasets, including low-resolution (CIFAR-10 and STL-10) as well as high-resolution (LSUN-Bedroom, Church, and ImageNet) datasets. Our proposed method is very simple and has a low computational burden, so it is expandable and can be used for diverse models."
  },
  "accv2022_main_3d-yogaa3dyogadatasetforvisual-basedhierarchicalsportsactionanalysis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "3D-Yoga: A 3D Yoga Dataset for Visual-based Hierarchical Sports Action Analysis",
    "authors": [
      "Jianwei Li",
      "Haiqing Hu",
      "Jinyang Li",
      "Xiaomei Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_3D-Yoga_A_3D_Yoga_Dataset_for_Visual-based_Hierarchical_Sports_Action_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_3D-Yoga_A_3D_Yoga_Dataset_for_Visual-based_Hierarchical_Sports_Action_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Visual-based human action analysis is an important research topic in the field of computer vision, and has great application prospect in sports performance analysis. Currently available 3D action analysis datasets have a number of limitations in sports application, including the lack of special sports actions, distinct class or score labels and variety of samples. Existing researches mainly use various special RGB videos for sports action analysis, but analysis with 2D features is less effective than 3D representation. In this paper, we introduce a new 3D yoga pose dataset (3D-Yoga) with more than 3,792 action samples and 16,668 RGB-D key frames, collected from 22 subjects performing 117 kinds of yoga poses with two RGB-D cameras. We have reconstructed 3D yoga poses with sparse multi-view data and carried out experiments with the proposed cascade two-stream adaptive graph convolutional neural network (Cascade 2S-AGCN) to recognize and assess these poses. Experimental results have shown the advantage of applying our 3D skeleton fusion and hierarchical analysis methods on 3D-Yoga, and the accuracy of Cascade 2S-AGCN outperforms the state-of-the-art methods. The introduction of 3D-Yoga will enable the community to apply, develop and adapt various methods for visual-based sports activity analysis."
  },
  "accv2022_main_havithybrid-attentionbasedvisiontransformerforvideoclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "HaViT: Hybrid-attention based Vision Transformer for Video Classification",
    "authors": [
      "Li Li",
      "Liansheng Zhuang",
      "Shenghua Gao",
      "Shafei Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_HaViT_Hybrid-attention_based_Vision_Transformer_for_Video_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_HaViT_Hybrid-attention_based_Vision_Transformer_for_Video_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Video transformers have become a promising tool for video classification due to its great success in modeling long-range interactions through the self-attention operation. However, existing transformer models only exploit the patch dependencies within a video when doing self-attention, while ignoring the patch dependencies across different videos. This paper argues that external patch prior information is beneficial to the performance of video transformer models for video classification. Motivated by this assumption, this paper proposes a novel Hybrid-attention based Vision Transformer (HaViT) model for video classification, which explicitly exploits both internal patch dependencies within a video and external patch dependencies across videos. Different from existing self-attention, the hybrid-attention is computed based on internal patch tokens and an external patch token dictionary which encodes external patch prior information across different videos. Experiments on Kinetics-400, Kinetics-600 and Something-something-v2 show that our HaViT model achieves state-of-the-art performance in the video classification task against existing methods. Moreover, experiments show that our proposed hybrid-attention scheme can be integrated into existing video transformer models to improve the performance."
  },
  "accv2022_main_filterpruningviaautomaticpruningratesearch": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Filter Pruning via Automatic Pruning Rate Search",
    "authors": [
      "Qiming Sun",
      "Shan Cao",
      "Zhixiang Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Sun_Filter_Pruning_via_Automatic_Pruning_Rate_Search_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Sun_Filter_Pruning_via_Automatic_Pruning_Rate_Search_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Model pruning is important for deploying models on devices with limited resources. However, the searching of optimal pruned model is still a significant challenge due to the large space to be exploited. In this paper, we propose an Automatic Pruning Rate Search(APRS) method to achieve automatic pruning. We reveal the connection between the model performance and Wasserstein distance to automatic searching optimal pruning rate. To reduce the search space, we quantify the sensitivity of each filter layer by layer and reveal the connection between model performance and Wasserstein distance. We introduce an end-to-end optimization method called Pareto plane to automatically search for the pruning rate to fit the overall size of the model. APRS can obtain more compact and efficient pruning models. To verify the effectiveness of our method, we conduct extensive experiments on ResNet, VGG and DenseNet, and the results show that our method outperforms the state-of-the-art methods under different parameter settings."
  },
  "accv2022_main_isanobject-centricvideorepresentationbeneficialfortransfer?": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Is an Object-Centric Video Representation Beneficial for Transfer?",
    "authors": [
      "Chuhan Zhang",
      "Ankush Gupta",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Is_an_Object-Centric_Video_Representation_Beneficial_for_Transfer_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Is_an_Object-Centric_Video_Representation_Beneficial_for_Transfer_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The objective of this work is to learn an object-centric video representation, withthe aim of improving transferability to novel tasks, ie, tasks different from the pre-training task of action classification.To this end, we introduce a new object-centric video recognition model based on a transformer architecture.The model learns a set of object-centricsummary vectors for the video, and uses these vectors to fuse the visual and spatio-temporal trajectory`modalities' of the video clip.We also introduce a novel trajectory contrast loss to further enhance objectness in these summary vectors. With experiments on four datasets -- SomethingSomething-V2, SomethingElse, Action Genome and EpicKitchens -- we show that the object-centric model outperforms prior video representations (both object-agnostic and object-aware), when: (1) classifying actions on unseen objects and unseen environments;(2) low-shot learning to novel classes; (3) linear probe to other downstream tasks;as well as (4) for standard action classification."
  },
  "accv2022_main_matchformerinterleavingattentionintransformersforfeaturematching": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "MatchFormer: Interleaving Attention in Transformers for Feature Matching",
    "authors": [
      "Qing Wang",
      "Jiaming Zhang",
      "Kailun Yang",
      "Kunyu Peng",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_MatchFormer_Interleaving_Attention_in_Transformers_for_Feature_Matching_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_MatchFormer_Interleaving_Attention_in_Transformers_for_Feature_Matching_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Local feature matching is a computationally intensive task at the subpixel level. While detector-based methods coupled with feature descriptors struggle in low-texture scenes, CNN-based methods with a sequential extract-to-match pipeline, fail to make use of the matching capacity of the encoder and tend to overburden the decoder for matching. In contrast, we propose a novel hierarchical extract-and-match transformer, termed as MatchFormer. Inside each stage of the hierarchical encoder, we interleave self-attention for feature extraction and cross-attention for feature matching, enabling a human-intuitive extract-and-match scheme. Such a match-aware encoder releases the overloaded decoder and makes the model highly efficient. Further, combining self- and cross-attention on multi-scale features in a hierarchical architecture improves matching robustness, particularly in low-texture indoor scenes or with less outdoor training data. Thanks to such a strategy, MatchFormer is a multi-win solution in efficiency, robustness, and precision. Compared to the previous best method in indoor pose estimation, our lite MatchFormer has only 45% GFLOPs, yet achieves a +1.3% precision gain and a 41% running speed boost. The large MatchFormer reaches state-of-the-art on four different benchmarks, including indoor pose estimation (ScanNet), outdoor pose estimation (MegaDepth), homography estimation and image matching (HPatch), and visual localization (InLoc)."
  },
  "accv2022_main_mgtrend-to-endmutualgazedetectionwithtransformer": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "MGTR: End-to-End Mutual Gaze Detection with Transformer",
    "authors": [
      "Hang Guo",
      "Zhengxi Hu",
      "Jingtai Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Guo_MGTR_End-to-End_Mutual_Gaze_Detection_with_Transformer_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Guo_MGTR_End-to-End_Mutual_Gaze_Detection_with_Transformer_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "People's looking at each other or mutual gaze is ubiquitous in our daily interactions, and detecting mutual gaze is of great significance for understanding human social scenes. Current mutual gaze detection methods focus on two-stage methods, whose inference speed is limited by the two-stage pipeline and the performance in the second stage is affected by the first one. In this paper, we propose a novel one-stage mutual gaze detection framework called Mutual Gaze TRansformer or MGTR to perform mutual gaze detection in an end-to-end manner. By designing mutual gaze instance triples, MGTR can detect each human head bounding box and simultaneously infer mutual gaze relationship based on global image information, which streamlines the whole process with simplicity. Experimental results on two mutual gaze datasets show that our method is able to accelerate mutual gaze detection process without losing performance. Ablation study shows that different components of MGTR can capture different levels of semantic information in images. Code is available at https://github.com/Gmbition/MGTR."
  },
  "accv2022_main_learningvideo-independenteyecontactsegmentationfromin-the-wildvideos": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning Video-independent Eye Contact Segmentation from In-the-Wild Videos",
    "authors": [
      "Tianyi Wu",
      "Yusuke Sugano"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wu_Learning_Video-independent_Eye_Contact_Segmentation_from_In-the-Wild_Videos_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wu_Learning_Video-independent_Eye_Contact_Segmentation_from_In-the-Wild_Videos_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Human eye contact is a form of non-verbal communication and can have a great influence on social behavior. Since the location and size of the eye contact targets vary across different videos, learning a generic video-independent eye contact detector is still a challenging task. In this work, we address the task of one-way eye contact detection for videos in the wild. Our goal is to build a unified model that can identify when a person is looking at his gaze targets in an arbitrary input video. Considering that this requires time-series relative eye movement information, we propose to formulate the task as a temporal segmentation. Due to the scarcity of labeled training data, we further propose a gaze target discovery method to generate pseudo-labels for unlabeled videos, which allows us to train a generic eye contact segmentation model in an unsupervised way using in-the-wild videos. To evaluate our proposed approach, we manually annotated a test dataset consisting of 52 videos of human conversations. Experimental results show that our eye contact segmentation model outperforms the previous video-dependent eye contact detector and can achieve 71.88% framewise accuracy on our annotated test set. Our code and evaluation dataset are available at https://github.com/ut-vision/Video-Independent-ECS."
  },
  "accv2022_main_aonetattentionalocclusion-awarenetworkforoccludedpersonre-identification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "AONet: Attentional Occlusion-aware Network for Occluded Person Re-identification",
    "authors": [
      "Guangyu Gao",
      "Qianxiang Wang",
      "Jing Ge",
      "Yan Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_AONet_Attentional_Occlusion-aware_Network_for_Occluded_Person_Re-identification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_AONet_Attentional_Occlusion-aware_Network_for_Occluded_Person_Re-identification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Occluded person Re-identification (Occluded ReID) aims to verify the identity of a pedestrian with occlusion across non-overlapping cameras. Previous works for this task often rely on external tasks, e.g., pose estimation or semantic segmentation, to extract local features over fixed given regions. However, these external models may perform poorly on Occluded ReID, since they themselves are still open problems with no reliable performance guarantee and also not oriented towards ReID tasks to provide discriminative local features. In this paper, we propose an Attentional Occlusion-aware Network (AONet) for Occluded ReID that does not rely on any external tasks. AONet adaptively learns discriminative local features over latent landmark regions by the trainable pattern vectors, and softly weights the summation of landmark-wise similarities based on the occlusion awareness. Also, as there are no ground truth occlusion annotations, we measure the occlusion of landmarks by the awareness scores, when referring to a memorized dictionary storing average landmark features. These awareness scores are then used as a soft weight for training and inferring. Meanwhile, the memorized dictionary is momenta updated according to the landmark features and their awareness scores of each input image. The AONet achieves 53.1% mAP and 66.5% Rank1 on the Occluded-DukeMTMC dataset, significantly outperforming state-of-the-arts without any bells and whistles, and also shows obvious improvements on the holistic datasets Market-1501 and DukeMTMC-reID, as well as partial datasets Partial-REID and Partial-iLIDS. Code and pre-trained models will be released online soon."
  },
  "accv2022_main_tcvmtemporalcontrastingvideomontageframeworkforself-supervisedvideorepresentationlearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "TCVM: Temporal Contrasting Video Montage Framework for Self-supervised Video Representation Learning",
    "authors": [
      "Fengrui Tian",
      "Jiawei Fan",
      "Xie Yu",
      "Shaoyi Du",
      "Meina Song",
      "Yu Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tian_TCVM_Temporal_Contrasting_Video_Montage_Framework_for_Self-supervised_Video_Representation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tian_TCVM_Temporal_Contrasting_Video_Montage_Framework_for_Self-supervised_Video_Representation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Extracting appropriate temporal differences and ignoring irrelevant backgrounds are two important perspectives on preserving sufficient motion information in video representation. In this paper, we propose a unified contrastive learning framework called Temporal Contrasting Video Montage (TCVM) to learn action-specific motion patterns, which can be implemented in a plug-and-play way. On the one hand, Temporal Contrasting (TC) module is designed to guarantee appropriate temporal difference between frames. It utilizes high-level feature space to capture raveled temporal information. On the other hand, Video Montage (VM) module is devised for alleviating the effect from video background. It demonstrates similar temporal motion variances in different positive samples by implicitly mixing up the backgrounds of different videos. Experimental results show that our TCVM reaches promising performances on both large action recognition dataset (i.e. Something-Somethingv2) and small datasets (i.e. UCF101 and HMDB51)."
  },
  "accv2022_main_depthestimationviasparseradarprioranddrivingscenesemantics": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Depth Estimation via Sparse Radar Prior and Driving Scene Semantics",
    "authors": [
      "Ke Zheng",
      "Shuguang Li",
      "Kongjian Qin",
      "Zhenxu LI",
      "Yang Zhao",
      "Zhinan Peng",
      "Hong Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zheng_Depth_Estimation_via_Sparse_Radar_Prior_and_Driving_Scene_Semantics_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zheng_Depth_Estimation_via_Sparse_Radar_Prior_and_Driving_Scene_Semantics_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Depth estimation is an essential module for the perception system of autonomous driving. The state-of-the-art methods introduce LiDAR to improve the performance of monocular depth estimation, but it faces the challenges of weather durability and high hardware cost. Unlike existing LiDAR and image-based methods, a two-stage network is proposed to integrate highly sparse radar data in this paper, in which sparse pre-mapping module and feature fusion module are proposed for radar feature extraction and feature fusion respectively. Considering the highly structured driving scenario, we introduce semantic information of the scenario to further improve the loss function, thus making the network more focused on the target region. Finally, we propose a novel depth dataset construction strategy by integrating binary mask-based filtering and interpolation methods based on the nuScenes dataset. And the effectiveness of our proposed method has been demonstrated through extensive experiments, which outperform existing methods in all metrics."
  },
  "accv2022_main_iou-enhancedattentionforend-to-endtaskspecificobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "IoU-Enhanced Attention for End-to-End Task Specific Object Detection",
    "authors": [
      "Jing Zhao",
      "Shengjian Wu",
      "Li Sun",
      "Qingli Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_IoU-Enhanced_Attention_for_End-to-End_Task_Specific_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_IoU-Enhanced_Attention_for_End-to-End_Task_Specific_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Without densely tiled anchor boxes or grid points in the image, sparse R-CNN achieves promising results through a set of object queries and proposal boxes updated in the cascaded training manner. However, due to the sparse nature and the one-to-one relation between the query and its attending region, it heavily depends on the self attention,which is usually inaccurate in the early training stage. Moreover, in a scene of dense objects, the object query interacts with many irrelevant ones, reducing its uniqueness and harming the performance. This paper proposes to use IoU between different boxes as a prior for the value routing in self attention. The original attention matrix multiplies the same size matrix computed from the IoU of proposal boxes, and they determine the routing scheme so that the irrelevant features can be suppressed. Furthermore, to accurately extract features for both classification and regression, we add two lightweight projection heads to provide the dynamic channel masks based on object query, and they multiply with the output from dynamic convs, making the results suitable for the two different tasks. We validate the proposed scheme on different datasets, including MS-COCO and CrowdHuman, showing that it significantly improves the performance and increases the model convergence speed."
  },
  "accv2022_main_epsanetanefficientpyramidsqueezeattentionblockonconvolutionalneuralnetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network",
    "authors": [
      "Hu Zhang",
      "Keke Zu",
      "Jian Lu",
      "Yuru Zou",
      "Deyu Meng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_EPSANet_An_Efficient_Pyramid_Squeeze_Attention_Block_on_Convolutional_Neural_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_EPSANet_An_Efficient_Pyramid_Squeeze_Attention_Block_on_Convolutional_Neural_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, it has been demonstrated that the performance of a deep convolutional neural network can be effectively improved by embedding an attention module into it. In this work, a novel lightweight and effective attention method named Pyramid Squeeze Attention (PSA) module is proposed. By replacing the 3x3 convolution with the PSA module in the bottleneck blocks of the ResNet, a novel representational block named Efficient Pyramid Squeeze Attention (EPSA) is obtained. The EPSA block can be easily added as a plug-and-play component into a well-established backbone network, and significant improvements on performance can be achieved. Hence, a simple and efficient backbone architecture named EPSANet is developed in this work by stacking these ResNet-style EPSA blocks. Correspondingly, a stronger multiscale representation ability can be offered by the proposed EPSANet for various computer vision tasks including but not limited to, image classification, object detection, instance segmentation, etc. Without bells and whistles, the performance of the proposed EPSANet outperforms most of the state-of-the-art channel attention methods. As compared to the SENet-50, the Top-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of +2.7 box AP for object detection and an improvement of +1.7 mask AP for instance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained."
  },
  "accv2022_main_swptsphericalwindow-basedpointcloudtransformer": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SWPT: Spherical Window-based Point Cloud Transformer",
    "authors": [
      "Xindong Guo",
      "Yu Sun",
      "Rong Zhao",
      "Liqun Kuang",
      "Xie Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Guo_SWPT_Spherical_Window-based_Point_Cloud_Transformer_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Guo_SWPT_Spherical_Window-based_Point_Cloud_Transformer_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "While the Transformer architecture has become the de-facto standard for natural language processing tasks and has shown promising prospects in image analysis domains, applying it to the 3D point cloud directly is still a challenge due to the irregularity and lack of order. Most current approaches adopt the farthest point searching as a downsampling method and construct local areas with the k-nearest neighbor strategy to extract features hierarchically. However, this scheme inevitably consumes lots of time and memory, which impedes its application to near-real-time systems and large-scale point cloud. This research designs a novel transformer-based network called Spherical Window-based Point Transformer (SWPT) for point cloud learning, which consists of a Spherical Projection module, a Spherical Window Transformer module and a crossing self-attention module. Specifically, we project the points on a spherical surface, then a window-based local self-attention is adopted to calculate the relationship between the points within a window. To obtain connections between different windows, the crossing self-attention is introduced, which rotates all the windows as a whole along the spherical surface and then aggregates the crossing features. It is inherently permutation invariant because of using simple and symmetric functions, making it suitable for point cloud processing. Extensive experiments demonstrate that SWPT can achieve the state-of-the-art performance with about 3-8 times faster than previous transformer-based methods on shape classification tasks, and achieve competitive results on part segmentation and the more difficult real-world classification tasks."
  },
  "accv2022_main_msf$2$dnmultiscalefeaturefusiondehazingnetworkwithdenseconnection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "MSF$^2$DN:Multi Scale Feature Fusion Dehazing  Network with Dense connection",
    "authors": [
      "Guangfa Wang",
      "Xiaokang Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_MSF2DNMulti_Scale_Feature_Fusion_Dehazing__Network_with_Dense_connection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_MSF2DNMulti_Scale_Feature_Fusion_Dehazing__Network_with_Dense_connection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Single image dehazing is a challenging problem in computer vision. Previous work has mostly focused on designing new encoder and decoder in common network architectures, while neglecting the connection between the two. In this paper, we propose a multi-scale feature fusion dehazing network based on dense connection, MSF^2DN. The design principle of this network is to make full use of dense connection to achieve efficient reuse of features. On the one hand, we use a dense connection inside the base module of the encoder-decoder to fuse the features of different convolutional layers several times, and on the other hand, we design a simple multi-stream feature fusion module which fuses the features of different stages after uniform scaling and feeds them into the base module of the decoder for enhancement. Numerous experiments have demonstrated that our network outperforms the existing state-of-the-art networks in real-world datasets."
  },
  "accv2022_main_synchronousbi-directionalpedestriantrajectorypredictionwitherrorcompensation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Synchronous Bi-Directional Pedestrian Trajectory Prediction with Error Compensation",
    "authors": [
      "Ce Xie",
      "Yuanman Li",
      "Rongqin Liang",
      "Li Dong",
      "Xia Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Xie_Synchronous_Bi-Directional_Pedestrian_Trajectory_Prediction_with_Error_Compensation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Xie_Synchronous_Bi-Directional_Pedestrian_Trajectory_Prediction_with_Error_Compensation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Pedestrian trajectory prediction as an essential part of reasoning human motion behaviors, has been deployed in a number of vision applications, such as autonomous navigation and video surveillance.Most existing methods adopt autoregressive frameworks to forecast the future trajectory, where the trajectory is iteratively generated based on the previous outputs. Such a process will suffer from large accumulated errors over the long-term forecast horizon. To address this issue, in this paper, we propose a Synchronous Bi-Directional framework (SBD) with error compensation for pedestrian trajectory prediction, which can greatly alleviate the error accumulation during prediction. Specifically, we first develop a bi-directional trajectory prediction mechanism, and force the predicting procedures for two opposite directions to be synchronous through a shared motion characteristic. Different from previous works, the mutual constraints inherent to our framework from the synchronous opposite-predictions can significantly prevent the error accumulation. In order to reduce the possible prediction error in each timestep, we further devise an error compensation network to model and compensate for the positional deviation between the ground-truth and the predicted trajectory, thus improving the prediction accuracy of our scheme. Experiments conducted on the Stanford Drone dataset and the ETH-UCY dataset show that our method achieves much better results than existing algorithms. Particularly, by resorting to our alleviation methodology for the error accumulation, our scheme exhibits superior performance in the long-term pedestrian trajectory prediction."
  },
  "accv2022_main_eai-stereoerrorawareiterativenetworkforstereomatching": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "EAI-Stereo: Error Aware Iterative Network for Stereo Matching",
    "authors": [
      "Haoliang Zhao",
      "Huizhou Zhou",
      "Yongjun Zhang",
      "Yong Zhao",
      "Yitong Yang",
      "Ting Ouyang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_EAI-Stereo_Error_Aware_Iterative_Network_for_Stereo_Matching_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_EAI-Stereo_Error_Aware_Iterative_Network_for_Stereo_Matching_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Current state-of-the-art stereo algorithms use a 2D CNN to extract features and then form a cost volume, which is fed into the following cost aggregation and regularization module composed of 2D or 3D CNNs. However, a large amount of high-frequency information like texture, color variation, sharp edge etc. is not well exploited during this process, which leads to relatively blurry and lacking detailed disparity maps. In this paper, we aim at making full use of the high-frequency information from the original image. Towards this end, we propose an error-aware refinement module that incorporates high-frequency information from the original left image and allows the network to learn error correction capabilities that can produce excellent subtle details and sharp edges. In order to improve the data transfer efficiency between our iterations, we propose the Iterative Multiscale Wide-LSTM Network which could carry more semantic information across iterations. We demonstrate the efficiency and effectiveness of our method on KITTI 2015, Middlebury, and ETH3D. At the time of writing this paper, EAI-Stereo ranks 1st on the Middlebury leaderboard and 1st on the ETH3D Stereo benchmark for 50% quantile metric and second for 0.5px error rate among all published methods. Our model performs well in cross-domain scenarios and outperforms current methods specifically designed for generalization."
  },
  "accv2022_main_readingarbitrary-shapedscenetextfromimagesthroughsplineregressionandrectification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Reading Arbitrary-Shaped Scene Text from Images Through Spline Regression and Rectification",
    "authors": [
      "Long Chen",
      "Feng Su",
      "Jiahao Shi",
      "Ye Qian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_Reading_Arbitrary-Shaped_Scene_Text_from_Images_Through_Spline_Regression_and_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Reading_Arbitrary-Shaped_Scene_Text_from_Images_Through_Spline_Regression_and_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Scene text in natural images contains a wealth of valuable semantic information. To read scene text from the image, various text spotting techniques that jointly detect and recognize scene text have been proposed in recent years. In this paper, we present a novel end-to-end text spotting network SPRNet for arbitrary-shaped scene text. We propose a parametric B-spline centerline-based representation model to describe the distinctive global shape characteristics of the text, which helps to effectively deal with interferences such as local connection and tight spacing of text and other object, and a text is detected by regressing its shape parameters. Further, exploiting the text's shape cues, we employ adaptive projection transformations to rectify the feature representation of an irregular text, which improves the accuracy of the subsequent text recognition network. Our method achieves competitive text spotting performance on standard benchmarks through a simple architecture equipped with the proposed text representation and rectification mechanism, which demonstrates the effectiveness of the method in detecting and recognizing scene text with arbitrary shapes."
  },
  "accv2022_main_visualexplanationgenerationbasedonlambdaattentionbranchnetworks": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Visual Explanation Generation Based on Lambda Attention Branch Networks",
    "authors": [
      "Tsumugi Iida",
      "Takumi Komatsu",
      "Kanta Kaneda",
      "Tsubasa Hirakawa",
      "Takayoshi Yamashita",
      "Hironobu Fujiyoshi",
      "Komei Sugiura"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Iida_Visual_Explanation_Generation_Based_on_Lambda_Attention_Branch_Networks_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Iida_Visual_Explanation_Generation_Based_on_Lambda_Attention_Branch_Networks_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Explanation generation for transformers enhances accountability for their predictions. However, there have been few studies on generating visual explanations for the transformers that use multidimensional context, such as LambdaNetworks. In this paper, we propose the Lambda Attention Branch Networks, which attend to important regions in detail and generate easily interpretable visual explanations. We also propose the Patch Insertion-Deletion score, an extension of the Insertion-Deletion score, as an effective evaluation metric for images with sparse important regions.Experimental results on two public datasets indicate that the proposed method successfully generates visual explanations."
  },
  "accv2022_main_pedtransafine-grainedvisualclassificationmodelforself-attentionpatchenhancementanddropout": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PEDTrans: A fine-grained visual classification model for self-attention patch enhancement and dropout",
    "authors": [
      "Xuhong Lin",
      "Qian Yan",
      "Caicong Wu",
      "Yifei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lin_PEDTrans_A_fine-grained_visual_classification_model_for_self-attention_patch_enhancement_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lin_PEDTrans_A_fine-grained_visual_classification_model_for_self-attention_patch_enhancement_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Fine-grained visual classification (FGVC) is an essential and challenging classification task in computer visual classification, aiming to identify different cars and birds. Recently, most studies use a convolutional neural network combined with an attention mechanism to find discriminant regions to improve algorithm accuracy automatically. However, the discriminant regions selected by the convolutional neural network are extensive. Vision Transformer divides the image into patches and relies on self-attention to select more accurate discriminant regions. However, the Vision Transformer model ignores the response between local patches before patch embedding. In addition, patches usually have high similarity, and they are considered redundant. Therefore, we propose a PEDTrans model based on Vision Transformer. The model has a patch enhancement module based on attention mechanism and a random similar group patch discarding module based on similarity. These two modules can establish patch local feature relationships and select patches that are easier to distinguish between images. Combining these two modules with the Vision Transformer backbone network can improve the fine-grained visual classification accuracy. We employ commonly used fine-grained visual classification datasets CUB-200-2011, Stanford Cars, Stanford Dogs and NABirds to get advanced results."
  },
  "accv2022_main_styleimageharmonizationviaglobal-localstylemutualguided": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Style Image Harmonization via Global-Local Style Mutual Guided",
    "authors": [
      "Xiao Yan",
      "Yang Lu",
      "Juncheng Shuai",
      "Sanyuan Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yan_Style_Image_Harmonization_via_Global-Local_Style_Mutual_Guided_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yan_Style_Image_Harmonization_via_Global-Local_Style_Mutual_Guided_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The process of style image harmonization is attaching an area of the source image to the target style image to form a harmonious new image. Existing methods generally have problems such as distorted foreground, missing content, and semantic inconsistencies caused by the excessive transfer of local style. In this paper, we present a framework for style image harmonization via global and local styles mutual guided to ameliorate these problems. Specifically, we learn to extract global and local information from the Vision Transformer and Convolutional Neural Networks, and adaptively fuse the two kinds of information under a multi-scale fusion structure to ameliorate disharmony between foreground and background styles. Then we train the blending network GradGAN to smooth the image gradient. Finally, we take both style and gradient into consideration to solve the sudden change in the blended boundary gradient. In addition, supervision is unnecessary in our training process. Our experimental results show that our algorithm can balance global and local styles in the foreground stylization, retaining the original information of the object while keeping the boundary gradient smooth, which is more advanced than other methods."
  },
  "accv2022_main_acompressivepriorguidedmaskpredictivecodingapproachforvideoanalysis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Compressive Prior Guided Mask Predictive Coding Approach for Video Analysis",
    "authors": [
      "Zhimeng Huang",
      "Chuanmin Jia",
      "Shanshe Wang",
      "Siwei Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Huang_A_Compressive_Prior_Guided_Mask_Predictive_Coding_Approach_for_Video_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Huang_A_Compressive_Prior_Guided_Mask_Predictive_Coding_Approach_for_Video_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In real-world scenarios, video analysis algorithms are conducted for visual signals after compression and transmission. Generally speaking, most codecs introduce irreversible distortion due to coarse quantization during compression. The distortion may lead to significant perception degradation in terms of video analysis performance. To tackle this problem, we propose an efficient plug-and-play approach to preserve the essential semantic information in video sequences explicitly. The proposed approach could boost the video analysis performance with a little extra bit cost. Specifically, we employ the proposed approach on an emerging video analysis task, video object segmentation(VOS). Massive experimental results prove that the our work outperforms the existing coding approaches over multiple VOS datasets. Concretely, it could improve the analysis performance by up to 13% at similar bitrates. Additional experiments also verifies the flexibility of our scheme because there is no dependency on any specific VOS model or encoding method. Essentially, the proposed approach provides novel insights for the emerging Video Coding for Machine (VCM) standard."
  },
  "accv2022_main_complexhandwritingtrajectoryrecoveryevaluationmetricsandalgorithm": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Complex Handwriting Trajectory Recovery: Evaluation Metrics and Algorithm",
    "authors": [
      "Zhounan Chen",
      "Daihui Yang",
      "Jinglin Liang",
      "Xinwu Liu",
      "Yuyi Wang",
      "Zhenghua Peng",
      "Shuangping Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_Complex_Handwriting_Trajectory_Recovery_Evaluation_Metrics_and_Algorithm_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Complex_Handwriting_Trajectory_Recovery_Evaluation_Metrics_and_Algorithm_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Many important tasks such as forensic signature verification, calligraphy synthesis, etc, rely on handwriting trajectory recovery of which, however, even an appropriate evaluation metric is still missing. Indeed, existing metrics only focus on the writing orders but overlook the fidelity of glyphs. Taking both facets into account, we come up with two new metrics, the adaptive intersection on union (AIoU) which eliminates the influence of various stroke widths, and the length-independent dynamic time warping (LDTW) which solves the trajectory-point alignment problem. After that, we then propose a novel handwriting trajectory recovery model named Parsing-and-tracing ENcoder-decoder Network (PEN-Net), in particular for characters with both complex glyph and long trajectory, which was believed very challenging. In the PEN-Net, a carefully designed double-stream parsing encoder parses the glyph structure, and a global tracing decoder overcomes the memory difficulty of long trajectory prediction. Our experiments demonstrate that the two new metrics AIoU and LDTW together can truly assess the quality of handwriting trajectory recovery and the proposed PEN-Net exhibits satisfactory performance in various complex-glyph languages including Chinese, Japanese and Indic. The source code is available at https://github.com/ChenZhounan/PEN-Net."
  },
  "accv2022_main_networkpruningviafeatureshiftminimization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Network Pruning via Feature Shift Minimization",
    "authors": [
      "Yuanzhi Duan",
      "Yue Zhou",
      "Peng He",
      "Qiang Liu",
      "Shukai Duan",
      "Xiaofang Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Duan_Network_Pruning_via_Feature_Shift_Minimization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Duan_Network_Pruning_via_Feature_Shift_Minimization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Channel pruning is widely used to reduce the complexity of deep network models. Recent pruning methods usually identify which parts of the network to discard by proposing a channel importance criterion. However, recent studies have shown that these criteria do not work well in all conditions. In this paper, we propose a novel Feature Shift Minimization (FSM) method to compress CNN models, which evaluates the feature shift by converging the information of both features and filters. Specifically, we first investigate the compression efficiency with some prevalent methods in different layer-depths and then propose the feature shift concept. Then, we introduce an approximation method to estimate the magnitude of the feature shift, since it is difficult to compute it directly. Besides, we present a distribution-optimization algorithm to compensate for the accuracy loss and improve the network compression efficiency. The proposed method yields state-of-the-art performance on various benchmark networks and datasets, verified by extensive experiments. Our codes are available at: https://github.com/lscgx/FSM."
  },
  "accv2022_main_training-freenasfor3dpointcloudprocessing": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Training-free NAS for 3D Point Cloud Processing",
    "authors": [
      "Ping Zhao",
      "Panyue Chen",
      "Guanming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_Training-free_NAS_for_3D_Point_Cloud_Processing_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_Training-free_NAS_for_3D_Point_Cloud_Processing_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep neural networks for 3D point cloud processing have exhibited superior performance on many tasks. However, the structure and computational complexity of existing networks are relatively fixed, which makes it difficult for them to be flexibly applied to devices with different computational constraints. Instead of manually designing the network structure for each specific device, in this paper, we propose a novel training-free neural architecture search algorithm which can quickly sample network structures that satisfy the computational constraints of various devices. Specifically, we design a cell-based search space that contains a large number of latent network structures. The computational complexity of these structures varies within a wide range to meet the needs of different devices. We also propose a multi-objective evolutionary search algorithm. This algorithm scores the candidate network structures in the search space based on multiple training-free proxies, encourages high-scoring networks to evolve, and gradually eliminates low-scoring networks, so as to search for the optimal network structure. Because the calculation of training-free proxies is very efficient, the whole algorithm can be completed in a short time. Experiments on 3D point cloud classification and part segmentation demonstrate the effectiveness of our method."
  },
  "accv2022_main_theeyecandiesdatasetforunsupervisedmultimodalanomalydetectionandlocalization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "The Eyecandies Dataset for Unsupervised Multimodal Anomaly Detection and Localization",
    "authors": [
      "Luca Bonfiglioli",
      "Marco Toschi",
      "Davide Silvestri",
      "Nicola Fioraio",
      "Daniele De Gregorio"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Bonfiglioli_The_Eyecandies_Dataset_for_Unsupervised_Multimodal_Anomaly_Detection_and_Localization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Bonfiglioli_The_Eyecandies_Dataset_for_Unsupervised_Multimodal_Anomaly_Detection_and_Localization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present Eyecandies, a novel synthetic dataset for unsupervised anomaly detection and localization. Photo-realistic images of procedurally generated candies are rendered in a controlled environment under multiple lightning conditions, also providing depth and normal maps in an industrial conveyor scenario. We make available anomaly-free samples for model training and validation, while anomalous instances with precise ground-truth annotations are provided only in the test set. The dataset comprises ten classes of candies, each showing different challenges, such as complex textures, self-occlusions and specularities. Furthermore, we achieve large intra-class variation by randomly drawing key parameters of a procedural rendering pipeline, which enables the creation of an arbitrary number of instances with photo-realistic appearance. Likewise, anomalies are injected into the rendering graph and pixel-wise annotations are automatically generated, overcoming human-biases and possible inconsistencies.We believe this dataset may encourage the exploration of original approaches to solve the anomaly detection task, e.g. by combining color, depth and normal maps, as they are not provided by most of the existing datasets. Indeed, in order to demonstrate how exploiting additional information may actually lead to higher detection performance, we show the results obtained by training a deep convolutional autoencoder to reconstruct different combinations of inputs."
  },
  "accv2022_main_maxgnradynamicweightstrategyviamaximizinggradient-to-noiseratioformulti-tasklearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning",
    "authors": [
      "Caoyun Fan",
      "Wenqing Chen",
      "Jidong Tian",
      "Yitian Li",
      "Hao He",
      "Yaohui Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Fan_MaxGNR_A_Dynamic_Weight_Strategy_via_Maximizing_Gradient-to-Noise_Ratio_for_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Fan_MaxGNR_A_Dynamic_Weight_Strategy_via_Maximizing_Gradient-to-Noise_Ratio_for_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "When modeling related tasks in computer vision, Multi-Task Learning (MTL) can outperform Single-Task Learning (STL) due to its ability to capture intrinsic relatedness among tasks. However, MTL may encounter the insufficient training problem, i.e., some tasks in MTL may encounter non-optimal situation compared with STL. A series of studies point out that too much gradient noise would lead to performance degradation in STL, however, in the MTL scenario, Inter-Task Gradient Noise (ITGN) is an additional source of gradient noise for each task, which can also affect the optimization process. In this paper, we point out ITGN as a key factor leading to the insufficient training problem. We define the Gradient-to-Noise Ratio (GNR) to measure the relative magnitude of gradient noise and design the MaxGNR algorithm to alleviate the ITGN interference of each task by maximizing the GNR of each task. We carefully evaluate our MaxGNR algorithm on two standard image MTL datasets: NYUv2 and Cityscapes. The results show that our algorithm outperforms the existing baselines under identical experimental conditions."
  },
  "accv2022_main_sg-netsemanticguidednetworkforimagedehazing": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SG-Net: Semantic Guided Network for Image Dehazing",
    "authors": [
      "Tao Hong",
      "Xiangyang Guo",
      "Zeren Zhang",
      "Jinwen Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hong_SG-Net_Semantic_Guided_Network_for_Image_Dehazing_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hong_SG-Net_Semantic_Guided_Network_for_Image_Dehazing_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "From traditional handcrafted priors to learning-based neural networks, image dehazing technique has gone through great development. In this paper, we propose an end-to-end Semantic Guided Network (SG-Net) for directly restoring the haze-free images. Inspired by the high similarity (mapping relationship) between the transmission maps and the segmentation results of hazy images, we found that the semantic information of the scene provides a strong natural prior for image restoration. To guide the dehazing more effectively and systematically, we utilize the information of semantic segmentation with three easily portable modes: Semantic Fusion (SF), Semantic Attention (SA), and Semantic Loss (SL), which compose our Semantic Guided (SG) mechanisms. By embedding these SG mechanisms into existing dehazing networks, we construct the SG-Net series: SG-AOD, SG-GCA, SG-FFA, and SG-AECR. The outperformance on image dehazing of these SG networks is demonstrated by the experiments in terms of both quantity and quality. It is worth mentioning that SG-FFA achieves the state-of-the-art performance."
  },
  "accv2022_main_robusthumanmattingviasemanticguidance": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Robust Human Matting via Semantic Guidance",
    "authors": [
      "XiangGuang Chen",
      "Ye Zhu",
      "Yu Li",
      "Bingtao Fu",
      "Lei Sun",
      "Ying Shan",
      "Shan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_Robust_Human_Matting_via_Semantic_Guidance_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Robust_Human_Matting_via_Semantic_Guidance_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Automatic human matting is highly desired for many real applications. We investigate recent human matting methods and show that common bad cases happen when semantic human segmentation fails. This indicates that semantic understanding is crucial for robust human matting. From this, we develop a fast yet accurate human matting framework, named Semantic Guided Human Matting (SGHM). It builds on a semantic human segmentation network and introduces a light-weight matting module with only marginal computational cost. Unlike previous works, our framework is data efficient, which requires a small amount of matting ground-truth to learn to estimate high quality object mattes. Our experiments show that trained with merely 200 matting images, our method can generalize well to real-world datasets, and outperform recent methods on multiple benchmarks, while remaining efficient. Considering the unbearable labeling cost of matting data and widely available segmentation data, our method becomes a practical and effective solution for the task of human matting. Source code is available at https://github.com/cxgincsu/SemanticGuidedHumanMatting."
  },
  "accv2022_main_kinstyleastrongbaselinephotorealistickinshipfacesynthesiswithanoptimizedstyleganencoder": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "KinStyle: A Strong Baseline Photorealistic Kinship Face Synthesis with An Optimized StyleGAN Encoder",
    "authors": [
      "Li-Chen Cheng",
      "Shu-Chuan Hsu",
      "Pin-Hua Lee",
      "Hsiu-Chieh Lee",
      "Che-Hsien Lin",
      "Jun-Cheng Chen",
      "Chih-Yu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cheng_KinStyle_A_Strong_Baseline_Photorealistic_Kinship_Face_Synthesis_with_An_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cheng_KinStyle_A_Strong_Baseline_Photorealistic_Kinship_Face_Synthesis_with_An_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "High-fidelity kinship face synthesis is a challenging task due to the limited amount of kinship data available for training and low-quality images. In addition, it is also hard to trace the genetic traits between parents and children from those low-quality training images. To address these issues, we leverage the pre-trained state-of-the-art face synthesis model, StyleGAN2, for kinship face synthesis. To handle large age, gender and other attribute variations between the parents and their children, we conduct a thorough study of its rich latent spaces and different encoder architectures for an optimized encoder design to repurpose StyleGAN2 for kinship face synthesis. The obtained latent representation from our developed encoder pipeline with stage-wise training strikes a better balance of editability and synthesis fidelity for identity preserving and attribute manipulations than other compared approaches. With extensive subjective, quantitative, and qualitative evaluations, the proposed approach consistently achieves better performance in terms of facial attribute heredity and image generation fidelity than other compared state-of-the-art methods. This demonstrates the effectiveness of the proposed approach which can yield promising and satisfactory kinship face synthesis using only a single and straightforward encoder architecture."
  },
  "accv2022_main_softlabelminingandaverageexpressionanchoringforfacialexpressionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Soft Label Mining and Average Expression Anchoring for Facial Expression Recognition",
    "authors": [
      "Haipeng Ming",
      "Wenhuan Lu",
      "Wei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ming_Soft_Label_Mining_and_Average_Expression_Anchoring_for_Facial_Expression_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ming_Soft_Label_Mining_and_Average_Expression_Anchoring_for_Facial_Expression_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Facial expression recognition (FER) suffers from high interclass similarity and large intraclass variation, leading to ambiguity or uncertainty and further confusing annotators. They also hinder the network in learning the valuable features of facial expression. Recently, many studies have revealed that the uncertainty or ambiguity is one of the key challenges in FER.In this paper, we propose a new method to address this issue from two aspects: a soft label mining module to convert the original hard labels to soft labels dynamically during training, and an average facial expression anchoring module to separate unique expression features from similarity expression features. The soft label mining module breaks the limits of the categorical model and mitigates the uncertainty or ambiguity.And the average facial expression anchoring module suppresses the high interclass similarity of facial expressions. Our method can train any backbone network for facial expression recognition. The experiments on the popular datasets show that our method achieves state-of-the-art results by 92.82% on RAF-DB and 67.91% on SFEW, and achieves a comparable result of 62.26% on AffectNet. The code is available at https://github.com/HaipengMing/SLM-AEA."
  },
  "accv2022_main_rove-tree-11thenot-so-wildrover,ahierarchicallystructuredimagedatasetfordeepmetriclearningresearch": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Rove-Tree-11: The not-so-Wild Rover, A hierarchically structured image dataset for deep metric learning research",
    "authors": [
      "Roberta Hunt",
      "Kim Steenstrup Pedersen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hunt_Rove-Tree-11_The_not-so-Wild_Rover_A_hierarchically_structured_image_dataset_for_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hunt_Rove-Tree-11_The_not-so-Wild_Rover_A_hierarchically_structured_image_dataset_for_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present a new dataset of images of pinned insects from museum collections along with a ground truth phylogeny (a graph representing the relative evolutionary distance between species). The images include segmentations, and can be used for clustering and deep hierarchical metric learning. As far as we know, this is the first dataset released specifically for generating phylogenetic trees. We provide several benchmarks for deep metric learning using a selection of state-of-the-art methods."
  },
  "accv2022_main_promptlearner-clipcontrastivemulti-modalactionrepresentationlearningwithcontextoptimization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PromptLearner-CLIP: Contrastive Multi-Modal Action Representation Learning with Context Optimization",
    "authors": [
      "Zhenxing Zheng",
      "Gaoyun An",
      "Shan Cao",
      "Zhaoqilin Yang",
      "Quqi Ruan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zheng_PromptLearner-CLIP_Contrastive_Multi-Modal_Action_Representation_Learning_with_Context_Optimization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zheng_PromptLearner-CLIP_Contrastive_Multi-Modal_Action_Representation_Learning_with_Context_Optimization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "An action contains rich multi-modal information, and current methods generally map the action class to a digital number as supervised information to train models. However, numerical labels cannot describe the semantic content contained in the action. This paper proposes PromptLearner-CLIP for action recognition, where the text pathway uses PromptLearner to automatically learn the text content of prompt as the input and calculates the semantic features of actions, and the vision pathway takes video data as the input to learn the visual features of actions. To strengthen the interaction between features of different modalities, this paper proposes a multi-modal information interaction module that utilizes Graph Neural Network(GNN) to process both the semantic features of text content and the visual features of a video. In addition, the single-modal video classification problem is transformed into a multi-modal video-text matching problem. Multi-modal contrastive learning is used to disclose the feature distance of the same but different modalities samples. The experimental results showed that PromptLearner-CLIP could utilize the textual semantic information to significantly improve the performance of various single-modal backbone networks on action recognition and achieved top-tier results on Kinetics400, UCF101, and HMDB51 datasets."
  },
  "accv2022_main_classspecializedknowledgedistillation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Class Specialized Knowledge Distillation",
    "authors": [
      "Li-Yun Wang",
      "Anthony Rhodes",
      "Wu-chi Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Class_Specialized_Knowledge_Distillation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Class_Specialized_Knowledge_Distillation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Knowledge Distillation (KD) is a compression framework that transfers distilled knowledge from a teacher to a smaller student model. KD approaches conventionally address problem domains where the teacher and student network have equal numbers of classes for classification. We provide a knowledge distillation solution tailored for the class specialization setting, where the user requires a compact and performant network specializing in a subset of classes from the class set used to train the teacher model. To this end, we introduce a novel knowledge distillation framework, Class Specialized Knowledge Distillation (CSKD), that combines two loss functions: Renormalized Knowledge Distillation (RKD) and Intra-Class Variance (ICV) to render a computationally-efficient, specialized student network. We report results on several popular architectural benchmarks and tasks. In particular, CSKD consistently demonstrates significant performance improvements over teacher models for highly restrictive specialization tasks (e.g., instances where the number of subclasses or datasets is relatively small), in addition to outperforming other state-of-the-art knowledge distillation approaches for class specialization tasks."
  },
  "accv2022_main_mvfi-netmotion-awarevideoframeinterpolationnetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "MVFI-Net: Motion-aware Video Frame Interpolation Network",
    "authors": [
      "XuHu Lin",
      "Lili Zhao",
      "Xi Liu",
      "Jianwen Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lin_MVFI-Net_Motion-aware_Video_Frame_Interpolation_Network_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lin_MVFI-Net_Motion-aware_Video_Frame_Interpolation_Network_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Video frame interpolation (VFI) is to synthesize the intermediate frame given successive frames. Most existing learning-based VFI methods generate each target pixel by using the warping operation with either one predicted kernel or flow, or both. However, their performances are often degraded due to the issues on the limited direction and scope of the reference regions, especially encountering complex motions. In this paper, we propose a novel motion-aware VFI network (MVFI-Net) to address these issues. One of the key novelties of our method lies in the newly developed warping operation,i.e., motion-aware convolution (MAC). By predicting multiple extensible temporal motion vectors (MVs) and filter kernels for each target pixel, the direction and scope could be enlarged simultaneously. Besides, we first attempt to incorporate the pyramid structure into the kernel-based VFI, which can decompose large motions into smaller scales to improve the prediction efficiency. The quantitative and qualitative experimental results have demonstrated the proposed method delivers the state-of-the-art performance on the diverse benchmarks with various resolutions. Our codes are available at https://github.com/MediaLabVFI/MVFI-Net"
  },
  "accv2022_main_lightweightalphamattingnetworkusingdistillation-basedchannelpruning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Lightweight Alpha Matting Network Using Distillation-Based Channel Pruning",
    "authors": [
      "Donggeun Yoon",
      "Jinsun Park",
      "Donghyeon Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yoon_Lightweight_Alpha_Matting_Network_Using_Distillation-Based_Channel_Pruning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yoon_Lightweight_Alpha_Matting_Network_Using_Distillation-Based_Channel_Pruning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, alpha matting has received a lot of attention because of its usefulness in mobile applications such as selfies. Therefore, there has been a demand for a lightweight alpha matting model due to the limited computational resources of commercial portable devices. To this end, we suggest a distillation-based channel pruning method for the alpha matting networks. In the pruning step, we remove channels of a student network having fewer impacts on mimicking the knowledge of a teacher network. Then, the pruned lightweight student network is trained by the same distillation loss. A lightweight alpha matting model from the proposed method outperforms existing lightweight methods. To show superiority of our algorithm, we provide various quantitative and qualitative experiments with in-depth analyses. Furthermore, we demonstrate the versatility of the proposed distillation-based channel pruning method by applying it to semantic segmentation."
  },
  "accv2022_main_labellingthegapsaweaklysupervisedautomaticeyegazeestimation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "`Labelling the Gaps': A Weakly Supervised Automatic Eye Gaze Estimation",
    "authors": [
      "Shreya Ghosh",
      "Abhinav Dhall",
      "Munawar Hayat",
      "Jarrod Knibbe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ghosh_Labelling_the_Gaps_A_Weakly_Supervised_Automatic_Eye_Gaze_Estimation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ghosh_Labelling_the_Gaps_A_Weakly_Supervised_Automatic_Eye_Gaze_Estimation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Over the past few years, there has been an increasing interest to interpret gaze direction in an unconstrained environment with limited supervision. Owing to data curation and annotation issues, replicating gaze estimation method to other platforms, such as unconstrained outdoor or AR/VR, might lead to significant drop in performance due to insufficient availability of accurately annotated data for model training. In this paper, we explore an interesting yet challenging problem of gaze estimation method with a limited amount of labelled data. The proposed method utilize domain knowledge from the labelled subset with visual features; including identity-specific appearance, gaze trajectory consistency and motion features. Given a gaze trajectory, the method utilizes label information of only the start and the end frames of a gaze sequence. An extension of the proposed method further reduces the requirement of labelled frames to only the start frame with a minor drop in the generated label's quality. We evaluate the proposed method on four benchmark datasets (CAVE, TabletGaze, MPII and Gaze360) as well as web-crawled YouTube videos. Our proposed method reduces the annotation effort to as low as 2.67%, with minimal impact on performance; indicating the potential of our model enabling gaze estimation `in-the-wild' setup."
  },
  "accv2022_main_cross-domainlocalcharacteristicenhanceddeepfakevideodetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Cross-Domain Local Characteristic Enhanced Deepfake Video Detection",
    "authors": [
      "Zihan Liu",
      "Hanyi Wang",
      "Shilin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Liu_Cross-Domain_Local_Characteristic_Enhanced_Deepfake_Video_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Liu_Cross-Domain_Local_Characteristic_Enhanced_Deepfake_Video_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "As ultra-realistic face forgery techniques emerge, deepfake detection has attracted increasing attention due to security concerns. Many detectors cannot achieve accurate results when detecting unseen manipulations despite excellent performance on known forgeries. In this paper, we are motivated by the observation that the discrepancies between real and fake videos are extremely subtle and localized, and inconsistencies or irregularities can exist in some critical facial regions across various information domains. To this end, we propose a novel pipeline, Cross-Domain Local Forensics (XDLF), for more general deepfake video detection. In the proposed pipeline, a specialized framework is presented to simultaneously exploit local forgery patterns from space, frequency, and time domains, thus learning cross-domain features to detect forgeries. Moreover, the framework leverages four high-level forgery-sensitive local regions of a human face to guide the model to enhance subtle artifacts and localize potential anomalies. Extensive experiments on several benchmark datasets demonstrate the impressive performance of our method, and we achieve superiority over several state-of-the-art methods on cross-dataset generalization. We also examined the factors that contribute to its performance through ablations, which suggests that exploiting cross-domain local characteristics is a noteworthy direction for developing more general deepfake detectors."
  },
  "accv2022_main_multi-streamfusionforclassincrementallearninginpillimageclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-stream Fusion for Class Incremental Learning in Pill Image Classification",
    "authors": [
      "Tung-Trong Nguyen",
      "Hieu H. Pham",
      "Phi Le Nguyen",
      "Thanh Hung Nguyen",
      "Minh Do"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Nguyen_Multi-stream_Fusion_for_Class_Incremental_Learning_in_Pill_Image_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Nguyen_Multi-stream_Fusion_for_Class_Incremental_Learning_in_Pill_Image_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Classifying pill categories from real-world images is crucial for various smart healthcare applications. Although existing approaches in image classification might achieve a good performance on fixed pill categories, they fail to handle novel instances of pill categories that are frequently presented to the learning algorithm. To this end, a trivial solution is to train the model with novel classes. However, this may result in a phenomenon known as catastrophic forgetting, in which the system forgets what it learned in previous classes. In this paper, we address this challenge by introducing the class incremental learning (CIL) ability to traditional pill image classification systems. Specifically, we propose a novel incremental multi-stream intermediate fusion framework enabling incorporation of an additional guidance information stream that best matches the domain of the problem into various state-of-the-art CIL methods. From this framework, we consider color-specific information of pill images as a guidance stream and devise an approach, namely \"Color Guidance with Multi-stream intermediate fusion\"(CG-IMIF) for solving CIL pill image classification task. We conduct comprehensive experiments on real-world incremental pill image classification dataset, namely VAIPE-PCIL, and find that the CG-IMIF consistently outperforms several state-of-the-art methods by a large margin in different task settings. Our code, data, and trained model are available at https://github.com/vinuni-vishc/CG-IMIF."
  },
  "accv2022_main_teacher-guidedlearningforblindimagequalityassessment": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Teacher-Guided Learning for Blind Image Quality Assessment",
    "authors": [
      "Zewen Chen",
      "Juan Wang",
      "Bing Li",
      "Chunfeng Yuan",
      "Weihua Xiong",
      "Rui Cheng",
      "Weiming Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_Teacher-Guided_Learning_for_Blind_Image_Quality_Assessment_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Teacher-Guided_Learning_for_Blind_Image_Quality_Assessment_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The performance of deep learning models for blind image quality assessment (BIQA) suffers from annotated data insufficiency. However, image restoration, as a closely-related task with BIQA, can easily acquire training data without annotation. Moreover, both image semantic and distortion information are vital knowledge for the two tasks to predict and improve image quality. Inspired by these, this paper proposes a novel BIQA framework, which builds an image restoration model as a teacher network (TN) to learn the two aspects of knowledge and then guides the student network (SN) for BIQA. In TN, multi-branch convolutions are leveraged for performing adaptive restoration from diversely distorted images to strengthen the knowledge learning. Then the knowledge is transferred to the SN and progressively aggregated by computing long-distance responses to improve BIQA on small annotated data. Experimental results show that our method outperforms many state-of-the-arts on both synthetic and authentic datasets. Besides, the generalization, robustness and effectiveness of our method are fully validated. The code is available in https://github.com/chencn2020/TeacherIQA."
  },
  "accv2022_main_layered-garmentnetgeneratingmultipleimplicitgarmentlayersfromasingleimage": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Layered-Garment Net: Generating Multiple Implicit Garment Layers from a Single Image",
    "authors": [
      "Alakh Aggarwal",
      "Jikai Wang",
      "Steven Hogue",
      "Saifeng Ni",
      "Madhukar Budagavi",
      "Xiaohu Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Aggarwal_Layered-Garment_Net_Generating_Multiple_Implicit_Garment_Layers_from_a_Single_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Aggarwal_Layered-Garment_Net_Generating_Multiple_Implicit_Garment_Layers_from_a_Single_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recent research works have focused on generating human models and garments from their 2D images. However, state-of-the-art researches focus either on only a single layer of the garment on a human model or on generating multiple garment layers without any guarantee of the intersection-free geometric relationship between them. In reality, people wear multiple layers of garments in their daily life, where an inner layer of garment could be partially covered by an outer one. In this paper, we try to address this multi-layer modeling problem and propose the Layered-Garment Net (LGN) that is capable of generating intersection-free multiple layers of garments defined by implicit function fields over the body surface, given the person's near front-view image. With a special design of garment indication fields (GIF), we can enforce an implicit covering relationship between the signed distance fields (SDF) of different layers to avoid self-intersections among different garment surfaces and the human body. Experiments demonstrate the strength of our proposed LGN framework in generating multi-layer garments as compared to state-of-the-art methods. To the best of our knowledge, LGN is the first research work to generate intersection-free multiple layers of garments on the human body from a single image."
  },
  "accv2022_main_pyramidalsigneddistancelearningforspatio-temporalhumanshapecompletion": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Pyramidal Signed Distance Learning for Spatio-Temporal Human Shape Completion",
    "authors": [
      "Boyao Zhou",
      "Jean-Sebastien Franco",
      "Martin de la gorce",
      "Edmond Boyer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhou_Pyramidal_Signed_Distance_Learning_for_Spatio-Temporal_Human_Shape_Completion_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhou_Pyramidal_Signed_Distance_Learning_for_Spatio-Temporal_Human_Shape_Completion_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We address the problem of completing partial human shape observations as obtained with a depth camera. Existing methods that solve this problem can provide robustness, with for instance model-based strategies that rely on parametric human models, or precision, with learning approaches that can capture local geometric patterns using implicit neural representations. We investigate how to combine both properties with a novel pyramidal spatio-temporal learning model. This model exploits neural signed distance fields in a coarse-to-fine manner, this in order to benefit from the ability of implicit neural representations to preserve local geometry details while enforcing more global spatial consistency for the estimated shapes through features at coarser levels. In addition, our model also leverages temporal redundancy with spatio-temporal features that integrate information over neighboring frames. Experiments on standard datasets show that both the coarse-to-fine and temporal aggregation strategies contribute to outperform the state-of-the-art methods on human shape completion."
  },
  "accv2022_main_neuralpuppeteerkeypoint-basedneuralrenderingofdynamicshapes": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Neural Puppeteer: Keypoint-Based Neural Rendering of Dynamic Shapes",
    "authors": [
      "Simon Giebenhain",
      "Urs Waldmann",
      "Ole Johannsen",
      "Bastian Goldluecke"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Giebenhain_Neural_Puppeteer_Keypoint-Based_Neural_Rendering_of_Dynamic_Shapes_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Giebenhain_Neural_Puppeteer_Keypoint-Based_Neural_Rendering_of_Dynamic_Shapes_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We introduce Neural Puppeteer, an efficient neural rendering pipeline for articulated shapes. By inverse rendering, we can predict 3D keypoints from multi-view 2D silhouettes alone, without requiring texture information. Furthermore, we can easily predict 3D keypoints of the same class of shapes with one and the same trained model and generalize more easily from training with synthetic data which we demonstrate by successfully applying zero-shot synthetic to real-world experiments. We demonstrate the flexibility of our method by fitting models to synthetic videos of different animals and a human, and achieve quantitative results which outperform our baselines. Our method uses 3D keypoints in conjunction with individual local feature vectors and a global latent code to allow for an efficient representation of time-varying and articulated shapes such as humans and animals. In contrast to previous work, we do not perform reconstruction in the 3D domain, but project the 3D features into 2D cameras and perform reconstruction of 2D RGB-D images from these projected features, which is significantly faster than volumetric rendering. Our synthetic dataset will be publicly available, to further develop the evolving field of animal pose and shape reconstruction."
  },
  "accv2022_main_scoadsingle-frameclicksupervisionforonlineactiondetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SCOAD: Single-frame Click Supervision for Online Action Detection",
    "authors": [
      "Na Ye",
      "Xing Zhang",
      "Dawei Yan",
      "Wei Dong",
      "Qingsen Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ye_SCOAD_Single-frame_Click_Supervision_for_Online_Action_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ye_SCOAD_Single-frame_Click_Supervision_for_Online_Action_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Online action detection based on supervised learning requires heavy manual annotation, which is difficult to obtain and may be impractical in real applications. Weakly supervised online action detection (WOAD) can effectively mitigate the problem of substantial labeling costs by using video-level labels. In this paper, we revisit WOAD and propose a weakly supervised online action detection using click-level labels for training, named Single-frame Click Supervision for Online Action Detection (SCOAD). Comparatively, click-level labels can effectively improve prediction accuracy by carrying a small amount of temporal information without massively increase the difficulty and cost of annotation. Specifically, SCOAD includes two joint training modules, i.e., Action Instance Miner (AIM) and Online Action Detector (OAD). To provide more guidance for training network as accuracy as possible, AIM mines pseudo-action instances under the supervision of click labels. Meanwhile, we generate video similarity instances offline by the similarity between video frames and use it to perform finer granularity filtering of error instances generated by AIM. OAD is trained jointly with AIM for online action detection by the pseudo frame-level labels converted from the filtered pseudo-action instances. We conduct extensive experiments on two benchmark datasets to demonstrate that SCOAD can effectively mine and utilize the small amount of temporal information in click-level labels. Code is available at https://github.com/zstarN70/SCOAD.git."
  },
  "accv2022_main_ovptoptimalviewsetpoolingtransformerfor3dobjectrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "OVPT: Optimal Viewset Pooling Transformer for 3D Object Recognition",
    "authors": [
      "Wenju Wang",
      "Gang Chen",
      "Haoran Zhou",
      "Xiaolin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_OVPT_Optimal_Viewset_Pooling_Transformer_for_3D_Object_Recognition_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_OVPT_Optimal_Viewset_Pooling_Transformer_for_3D_Object_Recognition_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The current methods for multi-view-based 3D object recognition have the problem of losing the correlation between views and rendering 3D objects with multi-view redundancy. This makes it difficult to improve recognition performance and unnecessarily increases the computational cost and running time of the network. Especially in the case of limited computing resources, the recognition performance is further affected. Our study developed an optimal viewset pooling transformer (OVPT) method for efficient and accurate 3D object recognition. The OVPT method constructs the optimal viewset based on information entropy to reduce the redundancy of the multi-view scheme. We used convolutional neural network (CNN) to extract the multi-view low-level local features of the optimal viewset. Embedding class token into the headers of multi-view low-level local features and splicing with position encoding generates local-view token sequences. This sequence was trained parallel with a pooling transformer to generate a local view information token sequence. At the same time, the global class token captured the global feature information of the local view token sequence. The two were aggregated next into a single compact 3D global feature descriptor. On two public benchmarks, ModelNet10 and ModelNet40, for each 3D object we only need a smaller number of optimal viewsets, achieving an overall recognition accuracy (OA) of 99.33% and 97.48%, respectively. Compared with other deep learning methods, our method still achieves state-of-the-art performance with limited computational resources. Our source code is available at https://github.com/shepherds001/OVPT."
  },
  "accv2022_main_digdrapingimplicitgarmentoverthehumanbody": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DIG: Draping Implicit Garment over the Human Body",
    "authors": [
      "Ren Li",
      "Benoit Guillard",
      "Edoardo Remelli",
      "Pascal Fua"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_DIG_Draping_Implicit_Garment_over_the_Human_Body_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_DIG_Draping_Implicit_Garment_over_the_Human_Body_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Existing data-driven methods for draping garments over human bodies, despite being effective, cannot handle garments of arbitrary topology and are typically not end-to-end differentiable. To address these limitations, we propose an end-to-end differentiable pipeline that represents garments using implicit surfaces and learns a skinning field conditioned on shape and pose parameters of an articulated body model. To limit body-garment interpenetrations and artifacts, we propose an interpenetration-aware pre-processing strategy of training data and a novel training loss that penalizes self-intersections while draping garments. We demonstrate that our method yields more accurate results for garment reconstruction and deformation with respect to state of the art methods. Furthermore, we show that our method, thanks to its end-to-end differentiability, allows to recover body and garments parameters jointly from image observations, something that previous work could not do. Our code is available at https://github.com/liren2515/DIG."
  },
  "accv2022_main_learningandtransforminggeneralrepresentationstobreakdownstability-plasticitydilemma": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning and Transforming General Representations to Break Down Stability-Plasticity Dilemma",
    "authors": [
      "Kengo Murata",
      "Seiya Ito",
      "Kouzou Ohara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Murata_Learning_and_Transforming_General_Representations_to_Break_Down_Stability-Plasticity_Dilemma_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Murata_Learning_and_Transforming_General_Representations_to_Break_Down_Stability-Plasticity_Dilemma_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In the Class Incremental Learning (CIL) setup, a learning model must have the ability to incrementally update its knowledge to recognize newly appeared classes (plasticity) while maintaining the knowledge to recognize the classes it has already learned (stability). Such conflicting requirements are known as the stability-plasticity dilemma, and most existing studies attempt to achieve a good balance between them by stability improvements. Unlike those attempts, we focus on the generality of representations. The basic idea is that a model does not need to change if it has already learned such general representations that they contain enough information to recognize new classes. However, the general representations are not optimal for recognizing the classes a model has already learned because the representations must contain unrelated and noisy information for recognizing them. To acquire representations suitable for recognizing known classes while leveraging general representations, in this paper, we propose a new CIL framework that learns general representations and transforms them into suitable ones for the target classification tasks. In our framework, we achieve the acquisition of general representations and their transformation by self-supervised learning and attention techniques, respectively. In addition, we introduce a novel knowledge distillation loss to make the transformation mechanism stable. Using benchmark datasets, we empirically confirm that our framework can improve the average incremental accuracy of four types of CIL methods that employ knowledge distillation in the CIL setting."
  },
  "accv2022_main_foreground-specializedmodelimitationforinstancesegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Foreground-Specialized Model Imitation for Instance Segmentation",
    "authors": [
      "Dawei Li",
      "Wenbo Li",
      "Hongxia Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Foreground-Specialized_Model_Imitation_for_Instance_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Foreground-Specialized_Model_Imitation_for_Instance_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Instance segmentation is formulated as a multi-task learning problem. However, knowledge distillation is not well-suited to all sub-tasks except the multi-class object classification. Based on such a competence, we introduce a lightweight foreground-specialized (FS) teacher model, which is trained with foreground-only images and highly optimized for object classification. Yet, this leads to discrepancy between inputs to the teacher and student models. Thus, we introduce a novel Foreground-Specialized model Imitation (FSI) method with two complementary components. First, a reciprocal anchor box selection method is introduced to distill from the most informative output of the FS teacher. Second, we embed the foreground-awareness into student's feature learning via either adding a co-learned foreground segmentation branch or applying a soft feature mask. We conducted an extensive evaluation against the others on COCO and Pascal VOC."
  },
  "accv2022_main_ppr-netpatch-basedmulti-scalepyramidregistrationnetworkfordefectdetectionofprintedlabels": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PPR-Net: Patch-based multi-scale pyramid registration network for defect detection of printed labels",
    "authors": [
      "Dongming Li",
      "Yingjian Li",
      "Jinxing Li",
      "Guangming Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_PPR-Net_Patch-based_multi-scale_pyramid_registration_network_for_defect_detection_of_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_PPR-Net_Patch-based_multi-scale_pyramid_registration_network_for_defect_detection_of_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Detecting defects in printed labels is essential to ensure product quality. Reference-based comparison is a potential method to challenge this task, which is widely used for defect detection. However, this method gets poor performance under large deformation, due to the lack of ability of registering the testing image with the reference image. Therefore, accurate image registration is an urgent case for defect detection of printed labels. In this paper, a patch-based multi-scale pyramid registration network (PPR-Net) is proposed. First, an image patch splitting and stitching strategy is proposed, which is scalable in image resolution. Second, a multi-scale pyramid registration module is designed to fuse multiple convolutional features to enhance the registration capability for large deformation, which gradually refines multi-scale deformation fields in a coarse-to-fine manner.Third, a distortion loss function is introduced to improve text distortions of registered images. Finally, a synthetic database is generated based on real printed labels, to simulate defective printed labels with large deformation for performance comparison. Extensive experimental results show that our method dramatically outperforms other comparable approaches."
  },
  "accv2022_main_exposingfaceforgerycluesviaretinex-basedimageenhancement": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Exposing Face Forgery Clues via Retinex-based Image Enhancement",
    "authors": [
      "Han Chen",
      "Yuzhen Lin",
      "Bin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_Exposing_Face_Forgery_Clues_via_Retinex-based_Image_Enhancement_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Exposing_Face_Forgery_Clues_via_Retinex-based_Image_Enhancement_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Public concerns about deepfake face forgery are continually rising in recent years. Existing deepfake detection approaches typically use convolutional neural networks (CNNs) to mine subtle artifacts under high-quality forged faces. However, most CNN-based deepfake detectors tend to over-fit the content-specific color textures, and thus fail to generalize across different data sources, forgery methods, and/or postprocessing operations. It motivates us to develop a method to expose the subtle forgery clues in RGB space. Herein, we propose to utilize multiscale retinex-based enhancement of RGB space and compose a novel modality, named MSR, to complementary capture the forgery traces. To take full advantage of the MSR information, we propose a two-stream network combined with salience-guided attention and feature re-weighted interaction modules. The salience-guided attention module guides the RGB feature extractor to concentrate more on forgery traces from an MSR perspective. The feature re-weighted interaction module implicitly learns the correlation between the two complementary modalities to promote feature learning for each other. Comprehensive experiments on several benchmarks show that our method outperforms the state-of-the-art face forgery detection methods in detecting severely compressed deepfakes. Besides, our method also shows superior performances on crossdatasets evaluation."
  },
  "accv2022_main_occludedfacialexpressionrecognitionusingself-supervisedlearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Occluded Facial Expression Recognition using Self-supervised Learning",
    "authors": [
      "Jiahe Wang",
      "Heyan Ding",
      "Shangfei Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Occluded_Facial_Expression_Recognition_using_Self-supervised_Learning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Occluded_Facial_Expression_Recognition_using_Self-supervised_Learning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recent studies on occluded facial expression recognition typically required fully expression-annotated facial images for training. However, it is time consuming and expensive to collect a large number of facial images with various occlusions and expression annotations. To address this problem, we propose an occluded facial expression recognition method through self-supervised learning, which leverages the profusion of available unlabeled facial images to explore robust facial representations. Specifically, we generate a variety of occluded facial images by randomly adding occlusions to unlabeled facial images. Then we define occlusion prediction as the pretext task for representation learning. We also adopt contrastive learning to make facial representation of a facial image and those of its variations with synthesized occlusions close.Finally, we train an expression classifier as the downstream task. The experimental results on several databases containing both synthesized and realistic occluded facial images demonstrate the superiority of the proposed method over state-of-the-art methods."
  },
  "accv2022_main_re-parameterizationmakinggc-net-style3dconvnetsmoreefficient": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Re-parameterization Making GC-Net-style 3DConvNets More Efficient",
    "authors": [
      "Takeshi Endo",
      "Seigo Kaji",
      "Haruki Matono",
      "Masayuki Takemura",
      "Takeshi Shima"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Endo_Re-parameterization_Making_GC-Net-style_3DConvNets_More_Efficient_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Endo_Re-parameterization_Making_GC-Net-style_3DConvNets_More_Efficient_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "For depth estimation using a stereo pair, deep learning methods using 3D convolution have been proposed. While the estimation accuracy is high, 3D convolutions on cost volumes are computationally expensive. Hence, we propose a method to reduce the computational cost of 3D convolution-based disparity networks. We apply kernel re-parameterization, which is used for constructing efficient backbones, to disparity estimation. We convert learned parameters, and these values are used for inference to reduce the computational cost of filtering cost volumes. Experimental results on the KITTI 2015 dataset show that our method can reduce the computational cost by 31-61% from those of trained models without any performance loss. Our method can be used for any disparity network that uses 3D convolution for cost volume filtering."
  },
  "accv2022_main_dhg-gandiverseimageoutpaintingviadecoupledhighfrequencysemantics": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DHG-GAN: Diverse Image Outpainting via Decoupled High Frequency Semantics",
    "authors": [
      "Yiwen Xu",
      "Maurice Pagnucco",
      "Yang Song"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Xu_DHG-GAN_Diverse_Image_Outpainting_via_Decoupled_High_Frequency_Semantics_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Xu_DHG-GAN_Diverse_Image_Outpainting_via_Decoupled_High_Frequency_Semantics_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Diverse image outpainting aims to restore large missing regions surrounding a known region while generating multiple plausible results. Although existing outpainting methods have demonstrated promising quality of image reconstruction, they are ineffective for providing both diverse and realistic content. This paper proposes a Decoupled High-frequency semantic Guidance-based GAN (DHG-GAN) for diverse image outpainting with the following contributions. 1) We propose a two-stage method, in which the first stage generates high-frequency semantic images for guidance of structural and textural information in the outpainting region and the second stage is a semantic completion network for completing the image outpainting based on this semantic guidance. 2) We design spatially varying stylemaps to enable targeted editing of high-frequency semantics in the outpainting region to generate diverse and realistic results. We evaluate the photorealism and quality of the diverse results generated by our model on CelebA-HQ, Place2 and Oxford Flower102 datasets. The experimental results demonstrate large improvement over state-of-the-art approaches."
  },
  "accv2022_main_generalizedpersonre-identificationbylocatingandeliminatingdomain-sensitivefeatures": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Generalized Person Re-identification by Locating and Eliminating Domain-Sensitive Features",
    "authors": [
      "Wendong Wang",
      "Fengxiang Yang",
      "Zhiming Luo",
      "Shaozi Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Generalized_Person_Re-identification_by_Locating_and_Eliminating_Domain-Sensitive_Features_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Generalized_Person_Re-identification_by_Locating_and_Eliminating_Domain-Sensitive_Features_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this paper, we study the problem of domain generalization for person re-identification (re-ID), which adopts training data from multiple domains to learn a re-ID model that can be directly deployed to unseen target domains without further fine-tuning. One promising idea is removing the subsets of features that are not beneficial to the generalization of models. This can be achieved by muting the subset features that correspond to high back-propagated gradients as these subsets are easy for the model to overfit. But this method ignores the interaction of multiple domains. Therefore, we propose a novel method to solve this problem by comparing the gradients from two different training schemes. One of the training schemes discriminates input data from their corresponding domain to obtain back-propagated temporary gradients in the intermediate features. At the same time, another scheme discriminates input data from all domains to obtain the temporary gradients. By comparing the temporary gradient between the two schemes, we can identify the domain-generalizable subset features from those domain-specific subset features. We thus mute them in the subsequent training process to enforce the model to learn domain-generalizable information and improve its generalization. Extensive experiments on four large-scale re-ID benchmarks have verified the effectiveness of our method. Code is available at https://github.com/Ssd111/LEDF.git."
  },
  "accv2022_main_patchembeddingaslocalfeaturesunifyingdeeplocalandglobalfeaturesviavisiontransformerforimageretrieval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Patch Embedding as Local Features: Unifying Deep Local and Global Features Via Vision Transformer for Image Retrieval",
    "authors": [
      "Lam Phan",
      "Hiep Thi Hong Nguyen",
      "Harikrishna Warrier",
      "Yogesh Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Phan_Patch_Embedding_as_Local_Features_Unifying_Deep_Local_and_Global_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Phan_Patch_Embedding_as_Local_Features_Unifying_Deep_Local_and_Global_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Image retrieval is the task of finding all images in the database that are similar to a query image. Two types of image representations have been studied to address this task: global and local image features. Those features can be extracted separately or jointly in a single model. State-of-the-art methods usually learn them with Convolutional Neural Networks (CNNs) and perform retrieval with multi-scale image representation. This paper's main contribution is to unify global and local features with Vision Transformers (ViTs) and multi-atrous convolutions for high-performing retrieval. We refer to the new model as ViTGaL, standing for Vision Transformer based Global and Local features (ViTGaL). Specifically, we add a multi-atrous convolution to the output of the transformer encoder layer of ViTs to simulate the image pyramid used in standard image retrieval algorithms. We use class attention to aggregate the token embeddings output from the multi-atrous layer to get both global and local features. The entire network can be learned end-to-end, requiring only image-level labels. Extensive experiments show the proposed method outperforms the state-of-the-art methods on the Revisited Oxford and Paris datasets."
  },
  "accv2022_main_multi-scalewavelettransformerforfaceforgerydetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-Scale Wavelet Transformer for Face Forgery Detection",
    "authors": [
      "Jie Liu",
      "Jingjing Wang",
      "Peng Zhang",
      "Chunmao Wang",
      "Di Xie",
      "Shiliang Pu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Liu_Multi-Scale_Wavelet_Transformer_for_Face_Forgery_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Liu_Multi-Scale_Wavelet_Transformer_for_Face_Forgery_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Currently, many face forgery detection methods aggregate spatial and frequency features to enhance the generalization ability and gain promising performance under the cross-dataset scenario. However, these methods only leverage one level frequency information which limits their expressive ability. To overcome these limitations, we propose a multi-scale wavelet transformer framework for face forgery detection. Specifically, to take full advantage of the multi-scale and multi-frequency wavelet representation, we gradually aggregate the multi-scale wavelet representation at different stages of the backbone network. To better fuse the frequency feature with the spatial features, frequency-based spatial attention is designed to guide the spatial feature extractor to concentrate more on forgery traces. Meanwhile, cross-modality attention is proposed to fuse the frequency features with the spatial features. These two attention modules are calculated through a unified transformer block for efficiency. A wide variety of experiments demonstrate that the proposed method is efficient and effective for both within and cross datasets."
  },
  "accv2022_main_full-scaleselectivetransformerforsemanticsegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Full-scale Selective Transformer for Semantic Segmentation",
    "authors": [
      "Fangjian Lin",
      "Sitong Wu",
      "Yizhe Ma",
      "Shengwei Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lin_Full-scale_Selective_Transformer_for_Semantic_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lin_Full-scale_Selective_Transformer_for_Semantic_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this paper, we rethink the multi-scale feature fusion from two perspectives (scale-level and spatial-level) and propose a full-scale selective fusion strategy for semantic segmentation. Based on such strategy, we design a novel segmentation network, named Full-scale Selective Transformer (FSFormer). Specifically, our FSFormer adaptively selects partial tokens from all tokens at all scales to construct a token subset of interest for each scale. Therefore, each token only interacts with the tokens within its corresponding token subset of interest. The proposed full-scale selective fusion strategy can not only filter out the noisy information propagation but also reduce the computational costs to some extent. We evaluate our FSFormer on four challenging semantic segmentation benchmarks, including PASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the state-of-the-art methods."
  },
  "accv2022_main_exp-gan3d-awarefacialimagegenerationwithexpressioncontrol": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Exp-GAN: 3D-Aware Facial Image Generation with Expression Control",
    "authors": [
      "Yeonkyeong Lee",
      "Taeho Choi",
      "Hyunsung Go",
      "Hyunjoon Lee",
      "Sunghyun Cho",
      "Junho Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lee_Exp-GAN_3D-Aware_Facial_Image_Generation_with_Expression_Control_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lee_Exp-GAN_3D-Aware_Facial_Image_Generation_with_Expression_Control_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper introduces Exp-GAN, a 3D-aware facial image generator with explicit control of facial expressions. Unlike previous 3D-aware GANs, Exp-GAN supports fine-grained control over facial shapes and expressions disentangled from poses. To this ends, we propose a novel hybrid approach that adopts a 3D morphable model (3DMM) with neural textures for the facial region and a neural radiance field (NeRF) for non-facial regions with multi-view consistency. The 3DMM allows fine-grained control over facial expressions, whereas the NeRF contains volumetric features for the non-facial regions. The two features, generated separately, are combined seamlessly with our depth-based integration method that integrates the two complementary features through volume rendering. We also propose a training scheme that encourages generated images to reflect control over shapes and expressions faithfully. Experimental results show that the proposed approach successfully synthesizes realistic view consistent face images with fine-grained controls. Code is available at https://github.com/kakaobrain/expgan."
  },
  "accv2022_main_cmt-cocontrastivelearningwithcharactermovementtaskforhandwrittentextrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CMT-Co: Contrastive Learning with Character Movement Task for Handwritten Text Recognition",
    "authors": [
      "Xiaoyi Zhang",
      "Jiapeng Wang",
      "Lianwen Jin",
      "Yujin Ren",
      "Yang Xue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_CMT-Co_Contrastive_Learning_with_Character_Movement_Task_for_Handwritten_Text_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_CMT-Co_Contrastive_Learning_with_Character_Movement_Task_for_Handwritten_Text_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Mainstream handwritten text recognition (HTR) approaches require large-scale labeled data for training to achieve satisfactory performance. Recently, contrastive learning has been introduced to perform self-supervised training on unlabeled data to improve representational capacity. It minimizes the distance between the positive pairs while maximizing their distance to the negative ones. Previous studies typically consider each frame or a fixed window of frames in a sequential feature map as a separate instance for contrastive learning. However, owing to the arbitrariness of handwriting and the diversity of word length, such modeling may contain the information of multiple consecutive characters or an over-segmented sub-character, which may confuse the model to perceive semantic clues information. To address this issue, in this paper, we design a character-level pretext task termed Character Movement Task, to assist word-level contrastive learning, namely CMT-Co. It moves the characters in a word to generate artifacts and guides the model to perceive the text content by using the moving direction and distance as supervision. In addition, we customize a data augmentation strategy specifically for handwritten text, which significantly contributes to the construction of training pairs for contrastive learning. Experiments have shown that the proposed CMT-Co achieves competitive or even superior performance compared to previous methods on public handwritten benchmarks."
  },
  "accv2022_main_learningtopredictdecomposeddynamicfiltersforsingleimagemotiondeblurring": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning to Predict Decomposed Dynamic Filters for Single Image Motion Deblurring",
    "authors": [
      "Zhiqiang Hu",
      "Tao Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hu_Learning_to_Predict_Decomposed_Dynamic_Filters_for_Single_Image_Motion_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hu_Learning_to_Predict_Decomposed_Dynamic_Filters_for_Single_Image_Motion_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper tackles the large motion variation problem in the single image dynamic scene deblurring task. Although fully convolutional multi-scale-based designs have recently advanced the state-of-the-art in single image motion deblurring. However, these approaches usually utilize vanilla convolution filters, which are not adapted to each spatial position. Consequently, it is hard to handle large motion blur variations at the pixel level. In this work, we propose Decomposed Dynamic Filters (DDF), a highly effective plug-and-play adaptive operator, to fulfill the goal of handling large motion blur variations across different spatial locations. In contrast to conventional dynamic convolution-based methods, which only predict either weight or offsets of the filter from the local feature at run time, in our work, both the offsets and weight are adaptively predicted from multi-scale local regions. The proposed operator comprises two components: 1) the offsets estimation module and 2) the pixel-specific filter weight generator. We incorporate the DDF into a lightweight encoder-decoder-based deblurring architecture to verify the performance gain. Extensive experiments conducted on the GoPro, HIDE, Real Blur, SIDD, and DND datasets demonstrate that the proposed method offers significant improvements over the state-of-the-art in accuracy as well as generalization capability. Code is available at: https://github.com/ZHIQIANGHU2021/DecomposedDynamicFilters"
  },
  "accv2022_main_whatroledoesdataaugmentationplayinknowledgedistillation?": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "What Role Does Data Augmentation Play in Knowledge Distillation?",
    "authors": [
      "Wei Li",
      "Shitong Shao",
      "Weiyan Liu",
      "Ziming Qiu",
      "Zhihao Zhu",
      "Wei Huan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_What_Role_Does_Data_Augmentation_Play_in_Knowledge_Distillation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_What_Role_Does_Data_Augmentation_Play_in_Knowledge_Distillation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Knowledge distillation is an effective way to transfer knowledge from a large model to a small model, which can significantly improve the performance of the small model. In recent years, some contrastive learning-based knowledge distillation methods (i.e., SSKD and HSAKD) have achieved excellent performance by utilizing data augmentation. However, the worth of data augmentation has always been overlooked by researchers in knowledge distillation, and no work analyzes its role in particular detail. To fix this gap, we analyze the effect of data augmentation on knowledge distillation from a multi-sided perspective. In particular, we demonstrate the following properties of data augmentation: (a) data augmentation can effectively help knowledge distillation work even if the teacher model does not have the information about augmented samples, and our proposed diverse and rich Joint Data Augmentation (JDA) is more valid than single rotating in knowledge distillation; (b) using diverse and rich augmented samples to assist the teacher model in training can improve its performance, but not the performance of the student model; (c) the student model can achieve excellent performance when the proportion of augmented samples is within a suitable range; (d) data augmentation enables knowledge distillation to work better in a few-shot scenario; (e) data augmentation is seamlessly compatible with some knowledge distillation methods and can potentially further improve their performance. Enlightened by the above analysis, we propose a method named Cosine Confidence Distillation (CCD) to transfer the augmented samples' knowledge more reasonably. And CCD achieves better performance than the latest SOTA HSAKD with fewer storage requirements on CIFAR-100 and ImageNet-1k. Our code will be released."
  },
  "accv2022_main_dualblndualbranchlut-awarenetworkforreal-timeimageretouching": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DualBLN: Dual Branch LUT-aware Network for Real-time Image Retouching",
    "authors": [
      "Xiang Zhang",
      "Chengzhe Lu",
      "Dawei Yan",
      "Wei Dong",
      "Qingsen Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_DualBLN_Dual_Branch_LUT-aware_Network_for_Real-time_Image_Retouching_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_DualBLN_Dual_Branch_LUT-aware_Network_for_Real-time_Image_Retouching_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The 3D Lookup Table (3D LUT) is an efficient tool for image retouching tasks, which models non-linear 3D color transformations by sparsely sampling them into a discrete 3D lattice. We propose DualBLN (Dual Branch LUT-aware Network) which innovatively incorporates the data representing the color transformation of 3D LUT into the real-time retouching process, which forces the network to learn the adaptive weights and the multiple 3D LUTs with strong representation capability. The estimated adaptive weights not only consider the content of the raw input but also use the information of the learned 3D LUTs. Specifically, the network contains two branches for feature extraction from the input image and 3D LUTs, to regard the information of the image and the 3D LUTs, and generate the precise LUT fusion weights. In addition, to better integrate the features of the input image and the learned 3D LUTs, we employ bilinear pooling to solve the problem of feature information loss that occurs when fusing features from the dual branch network, avoiding the feature distortion caused by direct concatenation or summation. Extensive experiments on several datasets demonstrate the effectiveness of our work, which is also efficient in processing high-resolution images. Our approach is not limited to image retouching tasks, but can also be applied to other pairwise learning-based tasks with fairly good generality."
  },
  "accv2022_main_temporal-viewpointtransportationplanforskeletalfew-shotactionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition",
    "authors": [
      "Lei Wang",
      "Piotr Koniusz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Temporal-Viewpoint_Transportation_Plan_for_Skeletal_Few-shot_Action_Recognition_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Temporal-Viewpoint_Transportation_Plan_for_Skeletal_Few-shot_Action_Recognition_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We propose a Few-shot Learning pipeline for 3D skeleton-based action recognition by Joint Temporal and Camera Viewpoint Alignment. To factor out misalignment between query and support sequences of 3D body joints, we propose an advanced variant of Dynamic Time Warping which jointly models each smooth path between the query and support frames to achieve simultaneously the best alignment in the temporal and simulated camera viewpoint spaces for end-to-end learning under the limited few-shot training data. Sequences are encoded with a temporal block encoder based on Simple Spectral Graph Convolution, a lightweight linear Graph Neural Network backbone (we also include a setting with a transformer).Finally, we propose a similarity-based loss which encourages the alignment of sequences of the same class while preventing the alignment of unrelated sequences. We show state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II."
  },
  "accv2022_main_neighborhoodregionsmoothingregularizationforfindingflatminimaindeepneuralnetworks": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Neighborhood Region Smoothing Regularization for Finding Flat Minima In Deep Neural Networks",
    "authors": [
      "Yang Zhao",
      "Hao Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_Neighborhood_Region_Smoothing_Regularization_for_Finding_Flat_Minima_In_Deep_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_Neighborhood_Region_Smoothing_Regularization_for_Finding_Flat_Minima_In_Deep_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Due to diverse architectures in deep neural networks (DNNs) with severe overparameterization, regularization techniques are critical for finding optimal solutions in the huge hypothesis space. In this paper, we propose an effective regularization technique, called Neighborhood Region Smoothing (NRS). NRS leverages the finding that models would benefit from converging to flat minima, and tries to regularize the neighborhood region in weight space to yield approximate outputs. Specifically, gap between outputs of models in the neighborhood region is gauged by a defined metric based on Kullback-Leibler divergence. This metric could provide insights in accordance with the minimum description length principle on interpreting flat minima. By minimizing both this divergence and empirical loss, NRS could explicitly drive the optimizer towards converging to flat minima, and meanwhile could be compatible with other common regularizations. We confirm the effectiveness of NRS by performing image classification tasks across a wide range of model architectures on commonly-used datasets such as CIFAR and ImageNet, where generalization ability could be universally improved. Also, we empirically show that the minima found by NRS would have relatively smaller Hessian eigenvalues compared to the conventional method, which is considered as the evidence of flat minima."
  },
  "accv2022_main_neo-3dfnovelediting-oriented3dfacecreationandreconstruction": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "NEO-3DF: Novel Editing-Oriented 3D Face Creation and Reconstruction",
    "authors": [
      "Peizhi Yan",
      "James Gregson",
      "Qiang Tang",
      "Rabab Ward",
      "Zhan Xu",
      "Shan Du"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yan_NEO-3DF_Novel_Editing-Oriented_3D_Face_Creation_and_Reconstruction_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yan_NEO-3DF_Novel_Editing-Oriented_3D_Face_Creation_and_Reconstruction_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Unlike 2D face images, obtaining a 3D face is not easy. Existing methods, therefore, create a 3D face from a 2D face image (3D face reconstruction). A user might wish to edit the reconstructed 3D face, but 3D face editing has seldom been studied. This paper presents such method and shows that reconstruction and editing can help each other. In the presented framework named NEO-3DF, the 3D face model we propose has independent sub-models corresponding to semantic face parts. It allows us to achieve both local intuitive editing and better 3D-to-2D alignment. Each face part in our model has a set of controllers designed to allow users to edit the corresponding features (e.g., nose height). In addition, we propose a differentiable module for blending the face parts and making it possible to automatically adjust the face parts (both the shapes and the locations) so that they are better aligned with the original 2D image. Experiments show that the results of NEO-3DF outperform existing methods in intuitive face editing and have better 3D-to-2D alignment accuracy (14% higher IoU) than global face model-based reconstruction."
  },
  "accv2022_main_trainingdynamicsawareneuralnetworkoptimizationwithstabilization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Training Dynamics Aware Neural Network Optimization with Stabilization",
    "authors": [
      "Zilin Fang",
      "Mohamad Shahbazi",
      "Thomas Probst",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Fang_Training_Dynamics_Aware_Neural_Network_Optimization_with_Stabilization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Fang_Training_Dynamics_Aware_Neural_Network_Optimization_with_Stabilization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We investigate the process of neural network training using gradient descent-based optimizers from a dynamic system point of view. To this end, we model the iterative parameter updates as a time-discrete switched linear system and analyze its stability behavior over the course of training. Accordingly, we develop a regularization scheme to encourage stable training dynamics by penalizing divergent parameter updates. Our experiments show promising stabilization and convergence effects on regression tasks, density-based crowd counting, and generative adversarial networks (GAN). Our results indicate that stable network training minimizes the variance of performance across different parameter initializations, and increases robustness to the choice of learning rate. Particularly in the GAN setup, the stability regularization enables faster convergence and lower FID with more consistency across runs. Our source code is available at: https://github.com/fangzl123/stableTrain.git"
  },
  "accv2022_main_spatialgroup-wiseenhanceenhancingsemanticfeaturelearningincnn": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Spatial Group-wise Enhance: Enhancing Semantic Feature Learning in CNN",
    "authors": [
      "Yuxuan Li",
      "Xiang Li",
      "Jian Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Spatial_Group-wise_Enhance_Enhancing_Semantic_Feature_Learning_in_CNN_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Spatial_Group-wise_Enhance_Enhancing_Semantic_Feature_Learning_in_CNN_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The success of attention modules in CNN has attracted increasing and widespread attention over the past years. However, most existing attention modules fail to consider two important factors: (1) For images, different semantic entities are located in different areas, thus they should be associated with different spatial attention masks; (2) most existing framework exploits individual local or global information to guide the generation of attention masks, which ignores the joint information of local-global similarities that can be more effective. To explore these two ingredients, we propose the Spatial Group-wise Enhance (SGE) module. SGE explicitly distributes different but accurate spatial attention masks for various semantics, through the guidance of local-global similarities inside each individual semantic feature group. Furthermore, SGE is lightweight with almost no extra parameters and calculations. Despite being trained with only category supervisions, SGE is effective in highlighting multiple active areas with various high-level semantics (such as the dog's eyes, nose, etc.). When integrated with popular CNN backbones, SGE can significantly boost their performance on image recognition tasks. Specifically, based on ResNet101 backbones, SGE improves the baseline by 0.7% Top-1 accuracy on ImageNet classification and 1.6% 1.8% AP on COCO detection tasks.The code and pretrained models are available at https://github.com/implus/PytorchInsight."
  },
  "accv2022_main_anrnn-basedframeworkforthemilpprobleminrobustnessverificationofneuralnetworks": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "An RNN-Based Framework for the MILP Problem in Robustness Verification of Neural Networks",
    "authors": [
      "Hao Xue",
      "Xia Zeng",
      "Wang Lin",
      "Zhengfeng Yang",
      "Chao Peng",
      "Zhenbing Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Xue_An_RNN-Based_Framework_for_the_MILP_Problem_in_Robustness_Verification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Xue_An_RNN-Based_Framework_for_the_MILP_Problem_in_Robustness_Verification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Robustness verification of deep neural networks is becoming increasingly crucial for their potential use in many safety-critical applications. Essentially, the problem of robustness verification can be encoded as a typical Mixed-Integer Linear Programming (MILP) problem, which can be solved via branch-and-bound strategies. However, these methods can only afford limited scalability and remain challenging for verifying large-scale neural networks. In this paper, we present a novel framework to speed up the solving of the MILP problems generated from the robustness verification of deep neural networks. It employs a semi-planet relaxation to abstract ReLU activation functions, via an RNN-based strategy for selecting the relaxed ReLU neurons to be tightened. We have developed a prototype tool L2T and conducted comparison experiments with state-of-the-art verifiers on a set of large-scale benchmarks. The experiments show that our framework is both efficient and scalable even when applied to verify the robustness of large-scale neural networks."
  },
  "accv2022_main_latentgazecross-domaingazeestimationthroughgaze-awareanalyticlatentcodemanipulation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "LatentGaze: Cross-Domain Gaze Estimation through Gaze-Aware Analytic Latent Code Manipulation",
    "authors": [
      "Isack Lee",
      "Jun-Seok Yun",
      "Hee Hyeon Kim",
      "Youngju Na",
      "Seok Bong Yoo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lee_LatentGaze_Cross-Domain_Gaze_Estimation_through_Gaze-Aware_Analytic_Latent_Code_Manipulation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lee_LatentGaze_Cross-Domain_Gaze_Estimation_through_Gaze-Aware_Analytic_Latent_Code_Manipulation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Although recent gaze estimation methods lay great emphasis on attentively extracting gaze-relevant features from facial or eye images, how to define features that include gaze-relevant components has been ambiguous. This obscurity makes the model learn not only gaze-relevant features but also irrelevant ones. In particular, it is fatal for the cross-dataset performance. To overcome this challenging issue, we propose a gaze-aware analytic manipulation method, based on a data-driven approach with generative adversarial network inversion's disentanglement characteristics, to selectively utilize gaze-relevant features in a latent code. Furthermore, by utilizing GAN-based encoder-generator process, we shift the input image from the target domain to the source domain image, which a gaze estimator is sufficiently aware. In addition, we propose gaze distortion loss in the encoder that prevents the distortion of gaze information. The experimental results demonstrate that our method achieves state-of-the-art gaze estimation accuracy in a cross-domain gaze estimation tasks. This code is available at https://github.com/leeisack/LatentGaze/."
  },
  "accv2022_main_decisionetabinary-treestructuredneuralnetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DecisioNet: A Binary-Tree Structured Neural Network",
    "authors": [
      "Noam Gottlieb",
      "Michael Werman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gottlieb_DecisioNet_A_Binary-Tree_Structured_Neural_Network_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gottlieb_DecisioNet_A_Binary-Tree_Structured_Neural_Network_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep neural networks (DNNs) and decision trees (DTs) are both state-of-the-art classifiers. DNNs perform well due to their representational learning capabilities, while DTs are computationally efficient as they perform inference along one route (root-to-leaf) that is dependent on the input data.In this paper, we present DecisioNet (DN), a binary-tree structured neural network. We propose a systematic way to convert an existing DNN into a DNto create a lightweight version of the original model.DecisioNet takes the best of both worlds - it uses neural modules to perform representational learning and utilizes its tree structure to perform only a portion of the computations.We evaluate various DN architectures, along with their corresponding baseline models on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN variants achieve similar accuracy while significantly reducing the computational cost of the original network."
  },
  "accv2022_main_conditionalganforpointcloudgeneration": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Conditional GAN for Point Cloud Generation",
    "authors": [
      "Zhulun Yang",
      "Yijun Chen",
      "Xianwei Zheng",
      "Yadong Chang",
      "Xutao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yang_Conditional_GAN_for_Point_Cloud_Generation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yang_Conditional_GAN_for_Point_Cloud_Generation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, 3D data generation problems have attracted more and more research attention and have been addressed through various approaches. However, most of them fail to generate objects with given desired categories and tend to produce hybrids of multiple types. Thus, this paper proposes a generative model for synthesizing high-quality point clouds with conditional information, which is called Point Cloud conditional Generative Adversarial Network (PC-cGAN). The generative model of the proposed PC-cGAN consists of two main components: a pre-generator to generate rough point clouds and a conditional modifier to refine the last outputs with specific categories. To improve the performance for multi-class conditional generation for point clouds, an improved tree-structured graph convolution network, called BranchGCN, is adopted to aggregate information from both ancestor and neighbor features. Experimental results demonstrate that the proposed PC-cGAN outperforms state-of-the-art GANs in terms of conventional distance metrics and novel latent metric, Frechet Point Distance, and avoids the intra-category hybridization problem and the unbalanced issue in generated sample distribution effectively.The results also show that PC-cGAN enables us to gain explicit control over the object category while maintaining good generation quality and diversity. The implementation of PC-cGAN is available at https://github.com/zlyang3/PC-cGAN."
  },
  "accv2022_main_comparingcomplexitiesofdecisionboundariesforrobusttrainingauniversalapproach": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Comparing Complexities of Decision Boundaries for Robust Training: A Universal Approach",
    "authors": [
      "Daniel Kienitz",
      "Ekaterina Komendantskaya",
      "Michael A Lones"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kienitz_Comparing_Complexities_of_Decision_Boundaries_for_Robust_Training_A_Universal_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kienitz_Comparing_Complexities_of_Decision_Boundaries_for_Robust_Training_A_Universal_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We investigate the geometric complexity of decision boundaries for robust training compared to standard training. By considering the local geometry of nearest neighbour sets, we study them in a model-agnostic way and theoretically derive a lower-bound \\Rstar \\in \\mathbb Ron the perturbation magnitude \\delta \\in \\mathbb Rfor which robust training provably requires a geometrically more complex decision boundary than accurate training. We show that state-of-the-art robust models learn more complex decision boundaries than their non-robust counterparts, confirming previous hypotheses. Then, we compute \\Rstar for common image benchmarks and find that it also empirically serves as an upper bound over which label noise is introduced. We demonstrate for deep neural network classifiers that perturbation magnitudes \\delta \\geq \\Rstar lead to reduced robustness and generalization performance. Therefore, \\Rstar bounds the maximum feasible perturbation magnitude for norm-bounded robust training and data augmentation. Finally, we show that \\Rstar < 0.5R for common benchmarks, where R is a distribution's minimum nearest neighbour distance. Thus, we improve previous work on determining a distribution's maximum robust radius."
  },
  "accv2022_main_rethinkingonlineknowledgedistillationwithmulti-exits": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Rethinking Online Knowledge Distillation with Multi-Exits",
    "authors": [
      "Hojung Lee",
      "Jong-Seok Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lee_Rethinking_Online_Knowledge_Distillation_with_Multi-Exits_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lee_Rethinking_Online_Knowledge_Distillation_with_Multi-Exits_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Online knowledge distillation is a method to train multiple networks simultaneously by distilling the knowledge among each other from scratch. An efficient way for this is to attach auxiliary classifiers (called exits) to the main network. However, in this multi-exit approach, there are important questions that have not been answered in previous studies: What structure should be used for exits? What can be a good teacher for distillation? How should the overall training loss be constructed? In this paper, we propose a new online knowledge distillation method using multi-exits by answering these questions. First, we examine the influence of the structure of the exits on the performance of the main network, and propose a bottleneck structure that leads to improved performance for a wide range of main network structures. Second, we propose a new distillation teacher using an ensemble of all the classifiers (main network and exits) by exploiting the diversity in the outputs and features of the classifiers. Third, we propose a new technique to form the overall training loss, which balances classification losses and distillation losses for effective training of the whole network. Our proposed method is termed balanced exit-ensemble distillation (BEED). Experimental results demonstrate that our method achieves significant improvement of classification performance on various popular convolutional neural network (CNN) structures. Code is available at https://github.com/hjdw2/BEED."
  },
  "accv2022_main_objectdetectioninfoggyscenesbyembeddingdepthandreconstructionintodomainadaptation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Object Detection in Foggy Scenes by Embedding Depth and Reconstruction into Domain Adaptation",
    "authors": [
      "Xin Yang",
      "Michael Bi Mi",
      "Yuan Yuan",
      "Xin Wang",
      "Robby T. Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yang_Object_Detection_in_Foggy_Scenes_by_Embedding_Depth_and_Reconstruction_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yang_Object_Detection_in_Foggy_Scenes_by_Embedding_Depth_and_Reconstruction_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Most existing domain adaptation (DA) methods align the features based on the domain feature distributions and ignore aspects related to fog, background and target objects, rendering suboptimal performance.In our DA framework, we retain the depth and background information during the domain feature alignment. A consistency loss between the generated depth and fog transmission map is introduced to strengthen the retention of the depth information in the aligned features. To address false object features potentially generated during the DA process, we propose an encoder-decoder framework to reconstruct the fog-free background image. This reconstruction loss also reinforces the encoder, i.e., our DA backbone, to minimize false object features. Moreover, we involve our target data in training both our DA module andour detection module in asemi-supervised manner,so that our detection module is also exposed to the unlabeled target data,the type of data used in the testing stage. Using these ideas, our methodsignificantly outperforms the state-of-the-art method (47.6 mAP against the 44.3 mAP on the Foggy Cityscapes dataset), and obtains the best performance on multiple real-image public datasets."
  },
  "accv2022_main_confidence-calibratedfaceimageforgerydetectionwithcontrastiverepresentationdistillation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Confidence-Calibrated Face Image Forgery Detection with Contrastive Representation Distillation",
    "authors": [
      "Puning Yang",
      "Huaibo Huang",
      "Zhiyong Wang",
      "Aijing Yu",
      "Ran He"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yang_Confidence-Calibrated_Face_Image_Forgery_Detection_with_Contrastive_Representation_Distillation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yang_Confidence-Calibrated_Face_Image_Forgery_Detection_with_Contrastive_Representation_Distillation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Face forgery detection has been increasingly investigated due to the great success of various deepfake techniques. While most existing face forgery detection methods have achieved excellent results on the test split of the same dataset or the same type of manipulations, they often do not work well on unseen datasets or unseen manipulations due to the issue of model generalization. Therefore, in this paper, we propose a novel contrastive distillation calibration (CDC) framework, which distills the contrastive representations with confidence calibration to address this generalization issue. Different from previous methods that equally treat the two forgery types, Face Swapping and Face Reenactment, we devise a dual-teacher module where the knowledge is separately learned for each forgery type. A contrastive representation learning strategy is further presented to enhance the representations of diverse forgery artifacts. To prevent the proposed model from being overconfident, we propose a novel Kullback-Leibler divergence loss with dynamic weights to moderate the dual-teacher's outputs. In addition, we introduce label smoothing to calibrate the model confidence with the target outputs. Extensive experiments on three popular datasets show that our proposed method achieves the state-of-the-art performance for cross-dataset face forgery detection."
  },
  "accv2022_main_learninginternalsemanticswithexpandedcategoriesforgenerativezero-shotlearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning Internal Semantics with Expanded Categories for Generative Zero-Shot Learning",
    "authors": [
      "Xiaojie Zhao",
      "Shidong Wang",
      "Haofeng Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_Learning_Internal_Semantics_with_Expanded_Categories_for_Generative_Zero-Shot_Learning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_Learning_Internal_Semantics_with_Expanded_Categories_for_Generative_Zero-Shot_Learning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In recent years, generative Zero-Shot Learning (ZSL) has attracted much attention due to its better performance than traditional embedding methods. Most generative ZSL methods exploit category semantics plus Gaussian noise to generate visual features. However, there is a contradiction between the unity of category semantics and the diversity of visual features. The semantics of a single category cannot accurately correspond to different individuals in the same category. This is due to the different visual expression of the same category. Therefore, to solve the above mentioned problem we propose a novel semantic augmentation method, which expands a single semantic to multi sub-semantics by learning expanded categories, so that the generated visual features are more in line with the real visual feature distribution. At the same time, according to the theory of Convergent Evolution, the sub-semantics of unseen classes are obtained on the basis of the expanded semantics of their similar seen classes. Four benchmark datasets are employed to verify the effectiveness of the proposed method. In addition, the category expansion is also applied to three generative methods, and the results demonstrate that category expansion can improve the performance of other generative methods."
  },
  "accv2022_main_self-supervisedlearningwithmulti-viewrenderingfor3dpointcloudanalysis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Self-Supervised Learning with Multi-View Rendering for 3D Point Cloud Analysis",
    "authors": [
      "Bach Tran",
      "Binh-Son Hua",
      "Anh Tuan Tran",
      "Minh Hoai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tran_Self-Supervised_Learning_with_Multi-View_Rendering_for_3D_Point_Cloud_Analysis_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tran_Self-Supervised_Learning_with_Multi-View_Rendering_for_3D_Point_Cloud_Analysis_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, great progress has been made in 3D deep learning with the emergence of deep neural networks specifically designed for 3D point clouds. These networks are often trained from scratch or from pre-trained models learned purely from point cloud data. Inspired by the success of deep learning in the image domain, we devise a novel pre-training technique for better model initialization by utilizing the multi-view rendering of the 3D data. Our pre-training is self-supervised by a local pixel/point level correspondence loss computed from perspective projection and a global image/point cloud level loss based on knowledge distillation, thus effectively improving upon popular point cloud networks, including PointNet, DGCNN and SR-UNet. These improved models outperform existing state-of-the-art methods on various datasets and downstream tasks. We also analyze the benefits of synthetic and real data for pre-training, and observe that pre-training on synthetic data is also useful for high-level downstream tasks. Code and pre-trained models are available at https://github.com/VinAIResearch/selfsup_pcd.git"
  },
  "accv2022_main_spotlightsprobingshapesfromsphericalviewpoints": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Spotlights: Probing Shapes from Spherical Viewpoints",
    "authors": [
      "Jiaxin Wei",
      "Lige Liu",
      "Ran Cheng",
      "Wenqing Jiang",
      "Minghao Xu",
      "Xinyu Jiang",
      "Tao Sun",
      "S\u00f6ren Schwertfeger",
      "Laurent Kneip"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wei_Spotlights_Probing_Shapes_from_Spherical_Viewpoints_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wei_Spotlights_Probing_Shapes_from_Spherical_Viewpoints_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recent years have witnessed the surge of learned representations that directly build upon point clouds. Inspired by spherical multi-view scanners, we propose a novel sampling model called Spotlights to represent a 3D shape as a compact 1D array of depth values. It simulates the configuration of cameras evenly distributed on a sphere, where each virtual camera casts light rays from its principal point to probe for possible intersections with the object surrounded by the sphere. The structured point cloud is hence given implicitly as a function of depths. We provide a detailed geometric analysis of this new sampling scheme and prove its effectiveness in the context of the point cloud completion task. Experimental results on both synthetic and real dataset demonstrate that our method achieves competitive accuracy and consistency while at a lower computational cost. The code and dataset will be released at https://github.com/goldoak/Spotlights."
  },
  "accv2022_main_pu-transformerpointcloudupsamplingtransformer": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PU-Transformer: Point Cloud Upsampling Transformer",
    "authors": [
      "Shi Qiu",
      "Saeed Anwar",
      "Nick Barnes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Qiu_PU-Transformer_Point_Cloud_Upsampling_Transformer_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Qiu_PU-Transformer_Point_Cloud_Upsampling_Transformer_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Given the rapid development of 3D scanners, point clouds are becoming popular in AI-driven machines. However, point cloud data is inherently sparse and irregular, causing major difficulties for machine perception. In this work, we focus on the point cloud upsampling task that intends to generate dense high-fidelity point clouds from sparse input data. Specifically, to activate the transformer's strong capability in representing features, we develop a new variant of a multi-head self-attention structure to enhance both point-wise and channel-wise relations of the feature map. In addition, we leverage a positional fusion block to comprehensively capture the local context of point cloud data, providing more position-related information about the scattered points. As the first transformer model introduced for point cloud upsampling, we demonstrate the outstanding performance of our approach by comparing with the state-of-the-art CNN-based methods on different benchmarks quantitatively and qualitatively."
  },
  "accv2022_main_alightweightlocal-globalattentionnetworkforsingleimagesuper-resolution": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Lightweight Local-Global Attention Network for Single Image Super-Resolution",
    "authors": [
      "Zijiang Song",
      "Baojiang Zhong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Song_A_Lightweight_Local-Global_Attention_Network_for_Single_Image_Super-Resolution_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Song_A_Lightweight_Local-Global_Attention_Network_for_Single_Image_Super-Resolution_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "For a given image, the self-attention mechanism aims to capture dependencies for each pixel. It has been proved that the performance of neural networks which employ self-attention is superior in various image processing tasks. However, the performance of self-attention has extensively correlated with the amount of computation. The vast majority of works tend to use local attention to capture local information to reduce the amount of calculation when using self-attention. The ability to capture information from the entire image is easily weakened on this occasion. In this paper, a local-global attention block (LGAB) is proposed to enhance both the local features and global features with low calculation complexity. To verify the performance of LGAB, a lightweight local-global attention network (LGAN) for single image super-resolution (SISR) is proposed and evaluated. Compared with other lightweight state-of-the-arts (SOTAs) of SISR, the superiority of our LGAN is demonstrated by extensive experimental results. The source code can be found at https://github.com/songzijiang/LGAN."
  },
  "accv2022_main_trackingsmallandfastmovingobjectsabenchmark": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Tracking Small and Fast Moving Objects: A Benchmark",
    "authors": [
      "Zhewen Zhang",
      "Fuliang Wu",
      "Yuming Qiu",
      "Jingdong Liang",
      "Shuiwang Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Tracking_Small_and_Fast_Moving_Objects_A_Benchmark_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Tracking_Small_and_Fast_Moving_Objects_A_Benchmark_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "With more and more large-scale datasets available for training, visual tracking has made great progress in recent years. However, current research in the field mainly focuses on tracking generic objects. In this paper, we present TSFMO, a benchmark for Tracking Small and Fast Moving Objects. This benchmark aims to encourage research in developing novel and accurate methods for this challenging task particularly. TSFMO consists of 250 sequences with about 50k frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box. To the best of our knowledge, TSFMO is the first benchmark dedicated to tracking small and fast moving objects, especially connected to sports. To understand how existing methods perform and to provide comparison for future research on TSFMO, we extensively evaluate 20 state-of-the-art trackers on the benchmark. The evaluation results exhibit that more effort are required to improve tracking small and fast moving objects. Moreover, to encourage future research, we proposed a novel tracker S-KeepTrack which surpasses all 20 evaluated approaches. By releasing TSFMO, we expect to facilitate future researches and applications of tracking small and fast moving objects. The TSFMO and evaluation results as well as S-KeepTrack are available at https://github.com/CodeOfGithub/S-KeepTrack."
  },
  "accv2022_main_airbirdsalarge-scalechallengingdatasetforbirdstrikepreventioninreal-worldairports": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "AirBirds: A Large-scale Challenging Dataset for Bird Strike Prevention in Real-world Airports",
    "authors": [
      "Hongyu Sun",
      "Yongcai Wang",
      "Xudong Cai",
      "Peng Wang",
      "Zhe Huang",
      "Deying Li",
      "Yu Shao",
      "Shuo Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Sun_AirBirds_A_Large-scale_Challenging_Dataset_for_Bird_Strike_Prevention_in_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Sun_AirBirds_A_Large-scale_Challenging_Dataset_for_Bird_Strike_Prevention_in_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "One fundamental limitation to the research of bird strike prevention is the lack of a large-scale dataset taken directly from real-world airports. Existing relevant datasets are either small in size or not dedicated for this purpose. To advance the research and practical solutions for bird strike prevention, in this paper, we present a large-scale challenging dataset AirBirds that consists of 118,312 time-series images, where a total of 409,967 bounding boxes of flying birds are manually, carefully annotated. The average size of all annotated instances is smaller than 10 pixels in 1920x1080 images. Images in the dataset are captured over 4 seasons of a whole year by a network of cameras deployed at a real-world airport, covering diverse bird species, lighting conditions and 13 meteorological scenarios. To the best of our knowledge, it is the first large-scale image dataset that directly collects flying birds in real-world airports for bird strike prevention. This dataset is publicly available at https://airbirdsdata.github.io/."
  },
  "accv2022_main_scfnetaspatial-channelfeaturesnetworkbasedonheterocentricsamplelossforvisible-infraredpersonre-identification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SCFNet: A Spatial-Channel Features Network based on Heterocentric Sample Loss for Visible-Infrared Person Re-Identification",
    "authors": [
      "Peng Su",
      "Rui Liu",
      "Jing Dong",
      "Pengfei Yi",
      "Dongsheng Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Su_SCFNet_A_Spatial-Channel_Features_Network_based_on_Heterocentric_Sample_Loss_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Su_SCFNet_A_Spatial-Channel_Features_Network_based_on_Heterocentric_Sample_Loss_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Cross-modality person re-identification between visible and infrared images has become a research hotspot in the image retrieval field due to its potential application scenarios. Existing research usually designs loss functions around samples or sample centers, mainly focusing on reducing cross-modality discrepancy and intra-modality variations. However, the sample-based loss function is susceptible to outliers, and the center-based loss function is not compact enough between features. To address the above issues, we propose a novel loss function called Heterocentric Sample Loss. It optimizes both the sample features and the center of the sample features in the batch. In addition, we also propose a network structure combining spatial and channel features and a random channel enhancement method, which improves feature discrimination and robustness to color changes. Finally, we conduct extensive experiments on the SYSU-MM01 and RegDB datasets to demonstrate the superiority of the proposed method."
  },
  "accv2022_main_skintonediagnosisinthewildtowardsmorerobustandinclusiveuserexperienceusingorientedaleatoricuncertainty": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Skin tone Diagnosis in the Wild: Towards More Robust and Inclusive User Experience Using Oriented Aleatoric Uncertainty",
    "authors": [
      "Emmanuel Malherbe",
      "Michel Remise",
      "Shuai Zhang",
      "Matthieu Perrot"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Malherbe_Skin_tone_Diagnosis_in_the_Wild_Towards_More_Robust_and_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Malherbe_Skin_tone_Diagnosis_in_the_Wild_Towards_More_Robust_and_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The past decade has seen major advances in deep learning models that are trained to predict a supervised label. However, estimating the uncertainty for a predicted value might provide great information beyond the prediction itself. To address this goal, using a probabilistic loss was proven efficient for aleatoric uncertainty, which aims at capturing noise originating from the observations. For multidimensional predictions, this estimated noise is generally a multivariate normal variable, characterized by a mean value and covariance matrix. While most of literature have focused on isotropic uncertainty, with diagonal covariance matrix, estimating full covariance brings additional information, such as the noise orientation in the output space.We propose in this paper a specific decomposition of the covariance matrix that can be efficiently estimated by the neural network. From our experimental comparison to the existing approaches, our model offers the best trade-off between uncertainty orientation likeliness, model accuracy and computation costs. Our industrial application is skin color estimation based on a selfie picture, which is at the core of an online make-up assistant but is a sensitive topic due to ethics and fairness considerations. Thanks to oriented uncertainty, we can reduce this risk by detecting uncertain cases and proposing a simplified color correction bar, thus making user experience more robust and inclusive."
  },
  "accv2022_main_dilanedynamicinstance-awarenetworkforlanedetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DILane: Dynamic Instance-Aware Network for Lane Detection",
    "authors": [
      "Zhengyun Cheng",
      "Guanwen Zhang",
      "Changhao Wang",
      "Wei Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cheng_DILane_Dynamic_Instance-Aware_Network_for_Lane_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cheng_DILane_Dynamic_Instance-Aware_Network_for_Lane_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Lane detection is a challenging task in computer vision and a critical technology in autonomous driving. The task requires the prediction of the topology of lane lines in complex scenarios; moreover, different types and instances of lane lines need to be distinguished. Most existing studies are based only on a single-level feature map extracted by deep neural networks. However, both high-level and low-level features are important for lane detection, because lanes are easily affected by illumination and occlusion, i.e., texture information is unavailable in non-visual evidence case; when the lanes are clearly visible, the curved and slender texture information plays a more important role in improving the detection accuracy. In this study, the proposed DILane utilizes both high-level and low-level features for accurate lane detection. First, in contrast to mainstream detection methods of predefined fixed-position anchors, we define learnable anchors to perform statistics of potential lane locations. Second, we propose a dynamic head aiming at leveraging low-level texture information to conditionally enhance high-level semantic features for each proposed instance. Finally, we present a self-attention module to gather global information in parallel, which remarkably improves detection accuracy. The experimental results on two mainstream public benchmarks demonstrate that our proposed method outperforms previous works with the F1 score of 79.43% for CULane and 97.80% for TuSimple dataset while achieving 148+ FPS."
  },
  "accv2022_main_bordernetanefficientborder-attentiontextdetector": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "BorderNet: An Efficient Border-Attention Text Detector",
    "authors": [
      "Juntao Cheng",
      "Liangru Xie",
      "Cheng Du"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cheng_BorderNet_An_Efficient_Border-Attention_Text_Detector_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cheng_BorderNet_An_Efficient_Border-Attention_Text_Detector_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, segmentation-based text detection methods are quite popular in the scene text detection field, because of their superiority for text instances with arbitrary shapes and extreme aspect ratios. However, the vast majority of the existing segmentation-based methods are difficult to detect curved and dense text instances due to principle of these methods. In this paper, we propose a novel text detection method named BorderNet. The key idea of BorderNet is making full use of border-center information to detect the curve and dense text. Furthermore, a efficientMulti-Scale Feature Enhancement Module is proposed to improve the scale and shape robustness by enhancingfeatures of different scales adaptively. Our method outperforms SOTA on multiple datasets, achieving 89% accuracy on ICDAR2015 and 87.1% accuracy on Total-Text. What's more, we can maintain 84.5% accuracy on DAST1500."
  },
  "accv2022_main_mgrln-netmask-guidedresiduallearningnetworkforjointsingle-imageshadowdetectionandremoval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "MGRLN-Net: Mask-Guided Residual Learning Network for Joint Single-Image Shadow Detection and Removal",
    "authors": [
      "Leiping Jie",
      "Hui Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Jie_MGRLN-Net_Mask-Guided_Residual_Learning_Network_for_Joint_Single-Image_Shadow_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Jie_MGRLN-Net_Mask-Guided_Residual_Learning_Network_for_Joint_Single-Image_Shadow_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Although significant progress has been made in single-image shadow detection or single-image shadow removal, only few works consider these two problems together. However, the two problems are complementary and can benefit from each other. In this work, we propose a Mask-Guided Residual Learning Network (MGRLN-Net) that jointly estimates shadow mask and shadow-free image. In particular, MGRLN-Net first generates a shadow mask, then utilizes a feature reassembling module to align the features from the shadow detection module to the shadow removal module. Finally, we leverage the learned shadow mask as guidance to generate a shadow-free image. We formulate shadow removal as a masked residual learning problem of the original shadow image. In this way, the learned shadow mask is used as guidance to produce better transitions in penumbra regions. Extensive experiments on ISTD, ISTD+, and SRD benchmark datasets demonstrate that our method outperforms current state-of-the-art approaches on both shadow detection and shadow removal tasks. Our code is available at https://github.com/LeipingJie/MGRLN-Net."
  },
  "accv2022_main_heterogeneousinteractivelearningnetworkforunsupervisedcross-modalretrieval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Heterogeneous Interactive Learning Network for Unsupervised Cross-modal Retrieval",
    "authors": [
      "Yuanchao Zheng",
      "Xiaowei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zheng_Heterogeneous_Interactive_Learning_Network_for_Unsupervised_Cross-modal_Retrieval_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zheng_Heterogeneous_Interactive_Learning_Network_for_Unsupervised_Cross-modal_Retrieval_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Cross-modal hashing has received a lot of attention because of its unique characteristic of low storage cost and high retrieval efficiency. However, these existing cross-modal retrieval approaches often fail to align effectively semantic information due to information asymmetry between image and text modality. To address this issue, we propose Heterogeneous Interactive Learning Network(HILN) for unsupervised cross-modal retrieval to alleviate the problem of the heterogeneous semantic gap. Specifically, we introduce a multi-head self-attention mechanism to capture the global dependencies of semantic features within the modality. Moreover, since the semantic relations among object entities from different modalities exist consistency, we perform heterogeneous feature fusion through the heterogeneous feature interaction module, especially through the cross attention in it to learn the interaction between different modal features. Finally, to further maintain semantic consistency, we introduce adversarial loss into network learning to generate more robust hash codes. Extensive experiments demonstrate that the proposed HILN improves the accuracy of T - I and I - T cross-modal retrieval tasks by 7.6% and 5.5% over the best competitor DGCPN on the NUS-WIDE dataset, respectively. Code is available at https://github.com/Z000204/HILN."
  },
  "accv2022_main_meta-prototypedecoupledtrainingforlong-tailedlearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Meta-Prototype Decoupled Training  for Long-tailed Learning",
    "authors": [
      "Siming Fu",
      "Huanpeng Chu",
      "Xiaoxuan He",
      "Hualiang Wang",
      "Zhenyu Yang",
      "Haoji Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Fu_Meta-Prototype_Decoupled_Training__for_Long-tailed_Learning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Fu_Meta-Prototype_Decoupled_Training__for_Long-tailed_Learning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Long-tailed learning aims to tackle the crucial challenge that head classes dominate the training procedure under severe class imbalance in real-world scenarios. Supervised contrastive learning has turned out to be aworth exploring research direction, which seeks to learn class-specific feature prototypes to enhance long-tailed learning performance. However, little attention has been paid to how to calibrate the empirical prototypes which are severely biased due to the scarce data in tail classes. Without the aid of correct prototypes, these explorations have not shown the significant promise expected. Motivated by this,we propose the meta-prototype contrastive learning to automatically learn the reliable representativeness of prototypes and more discriminative feature space via a meta-learning manner. In addition, on top of the calibrated prototypes, we leverage it to replace the mean of class statistics and predict the targeted distribution of balanced training data. By this procedure, we formulate the feature augmentation algorithm which samples additional features from the predicted distribution and further balances the over-whelming dominance severity of head classes. We summarize the above two stages as the meta-prototype decouple training scheme and conduct a series of experiments to validate the effectiveness of the framework. Our method outperforms previous work with a large margin and achieves state-of-the-art performance on long-tailed image classification and semantic segmentation tasks (e.g., we achieve 55.1% overall accuracy with ResNetXt-50 in ImageNet-LT)."
  },
  "accv2022_main_improvingsurveillanceobjectdetectionwithadaptiveomni-attentionoverbothinter-frameandintra-framecontext": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Improving Surveillance Object Detection with Adaptive Omni-Attention over both Inter-Frame and Intra-Frame Context",
    "authors": [
      "Tingting Yu",
      "Chen Chen",
      "Yichao Zhou",
      "Xiyuan Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yu_Improving_Surveillance_Object_Detection_with_Adaptive_Omni-Attention_over_both_Inter-Frame_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yu_Improving_Surveillance_Object_Detection_with_Adaptive_Omni-Attention_over_both_Inter-Frame_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Surveillance object detection is a challenging and practical sub-branch of object detection. Factors such as lighting variations, smaller objects, and motion blur in video frames affect detection results, but on the other hand, the temporal information and stable background of a surveillance video are major advantages that does not exist in generic object detection. In this paper, we propose an adaptive omni-attention model for surveillance object detection, which effectively and efficiently integrates inter-frame contextual information to improve the detection of low-quality frames and intra-frame attention to suppress false positive detections in the background regions. In addition, the training of the proposed network can converge quickly with less epochs because during multi-frame fusion stage, the pre-trained weights of the single-frame network can be used to update simultaneously in reverse in both single-frame and multi-frame feature maps. The experimental results on the UA-DETRAC and the UAVDT datasets have demonstrated the promising performance of our proposed detector in both accuracy and speed.(Code is available at https://github.com/Yubzsz/Omni-Attention-VOD.)"
  },
  "accv2022_main_decision-basedblack-boxattackspecifictolarge-sizeimages": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Decision-Based Black-Box Attack Specific to Large-Size Images",
    "authors": [
      "Dan Wang",
      "Yuan-Gen Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Decision-Based_Black-Box_Attack_Specific_to_Large-Size_Images_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Decision-Based_Black-Box_Attack_Specific_to_Large-Size_Images_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Decision-based black-box attacks can craft adversarial examples by only querying the target model for hard-label predictions. However, most existing methods are not efficient when attacking large-size images due to optimization difficulty in high-dimensional space, thus consuming lots of queries or obtaining relatively large perturbations. In this paper, we propose a novel decision-based black-box attack to generate adversarial examples, which is Specific to Large-size Image Attack (SLIA). We only perturb on the low-frequency component of discrete wavelet transform (DWT) of an image, reducing the dimension of the gradient to be estimated. Besides, when initializing the adversarial example of the untargeted attack, we remain the high-frequency components of the original image unchanged, and only update the low-frequency component with the randomly sampled uniform noise, thereby reducing the distortion at the beginning of the attack. Extensive experimental results demonstrate that the proposed SLIA outperforms state-of-the-art algorithms when attacking a variety of different threat models. The source code is publicly available at https://github.com/GZHU-DVL/SLIA."
  },
  "accv2022_main_focalandglobalspatial-temporaltransformerforskeleton-basedactionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition",
    "authors": [
      "Zhimin Gao",
      "Peitao Wang",
      "Pei Lv",
      "Xiaoheng Jiang",
      "Qidong Liu",
      "Pichao Wang",
      "Mingliang Xu",
      "Wanqing Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_Focal_and_Global_Spatial-Temporal_Transformer_for_Skeleton-based_Action_Recognition_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_Focal_and_Global_Spatial-Temporal_Transformer_for_Skeleton-based_Action_Recognition_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Despite great progress achieved by transformer in various vision tasks, it is still underexplored for skeleton-based action recognition with only a few attempts. Besides, these methods directly calculate the pair-wise global self-attention equally for all the joints in both the spatial and temporal dimensions, undervaluing the effect of discriminative local joints and the short-range temporal dynamics. In this work, we propose a novel Focal and Global Spatial-Temporal Transformer network (FG-STFormer), that is equipped with two key components: (1) FG-SFormer: focal joints and global parts coupling spatial transformer. It forces the network to focus on modelling correlations for both the learned discriminative spatial joints and human body parts respectively. The selective focal joints eliminate the negative effect of non-informative ones during accumulating the correlations. Meanwhile, the interactions between the focal joints and body parts are incorporated to enhance the spatial dependencies via mutual cross-attention. (2) FG-TFormer: focal and global temporal transformer. Dilated temporal convolution is integrated into the global self-attention mechanism to explicitly capture the local temporal motion patterns of joints or body parts, which is found to be vital important to make temporal transformer work. Extensive experimental results on three benchmarks, namely NTU-60, NTU-120 and NW-UCLA, show our FG-STFormer surpasses all existing transformer-based methods, and compares favourably with state-of-the-art GCN-based methods."
  },
  "accv2022_main_rethinkinglow-levelfeaturesforinterestpointdetectionanddescription": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Rethinking Low-level Features for Interest Point Detection and Description",
    "authors": [
      "Changhao Wang",
      "Guanwen Zhang",
      "Zhengyun Cheng",
      "Wei Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Rethinking_Low-level_Features_for_Interest_Point_Detection_and_Description_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Rethinking_Low-level_Features_for_Interest_Point_Detection_and_Description_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Although great efforts have been made for interest point detection and description, the current learning-based methods that use high-level features from the higher layers of Convolutional Neural Networks (CNN) do not completely outperform the conventional methods. On the one hand, interest points are semantically ill-defined and high-level features that emphasize semantic information are not adequate to describe interest points; On the other hand, the existing methods using low-level information usually perform detection on multi-level feature maps, which is time consuming for real time applications. To address these problems, we propose a Low-level descriptor-Aware Network (LANet) for interest point detection and description in self-supervised learning. Specifically, the proposed LANet exploits the low-level features for interest point description while using high-level features for interest point detection. Experimental results demonstrate that LANet achieves state-of-the-art performance on the homography estimation benchmark. Notably, the proposed LANet is a front-end feature learning framework that can be deployed in downstream tasks that require interest points with high-quality descriptors. (Code is available on https://github.com/wangch-g/lanet.)"
  },
  "accv2022_main_aff-camadaptivefrequencyfilteringbasedchannelattentionmodule": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "AFF-CAM: Adaptive Frequency Filtering based Channel Attention Module",
    "authors": [
      "DongWook Yang",
      "Min-Kook Suh",
      "Seung-Woo Seo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yang_AFF-CAM_Adaptive_Frequency_Filtering_based_Channel_Attention_Module_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yang_AFF-CAM_Adaptive_Frequency_Filtering_based_Channel_Attention_Module_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Locality from bounded receptive fields is one of the biggest problems that needs to be solved in convolutional neural networks. Meanwhile, operating convolutions in frequency domain provides complementary viewpoint to this dilemma, as a point-wise update in frequency domain can globally modulate all input features involved in Discrete Cosine Transform. However, Discrete Cosine Transform concentrates majority of its information in a handful of coefficients in lower regions of frequency spectrum, often discarding other potentially useful frequency components, such as those of middle and high frequency spectrum. We believe valuable feature representations can be learned not only from lower frequency components, but also from such disregarded frequency distributions. In this paper, we propose a novel Adaptive Frequency Filtering based Channel Attention Module (AFF-CAM), which exploits non-local characteristics of frequency domain and also adaptively learns the importance of different bands of frequency spectrum by modeling global cross-channel interactions, where each channel serves as a distinct frequency distribution. As a result, AFF-CAM is able to re-calibrate channel-wise feature responses and guide feature representations from spatial domain to reason over high-level, global context, which simply cannot be obtained from local kernels in spatial convolutions. Extensive experiments are conducted on ImageNet-1K classification and MS COCO detection benchmarks to validate our AFF-CAM. By effectively aggregating global information of various frequency spectrum from frequency domain with local information from spatial domain, our method achieves state-of-the-art results compared to other attention mechanisms."
  },
  "accv2022_main_structureguidedproposalcompletionfor3dobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Structure Guided Proposal Completion for 3D Object Detection",
    "authors": [
      "Chao Shi",
      "Chongyang Zhang",
      "Yan Luo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Shi_Structure_Guided_Proposal_Completion_for_3D_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Shi_Structure_Guided_Proposal_Completion_for_3D_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "3D object detection from point clouds is one of the key components in autonomous driving. Current two-stage detectors generate a small number of proposals, and then refine them in the second RCNN procedure. However, due to the inherent sparsity of point clouds, the first stage may predict some low quality proposals with incomplete structure and inaccurate localization. These low quality proposals fail to obtain adequate and precise proposal features which are essential for the following refinement, inevitably degrading the overall detection performance. To alleviate this problem, we propose Structure guided Proposal Completion (SPC) for 3D object detection from point clouds.Specifically, two completion strategies are developed to obtain high quality proposals:one is Structure Completion, in which a group of structural proposals are obtained by traversing most structures, and thus at least one proposal with ground truth similar structure can be guaranteed. The other is RoI Feature Completion, which is used to fill the empty area of proposals with virtual points under structure-aware manner.With the proposed SPC, high quality proposals with clearer structure and more precise localization can be obtained, and further promote the RCNN to perceive adequate proposal features. Extensive experiments on KITTI benchmark demonstrate the effectiveness of our proposed method, especially for hard setting objects with fewer LiDAR points."
  },
  "accv2022_main_emphasizingclosenessanddiversitysimultaneouslyfordeepfacerepresentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Emphasizing Closeness and Diversity Simultaneously for Deep Face Representation",
    "authors": [
      "Chaoyu Zhao",
      "Jianjun Qian",
      "Shumin Zhu",
      "Jin Xie",
      "Jian Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_Emphasizing_Closeness_and_Diversity_Simultaneously_for_Deep_Face_Representation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_Emphasizing_Closeness_and_Diversity_Simultaneously_for_Deep_Face_Representation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recent years have witnessed remarkable progress in deep face recognition due to the advancement of softmax-based methods. In this work, we first provide the analysis to reveal the working mechanism of softmax-based methods from the geometry view. Margin-based softmax methods enhance the feature discrimination by the extra margin.Mining-based softmax methods pay more attention to hard samples and try to enlarge their diversity during training. Both closeness and diversity are essential for discriminative features learning; however we observe that most previous works dealing with hard samples fail to balance the relationship between closeness and diversity. Therefore, we propose a novel approach to tackle the above issue. Specifically, we design a two-branch cooperative network: the Elementary Representation Branch (ERB) and the Refined Representation Branch (RRB). ERB employs the margin-based softmax to guide the network to learn elementary features and measure the difficulty for training samples. RRB employs the proposed sampling strategy in conjunction with two loss terms to enhance closeness and diversity simultaneously. Extensive experimental results on popular benchmarks demonstrate the superiority of our proposed method over state-of-the-art methods."
  },
  "accv2022_main_cvlnetcross-viewfeaturecorrespondencelearningforvideo-basedcameralocalization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CVLNet: Cross-View Feature Correspondence Learning for Video-based Camera Localization",
    "authors": [
      "Yujiao Shi",
      "Xin Yu",
      "Shan Wang",
      "Hongdong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper tackles the problem of Cross-view Video-based camera Localization (CVL). The task is to localize a query camera by leveraging information from its past observations, i.e., a continuous sequence of images observed at previous time stamps, and matching them to a large overhead-view satellite image.The critical challenge of this task is to learn a powerful global feature descriptor for the sequential ground-view images while considering its domain alignment with reference satellite images. For this purpose, we introduce CVLNet, which first projects the sequential ground-view images into an overhead view by exploring the ground-and-overhead geometric correspondences and then leverages the photo consistency among the projected images to form a global representation.In this way, the cross-view domain differences are bridged.Since the reference satellite images are usually pre-cropped and regularly sampled, there is always a misalignment between the query camera location and its matching satellite image center. Motivated by this, we propose estimating the query camera's relative displacement to a satellite image before similarity matching.In this displacement estimation process, we also consider the uncertainty of the camera location. For example, a camera is unlikely to be on top of trees.To evaluate the performance of the proposed method, we collect satellite images from Google Map for the KITTI dataset and construct a new cross-view video-based localization benchmark dataset, KITTI-CVL. Extensive experiments have demonstrated the effectiveness of video-based localization over single image-based localization and the superiority of each proposed module over other alternatives."
  },
  "accv2022_main_learnablesubspaceorthogonaltransformedprojectionforsemi-supervisedimageclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learnable Subspace Orthogonal Transformed Projection for Semi-supervised Image Classification",
    "authors": [
      "Lijian Li",
      "Yunhe Zhang",
      "Aiping Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Learnable_Subspace_Orthogonal_Transformed_Projection_for_Semi-supervised_Image_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Learnable_Subspace_Orthogonal_Transformed_Projection_for_Semi-supervised_Image_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this paper, we propose a learnable subspace orthogonal projection (LSOP) network for semi-supervised image classification.Although projection theory is widely used in various machine learning methods, solving projection matrix is a highly complex process.We employ an auto-encoder to construct a scalable and learnable subspace orthogonal projection network, thus enjoying lower computational consumption of subspace acquisition and smooth cooperation with deep neural networks. With these techniques, a promising end-to-end classification network is formulated. Extensive experimental results on real-world datasets demonstrate that the proposed classification algorithm achieves comparable performance with fewer training data than other projection methods."
  },
  "accv2022_main_visiontransformercompressionandarchitectureexplorationwithefficientembeddingspacesearch": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Vision Transformer Compression and Architecture Exploration with Efficient Embedding Space Search",
    "authors": [
      "Daeho Kim",
      "Jaeil Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kim_Vision_Transformer_Compression_and_Architecture_Exploration_with_Efficient_Embedding_Space_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kim_Vision_Transformer_Compression_and_Architecture_Exploration_with_Efficient_Embedding_Space_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper addresses theoretical and practical problems in the compression of vision transformers for resource-constrained environments. We found that deep feature collapse and gradient collapse can occur during the search process for the vision transformer compression. Deep feature collapse diminishes feature diversity rapidly as the layer depth deepens, and gradient collapse causes gradient explosion in training. Against these issues, we propose a novel framework, called VTCA, for accomplishing vision transformer compression and architecture exploration jointly with embedding space search using Bayesian optimization. In this framework, we formulate block-wise removal, shrinkage, and cross-block skip augmentation to prevent deep feature collapse, and Res-Post layer normalization to prevent gradient collapse under a knowledge distillation loss. In the search phase, we adopt a training speed estimation for a large-scale dataset and propose a novel elastic reward function that can represent a generalized manifold of rewards. Experiments were conducted with DeiT-Tiny/Small/Base backbones on the ImageNet, and our approach achieved competitive accuracy to recent patch reduction and pruning methods. The code is available at https://github. com/kdaeho27/VTCA."
  },
  "accv2022_main_progressiveattentionalmanifoldalignmentforarbitrarystyletransfer": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Progressive Attentional Manifold Alignment for Arbitrary Style Transfer",
    "authors": [
      "Xuan Luo",
      "Zhen Han",
      "Linkang Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Luo_Progressive_Attentional_Manifold_Alignment_for_Arbitrary_Style_Transfer_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Luo_Progressive_Attentional_Manifold_Alignment_for_Arbitrary_Style_Transfer_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Arbitrary style transfer algorithms can generate stylization results with arbitrary content-style image pairs but will distort content structures and bring degraded style patterns. The content distortion problem has been well issued using high-frequency signals, salient maps, and low-level features. However, the style degradation problem is still unsolved. Since there is a considerable semantic discrepancy between content and style features, we assume they follow two different manifold distributions. The style degradation happens because existing methods cannot fully leverage the style statistics to render the content feature that lies on a different manifold. Therefore we designed the progressive attentional manifold alignment (PAMA) to align the content manifold to the style manifold. This module consists of a channel alignment module to emphasize related content and style semantics, an attention module to establish the correspondence between features, and a spatial interpolation module to adaptively align the manifolds. The proposed PAMA can alleviate the style degradation problem and produce state-of-the-art stylization results."
  },
  "accv2022_main_3dposebasedfeedbackforphysicalexercises": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "3D Pose Based Feedback For Physical Exercises",
    "authors": [
      "Ziyi Zhao",
      "Sena Kiciroglu",
      "Hugues Vinzant",
      "Yuan Cheng",
      "Isinsu Katircioglu",
      "Mathieu Salzmann",
      "Pascal Fua"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_3D_Pose_Based_Feedback_For_Physical_Exercises_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_3D_Pose_Based_Feedback_For_Physical_Exercises_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Unsupervised self-rehabilitation exercises and physical training can cause serious injuries if performed incorrectly. We introduce a learning-based framework that identifies the mistakes made by a user and proposes corrective measures for easier and safer individual training. Our framework does not rely on hard-coded, heuristic rules. Instead, it learns them from data, which facilitates its adaptation to specific user needs. To this end, we use a Graph Convolutional Network (GCN) architecture acting on the user's pose sequence to model the relationship between the the body joints trajectories. To evaluate our approach, we introduce a dataset with 3 different physical exercises. Our approach yields 90.9% mistake identification accuracy and successfully corrects 94.2% of the mistakes."
  },
  "accv2022_main_neuralnetworkpanningscreeningtheoptimalsparsenetworkbeforetraining": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Neural Network Panning: Screening the Optimal Sparse Network Before Training",
    "authors": [
      "Xiatao Kang",
      "Ping Li",
      "Jiayi Yao",
      "Chengxi Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kang_Neural_Network_Panning_Screening_the_Optimal_Sparse_Network_Before_Training_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kang_Neural_Network_Panning_Screening_the_Optimal_Sparse_Network_Before_Training_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Pruning on neural networks before training not only compresses the original models, but also accelerates the network training phase, which has substantial application value. The current work focuses on fine-grained pruning, which uses metrics to calculate weight scores for weight screening, and extends from the initial single-order pruning to iterative pruning. Through these works, we argue that network pruning can be summarized as an expressive force transfer process of weights, where the reserved weights will take on the expressive force from the removed ones for the purpose of maintaining the performance of original networks. In order to achieve optimal expressive force scheduling, we propose a pruning scheme before training called Neural Network Panning which guides expressive force transfer through multi-index and multi-process steps, and designs a kind of panning agent based on reinforcement learning to automate processes. Experimental results show that Panning performs better than various available pruning before training methods."
  },
  "accv2022_main_boostingdenselong-tailedobjectdetectionfromdata-centricview": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Boosting Dense Long-Tailed Object Detection from Data-Centric View",
    "authors": [
      "Weichen Xu",
      "Jian Cao",
      "Tianhao Fu",
      "Hongyi Yao",
      "Yuan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Xu_Boosting_Dense_Long-Tailed_Object_Detection_from_Data-Centric_View_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Xu_Boosting_Dense_Long-Tailed_Object_Detection_from_Data-Centric_View_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Several re-sampling and re-weighting approaches have been proposed in recent literature to address long-tailed object detection. However, state-of-the-art approaches still struggle on the rare class. From data-centric view, this is due to few training data of the rare class and data imbalance. Some data augmentations which could generate more training data perform well in general object detection, while they are hardly leveraged in long-tailed object detection. We reveal that the real culprit lies in the fact that data imbalance has not been alleviated or even intensified. In this paper, we propose REDet: a rare data centric detection framework which could simultaneously generate training data of the rare class and deal with data imbalance. Our REDet contains data operations at two levels. At the instance-level, Copy-Move data augmentation could independently rebalance the number of instances of different classes according to their rarity. Specifically, we copy instances of the rare class in an image and then move them to other locations in the same image. At the anchor-level, to generate more supervision for the rare class within a reasonable range, we propose Long-Tailed Training Sample Selection (LTTSS) to dynamically determine the corresponding positive samples for each instance based on the rarity of the class. Comprehensive experiments performed on the challenging LVIS v1 dataset demonstrate the effectiveness of our proposed approach. We achieve an overall 30.2% AP and obtain significant performance improvements on the rare class. The code has been released at https://github.com/RookieXwc/REDet."
  },
  "accv2022_main_deepactiveensemblesamplingforimageclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Deep Active Ensemble Sampling For Image Classification",
    "authors": [
      "Salman Mohamadi",
      "Gianfranco Doretto",
      "Don Adjeroh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Mohamadi_Deep_Active_Ensemble_Sampling_For_Image_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Mohamadi_Deep_Active_Ensemble_Sampling_For_Image_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Conventional active learning (AL) frameworks aim to reduce the cost of data annotation by actively requesting the labeling for the most informative data points. However, introducing AL to data hungry deep learning algorithms has been a challenge. Some proposed approaches include uncertainty-based techniques, geometric methods, implicit combination of uncertainty-based and geometric approaches, and more recently, frameworks based on semi/self supervised techniques. In this paper, we address two specific problems in this area. The first is the need for efficient exploitation/exploration trade-off in sample selection in AL. For this, we present an innovative integration of recent progress in both uncertainty-based and geometric frameworks to enable an efficient exploration/exploitation trade-off in sample selection strategy. To this end, we build on a computationally efficient approximate of Thompson sampling with key changes as a posterior estimator for uncertainty representation. Our framework provides two advantages: (1) accurate posterior estimation, and (2) tune-able trade-off between computational over- head and higher accuracy. The second problem is the need for improved training protocols in deep AL. For this, we use ideas from semi/self super- vised learning to propose a general approach that is independent of the specific AL technique being used. Taken these together, our framework shows a significant improvement over the state-of-the-art, with results that are comparable to the performance of supervised-learning under the same setting. We show empirical results of our framework, and comparative performance with the state-of-the-art on four datasets, namely, MNIST, CIFAR10, CIFAR100 and ImageNet to establish a new baseline in two different settings."
  },
  "accv2022_main_structurerepresentationnetworkanduncertaintyfeedbacklearningfordensenon-uniformfogremoval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Structure Representation Network and Uncertainty Feedback Learning for Dense Non-Uniform Fog Removal",
    "authors": [
      "Yeying Jin",
      "Wending Yan",
      "Wenhan Yang",
      "Robby T. Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Jin_Structure_Representation_Network_and_Uncertainty_Feedback_Learning_for_Dense_Non-Uniform_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Jin_Structure_Representation_Network_and_Uncertainty_Feedback_Learning_for_Dense_Non-Uniform_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Few existing image defogging or dehazing methods consider dense and non-uniform particle distributions, which usually happen in smoke, dust and fog. Dealing with these dense and/or non-uniform distributions can be intractable, since fog's attenuation and airlight (or veiling effect) significantly weaken the background scene information in the input image. To address this problem, we introduce a structure-representation network with uncertainty feedback learning. Specifically, we extract the feature representations from apre-trained Vision Transformer (DINO-ViT) module to recover the background information. To guide our network to focus on non-uniform fog areas, and then remove the fog accordingly, we introduce the the uncertainty feedback learning, which produce the uncertainty maps, that have higher uncertainty in denser fog regions, and can be regarded as an attention map that represents fog's density and uneven distribution. Based on the uncertainty map, our feedback network refine our defogged output iteratively. Moreover, to handle the intractability of estimating the atmospheric light colors, we exploit the grayscale version of our input image, since it is less affected by varying light colors that are possibly present in the input image. The experimental results demonstrate the effectiveness of our method both quantitatively and qualitatively compared to the state-of-the-art methods in handling dense and non-uniform fog or smoke."
  },
  "accv2022_main_socialawaremulti-modalpedestriancrossingbehaviorprediction": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Social Aware Multi-Modal Pedestrian Crossing Behavior Prediction",
    "authors": [
      "Xiaolin Zhai",
      "Zhengxi Hu",
      "Dingye Yang",
      "Lei Zhou",
      "Jingtai Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhai_Social_Aware_Multi-Modal_Pedestrian_Crossing_Behavior_Prediction_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhai_Social_Aware_Multi-Modal_Pedestrian_Crossing_Behavior_Prediction_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "With the development of self-driving vehicles, pedestrian behavior prediction plays a vital role in constructing a safe human-robot interactive environment. Previous methods ignored the inherent uncertainty of pedestrian future actions and the temporal correlations of spatial interactions. To solve the aforementioned problems, we propose a novel social aware multi-modal pedestrian crossing behavior prediction network. In this research field, our network innovatively explores the multimodality nature of pedestrian future action prediction and forecasts diverse and plausible futures. Also, to model the social aware context in both the spatial and temporal domain, we construct a spatial-temporal heterogeneous graph, bridging the spatial-temporal gap between the scene and the pedestrian. Experiments show that our model achieves state-of-the-art performance on pedestrian action detection and prediction task. The code is available at https://github.com/zxll0106/Pedestrian_Crossing_Behavior_Prediction."
  },
  "accv2022_main_raftmlphowmuchcanbedonewithoutattentionandwithlessspatiallocality?": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "RaftMLP: How Much Can Be Done Without Attention and with Less Spatial Locality?",
    "authors": [
      "Yuki Tatsunami",
      "Masato Taki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tatsunami_RaftMLP_How_Much_Can_Be_Done_Without_Attention_and_with_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tatsunami_RaftMLP_How_Much_Can_Be_Done_Without_Attention_and_with_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "For the past ten years, CNN has reigned supreme in the world of computer vision, but recently, Transformer has been on the rise. However, the quadratic computational cost of self-attention has become a serious problem in practice applications. There has been much research on architectures without CNN and self-attention in this context. In particular, MLP-Mixer is a simple architecture designed using MLPs and hit an accuracy comparable to the Vision Transformer. However, the only inductive bias in this architecture is the embedding of tokens. This leaves open the possibility of incorporating a non-convolutional (or non-local) inductive bias into the architecture, so we used two simple ideas to incorporate inductive bias into the MLP-Mixer while taking advantage of its ability to capture global correlations. A way is to divide the token-mixing block vertically and horizontally. Another way is to make spatial correlations denser among some channels of token-mixing. With this approach, we were able to improve the accuracy of the MLP-Mixer while reducing its parameters and computational complexity. The small model that is RaftMLP-S is comparable to the state-of-the-art global MLP-based model in terms of parameters and efficiency per calculation. Our source code is available at https://github.com/okojoalg/raft-mlp."
  },
  "accv2022_main_fullytransformernetworkforchangedetectionofremotesensingimages": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Fully Transformer Network for Change Detection of Remote Sensing Images",
    "authors": [
      "Tianyu Yan",
      "Zifu Wan",
      "Pingping Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yan_Fully_Transformer_Network_for_Change_Detection_of_Remote_Sensing_Images_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yan_Fully_Transformer_Network_for_Change_Detection_of_Remote_Sensing_Images_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, change detection (CD) of remote sensing images have achieved great progress with the advances of deep learning. However, current methods generally deliver incomplete CD regions and irregular CD boundaries due to the limited representation ability of the extracted visual features. To relieve these issues, in this work we propose a novel learning framework named Fully Transformer Network (FTN) for remote sensing image CD, which improves the feature extraction from a global view and combines multi-level visual features in a pyramid manner. More specifically, the proposed framework first utilizes the advantages of Transformers in long-range dependency modeling. It can help to learn more discriminative global-level features and obtain complete CD regions. Then, we introduce a pyramid structure to aggregate multi-level visual features from Transformers for feature enhancement. The pyramid structure grafted with a Progressive Attention Module (PAM) can improve the feature representation ability with additional interdependencies through channel attentions. Finally, to better train the framework, we utilize the deeply-supervised learning with multiple boundaryaware loss functions. Extensive experiments demonstrate that our proposed method achieves a new state-of-the-art performance on four public CD benchmarks. For model reproduction, the source code is released at https://github.com/AI-Zhpp/FTN."
  },
  "accv2022_main_lightattenuationandcolorfluctuationforunderwaterimagerestoration": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Light Attenuation and Color Fluctuation for Underwater Image Restoration",
    "authors": [
      "Jingchun Zhou",
      "Dingshuo Liu",
      "Dehuan Zhang",
      "Weishi Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhou_Light_Attenuation_and_Color_Fluctuation_for_Underwater_Image_Restoration_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhou_Light_Attenuation_and_Color_Fluctuation_for_Underwater_Image_Restoration_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Underwater images with low contrast and color distortion of different scenarios often pose a significant challenge in improving image quality. Therefore, guidance by estimates of degenerated parameters related to its variety is beneficial, and they should be updated with degraded scenes to allow optimal vision. We propose a robust underwater image restoration method. Specifically, we adjust the color of the input image according to a background light estimation strategy guided by the depth map, light absorption priors considering hue, and Information entropy. At the same timewe adjust the depth map of the input image by computing color fluctuation and falloff. According to qualitative and quantitative evaluation, the proposed method generates vivid results with a more natural appearance and more valuable information."
  },
  "accv2022_main_decanustolegatussynthetictrainingfor2d-3dhumanposelifting": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Decanus to Legatus: Synthetic training for 2D-3D human pose lifting",
    "authors": [
      "Yue Zhu",
      "David Picard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhu_Decanus_to_Legatus_Synthetic_training_for_2D-3D_human_pose_lifting_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhu_Decanus_to_Legatus_Synthetic_training_for_2D-3D_human_pose_lifting_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "3D human pose estimation is a challenging task because of the difficulty to acquire ground-truth data outside of controlled environments. A number of further issues have been hindering progress in building a universal and robust model for this task, including domain gaps between different datasets, unseen actions between train and test datasets, various hardware settings and high cost of annotation, etc. In this paper, we propose an algorithm to generate infinite 3D synthetic human poses (Legatus) from a 3D pose distribution based on 10 initial handcrafted 3D poses (Decanus) during the training of a 2D to 3D human pose lifter neural network. Our results show that we can achieve 3D pose estimation performance comparable to methods using real data from specialized datasets but in a zero-shot setup, showing the generalization potential of our framework."
  },
  "accv2022_main_improvingfew-shotlearningbyspatially-awarematchingandcrosstransformer": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Improving Few-shot Learning by Spatially-aware Matching and CrossTransformer",
    "authors": [
      "Hongguang Zhang",
      "Philip H. S. Torr",
      "Piotr Koniusz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Improving_Few-shot_Learning_by_Spatially-aware_Matching_and_CrossTransformer_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Improving_Few-shot_Learning_by_Spatially-aware_Matching_and_CrossTransformer_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Current few-shot learning models capturevisual object relations in the so-called meta-learning setting under a fixed-resolution input. However, such models have a limited generalization ability under the scale and location mismatch between objects, as onlyfew samples from target classes are provided. Therefore, the lack of a mechanism to match the scale and location between pairs of compared images leads to the performance degradation. The importance of image contents varies across coarse-to-fine scales depending on the object and its class label, e. g., generic objects and scenes rely on their global appearance while fine-grained objects rely more on their localized visual patterns. In this paper, we study the impact of scale and location mismatch in the few-shot learning scenario, and propose a novel Spatially-aware Matching (SM) scheme to effectively perform matching across multiplescales and locations, and learn image relations by giving the highest weights tothe best matching pairs.The SM is trained to activate the most related locations and scales between support and query data. We apply and evaluate SM on various few-shot learning models and backbones for comprehensive evaluations. Furthermore, we leverage an auxiliary self-supervisory discriminator to train/predict the spatial- and scale-level index of feature vectors we use. Finally, we develop a novel transformer-based pipeline to exploit self- and cross-attention in a spatially-aware matching process. Our proposed design is orthogonal to the choice of backbone and/or comparator."
  },
  "accv2022_main_fromwithintobetweenknowledgedistillationforcrossmodalityretrieval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "From Within to Between: Knowledge Distillation for Cross Modality Retrieval",
    "authors": [
      "Vinh Tran",
      "Niranjan Balasubramanian",
      "Minh Hoai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tran_From_Within_to_Between_Knowledge_Distillation_for_Cross_Modality_Retrieval_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tran_From_Within_to_Between_Knowledge_Distillation_for_Cross_Modality_Retrieval_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We propose a novel loss function for training text-to-video and video-to-text retrieval networks based on knowledge distillation. This loss function addresses an important drawback of the max-margin loss function often used in existing cross-modality retrieval methods, in which a fixed margin is used in training to separate matching video-and-caption pairs from non-matching pairs, treating all non-matching pairs the same and failing to account for the different degrees of non-matching. We address this drawback by introducing a novel loss for the non-matching pairs; this loss leverages the knowledge within one domain to train a better network for matching between two domains. This proposed loss does not require extra annotation. It is complementary to the existing max-margin loss, and it can be integrated into the training pipeline of any cross-modality retrieval method.Experimental results on four cross-modal retrieval datasets namely MSRVTT, ActivityNet, DiDeMo, and MSVD show the effectiveness of the proposed method."
  },
  "accv2022_main_dreamnetadeepriemannianmanifoldnetworkforspdmatrixlearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DreamNet: A Deep Riemannian Manifold Network for SPD Matrix Learning",
    "authors": [
      "Rui Wang",
      "Xiao-Jun Wu",
      "Ziheng Chen",
      "Tianyang Xu",
      "Josef Kittler"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_DreamNet_A_Deep_Riemannian_Manifold_Network_for_SPD_Matrix_Learning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_DreamNet_A_Deep_Riemannian_Manifold_Network_for_SPD_Matrix_Learning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The methods of symmetric positive definite (SPD) matrix learning have attracted considerable attention in many pattern recognition tasks, as they are eligible to capture and learn appropriate statistical features while respecting the Riemannian geometry of SPD manifold where the data reside on. Accompanied with the advanced deep learning techniques, several Riemannian networks (RiemNets) for SPD matrix nonlinear processing have recently been studied. However, it is pertinent to ask, whether greater accuracy gains can be realized by simply increasing the depth of RiemNets. The answer appears to be negative, as deeper RiemNets may be difficult to train. To explore a possible solution to this issue, we propose a new architecture for SPD matrix learning. Specifically, to enrich the deep representations, we build a stacked Riemannian autoencoder (SRAE) on the tail of the backbone network, i.e., SPDNet [1]. With this design, the associated reconstruction error term can prompt the embedding functions of both SRAE and of each RAE to approach an identity mapping, which helps to prevent the degradation of statistical information. Then, we implant several residual-like blocks using shortcut connections to augment the representational capacity of SRAE, and to simplify the training of a deeper network. The experimental evidence demonstrates that our DreamNet can achieve improved accuracy with increased depth."
  },
  "accv2022_main_st-conalconsistency-basedacquisitioncriterionusingtemporalself-ensembleforactivelearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning",
    "authors": [
      "Jae Soon Baik",
      "In Young Yoon",
      "Jun Won Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Baik_ST-CoNAL_Consistency-Based_Acquisition_Criterion_Using_Temporal_Self-Ensemble_for_Active_Learning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Baik_ST-CoNAL_Consistency-Based_Acquisition_Criterion_Using_Temporal_Self-Ensemble_for_Active_Learning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Modern deep learning has achieved great success in various fields. However, it requires the labeling of huge amounts of data, which is expensive and labor-intensive. Active learning (AL), which identifies the most informative samples to be labeled, is becoming increasingly important to maximize the efficiency of the training process. The existing AL methods mostly use only a single final fixed model for acquiring the samples to be labeled. This strategy may not be good enough in that the structural uncertainty of a model for given training data is not considered to acquire the samples.In this study, we propose a novel acquisition criterion based on temporal self-ensemble generated by conventional stochastic gradient descent (SGD) optimization. These self-ensemble models are obtained by capturing the intermediate network weights obtained through SGD iterations. Our acquisition function relies on a consistency measure between the student and teacher models. The student models are given a fixed number of temporal self-ensemble models, and the teacher model is constructed by averaging the weights of the student models. Using the proposed acquisition criterion, we present an AL algorithm, namely student-teacher consistency-based AL (ST-CoNAL). Experiments conducted for image classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny ImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly better performance than the existing acquisition methods. Furthermore, extensive experiments show the robustness and effectiveness of our methods."
  },
  "accv2022_main_neuraldeformablevoxelgridforfastoptimizationofdynamicviewsynthesis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Neural Deformable Voxel Grid for Fast Optimization of Dynamic View Synthesis",
    "authors": [
      "Xiang Guo",
      "Guanying CHEN",
      "Yuchao Dai",
      "Xiaoqing Ye",
      "Jiadai Sun",
      "Xiao Tan",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Guo_Neural_Deformable_Voxel_Grid_for_Fast_Optimization_of_Dynamic_View_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Guo_Neural_Deformable_Voxel_Grid_for_Fast_Optimization_of_Dynamic_View_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel view synthesis (NVS) for its superior performance. In this paper, we propose to synthesize dynamic scenes. Extending the methods for static scenes to dynamic scenes is not straightforward as both the scene geometry and appearance change over time, especially under monocular setup. Also, the existing dynamic NeRF methods gen- erally require a lengthy per-scene training procedure, where multi-layer perceptrons (MLP) are fitted to model both motions and radiance. In this paper, built on top of the recent advances in voxel-grid optimization, we propose a fast deformable radiance field method to handle dynamic scenes. Our method consists of two modules. The first module adopts a deformation grid to store 3D dynamic features, and a light-weight MLP for decoding the deformation that maps a 3D point in the observation space to the canonical space using the interpolated features. The second module contains a density and a color grid to model the geometry and density of the scene. The occlusion is explicitly modeled to further im- prove the rendering quality. Experimental results show that our method achieves comparable performance to D-NeRF using only 20 minutes for training, which is more than 70x faster than D-NeRF, clearly demon- strating the efficiency of our proposed method."
  },
  "accv2022_main_seicsemanticembeddingwithintermediateclassesforzero-shotdomaingeneralization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SEIC: Semantic Embedding with Intermediate Classes for Zero-Shot Domain Generalization",
    "authors": [
      "Biswajit Mondal",
      "Soma Biswas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Mondal_SEIC_Semantic_Embedding_with_Intermediate_Classes_for_Zero-Shot_Domain_Generalization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Mondal_SEIC_Semantic_Embedding_with_Intermediate_Classes_for_Zero-Shot_Domain_Generalization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this work, we address the Zero-Shot Domain Generalization (ZSDG) task, where the goal is to learn a model from multiple source domains, such that it can generalize well to both unseen classes and unseen domains during testing. Since it combines the tasks of Domain Generalization (DG) and Zero-Shot Learning (ZSL), here we explore whether advances in these fields also translate to improved performance for the ZSDG task. Specifically, we build upon a state-of-the-art approach for domain generalization and appropriately modify it such that it can generalize to unseen classes during the testing stage. Towards this goal, we propose to make the feature embedding space semantically meaningful, by not only making an image feature close to its semantic attributes, but also taking into account its similarity with the other neighbouring classes. In addition, in order to reserve space for the unseen classes in the embedding space, we propose to introduce pseudo intermediate classes in between the semantically similar classes during training. This reduces confusion of the similar classes and thus increases the discriminability of the embedding space. Extensive experiments on two large-scale benchmark datasets, namely DomainNet and DomainNet-LS and comparisons with the state-of-the-art approaches show that the proposed framework outperforms all the other techniques on both the datasets."
  },
  "accv2022_main_css-netclassificationandsubstitutionforsegmentationofrotatorcufftear": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CSS-Net: Classification and Substitution for Segmentation of Rotator Cuff Tear",
    "authors": [
      "Kyungsu Lee",
      "Hah Min Lew",
      "Moon Hwan Lee",
      "Jun-Young Kim",
      "Jae Youn Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lee_CSS-Net_Classification_and_Substitution_for_Segmentation_of_Rotator_Cuff_Tear_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lee_CSS-Net_Classification_and_Substitution_for_Segmentation_of_Rotator_Cuff_Tear_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Magnetic resonance imaging (MRI) has been popularly used to diagnose orthopedic injuries because it offers high spatial resolution in a non-invasive manner. Since the rotator cuff tear (RCT) is a tear of the supraspinatus tendon (ST), a precise comprehension of both is required to diagnose the tear. However, previous deep learning studies have been insufficient in comprehending the correlations between the ST and RCT effectively and accurately. Therefore, in this paper, we propose a new method, substitution learning, wherein an MRI image is used to improve RCT diagnosis based on the knowledge transfer. Substitution learning mainly aims at segmenting RCT from MRI images by using the transferred knowledge while learning the correlations between RCT and ST. In substitution learning, the knowledge of correlations between RCT and ST is acquired by substituting the segmentation target (RCT) with the other target (ST), which has similar properties. To this end, we designed a novel deep learning model based on multi-task learning, which incorporates the newly developed substitution learning, with three parallel pipelines: (1) segmentation of RCT and ST regions, (2) classification of the existence of RCT, and (3) substitution of the ruptured ST regions, which are RCTs, with the recovered ST regions. We validated our developed model through experiments using 889 multi-categorical MRI images. The results exhibit that the proposed deep learning model outperforms other segmentation models to diagnose RCT with 6 8% improved IoU values. Remarkably, the ablation study explicates that substitution learning ensured more valid knowledge transfer."
  },
  "accv2022_main_groupguideddataassociationformultipleobjecttracking": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Group Guided Data Association for Multiple Object Tracking",
    "authors": [
      "Yubin Wu",
      "Hao Sheng",
      "Shuai Wang",
      "Yang Liu",
      "Zhang Xiong",
      "Wei Ke"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wu_Group_Guided_Data_Association_for_Multiple_Object_Tracking_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wu_Group_Guided_Data_Association_for_Multiple_Object_Tracking_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Multiple Object Tracking (MOT) usually adopts the Tracking-by-Detection paradigm, which transforms the problem into data association. However, these methods are restricted by detector performance, especially in dense scenes. In this paper, we propose a novel group-guided data association, which improves the robustness of MOT to error detections and increases tracking accuracy in occlusion areas. The tracklets are firstly clustered into groups of related motion patterns by a graph neural network. Using the idea of grouping, the data association is divided into two stages: intra-group and inter-group. For the intra-group, based on the structural relationship between objects, detections are recovered and associated by min-cost network flow. For inter-group, the tracklets are associated with the proposed hypotheses to solve long-term occlusion and reduce false positives. The experiments on the MOTChallenge benchmark prove our method's effects, which achieves competitive results over state-of-the-art methods."
  },
  "accv2022_main_clueconsolidatinglearnedandundergoingexperienceindomain-incrementalclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CLUE: Consolidating Learned and Undergoing Experience in Domain-Incremental Classification",
    "authors": [
      "Chengyi Cai",
      "Jiaxin Liu",
      "Wendi Yu",
      "Yuchen Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cai_CLUE_Consolidating_Learned_and_Undergoing_Experience_in_Domain-Incremental_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cai_CLUE_Consolidating_Learned_and_Undergoing_Experience_in_Domain-Incremental_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep neural networks tend to be vulnerable to catastrophic forgetting when learning new tasks. To address it, continual learning has become a promising and popular research field in recent years. It is noticed that plentiful research predominantly focuses on class-incremental (CI) settings. However, another practical setting, domain-incremental (DI) learning, where the domain distribution shifts in new tasks, also suffers from deteriorating rigidity and should be emphasized. Concentrating on the DI setting, in which the learned model is overwritten by new domains and is no longer valid for former tasks, a novel method named Consolidating Learned and Undergoing Experience (CLUE) is proposed in this paper. In particular, CLUE consolidates former and current experiences by setting penalties on feature extractor distortion and sample outputs alteration. CLUE is highly applicable to classification models as neither extra parameters nor processing steps are introduced. It is observed through extensive experiments that CLUE achieves significant performance improvement compared with other baselines in the three benchmarks. In addition, CLUE is robust even with fewer replay samples. Moreover, its feasibility is supported by both theoretical derivation and model interpretability visualization."
  },
  "accv2022_main_borexbayesian-optimization--basedrefinementofsaliencymapforimage-andvideo-classificationmodels": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "BOREx: Bayesian-Optimization--Based Refinement of Saliency Map for Image- and Video-Classification Models",
    "authors": [
      "Atsushi Kikuchi",
      "Kotaro Uchida",
      "Masaki Waga",
      "Kohei Suenaga"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kikuchi_BOREx_Bayesian-Optimization--Based_Refinement_of_Saliency_Map_for_Image-_and_Video-Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kikuchi_BOREx_Bayesian-Optimization--Based_Refinement_of_Saliency_Map_for_Image-_and_Video-Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Explaining a classification result produced by an image- and video-classification model is one of the important but challenging issues in computer vision.Many methods have been proposed for producing heat-map--based explanations for this purpose, including ones based on the white-box approach that uses the internal information of a model (e.g., LRP, Grad-CAM, and Grad-CAM++) and ones based on the black-box approach that does not use any internal information (e.g., LIME, SHAP, and RISE). We propose a new black-box method BOREx (Bayesian Optimization for Refinement of visual model EXplanation) to refine a heat map produced by any method.Our observation is that a heat-map--based explanation can be seen as a prior for an explanation method based on Bayesian optimization.Based on this observation, BOREx conducts Gaussian process regression (GPR) to estimate the saliency of each pixel in a given image starting from the one produced by another explanation method.Our experiments statistically demonstrate that the refinement by BOREx improves low-quality heat maps for image- and video-classification results."
  },
  "accv2022_main_adaptivefspadaptivearchitecturesearchwithfiltershapepruning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Adaptive FSP : Adaptive Architecture Search with Filter Shape Pruning",
    "authors": [
      "Aeri Kim",
      "Seungju Lee",
      "Eun Ji Kwon",
      "Seokhyeong Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kim_Adaptive_FSP__Adaptive_Architecture_Search_with_Filter_Shape_Pruning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kim_Adaptive_FSP__Adaptive_Architecture_Search_with_Filter_Shape_Pruning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "DeepConvolutionalNeuralNetworks(CNNs)havehighmem- ory footprint and computing power requirements, making their deploy- ment in embedded devices difficult. Network pruning has received at- tention in reducing those requirements of CNNs. Among the pruning methods, Stripe-Wise Pruning (SWP) achieved a further network com- pression than conventional filter pruning methods and can obtain op- timal kernel shapes of filters. However, the model pruned by SWP has filter redundancy because some filters have the same kernel shape. In this paper, we propose the Filter Shape Pruning (FSP) method, which prunes the networks using the kernel shape while maintaining the recep- tive fields. To obtain an architecture that satisfies the target FLOPs with the FSP method, we propose the Adaptive Architecture Search (AAS) framework. The AAS framework adaptively searches for the architec- ture that satisfies the target FLOPs with the layer-wise threshold. The layer-wise threshold is calculated at each iteration using the metric that reflects the filter's influence on accuracy and FLOPs together. Compre- hensive experimental results demonstrate that the FSP can achieve a higher compression ratio with an acceptable reduction in accuracy."
  },
  "accv2022_main_ralossrelation-awarelossforrobustpersonre-identification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "RA Loss: Relation-Aware Loss for Robust Person Re-identification",
    "authors": [
      "Kan Wang",
      "Shuping Hu",
      "Jun Cheng",
      "Jun Cheng",
      "Jianxin Pang",
      "Huan Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_RA_Loss_Relation-Aware_Loss_for_Robust_Person_Re-identification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_RA_Loss_Relation-Aware_Loss_for_Robust_Person_Re-identification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Previous relation-based losses in person re-identification (ReI- D) typically comprise two sequential steps: they firstly sample both pos- itive pair and negative pair and then deploy constraints to simultane- ously improve intra-identity compactness and inter-identity separability. However, existing relation-based losses usually place emphasis on ex- ploring the relation between images and therefore consider only several pairs during each optimization. This inevitably leads to different con- vergence status for pairs of the same kind and brings about the intra- pair variance problem. Accordingly, we propose a novel Relation-Aware (RA) loss to address the intra-pair variance via exploring the informa- tive relation across pairs. In brief, we introduce a macro-constraint and a micro-constraint. The macro-constraint encourages the separation of positive pair and negative pair via pushing far apart the two \"center- s\" of the positive pair and the negative pair. The \"center\" of each kind of pair are obtained via averaging all the pairs of the same kind. The micro-constraint further enhances the compactness by minimizing the discrepancies among pairs of the same kind. The two constraints work cooperatively to relieve the intra-pair variance and improve the quali- ty of pedestriansar representation. Results of extensive experiments on three widely used ReID benchmarks, i.e., Market-1501, DukeMTMC- ReID and CUHK03, demonstrate that the RA loss brings improvements over existing relation-based losses."
  },
  "accv2022_main_exploringadversariallyrobusttrainingforunsuperviseddomainadaptation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Exploring Adversarially Robust Training for Unsupervised Domain Adaptation",
    "authors": [
      "Shao-Yuan Lo",
      "Vishal Patel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lo_Exploring_Adversarially_Robust_Training_for_Unsupervised_Domain_Adaptation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lo_Exploring_Adversarially_Robust_Training_for_Unsupervised_Domain_Adaptation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Unsupervised Domain Adaptation (UDA) methods aim to transfer knowledge from a labeled source domain to an unlabeled target domain. UDA has been extensively studied in the computer vision literature. Deep networks have been shown to be vulnerable to adversarial attacks. However, very little focus is devoted to improving the adversarial robustness of deep UDA models, causing serious concerns about model reliability. Adversarial Training (AT) has been considered to be the most successful adversarial defense approach. Nevertheless, conventional AT requires ground-truth labels to generate adversarial examples and train models, which limits its effectiveness in the unlabeled target domain. In this paper, we aim to explore AT to robustify UDA models: How to enhance the unlabeled data robustness via AT while learning domain-invariant features for UDA? To answer this question, we provide a systematic study into multiple AT variants that can potentially be applied to UDA. Moreover, we propose a novel Adversarially Robust Training method for UDA accordingly, referred to as ARTUDA. Extensive experiments on multiple adversarial attacks and UDA benchmarks show that ARTUDA consistently improves the adversarial robustness of UDA models. Code is available at https://github.com/shaoyuanlo/ARTUDA"
  },
  "accv2022_main_physicalpassivepatchadversarialattacksonvisualodometrysystems": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Physical Passive Patch Adversarial Attacks on Visual Odometry Systems",
    "authors": [
      "Yaniv Nemcovsky",
      "Matan Jacoby",
      "Alex M. Bronstein",
      "Chaim Baskin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Nemcovsky_Physical_Passive_Patch_Adversarial_Attacks_on_Visual_Odometry_Systems_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Nemcovsky_Physical_Passive_Patch_Adversarial_Attacks_on_Visual_Odometry_Systems_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep neural networks are known to be susceptible to adversarial perturbations -- small perturbations that alter the output of the network and exist under strict norm limitations. While such perturbations are usually discussed as tailored to a specific input, a universal perturbation can be constructed to alter the model's output on a set of inputs. Universal perturbations present a more realistic case of adversarial attacks, as awareness of the model's exact input is not required. In addition, the universal attack setting raises the subject of generalization to unseen data, where given a set of inputs, the universal perturbations aim to alter the model's output on out-of-sample data. In this work, we study physical passive patch adversarial attacks on visual odometry-based autonomous navigation systems. A visual odometry system aims to infer the relative camera motion between two corresponding viewpoints, and is frequently used by vision-based autonomous navigation systems to estimate their state. For such navigation systems, a patch adversarial perturbation poses a severe security issue, as it can be used to mislead a system onto some collision course. To the best of our knowledge, we show for the first time that the error margin of a visual odometry model can be significantly increased by deploying patch adversarial attacks in the scene. We provide evaluation on synthetic closed-loop drone navigation data and demonstrate that a comparable vulnerability exists in real data. A reference implementation of the proposed method and the reported experiments is provided at https://github.com/patchadversarialattacks/patchadversarialattacks."
  },
  "accv2022_main_self-superviseddehazingnetworkusingphysicalpriors": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Self-Supervised Dehazing Network Using Physical Priors",
    "authors": [
      "Gwangjin Ju",
      "Yeongcheol Choi",
      "Donggun Lee",
      "Jee Hyun Paik",
      "Gyeongha Hwang",
      "Seungyong Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ju_Self-Supervised_Dehazing_Network_Using_Physical_Priors_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ju_Self-Supervised_Dehazing_Network_Using_Physical_Priors_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this paper, we propose a lightweight self-supervised dehazing network with the help of physical priors, called Self-Supervised Dehazing Network (SSDN). SSDN is a modified U-Net that estimates a clear image, transmission map, and atmospheric airlight out of the input hazy image based on the Atmospheric Scattering Model (ASM). It is trained in a self-supervised manner, utilizing recent self-supervised training methods and physical prior knowledge for obtaining realistic outputs. Thanks to the training objectives based on ASM, SSDN learns physically meaningful features. As a result, SSDN learns to estimate clear images that satisfy physical priors, instead of simply following data distribution, and it becomes generalized well over the data domain. With the self-supervision of SSDN, the dehazing performance can be easily finetuned with an additional dataset that can be built by simply collecting hazy images. Experimental results show that our proposed SSDN is lightweight and shows competitive dehazing performance with strong generalization capability over various data domains."
  },
  "accv2022_main_co-attentionalignedmutualcross-attentionforcloth-changingpersonre-identification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Co-Attention Aligned Mutual Cross-Attention for Cloth-Changing Person Re-Identification",
    "authors": [
      "Qizao Wang",
      "Xuelin Qian",
      "Yanwei Fu",
      "Xiangyang Xue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Co-Attention_Aligned_Mutual_Cross-Attention_for_Cloth-Changing_Person_Re-Identification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Co-Attention_Aligned_Mutual_Cross-Attention_for_Cloth-Changing_Person_Re-Identification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Person re-identification (Re-ID) has been widely studied and achieved significant progress. However, traditional person Re-ID methods primarily rely on cloth-related color appearance, which is unreliable under real-world scenarios when people change their clothes. Cloth-changing person Re-ID that takes this problem into account has received increasing attention recently, but it is more challenging to learn discriminative person identity features, since larger intra-class variation and smaller inter-class easily occur in the image feature space with clothing changes. Beyond appearance features, some known identity-related features can be implicitly encoded in images (e.g., body shapes). In this paper, we first design a novel Shape Semantics Embedding (SSE) module to encode body shape semantic information, which is one of the essential clues to distinguish pedestrians when their clothes change. To better complement image features, we further propose a Co-attention Aligned Mutual Cross-attention (CAMC) framework. Different from previous attention-based fusion strategies, it first aligns features from multiple modalities, then effectively interacts and transfers identity-aware but cloth-irrelevant knowledge between the image space and the body shape space, resulting in a more robust feature representation. To the best of our knowledge, this is the first work to adopt Transformer to handle the multi-modal interaction for cloth-changing person Re-ID. Extensive experiments demonstrate the effectiveness of our proposed method and show the superior performance achieved on several cloth-changing person Re-ID benchmarks. Codes will be available at https://github.com/QizaoWang/CAMC-CCReID."
  },
  "accv2022_main_exemplarfreeclassagnosticcounting": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Exemplar Free Class Agnostic Counting",
    "authors": [
      "Viresh Ranjan",
      "Minh Hoai Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ranjan_Exemplar_Free_Class_Agnostic_Counting_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ranjan_Exemplar_Free_Class_Agnostic_Counting_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We tackle the task of Class Agnostic Counting, which aims to count objects in a novel object category at test time without any access to labeled training data for that category. All previous class agnostic counting methods cannot work in a fully automated setting, and require computationally expensive test time adaptation. To address these challenges, we propose a visual counter which operates in a fully automated setting and does not require any test time adaptation. Our proposed approach first identifies exemplars from repeating objects in an image, and then counts the repeating objects. We propose a novel region proposal network for identifying the exemplars. After identifying the exemplars, we obtain the corresponding count by using a density estimation based Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and show that it achieves superior performance compared to the existing approaches. Our code and models will be made public."
  },
  "accv2022_main_self-distilledvisiontransformerfordomaingeneralization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Self-Distilled Vision Transformer for Domain Generalization",
    "authors": [
      "Maryam Sultana",
      "Muzammal Naseer",
      "Muhammad Haris Khan",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Sultana_Self-Distilled_Vision_Transformer_for_Domain_Generalization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Sultana_Self-Distilled_Vision_Transformer_for_Domain_Generalization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In the recent past, several domain generalization (DG) methods have been proposed, showing encouraging performance, however, almost all of them build on convolutional neural networks (CNNs). There is little to no progress on studying the DG performance of vision transformers (ViTs), which are challenging the supremacy of CNNs on standard benchmarks, often built on i.i.d assumption. This renders the real-world deployment of ViTs doubtful. In this paper, we attempt to explore ViTs towards addressing the DG problem. Similar to CNNs, ViTs also struggle in out-of-distribution scenarios and the main culprit is overfitting to source domains. Inspired by the modular architecture of ViTs, we propose a simple DG approach for ViTs, coined as self-distillation for ViTs. It reduces the overfitting of source domains by easing the learning of input-output mapping problem through curating non-zero entropy supervisory signals for intermediate transformer blocks. Further, it does not introduce any new parameters and can be seamlessly plugged into the modular composition of different ViTs. We empirically demonstrate notable performance gains with different DG baselines and various ViT backbones in five challenging datasets. Moreover, we report favorable performance against recent state-of-the-art DG methods. Our code along with pre-trained models are publicly available at: https://github.com/maryam089/SDViT."
  },
  "accv2022_main_three-stagetrainingpipelinewithpatchrandomdropforfew-shotobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Three-stage Training Pipeline with Patch Random Drop for Few-shot Object Detection",
    "authors": [
      "Shaobo Lin",
      "Xingyu Zeng",
      "Shilin Yan",
      "Rui Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lin_Three-stage_Training_Pipeline_with_Patch_Random_Drop_for_Few-shot_Object_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lin_Three-stage_Training_Pipeline_with_Patch_Random_Drop_for_Few-shot_Object_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Self-supervised learning (SSL) aims to design pretext tasks for exploiting the structural information of data without manual annotation, which has been widely used in few-shot image classification for improving the generalization of the model. However, few works explore the influence of SSL on Few-shot object detection (FSOD) which is a more challenging task. Besides, our experimental results demonstrate that using a weighted sum of different self-supervised losses causes performance degradation compared to using a single self-supervised task in FSOD. To solve these problems, firstly, we introduce SSL into FSOD by applying SSL tasks to the cropped positive samples. Secondly, we propose a novel self-supervised method: patch random drop, for predicting the location of the masked image patch. Finally, we design a three-stage training pipeline to associate two different self-supervised tasks. Extensive experiments on the few-shot object detection datasets, i.e., Pascal VOC, MS COCO, validate the effectiveness of our method."
  },
  "accv2022_main_sst-vlmsparsesampling-twiceinspiredvideo-languagemodel": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "SST-VLM: Sparse Sampling-Twice Inspired Video-Language Model",
    "authors": [
      "Yizhao Gao",
      "Zhiwu Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_SST-VLM_Sparse_Sampling-Twice_Inspired_Video-Language_Model_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_SST-VLM_Sparse_Sampling-Twice_Inspired_Video-Language_Model_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Most existing video-language modeling methods densely sample dozens (or even hundreds) of video clips from each raw video to learn the video representation for text-to-video retrieval. This paradigm requires high computational overload. Therefore, sparse sampling-based methods are proposed recently, which only sample a handful of video clips with short time duration from each raw video. However, they still struggle to learn a reliable video embedding with fragmented clips per raw video. To overcome this challenge, we present a novel video-language model called SST-VLM inspired by a Sparse Sampling-Twice (SST) strategy, where each raw video is represented with only two holistic video clips (each has a few frames, but throughout the entire video). For training our SST-VLM, we propose a new Dual Cross-modal MoCo (Dual X-MoCo) algorithm, which includes two cross-modal MoCo modules to respectively model the two clip-text pairs (for each video-text input). In addition to the classic cross-modal contrastive objective, we devise a clip-level alignment objective to obtain more consistent retrieval performance by aligning the prediction distributions of the two video clips (based on the negative queues of MoCo). Extensive experiments show that our SST-VLM achieves new state-of-the-art in text-to-video retrieval."
  },
  "accv2022_main_towardsreal-timehigh-definitionimagesnowremovalefficientpyramidnetworkwithasymmetricalencoder-decoderarchitecture": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Towards Real-time High-Definition Image Snow Removal: Efficient Pyramid Network with Asymmetrical Encoder-decoder Architecture",
    "authors": [
      "Tian Ye",
      "Sixiang Chen",
      "Yun Liu",
      "Yi Ye",
      "Jinbin Bai",
      "Erkang Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ye_Towards_Real-time_High-Definition_Image_Snow_Removal_Efficient_Pyramid_Network_with_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ye_Towards_Real-time_High-Definition_Image_Snow_Removal_Efficient_Pyramid_Network_with_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In winter scenes, the degradation of images taken under snow can be pretty complex, where the spatial distribution of snowy degradation varies from image to image. Recent methods adopt deep neural networks to recover clean scenes from snowy images directly. However, due to the paradox caused by the variation of complex snowy degradation, achieving reliable High-Definition image desnowing performance in real time is a considerable challenge. We develop a novel Efficient Pyramid Network with asymmetrical encoder-decoder architecture for real-time HD image desnowing. The general idea of our proposed network is to utilize the multi-scale feature flow fully and implicitly to mine clean cues from features. Compared with previous state-of-the-art desnowing methods, our approach achieves a better complexity-performance trade-off and effectively handles the processing difficulties of HD and Ultra-HD images.The extensive experiments on three large-scale image desnowing datasets demonstrate that our method surpasses all state-of-the-art approaches by a large margin both quantitatively and qualitatively, boosting the PSNR metric from 31.76 dB to 34.10 dB on the CSD test dataset and from 28.29 dB to 30.87 dB on the SRRS test dataset. The source code is available at \\href https://github.com/Owen718/Towards-Real-time-High-Definition-Image-Snow-Removal-Efficient-Pyramid-Networkhttps://github.com/Owen718/Towards-Real-time-High-Definition-Image-Snow-Removal-Efficient-Pyramid-Network"
  },
  "accv2022_main_utb180ahigh-qualitybenchmarkforunderwatertracking": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "UTB180: A High-quality Benchmark for Underwater Tracking",
    "authors": [
      "Basit Alawode",
      "Yuhang Guo",
      "Mehnaz Ummar",
      "Naoufel Werghi",
      "Jorge Dias",
      "Ajmal Mian",
      "Sajid Javed"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Alawode_UTB180_A_High-quality_Benchmark_for_Underwater_Tracking_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Alawode_UTB180_A_High-quality_Benchmark_for_Underwater_Tracking_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep learning methods have demonstrated encouraging performance on open-air visual object tracking (VOT) benchmarks, however, their strength remains unexplored on underwater video sequences due to the lack of challenging underwater VOT benchmarks. Apart from the challenges of open-air tracking, videos captured in underwater environments pose additional challenges for tracking such as low visibility, poor video quality, distortions in sharpness and contrast, reflectionsfrom suspended particles, and non-uniform lighting. We propose a new underwater tracking benchmark dataset (UTB180) consisting of 180 sequences to facilitate the development of underwater deep trackers. The sequences in UTB180 are selected from both underwater natural and online sources with over 58,000 annotated frames. Video-level attributes are also provided to facilitate the development of robust trackers for specific challenges. We benchmark 15 existing pre-trained State-Of-The-Art (SOTA) trackers on UTB180 and compare their performance on another publicly available underwater benchmark. The trackers consistently perform worse on UTB180 showing that it poses more challenging scenarios. Moreover, we show that fine-tuning five high-quality SOTA trackers on UTB180 still does not sufficiently boost their tracking performance. Our experiments show that the UTB180 sequences pose a major burden on the SOTA trackers as compared to their open-air tracking performance. The performance gap reveals the need for a dedicated end-to-end underwater deep tracker that takes into account the inherent properties of underwater environments. We believe that our proposed dataset will be of great value to the tracking community in advancing the state of the art in underwater VOT. Our dataset is publicly available on Kaggle."
  },
  "accv2022_main_region-of-interestattentiveheteromodalvariationalencoder-decoderforsegmentationwithmissingmodalities": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Region-of-interest Attentive Heteromodal Variational Encoder-Decoder for Segmentation with Missing Modalities",
    "authors": [
      "Seungwan Jeong",
      "Hwanho Cho",
      "Junmo Kwon",
      "Hyunjin Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Jeong_Region-of-interest_Attentive_Heteromodal_Variational_Encoder-Decoder_for_Segmentation_with_Missing_Modalities_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Jeong_Region-of-interest_Attentive_Heteromodal_Variational_Encoder-Decoder_for_Segmentation_with_Missing_Modalities_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The use of multimodal images generally improves segmentation. However, complete multimodal datasets are often unavailable due to clinical constraints. To address this problem, we propose a novel multimodal segmentation framework that is robust to missing modalities by using a region-of-interest (ROI) attentive modality completion. We use ROI attentive skip connection to focus on segmentation-related regions and a joint discriminator that combines tumor ROI attentive images and segmentation probability maps to learn segmentation-relevant shared latent representations. Our method is validated in the brain tumor segmentation challenge dataset of 285 cases for the three regions of the complete tumor, tumor core, and enhancing tumor. It is also validated on the ischemic stroke lesion segmentation challenge dataset with 28 cases of infarction lesions. Our method outperforms state-of-the-art methods in robust multimodal segmentation, achieving an average Dice of 84.15%, 75.59%, and 54.90% for the three types of brain tumor regions, respectively, and 48.29% for stroke lesions. Our method can improve the clinical workflow that requires multimodal images."
  },
  "accv2022_main_lightweightimagemattingviaefficientnon-localguidance": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Lightweight Image Matting via Efficient Non-Local Guidance",
    "authors": [
      "Zhaoxiang Kang",
      "Zonglin Li",
      "Qinglin Liu",
      "Yuhe Zhu",
      "Hongfei Zhou",
      "Shengping Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kang_Lightweight_Image_Matting_via_Efficient_Non-Local_Guidance_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kang_Lightweight_Image_Matting_via_Efficient_Non-Local_Guidance_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Natural image matting aims to estimate the opacity of foreground objects. Most existing approaches involve prohibitive parameters, daunting computational complexity, and redundant dependency. In this paper, we propose a lightweight matting method termed LiteMatting, which learns the local smoothness of color space and affinities between neighboring pixels to estimate the alpha mattes. Specifically, a modified mobile block is adopted to construct an encoder-decoder framework, which reduces parameters while retaining sufficient spatial and channel information. In addition, a Long-Short Range Pyramid Pooling Module (LSRPPM) is introduced to extend the reception field by capturing long-range dependency between regions distributed discretely. Finally, an Efficient Non-Local Block (ENB) is presented for guiding high-level semantics propagation from low-level detail features to refine the alpha mattes. Extensive experiments demonstrate that our method achieves a favorable trade-off between accuracy and efficiency. Compared with most state-of-the-art approaches, our method attains an immense descent in parameters and FLOPs with 30% and 13%, respectively, while achieving an improvement of over 15% in SAD metrics. Code and model are available at https://github.com/kzx2018/LiteMatting."
  },
  "accv2022_main_uncertainty-basedthincloudremovalnetworkviaconditionalvariationalautoencoders": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Uncertainty-Based Thin Cloud Removal Network via Conditional Variational Autoencoders",
    "authors": [
      "Haidong Ding",
      "Yue Zi",
      "Fengying Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ding_Uncertainty-Based_Thin_Cloud_Removal_Network_via_Conditional_Variational_Autoencoders_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ding_Uncertainty-Based_Thin_Cloud_Removal_Network_via_Conditional_Variational_Autoencoders_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Existing thin cloud removal methods treat this image restoration task as a point estimation problem, and produce a single cloud-free image following a deterministic pipeline. In this paper, we propose a novel thin cloud removal network via Conditional Variational Autoencoders (CVAE) to generate multiple reasonable cloud-free images for each input cloud image. We analyze the image degradation process with a probabilistic graphical model and design the network in an encoder-decoder fashion. Since the diversity in sampling from the latent space, the proposed method can avoid the shortcoming caused by the inaccuracy of a single estimation. With the uncertainty analysis, we can generate a more accurate clear image based on these multiple predictions. Furthermore, we create a new benchmark dataset with cloud and clear image pairs from real-world scenes, overcoming the problem of poor generalization performance caused by training on synthetic datasets. Quantitative and qualitative experiments show that the proposed method significantly outperforms state-of-the-art methods on real-world cloud images. The source code and dataset are available at https://github.com/haidong-Ding/Cloud-Removal."
  },
  "accv2022_main_meta-det3dlearntolearnfew-shot3dobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Meta-Det3D: Learn to Learn Few-Shot 3D Object Detection",
    "authors": [
      "Shuaihang Yuan",
      "Xiang Li",
      "Hao Huang",
      "Yi Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yuan_Meta-Det3D_Learn_to_Learn_Few-Shot_3D_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yuan_Meta-Det3D_Learn_to_Learn_Few-Shot_3D_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper addresses the problem of few-shot indoor 3D object detection by proposing a meta-learning-based framework that only relies on a few labeled samples from novel classes for training. Our model has two major components: a 3D meta-detector and a 3D object detector. Given a query 3D point cloud and a few support samples, the 3D meta-detector is trained over different 3D detection tasks to learn task distributions for different object classes and dynamically adapt the 3D object detector to complete a specific detection task. The 3D object detector takes task-specific information as input and produces 3D object detection results for the query point cloud. Specifically, the 3D object detector first extracts object candidates and their features from the query point cloud using a point feature learning network. Then, a class-specific re-weighting module generates class-specific re-weighting vectors from the support samples to characterize the task information, one for each distinct object class. Each re-weighting vector performs channel-wise attention to the candidate features to re-calibrate the query object features, adapting them to detect objects of the same classes. Finally, the adapted features are fed into a detection head to predict classification scores and bounding boxes for novel objects in the query point cloud. Several experiments on two 3D object detection benchmark datasets demonstrate that our proposed method acquired the ability to detect 3D objects in the few-shot setting."
  },
  "accv2022_main_weightedcontrativehashing": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Weighted Contrative Hashing",
    "authors": [
      "Jiaguo Yu",
      "Huming Qiu",
      "Dubing Chen",
      "Haofeng Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yu_Weighted_Contrative_Hashing_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yu_Weighted_Contrative_Hashing_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The development of unsupervised hashing is advanced by the recent popular contrastive learning paradigm. However, previous contrastive learning-based works have been hampered by (1) insufficient data similarity mining based on global-only image representations, and (2) the hash code semantic loss caused by the data augmentation. In this paper, we propose a novel method, namely Weighted Contrative Hashing (WCH), to take a step towards solving these two problems. We introduce a novel mutual attention module to alleviate the problem of information asymmetry in network features caused by the missing image structure during augmentation. Furthermore, we explore the fine-grained semantic relations between images, i.e., we divide the images into multiple patches and calculate similarities between patches. The aggregated similarities, which reflect a deep image relation, are distilled to facilitate the hash codes learning with a distillation loss, so as to obtain better retrieval performance. Extensive experiments show that the proposed WCH model significantly outperforms existing unsupervised hashing methods on three benchmark datasets."
  },
  "accv2022_main_multi-modalsegmentassemblagenetworkforadvideoeditingwithimportance-coherencereward": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward",
    "authors": [
      "Yunlong Tang",
      "Siting Xu",
      "Teng Wang",
      "Qin Lin",
      "Qinglin Lu",
      "Feng Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Advertisement video editing aims to automatically edit advertising videos into shorter videos while retaining coherent content and crucial information conveyed by advertisers. It mainly contains two stages: video segmentation and segment assemblage. The existing method performs well at video segmentation stages but suffers from the problems of dependencies on extra cumbersome models and poor performance at the segment assemblage stage. To address these problems, we propose M-SAN (Multi-modal Segment Assemblage Network) which can perform efficient and coherent segment assemblage task end-to-end. It utilizes multi-modal representation extracted from the segments and follows the Encoder-Decoder Ptr-Net framework with the Attention mechanism. Importance-coherence reward is designed for training M-SAN. We experiment on the Ads-1k dataset with 1000+ videos under rich ad scenarios collected from advertisers. To evaluate the methods, we propose a unified metric, Imp-Coh@Time, which comprehensively assesses the importance, coherence, and duration of the outputs at the same time. Experimental results show that our method achieves better performance than random selection and the previous method on the metric. Ablation experiments further verify that multi-modal representation and importance-coherence reward significantly improve the performance. Ads-1k dataset is available at: https://github.com/yunlong10/Ads-1k."
  },
  "accv2022_main_phylonetphysically-constrainedlongtermvideoprediction": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PhyLoNet: Physically-Constrained Long Term Video Prediction",
    "authors": [
      "Nir Ben Zikri",
      "Andrei Sharf"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zikri_PhyLoNet_Physically-Constrained_Long_Term_Video_Prediction_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zikri_PhyLoNet_Physically-Constrained_Long_Term_Video_Prediction_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Motions in videos are often governed by physical and biological laws such as gravity, collisions, flocking, etc. Accounting for such natural properties is an appealing way to improve realism in future frame video prediction. Nevertheless, the definition and computation of intricate physical and biological properties in motion videos are challenging. In this work, we introduce PhyLoNet, a PhyDNet extension that learns long-term future frame prediction and manipulation. Similar to PhyDNet, our network consists of a two-branch deep architecture that explicitly disentangles physical dynamics from complementary information. It uses a recurrent physical cell (PhyCell) for performing physicallyconstrained prediction in latent space. In contrast to PhyDNet, Phy- LoNet introduces a modified encoder-decoder architecture together with a novel relative flow loss. This enables a longer-term future frame prediction from a small input sequence with higher accuracy and quality. We have carried out extensive experiments, showing the ability of Phy- LoNet to outperform PhyDNet on various challenging natural motion datasets such as ball collisions, flocking, and pool games. Ablation studies highlight the importance of our new components. Finally, we show an application of PhyLoNet for video manipulation and editing by a novel class label modification architecture."
  },
  "accv2022_main_stagedadaptiveblindwatermarkingscheme": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Staged Adaptive Blind Watermarking Scheme",
    "authors": [
      "Baowei Wang",
      "Yufeng Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Staged_Adaptive_Blind_Watermarking_Scheme_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Staged_Adaptive_Blind_Watermarking_Scheme_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In traditional digital image watermarking methods, the stren- gth factor is calculated from the content of the carrier image, which can find a balance between the robustness and imperceptibility of encoded images. However, traditional methods do not consider the feature of the message and it is also unrealistic to calculate the strength factor of each image separately when faced with a huge number of images. In recent years, digital image watermarking methods based on deep learning have also introduced the strength factor. They assign the strength factor of each image to a fixed value to better adjust the robustness and imper- ceptibility of the image. We hope that the network can choose the most appropriate strength factor for each image to achieve a better balance. Therefore, we propose a staged adaptive blind watermarking scheme. We designed a new component - the adaptor, and used two stages of training by training different components in different stages, and improved the robustness and imperceptibility of watermarked images. By comparing the experimental results, our algorithmic scheme shows better results compared to current advanced algorithms."
  },
  "accv2022_main_eldetananchor-freegeneralellipseobjectdetector": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "ElDet: An Anchor-free General Ellipse Object Detector",
    "authors": [
      "Tianhao Wang",
      "Changsheng Lu",
      "Ming Shao",
      "Xiaohui Yuan",
      "Siyu Xia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_ElDet_An_Anchor-free_General_Ellipse_Object_Detector_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_ElDet_An_Anchor-free_General_Ellipse_Object_Detector_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Ellipse detection is a fundamental task in object shape analysis. Under complex environments, the traditional image processing based approaches may under-perform due to the hand-crated features. Instead, CNN-based approaches are more robust and powerful. In this paper, we introduce an efficient anchor-free data-augmentation based general ellipse detector, termed ElDet. Different from existing CNN-based methods, our ElDet relies more on edge information which could excavate more shape information into learning. Specifically, we first develop an edge fusion module to composite an overall edge map which has more complete boundary and better continuity. The edge map is treated as augmentation input for our ElDet for ellipse regression. Secondly, three loss functions are tailored to our ElDet, which are angle loss, IoU loss, and binary mask prediction loss to jointly improve the ellipse detection performance. Moreover, we contribute a diverse ellipse dataset by collecting multiple classes of elliptical objects in real scenes. Extensive experiments show that the proposed ellipse detector is very competitive to state-of-the-art methods."
  },
  "accv2022_main_unsupervised3dshaperepresentationlearningusingnormalizingflow": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Unsupervised 3D Shape Representation Learning using Normalizing Flow",
    "authors": [
      "Xiang Li",
      "Congcong Wen",
      "Hao Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Unsupervised_3D_Shape_Representation_Learning_using_Normalizing_Flow_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Unsupervised_3D_Shape_Representation_Learning_using_Normalizing_Flow_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Learning robust and compact shape representation learning plays an important role in many 3D vision tasks. Existing supervised learning-based methods have achieved remarkable performance, meanwhile requiring large-scale human-annotated datasets for model training. Self-supervised/unsupervised methods provide an attractive solution to this issue that can learn shape representations without the need for ground truth labels. In this paper, we introduce a novel self-supervised method for shape representation learning using normalizing flows. Specifically, we build a model upon a variational normalizing flow framework where a sequence of normalizing flow layers are adopted to model exact posterior latent distribution and enhance the representation power of the learned latent code. To further encourage inter-shape separability and intra-shape compactness among a batch of shapes, we design a contrastive-center loss that performs metric learning on features on a hypersphere. We validate the representation learning ability of our model on downstream classification tasks. Experiments on ModelNet40/10, ScanobjectNN, and ScanNet datasets demonstrate the superior performance of our method compared with current state-of-the-art methods."
  },
  "accv2022_main_energy-efficientimageprocessingusingbinaryneuralnetworkswithhadamardtransforms": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Energy-Efficient Image Processing Using Binary Neural Networks with Hadamard Transforms",
    "authors": [
      "Jaeyoon Park",
      "Sunggu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Park_Energy-Efficient_Image_Processing_Using_Binary_Neural_Networks_with_Hadamard_Transforms_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Park_Energy-Efficient_Image_Processing_Using_Binary_Neural_Networks_with_Hadamard_Transforms_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Binary neural networks have recently begun to be used as a highly energy- and computation-efficient image processing technique for computer vision tasks. This paper proposes a novel extension of existing binary neural network technology based on the use of a Hadamard transform in the input layer of a binary neural network. Previous state-of-the-art binary neural networks require floating-point arithmetic at several parts of the neural network model computation in order to maintain a sufficient level of accuracy. The Hadamard transform is similar to a Discrete Cosine Transform (used in the popular JPEG image compression method) except that it does not include expensive multiplication operations. In this paper, it is shown that the Hadamard transform can be used to replace the most expensive floating-point arithmetic portion of a binary neural network. In order to test the efficacy of this proposed method, three types of experiments were conducted: application of the proposed method to several state-of-the-art neural network models, verification of its effectiveness in a large image dataset (ImageNet), and experiments to verify the effectiveness of the Hadamard transform by comparing the performance of binary neural networks with and without the Hadamard transform. The results show that the Hadamard transform can be used to implement a highly energy-efficient binary neural network with only a miniscule loss of accuracy."
  },
  "accv2022_main_super-attentionforexemplar-basedimagecolorization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Super-attention for exemplar-based image colorization",
    "authors": [
      "Hernan Carrillo",
      "Micha\u00ebl Cl\u00e9ment",
      "Aurelie Bugeau"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Camilo_Super-attention_for_exemplar-based_image_colorization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Camilo_Super-attention_for_exemplar-based_image_colorization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In image colorization, exemplar-based methods use a reference color image to guide the colorization of a target grayscale image. In this article, we present a deep learning framework for exemplar-based image colorization which relies on attention layers to capture robust correspondences between high-resolution deep features from pairs of images. To avoid the quadratic scaling problem from classic attention, we rely on a novel attention block computed from superpixel features, which we call super-attention. Super-attention blocks can learn to transfer semantically related color characteristics from a reference image at different scales of a deep network. Our experimental validations highlight the interest of this approach for exemplar-based colorization. We obtain promising results, achieving visually appealing colorization and outperforming state-of-the-art methods on different quantitative metrics."
  },
  "accv2022_main_unifiedenergy-basedgenerativenetworkforsupervisedimagehashing": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Unified Energy-based Generative Network for Supervised Image Hashing",
    "authors": [
      "Khoa D Doan",
      "Sarkhan Badirli",
      "Chandan K Reddy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Doan_Unified_Energy-based_Generative_Network_for_Supervised_Image_Hashing_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Doan_Unified_Energy-based_Generative_Network_for_Supervised_Image_Hashing_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Hashing methods often face critical efficiency challenges, such as generalization with limited labeled data, and robustness issues (such as changes in the data distribution and missing information in the input data) in real-world retrieval applications. However, it is non-trivial to learn a hash function in existing supervised hashing methods with both acceptable efficiency and robustness. In this paper, we explore a unified generative hashing model based on an explicit energy-based model (EBM) that exhibits a better generalization with limited labeled data, and better robustness against distributional changes and missing data. Unlike the previous implicit generative adversarial network (GAN) based hashing approaches, which suffer from several practical difficulties since they simultaneously train two networks (the generator and the discriminator), our approach only trains one single generative network with multiple objectives. Specifically, the proposed generative hashing model is a bottom-up multipurpose network that simultaneously represents the images from multiple perspectives, including explicit probability density, binary hash code, and category. Our model is easier to train than GAN-based approaches as it is based on finding the maximum likelihood of the density function. The proposed model also exhibits significant robustness toward out-of-distribution query data and is able to overcome missing data in both the training and testing phase with minimal retrieval performance degradation. Extensive experiments on several real-world datasets demonstrate superior results in which the proposed model achieves up to 5%improvement over the current state-of-the-art supervised hashing methods and exhibits a significant performance boost and robustness in both out-of-distribution retrieval and missing data scenarios."
  },
  "accv2022_main_multispectral-basedimagingandmachinelearningfornoninvasivebloodlossestimation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multispectral-Based Imaging and Machine Learning for Noninvasive Blood Loss Estimation",
    "authors": [
      "Ara Abigail E. Ambita",
      "Catherine S. Co",
      "Laura T. David",
      "Charissa M. Ferrera",
      "Prospero C. Naval"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ambita_Multispectral-Based_Imaging_and_Machine_Learning_for_Noninvasive_Blood_Loss_Estimation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ambita_Multispectral-Based_Imaging_and_Machine_Learning_for_Noninvasive_Blood_Loss_Estimation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Blood loss estimation during surgical operations is crucial in determining the appropriate transfusion decisions. More practical emerging solutions, e.g. the Triton System, use image processing and artificial intelligence (AI) in quantifying blood loss from images of blood-soaked sponges. Triton utilizes an infrared or depth camera that's used to identify the region of a color (RGB) image corresponding to a surgical textile.However, calculating depth is computationally expensive and can provide only the shape information. In this research, we propose a multispectral-based imaging and machine learning approach to directly quantify blood loss from images of surgical sponges. Near infrared (NIR) and Visible(Vis) light sources in conjunction with an RGB imaging sensor without an NIR filter is used. With this, in addition to the improved focus and reduced background interference on the gauze image due to blood's IR absorption capacities, the color as well as the shape information may be utilized. Results show that the multispectral-based imaging approach rendered a +28.30%, +48%, +27.97%, and 25.72% improvement on the MAE, MSE, RMSE, and MAPE, compared to using a single Vis wavelength or RGB image."
  },
  "accv2022_main_heterogeneousavatarsynthesisbasedondisentanglementoftopologyandrendering": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Heterogeneous Avatar Synthesis Based on Disentanglement of Topology and Rendering",
    "authors": [
      "Nan Gao",
      "Zhi Zeng",
      "GuiXuan Zhang",
      "ShuWu Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_Heterogeneous_Avatar_Synthesis_Based_on_Disentanglement_of_Topology_and_Rendering_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_Heterogeneous_Avatar_Synthesis_Based_on_Disentanglement_of_Topology_and_Rendering_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "There are obviously structural and color discrepancies among different heterogeneous domains. In this paper, we explore the challenging heterogeneous avatar synthesis (HAS) task considering topology and rendering transfer. HAS transfers the topology as well as rendering styles of the referenced face to the source face, to produce high-fidelity heterogeneous avatars. Specifically, first, we utilize a Rendering Transfer Network (RT-Net) to render the grayscale source face based on the color palette of the referenced face. The grayscale features and color style are injected into RT-Net based on adaptive feature modulation. Second, we apply a Topology Transfer Network (TT-Net) to conduct heterogeneous facial topology transfer, where the image content of RT-Net is transferred based on AdaIN controlled by heterogeneous identity embedding. Comprehensive experimental results show that the disentanglement of rendering and topology is beneficial to the HAS task, and our HASNet has comparable performance compared with other state-of-the-art methods."
  },
  "accv2022_main_few-shotadaptiveobjectdetectionwithcross-domaincutmix": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Few-shot Adaptive Object Detection with Cross-Domain CutMix",
    "authors": [
      "Yuzuru Nakamura",
      "Yasunori Ishii",
      "Yuki Maruyama",
      "Takayoshi Yamashita"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Nakamura_Few-shot_Adaptive_Object_Detection_with_Cross-Domain_CutMix_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Nakamura_Few-shot_Adaptive_Object_Detection_with_Cross-Domain_CutMix_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In object detection, data amount and cost are a trade-off, and collecting a large amount of data in a specific domain is labor-intensive. Therefore, existing large-scale datasets are used for pre-training. However, conventional transfer learning and domain adaptation cannot bridge the domain gap when the target domain differs significantly from the source domain. We propose a data synthesis method that can solve the large domain gap problem. In this method, a part of the target image is pasted onto the source image, and the position of the pasted region is aligned by utilizing the information of the object bounding box. In addition, we introduce adversarial learning to discriminate whether the original or the pasted regions. The proposed method trains on a large number of source images and a few target domain images. The proposed method achieves higher accuracy than conventional methods in a very different domain problem setting, where RGB images are the source domain, and thermal infrared images are the target domain. Similarly, the proposed method achieves higher accuracy in the cases of simulation images to real images."
  },
  "accv2022_main_spatialtemporalnetworkforimageandskeletonbasedgroupactivityrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Spatial Temporal Network for Image and Skeleton Based Group Activity Recognition",
    "authors": [
      "Xiaolin Zhai",
      "Zhengxi Hu",
      "Dingye Yang",
      "Lei Zhou",
      "Jingtai Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhai_Spatial_Temporal_Network_for_Image_and_Skeleton_Based_Group_Activity_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhai_Spatial_Temporal_Network_for_Image_and_Skeleton_Based_Group_Activity_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Group activity recognition aims to infer group activity in multi-person scenes. Previous methods usually model inter-person relations and integrate individuals' features into group representations. However, they neglect intra-person relations contained in the human skeleton. Individual representations can also be inferred by analyzing the evolution of human skeletons. In this paper, we utilize RGB images and human skeletons as the inputs which contain complementary information. Considering different semantic attributes of the two inputs, we design two diverse branches, respectively. For RGB images, we propose Scene Encoded Transformer, Spatial Transformer, and Temporal Transformer to explore inter-person spatial and temporal relations. For skeleton inputs, we capture the intra-person spatial and temporal dynamics by designing Spatial and Temporal GCN. Our main contributions are: i) we propose a spatial-temporal network with two branches for group activity recognition utilizing RGB images and human skeletons. Experiments show that our model achieves 97.1 MCA and 96.1 MPCA on the Collective Activity dataset and 94.0 MCA and 94.4 MPCA on the Volleyball dataset. ii) we extend the two datasets by introducing human skeleton annotations, namely human joint coordinates and confidence, which can also be used in the action recognition task. The code is available at https://github.com/zxll0106/Image_and_Skeleton_Based_Group_Activity_Recognition."
  },
  "accv2022_main_trimixageneralframeworkformedicalimagesegmentationfromlimitedsupervision": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "TriMix: A General Framework for Medical Image Segmentation from Limited Supervision",
    "authors": [
      "Zhou Zheng",
      "Yuichiro Hayashi",
      "Masahiro Oda",
      "Takayuki Kitasaka",
      "Kensaku Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zheng_TriMix_A_General_Framework_for_Medical_Image_Segmentation_from_Limited_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zheng_TriMix_A_General_Framework_for_Medical_Image_Segmentation_from_Limited_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present a general framework for medical image segmentation from limited supervision, reducing the reliance on fully and densely labeled data. Our method is simple, jointly trains triple diverse models, and adopts a mix augmentation scheme, and thus is called TriMix. TriMix imposes consistency under a more challenging perturbation, i.e., combining data augmentation and model diversity on the tri-training framework. This straightforward strategy enables TriMix to serve as a strong and general learner learning from limited supervision using different kinds of imperfect labels. We conduct extensive experiments to show TriMix's generic purpose for semi- and weakly-supervised segmentation tasks. Compared to task-specific state-of-the-arts, TriMix achieves competitive performance and sometimes surpasses them by a large margin. The code is available at https://github.com/MoriLabNU/TriMix."
  },
  "accv2022_main_dcvqeahierarchicaltransformerforvideoqualityassessment": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DCVQE: A Hierarchical Transformer for Video Quality Assessment",
    "authors": [
      "Zutong Li",
      "Lei Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_DCVQE_A_Hierarchical_Transformer_for_Video_Quality_Assessment_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_DCVQE_A_Hierarchical_Transformer_for_Video_Quality_Assessment_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The explosion of user-generated videos stimulates a great demand for no-reference video quality assessment (NR-VQA). Inspired by our observation on the actions of human annotation, we put forward a Divide and Conquer Video Quality Estimator (DCVQE) for NR-VQA. Starting from extracting the frame-level quality embeddings (QE), our proposal splits the whole sequence into a number of shots and applies Transformers to learn the shot-level QE and update the frame-level QE simultaneously; another Transformer is introduced to combine the shot-level QE to generate the video-level QE. We call this hierarchical combination of Transformers as a Divide and Conquer Transformer (DCTr) layer. A great video quality feature extraction can be achieved by repeating the DCTr layer several times. Also, taking the order relationship among the annotated data into account, we propose a novel correlation loss term for training. Experiments confirm that our DCVQE outperforms most other algorithms by a great margin."
  },
  "accv2022_main_ffdaugmentortowardsfew-shotoraclecharacterrecognitionfromscratch": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "FFD Augmentor: Towards Few-Shot Oracle Character Recognition from Scratch",
    "authors": [
      "Xinyi Zhao",
      "Siyuan Liu",
      "Yikai Wang",
      "Yanwei Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhao_FFD_Augmentor_Towards_Few-Shot_Oracle_Character_Recognition_from_Scratch_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhao_FFD_Augmentor_Towards_Few-Shot_Oracle_Character_Recognition_from_Scratch_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recognizing oracle characters, the earliest hieroglyph discovered in China, is recently addressed with more and more attention. Due to the difficulty of collecting labeled data, recognizing oracle characters is naturally a Few-Shot Learning (FSL) problem, which aims to tackle the learning problem with only one or a few training data. Most current FSL methods assume a disjoint but related big dataset can be utilized such that one can transfer the related knowledge to the few-shot case. However, unlike common phonetic words like English letters, oracle bone inscriptions are composed of radicals representing graphic symbols. Furthermore, as time goes, the graphic symbols to represent specific objects were significantly changed. Hence we can hardly find plenty of prior knowledge to learn without negative transfer. Another perspective to solve this problem is to use data augmentation algorithms to directly enlarge the size of training data to help the training of deep models. But popular augment strategies, such as dividing the characters into stroke sequences, break the orthographic units of Chinese characters and destroy the semantic information. Thus simply adding noise to strokes perform weakly in enhancing the learning capacity. To solve these issues, we in this paper propose a new data augmentation algorithm for oracle characters such that (1) it will introduce informative diversity for the training data while alleviating the loss of semantics; (2) with this data augmentation algorithm, we can train the few-shot model from scratch without pre-training and still get a powerful recognition model with superior performance to models pre-trained with a large dataset. Specifically, our data augmentation algorithm includes a B-spline free form deformation method to randomly distort the strokes of characters but maintain the overall structures. We generate 20 - 40 augmented images for each training data and use this augmented training set to train a deep neural network model in a standard pipeline. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our augmentor. Code and models are released in https://github.com/Hide-A-Pumpkin/FFDAugmentor."
  },
  "accv2022_main_gscorecamwhatobjectsiscliplookingat?": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "gScoreCAM: What objects is CLIP looking at?",
    "authors": [
      "Peijie Chen",
      "Qi Li",
      "Saad Biaz",
      "Trung Bui",
      "Anh Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_gScoreCAM_What_objects_is_CLIP_looking_at_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_gScoreCAM_What_objects_is_CLIP_looking_at_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Large-scale, multimodal models trained on web data such as OpenAI's CLIP are becoming the foundation of many applications. Yet, they are also more complex to understand, test, and therefore align with human values. In this paper, we propose gScoreCAM--a state-of-the-art method for visualizing the main objects that CLIP is looking at in an image. On zero-shot object detection, gScoreCAM performs similarly to ScoreCAM, the best prior art on CLIP, yet 8 to 10 times faster. Our method outperforms other existing, well-known methods (HilaCAM, RISE, and the entire CAM family) by a large margin, especially in multi-object scenes. gScoreCAM sub-samples k = 300 channels (from 3,072 channels--i.e. reducing complexity by almost 10 times) of the highest gradients and linearly combines them into a final \"attention\" visualization. We demonstrate the utility and superiority of our method on three datasets: ImageNet, COCO, and PartImageNet. Our work opens up interesting future directions in understanding and de-biasing CLIP."
  },
  "accv2022_main_cross-architectureknowledgedistillation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Cross-Architecture Knowledge Distillation",
    "authors": [
      "Yufan Liu",
      "Jiajiong Cao",
      "Bing Li",
      "Weiming Hu",
      "Jingting Ding",
      "Liang Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Liu_Cross-Architecture_Knowledge_Distillation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Liu_Cross-Architecture_Knowledge_Distillation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Transformer attracts much attention because of its ability to learn global relations and superior performance. In order to achieve higher performance, it is natural to distill complementary knowledge from Transformer to convolutional neural network (CNN). However, most existing knowledge distillation methods only consider homologous-architecture distillation, such as distilling knowledge from CNN to CNN. They may not be suitable when applying to cross-architecture scenarios, such as from Transformer to CNN. To deal with this problem, a novel cross-architecture knowledge distillation method is proposed. Specifically, instead of directly mimicking output/intermediate features of the teacher, a partially cross attention projector and a group-wise linear projector are introduced to align the student features with the teacher's in two projected feature spaces. And a multi-view robust training scheme is further presented to improve the robustness and stability of the framework. Extensive experiments show that the proposed method outperforms 14 state-of-the-arts on both small-scale and large-scale datasets."
  },
  "accv2022_main_generatingmultiplehypothesesfor3dhumanmeshandposeusingconditionalgenerativeadversarialnets": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Generating Multiple Hypotheses for 3D Human Mesh and Pose using Conditional Generative Adversarial Nets",
    "authors": [
      "Xu Zheng",
      "Yali Zheng",
      "Shubing Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zheng_Generating_Multiple_Hypotheses_for_3D_Human_Mesh_and_Pose_using_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zheng_Generating_Multiple_Hypotheses_for_3D_Human_Mesh_and_Pose_using_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Despite recent successes in 3D human mesh/pose recovery, the human mesh/pose reconstruction ambiguity is a challenging problem that can not be avoided as lighting, occlusion or self-occlusion in scenes happens. We argue that it could be multiple 3D human meshes corresponding a single image from a view point, becausewe really do not know what happens in extreme lighting or behind occlusion/self occlusion.In this paper, we address the problem using Conditional Generative Adversarial Nets (CGANs) to generate multiple hypotheses for 3D human mesh and pose from a single image under the condition of 2D joints and relative depth of joints. The initial estimation of 2D human skeletons, relative depth between adjacent joints and features is taken as input of CGANs to train the generator and discriminator. Then generator of CGANs is usedto generate multiple human meshes via different conditions whichare consistent with human silhouette and 2D joint points. Selecting and clustering are utilized to eliminate abnormal and redundant human meshes. The number of hypothesis is not unified for each single image, and it is dependent on 2D pose ambiguity. Unlike the existing end-to-end 3D human mesh recovery methods, our approach consists of three task-specific deep networks trained separately to mitigate the training burden in terms of time and datasets. Our approach has evaluated not only on the datasets of laboratory and real scenes but also on Internet images qualitatively and quantitatively, and experimental results demonstrate the effectiveness of our approach."
  },
  "accv2022_main_repf-netdistortion-awarere-projectionfusionnetworkforobjectdetectioninpanoramaimage": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "RepF-Net: Distortion-aware Re-projection Fusion Network for Object Detection in Panorama Image",
    "authors": [
      "Mengfan Li",
      "Ming Meng",
      "Zhong Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_RepF-Net_Distortion-aware_Re-projection_Fusion_Network_for_Object_Detection_in_Panorama_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_RepF-Net_Distortion-aware_Re-projection_Fusion_Network_for_Object_Detection_in_Panorama_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Panorama image has a large 360deg field of view, providing rich contextual information for object detection, widely used in virtual reality, augmented reality, scene understanding, etc. However, existing methods for object detection on panorama image still have some problems. When 360deg content is converted to the projection plane, the geometric distortion brought by the projection model makes the neural network can not extract features efficiently, the objects at the boundary of the projection image are also incomplete. To solve these problems, in this paper, we propose a novel two-stage detection network, RepF-Net, comprehensively utilizing multiple distortion-aware convolution modules to deal with geometric distortion while performing effective features extraction, and using the non-maximum fusion algorithm to fuse the content of the detected object in the post-processing stage. Our proposed unified distortion-aware convolution modules can be used to deal with distortions from geometric transforms and projection models, and be used to solve the geometric distortion caused by equirectangular projection and stereographic projection in our network. Our proposed non-maximum fusion algorithm fuses the content of detected objects to deal with incomplete object content separated by the projection boundary. Experimental results show that our RepF-Net outperforms previous state-of-the-art methods by 6% on mAP. Based on RepF-Net, we present an implementation of 3D object detection and scene layout reconstruction application."
  },
  "accv2022_main_learningusingprivilegedinformationforzero-shotactionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning Using Privileged Information for Zero-Shot Action Recognition",
    "authors": [
      "Zhiyi Gao",
      "Yonghong Hou",
      "Wanqing Li",
      "Zihui Guo",
      "Bin Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_Learning_Using_Privileged_Information_for_Zero-Shot_Action_Recognition_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_Learning_Using_Privileged_Information_for_Zero-Shot_Action_Recognition_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have never been seen during training. Most existing methods assume a shared semantic space between seen and unseen actions and intend to directly learn a mapping from a visual space to the semantic space. This approach has been challenged by the semantic gap between the visual space and semantic space. This paper presents a novel method that uses object semantics as privileged information to narrow the semantic gap and, hence, effectively, assist the learning. In particular, a simple hallucination network is proposed to implicitly extract object semantics during testing without explicitly extracting objects and a cross-attention module is developed to augment visual feature with the object semantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have shown that the proposed method outperforms the state-of-the-art methods by a large margin."
  },
  "accv2022_main_csiecodedstrip-patternsimageenhancementembeddedinstructuredlight-basedmethods": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CSIE: Coded Strip-patterns Image Enhancement Embedded in Structured Light-based Methods",
    "authors": [
      "Wei Cao",
      "Yuping Ye",
      "Chu Shi",
      "Zhan Song"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Cao_CSIE_Coded_Strip-patterns_Image_Enhancement_Embedded_in_Structured_Light-based_Methods_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Cao_CSIE_Coded_Strip-patterns_Image_Enhancement_Embedded_in_Structured_Light-based_Methods_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "When a coded strip-patterns image (CSI) is captured in a structured light system (SLs), it often suffers from low visibility at low exposure settings. Besides degrading the visual perception of the CSI, this poor quality also significantly affects the performance of 3D model reconstruction. Most of the existing image-enhanced methods, however, focus on processing natural images but not CSI. In this paper, we propose a novel and effective CSI enhancement (CSIE) method designed for SLs. More concretely, a bidirectional perceptual consistency (BPC) criterion, including relative grayscale (RG), exposure, and texture level priors, is first introduced to ensure visual consistency before and after enhancement. Then, constrained by BPC, the optimization function estimates solutions of illumination with piecewise smoothness and reflectance with detail preservation. With well-refined solutions, CSIE results can be achieved accordingly and further improve the details performance of 3D model reconstruction. Experiments on multiple sets of challenging CSI sequences show that our CSIE outperforms the existing used for natural image-enhanced methods in terms of 2D enhancement, point clouds extraction (at least 17 % improvement), and 3D model reconstruction."
  },
  "accv2022_main_dolphinsdatasetforcollaborativeperceptionenabledharmoniousandinterconnectedself-driving": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DOLPHINS: Dataset for Collaborative Perception enabled Harmonious and Interconnected Self-driving",
    "authors": [
      "Ruiqing Mao",
      "Jingyu Guo",
      "Yukuan Jia",
      "Yuxuan Sun",
      "Sheng Zhou",
      "Zhisheng Niu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Mao_DOLPHINS_Dataset_for_Collaborative_Perception_enabled_Harmonious_and_Interconnected_Self-driving_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Mao_DOLPHINS_Dataset_for_Collaborative_Perception_enabled_Harmonious_and_Interconnected_Self-driving_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Vehicle-to-Everything (V2X) network has enabled collaborative perception in autonomous driving, which is a promising solution to the fundamental defect of stand-alone intelligence including blind zones and long-range perception. However, the lack of datasets has severely blocked the development of collaborative perception algorithms. In this work, we release DOLPHINS: Dataset for cOLlaborative Perception enabled Harmonious and INterconnected Self-driving, as a new simulated large-scale various-scenario multi-view multi-modality autonomous driving dataset, which provides a ground-breaking benchmark platform for interconnected autonomous driving. DOLPHINS outperforms current datasets in six dimensions: temporally-aligned images and point clouds from both vehicles and Road Side Units (RSUs) enabling both Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) based collaborative perception; 6 typical scenarios with dynamic weather conditions make the most various interconnected autonomous driving dataset; meticulously selected viewpoints providing full coverage of the key areas and every object; 42376 frames and 292549 objects, as well as the corresponding 3D annotations, geo-positions, and calibrations, compose the largest dataset for collaborative perception; Full-HD images and 64-line LiDARs construct high-resolution data with sufficient details; well-organized APIs and open-source codes ensure the extensibility of DOLPHINS. We also construct a benchmark of 2D detection, 3D detection, and multi-view collaborative perception tasks on DOLPHINS. The experiment results show that the raw-level fusion scheme through V2X communication can help to improve the precision as well as to reduce the necessity of expensive LiDAR equipment on vehicles when RSUs exist, which may accelerate the popularity of interconnected self-driving vehicles. DOLPHINS dataset and related codes are now available on www.dolphins-dataset.net."
  },
  "accv2022_main_thinkinghallucinationforvideocaptioning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Thinking Hallucination for Video Captioning",
    "authors": [
      "Nasib Ullah",
      "Partha Pratim Mohanta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Ullah_Thinking_Hallucination_for_Video_Captioning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Ullah_Thinking_Hallucination_for_Video_Captioning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "With the advent of rich visual representations and pre-trained language models, video captioning has seen continuous improvement over time. Despite the performance improvement, video captioning models are prone to hallucination. Hallucination refers to the generation of highly pathological descriptions that are detached from the source material. In video captioning, there are two kinds of hallucination: object and action hallucination. Instead of endeavoring to learn better representations of a video, in this work, we investigate the fundamental sources of the hallucination problem. We identify three main factors: (i) inadequate visual features extracted from pre-trained models, (ii) improper influences of source and target contexts during multi-modal fusion, and (iii) exposure bias in the training strategy. To alleviate these problems, we propose two robust solutions: (a) the introduction of auxiliary heads trained in multi-label settings on top of the extracted visual features and (b) the addition of context gates, which dynamically select the features during fusion. The standard evaluation metrics for video captioning measure similarity with ground truth captions and do not adequately capture object and action relevance. To this end, we propose a new metric, COAHA (caption object and action hallucination assessment), which assesses the degree of hallucination. Our method achieves state-of-the-art performance on the MSR-Video to Text (MSR-VTT) and the Microsoft Research Video Description Corpus (MSVD) datasets, especially by a massive margin in CIDEr score."
  },
  "accv2022_main_a2adaptiveaugmentationforeffectivelymitigatingdatasetbias": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A^2: Adaptive Augmentation for Effectively Mitigating Dataset Bias",
    "authors": [
      "Jaeju An",
      "Taejune Kim",
      "Donggeun Ko",
      "Sangyup Lee",
      "Simon S Woo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/An_A2_Adaptive_Augmentation_for_Effectively_Mitigating_Dataset_Bias_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/An_A2_Adaptive_Augmentation_for_Effectively_Mitigating_Dataset_Bias_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, deep neural networks (DNNs) have become the de facto standard to achieve outstanding performances and demonstrate significant impact on various computer vision tasks for real-world scenarios. However, the trained networks can often suffer from overfitting issues due to the unintended bias in a dataset causing inaccurate, unreliable, and untrustworthy results. Thus, recent studies have attempted to remove bias by augmenting the bias-conflict samples to address this challenge. Yet, it still remains a challenge since generating bias-conflict samples without human supervision is generally difficult. To tackle this problem, we propose a novel augmentation framework, Adaptive Augmentation (A^2), based on a generative model that help classifiers learn debiased representations. Our framework consists of three steps: 1) extracting bias-conflict samples from a biased dataset in an unsupervised manner, 2) training a generative model with the biased dataset and adapting the learned biased distribution to the extracted bias-conflict samples' distribution, and 3) augmenting bias-conflict samples by translating bias-align samples. Therefore, our classifier can effectively learn the debiased representation without human supervision. Our extensive experimental results demonstrate that A^2 effectively augments bias-conflict samples, mitigating widespread bias issues. The code is available in here."
  },
  "accv2022_main_boundary-awaretemporalsentencegroundingwithadaptiveproposalrefinement": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Boundary-aware Temporal Sentence Grounding with Adaptive Proposal Refinement",
    "authors": [
      "Jianxiang Dong",
      "Zhaozheng Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Dong_Boundary-aware_Temporal_Sentence_Grounding_with_Adaptive_Proposal_Refinement_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Dong_Boundary-aware_Temporal_Sentence_Grounding_with_Adaptive_Proposal_Refinement_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Temporal sentence grounding (TSG) in videos aims to localize the temporal interval from an untrimmed video that is relevant to a given query sentence. In this paper, we introduce an effective proposal-based approach to solve the TSG problem. A Boundary-aware Feature Enhancement (BAFE) module is proposed to enhance the proposal feature with its boundary information, by imposing a new temporal difference loss. Meanwhile, we introduce a Boundary-aware Feature Aggregation (BAFA) module to aggregate boundary features and propose a Proposal-level Contrastive Learning (PCL) method to learn query-related content features by maximizing the mutual information between the query and proposals. Furthermore, we introduce a Proposal Interaction (PI) module with Adaptive Proposal Selection (APS) strategies to effectively refine proposal representations and make the final localization. Extensive experiments on Charades-STA, ActivityNet-Captions and TACoS datasets show the effectiveness of our solution. Our code is available at https://github.com/DJX1995/BAN-APR."
  },
  "accv2022_main_twovideodatasetsfortrackingandretrievalofoutofdistributionobjects": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Two Video Data Sets for Tracking and Retrieval of Out of Distribution Objects",
    "authors": [
      "Kira Maag",
      "Robin Chan",
      "Svenja Uhlemeyer",
      "Kamil Kowol",
      "Hanno Gottschalk"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Maag_Two_Video_Data_Sets_for_Tracking_and_Retrieval_of_Out_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Maag_Two_Video_Data_Sets_for_Tracking_and_Retrieval_of_Out_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this work we present two video test data sets for the novel computer vision (CV) task of out of distribution tracking (OOD tracking). Here, OOD objects are understood as objects with a semantic class outside the semantic space of an underlying image segmentation algorithm, or an instance within the semantic space which however looks decisively different from the instances contained in the training data. OOD objects occurring on video sequences should be detected on single frames as early as possible and tracked over their time of appearance as long as possible. During the time of appearance, they should be segmented as precisely as possible. We present the SOS data set containing 20 video sequences of street scenes and more than 1000 labeled frames with up to two OOD objects. We furthermore publish the synthetic CARLA-WildLife data set that consists of 26 video sequences containing up to four OOD objects on a single frame. We propose metrics to measure the success of OOD tracking and develop a baseline algorithm that efficiently tracks the OOD objects. As an application that benefits from OOD tracking, we retrieve OOD sequences from unlabeled videos of street scenes containing OOD objects."
  },
  "accv2022_main_lsmd-netlidar-stereofusionwithmixturedensitynetworkfordepthsensing": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "LSMD-Net: LiDAR-Stereo Fusion with Mixture Density Network for Depth Sensing",
    "authors": [
      "HanXi Yin",
      "Lei Deng",
      "Zhixiang Chen",
      "Baohua Chen",
      "Ting Sun",
      "Xie Yusen",
      "Junewei Xiao",
      "Yeyu Fu",
      "Shuixin Deng",
      "Xiu Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yin_LSMD-Net_LiDAR-Stereo_Fusion_with_Mixture_Density_Network_for_Depth_Sensing_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yin_LSMD-Net_LiDAR-Stereo_Fusion_with_Mixture_Density_Network_for_Depth_Sensing_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Depth sensing is critical to many computer vision applications but remains challenge to generate accurate dense information with single type sensor. The stereo camera sensor can provide dense depth prediction but underperforms in texture-less, repetitive and occlusion areas while the LiDAR sensor can generate accurate measurements but results in sparse map. In this paper, we advocate to fuse LiDAR and stereo camera for accurate dense depth sensing. We consider the fusion of multiple sensors as a multimodal prediction problem. We propose a novel end-to-end learning framework, dubbed as LSMD-Net to faithfully generate dense depth. The proposed method has dual-branch disparity predictor and predicts a bimodal Laplacian distribution over disparity at each pixel. This distribution has two modes which captures the information from two branches. Predictions from the branch with higher confidence is selected as the final disparity result at each specific pixel. Our fusion method can be applied for different type of LiDARs. Besides the existing dataset captured by conventional spinning LiDAR, we build a multiple sensor system with a non-repeating scanning LiDAR and a stereo camera and construct a depth prediction dataset with this system. Evaluations on both KITTI datasets and our home-made dataset demonstrate the superiority of our proposed method in terms of accuracy and computation time."
  },
  "accv2022_main_3d-c2ftcoarse-to-finetransformerformulti-view3dreconstruction": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "3D-C2FT: Coarse-to-fine Transformer for Multi-view 3D Reconstruction",
    "authors": [
      "Leslie Ching Ow Tiong",
      "Dick Sigmund",
      "Andrew Beng Jin Teoh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tiong_3D-C2FT_Coarse-to-fine_Transformer_for_Multi-view_3D_Reconstruction_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tiong_3D-C2FT_Coarse-to-fine_Transformer_for_Multi-view_3D_Reconstruction_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, the transformer model has been successfully employed for the multi-view 3D reconstruction problem. However, challenges remain in designing an attention mechanism to explore the multi-view features and exploit their relations for reinforcing the encoding-decoding modules. This paper proposes a new model, namely 3D coarse-to-fine transformer (3D-C2FT), by introducing a novel coarse-to-fine (C2F) attention mechanism for encoding multi-view features and rectifying defective voxel-based 3D objects. C2F attention mechanism enables the model to learn multi-view information flow and synthesize 3D surface correction in a coarse to fine-grained manner. The proposed model is evaluated by ShapeNet and Multi-view Real-life voxel-based datasets. Experimental results show that 3D-C2FT achieves notable results and outperforms several competing models on these datasets."
  },
  "accv2022_main_ps-armanend-to-endattention-awarerelationmixernetworkforpersonsearch": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PS-ARM: An End-to-End Attention-aware Relation Mixer Network for Person Search",
    "authors": [
      "Mustansar Fiaz",
      "Hisham Cholakkal",
      "Sanath Narayan",
      "Rao Muhammad Anwer",
      "Fahad Shahbaz Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Fiaz_PS-ARM_An_End-to-End_Attention-aware_Relation_Mixer_Network_for_Person_Search_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Fiaz_PS-ARM_An_End-to-End_Attention-aware_Relation_Mixer_Network_for_Person_Search_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Person search is a challenging problem with various real-world applications, that aims at joint person detection and re-identification of a query person from uncropped gallery images.Although, previous study focuses on rich feature information learning, it's still hard to retrieve the query person due to the occurrence of appearance deformations and background distractors. In this paper, we propose a novel person search network, which exploits the global relation between different local regions within RoI of a person. Our design focuses on the introduction of a novel attention-aware relation mixer (ARM) module containing a relation mixer block and a spatio-channel attention layer. The relation mixer block introduces a spatially attended spatial mixing and a channel-wise attended channel mixing for effectively capturing discriminative relation features within an RoI.These discriminative relation features are further enriched by introducing a spatio-channel attention where the foreground and background features are delineated in a joint spatio-channel space.Our ARMmodule is generic and it does not rely onfine-grained supervisions or topological assumptions, hence being easily integrated into anyFaster R-CNN-based person search methods.Comprehensive experiments are performed on two challenging benchmark datasets: CUHK-SYSU [1]and PRW [2]. Our PS-ARM achieves state-of-the-art performance on both datasets.On the challenging PRW dataset,our PS-ARM achieves anabsolute gain of 5% in the mAP score over SeqNet, while operating at a comparable speed. Our source code and trained models will be made public."
  },
  "accv2022_main_efficienthardware-awareneuralarchitecturesearchforimagesuper-resolutiononmobiledevices": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Efficient Hardware-aware Neural Architecture Search for Image Super-resolution on Mobile Devices",
    "authors": [
      "Xindong Zhang",
      "Hui Zeng",
      "Lei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Efficient_Hardware-aware_Neural_Architecture_Search_for_Image_Super-resolution_on_Mobile_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Efficient_Hardware-aware_Neural_Architecture_Search_for_Image_Super-resolution_on_Mobile_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "With the ubiquitous use of mobile devices in our daily life, how to design a lightweight network for high-performance image super-resolution (SR) has become increasingly important. However, it is difficult and laborious to manually design and deploy different SR models on different mobile devices, while the existing network architecture search (NAS) techniques are expensive and unfriendly to find the desired SR networks for various hardware platforms. To mitigate these issues, we propose an efficient hardware-aware neural architecture search (EHANAS) method for SR on mobile devices. First, EHANAS supports searching in a large network architecture space, including the macro topology (e.g., number of blocks) and microstructure (e.g., kernel type, channel dimension, and activation type) of the network. By introducing a spatial and channel masking strategy and a re-parameterization technique, we are able to finish the whole searching procedure using one single GPU card within one day. Second, the hardware latency is taken as a direct constraint on the searching process, enabling hardware-adaptive optimization of the searched SR model. Experiments on two typical mobile devices demonstrate the effectiveness of the proposed EHANAS method, where the searched SR models obtain better performance than previously manually designed and automatically searched models. The source codes of EHANAS can be found at https://github.com/xindongzhang/EHANAS."
  },
  "accv2022_main_featuredecoupledknowledgedistillationviaspatialpyramidpooling": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Feature Decoupled Knowledge Distillation via Spatial Pyramid Pooling",
    "authors": [
      "Lei Gao",
      "Hui Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_Feature_Decoupled_Knowledge_Distillation_via_Spatial_Pyramid_Pooling_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_Feature_Decoupled_Knowledge_Distillation_via_Spatial_Pyramid_Pooling_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Knowledge distillation (KD) is an effective and widely used technique of model compression which enables the deployment of deep networks in low-memory or fast-execution scenarios. Feature-based knowledge distillation is an important component of KD which leverages intermediate layers to supervise the training procedure of a student network. Nevertheless, the potential mismatch of intermediate layers may be counterproductive in the training procedure. In this paper, we propose a novel distillation framework, termed Decoupled Spatial Pyramid Pooling Knowledge Distillation, to distinguish the importance of regions in feature maps. Specifically, we reveal that (1) spatial pyramid pooling is an outstanding method to define the knowledge and(2) the lower activation regions in feature maps play a more important role in KD. Our experiments on CIFAR-100 and Tiny-ImageNet achieve state-of-the-art results."
  },
  "accv2022_main_haze-nethigh-frequencyattentivesuper-resolvedgazeestimationinlow-resolutionfaceimages": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "HAZE-Net: High-Frequency Attentive Super-Resolved Gaze Estimation in Low-Resolution Face Images",
    "authors": [
      "Jun-Seok Yun",
      "Youngju Na",
      "Hee Hyeon Kim",
      "Hyung-Il Kim",
      "Seok Bong Yoo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Yun_HAZE-Net_High-Frequency_Attentive_Super-Resolved_Gaze_Estimation_in_Low-Resolution_Face_Images_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Yun_HAZE-Net_High-Frequency_Attentive_Super-Resolved_Gaze_Estimation_in_Low-Resolution_Face_Images_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Although gaze estimation methods have been developed with deep learning techniques, there has been no such approach as aim to attain accurate performance in low-resolution face images with a pixel width of 50 pixels or less. To solve a limitation under the challenging low-resolution conditions, we propose a high-frequency attentive super-resolved gaze estimation network, i.e., HAZE-Net. Our network improves the resolution of the input image and enhances the eye features and those boundaries via a proposed super-resolution module based on a high-frequency attention block. In addition, our gaze estimation module utilizes high-frequency components of the eye as well as the global appearance map. We also utilize the structural location information of faces to approximate head pose. The experimental results indicate that the proposed method exhibits robust gaze estimation performance even in low-resolution face images with 28x28 pixels. The source code of this work is available athttps://github.com/dbseorms16/HAZE_Net/."
  },
  "accv2022_main_pointcloudupsamplingviacascadedrefinementnetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Point Cloud Upsampling via Cascaded Refinement Network",
    "authors": [
      "Hang Du",
      "Xuejun Yan",
      "Jingjing Wang",
      "Di Xie",
      "Shiliang Pu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Du_Point_Cloud_Upsampling_via_Cascaded_Refinement_Network_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Du_Point_Cloud_Upsampling_via_Cascaded_Refinement_Network_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Point cloud upsampling focuses on generating a dense, uniform and proximity-to-surface point set. Most previous approaches accomplish these objectives by carefully designing a single-stage network, which makes it still challenging to generate a high-fidelity point distribution. Instead, upsampling point cloud in a coarse-to-fine manner is a decent solution. However, existing coarse-to-fine upsampling methods require extra training strategies, which are complicated and time consuming during the training. In this paper, we propose a simple yet effective cascaded refinement network, consisting of three generation stages that have the same network architecture but achieve different objectives. Specifically, the first two upsampling stages generate the dense but coarse points progressively, while the last refinement stage further adjust the coarse points to a better position. To mitigate the learning conflicts between multiple stages and decrease the difficulty of regressing new points, we encourage each stage to predict the point offsets with respect to the input shape. In this manner, the proposed cascaded refinement network can be easily optimized without extra learning strategies. Moreover, we design a transformer-based feature extraction module to learn the informative global and local shape context. In inference phase, we can dynamically adjust the model efficiency and effectiveness, depending on the available computational resources. Extensive experiments on both synthetic and real-scanned datasets demonstrate that the proposed approach outperforms the existing state-of-the-art methods. The code is publicly available at https://github.com/hikvision-research/3DVision."
  },
  "accv2022_main_videoobjectsegmentationviastructuralfeaturereconfiguration": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Video Object Segmentation via Structural Feature Reconfiguration",
    "authors": [
      "Zhenyu Chen",
      "Ping Hu",
      "Lu Zhang",
      "Huchuan Lu",
      "You He",
      "Shuo Wang",
      "Xiaoxing Zhang",
      "Maodi Hu",
      "Tao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_Video_Object_Segmentation_via_Structural_Feature_Reconfiguration_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Video_Object_Segmentation_via_Structural_Feature_Reconfiguration_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recent memory-based methods have made significant progress for semi-supervised video object segmentation, by explicitly modeling the semantic correspondences between the target frame and the historical ones. However, the indiscriminate acceptance of historical frames into the memory bank and the lack of fine-grained extraction for target objects may incur high latency and information redundancy in these approaches. In this paper, we circumvent the challenges by developing a Structural Feature Reconfiguration Network (SFRNet). The proposed SFRNet consists of two core sub-modules, which are the Global-temporal Attention Module (GAM) and the Local-spatial Attention Module (LAM). In GAM, we exploit self-attention-based encoders to capture the target objects' temporal context from historical frames. The LAM then reconfigures features with the current frame's spatial structural prior, which reinforces the objectness of foreground objects and suppresses the interference from background regions. By doing so, our model reduces the reliance on the large memory bank containing redundant historical frames, while instead effectively segmenting video objects with spatio-temporal context aggregated from a small set of key frames. We conduct extensive experiments with benchmark datasets, and the results demonstrate our method's favorable performance against the state-of-the-art approaches. The model and code will be publicly available."
  },
  "accv2022_main_denetdetection-drivenenhancementnetworkforobjectdetectionunderadverseweatherconditions": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DENet: Detection-driven Enhancement Network for Object Detection under Adverse Weather Conditions",
    "authors": [
      "Qingpao Qin",
      "Kan Chang",
      "Mengyuan Huang",
      "Guiqing Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Qin_DENet_Detection-driven_Enhancement_Network_for_Object_Detection_under_Adverse_Weather_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Qin_DENet_Detection-driven_Enhancement_Network_for_Object_Detection_under_Adverse_Weather_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Recently, the deep learning-based object detection methods have achieved a great success. However, the performance of such techniques deteriorates on the images captured under adverse weather conditions. To tackle this problem, a detection-driven enhancement network (DENet) which consists of three key modules for object detection is proposed. By using Laplacian pyramid, each input image is decomposed to a low-frequency (LF) component and several high-frequency (HF) components. For the LF component, a global enhancement module which consists of four parallel paths with different convolution kernel sizes is presented to well capture multi-scale features. For HF components, a cross-level guidance module is used to extract cross-level guidance information from the LF component, and affine transformation is applied in a detail enhancement module to incorporate the guidance information into the HF features. By cascading the proposed DENet and a common YOLO detector, we establish an elegant detection framework called DE-YOLO. Through experiments, we find that DENet avoids heavy computation and faithfully preserves the latent features which are beneficial to detection, and DE-YOLO is effective for images captured under both the normal condition and adverse weather conditions. The codes and pre-trained models are available at: https://github.com/NIvykk/DENet."
  },
  "accv2022_main_trulyunsupervisedimage-to-imagetranslationwithcontrastiverepresentationlearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Truly Unsupervised Image-to-Image Translation with Contrastive Representation Learning",
    "authors": [
      "Zhiwei Hong",
      "Jianxing Feng",
      "Tao Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hong_Truly_Unsupervised_Image-to-Image_Translation_with_Contrastive_Representation_Learning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hong_Truly_Unsupervised_Image-to-Image_Translation_with_Contrastive_Representation_Learning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Image-to-image translation is a classic image generation task that attempts to translate an image from the source domain to an analogous image in the target domain. Recent advances in deep generative networks have shown remarkable capabilities in translating images among different domains. Most of these models either require pixel-level (with paired input and output images) or domain-level (with image domain labels) supervision to help the translation task. However, there are practical situations where the required supervisory information is difficult to collect and one would need to perform truly unsupervised image translation on a large number of images without paired image information or domain labels. In this paper, we present a truly unsupervised image-to-image translation model that performs the image translation task without any extra supervision. The crux of our model is an embedding network that extracts the domain and style information of the input style (or reference) image with contrastive representation learning and serves the translation module that actually carries out the translation task. The embedding network and the translation module can be integrated together for training and benefit from each other. Extensive experimental evaluation has been performed on various datasets concerning both cross-domain and multi-domain translation. The results demonstrate that our model outperforms the best truly unsupervised image-to-image translation model in the literature. In addition, our model can be easily adapted to take advantage of available domain labels to achieve a performance comparable to the best supervised image translation methods when all domain labels are known or a superior performance when only some domain labels are known."
  },
  "accv2022_main_cv4codesourcecodeunderstandingviavisualcoderepresentations": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CV4Code: Sourcecode Understanding via Visual Code Representations",
    "authors": [
      "Ruibo Shi",
      "Lili Tao",
      "Rohan Saphal",
      "Fran Silavong",
      "Sean Moran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Shi_CV4Code_Sourcecode_Understanding_via_Visual_Code_Representations_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CV4Code_Sourcecode_Understanding_via_Visual_Code_Representations_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present CV4Code, a compact and effective computer vision method for sourcecode understanding. Our method leverages the contextual and the structural information available from the code snippet by treating each snippet as a two-dimensional image, which naturally encodes the context and retains the underlying structural information through an explicit spatial representation. To codify snippets as images, we propose an ASCII codepoint-based image representation that facilitates fast generation of sourcecode images and eliminates redundancy in the encoding that would arise from an RGB pixel representation. Furthermore, as sourcecode is treated as images, neither lexical analysis (tokenisation) nor syntax tree parsing is required, which makes the proposed method agnostic to any particular programming language and lightweight from the application pipeline point of view. CV4Code can even featurise syntactically incorrect code which is not possible from methods that depend on the Abstract Syntax Tree (AST). We demonstrate the effectiveness of CV4Code by learning Convolutional and Transformer networks to predict the functional task, i.e. the problem it solves, of the source code directly from its two-dimensional representation, and using an embedding from its latent space to derive a similarity score of two code snippets in a retrieval setup. Experimental results show that our approach achieves state-of-the-art performance in comparison to other methods with the same task and data configurations. For the first time we show the benefits of treating sourcecode understanding as a form of image processing task."
  },
  "accv2022_main_multi-modalcharacteristicguideddepthcompletionnetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-modal Characteristic Guided Depth Completion Network",
    "authors": [
      "Yongjin Lee",
      "Seokjun Park",
      "Beomgu Kang",
      "HyunWook Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lee_Multi-modal_Characteristic_Guided_Depth_Completion_Network_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lee_Multi-modal_Characteristic_Guided_Depth_Completion_Network_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Depth completion techniques fuse sparse depth map from LiDAR with color image to generate accurate dense depth map. Typically, multi-modal techniques utilize complementary characteristics of each modality, overcoming the limited information from a single modality. Especially in the depth completion, LiDAR data has relatively dense depth information for objects in the near distance but lacks the information of distant object and its boundary. On the other hand, color image has dense information for objects even in the far distance including the object boundary. Thus, the complementary characteristics of the two modalities are well suited for fusion, and many depth completion studies have proposed fusion networks to address the sparsity of LiDAR data. However, the previous fusion networks tend to simply concatenate the two-modality data and rely on deep neural network to extract useful features, not considering the inherited characteristics of each modality. To enable the effective modality-aware fusion, we propose a confidence guidance module (CGM) that estimates confidence maps which emphasizes salient region for each modality. In experiment, we showed that the confidence map for LiDAR data focused on near area and object surface, while those for color image focused on distant area and object boundary. Also, we propose a shallow feature fusion module (SFFM) to combine two types of input modality. Furthermore, a parallel refinement stage for each modality is proposed to reduce the computation time. Our results showed that the proposed model showed much faster computation time and competitive performance compared to the top-ranked models on the KITTI depth completion online leaderboard."
  },
  "accv2022_main_scaleadaptivefusionnetworkforrgb-dsalientobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Scale Adaptive Fusion Network for RGB-D Salient Object Detection",
    "authors": [
      "Yuqiu Kong",
      "Yushuo Zheng",
      "Cuili Yao",
      "Yang Liu",
      "He Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kong_Scale_Adaptive_Fusion_Network_for_RGB-D_Salient_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kong_Scale_Adaptive_Fusion_Network_for_RGB-D_Salient_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "RGB-D Salient Object Detection (SOD) is a fundamental problem in the field of computer vision and relies heavily on multi-modal interaction between the RGB and depth information. However, most existing approaches adopt the same fusion module to integrate RGB and depth features in multiple scales of the networks, without distinguishing the unique attributes of different layers, e.g.the geometric information in the shallower scales, the structural features in the middle scales, and the semantic cues in the deeper scales. In this work, we propose a Scale Adaptive Fusion Network (SAFNet) for RGB-D SOD which employs scale adaptive modules to fuse the RGB-D features. Specifically, for the shallow scale, we conduct the early fusion strategy by mapping the 2D RGB-D images to a 3D point cloud and learning a unified representation of the geometric information in the 3D space. For the middle scale, we model the structural features from multi-modalities by exploring spatial contrast information from the depth space. For the deep scale, we design a depth-aware channel-wise attention module to enhance the semantic representation of the two modalities. Extensive experiments demonstrate the superiority of the scale-adaptive fusion strategy adopted by our method. The proposed SAFNet achieves favourable performance against state-of-the-art algorithms on six large-scale benchmarks."
  },
  "accv2022_main_aprototype-orientedcontrastiveadaptionnetworkforcross-domainfacialexpressionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Prototype-Oriented Contrastive Adaption Network For Cross-domain Facial Expression Recognition",
    "authors": [
      "Chao Wang",
      "Jundi Ding",
      "Hui Yan",
      "Si Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_A_Prototype-Oriented_Contrastive_Adaption_Network_For_Cross-domain_Facial_Expression_Recognition_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_A_Prototype-Oriented_Contrastive_Adaption_Network_For_Cross-domain_Facial_Expression_Recognition_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Numerous well-performing facial expression recognition algorithms suffer from severe slippage when trained on one dataset and tested on another, due to inconsistencies in facial expression datasets caused by different acquisition conditions and subjective biases of annotators. In order to improve the generalization ability of the model, in this paper we propose a simple but effective Prototype-Oriented Contrastive Adaptation Network (POCAN) unified contrastive learning and prototype networks for cross-domain facial expression recognition. We employ a two-stage training pipeline. Specifically, in the first stage, we pre-train on the source domain to obtain semantically meaningful features and obtain good initial conditions for the target domain. In the second stage, we perform intra-domain feature learning and inter-domain feature fusion by narrowing the distance between samples and their corresponding prototypes and widening the distance with other prototypes, and we also use an adversarial loss function for domain-level alignment. In addition, we also consider the problem of data category imbalance, and category weights are introduced into our method so that the categories of the two domains are in a uniform distribution. Extensive experiments show that our method can yield competitive performance on both lab-controlled and in-the-wild datasets."
  },
  "accv2022_main_learningtextureenhancementpriorwithdeepunfoldingnetworkforsnapshotcompressiveimaging": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning Texture Enhancement Prior with Deep Unfolding Network for Snapshot Compressive Imaging",
    "authors": [
      "Mengying Jin",
      "Zhihui Wei",
      "Liang Xiao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Jin_Learning_Texture_Enhancement_Prior_with_Deep_Unfolding_Network_for_Snapshot_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Jin_Learning_Texture_Enhancement_Prior_with_Deep_Unfolding_Network_for_Snapshot_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Coded Aperture Snapshot Spectral Imaging (CASSI) utilizes a two-dimensional (2D) detector to capture three-dimensional (3D) data, significantly reducing the acquisition cost of hyperspectral images. However, such an ill-posed problem desires a reliable decoding algorithm with a well-designed prior term. This paper proposes a decoding model with a learnable prior term for snapshot compressive imaging. We expand the inference obtained by Half Quadratic Splitting (HQS) to construct our Texture Enhancement Prior learning network, TEP-net. Considering the high-frequency information representing the texture can effectively enhance the reconstruction quality. We then propose the residual Shuffled Multi-spectral Channel Attention(Shuffled-MCA) module to learn information corresponding to different frequency components by introducing the Discrete Cosine Transform (DCT) bases. In order to overcome the drawbacks of grouping operations within the MCA module efficiently, we employ the channel shuffle operation instead of a channel-wise operation. Channel shuffle rearranges the channel descriptors, allowing for better extraction of channel correlations subsequently. The experimental results show that our method outperforms the existing state-of-the-art method in numerical indicators. At the same time, the visualization results also show our superior performance in texture enhancement."
  },
  "accv2022_main_noisetransferimagenoisegenerationwithcontrastiveembeddings": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "NoiseTransfer: Image Noise Generation with Contrastive Embeddings",
    "authors": [
      "Seunghwan Lee",
      "Tae Hyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lee_NoiseTransfer_Image_Noise_Generation_with_Contrastive_Embeddings_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lee_NoiseTransfer_Image_Noise_Generation_with_Contrastive_Embeddings_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep image denoising networks have achieved impressive success with the help of a considerably large number of synthetic train datasets. However, real-world denoising is a still challenging problem due to the dissimilarity between distributions of real and synthetic noisy datasets. Although several real-world noisy datasets have been presented, the number of train datasets (i.e., pairs of clean and real noisy images) is limited, and acquiring more real noise datasets is laborious and expensive. To mitigate this problem, numerous attempts to simulate real noise models using generative models have been studied. Nevertheless, previous works had to train multiple networks to handle multiple different noise distributions. By contrast, we propose a new generative model that can synthesize noisy images with multiple different noise distributions. Specifically, we adopt recent contrastive learning to learn distinguishable latent features of the noise. Moreover, our model can generate new noisy images by transferring the noise characteristics solely from a single reference noisy image. We demonstrate the accuracy and the effectiveness of our noise model for both known and unknown noise removal."
  },
  "accv2022_main_multi-branchnetworkwithensemblelearningfortextremovalinthewild": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Multi-Branch Network with Ensemble Learning for Text Removal in the Wild",
    "authors": [
      "Yujie Hou",
      "Jiwei Ji Chen",
      "Zengfu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Hou_Multi-Branch_Network_with_Ensemble_Learning_for_Text_Removal_in_the_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Hou_Multi-Branch_Network_with_Ensemble_Learning_for_Text_Removal_in_the_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The scene text removal (STR) is to substitute visually realistic backgrounds for text regions. Due to the diversity of scene text and the intricacy of backgrounds, earlier STR approaches may not be able to successfully remove scene texts. We discover that different networks produce different text removal results. Thus, we present a novel STR approach with a multi-branch network to entirely erase the text while maintaining the integrity of the backgrounds. The main branch preserves high-resolution texture information, while two sub-branches learn multi-scale semantic features. The complementary erasure networks are integrated with two ensemble learning fusion mechanisms: a featurelevel fusion and an image-level fusion. Additionally, we propose a patch attention module to perceive text location and generate text attention features. Our method outperforms state-of-the-art approaches on both real-world and synthetic datasets, improving PSNR by 1.78 dB in the SCUT-EnsText dataset and 4.45 dB in the SCUT-Syn dataset."
  },
  "accv2022_main_acylindricalconvolutionnetworkfordensetop-viewsemanticsegmentationwithlidarpointclouds": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Cylindrical Convolution Network for Dense Top-View Semantic Segmentation with LiDAR Point Clouds",
    "authors": [
      "Jiacheng Lu",
      "Shuo Gu",
      "Cheng-Zhong Xu",
      "Hui Kong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lu_A_Cylindrical_Convolution_Network_for_Dense_Top-View_Semantic_Segmentation_with_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lu_A_Cylindrical_Convolution_Network_for_Dense_Top-View_Semantic_Segmentation_with_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Accurate semantic scene understanding of the surrounding environment is a challenge for autonomous driving systems. Recent LiDAR-based semantic segmentation methods mainly focus on predicting point-wise semantic classes, which cannot be directly used before the further densification process. In this paper, we propose a cylindrical convolution network for dense semantic understanding in the top-view LiDAR data representation. 3D LiDAR point clouds are divided into cylindrical partitions before feeding to the network, where semantic segmentation is conducted in the cylindrical representation. Then a cylinder-to-BEV transformation module is introduced to obtain sparse semantic feature maps in the top view. In the end, we propose a modified encoder-decoder network to get the dense semantic estimations. Experimental results on the SemanticKITTI and nuScenes-LidarSeg datasets show that our method outperforms the state-of-the-art methods with a large margin."
  },
  "accv2022_main_brightasthesunin-depthanalysisofimagination-drivenimagecaptioning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Bright as the Sun: In-depth Analysis of Imagination-driven Image Captioning",
    "authors": [
      "Huyen Thi Thanh Tran",
      "Takayuki Okatani"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tran_Bright_as_the_Sun_In-depth_Analysis_of_Imagination-driven_Image_Captioning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tran_Bright_as_the_Sun_In-depth_Analysis_of_Imagination-driven_Image_Captioning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Existing studies on image captioning mainly focus on generating \"literal\" captions based on visual entities in images and their basic properties such as colors and spatial relationships. However, to describe images, humans use not only literal descriptions but also \"imagination-driven\" descriptions that characterize visual entities by some different entities; they are often more vivid, precise, and visually comprehensible by readers/hearers. Nonetheless, none of the existing studies seriously consider captions of this type. This study presents the first comprehensive analysis of the generation and evaluation of imagination-driven captions. Specifically, we first analyze imagination-driven captions in existing image captioning datasets. Then, we present the comprehensive categorizations of imagination-driven captions and their usage, discussing the (potential) issues with the current image captioning models to generate such captions. Next, compiling these captions extracted from the existing datasets and synthesizing fake captions, we create a dataset named IdC-I and -II. Using this dataset, we examine nine existing metrics of image captioning about how accurately they can evaluate imagination-driven caption generation. Last, we propose a baseline model for imagination-driven captioning. It has a built-in mechanism to select which to generate between literal and imagination-driven captions, which existing image captioning models cannot do. Experimental results demonstrate that our model performs better than six existing models, especially for imagination-driven caption generation. Dataset and code will be publicly available at:https://github.com/TranHuyen1191/Imagination-driven-Image-Captioning."
  },
  "accv2022_main_cclslcombinationofcontrastivelearningandsupervisedlearningforhandwrittenmathematicalexpressionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CCLSL: Combination of Contrastive Learning and Supervised Learning for Handwritten Mathematical Expression Recognition",
    "authors": [
      "Qiqiang Lin",
      "Xiaonan Huang",
      "Ning Bi",
      "Ching Y Suen",
      "Jun Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lin_CCLSL_Combination_of_Contrastive_Learning_and_Supervised_Learning_for_Handwritten_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lin_CCLSL_Combination_of_Contrastive_Learning_and_Supervised_Learning_for_Handwritten_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Handwritten Mathematical Expressions differ considerably from ordinary linear handwritten texts, due to their two-dimentional structures plus many special symbols and characters. Hence, HMER(Handwritten Mathematical Expression Recognition) is a lot more challenging compared with normal handwriting recognition. At present, the mainstream offline recognition systems are generally built on deep learning methods, but these methods can hardly cope with HEMR due to the lack of training data. In this paper, we propose an encoder-decoder method combining contrastive learning and supervised learning(CCLSL) , whose encoder is trained to learn semantic-invariant features between printed and handwritten characters effectively. CCLSL improves the robustness of the model in handwritten styles. Extensive experiments on CROHME benchmark show that without data enhancement, our model achieves an expression accuracy of 58.07% on CROHME2014, 55.88% on CROHME2016 and 59.63% on CROHME2019, which is much better than all previous state-of-the-art methods. Furthermore, our ensemble model added a boost of 2.5% to 3.4% to the accuracy, achieving the state-of-the-art performance on public CROHME datasets for the first time."
  },
  "accv2022_main_invertingadversariallyrobustnetworksforimagesynthesis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Inverting Adversarially Robust Networks for Image Synthesis",
    "authors": [
      "Ren\u00e1n A. Rojas-G\u00f3mez",
      "Raymond A. Yeh",
      "Minh N. Do",
      "Anh Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Rojas-Gomez_Inverting_Adversarially_Robust_Networks_for_Image_Synthesis_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Rojas-Gomez_Inverting_Adversarially_Robust_Networks_for_Image_Synthesis_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Despite unconditional feature inversion being the foundation of many image synthesis applications, training an inverter demands a high computational budget, large decoding capacity and imposing conditions such as autoregressive priors. To address these limitations, we propose the use of adversarially robust representations as a perceptual primitive for feature inversion. We train an adversarially robust encoder to extract disentangled and perceptually-aligned image representations, making them easily invertible. By training a simple generator with the mirror architecture of the encoder, we achieve superior reconstruction quality and generalization over standard models. Based on this, we propose an adversarially robust autoencoder and demonstrate its improved performance on style transfer, image denoising and anomaly detection tasks. Compared to recent ImageNet feature inversion methods, our model attains improved performance with significantly less complexity."
  },
  "accv2022_main_applicationofmulti-modalfusionattentionmechanisminsemanticsegmentation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Application of Multi-modal Fusion Attention Mechanism in Semantic Segmentation",
    "authors": [
      "Yunlong Liu",
      "Osamu Yoshie",
      "Hiroshi Watanabe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Liu_Application_of_Multi-modal_Fusion_Attention_Mechanism_in_Semantic_Segmentation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Liu_Application_of_Multi-modal_Fusion_Attention_Mechanism_in_Semantic_Segmentation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The difficulty of semantic segmentation in computer vision has been reintroduced as a topic of interest for researchers thanks to the advancement of deep learning algorithms. This research aims into the logic of multi-modal semantic segmentation on images with two different modalities of RGB and Depth, which employs RGB-D images as input. For cross-modal calibration and fusion, this research presents a novel FFCA Module. It can achieve the goal of enhancing segmentation results by acquiring complementing information from several modalities. This module is plug-and-play compatible and can be used with existing neural networks. A multi-modal semantic segmentation network named FFCANet has been designed to test the validity, with a dual-branch encoder structure and a global context module developed using the classic combination of ResNet and DeepLabV3+ backbone. Compared with the baseline, the model used in this research has drastically improved the accuracy of the semantic segmentation task."
  },
  "accv2022_main_diffusionmodelsforcounterfactualexplanations": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Diffusion Models for Counterfactual Explanations",
    "authors": [
      "Guillaume Jeanneret",
      "Loic Simon",
      "Frederic Jurie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Jeanneret_Diffusion_Models_for_Counterfactual_Explanations_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Jeanneret_Diffusion_Models_for_Counterfactual_Explanations_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Counterfactual explanations have shown promising results as a post-hoc framework to make image classifiers more explainable. In this paper, we propose DiME, a method allowing the generation of counterfactual images using the recent diffusion models. By leveraging the guided generative diffusion process, our proposed methodology shows how to use the gradients of the target classifier to generate counterfactual explanations of input instances. Further, we analyze current approaches to evaluate spurious correlations and extend the evaluation measurements by proposing a new metric: Correlation Difference. Our experimental validations show that the proposed algorithm surpasses previous State-of-the-Art results on 5 out of 6 metrics on CelebA."
  },
  "accv2022_main_vectorizingbuildingblueprints": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Vectorizing Building Blueprints",
    "authors": [
      "Weilian Song",
      "Mahsa Maleki Abyaneh",
      "Mohammad Amin A Shabani",
      "Yasutaka Furukawa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Song_Vectorizing_Building_Blueprints_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Song_Vectorizing_Building_Blueprints_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "This paper proposes a novel vectorization algorithm for high-definition floorplans with construction-level intricate architectural details, namely a blueprint. A state-of-the-art floorplan vectorization algorithm starts by detecting corners, whose process does not scale to high-definition floorplans with thin interior walls, small door frames, and long exterior walls. Our approach 1) obtains rough semantic segmentation by running off-the-shelf segmentation algorithms; 2) learning to infer missing smaller architectural components; 3) adding the missing components by a refinement generative adversarial network; and 4) simplifying the segmentation boundaries by heuristics. We have created a vectorized blueprint database consisting of 200 production scanned blueprint images. Qualitative and quantitative evaluations demonstrate the effectiveness of the approach, making significant boost in standard vectorization metrics over the current state-of-the-art and baseline methods. We will share our code at https://github.com/weiliansong/blueprint-vectorizer."
  },
  "accv2022_main_few-shotmetriclearningonlineadaptationofembeddingforretrieval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Few-Shot Metric Learning: Online Adaptation of Embedding for Retrieval",
    "authors": [
      "Deunsol Jung",
      "Dahyun Kang",
      "Suha Kwak",
      "Minsu Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Jung_Few-Shot_Metric_Learning_Online_Adaptation_of_Embedding_for_Retrieval_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Jung_Few-Shot_Metric_Learning_Online_Adaptation_of_Embedding_for_Retrieval_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Metric learning aims to build a distance metric typically by learning an effective embedding function that maps similar objects into nearby points in its embedding space. Despite recent advances in deep metric learning, it remains challenging for the learned metric to generalize to unseen classes with a substantial domain gap.To tackle the issue, we explore a new problem of few-shot metric learning that aims to adapt the embedding function to the target domain with only a few annotated data. We introduce three few-shot metric learning baselines and propose the Channel-Rectifier Meta-Learning (CRML), which effectively adapts the metric space online by adjusting channels of intermediate layers.Experimental analyses on miniImageNet, CUB-200-2011, MPII, as well as a new dataset, miniDeepFashion, demonstrate that our method consistently improves the learned metric by adapting it to target classes and achieves a greater gain in image retrieval when the domain gap from the source classes is larger."
  },
  "accv2022_main_reagformerreaggregationtransformerwithaffinegroupfeaturesfor3dobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "ReAGFormer: Reaggregation Transformer with Affine Group Features for 3D Object Detection",
    "authors": [
      "Chenguang Lu",
      "Kang Yue",
      "Yue Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Lu_ReAGFormer_Reaggregation_Transformer_with_Affine_Group_Features_for_3D_Object_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Lu_ReAGFormer_Reaggregation_Transformer_with_Affine_Group_Features_for_3D_Object_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Direct detection of 3D objects from point clouds is a challenging task due to sparsity and irregularity of point clouds. To capture point features from the raw point clouds for 3D object detection, most previous researches utilize PointNet and its variants as the feature learning backbone and have seen encouraging results. However, these methods capture point features independently without modeling the interaction between points, and simple symmetric functions cannot adequately aggregate local contextual features, which are vital for 3D object recognition. To address such limitations, we propose ReAGFormer, a reaggregation Transformer backbone with affine group features for point feature learning in 3D object detection, which can capture the dependencies between points on the aligned group feature space while retaining the flexible receptive fields. The key idea of ReAGFormer is to alleviate the perturbation of the point feature space by affine transformation and extract the dependencies between points using self-attention, while reaggregating the local point set features with the learned attention. Moreover, we also design multi-scale connections in the feature propagation layer to reduce the geometric information loss caused by point sampling and interpolation. Experimental results show that by equipping our method as the backbone for existing 3D object detectors, significant improvements and state-of-the-art performance are achieved over original models on SUN RGB-D and ScanNet V2 benchmarks."
  },
  "accv2022_main_layout-guidedindoorpanoramainpaintingwithplane-awarenormalization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization",
    "authors": [
      "Chao-Chen Gao",
      "Cheng-Hsiu Chen",
      "Jheng-Wei Su",
      "Hung-Kuo Chu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Gao_Layout-guided_Indoor_Panorama_Inpainting_with_Plane-aware_Normalization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Gao_Layout-guided_Indoor_Panorama_Inpainting_with_Plane-aware_Normalization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present an end-to-end deep learning framework for indoor panoramic image inpainting. Although previous inpainting methods have shown impressive performance on natural perspective images, most fail to handle panoramic images, particularly the indoor scenes, which usually contain complex structure and texture content. To achieve better inpainting quality, we propose to exploit both the global and local context of indoor panorama during the inpainting process. Specifically, we take the low-level layout edges estimated from input panorama as a prior to guide the inpainting model for recovering the global indoor structure. A plane-aware normalization module is employed to embed plane-wise style features derived from the layout into the generator, encouraging local texture restoration from adjacent room structures (i.e. ceiling, floor, and walls). Experimental results show that our work outperforms the current state-of-the-art methods on a public panoramic dataset quantitatively and qualitatively."
  },
  "accv2022_main_end-to-endsurfacereconstructionfortouchingtrajectories": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "End-to-end Surface Reconstruction For Touching Trajectories",
    "authors": [
      "Jiarui Liu",
      "Yuanpei Zhang",
      "Zhuojun Zou",
      "Jie Hao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Liu_End-to-end_Surface_Reconstruction_For_Touching_Trajectories_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Liu_End-to-end_Surface_Reconstruction_For_Touching_Trajectories_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Whereas vision based 3D reconstruction strategies have progressed substantially with the abundance of visual data and emerging machine-learning tools, there are as yet no equivalent work or datasets with which to probe the use of the touching information. Unlike vision data organized in regularly arranged pixels or point clouds evenly distributed in space, touching trajectories are composed of continues basic lines, which brings more sparsity and ambiguity. In this paper we address this problem by proposing the first end-to-end haptic reconstruction network, which takes any arbitrary touching trajectory as input, learns an implicit representation of the underling shape and output a watertight triangle surface.It is composed of three modules, namely trajectory feature extraction, 3D feature interpolation, as well as implicit surface validation. Our key insight is that formulating the haptic reconstruction process into an implicit surface learning problem not only brings the ability to reconstruct shapes, but also improves the fitting ability of the network in small datasets. To tackle the sparsity of the trajectories, we use a spatial gridding operator to assign features of touching trajectories into grids. A surface validation module is used to tackle the dilemma of computing resources and calculation accuracy. We also build the first touching trajectory dataset, formulating touching process under the guide of Gaussian Process. We demonstrate that our method performs favorably against other methods both in qualitive and quantitative way. Insights from the tactile signatures of the touching will aid the future design of virtual-reality and human-robot interactions."
  },
  "accv2022_main_unsupervisedonlinehashingwithmulti-bitquantization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Unsupervised Online Hashing with Multi-Bit Quantization",
    "authors": [
      "Zhenyu Weng",
      "Yuesheng Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Weng_Unsupervised_Online_Hashing_with_Multi-Bit_Quantization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Weng_Unsupervised_Online_Hashing_with_Multi-Bit_Quantization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Online hashing methods aim to update hash functions with newly arriving data streams, which can process large-scale data online. To this end, most existing methods update projection functions online and adopt a single-bit quantization strategy that quantizes each projected component with one bit. However, single-bit quantization results in large information loss in the quantization process and thus cannot preserve the similarity information of original data well. In this paper, we propose a novel unsupervised online hashing method with multi-bit quantization towards solving this problem, which consists of online data sketching and online quantizer learning. By maintaining a small-size data sketch to preserve the streaming data information, an orthogonal transformation is learned from the data sketch to make the components of the streaming data independent. Then, an optimal quantizer is learned to adaptively quantize each component with multiple bits by modeling the data distribution. Therefore, our method can quantize each component with multiple bits rather than one bit to better preserve the data similarity online. The experiments show that our method can achieve better search accuracy than the relevant online methods for approximate nearest neighbor search."
  },
  "accv2022_main_improvingthequalityofsparse-viewcone-beamcomputedtomographyviareconstruction-friendlyinterpolationnetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Improving the Quality of Sparse-view Cone-Beam Computed Tomography via Reconstruction-Friendly Interpolation Network",
    "authors": [
      "Yanli Wang",
      "Lianying Chao",
      "Wenqi Shan",
      "Haobo Zhang",
      "Zhiwei Wang",
      "Qiang Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_Improving_the_Quality_of_Sparse-view_Cone-Beam_Computed_Tomography_via_Reconstruction-Friendly_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Improving_the_Quality_of_Sparse-view_Cone-Beam_Computed_Tomography_via_Reconstruction-Friendly_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Reconstructing cone-beam computed tomography (CBCT) typically utilizes a Feldkamp-Davis-Kress (FDK) algorithm to 'translate' hundreds of 2D X-ray projections on different angles into a 3D CT image. For minimizing the X-ray induced ionizing radiation, sparse-view CBCT takes fewer projections by a wider-angle interval, but suffers from an inferior CT reconstruction quality. To solve this, the recent solutions mainly resort to synthesizing missing projections, and force the synthesized projections to be as realistic as those actual ones, which is extremely difficult due to X-ray's tissue superimposing. In this paper, we argue that the synthetic projections should restore FDK-required information as much as possible, while the visual fidelity is the secondary importance. Inspired by a simple fact that FDK only relies on frequency information after ramp-filtering for reconstruction, we develop a Reconstruction-Friendly Interpolation Network (RFI-Net), which first utilizes a 3D-2D attention network to learn inter-projection relations for synthesizing missing projections, and then introduces a novel Ramp-Filter loss to constrain a frequency consistency between the synthesized and real projections after ramp-filtering. By doing so, RFI-Net's energy can be forcibly devoted to restoring more CT-reconstruction useful information in projection synthesis. We build a complete reconstruction framework consisting of our developed RFI-Net, FDK and a commonly-used CT post-refinement. Experimental results on reconstruction from only one-eighth projections demonstrate that using RFI-Net restored full-view projections can significantly improve the reconstruction quality by increasing PSNR by 2.59 dB and 2.03 dB on the walnut and patient CBCT datasets, respectively, comparing with using those restored by other state-of-the-arts."
  },
  "accv2022_main_cirlacategory-instancerepresentationlearningframeworkfortropicalcycloneintensityestimation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "CIRL: A Category-Instance Representation Learning Framework for Tropical Cyclone Intensity Estimation",
    "authors": [
      "DengKe Wang",
      "Yajing Xu",
      "Yicheng Luo",
      "Qifeng Qian",
      "Lv Yuan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_CIRL_A_Category-Instance_Representation_Learning_Framework_for_Tropical_Cyclone_Intensity_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_CIRL_A_Category-Instance_Representation_Learning_Framework_for_Tropical_Cyclone_Intensity_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Tropical Cyclone (TC) intensity estimation is a continuous label classification problem, which aims to build a mapping relationship from TC images to intensities. Due to the similar visual appearance of TCs in adjacent intensities, the discriminative image representation plays an important role in TC intensity estimation. Existing works mainly revolve around the continuity of intensity which may result in a crowded feature distribution and perform poorly at distinguishing the boundaries of categories. In this paper, we focus on jointly learning category-level and instance-level representations from tropical cyclone images. Specially, we propose a general framework containing a CI-extractor and a classifier, inside which the CI-extractor is used to extract an instance-separable and category-discriminative representation between images. Meanwhile, an inter-class distance consistency (IDC) loss is applied on top of the framework which can lead to a more uniform feature distribution. In addition, a non-parameter smoothing algorithm is proposed to aggregate temporal information from the image sequence. Extensive experiments demonstrate that our method, with the result of 7.35 knots at RMSE, outperforms the state-of-the-art TC intensity estimation method on the TCIR dataset."
  },
  "accv2022_main_adiffusion-refinementmodelforsketch-to-pointmodeling": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "A Diffusion-ReFinement Model for Sketch-to-Point Modeling",
    "authors": [
      "Di Kong",
      "Qiang Wang",
      "Yonggang Qi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kong_A_Diffusion-ReFinement_Model_for_Sketch-to-Point_Modeling_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kong_A_Diffusion-ReFinement_Model_for_Sketch-to-Point_Modeling_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Diffusion probabilistic model has been proven effective in generative tasks. However, its variants have not yet delivered on its effectiveness in practice of cross-dimensional multimodal generation task. Generating 3D models from single free-hand sketches is a typically tricky cross-domain problem that grows even more important and urgent due to the widespread emergence of VR/AR technologies and usage of portable touch screens. In this paper, we introduce a novel Sketch-to-Point Diffusion-ReFinement model to tackle this problem. By injecting a new conditional reconstruction network and a refinement network, we overcome thebarrier of multimodal generation between the two dimensions. By explicitly conditioning the generation process on a given sketch image, our method can generate plausible point clouds restoring the sharp details and topology of 3D shapes, also matching the input sketches. Extensive experiments on various datasets show that our model achieves highly competitive performance in sketch-to-point generation task. The code is available at https://github.com/Walterkd/diffusion-refine-sketch2point."
  },
  "accv2022_main_unreliability-awaredisentanglingforcross-domainsemi-supervisedpedestriandetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Unreliability-aware Disentangling for Cross-Domain Semi-supervised Pedestrian Detection",
    "authors": [
      "Wenhao Wu",
      "Si Wu",
      "Hau-San Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wu_Unreliability-aware_Disentangling_for_Cross-Domain_Semi-supervised_Pedestrian_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wu_Unreliability-aware_Disentangling_for_Cross-Domain_Semi-supervised_Pedestrian_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The rapid progress of pedestrian detection is supported by the ever-growing labeled training data and elaborate neural-network-based model. However, adequate labeled training data are not always accessible when it comes to a new scene. Semi-supervised learning is promising for the case where a small amount of manually annotated images and a large amount of unannotated images are handy. In the semi-supervised setting, data generation is a powerful technique as a type of data augmentation. Some methods conduct data generation by disentangling pedestrian instances into different codes in latent space and combining codes of different instances to reconstruct new instances. However, these methods either work in a single domain or cannot handle the case where some instances are partially represented in the images. In this work, we propose to solve code-level information transferring from reliable domains to unreliable domains by incorporating a domain classifier that competes with the disentangling module to generate domain-invariant codes. An external classifier is trained on appearance-enhanced instances and sends integrity signals to the generative module, which facilitates the generative module to recognize fully/partially represented pedestrian instances. The resulting classifier ultimately renders high-quality pseudo-annotations for the unannotated data. The pseudo-annotated data, combined with a small amount of manually annotated data, are used to achieve a detector with more generalization and accuracy. We perform extensive experiments on multiple challenging benchmarks to demonstrate the effectiveness of the proposed method."
  },
  "accv2022_main_two-stagemultimodalityfusionforhigh-performancetext-basedvisualquestionanswering": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Two-stage Multimodality Fusion for High-performance Text-based Visual Question Answering",
    "authors": [
      "Bingjia Li",
      "Jie Wang",
      "Minyi Zhao",
      "Shuigeng Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Two-stage_Multimodality_Fusion_for_High-performance_Text-based_Visual_Question_Answering_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Two-stage_Multimodality_Fusion_for_High-performance_Text-based_Visual_Question_Answering_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Text-based visual question answering (TextVQA) is to answer a text-related question by reading texts in a given image, which needs to jointly reason over three modalities --- question, visual objects and scene texts in images. Most existing works leverage graph or sophisticated attention mechanisms to enhance the interaction between scene texts and visual objects. In this paper, observing that compared with visual objects, the question and scene text modalities are more important in TextVQA while both layouts and visual appearances of scene texts are also useful, we propose a two-stage multimodality fusion based method for high-performance TextVQA, which first semantically combines the question and OCR tokens to understand texts better and then integrates the combined results into visual features as additional information. Furthermore, to alleviate the redundancy and noise in the recognized scene texts, we develop a denoising module with contrastive loss to make our model focus on the relevant texts and thus obtain more robust features. Experiments on the TextVQA and ST-VQA datasets show that our method achieves competitive performance without any large-scale pre-training used in recent works, and outperforms the state-of-the-art methods after being pre-trained."
  },
  "accv2022_main_self-supervisedaugmentedpatchessegmentationforanomalydetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Self-Supervised Augmented Patches Segmentation for Anomaly Detection",
    "authors": [
      "Jun Long",
      "Yuxi Yang",
      "Liujie Hua",
      "Yiqi Ou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Long_Self-Supervised_Augmented_Patches_Segmentation_for_Anomaly_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Long_Self-Supervised_Augmented_Patches_Segmentation_for_Anomaly_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this paper, our goal is to detect unknown defects in high- resolution images in the absence of anomalous data. Anomaly detec- tion is usually performed at image-level or pixel-level. Considering that pixel-level anomaly classification achieves better representation learning in a finer-grained manner, we regard data augmentation transforms as a self-supervised segmentation task from which to extract the critical and representative information from images. Due to the unpredictabil- ity of anomalies in real scenarios, we propose a novel abnormal sam- ple simulation strategy which augmented patches are randomly pasted to original image to create a generalized anomalous pattern. Following the framework of self-supervised, segmenting augmented patches is used as a proxy task in the training phase to extract representation sep- arating normal and abnormal patterns, thus constructing a one-class classifier with a robust decision boundary. During the inference phase, the classifier is used to perform anomaly detection on the test data, while directly determining regions of unknown defects in an end-to-end manner. Our experimental results on MVTec AD dataset and BTAD dataset demonstrate the proposed SSAPS outperforms any other self- supervised based methods in anomaly detection. Code is available at https://github.com/BadSeedX/SSAPS."
  },
  "accv2022_main_mushmulti-scalehierarchicalfeatureextractionforsemanticimagesynthesis": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "MUSH: Multi-Scale Hierarchical Feature Extraction for Semantic Image Synthesis",
    "authors": [
      "Zicong Wang",
      "Qiang Ren",
      "Junli Wang",
      "Chungang Yan",
      "Changjun Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_MUSH_Multi-Scale_Hierarchical_Feature_Extraction_for_Semantic_Image_Synthesis_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_MUSH_Multi-Scale_Hierarchical_Feature_Extraction_for_Semantic_Image_Synthesis_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Semantic image synthesis aims to translate semantic label masks to photo-realistic images. Previous methods have limitations that extract semantic features with limited convolutional kernels and ignores some crucial information, such as relative positions of pixels. To address these issues, we propose MUSH, a novel semantic image synthesis model that utilizes multi-scale information. In the generative network stage, a multi-scale hierarchical architecture is proposed for feature extraction and merged successfully with guided sampling operation to enhance semantic image synthesis. Meanwhile, in the discriminative network stage, the model contains two different modules for feature extraction of semantic masks and real images, respectively, which helps use semantic masks information more effectively. Furthermore, our proposed model achieves the state-of-the-art qualitative evaluation and quantitative metrics on some challenging datasets. Experimental results show that our method can be generalized to various models for semantic image synthesis. Our code is available at https://github.com/WangZC525/MUSH."
  },
  "accv2022_main_gestalt-guidedimageunderstandingforfew-shotlearning": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Gestalt-Guided Image Understanding for Few-Shot Learning",
    "authors": [
      "Kun Song",
      "Yuchen Wu",
      "Jiansheng Chen",
      "Tianyu Hu",
      "Huimin Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Song_Gestalt-Guided_Image_Understanding_for_Few-Shot_Learning_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Song_Gestalt-Guided_Image_Understanding_for_Few-Shot_Learning_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Due to the scarcity of available data, deep learning does not perform well on few-shot learning tasks. However, human can quickly learn the feature of a new category from very few samples. Nevertheless, previous work has rarely considered how to mimic human cognitive behavior and apply it to few-shot learning. This paper introduces Gestalt psychology to few-shot learning and proposes Gestalt-Guided Image Understanding, a plug-and-play method called GGIU. Referring to the principle of totality and the law of closure in Gestalt psychology, we design Totality-Guided Image Understanding and Closure-Guided Image Understanding to extract image features. After that, a feature estimation module is used to estimate the accurate features of images. Extensive experiments demonstrate that our method can improve the performance of existing models effectively and flexibly without retraining or fine-tuning."
  },
  "accv2022_main_revisitingimagepyramidstructureforhighresolutionsalientobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Revisiting Image Pyramid Structure for High Resolution Salient Object Detection",
    "authors": [
      "Taehun Kim",
      "Kunhee Kim",
      "Joonyeong Lee",
      "Dongmin Cha",
      "Jiho Lee",
      "Daijin Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Kim_Revisiting_Image_Pyramid_Structure_for_High_Resolution_Salient_Object_Detection_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Kim_Revisiting_Image_Pyramid_Structure_for_High_Resolution_Salient_Object_Detection_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Salient object detection (SOD) has been in the spotlight recently, yet has been studied less for high-resolution (HR) images. Unfortunately, HR images and their pixel-level annotations are certainly more labor-intensive and time-consuming compared to low-resolution (LR) images and annotations. Therefore, we propose an image pyramid-based SOD framework, Inverse Saliency Pyramid Reconstruction Network (InSPyReNet), for HR prediction without any of HR datasets. We design InSPyReNet to produce a strict image pyramid structure of saliency map, which enables to ensemble multiple results with pyramid-based image blending. For HR prediction, we design a pyramid blending method which synthesizes two different image pyramids from a pair of LR and HR scale from the same image to overcome effective receptive field (ERF) discrepancy. Our extensive evaluations on public LR and HR SOD benchmarks demonstrate that InSPyReNet surpasses the State-of-the-Art (SotA) methods on various SOD metrics and boundary accuracy."
  },
  "accv2022_main_dac-gandualauxiliaryconsistencygenerativeadversarialnetworkfortext-to-imagegeneration": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "DAC-GAN: Dual Auxiliary Consistency Generative Adversarial Network for Text-to-Image Generation",
    "authors": [
      "Zhiwei Wang",
      "Jing Yang",
      "Jiajun Cui",
      "Jiawei Liu",
      "Jiahao Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Wang_DAC-GAN_Dual_Auxiliary_Consistency_Generative_Adversarial_Network_for_Text-to-Image_Generation_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Wang_DAC-GAN_Dual_Auxiliary_Consistency_Generative_Adversarial_Network_for_Text-to-Image_Generation_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Synthesizing an image from a given text encounters two major challenges: the integrity of images and the consistency of text-image pairs. Although many decent performances have been achieved, two crucial problems are still not considered adequately. (i) The object frame is prone to deviate or collapse, making subsequent refinement unavailable. (ii) The non-target regions of the image are affected by text which is highly conveyed through phrases, instead of words. Current methods barely employ the word-level clue, leaving coherent implication in phrases broken. To tackle the issues, we propose DAC-GAN, a Dual Auxiliary Consistency Generative Adversarial Network(DAC-GAN). Specifically, we simplify the generation by a single-stage structure with dual auxiliary modules. (1) Class-Aware skeleton Consistency(CAC) module retains the integrity of image by exploring additional supervision from prior knowledge and (2) Multi-label-Aware Consistency(MAC) module strengthens the alignment of text-image pairs at phrase-level. Comprehensive experiments on two widely-used datasets show that DAC-GAN can maintain the integrity of the target and enhance the consistency of text-image pairs."
  },
  "accv2022_main_autoenhancertransformeronu-netarchitecturesearchforunderwaterimageenhancement": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "AutoEnhancer: Transformer on U-Net Architecture search for Underwater Image Enhancement",
    "authors": [
      "Yi Tang",
      "Takafumi Iwaguchi",
      "Hiroshi Kawasaki",
      "Ryusuke Sagawa",
      "Ryo Furukawa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Tang_AutoEnhancer_Transformer_on_U-Net_Architecture_search_for_Underwater_Image_Enhancement_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Tang_AutoEnhancer_Transformer_on_U-Net_Architecture_search_for_Underwater_Image_Enhancement_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep neural architecture plays an important role in underwater image enhancement in recent years.Although most approaches have successfully introduced different structures (e.g., U-Net, generative adversarial network (GAN) and attention mechanisms) and designed individual neural networks for this task, these networks usually rely on the designer's knowledge, experience and intensive trials for validation. In this paper, we employ Neural Architecture Search (NAS) to automatically search the optimal U-Net architecture for underwater image enhancement, so that we can easily obtain an effective and lightweight deep network. Besides, to enhance the representation capability of the neural network, we propose a new search space including diverse operators, which is not limited to common operators, such as convolution or identity, but also transformers in our search space. Further, we apply the NAS mechanism to the transformer and propose a selectable transformer structure. In our transformer, the multi-head self-attention module is regarded as an optional unit and different self-attention modules can be used to replace the original one, thus deriving different transformer structures. This modification is able to further expand the search space and boost the learning capability of the deep model. The experiments on widely used underwater datasets are conducted to show the effectiveness of the proposed method."
  },
  "accv2022_main_pointformeradualperceptionattention-basednetworkforpointcloudclassification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "PointFormer: A Dual Perception Attention-based Network for Point Cloud Classification",
    "authors": [
      "Yijun Chen",
      "Zhulun Yang",
      "Xianwei Zheng",
      "Yadong Chang",
      "Xutao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Chen_PointFormer_A_Dual_Perception_Attention-based_Network_for_Point_Cloud_Classification_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Chen_PointFormer_A_Dual_Perception_Attention-based_Network_for_Point_Cloud_Classification_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Point cloud classification is a fundamental but still challenging task in 3-D computer vision. The main issue is that learning representational features from initial point cloud objects is always difficult for existing models. Inspired by the Transformer, which has achieved successful performance in the field of natural language processing, we propose a purely attention-based network, named PointFormer, for point cloud classification. Specifically, we design a novel simple point multiplicative attention mechanism. Based on that, we then construct both a local attention block and a global attention block to learn fine geometric features and overall representational features of the point cloud, respectively. Consequently, compared to the existing approaches, PointFormer has superior perception of local details and overall contours of the point cloud objects. In addition, we innovatively propose the Graph-Multiscale Perceptual Field (GMPF) testing strategy that can significantly improve the overall performance of the proposed PointFormer. We have conducted extensive experiments on the real-world dataset ScanObjectNN and the synthetic dataset ModelNet40. The results show that the PointFormer has stronger robustness and achieves highly competitive performance compared to other state-of-the-art approaches."
  },
  "accv2022_main_learningcommonandspecificvisualpromptsfordomaingeneralization": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Learning Common and Specific Visual Prompts for Domain Generalization",
    "authors": [
      "Aodi Li",
      "Liansheng Zhuang",
      "Shuo Fan",
      "Shafei Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Li_Learning_Common_and_Specific_Visual_Prompts_for_Domain_Generalization_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Li_Learning_Common_and_Specific_Visual_Prompts_for_Domain_Generalization_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Although fine-tuning a pre-trained large-scale model has become an effective method for domain generalization, domain shifts still issue a huge challenge for successfully transferring models to unseen test domains. In this paper, we study how to effectively adapt pre-trained vision Transformers for domain generalization problems in image classification. To this end, this paper proposes a novel Common-Specific Visual Prompt Tuning (CSVPT) method to transfer large-scale vision Transformer models to unknown test domains. Different from existing methods which learn fixed visual prompts for each task, CSVPT jointly learns domain-common prompts to capture the task context and sample-specific prompts to capture information about data distribution, which are generated for each sample through a trainable prompt-generating module (PGM). Combining the domain-common prompts and the sample-specific prompts, visual prompts learned by CSVPT are conditioned on each input sample rather than fixed once learned, which helps out-of-distribution generalization. Extensive experimental results show the effectiveness of CSVPT, and CSVPT with the backbone ViT-L/14 achieves state-of-the-art (SOTA) performance on five widely used benchmark datasets."
  },
  "accv2022_main_contra(con)text(tra)nsformerforcross-modalvideoretrieval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval",
    "authors": [
      "Adriano  Fragomeni",
      "Michael Wray",
      "Dima Damen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Fragomeni_ConTra_Context_Transformer_for_Cross-Modal_Video_Retrieval_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Fragomeni_ConTra_Context_Transformer_for_Cross-Modal_Video_Retrieval_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In this paper, we re-examine the task of cross-modal clip-sentence retrieval, where the clip is part of a longer untrimmed video. When the clip is short or visually ambiguous, knowledge of its local temporal context (i.e. surrounding video segments) can be used to improve the retrieval performance. We propose Context Transformer; an encoder architecture that models the interaction between a video clip and its local temporal context in order to enhance its embedded representations. Importantly, we supervise the context transformer using contrastive losses in the cross-modal embedding space. We explore context transformers for video and text modalities. Results consistently demonstrate improved performance on three datasets: YouCook2, EPIC-KITCHENS and a clip-sentence version of ActivityNet Captions. Exhaustive ablation studies and context analysis show the efficacy of the proposed method."
  },
  "accv2022_main_spatio-channelattentionblocksforcross-modalcrowdcounting": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Spatio-channel Attention Blocks for Cross-modal Crowd Counting",
    "authors": [
      "Youjia Zhang",
      "Soyun Choi",
      "Sungeun Hong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Spatio-channel_Attention_Blocks_for_Cross-modal_Crowd_Counting_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_Spatio-channel_Attention_Blocks_for_Cross-modal_Crowd_Counting_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Crowd counting research has made significant advancements in real-world applications, but it remains a formidable challenge in cross modal settings. Most existing methods rely solely on the optical features of RGB images, ignoring the feasibility of other modalities such as thermal and depth images. The inherently significant differences between the different modalities and the diversity of design choices for model architectures make cross-mode crowd counting more challenging. In this paper, we propose Cross-modal Spatio-Channel Attention (CSCA) blocks, which can be easily integrated into any modality-specific architecture. The CSCA blocks first spatially capture global functional correlations among multimodality with less overhead through spatial-wise cross-modal attention. Cross-modal features with spatial attention are subsequently refined through adaptive channel-wise feature aggregation. In our experiments, the proposed block consistently shows significant performance improvement across various backbone networks, resulting in state-of-the-art results in RGB-T and RGB-D crowd counting."
  },
  "accv2022_main_neuralresidualflowfieldsforefficientvideorepresentations": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ACCV2022",
    "title": "Neural Residual Flow Fields for Efficient Video Representations",
    "authors": [
      "Daniel Rho",
      "Junwoo Cho",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022/html/Rho_Neural_Residual_Flow_Fields_for_Efficient_Video_Representations_ACCV_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022/papers/Rho_Neural_Residual_Flow_Fields_for_Efficient_Video_Representations_ACCV_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Neural fields have emerged as a powerful paradigm for representing various signals, including videos. However, research on improving the parameter efficiency of neural fields is still in its early stages. Even though neural fields that map coordinates to colors can be used to encode video signals, this scheme does not exploit the spatial and temporal redundancy of video signals. Inspired by standard video compression algorithms, we propose a neural field architecture for representing and compressing videos that deliberately removes data redundancy through the use of motion information across video frames. Maintaining motion information, which is typically smoother and less complex than color signals, requires a far fewer number of parameters. Furthermore, reusing color values through motion information further improves the network parameter efficiency. In addition, we suggest using more than one reference frame for video frame reconstruction and separate networks, one for optical flows and the other for residuals. Experimental results have shown that the proposed method outperforms the baseline methods by a significant margin."
  }
}