{
  "iccv2015_w1_hierarchicalunion-of-subspacesmodelforhumanactivitysummarization": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Video Summarization for Large-Scale Analytics",
    "title": "Hierarchical Union-of-Subspaces Model for Human Activity Summarization",
    "authors": [
      "Tong Wu",
      "Prudhvi Gurram",
      "Raghuveer M. Rao",
      "Waheed U. Bajwa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/html/Wu_Hierarchical_Union-of-Subspaces_Model_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/papers/Wu_Hierarchical_Union-of-Subspaces_Model_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " A hierarchical union-of-subspaces model is proposed for performing semi-supervised human activity summarization in large streams of video data. The union of low-dimensional subspaces model is used to learn meaningful action attributes from a collection of high-dimensional video sequences of human activities. An approach called hierarchical sparse subspace clustering (HSSC) is developed to learn this model from the data in an unsupervised manner by capturing the variations or movements of each action in different subspaces, which allow the human actions to be represented as sequences of transitions from one subspace to another. These transition sequences can be used for human action recognition. The action attributes can also be represented at multiple resolutions using the subspaces at different levels of the hierarchical structure. By visualizing and labeling these action attributes, the hierarchical model can be used to semantically summarize long video sequences of human actions at different scales. The effectiveness of the proposed model is demonstrated through experiments on three real-world human action datasets for action recognition and semantic summarization of the actions using different resolutions of the action attributes."
  },
  "iccv2015_w1_ascalablearchitectureforoperationalfmvexploitation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Video Summarization for Large-Scale Analytics",
    "title": "A Scalable Architecture for Operational FMV Exploitation",
    "authors": [
      "William R. Thissell",
      "Robert Czajkowski",
      "Francis Schrenk",
      "Timothy Selway",
      "Anthony J. Ries",
      "Shamoli Patel",
      "Patricia L. McDermott",
      "Rod Moten",
      "Ron Rudnicki",
      "Guna Seetharaman",
      "Ilker Ersoy",
      "Kannappan Palaniappan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/html/Thissell_A_Scalable_Architecture_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/papers/Thissell_A_Scalable_Architecture_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " A scalable open systems and standards derived software ecosystem is described for computer vision analytics (CVA) assisted exploitation of full motion video (FMV).The ecosystem, referred to as the Advanced Video Activity Analytics (AVAA), has two instantiations, one for size, weight, and power (SWAP) constrained conditions, and the other for large to massive cloud based configurations.The architecture is designed to meet operational analyst requirements to increase their productivity and accuracy for exploiting FMV using local cluster or scalable cloud-based computing resources.CVAs are encapsulated within a software plug-in architecture and FMV processing pipelines are constructed by combining these plug-ins to accomplish analytical tasks and manage provenance of processing history.An example pipeline for real-time motion detection and moving object characterization using the flux tensor approach is presented.An example video ingest experiment is described.Quantitative and qualitative methods for human factors engineering (HFE) assessment to evaluate cognitive loads for alternative work flow design choices are discussed. This HFE process is used for validating that an AVAA system instantiation with candidate workflow pipelines meets CVA assisted FMV exploitation operational goals for specific analyst workflows.AVAA offers a new framework for video understanding at scale for large enterprise applications in the government and commercial sectors."
  },
  "iccv2015_w1_videosummarizationviasegmentssummarygraphs": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Video Summarization for Large-Scale Analytics",
    "title": "Video Summarization via Segments Summary Graphs",
    "authors": [
      "Mahmut Demir",
      "H. Isil Bozma"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/html/Demir_Video_Summarization_via_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/papers/Demir_Video_Summarization_via_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we propose a novel approach to video summarization that is based on the coherency analysis of segmented video frames as represented by region adjacency graphs. Similar segments across consecutive region adjacency graphs are matched and trackedusing an efficient graph matching technique. Shot boundaries are detected based on a coherency score that measures the appearances and disappearances of tracked segments. As such,it is possible to form a compact representation of each detected shot based onprevalent segmented regions and their relations - referred to as the'Segments summary graphs'. Furthermore, the segments summary graphis amenable for further semantic analysis and understanding of the scene. Experiments on benchmark datasets demonstrate that our method outperforms the state of the art summarization approaches."
  },
  "iccv2015_w1_towardslarge-scalefacerecognitionbasedonvideos": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Video Summarization for Large-Scale Analytics",
    "title": "Towards Large-Scale Face Recognition Based on Videos",
    "authors": [
      "Meltem Yalcin",
      "Hakan Cevikalp",
      "Hasan Serhan Yavuz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/html/Yalcin_Towards_Large-Scale_Face_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/papers/Yalcin_Towards_Large-Scale_Face_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper introduces a new method to find the most important samples for classification in image sets to speed-up the classification phase and reduce the storage space for large-scale face recognition tasks that use image sets obtained from face videos. We approximate the image sets with the kernelized convex hulls and show that it is sufficient to use only the samples that participate to shape the image set boundaries in this setting. To find those important samples that form the image set boundaries in the feature space, we employed the kernelized Support Vector Data Description (SVDD) method which finds a compact hypersphere that fits the image set samples best. Then, we show that these kernelized hypersphere models can also be used to model image sets for classification purposes. Lastly, we introduce ESOGU-285 (ESkisehir OsmanGazi University) Face Videos database that includes 285 people since the most popular video datasets used for set based recognition methods include either a few amount of people or large amount of people with just a few (or single) video collections. The experimental results on small sized standard datasets and our new larger sized dataset show that the proposed method greatly improves the testing times of the classification system (we obtained speed-ups up to a factor of 10 in ESOGU Face Videos dataset) without a significant drop in accuracies."
  },
  "iccv2015_w1_faststructurefrommotionforsequentialandwideareamotionimagery": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Video Summarization for Large-Scale Analytics",
    "title": "Fast Structure from Motion for Sequential and Wide Area Motion Imagery",
    "authors": [
      "Hadi AliAkbarpour",
      "Kannappan Palaniappan",
      "Guna Seetharaman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/html/AliAkbarpour_Fast_Structure_from_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w1/papers/AliAkbarpour_Fast_Structure_from_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a fast and efficient Structure-from-Motion (SfM) pipeline for refinement of camera parameters and 3D scene reconstruction given initial noisy camera metadata measurements. Examples include aerial Wide Area Motion Imagery (WAMI) which is typically acquired in a circular trajectory and other sequentially ordered multiview stereo imagery like Middlebury [??], Fountain [??] or body-worn videos [??]. Image frames are assumed (partially) ordered with approximate camera position and orientation information available from (imprecise) IMU and GPS sensors. In the proposed BA4S pipeline the noisy cameraparameters or poses are directly used in a fast Bundle Adjustment (BA) optimization. Since the sequential ordering of the cameras is known, consecutive frame-to-frame matching is used to find a set of feature correspondences for the triangulation step of SfM.These putative correspondences are directly used in the BA optimization without any early-stage filtering (i.e. no RANSAC) using a statistical robust error function based on co-visibility, to deal with outliers (mismatches), which significantly speeds up our SfM pipeline by more than 100 times compared to VisualSfM."
  },
  "iccv2015_w2_facialmicro-expressionrecognitionusingspatiotemporallocalbinarypatternwithintegralprojection": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Affective Computing",
    "title": "Facial Micro-Expression Recognition Using Spatiotemporal Local Binary Pattern With Integral Projection",
    "authors": [
      "Xiaohua Huang",
      "Su-Jing Wang",
      "Guoying Zhao",
      "Matti Piteikainen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/html/Huang_Facial_Micro-Expression_Recognition_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/papers/Huang_Facial_Micro-Expression_Recognition_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Recently, there are increasing interests in inferring mirco-expression from facial image sequences. For micro-expression recognition, feature extraction is an important critical issue. In this paper, we proposes a novel framework based on a new spatiotemporal facial representation to analyze micro-expressions with subtle facial movement. Firstly, an integral projection method based on difference images is utilized for obtaining horizontal and vertical projection, which can preserve the shape attributes of facial images and increase the discrimination for micro-expressions. Furthermore, we employ the local binary pattern operators to extract the appearance and motion features on horizontal and vertical projections. Intensive experiments are conducted on three available published micro-expression databases for evaluating the performance of the method. Experimental results demonstrate that the new spatiotemporal descriptor can achieve promising performance in micro-expression recognition."
  },
  "iccv2015_w2_facialactionunitdetectionusingactivelearningandanefficientnon-linearkernelapproximation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Affective Computing",
    "title": "Facial Action Unit Detection Using Active Learning and an Efficient Non-Linear Kernel Approximation",
    "authors": [
      "Thibaud Senechal",
      "Daniel McDuff",
      "Rana el Kaliouby"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/html/Senechal_Facial_Action_Unit_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/papers/Senechal_Facial_Action_Unit_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper presents large-scale naturalistic and spontaneous facial expression classification on uncontrolled webcam data. We describe an active learning approach that helped us efficiently acquire and hand-label hundreds of thousands of non-neutral spontaneous and natural expressions from thousands of different individuals. With the increased numbers of training samples a classic RBF SVM classifier, widely used in facial expression recognition, starts to become computationally limiting for training and real-time performance. We propose combining two techniques: 1) smart selection of a subset of the training data and 2) the Nystrom kernel approximation method to train a classifier that performs at high-speed (300fps). We compare performance (accuracy and classification time) with respect to the size of the training dataset and the SVM kernel, using either an RBF kernel, a linear kernel or the Nystrom approximation method. We present facial action unit classifiers that perform extremely well on spontaneous and naturalistic webcam videos from around the world recorded over the Internet.When evaluated on a large public dataset (AM-FED) our method performed better than the previously published baseline. Our approach generalizes to many problems that exhibit large individual variability. "
  },
  "iccv2015_w2_dodeepneuralnetworkslearnfacialactionunitswhendoingexpressionrecognition?": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Affective Computing",
    "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?",
    "authors": [
      "Pooya Khorrami",
      "Thomas Le Paine",
      "Thomas S. Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/html/Khorrami_Do_Deep_Neural_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/papers/Khorrami_Do_Deep_Neural_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements."
  },
  "iccv2015_w2_facecept3drealtime3dfacetrackingandanalysis": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Affective Computing",
    "title": "FaceCept3D: Real Time 3D Face Tracking and Analysis",
    "authors": [
      "Sergey Tulyakov",
      "Radu-Laurentiu Vieriu",
      "Enver Sangineto",
      "Nicu Sebe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/html/Tulyakov_FaceCept3D_Real_Time_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w2/papers/Tulyakov_FaceCept3D_Real_Time_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": "We present an open source cross platform technology for 3D face tracking and analysis. It contains a full stack of components for complete face understanding: detection, head pose tracking, facial expression and action units recognition. Given a depth sensor, one can combine FaceCept3D modules to fulfill a specific application scenario. Key advantages of the technology include real time processing speed and ability to handle extreme head pose variations. Possible application areas of the technology range from human computer interaction to active aging, where precise and real-time analysis is required. The technology is available to community."
  },
  "iccv2015_w3_metamermismatchinganditsconsequencesforpredictinghowcoloursareaffectedbytheilluminant": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Color and Photometry in Computer Vision",
    "title": "Metamer Mismatching and Its Consequences for Predicting How Colours Are Affected by the Illuminant",
    "authors": [
      "Xiandou Zhang",
      "Brian Funt",
      "Hamidreza Mirzaei"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/html/Zhang_Metamer_Mismatching_and_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/papers/Zhang_Metamer_Mismatching_and_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Metamer mismatching (the phenomena that two objects matching in color under one illuminant may not match under a different illuminant) potentially has important consequences for color-based machine vision.Logvinenko et al. [1] show that in theory the extent of metamer mismatching can be very significant. This paper examines metamer mismatching in practice by computing empirical metamer mismatch volumes. A set of more than 20 million unique reflectance spectra is assembled using datasets from several sources. For a given color signal (i.e., RGB or CIE XYZ) recorded under a given first illuminant, its empirical metamer mismatch volume for a change to a second illuminant is computed as follows: the reflectances having the same color signal when lit by the first illuminant (i.e., that are metamers) are computationally relit by the second illuminant and the convex hull of the resulting color signals then defines the empirical metamer mismatch volume. The volume of these volumes is shown to vary systematically with Munsell value and chroma. The centroid of the empirical metamer mismatch volume is also tested as a predictor of what a given color signal might become under a specified illuminant"
  },
  "iccv2015_w3_hdrrecoveryunderrollingshutterdistortions": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Color and Photometry in Computer Vision",
    "title": "HDR Recovery Under Rolling Shutter Distortions",
    "authors": [
      "Sheetal B. Gupta",
      "A. N. Rajagopalan",
      "Gunasekaran Seetharaman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/html/Gupta_HDR_Recovery_Under_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/papers/Gupta_HDR_Recovery_Under_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Preserving the high dynamic irradiance of a scene is essential for many computer vision algorithms. In this paper, we develop a technique for high dynamic range (HDR) reconstruction from differently exposed frames captured with CMOS cameras which use a rolling shutter (RS) to good effect for reducing power consumption. However, because these sensors are exposed to the scene row-wise, any unintentional handshake poses a challenge for HDR reconstruction since each row experiences a different motion. We account for this motion in the irradiance domain by picking the correct warp for each row within a predefined search space. The RS effect is rectified and a clean frame is propagated from one exposure to another until we obtain rectified irradiance corresponding to all the exposures. The rectified irradiances are finally fused to yield an HDR map that is free from RS distortions. "
  },
  "iccv2015_w3_ahybridstrategyforilluminantestimationtargetinghardimages": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Color and Photometry in Computer Vision",
    "title": "A Hybrid Strategy for Illuminant Estimation Targeting Hard Images",
    "authors": [
      "Roshanak Zakizadeh",
      "Michael S. Brown",
      "Graham D. Finlayson"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/html/Zakizadeh_A_Hybrid_Strategy_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/papers/Zakizadeh_A_Hybrid_Strategy_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Illumination estimation is a well-studied topic in computer vision. Early work reported performance on benchmark datasets using simple statistical aggregates such as mean or median error. Recently, it has become accepted to report a wider range of statistics, e.g. top 25%, mean, and bottom 25% performance. While these additional statistics are more informative, their relationship across different methods is unclear. In this paper, we analyse the results of a number of methods to see if there exists 'hard' images that are challenging for multiple methods. Our findings indicate that there are certain images that are difficult for fast statistical-based methods, but that can be handled with more complex learning-based approaches at a significant cost in time-complexity. This has led us to design a hybrid method that first classifies an image as 'hard' or 'easy' and then uses the slower method when needed, thus providing a balance between time-complexity and performance. In addition, we have identifieddataset images that almost no method is able to process. We argue, however, that these images have problems with how the ground truth is established and recommend their removal from future performance evaluation."
  },
  "iccv2015_w3_learningadeepconvolutionalnetworkforlight-fieldimagesuper-resolution": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Color and Photometry in Computer Vision",
    "title": "Learning a Deep Convolutional Network for Light-Field Image Super-Resolution",
    "authors": [
      "Youngjin Yoon",
      "Hae-Gon Jeon",
      "Donggeun Yoo",
      "Joon-Young Lee",
      "In So Kweon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/html/Yoon_Learning_a_Deep_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/papers/Yoon_Learning_a_Deep_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Commercial Light-Field cameras provide spatial and angular information, but its limited resolution becomes an important problem in practical use. In this paper, we present a novel method for Light-Field image super-resolution (SR) via a deep convolutional neural network. Rather than the conventional optimization framework, we adopt a datadriven learning method to simultaneously up-sample the angular resolution as well as the spatial resolution of a Light-Field image. We first augment the spatial resolution of each sub-aperture image to enhance details by a spatial SR network. Then, novel views between the sub-aperture images are generated by an angular super-resolution network. These networks are trained independently but finally finetuned via end-to-end training. The proposed method shows the state-of-the-art performance on HCI synthetic dataset, and is further evaluated by challenging real-world applications including refocusing and depth map estimation."
  },
  "iccv2015_w3_n-to-srgbmappingforsingle-sensormultispectralimaging": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Color and Photometry in Computer Vision",
    "title": "N-to-SRGB Mapping for Single-Sensor Multispectral Imaging",
    "authors": [
      "Yusuke Monno",
      "Masayuki Tanaka",
      "Masatoshi Okutomi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/html/Monno_N-to-SRGB_Mapping_for_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w3/papers/Monno_N-to-SRGB_Mapping_for_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Visualization of a multispectral image in a standard color space, typically the sRGB space, is an important task for human color perception. When we reproduce the sRGB image from the multispectral image with N spectral bands, an N-to-sRGB mapping is required. The challenge of the N-to-sRGB mapping in single-sensor multispectral imaging with a multispectral filter array (MSFA) is to reduce demosaicking error amplification and propagation across different spectral bands, which are not trivial because of very sparse sampling of multiple spectral bands in a single image sensor. In this paper, we propose a novel N-to-sRGB mapping pipeline for effectively suppressing the demosaicking error amplification and propagation. Our idea is to apply guided filtering in the mapped sRGB space using one of input N band images before the amplification and propagation as a guide image. Experimental results demonstrate that our proposed pipeline improves the mapping accuracy for various MSFA types."
  },
  "iccv2015_w6_beyondphoto-domainobjectrecognitionbenchmarksforthecross-depictionproblem": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Beyond Photo-Domain Object Recognition: Benchmarks for the Cross-Depiction Problem",
    "authors": [
      "Hongping Cai",
      "Qi Wu",
      "Peter Hall"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w6/html/Cai_Beyond_Photo-Domain_Object_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w6/papers/Cai_Beyond_Photo-Domain_Object_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The cross-depiction problem is that of recognising visual objects regardless of whether they are photographed, painted, drawn, etc. It introduces great challenge as the variance across photo and art domains is much larger than either alone. We extensively evaluate classification, domain adaptation and detection benchmarks for leading techniques, demonstrating that none perform consistently well given the cross-depiction problem. Finally we refine the DPM model, based on query expansion, enabling it to bridge the gap across depiction boundaries to some extent."
  },
  "iccv2015_w6_adapteddomainspecificclassmeans": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Adapted Domain Specific Class Means",
    "authors": [
      "Gabriela  Csurka",
      "Boris Chidlovskii",
      "Stephane Clinchant"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w6/html/Csurka_Adapted_Domain_Specific_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w6/papers/Csurka_Adapted_Domain_Specific_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We address the problem of domain adaptation (DA) from one or multiple source domains to a target domain. Most of the existing DA methods assume that source data is largely available. Such an assumption rarely holds in real applications, for both technical and legal reasons.More realistic are situations where source domain observations become quickly unavailable, but only some domain representatives can be retained, either as source instances or as their aggregation. In this paper therefore we focus on the Domain Specific Class Means (DSCM) classifier that can handle such scenarioand we combine it with Stacked Marginalized Denoising Autoencoders. We propose a method that exploits the correlation between the target data and source prototypes without the need of targetlabels andautomatically adapts these class means to the target dataset.We show, on a variety of datasets and tasks, that the method can be applied successfully even when no labeled target isavailableand also that it can provideperformance comparable to the case where dense knowledge(all source data) is available. "
  },
  "iccv2015_w6_anatomicallandmarkdetectioninmedicalapplicationsdrivenbysyntheticdata": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Anatomical Landmark Detection in Medical Applications Driven by Synthetic Data",
    "authors": [
      "Gernot Riegler",
      "Martin Urschler",
      "Matthias Ruther",
      "Horst Bischof",
      "Darko Stern"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w6/html/Riegler_Anatomical_Landmark_Detection_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w6/papers/Riegler_Anatomical_Landmark_Detection_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " An important initial step in many medical image analysis applications is the accurate detection of anatomical landmarks.Most successful methods for this task rely on data-driven machine learning algorithms. However, modern machine learning techniques, e.g. convolutional neural networks, need a large corpus of training data, which is often an unrealistic setting for medical datasets. In this work, we investigate how to adapt synthetic image datasets from other computer vision tasks to overcome the under-representation of the anatomical pose and shape variations in medical image datasets. We transform both data domains to a common one in such a way that a convolutional neural network can be trained on the larger synthetic image dataset and fine-tuned on the smaller medical image dataset. Our evaluations on data of MR hand and whole body CT images demonstrate that this approach improves the detection results compared to training a convolutional neural network only on the medical data. The proposed approach may also be usable in other medical applications, where training data is scarce."
  },
  "iccv2015_w8_positioninterpolationusingfeaturepointscalefordecimetervisuallocalization": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Position Interpolation Using Feature Point Scale for Decimeter Visual Localization",
    "authors": [
      "David Wong",
      "Daisuke Deguchi",
      "Ichiro Ide",
      "Hiroshi Murase"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/html/Wong_Position_Interpolation_Using_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/papers/Wong_Position_Interpolation_Using_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Vehicle ego-localization is a critical task not only for in-car navigation systems, but also for emerging intelligent and autonomous vehicle technologies. Visual localization methods that determine current location by performing image matching against a pre-constructed database have an accuracy limited by the spatial distance between database images. In this paper we propose a method that uses the scale of feature points to interpolate the position of the query image between two database images. We show how this simple contribution offers an appreciable improvement in localization accuracy with an extremely minimal increase in processing time, especially when used in conjunction with image matching methods that already monitor feature scale. Our experiments showed an increase of up to 33% in average localization accuracy when compared to a method without any interpolation."
  },
  "iccv2015_w8_directvisuallocalisationandcalibrationforroadvehiclesinchangingcityenvironments": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Direct Visual Localisation and Calibration for Road Vehicles in Changing City Environments",
    "authors": [
      "Geoffrey Pascoe",
      "William Maddern",
      "Paul Newman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/html/Pascoe_Direct_Visual_Localisation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/papers/Pascoe_Direct_Visual_Localisation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper presents a large-scale evaluation of a visual localisation method in a challenging city environment. Our system makes use of a map built by combining data from LIDAR and cameras mounted on a survey vehicle to build a dense appearance prior of the environment. We then localise by minimising the normalised information distance (NID) between a live camera image and an image generated from our prior. The use of NID produces a localiser that is robust to significant changes in scene appearance. Furthermore, NID can be used to compare images across different modalities, allowing us to use the same system to determine the extrinsic calibration between LIDAR and camera on the survey vehicle. We evaluate our system with a large-scale experiment consisting of over 450,000 camera frames collected over 110km of driving over a period of six months, and demonstrate reliable localisation even in the presence of illumination change, snow and seasonal effects."
  },
  "iccv2015_w8_thestatisticsofdrivingsequences--andwhatwecanlearnfromthem": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "The Statistics of Driving Sequences -- And What We Can Learn From Them",
    "authors": [
      "Henry Bradler",
      "Birthe Anne Wiegand",
      "Rudolf Mester"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/html/Bradler_The_Statistics_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/papers/Bradler_The_Statistics_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The motion of a driving car is highly constrained and we claim that powerful predictors can be built that 'learn' the typical egomotion statistics, and support the typical tasks of feature matching, tracking, and egomotion estimation. We analyze the statistics of the 'ground truth' data given in the KITTI odometry benchmark sequences and confirm that a coordinated turn motion model, overlaid by moderate vibrations, is a very realistic model. We develop a predictor that is able to significantly reduce the uncertainty about the relative motion when a new image frame comes in. Such predictors can be used to steer the matching process from frame n to frame n + 1. We show that they can also be employed to detect outliers in the temporal sequence of egomotion parameters. "
  },
  "iccv2015_w8_latenthierarchicalpartbasedmodelsforroadsceneunderstanding": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Latent Hierarchical Part Based Models for Road Scene Understanding",
    "authors": [
      "Suhas Kashetty Venkateshkumar",
      "Muralikrishna Sridhar",
      "Patrick Ott"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/html/Venkateshkumar_Latent_Hierarchical_Part_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/papers/Venkateshkumar_Latent_Hierarchical_Part_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Road scenes can be naturally interpreted in terms of a hierarchical structure consisting of parts and sub-parts,which captures different degrees of abstraction at different levels of the hierarchy. We introduce Latent Hierarchical Part based Models (LHPMs), which provide a promising framework for interpreting an image using a tree structure, in the case when the root filter for non-leaf nodes may not be available. While HPMs have been developed in the context of object detection and pose estimation, their application to scene understanding is restricted, due to the requirement of having root filters for non-leaf nodes. In this work, we propose a generalization of HPMs that dispenses with the need for having root filters for non-leaf nodes, by treating them as latent variables within a Dynamic Programming based optimization scheme. We experimentally demonstrate the importance of LHPMs for road scene understanding on Continental and KITTI datasets respectively. We find that the hierarchicalinterpretation leads to intuitive scene descriptions, that is central for autonomous driving."
  },
  "iccv2015_w8_semanticmappingoflarge-scaleoutdoorscenesforautonomousoff-roaddriving": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Semantic Mapping of Large-Scale Outdoor Scenes for Autonomous Off-Road Driving",
    "authors": [
      "Fernando Bernuy",
      "Javier Ruiz del Solar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/html/Bernuy_Semantic_Mapping_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/papers/Bernuy_Semantic_Mapping_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Semantic mapping is a very active and growing research area, with important applications in indoor and outdoor robotic applications. However, most of the research on semantic mapping has focused on indoor mapping and there is a need for developing semantic mapping methodologies for large-scale outdoor scenarios. In this work, a novel semantic mapping methodology for large-scale outdoor scenes in autonomous off-road driving applications is proposed. The semantic map representation consists of a large-scale topological map built using semantic image information. Thus, the proposed representation aims to solve the large-scale outdoors semantic mapping problem by using a graph based topological map, where relevant information for autonomous driving is added using semantic information from the image description. As a proof of concept, the proposed methodology is applied to the semantic map building of a real outdoor scenario."
  },
  "iccv2015_w8_sequentialscoreadaptationwithextremevaluetheoryforrobustrailwaytrackinspection": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Sequential Score Adaptation With Extreme Value Theory for Robust Railway Track Inspection",
    "authors": [
      "Xavier Gibert",
      "Vishal M. Patel",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/html/Gibert_Sequential_Score_Adaptation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/papers/Gibert_Sequential_Score_Adaptation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Periodic inspections are necessary to keep railroad tracks in state of good repair and prevent train accidents. Automatic track inspection using machine vision technology has become a very effective inspection tool. Because of its non-contact nature, this technology can be deployed on virtually any railway vehicle to continuously survey the tracks and send exception reports to track maintenance personnel. However, as appearance and imaging conditions vary, false alarm rates can dramatically change, making it difficult to select a good operating point. In this paper, we use extreme value theory (EVT) within a Bayesian framework to optimally adjust the sensitivity of anomaly detectors. We show that by approximating the lower tail of the probability density function (PDF) of the scores with an Exponential distribution (a special case of the Generalized Pareto distribution), and using the Gamma conjugate prior learned from the training data, it is possible to reduce the variability in false alarm rate and improve the overall performance. This method has shown an increase in the defect detection rate of rail fasteners in the presence of clutter (at PFA 0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013 concrete tie dataset."
  },
  "iccv2015_w8_goal-directedpedestrianprediction": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Goal-Directed Pedestrian Prediction",
    "authors": [
      "Eike Rehder",
      "Horst Kloeden"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/html/Rehder_Goal-Directed_Pedestrian_Prediction_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w8/papers/Rehder_Goal-Directed_Pedestrian_Prediction_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Recent advances in road safety have lead to a constant decline of injured traffic participants in Europe per year. Still, the number of injured pedestrians remains nearly constant. As a countermeasure, active pedestrian safety is the focus of current research, for which accurate pedestrian prediction is a prerequisite. In this scope, we propose a method for dynamics- and environment-based pedestrian prediction. We introduce the pedestrian's destination as a latent variable and thus convert the prediction problem into a planning problem. The planning is executed based on the current dynamics of the pedestrian. The distribution over the destinations is modeled using a Particle Filter. Experimental results show a significant improvement over existing approaches such as Kalman Filters. "
  },
  "iccv2015_w9_fusionofinertialandvisualmeasurementsforrgb-dslamonmobiledevices": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Fusion of Inertial and Visual Measurements for RGB-D SLAM on Mobile Devices",
    "authors": [
      "Nicholas Brunetto",
      "Samuele Salti",
      "Nicola Fioraio",
      "Tommaso Cavallari",
      "Luigi Di Stefano"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/html/Brunetto_Fusion_of_Inertial_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/papers/Brunetto_Fusion_of_Inertial_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Simultaneous Localization and Mapping (SLAM) algorithms have been recently deployed on mobile devices, where they can enable a broad range of novel applications. Nevertheless, pure visual SLAM is inherently weak at operating in environments with a reduced number of visual features. Indeed, even many recent proposals based on RGB-D sensors cannot handle properly such scenarios, as several steps of the algorithms are based on matching visual features. In this work we propose a framework suitable for mobile platforms to fuse pose estimations attained from visual and inertial measurements, with the aim of extending the range of scenarios addressable by mobile visual SLAM. The framework deploys an array of Kalman filters where the careful selection of the state variables and the preprocessing of the inertial sensor measurements result in a simple and effective data fusion process. We present qualitative and quantitative experiments to show the improved SLAM performance delivered by the proposed approach."
  },
  "iccv2015_w9_incrementaldivisionofverylargepointcloudsforscalable3dsurfacereconstruction": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Incremental Division of Very Large Point Clouds for Scalable 3D Surface Reconstruction",
    "authors": [
      "Andreas Kuhn",
      "Helmut Mayer"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/html/Kuhn_Incremental_Division_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/papers/Kuhn_Incremental_Division_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The recent progress in Structure from Motion and Multi-View Stereo as well as the always rising number of high resolution images lead to ever larger 3D point clouds. Unfortunately, due to the large amount of memory and processing power needed, there are no suitable means for manipulating these massive amounts of data as a whole, such as fusion by 3D surface reconstruction methods. In this paper we, therefore, present an algorithm for division of very large 3D point clouds into smaller subsets allowing for a parallel 3D reconstruction of many suitably small parts. Within our space division algorithm, octrees are built representing the divided space. To limit the maximum size of the underlying data structure, we present an incremental extension of the algorithm which renders a division of billions of 3D points possible and speeds up the processing on multi-core systems. As the proposed space division does not guarantee a density-based decomposition, we show the limitations of kd-trees as an alternative data structure. Space division is especially important for volumetric 3D reconstruction, as the latter has a high memory requirement. To this end, we finally discuss the adaptability of the space division to existing surface reconstruction methods to achieve scalable 3D reconstruction and show examples on existing and novel datasets which demonstrate the potential of the incremental space division algorithm. "
  },
  "iccv2015_w9_videoeventrecognitionbycombininghdpandgaussianprocess": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Video Event Recognition by Combining HDP and Gaussian Process",
    "authors": [
      "Wentong Liao",
      "Bodo Rosenhahn",
      "Machael Ying Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/html/Liao_Video_Event_Recognition_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/papers/Liao_Video_Event_Recognition_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we present a framework for automatically analyzing activities and interactions, and recognizing traffic states from surveillance video.Activities and interactions are firstly learned by Hierarchical Dirichlet Process (HDP) models based on low-level visual features.Based on the learning results, a Gaussian Process (GP) classifier is trained to classify the traffic states in online video.Furthermore, the temporal dependencies between video events learned by HDP-Hidden Markov Models (HMM) are effectively integrated into GP classifier to enhance the accuracy of the classification.Our framework couples the benefits of the generative models-HDP with the discriminant models-GP.We validate the proposed model by applying it to the analysis of the three standard video datasets over crowded traffic scenes and compare it with other baseline models.Experimental results demonstrate that our model is effective and efficient."
  },
  "iccv2015_w9_surfacerecoveryfusionofimageandpointcloud": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Surface Recovery: Fusion of Image and Point Cloud",
    "authors": [
      "Siavash Hosseinyalamdary",
      "Alper Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/html/Hosseinyalamdary_Surface_Recovery_Fusion_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/papers/Hosseinyalamdary_Surface_Recovery_Fusion_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The point cloud of the laser scanner is a rich source of information for high level tasks in computer vision such as traffic understanding. However, cost-effective laser scanners provide noisy and low resolution point cloud and they are prone to systematic errors. In this paper, we propose two surface recovery approaches based on geometry and brightness of the surface. The proposed approaches are tested in the realistic outdoor scenarios and the results show that both approaches have superior performance over the-state-of-art methods."
  },
  "iccv2015_w9_amulti-viewpedestriantrackingmethodinanuncalibratedcameranetwork": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "A Multi-View Pedestrian Tracking Method in an Uncalibrated Camera Network",
    "authors": [
      "Domonkos Varga",
      "Tamas Sziranyi",
      "Attila Kiss",
      "Laszlo Sporas",
      "Laszlo Havasi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/html/Varga_A_Multi-View_Pedestrian_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/papers/Varga_A_Multi-View_Pedestrian_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Combining multiple observation views has proven beneficial for pedestrian tracking. In this paper, we present a methodology for tracking pedestrians in an uncalibrated multi-view camera network. Using a set of color and infrared cameras, we can accurately tracking pedestrians for a general scene configuration.We design an algorithmic framework that can be generalized to an arbitrary number of cameras. A novel pedestrian detection algorithm based on Center-symmetric Local Binary Patterns is integrated into the proposed system. In our experiments the common field of view of two neighboring cameras was about 30%. The system improves upon existing systems in the following ways: (1) The system registers partially overlapping camera-views automatically and does not require any manual input. (2) The system reaches the state-of-the-art performance when the common field of view of any two cameras is low and successfully integrates optical and infrared cameras. Our experiments also demonstrate that the proposed architecture is able to provide robust, real-time input to a video surveillance system. Our system was tested in a multi-view, outdoor environment with uncalibrated cameras. "
  },
  "iccv2015_w9_amodifiedsequentialmontecarlobayesianoccupancyfilterusinglinearopinionpoolforgridmapping": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "A Modified Sequential Monte Carlo Bayesian Occupancy Filter Using Linear Opinion Pool for Grid Mapping",
    "authors": [
      "Sang-Il Oh",
      "Hang-Bong Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/html/Oh_A_Modified_Sequential_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w9/papers/Oh_A_Modified_Sequential_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Occupancy grid state mapping is a key process in robotics and autonomous driving systems. It divides the environment into grid cells that contain information states. In this paper, we propose a modified SMC-BOF method to map and predict occupancy grids. The original SMC-BOF has been widely used in the occupancy grid mapping because it has lower computational costs than the BOF method. However, there are some issues related to conflicting information in dynamic situations. The original SMC-BOF cannot completely control an elongated vehicle that has conflicting information caused by the height difference between backward of vehicle and ground. To overcome this problem, we add confidence weights onto a part of the grid mapping process of the original SMC-BOF using the Linear Opinion Pool. We evaluate our method by LIDAR and stereo vision data in the KITTI dataset."
  },
  "iccv2015_w10_jointestimationofdepth,reflectanceandilluminationfordepthrefinement": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Inverse Rendering",
    "title": "Joint Estimation of Depth, Reflectance and Illumination for Depth Refinement",
    "authors": [
      "Kichang Kim",
      "Akihiko Torii",
      "Masatoshi Okutomi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/html/Kim_Joint_Estimation_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/papers/Kim_Joint_Estimation_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we propose a method for joint estimation of depth, reflectance and illumination from a single RGB-D image for depth refinement.This is achieved by a simple optimization based approach with smoothness constraints on depth, reflectance and illumination. We introduce an adaptively weighted local similarity constraint for reflectance, a normalized spherical-harmonic model for illumination, and an edge-aware local smoothness constraint for depth.This allows us to generate high quality depth without additional processes such as pre-training of stochastic models or image segmentation.Experimental results demonstrate that our method estimates high quality depth in comparison with ground-truth data not only for laboratory conditions but also for complex real-world scenes."
  },
  "iccv2015_w10_bilayerblinddeconvolutionwiththelightfieldcamera": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Inverse Rendering",
    "title": "Bilayer Blind Deconvolution With the Light Field Camera",
    "authors": [
      "Meiguang Jin",
      "Paramanand Chandramouli",
      "Paolo Favaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/html/Jin_Bilayer_Blind_Deconvolution_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/papers/Jin_Bilayer_Blind_Deconvolution_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we propose a solution to blind deconvolution of a scene with two layers (foreground/background). We show that the reconstruction of the support of these two layers from a single image of a conventional camera is not possible. As a solution we propose to use a light field camera. We demonstrate that a single light field image captured with a Lytro camera can be successfully deblurred. More specifically, we consider the case of space-varying motion blur, where the blur magnitude depends on the depth changes in the scene. Our method employs a layered model that handles occlusions and partial transparencies due to both motion blur and out of focus blur of the plenoptic camera. We reconstruct each layer support, the corresponding sharp textures, and motion blurs via an optimization scheme. The performance of our algorithm is demonstrated on synthetic as well as real light field images."
  },
  "iccv2015_w10_multi-shotdeblurringfor3dscenes": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Inverse Rendering",
    "title": "Multi-Shot Deblurring for 3D Scenes",
    "authors": [
      "M. Arun",
      "A. N. Rajagopalan",
      "Gunasekaran Seetharaman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/html/Arun_Multi-Shot_Deblurring_for_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/papers/Arun_Multi-Shot_Deblurring_for_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The presence of motion blur is unavoidable in hand-held cameras, especially in low-light conditions. In this paper, we address the inverse rendering problem of estimating the latent image, scene depth and camera motion from a set of differently blurred images of the scene. Our framework can account for depth variations, non-uniform motion blur as well as mis-alignments in the captured observations. We initially describe an iterative algorithm to estimate ego motion in 3D scenes by suitably harnessing the point spread functions across the blurred images at different spatial locations. This is followed by recovery of latent image and scene depth by alternate minimization."
  },
  "iccv2015_w10_efficientandrobustinverselightingofasinglefaceimageusingcompressivesensing": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Inverse Rendering",
    "title": "Efficient and Robust Inverse Lighting of a Single Face Image Using Compressive Sensing",
    "authors": [
      "Miguel Heredia Conde",
      "Davoud Shahlaei",
      "Volker Blanz",
      "Otmar Loffeld"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/html/Conde_Efficient_and_Robust_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/papers/Conde_Efficient_and_Robust_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we show that the recent theory of Compressive Sensing (CS) can successfully be applied to solve a model-based inverse lighting problem for single face images, even in harsh lighting with multiple light sources, including cast shadows and specularities. It has been shown that an illumination cone can be used to perform realistic inverse lighting. In this work, the cone images are synthetically generated using directional lights and a realistic reflectance of faces. Thereby, the face model is achieved by fitting a 3D Morphable Model to the input image. We apply CS to find the sparsest illumination setup from few random measurements of the RGB input and the cone images. The proposed method significantly reduces the dimensionality through stochastic sampling and a greedy algorithm for the sparse support estimation, yielding low runtimes. The greedy search is designed to handle non-negativity of the light sources and joint-support selection. We show that the proposed method reaches a quality of illumination estimation equal to previous work, while dramatically reducing the number of active light sources. Thorough experimental evaluation shows that stable recovery is achievable for compression rates up to 99%. The method exhibits outstanding robustness to additive noise in the input image."
  },
  "iccv2015_w10_sceneintrinsicsanddepthfromasingleimage": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Inverse Rendering",
    "title": "Scene Intrinsics and Depth From a Single Image",
    "authors": [
      "Evan Shelhamer",
      "Jonathan T. Barron",
      "Trevor Darrell"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/html/Shelhamer_Scene_Intrinsics_and_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w10/papers/Shelhamer_Scene_Intrinsics_and_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Intrinsic image decomposition factorizes an observed image into its physical causes. This is most commonly framed as a decomposition into reflectance and shading, although recent progress has made full decompositions into shape, illumination, reflectance, and shading possible. However, existing factorization approaches require depth sensing to initialize the optimization of scene intrinsics. Rather than relying on depth sensors, we show that depth estimated purely from monocular appearance can provide sufficient cues for intrinsic image analysis. Our full intrinsic pipeline regresses depth by a fully convolutional network then jointly optimizes the intrinsic factorization to recover the input image. This combination yields full decompositions by uniting feature learning through deep network regression with physical modeling through statistical priors and random field regularization. This work demonstrates the first pipeline for full intrinsic decomposition of scenes from a single color image input alone."
  },
  "iccv2015_w11_chalearnlookingatpeople2015apparentageandculturaleventrecognitiondatasetsandresults": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "ChaLearn Looking at People 2015: Apparent Age and Cultural Event Recognition Datasets and Results",
    "authors": [
      "Sergio Escalera",
      "Junior Fabian",
      "Pablo Pardo",
      "Xavier Baro",
      "Jordi Gonzalez",
      "Hugo J. Escalante",
      "Dusan Misevic",
      "Ulrich Steiner",
      "Isabelle Guyon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Escalera_ChaLearn_Looking_at_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Escalera_ChaLearn_Looking_at_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Following previous series on Looking at People (LAP) competitions, in 2015 ChaLearn ran two new competitions within the field of Looking at People: age and cultural event recognition in still images. We proposed a crowd-sourcing application to collect and label data about the age people looks like instead of the real age. In terms of cultural event recognition, one hundred categories had to be recognized. This involved scene understanding and human body analysis. This paper summarizes both challenges and data, as well as the results achieved by the participants of the competition. Details of the ChaLearn LAP competitions can be found at http://gesture.chalearn.org/"
  },
  "iccv2015_w11_dexdeepexpectationofapparentagefromasingleimage": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "DEX: Deep EXpectation of Apparent Age From a Single Image",
    "authors": [
      "Rasmus Rothe",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Rothe_DEX_Deep_EXpectation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Rothe_DEX_Deep_EXpectation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we tackle the estimation of apparent age in still face images with deep learning. Our convolutional neural networks (CNNs) use the VGG-16 architecture and are pretrained on ImageNet for image classification. In addition, due to the limited number of apparent age annotated images, we explore the benefit of finetuning over crawled Internet face images with available age. We crawled 0.5 million images of celebrities from IMDB and Wikipedia that we make public. This is the largest public dataset for age prediction to date. We pose the age regression problem as a deep classification problem followed by a softmax expected value refinement and show improvements over direct regression training of CNNs. Our proposed method, Deep EXpectation (DEX) of apparent age, first detects the face in the test image and then extracts the CNN predictions from an ensemble of 20 networks on the cropped face.The CNNs of DEX were finetuned on the crawled images and then on the provided images with apparent age annotations. DEX does not use explicit facial landmarks. Our DEX is the winner (1st place) of the ChaLearn LAP 2015 challenge on apparent age estimation with 115 registered teams, significantly outperforming the human reference."
  },
  "iccv2015_w11_agenetdeeplylearnedregressorandclassifierforrobustapparentageestimation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "AgeNet: Deeply Learned Regressor and Classifier for Robust Apparent Age Estimation",
    "authors": [
      "Xin Liu",
      "Shaoxin Li",
      "Meina Kan",
      "Jie Zhang",
      "Shuzhe Wu",
      "Wenxian Liu",
      "Hu Han",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Liu_AgeNet_Deeply_Learned_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Liu_AgeNet_Deeply_Learned_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Apparent age estimation from face image has attracted more and more attentions as it is favorable in some real-world applications. In this work, we propose an end-to-end learning approach for robust apparent age estimation, named by us AgeNet. Specifically, we address the apparent age estimation problem by fusing two kinds of models, i.e., real-value based regression models and Gaussian label distribution based classification models. For both kind of models, large-scale deep convolutional neural network is adopted to learn informative age representations. Another key feature of the proposed AgeNet is that, to avoid the problem of over-fitting on small apparent age training set, we exploit a general-to-specific transfer learning scheme. Technically, the AgeNet is first pre-trained on a large-scale web-collected face dataset with identity label, and then it is fine-tuned on a large-scale real age dataset with noisy age label. Finally, it is fine-tuned on a small training set with apparent age label. The experimental results on the ChaLearn 2015 Apparent Age Competition demonstrate that our AgeNet achieves the state-of-the-art performance in apparent age estimation."
  },
  "iccv2015_w11_astudyonapparentageestimation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "A Study on Apparent Age Estimation",
    "authors": [
      "Yu Zhu",
      "Yan Li",
      "Guowang Mu",
      "Guodong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Zhu_A_Study_on_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Zhu_A_Study_on_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Age estimation from facial images is an important problem in computer vision and pattern recognition. Typically the goal is to predict the chronological age of a person given his or her face picture. It is seldom to study a related problem, that is, how old does a person look like from the face photo? It is called apparent age estimation. A key difference between apparent age estimation and the traditional age estimation is that the age labels are annotated by human assessors rather than the real chronological age. The challenge for apparent age estimation is that there are not many face images available with annotated age labels. Further, the annotated age labels for each face photo may not be consistent among different assessors. We study the problem of apparent age estimation by addressing the issues from different aspects, such as how to utilize a large number of face images without apparent age labels to learn a face representation using the deep neural networks, how to tune the deep networks using a limited number of examples with apparent age labels, and how well the machine learning methods can perform to estimate apparent ages. The apparent age data is from the ChaLearn Looking At People (LAP) challenge 2015. Using the protocol and time frame given by the challenge competition, we have achieved an error of 0.294835 on the final evaluation, and our result has been ranked the 3rd place in this competition."
  },
  "iccv2015_w11_exploitingfeaturehierarchieswithconvolutionalneuralnetworksforculturaleventrecognition": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Exploiting Feature Hierarchies With Convolutional Neural Networks for Cultural Event Recognition",
    "authors": [
      "Mengyi Liu",
      "Xin Liu",
      "Yan Li",
      "Xilin Chen",
      "Alexander G. Hauptmann",
      "Shiguang Shan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Liu_Exploiting_Feature_Hierarchies_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Liu_Exploiting_Feature_Hierarchies_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Cultural events are kinds of typical events closely related to history and nationality, which play an important role in cultural heritage through generations. However, automatically recognizing cultural events still remains a great challenge since it depends on understanding of complex image contents such as people, objects, and scene context. Therefore, it is intuitive to associate this task with other high-level vision problems, e.g., object detection, recognition, and scene understanding. In this paper, we address this problem by combining both ideas of object / scene contents mining and strong image representation via CNN into a whole framework. Specifically, for object / scene contents mining, we employ selective search to extract a batch of bottom-up region proposals, which are served as key object / scene candidates in each event image; while for representation via CNN, we investigate two state-of-the-art deep architectures, VGGNet and GoogLeNet, and adapt them to our task by performing domain-specific (i.e., event) fine-tuning on both global image and hierarchical region proposals. These two models can complementarily exploit feature hierarchies spatially, which simultaneously capture the global context and local evidences within the image. In our final submission for ChaLearn LAP Challenge ICCV 2015, nine kinds of features extracted from five different deep models were exploited and followed with two kinds of classifiers for decision level fusion. Our method achieves the best performance of mAP=0.854 among all the participants in the track of cultural event recognition."
  },
  "iccv2015_w11_deepspatialpyramidensembleforculturaleventrecognition": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Deep Spatial Pyramid Ensemble for Cultural Event Recognition",
    "authors": [
      "Xiu-Shen Wei",
      "Bin-Bin Gao",
      "Jianxin Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Wei_Deep_Spatial_Pyramid_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Wei_Deep_Spatial_Pyramid_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Semantic event recognition based only on image-based cues is a challenging problem in computer vision. In order to capture rich information and exploit important cues like human poses, human garments and scene categories, we propose the Deep Spatial Pyramid Ensemble framework, which is mainly based on our previous work, i.e., Deep Spatial Pyramid (DSP). DSP could build universal and powerful image representations from CNN models. Specifically, we employ five deep networks trained on different data sources to extract five corresponding DSP representations for event recognition images. For combining the complementary information from different DSP representations, we ensemble these features by both \"early fusion\" and \"late fusion\". Finally, based on the proposed framework, we come up with a solution for the track of the Cultural Event Recognition competition at the ChaLearn Looking at People (LAP) challenge in association with ICCV 2015. Our framework achieved one of the best cultural event recognition performance in this challenge."
  },
  "iccv2015_w11_betterexploitingos-cnnsforbettereventrecognitioninimages": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Better Exploiting OS-CNNs for Better Event Recognition in Images",
    "authors": [
      "Limin Wang",
      "Zhe Wang",
      "Sheng Guo",
      "Yu Qiao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Wang_Better_Exploiting_OS-CNNs_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Wang_Better_Exploiting_OS-CNNs_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Event recognition from still images is one of the most important problems for image understanding. However, compared with object recognition and scene recognition, event recognition has received much less research attention in computer vision community. This paper addresses the problem of cultural event recognition in still images and focuses on applying deep learning methods on this problem. In particular, we utilize the successful architecture of -phObject-Scene Convolutional Neural Networks (OS-CNNs) to perform event recognition. OS-CNNs are composed of object nets and scene nets, which transfer the learned representations from the pre-trained models on large-scale object and scene recognition datasets, respectively. We propose four types of scenarios to explore OS-CNNs for event recognition by treating them as either ``end-to-end event predictors'' or ``generic feature extractors''. Our experimental results demonstrate that the global and local representations of OS-CNNs are complementary to each other. Finally, based on our investigation of OS-CNNs, we come up with a solution for the cultural event recognition track at the ICCV ChaLearn Looking at People (LAP) challenge 2015. Our team secures the third place at this challenge and our result is very close to the best performance."
  },
  "iccv2015_w11_dldrdeeplineardiscriminativeretrievalforculturaleventclassificationfromasingleimage": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "DLDR: Deep Linear Discriminative Retrieval for Cultural Event Classification From a Single Image",
    "authors": [
      "Rasmus Rothe",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Rothe_DLDR_Deep_Linear_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Rothe_DLDR_Deep_Linear_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we tackle the classification of cultural events from a single image with a deep learning based method. We use convolutional neural networks (CNNs) with VGG-16 architecture, pretrained on ImageNet or the Places205 dataset for image classification, and fine-tuned on cultural events data. CNN features are robustly extracted at 4 different layers in each image. At each layer Linear Discriminant Analysis (LDA) is employed for discriminative dimensionality reduction. An image is represented by the concatenated LDA-projected features from all layers or by the concatenation of CNN pooled features at each layer. The classification is then performed through the Iterative Nearest Neighbors-based Classifier (INNC). Classification scores are obtained for different image representation setups at train and test. The average of the scores is the output of our deep linear discriminative retrieval (DLDR) system. With 0.80 mean average precision (mAP) DLDR is a top entry for the ChaLearn LAP 2015 cultural event recognition challenge. "
  },
  "iccv2015_w11_movingposeletsadiscriminativeandinterpretableskeletalmotionrepresentationforactionrecognition": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Moving Poselets: A Discriminative and Interpretable Skeletal Motion Representation for Action Recognition",
    "authors": [
      "Lingling Tao",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Tao_Moving_Poselets_A_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Tao_Moving_Poselets_A_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Given a video or time series of skeleton data, action recognition systems perform classification using cues such as motion, appearance, and pose.For the past decade, actions have been modeled using low-level feature representations such as Bag of Features. More recent work has shown that mid-level representations that model body part movements (e.g., hand moving forward) can be very effective.However, these mid-level features are usually hand-crafted and the dictionary of representative features is learned using ad-hoc heuristics. While automatic feature learning methods such as supervised sparse dictionary learning or neural networks can be applied to learn feature representation and action classifiers jointly, the resulting features are usually uninterpretable. In contrast, our goal is to develop a principled feature learning framework to learn discriminative and interpretable skeletal motion patterns for action recognition. For this purpose, we propose a novel body-part motion based feature called Moving Poselet, which corresponds to a specific body part configuration undergoing a specific movement. We also propose a simple algorithm for jointly learning Moving Poselets and action classifiers. Experiments on MSR Action3D, MSR DailyActivity3D and Berkeley MHAD datasets show that our two-layer model outperforms other two-layer models using hand-crafted features, and achieves results comparable to those of recent multi-layer Hierarchical Recurrent Neural Network (HRNN) models, which use multiple layers of RNN to model the human body hierarchy."
  },
  "iccv2015_w11_skeleton-freebodyposeestimationfromdepthimagesformovementanalysis": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Skeleton-Free Body Pose Estimation From Depth Images for Movement Analysis",
    "authors": [
      "Ben Crabbe",
      "Adeline Paiement",
      "Sion Hannuna",
      "Majid Mirmehdi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Crabbe_Skeleton-Free_Body_Pose_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Crabbe_Skeleton-Free_Body_Pose_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In movement analysis frameworks, body pose may often be adequately represented in a simple, low-dimensional, and high-level space, while full body joints' locations constitute excessively redundant and complex information. We propose a method for estimating body pose in such high-level pose spaces, directly from a depth image and without relying on intermediate skeleton-based steps. Our method is based on a convolutional neural network (CNN) that maps the depth-silhouette of a person to its position in the pose space. We apply our method to a pose representation proposed in [16] that was initially built from skeleton data. We find our estimation of pose to be consistent with the original one, and to be suitable for use in the movement quality assessment framework of [16]. This opens the perspective of a wider applicability of the movement analysis method to movement types and view-angles that are not supported by its skeleton tracking algorithm."
  },
  "iccv2015_w11_motionrecognitionemployingmultiplekernellearningoffishervectorsusinglocalskeletonfeatures": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Motion Recognition Employing Multiple Kernel Learning of Fisher Vectors Using Local Skeleton Features",
    "authors": [
      "Yusuke Goutsu",
      "Wataru Takano",
      "Yoshihiko Nakamura"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Goutsu_Motion_Recognition_Employing_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Goutsu_Motion_Recognition_Employing_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We propose a skeleton-based motion recognition system focusing on local parts of the human body closely related to a target motion. In this system, a skeleton feature is com- posed of a sequence of relative positions between paired joints calculated by Inverse Kinematics. Several joints of skeleton model are connected as a Local Skeleton Feature. The temporal sequence is modeled as human motion model by using Hidden Markov Model. Motion features are rep- resented as Fisher vectors parameterized by the human mo- tion models, and weighted and integrated by using Multiple Kernel Learning. This system makes it possible for robots to recognize human actions in our daily life. The experimental results based on two datasets show an improvement in per- formance of classification rate, which shows that the design of motion feature is effective for motion recognition."
  },
  "iccv2015_w11_personattributerecognitionwithajointly-trainedholisticcnnmodel": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Person Attribute Recognition With a Jointly-Trained Holistic CNN Model",
    "authors": [
      "Patrick Sudowe",
      "Hannah Spitzer",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Sudowe_Person_Attribute_Recognition_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Sudowe_Person_Attribute_Recognition_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper addresses the problem of human visual attribute recognition, i.e., the prediction of a fixed set of semantic attributes given an image of a person. Previous work often considered the different attributes independently from each other, without taking advantage of possible dependencies between them. In contrast, we propose a method to jointly train a CNN model for all attributes that can take advantage of those dependencies, considering as input only the image without additional external pose, part or context information. We report detailed experiments examining the contribution of individual aspects, which yields beneficial insights for other researchers. Our holistic CNN achieves superior performance on two publicly available attribute datasets improving on methods that additionally rely on pose-alignment or context. To support further evaluations, we present a novel dataset, based on realistic outdoor video sequences, that contains more than 27,000 pedestrians annotated with 10 attributes. Finally, we explore design options to embrace the N/A labels inherently present in this task. "
  },
  "iccv2015_w11_deeplylearnedrichcodingforcross-datasetfacialageestimation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Deeply Learned Rich Coding for Cross-Dataset Facial Age Estimation",
    "authors": [
      "Zhanghui Kuang",
      "Chen Huang",
      "Wei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Kuang_Deeply_Learned_Rich_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Kuang_Deeply_Learned_Rich_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We propose a method for leveraging publicly available labeled facial age datasets to estimate age from unconstrained face images at the ChaLearn Looking at People (LAP) challenge 2015. We first learn discriminative age related representation on multiple publicly available age datasets using deep Convolutional Neural Networks (CNN). Training CNN is supervised by rich binary codes, and thus modeled as a multi-label classification problem. The codes represent different age group partitions at multiple granularities, and also gender information. We then train a regressor from deep representation to age on the small training dataset provided by LAP organizer by fusing random forest and quadratic regression with local adjustment. Finally, we evaluate the proposed methodon the provided testing data. It obtains the performance of 0.287, and ranks the 3rd place in the challenge. The experimental results demonstrate that the proposed deep representation is insensitive to cross-dataset bias, and thus generalizable to new datasets collected from other sources. "
  },
  "iccv2015_w11_deeplabeldistributionlearningforapparentageestimation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Deep Label Distribution Learning for Apparent Age Estimation",
    "authors": [
      "Xu Yang",
      "Bin-Bin Gao",
      "Chao Xing",
      "Zeng-Wei Huo",
      "Xiu-Shen Wei",
      "Ying Zhou",
      "Jianxin Wu",
      "Xin Geng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Yang_Deep_Label_Distribution_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Yang_Deep_Label_Distribution_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In the age estimation competition organized by ChaLearn, apparent ages of images are provided. Uncertainty of each apparent age is induced because each image is labeled by multiple individuals. Such uncertainty makes this age estimation task different from common chronological age estimation tasks. In this paper, we propose a method using deep CNN (Convolutional Neural Network) with distribution-based loss functions. Using distributions as the training tasks can exploit the uncertainty induced by manual labeling to learn a better model than using ages as the target. To the best of our knowledge, this is one of the first attempts to use the distribution as the target of deep learning. In our method, two kinds of deep CNN models are built with different architectures. After pre-training each deep CNN model with different datasets as one corresponding stream, the competition dataset is then used to fine-tune both deep CNN models. Moreover, we fuse the results of two streams as the final predicted ages. In the final testing dataset provided by competition, the age estimation performance of our method is 0.3057, which is significantly better than the human-level performance (0.34) provided by the competition organizers."
  },
  "iccv2015_w11_unconstrainedageestimationwithdeepconvolutionalneuralnetworks": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Unconstrained Age Estimation With Deep Convolutional Neural Networks",
    "authors": [
      "Rajeev Ranjan",
      "Sabrina Zhou",
      "Jun Cheng Chen",
      "Amit Kumar",
      "Azadeh Alavi",
      "Vishal M. Patel",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Ranjan_Unconstrained_Age_Estimation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Ranjan_Unconstrained_Age_Estimation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We propose an approach for age estimation from unconstrained images based on deep convolutional neural networks (DCNN). Our method consists of four steps: face detection, face alignment, DCNN-based feature extraction and neural network regression for age estimation. The proposed approach exploits two insights: (1) Features obtained from DCNN trained for face-identification task can be used for age estimation. (2) The three-layer neural network regression method trained on Gaussian loss performs better than traditional regression methods for apparent age estimation. Our method is evaluated on the apparent age estimation challenge developed for the ICCV 2015 ChaLearn Looking at People Challenge for which it achieves the error of 0:373."
  },
  "iccv2015_w11_anend-to-endsystemforunconstrainedfaceverificationwithdeepconvolutionalneuralnetworks": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "An End-To-End System for Unconstrained Face Verification With Deep Convolutional Neural Networks",
    "authors": [
      "Jun-Cheng Chen",
      "Rajeev Ranjan",
      "Amit Kumar",
      "Ching-Hui Chen",
      "Vishal M. Patel",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Chen_An_End-To-End_System_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Chen_An_End-To-End_System_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we present an end-to-end system for the unconstrained face verification problem based on deep convolutional neural networks (DCNN). The end-to-end system consists of three modules for face detection, alignment and verification and is evaluated using the newly released IARPA Janus Benchmark A (IJB-A) dataset and its extended version Janus Challenging set 2 (JANUS CS2) dataset. The IJB-A and CS2 datasets include real-world unconstrained faces of 500 subjects with significant pose and illumination variations which are much harder than the Labeled Faces in the Wild (LFW) and Youtube Face (YTF) datasets. Results of experimental evaluations for the proposed system on the IJB-A dataset are provided."
  },
  "iccv2015_w11_coordinatedlocalmetriclearning": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Coordinated Local Metric Learning",
    "authors": [
      "Shreyas Saxena",
      "Jakob Verbeek"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Saxena_Coordinated_Local_Metric_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Saxena_Coordinated_Local_Metric_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Mahalanobis metric learning amounts to learning a linear data projection, after which the L2 metric is used to compute distances. To allow more flexible metrics, not restricted to linear projections, local metric learning techniques have been developed. Most of these methodspartition the data spaceusing clustering, and for each cluster a separate metric is learned. Using local metrics, however, it is not clear how to measure distances between data points assigned to different clusters. In this paper we propose to embed the local metrics in a global low-dimensional representation, in which the L2 metric can be used. With each cluster we associate a linear mapping that projects the data to the global representation. This global representation directly allows computing distances between pointsregardless to which local cluster they belong. Moreover, it also enables data visualization in a single view, and the use of L2-based efficient retrieval methods. Experiments on the Labeled Faces in the Wild dataset show that our approach improves over previous global and local metric learning approaches."
  },
  "iccv2015_w11_faciallandmarklocalizationindepthimagesusingsupervisedridgedescent": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "Facial Landmark Localization in Depth Images Using Supervised Ridge Descent",
    "authors": [
      "Necati Cihan Camgoz",
      "Vitomir Struc",
      "Berk Gokberk",
      "Lale Akarun",
      "Ahmet Alp Kindiroglu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Camgoz_Facial_Landmark_Localization_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Camgoz_Facial_Landmark_Localization_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Supervised Descent Method (SDM) has proven successful in many computer vision applications such as face alignment, tracking and camera calibration. Recent studies which used SDM, achieved state of the-art performance on facial landmark localization in depth images. In this study, we propose to use ridge regression instead of least squares regression for learning the SDM, and to change feature sizes in each iteration, effectively turning the landmark search into a coarse to fine process. We apply the proposed method to facial landmark localization on the Bosphorus 3D Face Database; using frontal depth images with no occlusion. Experimental results confirm that both ridge regression and using adaptive feature sizes improve the localization accuracy considerably."
  },
  "iccv2015_w11_whenfacerecognitionmeetswithdeeplearninganevaluationofconvolutionalneuralnetworksforfacerecognition": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - ChaLearn Looking at People",
    "title": "When Face Recognition Meets With Deep Learning: An Evaluation of Convolutional Neural Networks for Face Recognition",
    "authors": [
      "Guosheng Hu",
      "Yongxin Yang",
      "Dong Yi",
      "Josef Kittler",
      "William Christmas",
      "Stan Z. Li",
      "Timothy Hospedales"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/html/Hu_When_Face_Recognition_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w11/papers/Hu_When_Face_Recognition_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather thaninvestigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluates the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, a traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available."
  },
  "iccv2015_w12_recognizingpersonalcontextsfromegocentricimages": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Recognizing Personal Contexts From Egocentric Images",
    "authors": [
      "Antonino Furnari",
      "Giovanni M. Farinella",
      "Sebastiano Battiato"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Furnari_Recognizing_Personal_Contexts_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Furnari_Recognizing_Personal_Contexts_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Wearable cameras can gather first-person images of the environment, opening new opportunities for the development of systems able to assist the users in their daily life. This paper studies the problem of recognizing personal contexts from images acquired by wearable devices, which finds useful applications in daily routine analysis and stress monitoring. To assess the influence of different device-specific features, such as the Field Of View and the wearing modality, a dataset of five personal contexts is acquired using four different devices. We propose a benchmark classification pipeline which combines a one-class classifier to detect the negative samples (i.e., images not representing any of the personal contexts under analysis) with a classic one-vs-one multi-class classifier to discriminate among the contexts. Several experiments are designed to compare the performances of many state-of-the-art representations for object and scene classification when used with data acquired by different wearable devices."
  },
  "iccv2015_w12_anevaluationofsupervised,novelty-basedandhybridapproachestofalldetectionusingsilmeeaccelerometerdata": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "An Evaluation of Supervised, Novelty-Based and Hybrid Approaches to Fall Detection Using Silmee Accelerometer Data",
    "authors": [
      "Aneta Lisowska",
      "Gavin Wheeler",
      "Victor Ceballos Inza",
      "Ian Poole"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Lisowska_An_Evaluation_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Lisowska_An_Evaluation_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Elderly people often experience a fear of falling. A reliable fall detector could increase their confidence in receiving prompt help after a fall, thus reducing their mental distress. A wearable sensors such as Toshiba's Silmee device can gather accelerometer data, which can be used to detect falls. We collected data from 20 volunteers wearing Silmee during simulated falls and activities of daily living (ADL). This gave 168 fall and 375 ADL recordings. We used these recordings in three experiments conducted to compare the performance of machine learning techniques for the detection of falls from accelerometer data.These experiments evaluate supervised methods, novelty based fall detection techniques, and finally our proposed hybrid techniques which use supervised methods for feature learning, but can be applied in the context of novelty detection. We found that the best performing supervised method was the Convolutional Neural Network (CNN) and the best performing unsupervised method was the one-class Nearest Neighbour Classifier. The best performing hybrid approach resulted from a combination of the CNN and the one-class Support Vector Machine. It draws on the strengths of the CNN (appropriate feature learning) and may offer more accurate real world fall identification."
  },
  "iccv2015_w12_anintuitivemobilityaidforvisuallyimpairedpeoplebasedonstereovision": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "An Intuitive Mobility Aid for Visually Impaired People Based on Stereo Vision",
    "authors": [
      "Tobias Schwarze",
      "Martin Lauer",
      "Manuel Schwaab",
      "Michailas Romanovas",
      "Sandra Bohm",
      "Thomas Jurgensohn"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Schwarze_An_Intuitive_Mobility_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Schwarze_An_Intuitive_Mobility_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a wearable assistance system for visually impaired persons that perceives the environment with a stereo camera and communicates obstacles and other objects to the user. We develop our idea of combining perception on an increased level of scene understanding with acoustic feedback to obtain an intuitive mobility aid. We describe our core techniques of scene modelling, object tracking, and acoustic feedback and show in an experimental study how our system can help improving the mobility and safety of visually impaired users."
  },
  "iccv2015_w12_improvingindoormobilityofthevisuallyimpairedwithdepth-basedspatialsound": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Improving Indoor Mobility of the Visually Impaired With Depth-Based Spatial Sound",
    "authors": [
      "Simon Blessenohl",
      "Cecily Morrison",
      "Antonio Criminisi",
      "Jamie Shotton"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Blessenohl_Improving_Indoor_Mobility_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Blessenohl_Improving_Indoor_Mobility_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a novel system to help visually impaired people to move efficiently and safely in indoor environments by mapping input from a depth camera to spatially localized auditory cues. We propose a set of context-specific cues which are suitable for use in systems that provide minimal audio feedback and hence reduce masking of natural sounds compared to the audio provided by general-purpose sense substitution devices. Using simple but effective heuristics for detecting the floor and the side walls, we propose auditory cues that encode information about the distances to walls, obstacles, the orientation of the corridor or room, and openings into corridors or rooms. But the key to our system is the use of a spatial sound engine that localizes the generated sounds in 3D. We evaluate our system, comparing with MeloSee. Our preliminary pilot study with ten blindfolded participants suggests that our system was more helpful for spotting smaller obstacles on the floor, though neither system had a significant edge in terms of walking speed or safety."
  },
  "iccv2015_w12_estimatingbodyposeofinfantsindepthimagesusingrandomferns": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Estimating Body Pose of Infants in Depth Images Using Random Ferns",
    "authors": [
      "Nikolas Hesse",
      "Gregor Stachowiak",
      "Timo Breuer",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Hesse_Estimating_Body_Pose_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Hesse_Estimating_Body_Pose_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In recent years, many systems for motion analysis of infants have been developed which either use markers or lack 3D information. We propose a system that can be trained fast and flexibly to fit the requirements of markerless 3D movement analysis of infants. Random Ferns are used as an efficient and robust alternative to Random Forests to find the 3D positions of body joints in single depth images. The training time is reduced by several orders of magnitude compared to the Kinect approach using a similar amount of data. Our system is trained in 9 hours on a 32 core workstation opposed to 24 hours on a 1000 core cluster, achieving comparable accuracy to the Kinect SDK on a publicly available pose estimation benchmark dataset containing adults. On manually annotated recordings of an infant, we obtain an average distance error over all joints of 41 mm. Building on the proposed approach, we aim to develop an automated, unintrusive, cheap and objective system for the early detection of infantile movement disorders like cerebral palsy using 3D motion analysis techniques."
  },
  "iccv2015_w12_accuratehuman-limbsegmentationinrgb-dimagesforintelligentmobilityassistancerobots": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Accurate Human-Limb Segmentation in RGB-D Images for Intelligent Mobility Assistance Robots",
    "authors": [
      "Siddhartha Chandra",
      "Stavros Tsogkas",
      "Iasonas Kokkinos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Chandra_Accurate_Human-Limb_Segmentation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Chandra_Accurate_Human-Limb_Segmentation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Mobility impairment is one of the biggest challenges faced by elderly people in today's society. The inability to move about freely poses severe restrictions on their independence and general quality of life. This work is dedicated to developing intelligent robotic platforms that assist users to move without requiring a human attendant. This work was done in the context of an EU project involved in developing an intelligent robot for elderly user assistance. The robot is equipped with a Kinect sensor, and the vision component of the project has the responsibility of locating the user, estimating the user's pose, and recognizing gestures by the user. All these goals can take advantage of a method that accurately segments human-limbs in the colour (RGB) and depth (D) images captured by the Kinect sensor. We exploit recent advances in deep-learning to develop a system that performs accurate semantic segmentation of human limbs using colour and depth images. Our novel technical contributions are the following: 1) we describe a scheme for manual annotation of videos, that eliminates the need to annotate segmentation masks in every single frame; 2) we extend a state of the art deep learning system for semantic segmentation, to exploit diverse RGB and depth data, in a single framework for training and testing; 3) we evaluate different variants of our system and demonstrate promising performance, as well the contribution of diverse data, on our in-house Human-Limb dataset. Our method is very efficient, running at 8 frames per second on a GPU."
  },
  "iccv2015_w12_summarizingwhilerecordingcontext-basedhighlightdetectionforegocentricvideos": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Summarizing While Recording: Context-Based Highlight Detection for Egocentric Videos",
    "authors": [
      "Yen-Liang Lin",
      "Vlad I. Morariu",
      "Winston Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Lin_Summarizing_While_Recording_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Lin_Summarizing_While_Recording_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In conventional video summarization problems, contexts (e.g., scenes, activities) are often fixed or in a specific structure (e.g., movie, sport, surveillance videos). However, egocentric videos often include a variety of scene contexts as users can bring the cameras anywhere, which makes these conventional methods not directly applicable, especially because there is limited memory storage and computing power on the wearable devices. To resolve these difficulties, we propose a context-based highlight detection method that immediately generates summaries without watching the whole video sequences. In particular, our method automatically predicts the contexts of each video segment and uses a context-specific highlight model to generate the summaries. To further reduce computational and storage cost, we develop a joint approach that simultaneously optimizes the context and highlight models in an unified learning framework. We evaluate our method on a public Youtube dataset, demonstrating our method outperforms state-of-the-art approaches. In addition, we show the utility of our joint approach and early prediction for achieving competitive highlight detection results while requiring less computational and storage cost."
  },
  "iccv2015_w12_evaluatingreal-timemirroringofheadgesturesusingsmartglasses": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Evaluating Real-Time Mirroring of Head Gestures Using Smart Glasses",
    "authors": [
      "Juan R. Terven",
      "Bogdan Raducanu",
      "Maria-Elena  Meza",
      "Joaquin Salas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Terven_Evaluating_Real-Time_Mirroring_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Terven_Evaluating_Real-Time_Mirroring_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Mirroring occurs when one person tends to mimic the non-verbal communication of their counterparts. Even though mirroring is a complex phenomenon, in this study, we focus on the detection of head-nodding as a simple non-verbal communication cue due to its significance as a gesture displayed during social interactions. This paper introduces a computer vision-based method to detect mirroring through the analysis of head gestures using wearable cameras (smart glasses). In addition, we study how such a method can be used to explore perceived competence. The proposed method has been evaluated and the experiments demonstrate how static and wearable cameras seem to be equally effective to gather the information required for the analysis."
  },
  "iccv2015_w12_visualattention-guidedapproachtomonitoringofmedicationdispensingusingmulti-locationfeaturesaliencypatterns": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Visual Attention-Guided Approach to Monitoring of Medication Dispensing Using Multi-Location Feature Saliency Patterns",
    "authors": [
      "Roman Palenichka",
      "Ahmed Lakhssassi",
      "Myroslav Palenichka"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Palenichka_Visual_Attention-Guided_Approach_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Palenichka_Visual_Attention-Guided_Approach_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper is dedicated to the development of a computer vision-based system for medication (pills and capsules) identification and counting in order to increase the productivity of medication dispensing and maintain its high safety. The algorithmic basis of the system is the attentive vision approach to robust and fast object detection in images. It consists in time-efficient image analysis by a multi-scale visual attention operator to detect feature-point areas located inside the pill and capsule regions. The attention operator combines a spatial saliency filter with a temporal change (novelty) detector in order to robustly detect salient and object-relevant feature points. The medication recognition algorithm involves a set of image descriptors at the feature-point areas called the multi-location feature-saliency pattern, which fully discriminates between different types of medication. The method detects pills and extracts area-based descriptors without any image pre-segmentation procedure due to the proposed multi-scale attention operator."
  },
  "iccv2015_w12_saliencydetectionusingquaternionsparsereconstruction": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Saliency Detection Using Quaternion Sparse Reconstruction",
    "authors": [
      "Yi Zeng",
      "Yi Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Zeng_Saliency_Detection_Using_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Zeng_Saliency_Detection_Using_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We proposed a visual saliency detection model for color images based on the reconstruction residual of quaternion sparse model in this paper. This algorithm measures saliency of color image region by the reconstruction residual and performs more consistent with visual perception than current sparse models. In current sparse models, they treat the color images as multiple independent channel images and take color image pixel as a scalar entity. Consequently, the important information about interrelationship between color channels is lost during sparse representation. In contrast, the quaternion sparse model treats the color image pixels as a quaternion matrix, completely preserving the inherent color structures during the sparse coding. Therefore, the salient regions can be reliably extracted according to quaternion sparse reconstruction residual since these regions cannot be well approximated using its neighbouring blocks as dictionaries. The proposed saliency detection method achieves better performance on Bruce-Tsotsos dataset and OSIE dataset as compared with traditional sparse reconstruction based models and other state-of-art saliency models. Specifically, our model can achieve higher consistency with human perception without training step and gains higher AUC scores than traditional sparse reconstruction based models."
  },
  "iccv2015_w12_deeplearningofmouthshapesforsignlanguage": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Deep Learning of Mouth Shapes for Sign Language",
    "authors": [
      "Oscar Koller",
      "Hermann Ney",
      "Richard Bowden"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Koller_Deep_Learning_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Koller_Deep_Learning_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper deals with robust modelling of mouth shapes in the context of sign language recognition using deep convolutional neural networks. Sign language mouth shapes are difficult to annotate and thus hardly any publicly available annotations exist. As such, this work exploits related information sources as weak supervision. Humans mainly look at the face during sign language communication, where mouth shapes play an important role and constitute natural patterns with large variability. However, most scientific research on sign language recognition still disregards the face. Hardly any works explicitly focus on mouth shapes. This paper presents our advances in the field of sign language recognition. We contribute in following areas: We present a scheme to learn a convolutional neural network in a weakly supervised fashion without explicit frame labels. We propose a way to incorporate neural network classifier outputs into a HMM approach. Finally, we achieve a significant improvement in classification performance of mouth shapes over the current state of the art."
  },
  "iccv2015_w12_astructuredcommitteeforfoodrecognition": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "A Structured Committee for Food Recognition",
    "authors": [
      "Niki Martinel",
      "Claudio Piciarelli",
      "Christian Micheloni",
      "Gian Luca Foresti"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Martinel_A_Structured_Committee_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Martinel_A_Structured_Committee_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Food recognition is an emerging computer vision topic. The problem is characterized by the absence of rigid structure of the food and by the large intra-class variations. Existing approaches tackle the problem by designing ad-hoc feature representations based on a priori knowledge of the problem. Differently from these, we propose a committee-based recognition system that chooses the optimal features out of the existing plethora of available ones (e.g., color, texture, etc.). Each committee member is an Extreme Learning Machine trained to classify food plates on the basis of a single feature type. Single member classifications are then considered by a structural Support Vector Machine to produce the final ranking of possible matches. This is achieved by filtering out the irrelevant features/classifiers, thus considering only the relevant ones. Experimental results show that the proposed system outperforms state-of-the-art works on the most used three publicly available benchmark datasets."
  },
  "iccv2015_w12_single-frameindexingfor3dhandposeestimation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Single-Frame Indexing for 3D Hand Pose Estimation",
    "authors": [
      "Cassandra Carley",
      "Carlo Tomasi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Carley_Single-Frame_Indexing_for_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Carley_Single-Frame_Indexing_for_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Hand pose estimation from 3D sensor data matches a point cloud to a hand model, and has broad applications from gestural interfaces to scene understanding. We propose a novel scheme to index into a database of precomputed hand poses to initialize the match. Our index describes 2D hand silhouettes, which can be computed from either depth maps or standard video, in the form of simple yet expressive signatures. We compare signatures to each other through a new variant of the Earth Mover's Distance that makes small distances in feature space correlate highly with those in pose space. We present a new technique that uses a depth sensor and a sensor glove to create databases of real images and ground-truth poses for both training and testing. We show state-of-the-art accuracy and speed for both gesture classification and joint-pose regression, even when comparing our 2D single-frame method with those that employ RGB-D features or multi-sensor inputs and report quantitative results."
  },
  "iccv2015_w12_afastandaccurateeyetrackerusingstroboscopicdifferentiallighting": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "A Fast and Accurate Eye Tracker Using Stroboscopic Differential Lighting",
    "authors": [
      "Frank H. Borsato",
      "Fernando O. Aluani",
      "Carlos H. Morimoto"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Borsato_A_Fast_and_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Borsato_A_Fast_and_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The use of video-based eye trackers (VETs) for gaze interaction have allowed people with severe motor disabilities to communicate more effectively. Nonetheless, current low cost VETs present limited accuracy and precision that impose several design constraints for gaze-based computer applications. In this paper we present an extension of the differential lighting (DL) technique for pupil detection and tracking using active light sources. The original technique was developed for analog interlaced cameras with external sync signal to synchronize the light sources with the video signal. In this paper we introduce the Stroboscopic DL technique that can be used with any rolling shutter camera, even those with no external sync. We have developed a computer vision technique to adjust the firing of the stroboscopic lights. Our new algorithm also exploits characteristics of pupil images to improve the accuracy of the tracking algorithm. Another advantage of the method is that using flashed pulses of light creates a virtual exposure time, reducing motion blur and temporal shear in the video volume. A real-time 187 fps prototype of the system was implemented using a low cost PS3 camera. Experimental results comparing the performance of our algorithm with Starburst show significant accuracy and speed improvement."
  },
  "iccv2015_w12_quantifyinglevodopa-induceddyskinesiausingdepthcamera": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Quantifying Levodopa-Induced Dyskinesia Using Depth Camera",
    "authors": [
      "Maria Dyshel",
      "David Arkadir",
      "Hagai Bergman",
      "Daphna Weinshall"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Dyshel_Quantifying_Levodopa-Induced_Dyskinesia_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Dyshel_Quantifying_Levodopa-Induced_Dyskinesia_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a novel method to detect and assess the severity of Levodopa-Induced Dyskinesia (LID) in Parkinson's Disease (PD) patients, based on Microsoft Kinect recordings of the patients. Dyskinesia denotes involuntary movements induced by chronic treatment with levodopa in patients with PD. Detection and objective quantification of dyskinesia is essential for optimizing the medication regime and developing novel treatments for PD. We used Microsoft Kinect sensor to track limb and neck movements of a patient performing two motor tasks. Using a new motion segmentation algorithm, kinematic features were extracted from the videos and classified using Support Vector Machines (SVMs). The method was tested on 25 recordings of 9 PD patients, and achieved sensitivity of 0.82 at EER in overall dyskinesia detection. Moreover, it provided a numerical overall score for the severity of dyskinesia, which showed high correlation with the neurologist's assessment of the patient's state. The study shows that depth camera recordings can be used to monitor and grade the severity of levodopa-induced dyskinesia, and therefore can potentially provide valuable aid to clinicians and researchers."
  },
  "iccv2015_w12_astereovisionapproachforcooperativeroboticmovementtherapy": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "A Stereo Vision Approach for Cooperative Robotic Movement Therapy",
    "authors": [
      "Benjamin Busam",
      "Marco Esposito",
      "Simon Che'Rose",
      "Nassir Navab",
      "Benjamin Frisch"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Busam_A_Stereo_Vision_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Busam_A_Stereo_Vision_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Movement therapy is an integrating part of stroke rehabilitation. The positive influence of intensive, repetitive motion training and the importance of active patient participation trigger the development of cooperative robotic assistants. We suggest a device for the re-education of upper limb movements in hemiparetic patients where a light-weight robotic arm that supports the deficient arm is equipped with a stereoscopic camera system. It follows the movements of the healthy arm that wears a sleeve equipped with flat round reflective markers detected by the cameras. We introduce an advanced robust and real-time algorithm to provide the tracking information. It performs a sparse marker based point cloud registration based on subpixel precision contour fits to enable high accuracy pose estimates while being capable of online model adjustments. The update rate of the tracking is 9 ms and the precision of the system is measured to be 0.5 mm. Tests with healthy subjects show that the system is able to accurately reproduce the movement of the healthy arm on an impaired arm."
  },
  "iccv2015_w12_headnoddetectionfromafull3dmodel": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Head Nod Detection From a Full 3D Model",
    "authors": [
      "Yiqiang Chen",
      "Yu Yu",
      "Jean-Marc Odobez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Chen_Head_Nod_Detection_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Chen_Head_Nod_Detection_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " As a non-verbal communication mean, head gestures play an important role in face-to-face conversationand recognizing them is therefore of high value for social behavior analysis orHuman Robotic Interactions (HRI) modelling. Among the various gestures, head nod is the most common one and can convey agreement or emphasis.In this paper, we propose a novel nod detection approach based on a full 3D face centeredrotation model. Compared to previous approaches, we make two contributions. Firstly,the head rotation dynamic is computed within the head coordinate instead of the camera coordinate,leading topose invariant gesture dynamics. Secondly, besides the rotation parameters, a feature related to the head rotation axis is proposed so that nod-like false positives due to body movementscould be eliminated. The experiments on two-party and four-party conversations demonstrate the validityof the approach."
  },
  "iccv2015_w12_automaticemotionrecognitioninrobot-childreninteractionforasdtreatment": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Automatic Emotion Recognition in Robot-Children Interaction for ASD Treatment",
    "authors": [
      "Marco Leo",
      "Marco Del Coco",
      "Pierluigi Carcagni",
      "Cosimo Distante",
      "Massimo Bernava",
      "Giovanni Pioggia",
      "Giuseppe Palestra"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Leo_Automatic_Emotion_Recognition_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Leo_Automatic_Emotion_Recognition_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Autism Spectrum Disorders(ASD) are a group of lifelong disabilities that affect people's communication and understanding social cues.The state of the art witnesses how technology, and in particular robotics, may offer promising tools to strengthen the research and therapy of ASD. This work represents the first attempt to use machine-learning strategies during robot-ASD children interactions, in terms of facial expression imitation, making possible an objective evaluation of children's behaviours and then giving the possibility to introduce a metric about the effectiveness of the therapy. In particular, the work focuses on the basic emotion recognition skills. In addition to the aforementioned applicative innovations this work contributes also to introduce a facial expression recognition (FER) engine that automatically detects and tracks the child's face and then recognize emotions on the basis of a machine learning pipeline based on HOG descriptor and Support Vector Machines. Two different experimental sessions were carried out: the first one tested the FER engine on publicly available datasets demonstrating that the proposed pipeline outperforms the existing strategies in terms of recognition accuracy. The second one involved ASD children and it was a preliminary exploration of how the introduction of the FER engine in the therapeutic protocol can be effectively used to monitor children's behaviours."
  },
  "iccv2015_w12_fine-grainedproductclassrecognitionforassistedshopping": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Fine-Grained Product Class Recognition for Assisted Shopping",
    "authors": [
      "Marian George",
      "Dejan Mircic",
      "Gabor Soros",
      "Christian Floerkemeier",
      "Friedemann Mattern"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/George_Fine-Grained_Product_Class_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/George_Fine-Grained_Product_Class_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Assistive solutions for a better shopping experience can improve the quality of life of people, in particular also of visually impaired shoppers. We present a system that visually recognizes the fine-grained product classes of items on a shopping list, in shelves images taken with a smartphone in a grocery store. Our system consists of three components: (a) We automatically recognize useful text on product packaging, e.g., product name and brand, and build a mapping of words to product classes based on the large-scale GroceryProducts dataset. When the user populates the shopping list, we automatically infer the product class of each entered word. (b) We perform fine-grained product class recognition when the user is facing a shelf. We discover discriminative patches on product packaging to differentiate between visually similar product classes and to increase the robustness against continuous changes in product design. (c) We continuously improve therecognition accuracy through active learning. Our experiments show the robustness of the proposed method against cross-domain challenges, and the scalability to an increasing number of products with minimal re-training. "
  },
  "iccv2015_w12_pedestriandetectionviamixtureofcnnexpertsandthresholdedaggregatedchannelfeatures": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Assistive Computer Vision and Robotics",
    "title": "Pedestrian Detection via Mixture of CNN Experts and Thresholded Aggregated Channel Features",
    "authors": [
      "Ankit Verma",
      "Ramya  Hebbalaguppe",
      "Lovekesh Vig",
      "Swagat Kumar",
      "Ehtesham Hassan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/html/Verma_Pedestrian_Detection_via_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w12/papers/Verma_Pedestrian_Detection_via_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we propose a two stage pedestrian detector. The first stage involves a cascade of Aggregated Channel Features (ACF) to extract potential pedestrian windows from an image. We further introduce a thresholding technique on the ACF confidence scores that segregates candidate windows lying at the extremes of the ACF score distribution. The windows with ACF scores in between the upper and lower bounds are passed on to a Mixture of Expert (MoE) CNNs for more refined classification in the second stage.Results show that the designed detector yields better than state-of-the-art performance on the INRIA benchmark dataset and yields a miss rate of 10.35% at FPPI=0.1."
  },
  "iccv2015_w14_thevisualobjecttrackingvot2015challengeresults": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "The Visual Object Tracking VOT2015 Challenge Results",
    "authors": [
      "Matej Kristan",
      "Jiri Matas",
      "Ales Leonardis",
      "Michael Felsberg",
      "Luka Cehovin",
      "Gustavo Fernandez",
      "Tomas Vojir",
      "Gustav Hager",
      "Georg Nebehay",
      "Roman Pflugfelder"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Kristan_The_Visual_Object_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Kristan_The_Visual_Object_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features ofthe VOT2015 challenge that go beyond its VOT2014 predecessor are: (i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website."
  },
  "iccv2015_w14_scalablekernelcorrelationfilterwithsparsefeatureintegration": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "Scalable Kernel Correlation Filter With Sparse Feature Integration",
    "authors": [
      "Andres Solis Montero",
      "Jochen Lang",
      "Robert Laganiere"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Montero_Scalable_Kernel_Correlation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Montero_Scalable_Kernel_Correlation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Correlation filters for long-term visual object trackinghave recently seen great interest. Although they presentcompetitive performance results, there is still a need for im-proving their tracking capabilities. In this paper, we presenta fast scalable solution based on the Kernalized CorrelationFilter (KCF) framework. We introduce an adjustable Gaus-sian window function and a keypoint-based model for scaleestimation to deal with the fixed size limitation in the Ker-nelized Correlation Filter. Furthermore, we integrate thefast HoG descriptors and Intel's Complex Conjugate Sym-metric (CCS) packed format to boost the achievable framerates. We test our solution using the Visual Tracker Bench-mark and the VOT Challenge datasets. We evaluate ourtracker in terms of precision and success rate, accuracy,robustness and speed. The empirical evaluations demon-strate clear improvements by the proposed tracker over theKCF algorithm while ranking among the top state-of-the-art trackers."
  },
  "iccv2015_w14_jointscale-spatialcorrelationtrackingwithadaptiverotationestimation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "Joint Scale-Spatial Correlation Tracking With Adaptive Rotation Estimation",
    "authors": [
      "Mengdan Zhang",
      "Junliang Xing",
      "Jin Gao",
      "Xinchu Shi",
      "Qiang Wang",
      "Weiming Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Zhang_Joint_Scale-Spatial_Correlation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Zhang_Joint_Scale-Spatial_Correlation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Boosted by large and standardized benchmark datasets, visual object tracking has made great progress in recent years and brought about many new trackers. Among these trackers, correlation filter based tracking schema exhibits impressive robustness and accuracy. In this work, we present a fully functional correlation filter based tracking algorithm which is able to simultaneously model target appearance changes from spatial displacements, scale variations, and rotation transformations. The proposed tracker first represents the exhaustive template searching in the joint scale and spatial space by a block-circulant matrix. Then, by transferring the target template from the Cartesian coordinate system to the Log-Polar coordinate system, the circulant structure is well preserved for the target even after whole orientation rotation. With these novel representation and transformation, object tracking is efficiently and effectively performed in the joint space with fast Fourier Transform. Experimental results on the VOT 2015 benchmark dataset demonstrate its superior performance over state-of-the-art tracking algorithms."
  },
  "iccv2015_w14_robustvisualtrackingbyexploitingthehistoricaltrackersnapshots": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "Robust Visual Tracking by Exploiting the Historical Tracker Snapshots",
    "authors": [
      "Jiatong Li",
      "Zhibin Hong",
      "Baojun Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Li_Robust_Visual_Tracking_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Li_Robust_Visual_Tracking_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Variations of target appearances due to illumination changes, heavy occlusions and abrupt motions are the major factors for tracking failures. In this paper, we show that these failures can be effectively handled by exploiting the trajectory consistency between the current tracker and its historical trained snapshots. Here, we propose a Scale-adaptive Multi-Expert (SME) tracker, which combines the current tracker and its historical trained snapshots to construct a multi-expert ensemble. The best expert in the ensemble is then selected according to the accumulated trajectory consistency criteria. The base tracker estimates the translation accurately with regression based correlation filter, and an effective scale adaptive scheme is introduced to handle scale changes on-the-fly. SME is extensively evaluated on the 51 sequences tracking benchmark and VOT2015 dataset. The experimental results demonstrate the excellent performance of the proposed approach against state-of-the-art methods with real-time speed."
  },
  "iccv2015_w14_multi-templatescale-adaptivekernelizedcorrelationfilters": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "Multi-Template Scale-Adaptive Kernelized Correlation Filters",
    "authors": [
      "Adel Bibi",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Bibi_Multi-Template_Scale-Adaptive_Kernelized_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Bibi_Multi-Template_Scale-Adaptive_Kernelized_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper identifies the major drawbacks of a very computationally efficient and state-of- the-art-tracker known as the Kernelized Correlation Filter (KCF) tracker. These drawbacks include an assumed fixed scale of the target in every frame, as well as, a heuristic update strategy of the filter taps to incorporate historical tracking information (i.e. simple linear combination of taps from the previous frame). In our approach, we update the scale of the tracker by maximizing over the posterior distribution of a grid of scales. As for the filter update, we prove and show that it is possible to use all previous training examples to update the filter taps very efficiently using fixed-point optimization. We validate the efficacy of our approach on two tracking datasets, VOT2014 and VOT2015. "
  },
  "iccv2015_w14_convolutionalfeaturesforcorrelationfilterbasedvisualtracking": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "Convolutional Features for Correlation Filter Based Visual Tracking",
    "authors": [
      "Martin Danelljan",
      "Gustav Hager",
      "Fahad Shahbaz Khan",
      "Michael Felsberg"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Danelljan_Convolutional_Features_for_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Danelljan_Convolutional_Features_for_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Visual object tracking is a challenging computer vision problem with numerous real-world applications. This pa- per investigates the impact of convolutional features for the visual tracking problem. We propose to use activations from the convolutional layer of a CNN in discriminative correlation filter based tracking frameworks. These acti- vations have several advantages compared to the standard deep features (fully connected layers). Firstly, they miti- gate the need of task specific fine-tuning. Secondly, they contain structural information crucial for the tracking prob- lem. Lastly, these activations have low dimensionality. We perform comprehensive experiments on three benchmark datasets: OTB, ALOV300++ and the recently introduced VOT2015. Surprisingly, different to image classification, our results suggest that activations from the first layer pro- vide superior tracking performance compared to the deeper layers. Our results further show that the convolutional fea- tures provide improved results compared to standard hand- crafted features. Finally, results comparable to state-of-the- art trackers are obtained on all three benchmark datasets."
  },
  "iccv2015_w14_trackerfusiononvotchallengehowdoesitperformandwhatcanwelearnaboutsingletrackers?": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "Tracker Fusion on VOT Challenge: How Does It Perform and What Can We Learn About Single Trackers?",
    "authors": [
      "Christian Bailer",
      "Didier Stricker"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Bailer_Tracker_Fusion_on_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Bailer_Tracker_Fusion_on_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Tracker fusion i.e. the fusion of the outputs of different tracking methods is an interesting new concept. Thus it should also be considered in the VOT challenges. In this paper we evaluate the performance of tracker fusion on the VOT2013 and VOT2014 datasets. Furthermore, we utilize the fusion concept to create novel fusion based measures for evaluating trackers. Fusion based evaluation is interesting as it does not evaluate trackers independently but in the context of all other trackers. It allows us for example to identify trackers that could despite poor average performance be interesting for research in object tracking. We found e.g. that all state-of-the-art trackers lack some strengths of a simple NCC tracker. Tracker fusion can exploit this and profit from an additional NCC tracker. We raise the question: Can this also be exploited in a more direct way i.e. can we e.g. combine NCC concepts with a state-of-the-art tracker?"
  },
  "iccv2015_w14_thethermalinfraredvisualobjecttrackingvot-tir2015challengeresults": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Visual Object Tracking",
    "title": "The Thermal Infrared Visual Object Tracking VOT-TIR2015 Challenge Results",
    "authors": [
      "Michael Felsberg",
      "Amanda Berg",
      "Gustav Hager",
      "Jorgen Ahlberg",
      "Matej Kristan",
      "Jiri Matas",
      "Ales Leonardis",
      "Luka Cehovin",
      "Gustavo Fernandez",
      "Tomas Vojir",
      "Georg Nebehay",
      "Roman Pflugfelder"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/html/Felsberg_The_Thermal_Infrared_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w14/papers/Felsberg_The_Thermal_Infrared_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " The Thermal Infrared Visual Object Tracking challenge 2015, VOT-TIR2015, aims at comparing short-term single-object visual trackers that work on thermal infrared (TIR) sequences and do not apply pre-learned models of object appearance. VOT-TIR2015 is the first benchmark on short-term tracking in TIR sequences. Results of 24 trackers are presented. For each participating tracker, a short description is provided in the appendix.The VOT-TIR2015 challenge is based on the VOT2013 challenge, but introduces the following novelties: (i) the newly collected LTIR (Linkoping TIR) dataset is used, (ii) the VOT2013 attributes are adapted to TIR data, (iii) the evaluation is performed using insights gained during VOT2013 and VOT2014 and is similar to VOT2015. "
  },
  "iccv2015_w18_acenturyofportraitsavisualhistoricalrecordofamericanhighschoolyearbooks": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Extreme Imaging",
    "title": "A Century of Portraits: A Visual Historical Record of American High School Yearbooks",
    "authors": [
      "Shiry Ginosar",
      "Kate Rakelly",
      "Sarah Sachs",
      "Brian Yin",
      "Alexei A. Efros"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/html/Ginosar_A_Century_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/papers/Ginosar_A_Century_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Many details about our world are not captured in written records because they are too mundane or too abstract to describe in words. Fortunately, since the invention of the camera, an ever-increasing number of photographs capture much of this otherwise lost information. This plethora of artifacts documenting our \"visual culture\" is a treasure trove of knowledge as yet untapped by historians. We present a dataset of 37,921 frontal-facing American high school yearbook photos that allow us to use computation to glimpse into the historical visual record too voluminous to be evaluated manually. The collected portraits provide a constant visual frame of reference with varying content. We can therefore use them to consider issues such as a decade's defining style elements, or trends in fashion and social norms over time. We demonstrate that our historical image dataset may be used together with weakly-supervised data-driven techniques to perform scalable historical analysis of large image corpora with minimal human effort, much in the same way that large text corpora together with natural language processing revolutionized historians' workflow. Furthermore, we demonstrate the use of our dataset in dating grayscale portraits using deep learning methods."
  },
  "iccv2015_w18_scotopicvisualrecognition": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Extreme Imaging",
    "title": "Scotopic Visual Recognition",
    "authors": [
      "Bo Chen",
      "Pietro Perona"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/html/Chen_Scotopic_Visual_Recognition_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/papers/Chen_Scotopic_Visual_Recognition_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Recognition from a small number of photons is important for biomedical imaging, security, astronomy and many other fields. We develop a framework that allows a machine to classify objects as quickly as possible, hence requiring as few photons as possible, while maintaining the error rate below an acceptable threshold. The framework also allows for a dynamic speed versus accuracy tradeoff. Given a generative model of the scene, the optimal tradeoff can be obtained from operations akin to the feedforward computation in a deep neural network. The generative model may also be learned from the data. The learned model requires less than 1 photon per pixel to achieve the same performance obtained with images in normal lighting conditions on the MNIST dataset, and 10 photons per unit to be within 1% of the best accuracy of the CIFAR10 dataset. "
  },
  "iccv2015_w18_flatcamreplacinglenseswithmasksandcomputation": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Extreme Imaging",
    "title": "FlatCam: Replacing Lenses With Masks and Computation",
    "authors": [
      "M. Salman Asif",
      "Ali Ayremlou",
      "Ashok Veeraraghavan",
      "Richard Baraniuk",
      "Aswin Sankaranarayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/html/Asif_FlatCam_Replacing_Lenses_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/papers/Asif_FlatCam_Replacing_Lenses_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a thin form-factor lensless camera, FlatCam, that consists of a coded mask placed on top of a bare, conventional sensor array. FlatCam is an instance of a coded aperture imaging system in which each pixel records a linear combination of light from multiple scene elements. A computational algorithm is then used to demultiplex the recorded measurements and reconstruct an image of the scene. In contrast with vast majority of coded aperture systems, we place the coded mask extremely close to the image sensor that can enable a thin system. We use a separable mask to ensure that both calibration and image reconstruction are scalable in terms of memory requirements and computational complexity. We demonstrate the potential of our design using a prototype camera built using commercially available sensor and mask."
  },
  "iccv2015_w18_lowpowerdepthandvelocityfromapassivemovingsensor": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Extreme Imaging",
    "title": "Low Power Depth and Velocity From a Passive Moving Sensor",
    "authors": [
      "Emma Alexander",
      "Sanjeev J. Koppal",
      "Todd Zickler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/html/Alexander_Low_Power_Depth_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/papers/Alexander_Low_Power_Depth_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": "We present an opportunity for the visual sensing of depth and 3D velocity using a passive sensor that has extremely low power requirements. This opportunity comes from a new mathematical constraint, which we derive, that relates depth and velocity to spatial and temporal derivatives of image values captured by a coded-aperture camera that observes a moving scene. The constraint exploits the fact that there are two causes of brightness change in this situation: features move across the image due to motion, and contrast changes because of time-varying optical blur. The sensor that could be realized from this constraint is called a focal flow sensor. We analytically characterize the working volume of such a sensor in relation to its size, and we provide simulation results that affirm its viability."
  },
  "iccv2015_w18_estimatingasmallsignalinthepresenceoflargenoise": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Extreme Imaging",
    "title": "Estimating a Small Signal in the Presence of Large Noise",
    "authors": [
      "Amy Zhao",
      "Fredo Durand",
      "John Guttag"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/html/Zhao_Estimating_a_Small_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/papers/Zhao_Estimating_a_Small_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Video magnification techniques are useful for visualizing small changes in videos. For instance, Eulerian video magnification has been used to visualize the flow of blood in the human face. Such visualizations have possible applications in remote monitoring or screening for diseases. However, when visualizing blood flow, the signal of interest may be similar in amplitude to the noise in the video. This raises the question of what one is actually seeing in a magnified video: signal or noise? We seek to understand these signal and noise characteristics with the goal of producing informative and accurate visualizations. We present a preliminary algorithm for estimating the signal amplitude in the presence of relatively high noise. We demonstrate that the algorithm can be used to accurately estimate the signal amplitude in an uncompressed simulated video, but is susceptible to compression noise and motion."
  },
  "iccv2015_w18_exploringtheresolutionlimitforin-airsynthetic-apertureaudioimaging": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Extreme Imaging",
    "title": "Exploring the Resolution Limit for In-Air Synthetic-Aperture Audio Imaging",
    "authors": [
      "Hisham Bedri",
      "Micha Feigin",
      "Petros T. Boufounos",
      "Ramesh Raskar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/html/Bedri_Exploring_the_Resolution_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/papers/Bedri_Exploring_the_Resolution_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " SONAR imaging can detect reflecting objects in the dark and around corners, however many SONAR systems require large phased-arrays and immobile equipment. In order to enable sound imaging with a mobile device, one can move a microphone and speaker in the air to form a large synthetic aperture. We demonstrate resolution limited audio images using a moving microphone and speaker of a mannequin in free-space and a mannequin located around a corner. This paper also explores the 2D resolution limit due to aperture size as well as the time resolution limit due to bandwidth, and proposes Continuous Basis Pursuits (CBP) to super-resolve. "
  },
  "iccv2015_w18_crowdpaintingwithlightparticipatoryimagingatthebigshot": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Extreme Imaging",
    "title": "Crowdpainting With Light: Participatory Imaging at the Big Shot",
    "authors": [
      "Michael Peres",
      "Andreas Savakis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/html/Peres_Crowdpainting_With_Light_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w18/papers/Peres_Crowdpainting_With_Light_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Painting with light is a night photography technique where the photographer illuminates the subject with moving lights over a long exposure time.The Big Shot project has taken this technique to the extreme over the past 30 years by engaging large crowds for the lighting of landmarks resulting in unique night photographs.In this paper, we overview the fundamental techniques of crowdpainting with light and show representative examples of images taken through the years.We also discuss some of the social aspects of this participatory experience that make it memorable and engaging.Finally, we propose computational techniques for painting with light and demonstrate, through a small experiment, how crowdpainting with light can be accessible to broad audiences for experimentation, social interaction and fun."
  },
  "iccv2015_w20_seeingthesoundanewmultimodalimagingdeviceforcomputervision": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Reconstruction and Understanding with Video and Sound",
    "title": "Seeing the Sound: A New Multimodal Imaging Device for Computer Vision",
    "authors": [
      "Andrea Zunino",
      "Marco Crocco",
      "Samuele Martelli",
      "Andrea Trucco",
      "Alessio Del Bue",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w20/html/Zunino_Seeing_the_Sound_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w20/papers/Zunino_Seeing_the_Sound_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Audio imaging can play a fundamental role in computer vision, in particular in automated surveillance, boosting the accuracy of current systems based on standard optical cameras. We present here a new hybrid device for acoustic-optic imaging, whose characteristics are tailored to automated surveillance. In particular, the device allows realtime, high frame rate generation of an acoustic map, overlaid over a standard optical image using a geometric calibration of audio and video streams. We demonstrate the potentialities of the device for target tracking on three challenging setup showing the advantages of using acoustic images against baseline algorithms on image tracking. In particular, the proposed approach is able to overcome, often dramatically, visual tracking with state-of-art algorithms, dealing efficiently with occlusions, abrupt variations in visual appearence and camouflage. These results pave the way to a widespread use of acoustic imaging in application scenarios such as in surveillance and security."
  },
  "iccv2015_w20_trackingtheactivespeakerbasedonajointaudio-visualobservationmodel": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Reconstruction and Understanding with Video and Sound",
    "title": "Tracking the Active Speaker Based on a Joint Audio-Visual Observation Model",
    "authors": [
      "Israel D. Gebru",
      "Sileye Ba",
      "Georgios Evangelidis",
      "Radu Horaud"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w20/html/Gebru_Tracking_the_Active_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w20/papers/Gebru_Tracking_the_Active_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Any multi-party conversation system benefits from speaker diarization, that is, the assignment of speech signals among the participants. We here cast the diarization problem into a tracking formulation whereby the active speaker is detected and tracked over time. A probabilistic tracker exploits the on-image (spatial) coincidence of visual and auditory observations and infers a single latent variable which represents the identity of the active speaker. Both visual and auditory observations are explained by a recently proposed weighted-data mixture model, while several options for the speaking turns dynamics are fulfilled by a multi-case transition model. The modules that translate raw audio and visual data into on-image observations are also described in detail. The performance of the proposed tracker is tested on challenging data-sets that are available from recent contributions which are used as baselines for comparison. mixture model, while several options for the speaking turns dynamics are fulfilled by a multi-case transition model. The modules that translate raw audio and visual data into on-image observations are also described in detail. The performance of the proposed tracker is tested on challenging data-sets that are available from recent contributions which are used as baselines for comparison."
  },
  "iccv2015_w20_persontrackingusingaudioanddepthcues": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Reconstruction and Understanding with Video and Sound",
    "title": "Person Tracking Using Audio and Depth Cues",
    "authors": [
      "Qingju Liu",
      "Teofilo de Campos",
      "Wenwu Wang",
      "Philip Jackson",
      "Adrian Hilton"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w20/html/Liu_Person_Tracking_Using_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w20/papers/Liu_Person_Tracking_Using_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, a novel probabilistic Bayesian tracking scheme is proposed and applied to bimodal measurements consisting of tracking results from the depth sensor and audio recordings collected using binaural microphones. We use random finite sets to cope with varying number of tracking targets. A measurement-driven birth process is integrated to quickly localize any emerging person. A new bimodal fusion method that prioritizes the most confident modality is employed. The approach was tested on real room recordings and experimental results show that the proposed combination of audio and depth outperforms individual modalities, particularly when there are multiple people talking simultaneously and when occlusions are frequent. "
  },
  "iccv2015_w21_tennisplayersegmentationforsemanticbehavioranalysis": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Tennis Player Segmentation for Semantic Behavior Analysis",
    "authors": [
      "Vito Reno",
      "Nicola Mosca",
      "Massimiliano Nitti",
      "Tiziana D'Orazio",
      "Donato Campagnoli",
      "Andrea Prati",
      "Ettore Stella"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Reno_Tennis_Player_Segmentation_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Reno_Tennis_Player_Segmentation_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Tennis player silhouette extraction is a preliminary step fundamental for any behavior analysis processing. Automatic systems for the evaluation of player tactics, in terms of position in the court, postures during the game and types of strokes, are highly desired for coaches and training purposes. These systems require accurate segmentation of players in order to apply posture analysis and high level semantic analysis. Background subtraction algorithms have been largely used in sportive context when fixed cameras are used. In this paper an innovative background subtraction algorithm is presented, which has been adapted to the tennis context and allows high precision in player segmentation both for the completeness of the extracted silhouettes. The algorithm is able to achieve interactive frame rates with up to 30 frames per second, and is suitable for smart cameras embedding. Real experiments demonstrate that the proposed approach is suitable in tennis contexts."
  },
  "iccv2015_w21_stroboscopicimagesynthesisofsportsplayerfromhand-heldcamerasequence": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Stroboscopic Image Synthesis of Sports Player From Hand-Held Camera Sequence",
    "authors": [
      "Kunihiro Hasegawa",
      "Hideo Saito"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Hasegawa_Stroboscopic_Image_Synthesis_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Hasegawa_Stroboscopic_Image_Synthesis_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " A method for synthesizing the stroboscopic image of a moving sports player from a hand-held camera sequence is proposed. In this method, a player is extracted from each frame of an input video sequence using a HOG-based people detector. After removing the bounding box of the extracted player from each frame, all frames are stitched together to generate a background image without the player. The player's area is then overlaid onto the stitched background image and a stroboscopic image of the player is synthesized. By using the bounding box of the player detected by HOG and by subtracting the images, computational speed and accuracy can be improved compared with the conventional method that segments the player's region. In addition to the stroboscopic image synthesis, we also remove the player's shadow to improve the appearance of the resultant stroboscopic image. Experimental results demonstrate the effectiveness of the proposed method."
  },
  "iccv2015_w21_soccerjerseynumberrecognitionusingconvolutionalneuralnetworks": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Soccer Jersey Number Recognition Using Convolutional Neural Networks",
    "authors": [
      "Sebastian Gerke",
      "Karsten Muller",
      "Ralf Schafer"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Gerke_Soccer_Jersey_Number_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Gerke_Soccer_Jersey_Number_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, a deep convolutional neural network based approach to the problem of automatically recognizing jersey numbers from soccer videos is presented. Two different jersey number vector encoding schemes are presented and compared to each other. The first treats every number as a separate class, while the second one treats each digit as a class. Additionally, the semi-automatic process for the annotation of a jersey number dataset consisting of8281 jersey numbers is described. The best recognition rate of 0.83 was achieved by the proposed approach with data augmentation and without using dropout, compared to 0.4 for a more traditional histogram of oriented gradients (HOG) and support vector machine (SVM) based approach. "
  },
  "iccv2015_w21_trackingwhenthecameralooksaway": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Tracking When the Camera Looks Away",
    "authors": [
      "Khurram Soomro",
      "Salman Khokhar",
      "Mubarak Shah"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Soomro_Tracking_When_the_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Soomro_Tracking_When_the_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Tracking players in sports videos presents numerous challenges due to weak distinguishing features and unpredictable motion. Considerable work has been done to track players in such videos using a combination of appearance and motion modeling, mostly in continuous streams of video. However, in a broadcast sports video, having advertisements, replays and intermittent change of camera view, it becomes a challenging task to keep track of players over an entire game. In this work, we solve a novel problem of tracking over a sequence of temporally disjoint soccer videos without the use of appearance cue, using a Graph based optimization approach. Each team is represented by a graph, in which the nodes correspond to player positions and the edge weights depend on spatial inter-player distance. We use team formation to associate tracks between clips and provide an end-to-end system that is able to perform statistical and tactical analysis of the game. We also introduce a new challenging dataset of an international soccer game. "
  },
  "iccv2015_w21_attributedgraphsfortrackingmultipleobjectsinstructuredsportsvideos": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Attributed Graphs for Tracking Multiple Objects in Structured Sports Videos",
    "authors": [
      "Henrique Morimitsu",
      "Roberto M. Cesar-Jr.",
      "Isabelle Bloch"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Morimitsu_Attributed_Graphs_for_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Morimitsu_Attributed_Graphs_for_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we propose a novel approach for tracking multiple object in structured sports videos using graphs. The objects are tracked by combining particle filter and frame description with Attributed Relational Graphs. We start by learning a probabilistic structural model graph from annotated images and then use it to evaluate and correct the current tracking state. Different from previous studies, our approach is also capable of using the learned model to generate new hypotheses of where the object is likely to be found after situations of occlusion or abrupt motion. We test the proposed method on two datasets: videos of table tennis matches extracted from YouTube and badminton matches from the ACASVA dataset. We show that all the players are successfully tracked even after they occlude each other or when there is a camera cut."
  },
  "iccv2015_w21_understandingsportactivitiesfromcorrespondencesofclusteredtrajectories": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Understanding Sport Activities From Correspondences of Clustered Trajectories",
    "authors": [
      "Francesco Turchini",
      "Lorenzo Seidenari",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Turchini_Understanding_Sport_Activities_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Turchini_Understanding_Sport_Activities_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Human activity recognition is a fundamental problem in computer vision with many applications such as video retrieval, automatic visual surveillance and human computer interaction. Sports represent one of the most viewed content on digital tv and the web. Automatically collected statistics of team sports game play represent actionable information for many end users such as coaches and broadcast speakers. Many computer vision methods applied to sport activity classification are often based on multi-camera setups, player tracking and exploit information on the ground-plane. In this work we overcome this limitations and propose an approach that exploits the spatio-temporal structure of a video grouping local spatio-temporal features unsupervisedly. Our robust representation allows to measure video similarity making correspondences among arbitrary patterns. We tested our method on two dataset of Volleyball and Soccer actions outperforming previous results by a large margin. Finally we show how our representation allows to highlight discriminative regions for each action."
  },
  "iccv2015_w21_audio-visualclassificationofsportstypes": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Audio-Visual Classification of Sports Types",
    "authors": [
      "Rikke Gade",
      "Mohamed Abou-Zleikha",
      "Mads Graesboll Christensen",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Gade_Audio-Visual_Classification_of_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Gade_Audio-Visual_Classification_of_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this work we propose a method for classification of sports types from combined audio and visual features extracted from thermal video. From audio Mel Frequency Cepstral Coefficients (MFCC) are extracted, and PCA are applied to reduce the feature space to 10 dimensions. From the visual modality short trajectories are constructed to represent the motion of players. From these, four motion features are extracted and combined directly with audio features for classification. A k-nearest neighbour classifier is applied for classification of 180 1-minute video sequences from three sports types. Using 10-fold cross validation a correct classification rate of 96.11% is obtained with multimodal features, compared to 86.67% and 90.00% using only visual or audio features, respectively."
  },
  "iccv2015_w21_injurymechanismclassificationinsoccervideos": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Injury Mechanism Classification in Soccer Videos",
    "authors": [
      "O.V. Ramana Murthy",
      "Roland Goecke"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Murthy_Injury_Mechanism_Classification_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Murthy_Injury_Mechanism_Classification_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Soccer is a very popular sport but also has a high rate of injuries. In this paper, player falling events in Soccer videos are classified into five major categories. These categories have been identified by Soccer coaches as the major mechanisms behind injuries of the players. Automatic detection of these events will be useful to the coaches to impart individual training to the players that will enhance their physical strength and also the playing style. A Bag-of-Words framework is used and a baseline classification accuracy is established."
  },
  "iccv2015_w21_predictingballownershipinbasketballfromamonocularviewusingonlyplayertrajectories": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Predicting Ball Ownership in Basketball From a Monocular View Using Only Player Trajectories",
    "authors": [
      "Xinyu Wei",
      "Long Sha",
      "Patrick Lucey",
      "Peter Carr",
      "Sridha Sridharan",
      "Iain Matthews"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Wei_Predicting_Ball_Ownership_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Wei_Predicting_Ball_Ownership_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Tracking objects like a basketball from a monocular view is challenging due to its small size, potential to move at high velocities as well as the high frequency of occlusion.However, humans with a deep knowledge of a game like basketball can predict with high accuracy the location of the ball even without seeing it due to the location and motion of nearby objects, as well as information of where it was last seen. Learning from tracking data is problematic however, due to the high variance in player locations.In this paper, we show that by simply ``permuting'' the multi-agent data we obtain a compact role-ordered feature which accurately predict the ball owner. We also show that our formulation can incorporate other information sources such as a vision-based ball detector to improve prediction accuracy. "
  },
  "iccv2015_w21_depthcompensationmodelforgazeestimationinsportanalysis": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Computer Vision in Sports",
    "title": "Depth Compensation Model for Gaze Estimation in Sport Analysis",
    "authors": [
      "Fabricio Batista Narcizo",
      "Dan Witzner Hansen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/html/Narcizo_Depth_Compensation_Model_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w21/papers/Narcizo_Depth_Compensation_Model_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " A depth compensation model is presented as a novel approach to reduce the effects of parallax error for head-mounted eye trackers. The method can reduce the parallax error when the distance between the user and the target is prior known. The model is geometrically presented and its performance is tested in a totally controlled environment with aim to check the influences of eye tracker parameters and ocular biometric parameters on its behavior. We also present a gaze estimation method based on epipolar geometry for binocular eye tracking setups. The depth compensation model has shown very promising to the field of eye tracking. It can reduce 10 times less the influence of parallax error in multiple depth planes."
  },
  "iccv2015_w22_3-dvolumetricshapeabstractionfromasingle2-dimage": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Representation and Recognition",
    "title": "3-D Volumetric Shape Abstraction From a Single 2-D Image",
    "authors": [
      "Pablo Sala",
      "Sven Dickinson"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/html/Sala_3-D_Volumetric_Shape_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/papers/Sala_3-D_Volumetric_Shape_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a novel approach to recovering the qualitative 3-D part structure from a single 2-D image. We do not assume any knowledge of the objects contained in the scene, but rather assume that they are composed from a user-defined vocabulary of qualitative 3-D volumetric part categories input to the system. Given a set of 2-D part hypotheses recovered from an image, representing projections of the surfaces of the 3-D part categories, our method simultaneously selects and groups subsets of the 2-D part hypotheses into 3-D part \"views\", from which the shape and pose parameters of the volumetric parts are recovered. The resulting 3-D parts and their relations offer the potential for a domain-independent, viewpoint-invariant shape indexing mechanism that can help manage the complexity of recognizing an object from a large database."
  },
  "iccv2015_w22_buildingtheviewgraphofacategorybyexploitingimagerealism": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Representation and Recognition",
    "title": "Building the View Graph of a Category by Exploiting Image Realism",
    "authors": [
      "Attila Szabo",
      "Andrea Vedaldi",
      "Paolo Favaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/html/Szabo_Building_the_View_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/papers/Szabo_Building_the_View_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We propose a weakly supervised method to arrange images of a given category based on the relative pose between the camera and the object in the scene. Relative poses are points on a sphere centered at the object in a given canonical pose, which we call object viewpoints. Our method builds a graph on this sphere by assigning images with similar viewpoint to the same node and by connecting nodes if they are related by a small rotation. The key idea is to exploit a large unlabeled dataset to validate the likelihood of dominant 3D planes of the object geometry. A number of 3D plane hypotheses are evaluated by applying small 3D rotations to each hypothesis and by measuring how well the deformed images match other images in the dataset. Correct hypotheses will result in deformed images that correspond to plausible views of the object, and thus will likely match well other images in the same category. The identified 3D planes are then used to compute affinities between images related by a change of viewpoint. We then use the affinities to build a view graph via a greedy method and the maximum spanning tree."
  },
  "iccv2015_w22_denserigidreconstructionfromunstructureddiscontinuousvideo": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Representation and Recognition",
    "title": "Dense Rigid Reconstruction From Unstructured Discontinuous Video",
    "authors": [
      "Karel Lebeda",
      "Simon Hadfield",
      "Richard Bowden"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/html/Lebeda_Dense_Rigid_Reconstruction_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/papers/Lebeda_Dense_Rigid_Reconstruction_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Although 3D reconstruction from a monocular video has been an active area of research for a long time, and the resulting models offer great realism and accuracy, strong conditions must be typically met when capturing the video to make this possible. This prevents general reconstruction of moving objects in dynamic, uncontrolled scenes. In this paper, we address this issue. We present a novel algorithm for modelling 3D shapes from unstructured, unconstrained discontinuous footage. The technique is robust against distractors in the scene, background clutter and even shot cuts. We show reconstructed models of objects, which could not be modelled by conventional Structure from Motion methods without additional input. Finally, we present results of our reconstruction in the presence of shot cuts, showing the strength of our technique at modelling from existing footage."
  },
  "iccv2015_w22_reconstructionofarticulatedobjectsfromamovingcamera": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Representation and Recognition",
    "title": "Reconstruction of Articulated Objects From a Moving Camera",
    "authors": [
      "Kaan Yucer",
      "Oliver Wang",
      "Alexander Sorkine-Hornung",
      "Olga Sorkine-Hornung"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/html/Yucer_Reconstruction_of_Articulated_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/papers/Yucer_Reconstruction_of_Articulated_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Many scenes that we would like to reconstruct contain articulated objects, and are often captured by only a single, non-fixed camera. Existing techniques for reconstructing articulated objects either require templates, which can be challenging to acquire, or have difficulties with perspective effects and missing data. In this paper, we present a novel reconstruction pipeline that first treats each feature point tracked on the object independently and incrementally imposes constraints. We make use of the idea that the unknown 3D trajectory of a point tracked in 2D should lie on a manifold that is described by the camera rays going through the tracked 2D positions. We compute an initial reconstruction by solving for latent 3D trajectories that maximize temporal smoothness on these manifolds. We then leverage these 3D estimates to automatically segment an object into piecewise rigid parts, and compute a refined shape and motion using sparse bundle adjustment. Finally, we apply kinematic constraints on automatically computed joint positions to enforce connectivity between different rigid parts, which further reduces ambiguous motion and increases reconstruction accuracy. Each step of our pipeline enforces temporal smoothness, and together results in a high quality articulated object reconstruction. We show the usefulness of our approach in both synthetic and real datasets and compare against other non-rigid reconstruction techniques."
  },
  "iccv2015_w22_geodesicconvolutionalneuralnetworksonriemannianmanifolds": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 3D Representation and Recognition",
    "title": "Geodesic Convolutional Neural Networks on Riemannian Manifolds",
    "authors": [
      "Jonathan Masci",
      "Davide Boscaini",
      "Michael M. Bronstein",
      "Pierre Vandergheynst"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/html/Masci_Geodesic_Convolutional_Neural_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w22/papers/Masci_Geodesic_Convolutional_Neural_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional neural networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract \"patches\", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use ShapeNet to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence."
  },
  "iccv2015_w24_asimplemethodforsubspaceestimationwithcorruptedcolumns": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "A Simple Method for Subspace Estimation With Corrupted Columns",
    "authors": [
      "Viktor Larsson",
      "Carl Olsson",
      "Fredrik Kahl"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Larsson_A_Simple_Method_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Larsson_A_Simple_Method_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper presents a simple and effective way of solving the robust subspace estimation problem where the corruptions are column-wise. The method we present can handle a large class of robust loss functions and is simple to implement. It is based on Iteratively Reweighted Least Squares (IRLS) and works in an iterative manner by solving a weighted least-squares rank-constrained problem in every iteration. By considering the special case of column-wise loss functions, we show that each such surrogate problem admits a closed form solution. Unlike many other approaches to subspace estimation, we make no relaxation of the low-rank constraint and our method is guaranteed to produce a subspace estimate with the correct dimension.Subspace estimation is a core problem for several applications in computer vision. We empirically demonstrate the performance of our method and compare it to several other techniques for subspace estimation. Experimental results are given for both synthetic and real image data including the following applications: linear shape basis estimation, plane fitting and non-rigid structure from motion."
  },
  "iccv2015_w24_dualprincipalcomponentpursuit": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Dual Principal Component Pursuit",
    "authors": [
      "Manolis C. Tsakiris",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Tsakiris_Dual_Principal_Component_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Tsakiris_Dual_Principal_Component_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": "We consider the problem of outlier rejection in single subspace learning. Classical approaches work directly with a low-dimensional representation of the subspace. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement. We pose this problem as an l_1-minimization problem on the sphere and show that, under certain conditions on the distribution of the data, any global minimizer of this non-convex problem gives a vector orthogonal to the subspace. Moreover, we show that such a vector can still be found by relaxing the non-convex problem with a sequence of linear programs. Experiments on synthetic and real data show that the proposed approach, which we call Dual Principal Component Pursuit (DPCP), outperforms state-of-the art methods, especially in the case of high-dimensional subspaces."
  },
  "iccv2015_w24_sparsesubspaceclusteringforincompleteimages": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Sparse Subspace Clustering for Incomplete Images",
    "authors": [
      "Xiao Wen",
      "Linbo Qiao",
      "Shiqian Ma",
      "Wei Liu",
      "Hong Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Wen_Sparse_Subspace_Clustering_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Wen_Sparse_Subspace_Clustering_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we propose a novel approach to cluster incomplete images leveraging sparse subspace structure and total variation regularization. Sparse subspace clustering obtains a sparse representation coefficient matrix for input data points by solving an l_1 minimization problem, and then uses the coefficient matrix to construct a sparse similarity graph over which spectral clustering is performed. However, conventional sparse subspace clustering methods are not exclusively designed to deal with incomplete images. To this end, our goal in this paper is to simultaneously recover incomplete images and cluster them into appropriate clusters. A new nonconvex optimization framework is established to achieve this goal, and an efficient first-order exact algorithm is developed to tackle the nonconvex optimization. Extensive experiments carried out on three public datasets show that our approach can restore and cluster incomplete images very well when up to 30% image pixels are missing."
  },
  "iccv2015_w24_filtratedspectralalgebraicsubspaceclustering": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Filtrated Spectral Algebraic Subspace Clustering",
    "authors": [
      "Manolis C. Tsakiris",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Tsakiris_Filtrated_Spectral_Algebraic_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Tsakiris_Filtrated_Spectral_Algebraic_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces. In practice, however, ASC is limited to equi-dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise. This paper proposes a new ASC algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions. The key ideas are (1) to construct, at each point, a decreasing sequence of subspaces containing the subspace passing through that point; (2) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity, which is superior to alternative affinities both in theory and in practice. Experiments on the Hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods."
  },
  "iccv2015_w24_poseandexpression-coherentfacerecoveryinthewild": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Pose and Expression-Coherent Face Recovery in the Wild",
    "authors": [
      "Xavier P. Burgos-Artizzu",
      "Joaquin Zepeda",
      "Francois Le Clerc",
      "Patrick Perez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Burgos-Artizzu_Pose_and_Expression-Coherent_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Burgos-Artizzu_Pose_and_Expression-Coherent_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a novel method to recover images of faces, particularly when large spatial regions of the face are unavailable due to data losses or occlusions. In contrast with previous work, we do not make assumptions on the data neither during training nor testing (such as assuming that the person was seen before or that all faces are perfectly aligned and have identical head pose, expression, etc.). Instead, we propose to tackle the problem in a purely unsupervised way, leveraging a large face dataset. During training, first we cluster faces based on their landmark's positions (obtained by an automatic face landmark estimator). Then, we model the face appearance for each group using sparse coding with learned dictionaries, with one dictionary per cluster. At test time, given a face to recover, we find its belonging cluster and occluded area and restore missing pixels by applying the group-specific sparse appearance representation learned during training. We show results on two \"in the wild\" datasets. Our method shows promising results on challenging faces and our sparse coding approach outperforms prior subspace learning techniques."
  },
  "iccv2015_w24_robustmatrixregressionforilluminationandocclusiontolerantfacerecognition": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Robust Matrix Regression for Illumination and Occlusion Tolerant Face Recognition",
    "authors": [
      "Jianchun Xie",
      "Jian Yang",
      "Jianjun Qian",
      "Ying Tai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Xie_Robust_Matrix_Regression_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Xie_Robust_Matrix_Regression_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Face recognition (FR) via regression analysis based classification has been widely applied in the past several years. In the existing regression methods, the testing image is represented as a linear combination of the training samples and the error image is converted into vector which is characterized by l1-norm or l2-norm. Therefore the two-dimensional structure of the error image is neglected in practice. In this paper, we operate on the two-dimensional image matrix directly, and propose a new face recognition method, namely Robust Matrix Regression (RMR). We perform the minimal weighted nuclear normconstraint on the representation error image as criterion to make full use of the low rank structural information. The proposed model is efficiently solved by an alternating direction method of multipliers (ADMM) and experimental results on public face databases demonstrate the effectiveness of our model in dealing with variations of occlusion and illumination."
  },
  "iccv2015_w24_imagesaliencydetectionwithsparserepresentationoflearnttextureatoms": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Image Saliency Detection With Sparse Representation of Learnt Texture Atoms",
    "authors": [
      "Lai Jiang",
      "Mai Xu",
      "Zhaoting Ye",
      "Zulin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Jiang_Image_Saliency_Detection_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Jiang_Image_Saliency_Detection_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper proposes a saliency detection method using a novel feature on sparse representation of learnt texture atoms (SR-LTA), which are encoded in salient and non-salient dictionaries. For salient dictionary, a novel formulation is proposed to learn salient texture atoms from image patches attracting extensive attention. Then, online salient dictionary learning (OSDL) algorithm is provided to solve the proposed formulation. Similarly, the non-salient dictionary can be learnt from image patches without any attention. A new pixel-wise feature, namely SR-LTA, is yielded based on the difference of sparse representation errors regarding the learnt salient and non-salient dictionaries.Finally, image saliency can be predicted via linear combination of the proposed SR-LTA feature and conventional features, i.e., luminance and contrast. For the linear combination, the weights corresponding to different feature channels are determined by least square estimation on the training data. The experimental results show that our method outperforms several state-of-the-art saliency detection methods."
  },
  "iccv2015_w24_objectextractionfromboundingboxpriorwithdoublesparsereconstruction": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Object Extraction From Bounding Box Prior With Double Sparse Reconstruction",
    "authors": [
      "Lingzheng Dai",
      "Jundi Ding",
      "Jian Yang",
      "Fanlong Zhang",
      "Junxia Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Dai_Object_Extraction_From_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Dai_Object_Extraction_From_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Extracting objects from natural images has long been an active problem in image processing. Despite various attempts, it has not been completely solved up to date. Current state-of-the-art object proposal methods tend to extract a set of object segments from an image, and often these are consequential differences among these results for each image. Another type of methods strive to detect one object into a bounding box where some background parts are often covered. For these two methodologies, we observe: 1) there are generally some regions overlapped among different proposals, which are usually from one object; they could be as object `segment hypotheses'; 2) pixels outside the detected bounding box could be as `background hypotheses' as they are with high probability from the background. With them, we formulate the object extraction as a ``double\" sparse reconstruction problem in terms of the bounding box results. The idea is that object regions should be with small reconstruction errors to segment hypotheses bases, simultaneously, they should have large reconstruction errors to background hypotheses bases. Comprehensive experiments and evaluations on PASCAL VOC object segmentation dataset and GrabCut-50 database demonstrate the superiority of our built method. In particular, we achieve the state-of-the-art performance for the object segmentation with bounding box prior on these two benchmark datasets."
  },
  "iccv2015_w24_visualtrackingvianonnegativeregularizationmultiplelocalitycoding": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Visual Tracking via Nonnegative Regularization Multiple Locality Coding",
    "authors": [
      "Fanghui Liu",
      "Tao Zhou",
      "Jie Yang",
      "Irene Y.H. Gu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Liu_Visual_Tracking_via_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Liu_Visual_Tracking_via_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " This paper presents a novel object tracking method based on approximated Locality-constrained Linear Coding (LLC). Rather than using a non-negativity constraint on encoding coefficients to guarantee these elements nonnegative, in this paper, the non-negativity constraint is substituted for a conventional l2 norm regularization term in approximated LLC to obtain the similar nonnegative effect. And we provide a detailed and adequate explanation in theoretical analysis to clarify the rationality of this replacement. Instead of specifying fixed K nearest neighbors to construct the local dictionary, a series of different dictionaries with pre-defined numbers of nearest neighbors are selected. Weights of these various dictionaries are also learned from approximated LLC in the similar framework. In order to alleviate tracking drifts, we propose a simple and efficient occlusion detection method. The occlusion detection criterion mainly depends on whether negative templates are selected to represent the severe occluded target. Both qualitative and quantitative evaluations on several challenging sequences show that the proposed tracking algorithm achieves favorable performance compared with other state-of-the-art methods."
  },
  "iccv2015_w24_multi-resolutiondynamicmodedecompositionforforeground/backgroundseparationandobjecttracking": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Multi-Resolution Dynamic Mode Decomposition for Foreground/Background Separation and Object Tracking",
    "authors": [
      "J. Nathan Kutz",
      "Xing Fu",
      "Steve L. Brunton",
      "N. Benjamin Erichson"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Kutz_Multi-Resolution_Dynamic_Mode_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Kutz_Multi-Resolution_Dynamic_Mode_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We demonstrate that the integration of the recently developed dynamic mode decomposition with a multi-resolution analysis allows for a decomposition of video streams into multi-time scale features and objects. A one-level separation allows for background (low-rank) and foreground (sparse) separation of the video, or robust principal component analysis.Further iteration of the method allows a video data set to be separated into objects moving at different rates against the slowly varying background, thus allowing for multiple-target tracking and detection.The algorithm is computationally efficient and can be integrated with many further innovations including compressive sensing architectures and GPU algorithms. "
  },
  "iccv2015_w24_backgroundsubtractionviasuperpixel-basedonlinematrixdecompositionwithstructuredforegroundconstraints": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Background Subtraction via Superpixel-Based Online Matrix Decomposition With Structured Foreground Constraints",
    "authors": [
      "Sajid Javed",
      "Seon Ho Oh",
      "Andrews Sobral",
      "Thierry Bouwmans",
      "Soon Ki Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Javed_Background_Subtraction_via_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Javed_Background_Subtraction_via_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Background subtraction process plays a very essential role for various computer vision tasks. The process becomes more critical when the input scene contains variation of pixels such as swaying trees, rippling of water, illumination variations, etc. Recent methods of matrix decomposition into low-rank (e.g., corresponds to the background) and sparse (e.g., constitutes the moving objects) components such as Robust Principal Component Analysis (RPCA), have been shown to be very efficient framework for background subtraction. However, when the size of the input data grows and due to the lack of sparsity-constraints, these methods cannot cope with the real-time challenges and always show a weak performance due to the erroneous foreground regions. In order to address the above mentioned issues, this paper presents a superpixel-based matrix decomposition method together with maximum norm (max-norm) regularizations and structured sparsity constraints. The low-rank component estimated from each homogeneous region is more perfect, reliable, and efficient, since each superpixel provides different characteristics with a reduced value of rank. Online max-norm based matrix decomposition is employed on each segmented superpixel to separate the low rank and initial outliers support. And then, the structured sparsity constraints such as the generalized fussed lasso (GFL) are adopted for exploiting structural information continuously as the foreground pixels are both spatially connected and sparse. We propose an online single unified optimization framework for detecting foreground and learning the background model simultaneously. Rigorous experimental evaluations on challenging datasets demonstrate the superior performance of the proposed scheme in terms of both accuracy and computational time. "
  },
  "iccv2015_w24_adaptivelowrankapproximationfortensors": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Adaptive Low Rank Approximation for Tensors",
    "authors": [
      "Xiaofei Wang",
      "Carmeliza Navasca"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Wang_Adaptive_Low_Rank_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Wang_Adaptive_Low_Rank_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we propose a novel framework for finding low rank approximation of a given tensor. This framework is based on the adaptive lasso with coefficient weights for sparse computation in tensor rank detection. We also provide an algorithm for solving the adaptive lasso model problem for tensor approximation. In a special case, the convergence of the algorithm and the probabilistic consistency of the sparsity have been addressed [16] when each weight equals to one. The method is applied to background extraction and video compression problems."
  },
  "iccv2015_w24_onlinestochastictensordecompositionforbackgroundsubtractioninmultispectralvideosequences": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Robust Subspace Learning and Computer Vision",
    "title": "Online Stochastic Tensor Decomposition for Background Subtraction in Multispectral Video Sequences",
    "authors": [
      "Andrews Sobral",
      "Sajid Javed",
      "Soon Ki Jung",
      "Thierry Bouwmans",
      "El-hadi Zahzah"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/html/Sobral_Online_Stochastic_Tensor_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w24/papers/Sobral_Online_Stochastic_Tensor_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Background subtraction is an important task for visual surveillance systems. However, this task becomes more complex when the data size grows since the real-world scenario requires larger data to be processed in a more efficient way, and in some cases, in a continuous manner. Until now, most of background subtraction algorithms were designed for mono or trichromatic cameras within the visible spectrum or near infrared part. Recent advances in multispectral imaging technologies give the possibility to record multispectral videos for video surveillance applications. Due to the specific nature of these data, many of the bands within multispectral images are often strongly correlated. In addition, processing multispectral images with hundreds of bands can be computationally burdensome. In order to address these major difficulties of multispectral imaging for video surveillance, this paper propose an online stochastic framework for tensor decomposition of multispectral video sequences (OSTD). First, the experimental evaluations on synthetic generated data show the robustness of the OSTD with other state of the art approaches then, we apply the same idea on seven multispectral video bands to show that only RGB features are not sufficient to tackle color saturation, illumination variations and shadows problem, but the addition of six visible spectral bands together with one near infra-red spectra provides a better background/foreground separation."
  },
  "iccv2015_w25_offlinedeformablefacetrackinginarbitraryvideos": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 300 Videos in the Wild: Facial Landmark Tracking in-the-Wild",
    "title": "Offline Deformable Face Tracking in Arbitrary Videos",
    "authors": [
      "Grigoris G. Chrysos",
      "Epameinondas Antonakos",
      "Stefanos Zafeiriou",
      "Patrick Snape"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/html/Chrysos_Offline_Deformable_Face_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/papers/Chrysos_Offline_Deformable_Face_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Generic face detection and facial landmark localization in static imagery are among the most mature and well-studied problems in machine learning and computer vision. Currently, the top performing face detectors achieve a true positive rate of around 75-80% whilst maintaining low false positive rates. Furthermore, the top performing facial landmark localization algorithms obtain low point-to-point errors for more than 70% of commonly benchmarked images captured under unconstrained conditions. The task of facial landmark tracking in videos, however, has attracted much less attention. Generally, a tracking-by-detection framework is applied, where face detection and landmark localization are employed in every frame in order to avoid drifting. Thus, this solution is equivalent to landmark detection in static imagery. Empirically, a straightforward application of such a framework cannot achieve higher performance, on average, than the one reported for static imagery. In this paper, we show for the first time, to the best of our knowledge, that the results of generic face detection and landmark localization can be used to recursively train powerful and accurate person-specific face detectors and landmark localization methods for offline deformable tracking. The proposed pipeline can track landmarks in very challenging long-term sequences captured under arbitrary conditions. The pipeline was used as a semi-automatic tool to annotate the majority of the videos of the 300-VW Challenge."
  },
  "iccv2015_w25_faciallandmarktrackingbytree-baseddeformablepartmodelbaseddetector": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 300 Videos in the Wild: Facial Landmark Tracking in-the-Wild",
    "title": "Facial Landmark Tracking by Tree-Based Deformable Part Model Based Detector",
    "authors": [
      "Michal Uricar",
      "Vojtech Franc",
      "Vaclav Hlavac"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/html/Uricar_Facial_Landmark_Tracking_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/papers/Uricar_Facial_Landmark_Tracking_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we describe a tracker of facial landmarks submitted to the 300 Videos in the Wild (300-VW) challenge. Our tracker is a straightforward extension of a well tuned tree-based DPM landmark detector originally developed for static images. The tracker is obtained by applying the static detector independently in each frame and using the Kalman filter to smooth estimates of the face positions as well as to compensate possible failures of the face detector. The resulting tracker provides a robustestimate of 68 landmarks running at 5 fps on an ordinary PC. We provide an open-source implementation of the proposed tracker at (http://cmp.felk.cvut.cz/ uricamic/clandmark/)."
  },
  "iccv2015_w25_multi-viewconstrainedlocalmodelsforlargeheadanglefacialtracking": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 300 Videos in the Wild: Facial Landmark Tracking in-the-Wild",
    "title": "Multi-View Constrained Local Models for Large Head Angle Facial Tracking",
    "authors": [
      "Georgia Rajamanoharan",
      "Timothy F. Cootes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/html/Rajamanoharan_Multi-View_Constrained_Local_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/papers/Rajamanoharan_Multi-View_Constrained_Local_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We propose Multi-View Constrained Local Models - a simple but effective technique for improving facial point detection under large head angles, such as in a car driving setting. Our approach combines a global shape model with separate sets of response maps targeted at different head angles, indexed on the shape model parameters. We explore shape-space division strategies and show that, as well as outperforming the traditional method, our approach also provides a marked speed-up which demonstrates the suitability of this technique for real-time face tracking."
  },
  "iccv2015_w25_shapeaugmentedregressionmethodforfacealignment": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 300 Videos in the Wild: Facial Landmark Tracking in-the-Wild",
    "title": "Shape Augmented Regression Method for Face Alignment",
    "authors": [
      "Yue Wu",
      "Qiang Ji"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/html/Wu_Shape_Augmented_Regression_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/papers/Wu_Shape_Augmented_Regression_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " There have been tremendous improvements of the face alignment algorithms, among which the regression framework becomes the most popular one recently. The regression based works start from an initial face shape, and they learn regression models to predict the face shape updates based on the shape-indexed local appearance features. However, most of the regression methods ignore the fact that the regression function should directly rely on the current shape (e.g. regression function for frontal face should be different from that for the left profile face). To utilize this information and improve over the existing regression based methods, we propose the shape augmented regression method for face alignment where the regression function would automatically change for different face shapes. We evaluated the performance of the proposed method on both the general \"in-the-wild\" database and the 300 Video in the Wild (300-VW) challenge data set. The results show that the proposed method outperforms the state-of-the-art works."
  },
  "iccv2015_w25_faciallandmarkdetectionviaprogressiveinitialization": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 300 Videos in the Wild: Facial Landmark Tracking in-the-Wild",
    "title": "Facial Landmark Detection via Progressive Initialization",
    "authors": [
      "Shengtao Xiao",
      "Shuicheng Yan",
      "Ashraf A. Kassim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/html/Xiao_Facial_Landmark_Detection_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/papers/Xiao_Facial_Landmark_Detection_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we present a multi-stage regression-based approach for the 300 Videos in-the-Wild (300-VW) Challenge, which progressively initializes the shape from obvious landmarks with strong semantic meanings, e.g. eyes and mouth corners, to landmarks on face contour, eyebrows and nose bridge which have more challenging features. Compared with initialization based on mean shape and multiple random shapes, our proposed progressive initialization can very robustly handle challenging poses. It also guarantees an accurate landmark localization result and shows smooth tracking performance in real-time."
  },
  "iccv2015_w25_facialshapetrackingviaspatio-temporalcascadeshaperegression": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - 300 Videos in the Wild: Facial Landmark Tracking in-the-Wild",
    "title": "Facial Shape Tracking via Spatio-Temporal Cascade Shape Regression",
    "authors": [
      "Jing Yang",
      "Jiankang Deng",
      "Kaihua Zhang",
      "Qingshan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/html/Yang_Facial_Shape_Tracking_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w25/papers/Yang_Facial_Shape_Tracking_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper, we develop a spatio-temporal cascade shape regression (STCSR) model for robust facial shape tracking. It is different from previous works in three aspects. Firstly, a multi-view cascade shape regression (MCSR) model is employed to decrease the shape variance in shape regression model construction, which is able to make the learned regression model more robust to shape variances. Secondly, a time series regression (TSR) model is explored to enhance the temporal consecutiveness between adjacent frames. Finally, a novel re-initialization mechanism is adopted to effectively and accurately locate the face when it is misaligned or lost. Extensive experiments on the 300 Videos in the Wild (300-VW) demonstrate the superior performance of our algorithm."
  },
  "iccv2015_w27_scalablesketch-basedimageretrievalusingcolorgradientfeatures": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Web-Scale Vision and Social Media",
    "title": "Scalable Sketch-Based Image Retrieval Using Color Gradient Features",
    "authors": [
      "Tu Bui",
      "John Collomosse"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w27/html/Bui_Scalable_Sketch-Based_Image_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w27/papers/Bui_Scalable_Sketch-Based_Image_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " We present a scalable system for sketch-based image retrieval (SBIR), extending the state of the art Gradient Field HoG (GF-HoG) retrieval framework through two technical contributions. First, we extend GF-HoG to enable color-shape retrieval and comprehensively evaluate several early- and late-fusion approaches for integrating the modality of color, considering both the accuracy and speed of sketch retrieval. Second, we propose an efficient inverse-index representation for GF-HoG that delivers scalable search with interactive query times over millions of images.A mobile app demo accompanies this paper (Android) - see play.google.com/store/apps/details?id=com.collomosse.sketcher "
  },
  "iccv2015_w27_fisherencodedconvolutionalbag-of-windowsforefficientimageretrievalandsocialimagetagging": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Web-Scale Vision and Social Media",
    "title": "Fisher Encoded Convolutional Bag-Of-Windows for Efficient Image Retrieval and Social Image Tagging",
    "authors": [
      "Tiberio Uricchio",
      "Marco Bertini",
      "Lorenzo Seidenari",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w27/html/Uricchio_Fisher_Encoded_Convolutional_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w27/papers/Uricchio_Fisher_Encoded_Convolutional_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " In this paper we present an efficient and accurate method to aggregate a set of Deep Convolutional Neural Network (CNN) responses, extracted from a set of image windows. CNN features are usually computed on the whole frame or with a dense multi scale approach. There is evidence that using multiple windows yields a better image representation nonetheless it is still not clear how windows should be sampled and how CNN responses should be aggregated. Instead of sampling the image densely in scale and space we show that selecting a few hundred windows is enough to obtain an effective image signature. We show how to use Fisher Vectors and PCA to obtain a short and highly descriptive signature that can be used effectively for image retrieval. We test our method on two relevant computer vision tasks: image retrieval and image tagging. We report state-of-the art results for both tasks on three standard datasets."
  },
  "iccv2015_w27_geometricminingscalinggeometrichashingtolargedatasets": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Web-Scale Vision and Social Media",
    "title": "Geometric Mining: Scaling Geometric Hashing to Large Datasets",
    "authors": [
      "Andrew Gilbert",
      "Richard Bowden"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w27/html/Gilbert_Geometric_Mining_Scaling_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w27/papers/Gilbert_Geometric_Mining_Scaling_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " It is known that relative feature location is important in representing objects, but assumptions that make learning tractable often simplify how structure is encoded e.g. spatial pooling or star models. For example, techniques such as spatial pyramid matching (SPM), in-conjunction with machine learning techniques perform well. However, there are limitations to such spatial encoding schemes which discard important information about the layout of features. In contrast, we propose to use the object itself to choose the basis of the features in an object centric approach. In doing so we return to the early work of geometric hashing but demonstrate how such approaches can be scaled-up to modern day object detection challenges in terms of both the number of examples and their variability. We apply a two stage process; initially filtering background features to localise the objects and then hashing the remaining pairwise features in an affine invariant model. During learning, we identify class-wise key feature predictors.We validate our detection and classification of objects on the PASCAL VOC'07 and '11 and CarDb datasets and compare with state of the art detectors and classifiers. Importantly we demonstrate how structure in features can be efficiently identified and how its inclusion can increase performance. This feature centric learning technique allows us to localise objects even without object annotation during training and the resultant segmentation provides accurate state of the art object localization, without the need for annotations."
  },
  "iccv2015_w28_singleframebasedvideogeo-localisationusingstructureprojection": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Vision from Satellite to Street",
    "title": "Single Frame Based Video Geo-Localisation Using Structure Projection",
    "authors": [
      "Christoph Bodensteiner",
      "Sebastian Bullinger",
      "Simon Lemaire",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w28/html/Bodensteiner_Single_Frame_Based_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w28/papers/Bodensteiner_Single_Frame_Based_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Community image and video platforms like FlickR and Youtube offer large image collections from different perspectives. However, the majority of publicly available imagery from online communities lack a reasonable exact location and orientation information, which is important for many geo-spatial applications like object geo-referencing, knowledge transfer or augmented reality. In this work we exploit publicly available drone videos in order to bridge the gap between ground and aerial imagery. We propose a framework for the fast determination of full 6-D georeferenced motion trajectories of online community drone video footage using geo-localized map data. Our method requires the registration of a single video frame from a video sequence in order to exactly geo-reference complete motion trajectories w.r.t. to existing geo-referenced map data. The method relies on SfM and SLAM techniques in combination with a simple, yet efficient appearance and structure matching based on rendered map data (e.g. LiDAR) in order to generate geo-registered 3D feature maps. These maps enable a simple and fast global appearance based geo-registration of visually overlapping community videos and images. We evaluate our method on a large set of community drone videos. Our method produces drift free geo-data overlays at an average speed of 29,7 frames per second with an average positional error of 0,4m. In addition we release a large scale processed LiDAR dataset and geo-registered feature maps as an extension to the converging perspectives dataset. This data may provide visual links from ground based sensors to aerial imagery. Possible applications are numerous and include autonomous navigation, map updating/extension, image and video dehazing, object localisation or augmented reality."
  },
  "iccv2015_w28_semanticcross-viewmatching": {
    "conf_id": "ICCV2015",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2015_workshops - Vision from Satellite to Street",
    "title": "Semantic Cross-View Matching",
    "authors": [
      "Francesco Castaldo",
      "Amir Zamir",
      "Roland Angst",
      "Francesco Palmieri",
      "Silvio Savarese"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w28/html/Castaldo_Semantic_Cross-View_Matching_ICCV_2015_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2015_workshops/../content_iccv_2015_workshops/w28/papers/Castaldo_Semantic_Cross-View_Matching_ICCV_2015_paper.pdf",
    "published": "2015-12",
    "summary": " Matching cross-view images is challenging because the appearance and viewpoints are significantly different. While low-level features based on gradient orientations or filter responses can drastically vary with such changes in viewpoint, semantic information of images however shows an invariant characteristic in this respect. Consequently, semantically labeled regions can be used for performing cross-view matching. In this paper, we therefore explore this idea and propose an automatic method for detecting and representing the semantic information of an RGB image with the goal of performing cross-view matching with a (non-RGB) geographic information system (GIS). A segmented image forms the input to our system with segments assigned to semantic concepts such as traffic signs, lakes, roads, foliage, etc. We design a descriptor to robustly capture both, the presence of semantic concepts and the spatial layout of those segments.Pairwise distances between the descriptors extracted from the GIS map and the query image are then used to generate a shortlist of the most promising locations with similar semantic concepts in a consistent spatial layout. An experimental evaluation with challenging query images and a large urban area shows promising results."
  }
}