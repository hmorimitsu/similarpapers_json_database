{
  "iclr2016_workshop_deepmotifvisualizinggenomicsequenceclassifications": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Motif: Visualizing Genomic Sequence Classifications",
    "authors": [
      "Jack Lanchantin",
      "Ritambhara Singh",
      "Zeming Lin",
      "& Yanjun Qi"
    ],
    "page_url": "https://openreview.net/forum?id=k80kv3zjGtOYKX7ji4V7",
    "pdf_url": "https://openreview.net/pdf?id=k80kv3zjGtOYKX7ji4V7",
    "published": "2016-05",
    "summary": "This paper applies a deep convolutional/highway MLP framework to classify genomic sequences on the transcription factor binding site task. To make the model understandable, we propose an optimization driven strategy to extract \u201cmotifs\u201d, or symbolic patterns which visualize the positive class learned by the network. We show that our system, Deep Motif (DeMo), extracts motifs that are similar to, and in some cases outperform the current well known motifs. In addition, we find that a deeper model consisting of multiple convolutional and highway layers can outperform a single convolutional and fully connected layer in the previous state-of-the-art."
  },
  "iclr2016_workshop_lookaheadconvolutionlayerforunidirectionalrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks",
    "authors": [
      "Chong Wang",
      "Dani Yogatama",
      "Adam Coates",
      "Tony Han",
      "Awni Hannun",
      "Bo Xiao"
    ],
    "page_url": "https://openreview.net/forum?id=91EowxONgIkRlNvXUVog",
    "pdf_url": "https://openreview.net/pdf?id=91EowxONgIkRlNvXUVog",
    "published": "2016-05",
    "summary": "Recurrent neural networks (RNNs) have been shown to be very effective for many sequential prediction problems such as speech recognition, machine translation, part-of-speech tagging, and others. The best variant is typically a bidirectional RNN that learns representation for a sequence by performing a forward and a backward pass through the entire sequence. However, unlike unidirectional RNNs, bidirectional RNNs are challenging to deploy in an online and low-latency setting (e.g., in a speech recognition system), because they need to see an entire sequence before making a prediction. We introduce a lookahead convolution layer that incorporates information from future subsequences in a computationally efficient manner to improve unidirectional recurrent neural networks. We evaluate our method on speech recognition tasks for two languages---English and Chinese. Our experiments show that the proposed method outperforms vanilla unidirectional RNNs and is competitive with bidirectional RNNs in terms of character and word error rates."
  },
  "iclr2016_workshop_jointstochasticapproximationlearningofhelmoltzmachines": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Joint Stochastic Approximation Learning of Helmoltz Machines",
    "authors": [
      "HaotianXu",
      "Zhijian Ou"
    ],
    "page_url": "https://openreview.net/forum?id=XL9vKJ98DCXB8D1RUGV0",
    "pdf_url": "https://openreview.net/pdf?id=XL9vKJ98DCXB8D1RUGV0",
    "published": "2016-05",
    "summary": "Though with progress, model learning and performing posterior inference still re- mains a common challenge for using deep generative models, especially for han- dling discrete hidden variables. This paper is mainly concerned with algorithms for learning Helmholz machines, which is characterized by pairing the genera- tive model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we successfully develop a new class of al- gorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously mini- mize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the JSA\u2019s performance is consis- tently superior to that of competing algorithms like RWS, for learning a range of difficult models."
  },
  "iclr2016_workshop_aminimalisticapproachtosum-productnetworklearningforrealapplications": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Minimalistic Approach to Sum-Product Network Learning for Real Applications",
    "authors": [
      "Viktoriya Krakovna",
      "Moshe Looks"
    ],
    "page_url": "https://openreview.net/forum?id=BNYYGWVA1F7PwR1riED4",
    "pdf_url": "https://openreview.net/pdf?id=BNYYGWVA1F7PwR1riED4",
    "published": "2016-05",
    "summary": "Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features."
  },
  "iclr2016_workshop_hardware-orientedapproximationofconvolutionalneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Hardware-Oriented Approximation of Convolutional Neural Networks",
    "authors": [
      "Philipp Gysel",
      "Mohammad Motamedi",
      "Soheil Ghiasi"
    ],
    "page_url": "https://openreview.net/forum?id=81DnLL9OEI6O2Pl0UV1w",
    "pdf_url": "https://openreview.net/pdf?id=81DnLL9OEI6O2Pl0UV1w",
    "published": "2016-05",
    "summary": "High computational complexity hinders the widespread usage of Convolutional Neural Networks (CNNs), especially in mobile devices. Hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. In this paper we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available."
  },
  "iclr2016_workshop_neurogenicdeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neurogenic Deep Learning",
    "authors": [
      "Timothy J. Draelos",
      "Nadine E. Miner",
      "Jonathan A. Cox",
      "Christopher C. Lamb",
      "Conrad D. James",
      "James B. Aimone"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xQwqKZh5Pk8ELfEzD",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xQwqKZh5Pk8ELfEzD",
    "published": "2016-05",
    "summary": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. "
  },
  "iclr2016_workshop_deepbayesianneuralnetsasdeepmatrixgaussianprocesses": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes",
    "authors": [
      "Christos Louizos",
      "Max Welling"
    ],
    "page_url": "https://openreview.net/forum?id=wVqzjWP0JfG0qV7mtLvp",
    "pdf_url": "https://openreview.net/pdf?id=wVqzjWP0JfG0qV7mtLvp",
    "published": "2016-05",
    "summary": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. "
  },
  "iclr2016_workshop_neuralnetworktrainingvariationsinspeechandsubsequentperformanceevaluation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Network Training Variations in Speech and Subsequent Performance Evaluation",
    "authors": [
      "Ewout van den Berg",
      "Bhuvana Ramabhadran",
      "Michael Picheny"
    ],
    "page_url": "https://openreview.net/forum?id=OM0jKROjrFp57ZJjtNkv",
    "pdf_url": "https://openreview.net/pdf?id=OM0jKROjrFp57ZJjtNkv",
    "published": "2016-05",
    "summary": "In this work we study variance in the results of neural network training on a wide variety of configurations in automatic speech recognition. Although this variance itself is well known, this is, to the best of our knowledge, the first paper that performs an extensive empirical study on its effects in speech recognition. We view training as sampling from a distribution and show that these distributions can have a substantial variance. These observations have important implications on way results in the literature are reported and interpreted. "
  },
  "iclr2016_workshop_neuralvariationalrandomfieldlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Variational Random Field Learning",
    "authors": [
      "Volodymyr Kuleshov",
      "Stefano Ermon"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9x1mJ3zS5Pk8ELfEjD",
    "pdf_url": "https://openreview.net/pdf?id=ZY9x1mJ3zS5Pk8ELfEjD",
    "published": "2016-05",
    "summary": "We propose variational bounds on the log-likelihood of an undirected probabilistic graphical model p that are parametrized by flexible approximating distributions q. These bounds are tight when q = p, are convex in the parameters of q for interesting classes of q, and may be further parametrized by an arbitrarily complex neural network. When optimized jointly over q and p, our bounds enable us to accurately track the partition function during learning."
  },
  "iclr2016_workshop_improvingvariationalinferencewithinverseautoregressiveflow": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Improving Variational Inference with Inverse Autoregressive Flow",
    "authors": [
      "Diederik P. Kingma",
      "Tim Salimans",
      "Max Welling"
    ],
    "page_url": "https://openreview.net/forum?id=ANYzpXg3LcNrwlgXCq9G",
    "pdf_url": "https://openreview.net/pdf?id=ANYzpXg3LcNrwlgXCq9G",
    "published": "2016-05",
    "summary": "We propose a simple and practical method for improving the flexibility of the approximate posterior in variational auto-encoders (VAEs) through a transformation with autoregressive networks.Autoregressive networks, such as RNNs and RNADE networks, are very powerful models. However, their sequential nature makes them impractical for direct use with VAEs, as sequentially sampling the latent variables is slow when implemented on a GPU. Fortunately, we find that by inverting autoregressive networks we can obtain equally powerful data transformations that can be computed in parallel. We call these data transformations inverse autoregressive flows (IAF), and we show that they can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is computationally cheap, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series.The method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images."
  },
  "iclr2016_workshop_learninggenomicrepresentationstopredictclinicaloutcomesincancer": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Genomic Representations to Predict Clinical Outcomes in Cancer",
    "authors": [
      "Safoora Yousefi",
      "Congzheng Song",
      "Nelson Nauata",
      "Lee Cooper"
    ],
    "page_url": "https://openreview.net/forum?id=xnrAg7jmLF1m7RyVi3vG",
    "pdf_url": "https://openreview.net/pdf?id=xnrAg7jmLF1m7RyVi3vG",
    "published": "2016-05",
    "summary": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract we present an investigation in learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data."
  },
  "iclr2016_workshop_understandingverydeepnetworksviavolumeconservation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Understanding Very Deep Networks via Volume Conservation",
    "authors": [
      "Thomas Unterthiner",
      "Sepp Hochreiter"
    ],
    "page_url": "https://openreview.net/forum?id=ROVmN8wyOSvnM0J1IpNm",
    "pdf_url": "https://openreview.net/pdf?id=ROVmN8wyOSvnM0J1IpNm",
    "published": "2016-05",
    "summary": "Recently, very deep neural networks set new records across many application domains, like Residual Networks at the ImageNet challenge and Highway Networks at language processing tasks. We expect further excellent performance improvements in different fields from these very deep networks. However these networks are still poorly understood, especially since they rely on non-standard architectures. In this contribution we analyze the learning dynamics which are required for successfully training very deep neural networks. For the analysis we use a symplectic network architecture which inherently conserves volume when mapping a representation from one to the next layer. Therefore it avoids the vanishing gradient problem, which in turn allows to effectively train thousands of layers. We consider highway and residual networks as well as the LSTM model, all of which have approximately volume conserving mappings. We identified two important factors for making deep architectures working: (1) (near) volume conserving mappings through $x = x + f(x)$ or similar (cf.\\ avoiding the vanishing gradient); (2) Controlling the drift effect, which increases/decreases $x$ during propagation toward the output (cf.\\ avoiding bias shifts);"
  },
  "iclr2016_workshop_fixedpointquantizationofdeepconvolutionalnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Fixed Point Quantization of Deep Convolutional Networks",
    "authors": [
      "Darryl D. Lin",
      "Sachin S. Talathi",
      "V. Sreekanth Annapureddy"
    ],
    "page_url": "https://openreview.net/forum?id=yovBjmpo1ur682gwszM7",
    "pdf_url": "https://openreview.net/pdf?id=yovBjmpo1ur682gwszM7",
    "published": "2016-05",
    "summary": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we formulate and solve an optimization problem to identify the optimal fixed point bit-width allocation across layers to enable efficient fixed point implementation of DCNs. Our experiments show that in comparison to equal bit-width settings, optimized bit-width allocation offers >20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark."
  },
  "iclr2016_workshop_cma-esforhyperparameteroptimizationofdeepneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks",
    "authors": [
      "Ilya Loshchilov",
      "Frank Hutter"
    ],
    "page_url": "https://openreview.net/forum?id=xnrA4qzmPu1m7RyVi38Z",
    "pdf_url": "https://openreview.net/pdf?id=xnrA4qzmPu1m7RyVi38Z",
    "published": "2016-05",
    "summary": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization.As an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. "
  },
  "iclr2016_workshop_understandingvisualconceptswithcontinuationlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Understanding Visual Concepts with Continuation Learning",
    "authors": [
      "William F. Whitney",
      "Michael Chang",
      "Tejas Kulkarni",
      "Joshua B. Tenenbaum"
    ],
    "page_url": "https://openreview.net/forum?id=r8lrDJ89Pf8wknpYt5zq",
    "pdf_url": "https://openreview.net/pdf?id=r8lrDJ89Pf8wknpYt5zq",
    "published": "2016-05",
    "summary": "We introduce a neural network architecture and a learning algorithm to produce fac- torized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from previous frame, and let the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games."
  },
  "iclr2016_workshop_input-convexdeepnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Input-Convex Deep Networks",
    "authors": [
      "(moved to May 3rd)"
    ],
    "page_url": "https://openreview.net/forum?id=ROVmA279BsvnM0J1IpNn",
    "pdf_url": "https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn",
    "published": "2016-05",
    "summary": "This paper introduces a new class of neural networks that we refer to as input-convex neural networks, networks that are convex in their inputs (as opposed to their parameters).We discuss the nature and representational power of these networks, illustrate how the prediction (inference) problem can be solved via convex optimization, and discuss their application to structured prediction problems.We highlight a few simple examples of these networks applied to classification tasks, where we illustrate that the networks perform substantially better than any other approximator we are aware of that is convex in its inputs."
  },
  "iclr2016_workshop_learningtosmile(s)": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning to SMILE(S)",
    "authors": [
      "Stanis\u0142\u0082aw Jastrz\u0119bski",
      "Damian Le\u015bniak",
      "Wojciech Marian Czarnecki"
    ],
    "page_url": "https://openreview.net/forum?id=VAVqG11WmSx0Wk76TAzp",
    "pdf_url": "https://openreview.net/pdf?id=VAVqG11WmSx0Wk76TAzp",
    "published": "2016-05",
    "summary": "This paper shows how one can directly apply natural language processing (NLP) methods to classification problems in cheminformatics.Connection between these seemingly separate fields is shown by considering standard textual representation of compound, SMILES.The problem of activity prediction against a target protein is considered, which is a crucial part of computer aided drug design process.Conducted experiments show that this way one can not only outrank state of the art results of hand crafted representations but also gets direct structural insights into the way decisions are made."
  },
  "iclr2016_workshop_learningretinaltilinginamodelofvisualattention": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Retinal Tiling in a Model of Visual Attention",
    "authors": [
      "Brian Cheung",
      "Eric Weiss",
      "Bruno Olshausen"
    ],
    "page_url": "https://openreview.net/forum?id=1WvOZJ0yDTMnPB1oinGN",
    "pdf_url": "https://openreview.net/pdf?id=1WvOZJ0yDTMnPB1oinGN",
    "published": "2016-05",
    "summary": "We describe a neural network model in which the tiling of the input array is learned by performing a joint localization and classification task. After training, the optimal tiling that emerges resembles the eccentricity dependent tiling of the human retina."
  },
  "iclr2016_workshop_hardware-friendlyconvolutionalneuralnetworkwitheven-numberfiltersize": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Hardware-Friendly Convolutional Neural Network with Even-Number Filter Size",
    "authors": [
      "Song Yao",
      "Song Han",
      "Kaiyuan Guo",
      "Jianqiao Wangni",
      "Yu Wang"
    ],
    "page_url": "https://openreview.net/forum?id=k80kn82ywfOYKX7ji42O",
    "pdf_url": "https://openreview.net/pdf?id=k80kn82ywfOYKX7ji42O",
    "published": "2016-05",
    "summary": "Convolutional Neural Network (CNN) has led to great advances in computer vision. Various customized CNN accelerators on embedded FPGA or ASIC platforms have been designed to accelerate CNN and improve energy efficiency. However, the odd-number filter size in existing CNN models prevents hardware accelerators from having optimal efficiency. In this paper, we analyze the influences of filter size on CNN accelerator performance and show that even-number filter size is much more hardware-friendly that can ensure high bandwidth and resource utilization. Experimental results on MNIST and CIFAR-10 demonstrate that hardware-friendly even kernel CNNs can reduce the FLOPs by 1.4x to 2x with comparable accuracy; With same FLOPs, even kernel can have even higher accuracy than odd size kernel."
  },
  "iclr2016_workshop_dodeepconvolutionalnetsreallyneedtobedeep(orevenconvolutional)?": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?",
    "authors": [
      "Gregor Urban",
      "Krzysztof J. Geras",
      "Samira Ebrahimi Kahou",
      "Ozlem Aslan Shengjie Wang",
      "Rich Caruana",
      "Abdelrahman Mohamed",
      "Matthai Philipose",
      "Matt Richardson"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOrG6lVsRNGwArs4qo",
    "pdf_url": "https://openreview.net/pdf?id=L7VOrG6lVsRNGwArs4qo",
    "published": "2016-05",
    "summary": "Yes, apparently they do.Previous research by Ba and Caruana (2014) demonstrated that shallow feed-forward nets sometimes can learn the complex functions pre- viously learned by deep nets while using a simi- lar number of parameters as the deep models they mimic. In this paper we investigate if shallow models can learn to mimic the functions learned by deep convolutional models. We experiment with shallow models and models with a vary- ing number of convolutional layers, all trained to mimic a state-of-the-art ensemble of CIFAR-10 models. We demonstrate that we are unable to train shallow models to be of comparable accu- racy to deep convolutional models. Although the student models do not have to be as deep as the teacher models they mimic, the student models apparently need multiple convolutional layers to learn functions of comparable accuracy. "
  },
  "iclr2016_workshop_generativeadversarialmetric": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Generative Adversarial Metric",
    "authors": [
      "Daniel Jiwoong Im",
      "Chris Dongjoo Kim",
      "Hui Jiang",
      "Roland Memisevic"
    ],
    "page_url": "https://openreview.net/forum?id=wVqzLo88YsG0qV7mtLq7",
    "pdf_url": "https://openreview.net/pdf?id=wVqzLo88YsG0qV7mtLq7",
    "published": "2016-05",
    "summary": "We introduced a new metric for comparing adversarial networks quantitatively. "
  },
  "iclr2016_workshop_revisesaturatedactivationfunctions": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Revise Saturated Activation Functions",
    "authors": [
      "Bing Xu",
      "Ruitong Huang",
      "Mu Li"
    ],
    "page_url": "https://openreview.net/forum?id=D1VDjyJjXF5jEJ1zfE53",
    "pdf_url": "https://openreview.net/pdf?id=D1VDjyJjXF5jEJ1zfE53",
    "published": "2016-05",
    "summary": "In this paper, we revise two commonly used saturated functions, the logistic sigmoid and the hyperbolic tangent (tanh).We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible reason making training deep networks with the logistic function difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid achieves comparable results with tanh.Then following the same argument, we improve tahn by penalizing in the negative part.We show that ``penalized tanh'' is comparable and even outperforms the state-of-the-art non-saturated functions including ReLU and leaky ReLU on deep convolution neural networks.Our results contradict to the conclusion of previous works that the saturation property causes the slow convergence. It suggests further investigation is necessary to better understand activation functions in deep architectures."
  },
  "iclr2016_workshop_multi-layerrepresentationlearningformedicalconcepts": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Multi-layer Representation Learning for Medical Concepts",
    "authors": [
      "Edward Choi",
      "Mohammad Taha Bahadori",
      "Jimeng Sun",
      "Elizabeth Searles",
      "Catherine Coffey"
    ],
    "page_url": "https://openreview.net/forum?id=mO9mQWp8Rij1gPZ3Ul5q",
    "pdf_url": "https://openreview.net/pdf?id=mO9mQWp8Rij1gPZ3Ul5q",
    "published": "2016-05",
    "summary": "Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification.Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit.This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit.In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation."
  },
  "iclr2016_workshop_alternativestructuresforcharacter-levelrnns": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Alternative structures for character-level RNNs",
    "authors": [
      "Piotr Bojanowski",
      "Armand Joulin",
      "Tomas Mikolov"
    ],
    "page_url": "https://openreview.net/forum?id=wVqzL1ypocG0qV7mtLqm",
    "pdf_url": "https://openreview.net/pdf?id=wVqzL1ypocG0qV7mtLqm",
    "published": "2016-05",
    "summary": "Recurrent neural networks are convenient and efficient models for learning patterns in sequential data. However, when applied to signals with very low cardinality such as character-level language modeling, they suffer from several problems. In order to success- fully model longer-term dependencies, the hidden layer needs to be large, which in turn implies high computational cost. Moreover, the accuracy of these models is significantly lower than that of baseline word-level models. We propose two structural modifications of the classic RNN LM architecture. The first one consists on conditioning the RNN both on the character-level and word-level information. The other one uses the recent history to condition the computation of the output probability. We evaluate the performance of the two proposed modifications on multi-lingual data. The experiments show that both modifications can improve upon the basic RNN architecture, which is even more visible in cases when the input and output signals are represented by single bits. These findings suggest that more research needs to be done to develop general RNN architecture that would perform optimally across wide range of tasks."
  },
  "iclr2016_workshop_inception-v4,inception-resnetandtheimpactofresidualconnectionsonlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
    "authors": [
      "Christian Szegedy",
      "Sergey Ioffe",
      "Vincent Vanhoucke"
    ],
    "page_url": "https://openreview.net/forum?id=q7kqBkL33f8LEkD3t7X9",
    "pdf_url": "https://openreview.net/pdf?id=q7kqBkL33f8LEkD3t7X9",
    "published": "2016-05",
    "summary": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly, however, when fully trained, the final quality of the non-residual Inception variants seem to be close to those of residual versions. We present several new streamlined architectures for both residual and non-residual Inception networks. With an ensemble of three residual and one pure Inception-v4, we achieve 3.08\\% top-5 error on the test set of the ImageNet classification (CLS) challenge"
  },
  "iclr2016_workshop_revisitingdistributedsynchronoussgd": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Revisiting Distributed Synchronous SGD",
    "authors": [
      "Jianmin Chen",
      "Rajat Monga",
      "Samy Bengio",
      "Rafal Jozefowicz"
    ],
    "page_url": "https://openreview.net/forum?id=D1VDZ5kMAu5jEJ1zfEWL",
    "pdf_url": "https://openreview.net/pdf?id=D1VDZ5kMAu5jEJ1zfEWL",
    "published": "2016-05",
    "summary": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. Previous works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently."
  },
  "iclr2016_workshop_adifferentiabletransitionbetweenadditiveandmultiplicativeneurons": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Differentiable Transition Between Additive and Multiplicative Neurons",
    "authors": [
      "Wiebke Koepp",
      "Patrick van der Smagt",
      "Sebastian Urban"
    ],
    "page_url": "https://openreview.net/forum?id=MwVPvKwRvsqxwkg1t7kY",
    "pdf_url": "https://openreview.net/pdf?id=MwVPvKwRvsqxwkg1t7kY",
    "published": "2016-05",
    "summary": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure."
  },
  "iclr2016_workshop_deepautoresolutionnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Autoresolution Networks",
    "authors": [
      "Gabriel Pereyra",
      "Christian Szegedy"
    ],
    "page_url": "https://openreview.net/forum?id=E8VEozRYyi31v0m2iDwy",
    "pdf_url": "https://openreview.net/pdf?id=E8VEozRYyi31v0m2iDwy",
    "published": "2016-05",
    "summary": "Despite the success of very deep convolutional neural networks, they currently operate at very low resolutions relative to modern cameras. Visual attention mechanisms address this by allowing models to access higher resolutions only when necessary. However, in certain cases, this higher resolution isn\u2019t available. We show that autoresolution networks, which learn correspondences between lowresolution and high-resolution images, learn representations that improve lowresolution classification - without needing labeled high-resolution images."
  },
  "iclr2016_workshop_unsupervisedlearningwithimbalanceddataviastructureconsolidationlatentvariablemodel": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Unsupervised Learning with Imbalanced Data via Structure Consolidation Latent Variable Model",
    "authors": [
      "Fariba Yousefi",
      "Zhenwen Dai",
      "Carl Henrik Ek",
      "Neil Lawrence"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xMOwxPf5Pk8ELfEjV",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xMOwxPf5Pk8ELfEjV",
    "published": "2016-05",
    "summary": "Unsupervised learning on imbalanced data is challenging because, when given imbalanced data, current model is often dominated by the major category and ignores the categories with small amount of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derive an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset."
  },
  "iclr2016_workshop_robustconvolutionalneuralnetworksunderadversarialnoise": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Robust Convolutional Neural Networks under Adversarial Noise",
    "authors": [
      "Jonghoon Jin",
      "Aysegul Dundar",
      "Eugenio Culurciello"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOOy8B6hRNGwArs4Bn",
    "pdf_url": "https://openreview.net/pdf?id=L7VOOy8B6hRNGwArs4Bn",
    "published": "2016-05",
    "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called adversarial examples. In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise."
  },
  "iclr2016_workshop_gradnetsdynamicinterpolationbetweenneuralarchitectures": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "GradNets: Dynamic Interpolation Between Neural Architectures",
    "authors": [
      "Diogo Almeida",
      "Nate Sauder"
    ],
    "page_url": "https://openreview.net/forum?id=Qn8lE8x17fkB2l8pUYPk",
    "pdf_url": "https://openreview.net/pdf?id=Qn8lE8x17fkB2l8pUYPk",
    "published": "2016-05",
    "summary": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers."
  },
  "iclr2016_workshop_resnetinresnetgeneralizingresidualarchitectures": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Resnet in Resnet: Generalizing Residual Architectures",
    "authors": [
      "Sasha Targ",
      "Diogo Almeida",
      "Kevin Lyman"
    ],
    "page_url": "https://openreview.net/forum?id=lx9l4r36gU2OVPy8Cv9g",
    "pdf_url": "https://openreview.net/pdf?id=lx9l4r36gU2OVPy8Cv9g",
    "published": "2016-05",
    "summary": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100."
  },
  "iclr2016_workshop_doctoraipredictingclinicaleventsviarecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Doctor AI: Predicting Clinical Events via Recurrent Neural Networks",
    "authors": [
      "Edward Choi",
      "Mohammad Taha Bahadori",
      "Andy Schuetz",
      "Walter F. Stewart",
      "Joshua C. Denny",
      "Bradley A. Malin",
      "Jimeng Sun"
    ],
    "page_url": "https://openreview.net/forum?id=zvwDjZ3GDfM8kw3ZinXB",
    "pdf_url": "https://openreview.net/pdf?id=zvwDjZ3GDfM8kw3ZinXB",
    "published": "2016-05",
    "summary": "Large amount ofElectronic Health Record (EHR) data have been collected over millions of patients over multiple years. The rich longitudinal EHR data documented the collective experiences of physicians including diagnosis, medication prescription and procedures. We argue it is possible now to leverage the EHR data to model how physicians behave, and we call our model Doctor AI.Towards this direction of modeling clinical behavior of physicians,we develop a successful application of Recurrent Neural Networks (RNN) to jointly forecast the future disease diagnosis and medication prescription along with their timing. Unlike traditional classification models where a single target is of interest, our model can assess the entire history of patients and make continuous and multilabel predictions based on patients' historical data. We evaluate the performance of the proposed method on a large real-world EHR data over 260K patients over 8 years.We observed Doctor AI can perform differential diagnosis with similar accuracy to physicians. In particular, Doctor AI achieves up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by applying the resulting models on data from a completely different medication institution achieving comparable performance."
  },
  "iclr2016_workshop_on-the-flynetworkpruningforobjectdetection": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "On-the-fly Network Pruning for Object Detection",
    "authors": [
      "Marc Masana",
      "Joost van de Weijer",
      "Andrew D. Bagdanov"
    ],
    "page_url": "https://openreview.net/forum?id=p8jp5lzPWSnQVOGWfpDD",
    "pdf_url": "https://openreview.net/pdf?id=p8jp5lzPWSnQVOGWfpDD",
    "published": "2016-05",
    "summary": "Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. These bounding boxes are highly correlated since they originate from the same image. In this paper we investigate how to exploit featureoccurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result."
  },
  "iclr2016_workshop_deepdirectedgenerativemodelswithenergy-basedprobabilityestimation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Directed Generative Models with Energy-Based Probability Estimation",
    "authors": [
      "Taesup Kim",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=BNYAGZZj5S7PwR1riXzA",
    "pdf_url": "https://openreview.net/pdf?id=BNYAGZZj5S7PwR1riXzA",
    "published": "2016-05",
    "summary": "Energy-based probabilistic models have been confronted with intractable computations during the learning that requires to have appropriate samples drawn from the estimated probability distribution. It can be approximately achieved by a Monte Carlo Markov Chain sampling process, but still has mixing problems especially with deep models that slow the learning. We introduce an auxiliary deep model that deterministically generates samples based on the estimated distribution, and this makes the learning easier without any high cost sampling process. As a result, we propose a new framework to train the energy-based probabilistic models with two separate deep feed-forward models. The one is only to estimate the energy function, and the another is to deterministically generate samples based on it. Consequently, we can estimate the probability distribution and its corresponding deterministic generator with deep models."
  },
  "iclr2016_workshop_rectifiedfactornetworksforbiclustering": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Rectified Factor Networks for Biclustering",
    "authors": [
      "Djork-Arn\u00e9 Clevert",
      "Thomas Unterthiner",
      "Sepp Hochreiter"
    ],
    "page_url": "https://openreview.net/forum?id=mO9m42yWgSj1gPZ3UlGA",
    "pdf_url": "https://openreview.net/pdf?id=mO9m42yWgSj1gPZ3UlGA",
    "published": "2016-05",
    "summary": "Biclustering is evolving into one of the major tools for analyzing large datasets given as matrix of samples times features. Biclustering has been successfully applied in life sciences, e.g. for drug design, in e-commerce, e.g. for internet retailing or recommender systems.FABIA is one of the most successful biclustering methods which excelled in different projects and is used by companies like Janssen, Bayer, or Zalando. FABIA is a generative model that represents each bicluster by two sparse membership vectors: one for the samples and one for the features. However, FABIA is restricted to about 20 code units because of the high computational complexity of computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated. Sample membership is difficult to determine because vectors do not have exact zero entries and can have both large positive and large negative values.We propose to use the recently introduced unsupervised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks of FABIA. RFNs efficiently construct very sparse, non-linear, high-dimensional representations of the input via their posterior means. RFN learning is a generalized alternating minimization algorithm based on the posterior regularization method which enforces non-negative and normalized posterior means. Each code unit represents a bicluster, where samples for which the code unit is active belong to the bicluster and features that have activating weights to the code unit belong to the bicluster.On 400 benchmark datasets with artificially implanted biclusters, RFN significantly outperformed 13 other biclustering competitors including FABIA. In biclustering experiments on three gene expression datasets with known clusters that were determined by separate measurements, RFN biclustering was two times significantly better than the other 13 methods and once on second place."
  },
  "iclr2016_workshop_randomoutusingaconvolutionalgradientnormtowinthefilterlottery": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "RandomOut: Using a convolutional gradient norm to win The Filter Lottery",
    "authors": [
      "Joseph Paul Cohen",
      "Henry Z. Lo",
      "Wei Ding"
    ],
    "page_url": "https://openreview.net/forum?id=2xwPmERVBtpKBZvXtQnD",
    "pdf_url": "https://openreview.net/pdf?id=2xwPmERVBtpKBZvXtQnD",
    "published": "2016-05",
    "summary": "Convolutional neural networks are sensitive to the random initialization of filters. We call this The Filter Lottery (TFL) because the random numbers used to initialize the network determine if you will ``win'' and converge to a satisfactory local minimum. This issue forces networks to contain more filters (be wider) to achieve higher accuracy because they have better odds of being transformed into highly discriminative features at the risk of introducing redundant features. To deal with this, we propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold.This consistently improves accuracy across two datasets by up to 1.8%. Our scheme RandomOut allows us to increase the number of filters explored without increasing the size of the network. This yields more compact networks which can train and predict with less computation, thus allowing more powerful CNNs to run on mobile devices. "
  },
  "iclr2016_workshop_persistentrnnsstashingweightsonchip": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Persistent RNNs: Stashing Weights on Chip",
    "authors": [
      "Greg Diamos",
      "Shubho Sengupta",
      "Bryan Catanzaro",
      "Mike Chrzanowski",
      "Adam Coates",
      "Erich Elsen",
      "Jesse Engel",
      "Awni Hannun",
      "Sanjeev Satheesh"
    ],
    "page_url": "https://openreview.net/forum?id=XL9v5ZZ2qtXB8D1RUG6V",
    "pdf_url": "https://openreview.net/pdf?id=XL9v5ZZ2qtXB8D1RUG6V",
    "published": "2016-05",
    "summary": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers."
  },
  "iclr2016_workshop_scalenormalization": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Scale Normalization",
    "authors": [
      "Henry Z Lo",
      "Kevin Amaral",
      "Wei Ding"
    ],
    "page_url": "https://openreview.net/forum?id=OM0jjYW3BHp57ZJjtNEO",
    "pdf_url": "https://openreview.net/pdf?id=OM0jjYW3BHp57ZJjtNEO",
    "published": "2016-05",
    "summary": "One of the difficulties of training deep neural networks is caused by improper scaling between layers.These scaling issues introduce exploding / gradient problems, and have typically been addressed by careful variance-preserving initialization.We consider this problem as one of preserving scale, rather than preserving variance.This leads to a simple method of scale-normalizing weight layers, which ensures that scale is approximately maintained between layers.Our method of scale-preservation ensures that forward propagation is impacted minimally, while backward passes maintain gradient scales.Preliminary experiments show that scale normalization effectively speeds up learning, without introducing additional hyperparameters or parameters. "
  },
  "iclr2016_workshop_close-to-cleanregularizationrelatesvirtualadversarialtraining,laddernetworksandothers": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others",
    "authors": [
      "Mudassar Abbas",
      "Jyri Kivinen",
      "Tapani Raiko"
    ],
    "page_url": "https://openreview.net/forum?id=6XAwLR8gysrVp0EvsEW3",
    "pdf_url": "https://openreview.net/pdf?id=6XAwLR8gysrVp0EvsEW3",
    "published": "2016-05",
    "summary": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods."
  },
  "iclr2016_workshop_guidedsequence-to-sequencelearningwithexternalrulememory": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Guided Sequence-to-Sequence Learning with External Rule Memory",
    "authors": [
      "Jiatao Gu",
      "Baotian Hu",
      "Zhengdong Lu",
      "Hang Li",
      "Victor O.K. Li"
    ],
    "page_url": "https://openreview.net/forum?id=BNYAA7gNBi7PwR1riXzR",
    "pdf_url": "https://openreview.net/pdf?id=BNYAA7gNBi7PwR1riXzR",
    "published": "2016-05",
    "summary": "External memory has been proven to be essential for the success of neural network-based systems on many tasks, including Question-Answering, classification, machine translation and reasoning. In all those models the memory is used to store instance representations of multiple levels, analogous to \u201cdata\u201d in the Von Neumann architecture of a computer, while the \u201cinstructions\u201d are stored in the weights. In this paper, we however propose to use the memory for storing part of the instructions, and more specifically, the transformation rules in sequence-to-sequence learning tasks, in an external memory attached to a neural system. This memory can be accessed both by the neural network and by the human experts, hence serving as an interface for a novel learning paradigm where not only the instances but also the rule can be taught to the neural network. Our empirical study on a synthetic but challenging dataset verifies that our model is effective."
  },
  "iclr2016_workshop_neuraltextunderstandingwithattentionsumreader": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Text Understanding with Attention Sum Reader",
    "authors": [
      "Rudolf Kadlec",
      "Martin Schmid",
      "Ond\u0159\u0099ej Bajgar",
      "Jan Kleindienst"
    ],
    "page_url": "https://openreview.net/forum?id=Qn8lxPngJFkB2l8pUYxg",
    "pdf_url": "https://openreview.net/pdf?id=Qn8lxPngJFkB2l8pUYxg",
    "published": "2016-05",
    "summary": "Two large-scale cloze-style context-question-answer datasets have been introduced recently: i) the CNN and Daily Mail news data and ii) the Children's Book Test. Thanks to the size of these datasets, the associated task is well suited for deep-learning techniques that seem to outperform all alternative approaches. We present a new, simple model that is tailor made for such question-answering problems. Our model directly sums attention over candidate answer words in the document instead of using it to compute weighted sum of word embeddings. Our model outperforms models previously proposed for these tasks by a large margin."
  },
  "iclr2016_workshop_incorporatingnesterovmomentumintoadam": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Incorporating Nesterov Momentum into Adam",
    "authors": [
      "Timothy Dozat"
    ],
    "page_url": "https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ",
    "pdf_url": "https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ",
    "published": "2016-05",
    "summary": "This work aims to improve upon the recently proposed and rapidly popular- ized optimization algorithm Adam (Kingma & Ba, 2014). Adam has two main components\u2014a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in- ferior to a similar algorithm known as Nesterov\u2019s accelerated gradient (NAG). We show how to modify Adam\u2019s momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod- els. "
  },
  "iclr2016_workshop_variationalinferenceforon-lineanomalydetectioninhigh-dimensionaltimeseries": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series",
    "authors": [
      "Maximilian S\u00f6lch",
      "Justin Bayer",
      "Marvin Ludersdorfer",
      "Patrick van der Smagt"
    ],
    "page_url": "https://openreview.net/forum?id=oVgoWpz5LsrlgPMRsB1v",
    "pdf_url": "https://openreview.net/pdf?id=oVgoWpz5LsrlgPMRsB1v",
    "published": "2016-05",
    "summary": "Approximate variational inference has shown to be a powerful tool for modeling unknown, complex probability distributions. Recent advances in the field allow us to learn probabilistic sequence models. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line."
  },
  "iclr2016_workshop_sequence-to-sequencernnsfortextsummarization": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Sequence-to-Sequence RNNs for Text Summarization",
    "authors": [
      "Ramesh Nallapati",
      "Bing Xiang",
      "Bowen Zhou"
    ],
    "page_url": "https://openreview.net/forum?id=gZ9OMgQWoIAPowrRUAN6",
    "pdf_url": "https://openreview.net/pdf?id=gZ9OMgQWoIAPowrRUAN6",
    "published": "2016-05",
    "summary": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation. Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et. al. (2015), on the Gigaword dataset without any additional tuning. We also propose additional extensions to the standard architecture, which we show contribute to further improvement in performance. "
  },
  "iclr2016_workshop_neuralgenerativequestionanswering": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Generative Question Answering",
    "authors": [
      "Jun Yin",
      "Xin Jiang",
      "Zhengdong Lu",
      "Lifeng Shang",
      "Hang Li",
      "Xiaoming Li"
    ],
    "page_url": "https://openreview.net/forum?id=OM0vWYM7Eup57ZJjtNql",
    "pdf_url": "https://openreview.net/pdf?id=OM0vWYM7Eup57ZJjtNql",
    "published": "2016-05",
    "summary": "This paper presents an end-to-end neural network model, named Neural Generative Question Answering (genQA), that can generate answers to simple factoid questions, both in natural language. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to access an embedded knowledge-base through an attention-like mechanism. The model is trained on a corpus of question-answer pairs, with their associated triples in the given knowledge-base. Empirical study shows the proposed model can effectively deal with the language variation of the question and generate a right answer by referring to the facts in the knowledge-base. The experiment on question answering demonstrates that the proposed model can outperform the embedding-based QA model as well as the neural dialogue models trained on the same data."
  },
  "iclr2016_workshop_learningdocumentembeddingsbypredictingn-gramsforsentimentclassificationoflongmoviereviews": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews",
    "authors": [
      "Bofang Li",
      "Tao Liu",
      "Xiaoyong Du",
      "Deyuan Zhang",
      "Zhe Zhao"
    ],
    "page_url": "https://openreview.net/forum?id=XL92M93mzhXB8D1RUWBz",
    "pdf_url": "https://openreview.net/pdf?id=XL92M93mzhXB8D1RUWBz",
    "published": "2016-05",
    "summary": "Bag-of-ngram based methods still achieve state-of-the-art results for tasks such as sentiment classification of long movie reviews, though semantic information is partially lost for these methods. Many document embeddings methods have been proposed to capture semantics, but they still can't outperform bag-of-ngram based methods on this task. In this paper, we modify the architecture of the recently proposed Paragraph Vector, allowing it to learn document vectors by predicting not only words, but n-gram features as well. Our model is able to capture both semantics and word order in documents while keeping the expressive power of learned vectors. Experimental results on IMDB movie review dataset show that our model outperforms previous deep learning models and bag-of-ngram based models due to the above advantages."
  },
  "iclr2016_workshop_autoencodingforjointrelationfactorizationanddiscoveryfromtext": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Autoencoding for Joint Relation Factorization and Discovery from Text",
    "authors": [
      "Diego Marcheggiani",
      "Ivan Titov"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOzGWB5hRNGwArs4BJ",
    "pdf_url": "https://openreview.net/pdf?id=L7VOzGWB5hRNGwArs4BJ",
    "published": "2016-05",
    "summary": "We present a method for unsupervised open-domain relation discovery.In contrast to previous (mostly generative and agglomerative clustering) approaches, our model relies on rich contextual features andmakes minimal independence assumptions.The model is composed of two parts: a feature-rich relation extractor, which predicts a semantic relation between two entities, and a factorization model, which reconstructs arguments (i.e., the entities) relying on the predicted relation. We use a variational autoencoding objective and estimate the two components jointly so as to minimize errors in recovering arguments. We study factorization models inspired by previous work in relation factorization. Our models substantially outperform the generative and agglomerative-clustering counterparts and achieve state-of-the-art performance."
  },
  "iclr2016_workshop_adaptivenaturalgradientlearningbasedonriemannianmetricofscorematching": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Adaptive Natural Gradient Learning Based on Riemannian Metric of Score Matching",
    "authors": [
      "Ryo Karakida",
      "Masato Okada",
      "Shun-ichi Amari"
    ],
    "page_url": "https://openreview.net/forum?id=lx9lNjDDvU2OVPy8CvGJ",
    "pdf_url": "https://openreview.net/pdf?id=lx9lNjDDvU2OVPy8CvGJ",
    "published": "2016-05",
    "summary": "The natural gradient is a powerful method to improve the transient dynamics of learning by considering the geometric structure of the parameter space. Many natural gradient methods have been developed with regards to Kullback-Leibler (KL) divergence and its Fisher metric, but the framework of natural gradient can be essentially extended to other divergences. In this study, we focus on score matching, which is an alternative to maximum likelihood learning for unnormalized statistical models, and introduce its Riemannian metric. By using the score matching metric, we derive an adaptive natural gradient algorithm that does not require computationally demanding inversion of the metric. Experimental results in a multi-layer neural network model demonstrate that the proposed method avoids the plateau phenomenon and accelerates the convergence of learning compared to the conventional stochastic gradient descent method."
  },
  "iclr2016_workshop_neuralenquirerlearningtoquerytablesinnaturallanguage": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Enquirer: Learning to Query Tables in Natural Language",
    "authors": [
      "Pengcheng Yin",
      "Zhengdong Lu",
      "Hang Li",
      "Ben Kao"
    ],
    "page_url": "https://openreview.net/forum?id=3QxgvRAolhp7y9wltPg8",
    "pdf_url": "https://openreview.net/pdf?id=3QxgvRAolhp7y9wltPg8",
    "published": "2016-05",
    "summary": "We propose Neural Enquirer \u2014 a neural network architecture for answering natural language (NL) questions given a knowledge base (KB) table. Unlike previous work on end-to-end training of semantic parsers, Neural Enquirer is fully \u201cneuralized\u201d: it gives distributed representations of queries and KB tables, and executes queries through a series of differentiable operations. The model can be trained with gradient descent using both end-to-end and step-by-step supervision. During training the representations of queries and the KB table are jointly optimized with the query execution logic. Our experiments show that the model can learn to execute complex NL queries on KB tables with rich structures."
  },
  "iclr2016_workshop_endtoendspeechrecognitioninenglishandmandarin": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "End to end speech recognition in English and Mandarin",
    "authors": [
      "Dario Amodei",
      "Rishita Anubhai",
      "Eric Battenberg",
      "Carl Case",
      "Jared Casper",
      "Bryan Catanzaro",
      "Jingdong Chen",
      "Mike Chrzanowski",
      "Adam Coates",
      "Greg Diamos",
      "Erich Elsen",
      "Jesse Engel",
      "Linxi Fan",
      "Christopher Fougner",
      "Tony Han",
      "Awni Hannun",
      "Billy Jun",
      "Patrick LeGresley",
      "Libby Lin",
      "Sharan Narang",
      "Andrew Ng",
      "Sherjil Ozair",
      "Ryan Prenger",
      "Jonathan Raiman",
      "Sanjeev Satheesh",
      "David Seetapun",
      "Shubho Sengupta",
      "Yi Wang",
      "Zhiqian Wang",
      "Chong Wang",
      "Bo Xiao",
      "Dani Yogatama",
      "Jun Zhan",
      "Zhenyao Zhu"
    ],
    "page_url": "https://openreview.net/forum?id=XL9vPjMAjuXB8D1RUG6L",
    "pdf_url": "https://openreview.net/pdf?id=XL9vPjMAjuXB8D1RUG6L",
    "published": "2016-05",
    "summary": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\u2013two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages.Key to our approach is our application of HPC techniques,enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms.As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale"
  },
  "iclr2016_workshop_lessonsfromtherademachercomplexityfordeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Lessons from the Rademacher Complexity for Deep Learning",
    "authors": [
      "Jure Sokolic",
      "Raja Giryes",
      "Guillermo Sapiro",
      "Miguel R. D. Rodrigues"
    ],
    "page_url": "https://openreview.net/forum?id=P7Vk63koAhKvjNORtJzZ",
    "pdf_url": "https://openreview.net/pdf?id=P7Vk63koAhKvjNORtJzZ",
    "published": "2016-05",
    "summary": "Understanding the generalization properties of deep learning models is critical for successful applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks via the empirical Rademacher complexity and show that it is easier to control the complexity ofconvolutional networks compared to general fully connected networks. In particular, we justify the usage of small convolutional kernels in deep networks as they lead to a better generalization error. Moreover, we propose a representation based regularization method that allows to decrease the generalization errorby controlling the coherence of the representation. Experiments on the MNIST dataset support these foundations."
  },
  "iclr2016_workshop_coverage-basedneuralmachinetranslation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Coverage-based Neural Machine Translation",
    "authors": [
      "Zhaopeng Tu",
      "Zhengdong Lu",
      "Yang Liu",
      "Xiaohua Liu",
      "Hang Li"
    ],
    "page_url": "https://openreview.net/forum?id=jZ9WrEWPmsnlBG2XfGLl",
    "pdf_url": "https://openreview.net/pdf?id=jZ9WrEWPmsnlBG2XfGLl",
    "published": "2016-05",
    "summary": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both translation and alignment qualities over NMT without coverage."
  },
  "iclr2016_workshop_feed-forwardnetworkswithattentioncansolvesomelong-termmemoryproblems": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems",
    "authors": [
      "Colin Raffel",
      "Daniel P. W. Ellis"
    ],
    "page_url": "https://openreview.net/forum?id=81DD7ZNyxI6O2Pl0Ul5j",
    "pdf_url": "https://openreview.net/pdf?id=81DD7ZNyxI6O2Pl0Ul5j",
    "published": "2016-05",
    "summary": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic addition and multiplication long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks."
  },
  "iclr2016_workshop_learningstablerepresentationsinachangingworldwithon-linet-sneproofofconceptinthesongbird": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning stable representations in a changing world withon-line t-SNE: proof of concept in the songbird",
    "authors": [
      "St\u00e9phane Deny",
      "Emily Mackevicius",
      "Tatsuo Okubo",
      "Gordon Berman",
      "Joshua Shaevitz",
      "Michale Fee"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo1jRRDsrlgPMRsBzY",
    "pdf_url": "https://openreview.net/pdf?id=oVgo1jRRDsrlgPMRsBzY",
    "published": "2016-05",
    "summary": "Many real-world time series involve repeated patterns that evolve gradually by following slow underlying trends. The evolution of relevant features prevents conventional learning methods from extracting representations that separate differing patterns while being consistent over the whole time series. Here, we present an unsupervised learning method to finding representations that are consistent over time and which separate patterns in non-stationary time-series. We developed an on-line version of t-Distributed Stochastic Neighbor Embedding (t-SNE). We apply t-SNE to the time series iteratively on a running window, and for each displacement of the window, we choose as the seed of the next embedding the final positions of the points obtained in the previous embedding. This process ensures consistency of the representation of slowly evolving patterns, while ensuring that the embedding at each step is optimally adapted to the current window. We apply this method to the song of the developing zebra finch, and we show that we are able to track multiple distinct syllables that are slowly emerging over multiple days, from babbling to the adult song stage."
  },
  "iclr2016_workshop_mixturesofsparseautoregressivenetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Mixtures of Sparse Autoregressive Networks",
    "authors": [
      "Marc Goessling",
      "Yali Amit"
    ],
    "page_url": "https://openreview.net/forum?id=q7kEZL1W2U8LEkD3tgnV",
    "pdf_url": "https://openreview.net/pdf?id=q7kEZL1W2U8LEkD3tgnV",
    "published": "2016-05",
    "summary": "We consider high-dimensional distribution estimation through autoregressive networks. By combining the concepts of sparsity, mixtures and parameter sharing we obtain a simple model which is fast to train and which achieves state-of-the-art or better results on several standard benchmark datasets. Specifically, we use an L1-penalty to regularize the conditional distributions and introduce a procedure for automatic parameter sharing between mixture components. Moreover, we propose a simple distributed representation which permits exact likelihood evaluations since the latent variables are interleaved with the observable variables and can be easily integrated out. Our model achieves excellent generalization performance and scales well to extremely high dimensions."
  },
  "iclr2016_workshop_comparativestudyofcaffe,neon,theano,andtorchfordeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning",
    "authors": [
      "Soheil Bahrampour",
      "Naveen Ramakrishnan",
      "Lukas Schott",
      "Mohak Shah"
    ],
    "page_url": "https://openreview.net/forum?id=q7kEN7WoXU8LEkD3t7BQ",
    "pdf_url": "https://openreview.net/pdf?id=q7kEN7WoXU8LEkD3t7BQ",
    "published": "2016-05",
    "summary": "Deep learning methods have resulted in significant performance improvements in several application domains and as such several software frameworks have been developed to facilitate their implementation. This paper presents a comparative study of four deep learning frameworks, namely Caffe, Neon, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance metrics used here include the gradient computation time, which is important during the training phase of deep networks, and the forward time, which is important from the deployment perspective of trained networks. For convolutional networks, we also report how each of these frameworks support various convolutional algorithms and their corresponding performance. From our experiments, we observe that Theano and Torch are the most easily extensible frameworks. We observe that Torch is best suited for any deep architecture on CPU, followed by Theano. It also achieves the best performance on the GPU for large convolutional and fully connected networks, followed closely by Neon. Theano achieves the best performance on GPU for training and deployment of LSTM networks. Finally Caffe is the easiest for evaluating the performance of standard deep architectures."
  },
  "iclr2016_workshop_actionrecognitionusingvisualattention": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Action Recognition using Visual Attention",
    "authors": [
      "Shikhar Sharma",
      "Ryan Kiros",
      "Ruslan Salakhutdinov"
    ],
    "page_url": "https://openreview.net/forum?id=3QxX9NwWgIp7y9wlt5L7",
    "pdf_url": "https://openreview.net/pdf?id=3QxX9NwWgIp7y9wlt5L7",
    "published": "2016-05",
    "summary": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed."
  },
  "iclr2016_workshop_improvingperformanceofrecurrentneuralnetworkwithrelunonlinearity": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Improving performance of recurrent neural network with relu nonlinearity",
    "authors": [
      "Sachin S. Talathi",
      "Aniket Vartak"
    ],
    "page_url": "https://openreview.net/forum?id=wVqq536NJiG0qV7mtBNp",
    "pdf_url": "https://openreview.net/pdf?id=wVqq536NJiG0qV7mtBNp",
    "published": "2016-05",
    "summary": "In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem."
  },
  "iclr2016_workshop_visualizingandunderstandingrecurrentnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Visualizing and Understanding Recurrent Networks",
    "authors": [
      "Andrej Karpathy",
      "Justin Johnson",
      "Li Fei-Fei"
    ],
    "page_url": "https://openreview.net/forum?id=71BmK0m6qfAE8VvKUQWB",
    "pdf_url": "https://openreview.net/pdf?id=71BmK0m6qfAE8VvKUQWB",
    "published": "2016-05",
    "summary": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study."
  },
  "iclr2016_workshop_learningtodecomposeforobjectdetectionandinstancesegmentation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning to Decompose for Object Detection and Instance Segmentation",
    "authors": [
      "Eunbyung Park",
      "Alexander C. Berg"
    ],
    "page_url": "https://openreview.net/forum?id=oVgBRXX9nsrlgPMRsrP4",
    "pdf_url": "https://openreview.net/pdf?id=oVgBRXX9nsrlgPMRsrP4",
    "published": "2016-05",
    "summary": "Although deep convolutional neural networks(CNNs) have achieved remarkable results on object detection and segmentation, pre- and post-processing steps such as region proposals and non-maximum suppression(NMS), have been required. These steps result in high computational complexity and sensitivity to hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel end-to-end trainable deep neural network architecture, which consists of convolutional and recurrent layers, that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any pre- or post-processing steps. We have tested on detecting digits in multi-digit images synthesized using MNIST, automatically segmenting digits in these images, and detecting cars in theKITTI benchmark dataset.The proposed approach outperforms a strong CNN baseline on the synthesized digits datasets and shows promising results on KITTI car detection."
  },
  "iclr2016_workshop_learningvisualgroupsfromco-occurrencesinspaceandtime": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning visual groups from co-occurrences in space and time",
    "authors": [
      "Phillip Isola",
      "Daniel Zoran",
      "Dilip Krishnan",
      "Edward H. Adelson"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo4M4RRIrlgPMRsBz5",
    "pdf_url": "https://openreview.net/pdf?id=oVgo4M4RRIrlgPMRsBz5",
    "published": "2016-05",
    "summary": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories."
  },
  "iclr2016_workshop_spatio-temporalvideoautoencoderwithdifferentiablememory": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Spatio-Temporal Video Autoencoder with Differentiable Memory",
    "authors": [
      "Viorica Patraucean",
      "Ankur Handa",
      "Roberto Cipolla"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo4p8O1CrlgPMRsBzE",
    "pdf_url": "https://openreview.net/pdf?id=oVgo4p8O1CrlgPMRsBzE",
    "published": "2016-05",
    "summary": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We believe these features can in turn facilitate learning high-level tasks such as path planning, semantic segmentation, or action recognition, reducing the overall supervision effort."
  },
  "iclr2016_workshop_tasklossestimationforstructuredprediction": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Task Loss Estimation for Structured Prediction",
    "authors": [
      "Dzmitry Bahdanau",
      "Dmiriy Serdyuk",
      "Phil\u00e9mon Brakel",
      "Nan Rosemary Ke",
      "Jan Chorowski",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=GvVJNQZM5f1WDOmRiMKy",
    "pdf_url": "https://openreview.net/pdf?id=GvVJNQZM5f1WDOmRiMKy",
    "published": "2016-05",
    "summary": "Often, the performance on a supervised machine learning task is evaluated with a \\emph{task loss} function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a \\emph{surrogate loss} function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call \\emph{consistency with the task loss}. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error maybe used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this propertyto design specialized surrogate losses forEncoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant ~9\\% relative improvement interms of Character Error Rate (CER) in the case when noextra corpora are used for language modeling. "
  },
  "iclr2016_workshop_conditionalcomputationinneuralnetworksforfastermodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Conditional computation in neural networks for faster models",
    "authors": [
      "Emmanuel Bengio",
      "Pierre-Luc Bacon",
      "Joelle Pineau",
      "Doina Precup"
    ],
    "page_url": "https://openreview.net/forum?id=BNYMo3QRxh7PwR1riEDL",
    "pdf_url": "https://openreview.net/pdf?id=BNYMo3QRxh7PwR1riEDL",
    "published": "2016-05",
    "summary": "Deep learning has become the state-of-art tool in many applications, but the evaluation of expressive deep models can be unfeasible on resource-constrained devices. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively activating only parts of the network at a time. We propose to use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as reinforcement learning. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation."
  },
  "iclr2016_workshop_ametriclearningapproachforgraph-basedlabelpropagation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A metric learning approach for graph-based label propagation",
    "authors": [
      "Pauline Wauquier",
      "Mikaela Keller"
    ],
    "page_url": "https://openreview.net/forum?id=oVgoWgzpZSrlgPMRsB19",
    "pdf_url": "https://openreview.net/pdf?id=oVgoWgzpZSrlgPMRsB19",
    "published": "2016-05",
    "summary": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. We claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently."
  },
  "iclr2016_workshop_bidirectionalhelmholtzmachines": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Bidirectional Helmholtz Machines",
    "authors": [
      "Jorg Bornschein",
      "Samira Shabanian",
      "Asja Fischer",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=91Eo10XnqckRlNvXUVkL",
    "pdf_url": "https://openreview.net/pdf?id=91Eo10XnqckRlNvXUVkL",
    "published": "2016-05",
    "summary": "Efficient unsupervised training and inference in deep generative models remains a challenging problem. One basic approach, called Helmholtz machine, involves training a top-down directed generative model together with a bottom-up auxiliary model that is trained to help perform approximate inference. Recent results indicate that better results can be obtained with better approximate inference procedures. Instead of employing more powerful procedures, we here propose to regularize the generative model to stay close to the class of distributions that can be efficiently inverted by the approximate inference model. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. We demonstrate that we can use this approach to fit generative models with many layers of hidden binary stochastic variables to complex training distributions and that this method prefers significantly deeper architectures while it supports orders of magnitude more efficient approximate inference than other approaches. "
  },
  "iclr2016_workshop_acontroller-recognizerframeworkhownecessaryisrecogntionforcontrol?": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Controller-Recognizer Framework: How Necessary is Recogntion for Control?",
    "authors": [
      "Marcin Moczulski",
      "Kelvin Xu",
      "Aaron Courville",
      "Kyunghyun Cho"
    ],
    "page_url": "https://openreview.net/forum?id=5QzkZ8LvJFZgXpo7i3OQ",
    "pdf_url": "https://openreview.net/pdf?id=5QzkZ8LvJFZgXpo7i3OQ",
    "published": "2016-05",
    "summary": "Recently there has been growing interest in building ``active'' visual object recognizers, as opposed to ``passive'' recognizers which classifies a given static image into a predefined set of object categories. In this paper we propose to generalize recent end-to-end active visual recognizers into a controller-recognizer framework. In this framework, the interfaces with an external manipulator, while the recognizer classifies the visual input adjusted by the manipulator. We describe two recently proposed controller-recognizer models-- the recurrent attention model (Mnih et al., 2014) and spatial transformer network (Jaderberg et al., 2015)-- as representative examples of controller-recognizer models. Based on this description we observe that most existing end-to-end controller-recognizers tightly couple the controller and recognizer. We consider whether this tight coupling is necessary, and try to answer this empirically by investigating a decoupled controller and recognizer.Our experiments revealed that it is not always necessary to tightly couple them, and that by decoupling the controller and recognizer, there is a possibility to build a generic controller that is pretrained and works together with any subsequent recognizer. "
  },
  "iclr2016_workshop_onlinebatchselectionforfastertrainingofneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Online Batch Selection for Faster Training of Neural Networks",
    "authors": [
      "Ilya Loshchilov",
      "Frank Hutter"
    ],
    "page_url": "https://openreview.net/forum?id=r8lrkABJ7H8wknpYt5KB",
    "pdf_url": "https://openreview.net/pdf?id=r8lrkABJ7H8wknpYt5KB",
    "published": "2016-05",
    "summary": "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset.While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. As the loss function to be minimized for the whole dataset is an aggregation of loss functions of individualdatapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank.Our experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5."
  },
  "iclr2016_workshop_nonparametriccanonicalcorrelationanalysis": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Nonparametric Canonical Correlation Analysis",
    "authors": [
      "Tomer Michaeli",
      "Weiran Wang",
      "Karen Livescu"
    ],
    "page_url": "https://openreview.net/forum?id=3Qxgwzv3ZCp7y9wltPg2",
    "pdf_url": "https://openreview.net/pdf?id=3Qxgwzv3ZCp7y9wltPg2",
    "published": "2016-05",
    "summary": "Canonical correlation analysis (CCA) is a fundamental technique in multi-view data analysis and representation learning. Several nonlinear extensions of the classical linear CCA method have been proposed, including kernel and deep neural network methods. These approaches restrict attention to certain families of nonlinear projections, which the user must specify (by choosing a kernel or a neural network architecture), and are computationally demanding. Interestingly, the theory of nonlinear CCA without any functional restrictions, has been studied in the population setting by Lancaster already in the 50\u2019s. However, these results, have not inspired practical algorithms. In this paper, we revisit Lancaster\u2019s theory, and use it to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the most correlated nonlinear projections of two random vectors can be expressed in terms of the singular value decomposition of a certain operator associated with their joint density. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without having to compute the inverse of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. PLCCA turns out to have a similar form to the classical linear CCA, but with a nonparametric regression term replacing the linear regression in CCA. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and achieve better performance than kernel CCA and comparable performance to deep CCA."
  },
  "iclr2016_workshop_documentcontextlanguagemodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Document Context Language Models",
    "authors": [
      "Yangfeng Ji",
      "Trevor Cohn",
      "Lingpeng Kong",
      "Chris Dyer",
      "Jacob Eisenstein"
    ],
    "page_url": "https://openreview.net/forum?id=vlpy96kV2C7OYLG5inQw",
    "pdf_url": "https://openreview.net/pdf?id=vlpy96kV2C7OYLG5inQw",
    "published": "2016-05",
    "summary": "Text documents are structured on multiple levels of detail: individual words are related by syntax, and larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLMs), which incorporate contextual information both within and beyond the sentence. In comparison with sentence-level recurrent neural network language models, the DCLMs obtain slightly better predictive likelihoods, and considerably better assessments of document coherence."
  },
  "iclr2016_workshop_unsupervisedlearningofvisualstructureusingpredictivegenerativenetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Unsupervised Learning of Visual Structure using Predictive Generative Networks",
    "authors": [
      "William Lotter",
      "Gabriel Kreiman",
      "David Cox"
    ],
    "page_url": "https://openreview.net/forum?id=BNYAGG8r9F7PwR1riXz8",
    "pdf_url": "https://openreview.net/pdf?id=BNYAGG8r9F7PwR1riXz8",
    "published": "2016-05",
    "summary": "The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features."
  },
  "iclr2016_workshop_parsenetlookingwidertoseebetter": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "ParseNet: Looking Wider to See Better",
    "authors": [
      "Wei Liu",
      "Andrew Rabinovich",
      "Alexander C. Berg"
    ],
    "page_url": "https://openreview.net/forum?id=Qn8x8rGr5CkB2l8pUY8P",
    "pdf_url": "https://openreview.net/pdf?id=Qn8x8rGr5CkB2l8pUY8P",
    "published": "2016-05",
    "summary": "We present a technique for adding global context to fully convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN~\\cite{long2014fully}). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at \\url{https://github.com/weiliu89/caffe/tree/fcn} ."
  },
  "iclr2016_workshop_whyaredeepnetsreversibleasimpletheory,withimplicationsfortraining": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Why are deep nets reversible: A simple theory, with implications for training",
    "authors": [
      "Sanjeev Arora",
      "Yingyu Liang",
      "Tengyu Ma"
    ],
    "page_url": "https://openreview.net/forum?id=q7kqBKMN2U8LEkD3t7Xy",
    "pdf_url": "https://openreview.net/pdf?id=q7kqBKMN2U8LEkD3t7Xy",
    "published": "2016-05",
    "summary": "Generative models fordeep learning are promisingboth to improve understanding of the model, and yield training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above.However, there is no accompanying proof of correctness for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input.Furthermore, these models are complicated.The current paper takes a more theoretical tack. It presents a very simple generative model for ReLU deep nets, with the following characteristics: (i) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.) (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method forrecovering the hidden layer. The generative model suggests a simple modification for training: use the generative model to produce synthetic data with labels andinclude it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training.This extended abstract provides a succinct description of our results while the full paper is available on arXiv."
  },
  "iclr2016_workshop_bindingviareconstructionclustering": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Binding via Reconstruction Clustering",
    "authors": [
      "Klaus Greff",
      "Rupesh Srivastava",
      "J\u00fcrgen Schmidhuber"
    ],
    "page_url": "https://openreview.net/forum?id=gZ9Oq3ZGVCAPowrRUANz",
    "pdf_url": "https://openreview.net/pdf?id=gZ9Oq3ZGVCAPowrRUANz",
    "published": "2016-05",
    "summary": "Disentangled distributed representations of data are desirable for machine learning, since they are more expressive and can generalize from fewer examples. However, for complex data, the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities, which is commonly referred to as the binding problem.We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. We propose an algorithm that uses a denoising autoencoder to dynamically bind features together in multi-object inputs through an Expectation-Maximization-like clustering process.The effectiveness of this method is demonstrated on artificially generated datasets of binary images, showing that it can even generalize to bind together new objects never seen by the autoencoder during training."
  },
  "iclr2016_workshop_dynamiccapacitynetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Dynamic Capacity Networks",
    "authors": [
      "Amjad Almahairi",
      "Nicolas Ballas",
      "Tim Cooijmans",
      "Yin Zheng",
      "Hugo Larochelle",
      "Aaron Courville"
    ],
    "page_url": "https://openreview.net/forum?id=oVgBVKDQmCrlgPMRsrPQ",
    "pdf_url": "https://openreview.net/pdf?id=oVgBVKDQmCrlgPMRsrPQ",
    "published": "2016-05",
    "summary": "We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN\u2019s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance."
  },
  "iclr2016_workshop_learningrepresentationsofaffectfromspeech": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Representations of Affect from Speech",
    "authors": [
      "Sayan Ghosh",
      "Eugene Laksana",
      "Louis-Philippe Morency",
      "Stefan Scherer"
    ],
    "page_url": "https://openreview.net/forum?id=Jy9kxYV3ziqp6ARvtWXZ",
    "pdf_url": "https://openreview.net/pdf?id=Jy9kxYV3ziqp6ARvtWXZ",
    "published": "2016-05",
    "summary": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes, i.e. categorical and dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence (sadness and anger) from positive valence (happiness). We experiment with different input speech features (such as FFT and log-mel spectrograms with temporal context windows), and different autoencoder architectures (such as stacked and deep autoencoders). We also learn utterance specific representations by a combination of denoising autoencoders and BLSTM based recurrent autoencoders. Emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the learnt representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition."
  },
  "iclr2016_workshop_neuralvariationalinferencefortextprocessing": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Variational Inference for Text Processing",
    "authors": [
      "Yishu Miao",
      "Lei Yu",
      "Phil Blunsom"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOPGYLgCRNGwArs4Bm",
    "pdf_url": "https://openreview.net/pdf?id=L7VOPGYLgCRNGwArs4Bm",
    "published": "2016-05",
    "summary": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks."
  },
  "iclr2016_workshop_recurrentmodelsforauditoryattentioninmulti-microphonedistancespeechrecognition": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distance Speech Recognition",
    "authors": [
      "Suyoun Kim",
      "Ian Lane"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo9jD93urlgPMRsB1B",
    "pdf_url": "https://openreview.net/pdf?id=oVgo9jD93urlgPMRsB1B",
    "published": "2016-05",
    "summary": "Integration of multiple microphone data is one of the key ways to achieve robust speech recognition in noisy environments or when the speaker is located at some distance from the input device. Signal processing techniques such as beamforming are widely used to extract a speech signal of interest from background noise. These techniques, however, are highly dependent on prior spatial information about the microphones and the environment in which the system is being used. In this work, we present a neural attention network that directly combines multichannel audio to generate phonetic states without requiring any prior knowledge of the microphone layout or any explicit signal preprocessing for speech enhancement. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source. Unlike traditional multi-channel preprocessing, our system can be optimized towards the desired output in one step. Although attention-based models have recently achieved impressive results on sequence-to-sequence learning, no attention mechanisms have previously been applied to learn potentially asynchronous and non-stationary multiple inputs. We evaluate our neural attention model on the CHiME-3 challenge task, and show that the model achieves comparable performance to beamforming using a purely data-driven method."
  },
  "iclr2016_workshop_adeepmemory-basedarchitectureforsequence-to-sequencelearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning",
    "authors": [
      "Fandong Meng",
      "Zhengdong Lu",
      "Zhaopeng Tu",
      "Hang Li",
      "Qun Liu"
    ],
    "page_url": "https://openreview.net/forum?id=GvVr3PmmGC1WDOmRiEo6",
    "pdf_url": "https://openreview.net/pdf?id=GvVr3PmmGC1WDOmRiEo6",
    "published": "2016-05",
    "summary": "We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequence-to-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size."
  },
  "iclr2016_workshop_deconstructingtheladdernetworkarchitecture": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deconstructing the Ladder Network Architecture",
    "authors": [
      "Mohammad Pezeshki",
      "Linxi Fan",
      "Philemon Brakel",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=NL6ggGZ5wI0VOPA8ixm2",
    "pdf_url": "https://openreview.net/pdf?id=NL6ggGZ5wI0VOPA8ixm2",
    "published": "2016-05",
    "summary": "The Ladder Network is a recent new approach to semi-supervised learning that turned out to be very successful. While showing impressive performance, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. This paper presents an extensive experimental investigation of variants of the Ladder Network in which we replaced or removed individual components to learn about their relative importance. For semi-supervised tasks, we conclude thatthe most important contribution is made by the lateral connections, followed by the application of noise, and the choice of what we refer to as the `combinator function'. As the number of labeled training examples increases, the lateral connections and the reconstruction criterion become less important, with most of the generalization improvement coming from the injection of noise in each layer. Finally, we introduce a combinator function that reduces test error rates on Permutation-Invariant MNIST to 0.57\\% for the supervised setting, and to 0.97 % and 1.0 % for semi-supervised settings with 1000 and 100 labeled examples, respectively."
  },
  "iclr2016_workshop_neuralnetwork-basedclusteringusingpairwiseconstraints": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural network-based clustering using pairwise constraints",
    "authors": [
      "Yen-Chang Hsu",
      "Zsolt Kira"
    ],
    "page_url": "https://openreview.net/forum?id=0YrNVxKMAUGJ7gK5tR5K",
    "pdf_url": "https://openreview.net/pdf?id=0YrNVxKMAUGJ7gK5tR5K",
    "published": "2016-05",
    "summary": "This paper presents a neural network-based end-to-end clustering framework. We design a novel strategy to utilize the contrastive criteria for pushing data-forming clusters directly from raw data, in addition to learning a feature embedding suitable for such clustering. The network is trained with weak labels, specifically partial pairwise relationships between data instances. The cluster assignments and their probabilities are then obtained at the output layer by feed-forwarding the data. The framework has the interesting characteristic that no cluster centers need to be explicitly specified, thus the resulting cluster distribution is purely data-driven and no distance metrics need to be predefined. The experiments show that the proposed approach beats the conventional two-stage method (feature embedding with k-means) by a significant margin. It also compares favorably to the performance of the standard cross entropy loss for classification. Robustness analysis also shows that the method is largely insensitive to the number of clusters. Specifically, we show that the number of dominant clusters is close to the true number of clusters even when a large k is used for clustering. "
  },
  "iclr2016_workshop_lstm-baseddeeplearningmodelsfornon-factoidanswerselection": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "LSTM-based Deep Learning Models for non-factoid answer selection",
    "authors": [
      "Ming Tan",
      "Cicero dos Santos",
      "Bing Xiang",
      "Bowen Zhou"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xwl3PDS5Pk8ELfEzP",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xwl3PDS5Pk8ELfEzP",
    "published": "2016-05",
    "summary": "In this paper, we apply a general deep learning framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One direction is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework. The other direction is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. The models are examined by two datasets, including TREC-QA and InsuranceQA. Experimental results demonstrate that the proposed models substantially outperform several strong baselines."
  },
  "iclr2016_workshop_usingdeeplearningtopredictdemographicsfrommobilephonemetadata": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Using Deep Learning to Predict Demographics from Mobile Phone Metadata",
    "authors": [
      "Bjarke Felbo",
      "P\u00e5l Sunds\u00f8y",
      "Alex 'Sandy' Pentland",
      "Sune Lehmann",
      "Yves-Alexandre de Montjoye"
    ],
    "page_url": "https://openreview.net/forum?id=91EEnoZX0HkRlNvXUKLA",
    "pdf_url": "https://openreview.net/pdf?id=91EEnoZX0HkRlNvXUKLA",
    "published": "2016-05",
    "summary": "Mobile phone metadata are increasingly used to study human behavior at large-scale. There has recently been a growing interest in predicting demographic information from metadata. Previous approaches relied on hand-engineered features. We here apply, for the first time, deep learning methods to mobile phone metadata using a convolutional network. Our method provides high accuracy on both age and gender prediction. These results show great potential for deep learning approaches for prediction tasks using standard mobile phone metadata."
  },
  "iclr2016_workshop_efficientinferenceinocclusion-awaregenerativemodelsofimages": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Efficient Inference in Occlusion-Aware Generative Models of Images",
    "authors": [
      "Jonathan Huang",
      "Kevin Murphy"
    ],
    "page_url": "https://openreview.net/forum?id=E8Vg037q7f31v0m2iqn3",
    "pdf_url": "https://openreview.net/pdf?id=E8Vg037q7f31v0m2iqn3",
    "published": "2016-05",
    "summary": "We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image \u2014 and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a consistent shape, without needing access to motion cues or any labeled data. We show that ordinary stochastic gradient variational bayes (SGVB), which optimizes our fully differentiable lower-bound on the log-likelihood, is sufficient to learn an interpretable representation of images. Finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images."
  },
  "iclr2016_workshop_convolutionalmodelsforjointobjectcategorizationandposeestimation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Convolutional Models for Joint Object Categorization and Pose Estimation",
    "authors": [
      "Mohamed Elhoseiny",
      "Tarek El-Gaaly",
      "Amr Bakry",
      "Ahmed Elgammal"
    ],
    "page_url": "https://openreview.net/forum?id=WL9EYwkKyI5zMX2Kf2V2",
    "pdf_url": "https://openreview.net/pdf?id=WL9EYwkKyI5zMX2Kf2V2",
    "published": "2016-05",
    "summary": "In the task of Object Recognition, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose regression using these approaches has received relatively much less attention. In this paper we show how deep architectures, specifically Convolutional Neural Networks (CNN), can be adapted to the task of simultaneous categorization and pose estimation of objects. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations of CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets. Our models achieve better than state-of-the-art performance on both datasets."
  },
  "iclr2016_workshop_basiclevelcategorizationfacilitatesvisualobjectrecognition": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Basic Level Categorization Facilitates Visual Object Recognition",
    "authors": [
      "Panqu Wang",
      "Garrison Cottrell"
    ],
    "page_url": "https://openreview.net/forum?id=4QyyQzGjptBYD9yOF80y",
    "pdf_url": "https://openreview.net/pdf?id=4QyyQzGjptBYD9yOF80y",
    "published": "2016-05",
    "summary": "Recent advances in deep learning have led to significant progress in the computer vision field, especially for visual object recognition tasks. The features useful for object classification are learned by feed-forward deep convolutional neural networks (CNNs) automatically, and they are shown to be able to predict and decode neural representations in the ventral visual pathway of humans and monkeys. However, despite the huge amount of work on optimizing CNNs, there has not been much research focused on linking CNNs with guiding principles from the human visual cortex. In this work, we propose a network optimization strategy inspired by both of the developmental trajectory of children\u2019s visual object recognition capabilities, and Bar (2003), who hypothesized that basic level information is carried in the fast magnocellular pathway through the prefrontal cortex (PFC) and then projected back to inferior temporal cortex (IT), where subordinate level categorization is achieved. We instantiate this idea by training a deep CNN to perform basic level object categorization first, and then train it on subordinate level categorization. We apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC 2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%, demonstrating the effectiveness of the method. We also show that subsequent transfer learning on smaller datasets gives superior results. "
  },
  "iclr2016_workshop_learningtorepresentwordsincontextwithmultilingualsupervision": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning to Represent Words in Context with Multilingual Supervision",
    "authors": [
      "Kazuya Kawakami",
      "Chris Dyer"
    ],
    "page_url": "https://openreview.net/forum?id=nx9Av7Am9T7lP3z2ioYK",
    "pdf_url": "https://openreview.net/pdf?id=nx9Av7Am9T7lP3z2ioYK",
    "published": "2016-05",
    "summary": "We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these."
  },
  "iclr2016_workshop_fine-grainedposeprediction,normalization,andrecognition": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Fine-grained pose prediction, normalization, and recognition",
    "authors": [
      "Ning Zhang",
      "Evan Shelhamer",
      "Yang Gao",
      "Trevor Darrell"
    ],
    "page_url": "https://openreview.net/forum?id=ROVAEyEoPHvnM0J1IpPO",
    "pdf_url": "https://openreview.net/pdf?id=ROVAEyEoPHvnM0J1IpPO",
    "published": "2016-05",
    "summary": "Pose variation and subtle differences in appearance are key challenges to fine- grained classification. While deep networks have markedly improved general recognition, many approaches to fine-grained recognition rely on anchoring net- works to parts for better accuracy. Identifying parts to find correspondence dis- counts pose variation so that features can be tuned to appearance. To this end previous methods have examined how to find parts and extract pose-normalized features. These methods have generally separated fine-grained recognition into stages which first localize parts using hand-engineered and coarsely-localized pro- posal features, and then separately learn deep descriptors centered on inferred part positions. We unify these steps in an end-to-end trainable network supervised by keypoint locations and class labels that localizes parts by a fully convolutional network to focus the learning of feature representations for the fine-grained clas- sification task. Experiments on the popular CUB200 dataset show that our method is state-of-the-art and suggest a continuing role for strong supervision."
  },
  "iclr2016_workshop_scalablegradient-basedtuningofcontinuousregularizationhyperparameters": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters",
    "authors": [
      "Jelena Luketina",
      "Mathias Berglund",
      "Tapani Raiko"
    ],
    "page_url": "https://openreview.net/forum?id=ANYzz8LXgINrwlgXCqGj",
    "pdf_url": "https://openreview.net/pdf?id=ANYzz8LXgINrwlgXCqGj",
    "published": "2016-05",
    "summary": "Hyperparameter selection generally relies on running multiple full training trials, with hyperparameter selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters during training of the model. Hyperparameters are adjusted so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST the resulting regularization levels are within the optimal regions. The method is significantly less computationally demanding compared to similar gradient-based approaches to hyperparameter optimization and consistently finds good hyperparameter values, which makes it a useful tool for training neural network models."
  },
  "iclr2016_workshop_unitaryevolutionrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Unitary Evolution Recurrent Neural Networks",
    "authors": [
      "Martin Arjovsky",
      "Amar Shah",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=71BmDZPEluAE8VvKUQ80",
    "pdf_url": "https://openreview.net/pdf?id=71BmDZPEluAE8VvKUQ80",
    "published": "2016-05",
    "summary": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies."
  },
  "iclr2016_workshop_temporalconvolutionalneuralnetworksfordiagnosisfromlabtests": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Temporal Convolutional Neural Networks for Diagnosis from Lab Tests",
    "authors": [
      "Narges Razavian",
      "David Sontag"
    ],
    "page_url": "https://openreview.net/forum?id=ROVmO430RTvnM0J1Ip9z",
    "pdf_url": "https://openreview.net/pdf?id=ROVmO430RTvnM0J1Ip9z",
    "published": "2016-05",
    "summary": "Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases' onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 298K individuals over 8 years, 18 common lab measurements, and 171 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis."
  },
  "iclr2016_workshop_perforatedcnnsaccelerationthrougheliminationofredundantconvolutions": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions",
    "authors": [
      "Michael Figurnov",
      "Dmitry Vetrov",
      "Pushmeet Kohli"
    ],
    "page_url": "https://openreview.net/forum?id=k80x272vDcOYKX7jiYKw",
    "pdf_url": "https://openreview.net/pdf?id=k80x272vDcOYKX7jiYKw",
    "published": "2016-05",
    "summary": "We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. Our method allows to reduce the evaluation time of modern convolutional neural networks by 50% with a small decrease in accuracy."
  },
  "iclr2016_workshop_howfarcanwegowithoutconvolutionimprovingfully-connectednetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "How far can we go without convolution: Improving fully-connected networks",
    "authors": [
      "Zhouhan Lin",
      "Roland Memisevic",
      "Kishore Konda"
    ],
    "page_url": "https://openreview.net/forum?id=1WvovwjA7UMnPB1oinBL",
    "pdf_url": "https://openreview.net/pdf?id=1WvovwjA7UMnPB1oinBL",
    "published": "2016-05",
    "summary": "We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases. We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutation-invariant CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is close to the performance of a decent convolutional network. "
  },
  "iclr2016_workshop_learningdenseconvolutionalembeddingsforsemanticsegmentation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Dense Convolutional Embeddings for Semantic Segmentation",
    "authors": [
      "Adam W. Harley",
      "Konstantinos G. Derpanis",
      "Iasonas Kokkinos"
    ],
    "page_url": "https://openreview.net/forum?id=mO9mx9y48ij1gPZ3UlOk",
    "pdf_url": "https://openreview.net/pdf?id=mO9mx9y48ij1gPZ3UlOk",
    "published": "2016-05",
    "summary": "This paper proposes a new deep convolutional neural network (DCNN) architecture that learns pixel embeddings, such that pairwise distances between the embeddings can be used to infer whether or not the pixels lie in the same region. That is, for any two pixels on the same object, the embeddings are trained to be similar; for any pair that straddles an object boundary, the embeddings are trained to be dissimilar. Experimental results show that when the embeddings are used in conjunction with a DCNN trained on semantic segmentation, there is a systematic improvement in per-pixel classification accuracy. These contributions are integrated in the popular Caffe deep learning framework, and consist in straightforward modifications to convolution routines. As such, they can be exploited for any task involving convolution layers."
  },
  "iclr2016_workshop_generatingsentencesfromacontinuousspace": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Generating Sentences from a Continuous Space",
    "authors": [
      "Samuel R. Bowman",
      "Luke Vilnis",
      "Oriol Vinyals",
      "Andrew M. Dai",
      "Rafal Jozefowicz",
      "Samy Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=D1VVBv7BKS5jEJ1zfxJg",
    "pdf_url": "https://openreview.net/pdf?id=D1VVBv7BKS5jEJ1zfxJg",
    "published": "2016-05",
    "summary": "The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space."
  },
  "iclr2016_workshop_stackedwhat-whereauto-encoders": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Stacked What-Where Auto-encoders",
    "authors": [
      "Junbo Zhao",
      "Michael Mathieu",
      "Ross Goroshin",
      "Yann LeCun"
    ],
    "page_url": "https://openreview.net/forum?id=1WvkoMpX3FMnPB1oiq8Z",
    "pdf_url": "https://openreview.net/pdf?id=1WvkoMpX3FMnPB1oiq8Z",
    "published": "2016-05",
    "summary": "We present a novel architecture, the stacked what-where auto-encoders (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the what which are fed to the next layer, and its complementary variable where that are fed to the corresponding layer in the generative decoder."
  },
  "iclr2016_workshop_bayesianconvolutionalneuralnetworkswithbernoulliapproximatevariationalinference": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference",
    "authors": [
      "Yarin Gal",
      "Zoubin Ghahramani"
    ],
    "page_url": "https://openreview.net/forum?id=3QxqXoJEyfp7y9wltP11",
    "pdf_url": "https://openreview.net/pdf?id=3QxqXoJEyfp7y9wltP11",
    "published": "2016-05",
    "summary": "Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data \u2013 as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN\u2019s kernels. We approximate our model\u2019s intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10."
  },
  "iclr2016_workshop_blendinglstmsintocnns": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Blending LSTMs into CNNs",
    "authors": [
      "Krzysztof J. Geras",
      "Abdel-rahman Mohamed",
      "Rich Caruana",
      "Gregor Urban",
      "Shengjie Wang",
      "Ozlem Aslan",
      "Matthai Philipose",
      "Matthew Richardson",
      "Charles Sutton"
    ],
    "page_url": "https://openreview.net/forum?id=k80kvBj55IOYKX7ji4V4",
    "pdf_url": "https://openreview.net/pdf?id=k80kvBj55IOYKX7ji4V4",
    "published": "2016-05",
    "summary": "We consider whether deep convolutional networks (CNNs) can represent decision functions with similar accuracy as recurrent networks such as LSTMs. First, we show that a deep CNN with an architecture inspired by the models recently introduced in image recognition can yield better accuracy than previous convolutional and LSTM networks on the standard 309h Switchboard automatic speech recognition task. Then we show that even more accurate CNNs can be trained under the guidance of LSTMs using a variant of model compression, which we call model blending because the teacher and student models are similar in complexity but different in inductive bias. Blending further improves the accuracy of our CNN, yielding a computationally efficient model of accuracy higher than any of the other individual models. Examining the effect of \u201cdark knowledge\u201d in this model compression task, we find that less than 1% of the highest probability labels are needed for accurate model compression."
  },
  "iclr2016_workshop_empiricalperformanceupperboundsforimageandvideocaptioning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Empirical performance upper bounds for image and video captioning",
    "authors": [
      "Li Yao",
      "Nicolas Ballas",
      "Kyunghyun Cho",
      "John R. Smith",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=vlprXrN35c7OYLG5irDB",
    "pdf_url": "https://openreview.net/pdf?id=vlprXrN35c7OYLG5irDB",
    "published": "2016-05",
    "summary": "The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets."
  },
  "iclr2016_workshop_adversarialautoencoders": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Adversarial Autoencoders",
    "authors": [
      "Alireza Makhzani",
      "Jonathon Shlens",
      "Navdeep Jaitly",
      "Ian Goodfellow"
    ],
    "page_url": "https://openreview.net/forum?id=2xwp4Zwr3TpKBZvXtWoj",
    "pdf_url": "https://openreview.net/pdf?id=2xwp4Zwr3TpKBZvXtWoj",
    "published": "2016-05",
    "summary": "In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named adversarial autoencoder, uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no holes in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets."
  },
  "iclr2016_workshop_deepreinforcementlearningwithanactionspacedefinedbynaturallanguage": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Reinforcement Learning with an Action Space Defined by Natural Language",
    "authors": [
      "Ji He",
      "Jianshu Chen",
      "Xiaodong He",
      "Jianfeng Gao",
      "Lihong Li",
      "Li Deng",
      "Mari Ostendorf"
    ],
    "page_url": "https://openreview.net/forum?id=WL9AjgWvPf5zMX2Kfoj5",
    "pdf_url": "https://openreview.net/pdf?id=WL9AjgWvPf5zMX2Kfoj5",
    "published": "2016-05",
    "summary": "In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, to design a model for handling an action space characterized using natural language with applications to text-based games. For a particular class of games, a user must choose among a number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically what fits the current situation best (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Even with a constrained vocabulary, the action space is very large and sparse, posing challenges for learning. To address this challenge, the DRRN extracts separate high-level embedding vectors from the texts that describe states and actions, respectively, using a general interaction function, such as inner product, bilinear, and DNN interaction, between these embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. "
  },
  "iclr2016_workshop_universumprescriptionregularizationusingunlabeleddata": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Universum Prescription: Regularization using Unlabeled Data",
    "authors": [
      "Xiang Zhang",
      "Yann LeCun"
    ],
    "page_url": "https://openreview.net/forum?id=91EvmZNlwtkRlNvXUKL9",
    "pdf_url": "https://openreview.net/pdf?id=91EvmZNlwtkRlNvXUKL9",
    "published": "2016-05",
    "summary": "This paper shows that simply prescribing none of the above labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100 and STL-10 datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter -- probability of sampling from unlabeled data -- is also studied empirically."
  },
  "iclr2016_workshop_variancereductioninsgdbydistributedimportancesampling": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Variance Reduction in SGD by Distributed Importance Sampling",
    "authors": [
      "Guillaume Alain",
      "Alex Lamb",
      "Chinnadhurai Sankar",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=r8lrEqPpYF8wknpYt57j",
    "pdf_url": "https://openreview.net/pdf?id=r8lrEqPpYF8wknpYt57j",
    "published": "2016-05",
    "summary": "Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty.We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling.This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set."
  },
  "iclr2016_workshop_addinggradientnoiseimproveslearningforverydeepnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Adding Gradient Noise Improves Learning for Very Deep Networks",
    "authors": [
      "Arvind Neelakantan",
      "Luke Vilnis",
      "Quoc V. Le",
      "Ilya Sutskever",
      "Lukasz Kaiser",
      "Karol Kurach",
      "James Martens"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xxQDMMu5Pk8ELfEz4",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xxQDMMu5Pk8ELfEz4",
    "published": "2016-05",
    "summary": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. A major reason for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures. "
  },
  "iclr2016_workshop_blackboxvariationalinferenceforstatespacemodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Black Box Variational Inference for State Space Models",
    "authors": [
      "Evan Archer",
      "Il Memming Park",
      "Lars Buesing",
      "John Cunningham",
      "Liam Paninski"
    ],
    "page_url": "https://openreview.net/forum?id=P7q1lVQQvSKvjNORtJZL",
    "pdf_url": "https://openreview.net/pdf?id=P7q1lVQQvSKvjNORtJZL",
    "published": "2016-05",
    "summary": "Latent variable time-series models are among the most heavily used tools from machine learning and applied statistics. These models have the advantage of learning latent structure both from noisy observations and from the temporal ordering in the data, where it is assumed that meaningful correlation structure exists across time. A few highly-structured models, such as the linear dynamical system with linear-Gaussian observations, have closed-form inference procedures (e.g. the Kalman Filter), but this case is an exception to the general rule that exact posterior inference in more complex generative models is intractable. Consequently, much work in time-series modeling focuses on approximate inference procedures for one particular class of models. Here, we extend recent developments in stochastic variational inference to develop a `black-box' approximate inference technique for latent variable models with latent dynamical structure. We propose a structured Gaussian variational approximate posterior that carries the same intuition as the standard Kalman filter-smoother but, importantly, permits us to use the same inference approach to approximate the posterior of much more general, nonlinear latent variable generative models. We show that our approach recovers accurate estimates in the case of basic models with closed-form posteriors, and more interestingly performs well in comparison to variational approaches that were designed in a bespoke fashion for specific non-conjugate models."
  }
}