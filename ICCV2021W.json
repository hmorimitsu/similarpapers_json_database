{
  "iccv2021_arow_canopticaltrojansassistadversarialperturbations?": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Can Optical Trojans Assist Adversarial Perturbations?",
    "authors": [
      "Adith Boloor",
      "Tong Wu",
      "Patrick Naughton",
      "Ayan Chakrabarti",
      "Xuan Zhang",
      "Yevgeniy Vorobeychik"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Boloor_Can_Optical_Trojans_Assist_Adversarial_Perturbations_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Boloor_Can_Optical_Trojans_Assist_Adversarial_Perturbations_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recent work has demonstrated how physically realizable attacks on neural network vision pipelines can consistently produce misclassifications of a given target object. A smaller body of work has also produced modifications that can be applied directly to the neural network to generate incorrect predictions. However, although these perturbations are difficult to detect from examining the resulting images themselves, they are obvious if any testing is done on the network to check its accuracy. Here, we combine methods from both these lines of work to generate attacks that can be switched on or off. Specifically, we simulate a physically realizable Trojaned lens to attach to a camera that only causes the neural network vision pipeline to produce incorrect classifications if a specific adversarial patch is present in the scene. This novel Optical Trojan is used to amplify the effect of the adversarial patch so that we can achieve similar attack performance with smaller and less noticeable patches. To improve the robustness of our proposed method, we take into account the fabrication process with quantized lens parameters, deal with lens defocus using kernel scaling, and make it resilient against noise caused by the camera sensor readouts and test in various simulated settings. Finally, we propose a simple yet effective approach to detect such Trojaned lenses by analyzing the distributions of benign and Trojaned kernels.",
    "code_link": ""
  },
  "iccv2021_arow_onadversarialrobustnessaneuralarchitecturesearchperspective": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "On Adversarial Robustness: A Neural Architecture Search Perspective",
    "authors": [
      "Chaitanya Devaguptapu",
      "Devansh Agarwal",
      "Gaurav Mittal",
      "Pulkit Gopalani",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Devaguptapu_On_Adversarial_Robustness_A_Neural_Architecture_Search_Perspective_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Devaguptapu_On_Adversarial_Robustness_A_Neural_Architecture_Search_Perspective_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Adversarial robustness of deep learning models has gained much traction in the last few years. Various attacks and defenses are proposed to improve the adversarial robustness of modern-day deep learning architectures. While all these approaches help improve the robustness, one promising direction for improving adversarial robustness is unexplored, i.e, the complex topology of the neural network architecture. In this work, we address the following question: \"Can the complex topology of a neural network give adversarial robustness without any form of adversarial training?\". We answer this empirically by experimenting with different hand-crafted and NAS-based architectures. Our findings show that, for small-scale attacks, NAS-based architectures are more robust for small-scale datasets and simple tasks than hand-crafted architectures. However, as the size of the dataset or the complexity of task increases, hand-crafted architectures are more robust than NAS-based architectures. Our work is the first large-scale study to understand adversarial robustness purely from an architectural perspective. Our study shows that random sampling in the search space of DARTS (a popular NAS method) with simple ensembling can improve the robustness to PGD attack by nearly 12%. We show that NAS, which is popular for achieving SoTA accuracy, can provide adversarial accuracy as a free add-on without any form of adversarial training. Our results show that leveraging the search space of NAS methods with methods like ensembles can be an excellent way to achieve adversarial robustness without any form of adversarial training. We also introduce a metric that can be used to calculate the trade-off between clean accuracy and adversarial robustness. Code and pre-trained models will be made available at https://github.com/tdchaitanya/nas-robustness",
    "code_link": "https://github.com/tdchaitanya/nas-robustness"
  },
  "iccv2021_arow_towardscategoryanddomainalignmentcategory-invariantfeatureenhancementforadversarialdomainadaptation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Towards Category and Domain Alignment: Category-Invariant Feature Enhancement for Adversarial Domain Adaptation",
    "authors": [
      "Yuan Wu",
      "Diana Inkpen",
      "Ahmed El-Roby"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Wu_Towards_Category_and_Domain_Alignment_Category-Invariant_Feature_Enhancement_for_Adversarial_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Wu_Towards_Category_and_Domain_Alignment_Category-Invariant_Feature_Enhancement_for_Adversarial_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Adversarial domain adaptation has made impressive advances in transferring knowledge from the source domain to the target domain by aligning feature distributions of both domains. These methods focus on minimizing domain divergence and regard the adaptability, which is measured as the expected error of the ideal joint hypothesis on these two domains, as a small constant. However, these approaches still face two issues: (1) Adversarial domain alignment distorts the original feature distributions, deteriorating the adaptability; (2) Transforming feature representations to be domain-invariant needs to sacrifice domain-specific variations, resulting in weaker discriminability. In order to alleviate these issues, we propose category-invariant feature enhancement (CIFE), a general mechanism that enhances the adversarial domain adaptation through optimizing the adaptability. Specifically, the CIFE approach introduces category-invariant features to boost the discriminability of domain-invariant features with preserving the transferability. Experiments show that the CIFE could improve upon representative adversarial domain adaptation methods to yield state-of-the-art results on five benchmarks.",
    "code_link": ""
  },
  "iccv2021_arow_evasionattacksteganographyturningvulnerabilityofmachinelearningtoadversarialattacksintoareal-worldapplication": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Evasion Attack STeganography: Turning Vulnerability of Machine Learning To Adversarial Attacks Into a Real-World Application",
    "authors": [
      "Salah Ghamizi",
      "Maxime Cordy",
      "Mike Papadakis",
      "Yves Le Traon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Ghamizi_Evasion_Attack_STeganography_Turning_Vulnerability_of_Machine_Learning_To_Adversarial_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Ghamizi_Evasion_Attack_STeganography_Turning_Vulnerability_of_Machine_Learning_To_Adversarial_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Evasion Attacks have been commonly seen as a weakness of Deep Neural Networks. In this paper, we flip the paradigm and envision this vulnerability as a useful application. We propose EAST, a new steganography and watermarking technique based on multi-label targeted evasion attacks. The key idea of EAST is to encode data as the labels of the image that the evasion attacks produce. Our results confirm that our embedding is elusive; it not only passes unnoticed by humans, steganalysis methods, and machine-learning detectors. In addition, our embedding is resilient to soft and aggressive image tampering (87% recovery rate under jpeg compression). EAST outperforms existing deep-learning-based steganography approaches with images that are 70% denser and 73% more robust and supports multiple datasets and architectures.",
    "code_link": "https://github.com/yamizi/Adversarial-Embedding"
  },
  "iccv2021_arow_encouragingintra-classdiversitythroughareversecontrastivelossforsingle-sourcedomaingeneralization": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Single-Source Domain Generalization",
    "authors": [
      "Thomas Duboudin",
      "Emmanuel Dellandr\u00e9a",
      "Corentin Abgrall",
      "Gilles H\u00e9naff",
      "Liming Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Duboudin_Encouraging_Intra-Class_Diversity_Through_a_Reverse_Contrastive_Loss_for_Single-Source_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Duboudin_Encouraging_Intra-Class_Diversity_Through_a_Reverse_Contrastive_Loss_for_Single-Source_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Traditional deep learning algorithms often fail to generalize when they are tested outside of the domain of the training data. The issue can be mitigated by using unlabeled data from the target domain at training time, but because data distributions can change dynamically in real-life applications once a learned model is deployed, it is critical to create networks robust to unknown and unforeseen domain shifts. In this paper we focus on one of the reasons behind the inability of neural networks to be so: deep networks focus only on the most obvious, potentially spurious, clues to make their predictions and are blind to useful but slightly less efficient or more complex patterns. This behaviour has been identified and several methods partially addressed the issue. To investigate their effectiveness and limits, we first design a publicly available MNIST-based benchmark to precisely measure the ability of an algorithm to find the \"hidden\" patterns. Then, we evaluate state-of-the-art algorithms through our benchmark and show that the issue is largely unsolved. Finally, we propose a partially reversed contrastive loss to encourage intra-class diversity and find less strongly correlated patterns, whose efficiency is demonstrated by our experiments.",
    "code_link": ""
  },
  "iccv2021_arow_trojansignaturesindnnweights": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Trojan Signatures in DNN Weights",
    "authors": [
      "Greg Fields",
      "Mohammad Samragh",
      "Mojan Javaheripi",
      "Farinaz Koushanfar",
      "Tara Javidi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Fields_Trojan_Signatures_in_DNN_Weights_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Fields_Trojan_Signatures_in_DNN_Weights_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep neural networks have been shown to be vulnerable to backdoor, or Trojan, attacks where an adversary has embedded a trigger in the network at training time such that the model correctly classifies all standard inputs, but generates a targeted, incorrect classification on any input which contains the trigger. In this paper, we present the first ultra light-weight and highly effective Trojan detection method that does not require access to the training/test data, does not involve any expensive computations, and makes no assumptions on the nature of the Trojan trigger. Our approach focuses on analysis of the weights of the final, linear layer of the network. We empirically demonstrate several characteristics of these weights that occur frequently in Trojaned networks, but not in benign networks. In particular, we show that the distribution of the weights associated with the Trojan target class is clearly distinguishable from the weights associated with other classes. Using this, we demonstrate the effectiveness of our proposed detection method against state-of-the-art attacks across a variety of architectures, datasets, and trigger types.",
    "code_link": "https://github.com/PurduePAML/TrojanNN"
  },
  "iccv2021_arow_ontheeffectofpruningonadversarialrobustness": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "On the Effect of Pruning on Adversarial Robustness",
    "authors": [
      "Artur Jord\u00e3o",
      "H\u00e9lio Pedrini"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Jordao_On_the_Effect_of_Pruning_on_Adversarial_Robustness_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Jordao_On_the_Effect_of_Pruning_on_Adversarial_Robustness_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Pruning is a well-known mechanism for reducing the computational cost of deep convolutional networks. However, studies have shown the potential of pruning as a form of regularization, which reduces overfitting and improves generalization. We demonstrate that this family of strategies provides additional benefits beyond computational performance and generalization. Our analyses reveal that pruning structures (filters and/or layers) from convolutional networks increase not only generalization but also robustness to adversarial images (natural images with content modified). Such achievements are possible since pruning reduces network capacity and provides regularization, which have been proven effective tools against adversarial images. In contrast to promising defense mechanisms that require training with adversarial images and careful regularization, we show that pruning obtains competitive results considering only natural images (e.g., the standard and low-cost training). We confirm these findings on several adversarial attacks and architectures; thus suggesting the potential of pruning as a novel defense mechanism against adversarial images.",
    "code_link": ""
  },
  "iccv2021_arow_opticaladversarialattack": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Optical Adversarial Attack",
    "authors": [
      "Abhiram Gnanasambandam",
      "Alex M. Sherman",
      "Stanley H. Chan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Gnanasambandam_Optical_Adversarial_Attack_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Gnanasambandam_Optical_Adversarial_Attack_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We introduce OPtical ADversarial attack (OPAD). OPAD is an adversarial attack in the physical space aiming to fool image classifiers without physically touching the objects (e.g., moving or painting the objects). The principle of OPAD is to use structured illumination to alter the appearance of the target objects. The system consists of a low-cost projector, a camera and a computer. The challenge of the problem is the non-linearity of the radiometric response of the projector and the spatially varying spectral response of the scene. In this setting, attacks generated in a conventional approach do not work unless they are calibrated to compensate for such a projector-camera model. The proposed solution incorporates the projector-camera model into the adversarial attack optimization where a new attack formulation is derived. Experimental results prove the validity of the solution. It is demonstrated that OPAD can optically attack a real 3D object in the presence of background lighting, for white-box, black-box, targeted, and untargeted attacks. Theoretical analysis is presented to quantify the fundamental performance limit of the system.",
    "code_link": ""
  },
  "iccv2021_arow_enhancingadversarialrobustnessviatest-timetransformationensembling": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Enhancing Adversarial Robustness via Test-Time Transformation Ensembling",
    "authors": [
      "Juan C. P\u00e9rez",
      "Motasem Alfarra",
      "Guillaume Jeanneret",
      "Laura Rueda",
      "Ali Thabet",
      "Bernard Ghanem",
      "Pablo Arbel\u00e1ez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Perez_Enhancing_Adversarial_Robustness_via_Test-Time_Transformation_Ensembling_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Perez_Enhancing_Adversarial_Robustness_via_Test-Time_Transformation_Ensembling_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep learning models are prone to being fooled by imperceptible perturbations known as adversarial attacks. In this work, we study how equipping models with Test-time Transformation Ensembling (TTE) can work as a reliable defense against such attacks. While transforming the input data, both at train and test times, is known to enhance model performance, its effects on adversarial robustness have not been studied. Here, we present a comprehensive empirical study of the impact of TTE, in the form of widely-used image transforms, on adversarial robustness. We show that TTE consistently improves model robustness against a variety of powerful attacks without any need for re-training, and that this improvement comes at virtually no trade-off with accuracy on clean samples. Finally, we show that the benefits of TTE transfer even to the certified robustness domain, in which TTE provides sizable and consistent improvements.",
    "code_link": ""
  },
  "iccv2021_arow_advfoolgencreatingpersistenttroublesfordeepclassifiers": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "AdvFoolGen: Creating Persistent Troubles for Deep Classifiers",
    "authors": [
      "Yuzhen Ding",
      "Nupur Thakur",
      "Baoxin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Ding_AdvFoolGen_Creating_Persistent_Troubles_for_Deep_Classifiers_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Ding_AdvFoolGen_Creating_Persistent_Troubles_for_Deep_Classifiers_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Researches have shown that deep neural networks are vulnerable to malicious attacks, where adversarial images are created to trick a network into misclassification even if the images may give rise to totally different labels by human eyes. To make deep networks more robust to such attacks, many defense mechanisms have been proposed in the literature, some of which are quite effective for guarding against typical attacks. In this paper, we present a new generative attack model termed AdvFoolGen, which can generate attacking images from the same feature space as that of the natural images, so as to keep baffling the network even though state-of-the-art defense mechanisms have been applied. We systematically evaluate our model by comparing with well-established attack algorithms. Through experiments, we demonstrate the effectiveness and robustness of our attack in the face of state-of-the-art defense techniques and unveil the potential reasons for its effectiveness through principled analysis. As such, AdvFoolGen contributes to understanding the vulnerability of deep networks from a new perspective and may, in turn, help in developing and evaluating new defense mechanisms.",
    "code_link": ""
  },
  "iccv2021_arow_counteringadversarialexamplescombininginputtransformationandnoisytraining": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Countering Adversarial Examples: Combining Input Transformation and Noisy Training",
    "authors": [
      "Cheng Zhang",
      "Pan Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Zhang_Countering_Adversarial_Examples_Combining_Input_Transformation_and_Noisy_Training_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Zhang_Countering_Adversarial_Examples_Combining_Input_Transformation_and_Noisy_Training_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recent studies have shown that neural network (NN) based image classifiers are highly vulnerable to adversarial examples, which poses a threat to security-sensitive image recognition task. Prior work has shown that JPEG compression can combat the drop in classification accuracy on adversarial examples to some extent. But, as the compression ratio increases, traditional JPEG compression is insufficient to defend those attacks but can cause an abrupt accuracy decline to the benign images. In this paper, with the aim of fully filtering the adversarial perturbations, we firstly make modifications to traditional JPEG compression algorithm which becomes more favorable for NN. Specifically, based on an analysis of the frequency coefficient, we design a NN-favored quantization table for compression. Considering compression as a data augmentation strategy, we then combine our model-agnostic preprocess with noisy training. We fine-tune the pre-trained model by training with images encoded at different compression levels, thus generating multiple classifiers. Finally, since lower (higher) compression ratio can remove both perturbations and original features slightly (aggressively), we use these trained multiple models for model ensemble. The majority vote of the ensemble of models is adopted as final predictions. Experiments results show our method can improve defense efficiency while maintaining original accuracy.",
    "code_link": ""
  },
  "iccv2021_arow_ahierarchicalassessmentofadversarialseverity": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "A Hierarchical Assessment of Adversarial Severity",
    "authors": [
      "Guillaume Jeanneret",
      "Juan C. P\u00e9rez",
      "Pablo Arbel\u00e1ez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Jeanneret_A_Hierarchical_Assessment_of_Adversarial_Severity_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Jeanneret_A_Hierarchical_Assessment_of_Adversarial_Severity_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Adversarial Robustness is a growing field that evidences the brittleness of neural networks. Although the literature on adversarial robustness is vast, a dimension is missing in these studies: assessing how severe the mistakes are. We call this notion \"Adversarial Severity\" since it quantifies the downstream impact of adversarial corruptions by computing the semantic error between the misclassification and the proper label. We propose to study the effects of adversarial noise by measuring the Robustness and Severity into a large-scale dataset: iNaturalist-H. Our contributions are: (i) we introduce novel Hierarchical Attacks that harness the rich structured space of labels to create adversarial examples. (ii) These attacks allow us to benchmark the Adversarial Robustness and Severity of classification models. (iii) We enhance the traditional adversarial training with a simple yet effective Hierarchical Curriculum Training to learn these nodes gradually within the hierarchical tree. We perform extensive experiments showing that hierarchical defenses allow deep models to boost the adversarial Robustness by 1.85% and reduce the severity of all attacks by 0.17, on average.",
    "code_link": "https://github.com/BCV-Uniandes/AdvSeverity"
  },
  "iccv2021_arow_patchattackinvariancehowsensitivearepatchattacksto3dpose?": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Patch Attack Invariance: How Sensitive Are Patch Attacks to 3D Pose?",
    "authors": [
      "Max Lennon",
      "Nathan Drenkow",
      "Phil Burlina"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Lennon_Patch_Attack_Invariance_How_Sensitive_Are_Patch_Attacks_to_3D_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Lennon_Patch_Attack_Invariance_How_Sensitive_Are_Patch_Attacks_to_3D_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Perturbation-based attacks, while not physically realizable, have been the main emphasis of adversarial machine learning (ML) research. Patch-based attacks by contrast are physically realizable, yet most work has focused on 2D domain with recent forays into 3D. Characterizing the robustness properties of patch attacks and their invariance to 3D pose is important, yet not fully elucidated, and is the focus of this paper. To this end, several contributions are made here: A) we develop a new metric called mean Attack Success over Transformations (mAST) to evaluate patch attack robustness and invariance; and B), we systematically assess robustness of patch attacks to 3D position and orientation for various conditions; in particular, we conduct a sensitivity analysis which provides important qualitative insights into attack effectiveness as a function of the 3D pose of a patch relative to the camera (rotation, translation) and sets forth some properties for patch attack 3D invariance; and C), we draw novel qualitative conclusions including: 1) we demonstrate that for some 3D transformations, namely rotation and loom, increasing the training distribution support yields an increase in patch success over the full range at test time. 2) We provide new insights into the existence of a fundamental cutoff limit in patch attack effectiveness that depends on the extent of out-of-plane rotation angles. These findings should collectively guide future design of 3D patch attacks and defenses.",
    "code_link": ""
  },
  "iccv2021_arow_detectingandsegmentingadversarialgraphicspatternsfromimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Detecting and Segmenting Adversarial Graphics Patterns From Images",
    "authors": [
      "Xiangyu Qu",
      "Stanley H. Chan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Qu_Detecting_and_Segmenting_Adversarial_Graphics_Patterns_From_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Qu_Detecting_and_Segmenting_Adversarial_Graphics_Patterns_From_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Adversarial attacks pose a substantial threat to computer vision system security, but the social media industry constantly faces another form of \"adversarial attack\" in which the hackers attempt to upload inappropriate images and fool the automated screening systems by adding artificial graphics patterns. In this paper, we formulate the defense against such attacks as an artificial graphics pattern segmentation problem. We evaluate the efficacy of several segmentation algorithms and, based on observation of their performance, propose a new method tailored to this specific problem. Extensive experiments show that the proposed method outperforms the baselines and has a promising generalization capability, which is the most crucial aspect in segmenting artificial graphics patterns.",
    "code_link": ""
  },
  "iccv2021_arow_cantargetedadversarialexamplestransferwhenthesourceandtargetmodelshavenolabelspaceoverlap?": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Can Targeted Adversarial Examples Transfer When the Source and Target Models Have No Label Space Overlap?",
    "authors": [
      "Nathan Inkawhich",
      "Kevin J Liang",
      "Jingyang Zhang",
      "Huanrui Yang",
      "Hai Li",
      "Yiran Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Inkawhich_Can_Targeted_Adversarial_Examples_Transfer_When_the_Source_and_Target_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Inkawhich_Can_Targeted_Adversarial_Examples_Transfer_When_the_Source_and_Target_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We design blackbox transfer-based targeted adversarial attacks for an environment where the attacker's source model and the target blackbox model may have disjoint label spaces and training datasets. This scenario significantly differs from the \"standard\" blackbox setting, and warrants a unique approach to the attacking process. Our methodology begins with the construction of a class correspondence matrix between the whitebox and blackbox label sets. During the online phase of the attack, we then leverage representations of highly related proxy classes from the whitebox distribution to fool the blackbox model into predicting the desired target class. Our attacks are evaluated in three complex and challenging test environments where the source and target models have varying degrees of conceptual overlap amongst their unique categories. Ultimately, we find that it is indeed possible to construct targeted transfer-based adversarial attacks between models that have non-overlapping label spaces! We also analyze the sensitivity of attack success to properties of the clean data. Finally, we show that our transfer attacks serve as powerful adversarial priors when integrated with query-based methods, markedly boosting query efficiency and adversarial success.",
    "code_link": ""
  },
  "iccv2021_arow_impactofcolouronrobustnessofdeepneuralnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AROW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Adversarial Robustness in the Real World",
    "title": "Impact of Colour on Robustness of Deep Neural Networks",
    "authors": [
      "Kanjar De",
      "Marius Pedersen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/html/De_Impact_of_Colour_on_Robustness_of_Deep_Neural_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/De_Impact_of_Colour_on_Robustness_of_Deep_Neural_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Convolutional neural networks have become the most widely used tool for computer vision applications like image classification, segmentation, object localization etc. Recent studies have shown that the quality of images has a significant impact on the performance of these deep neural networks and the accuracy of the computer vision tasks gets significantly influenced by the image quality due to the shift in the distribution of the images on which the networks are trained on. Although, the effects of perturbations like image noise, image blur, image contrast, compression artifacts, etc. on the performance of deep neural networks on image classification have been studied, the effects of colour and quality of colour in digital images have been a mostly unexplored direction. One of the biggest challenges is that there is no particular dataset dedicated to colour distortions and colour aspects of images in image classification. The main aim of this paper is to study the impact of colour distortions on the performance of image classification of deep neural networks. Experiments performed using multiple state-of--of-the--the-art deep convolutional neural architectures on a proposed colour distorted dataset are presented in this paper and the impact of colour on image classification task is demonstrated.",
    "code_link": ""
  },
  "iccv2021_rslcv_relaxationsfornon-separablecardinality/rankpenalties": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Relaxations for Non-Separable Cardinality/Rank Penalties",
    "authors": [
      "Carl Olsson",
      "Daniele Gerosa",
      "Marcus Carlsson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Olsson_Relaxations_for_Non-Separable_CardinalityRank_Penalties_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Olsson_Relaxations_for_Non-Separable_CardinalityRank_Penalties_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Rank and cardinality penalties are hard to handle in optimization frameworks due to non-convexity and discontinuity. Strong approximations have been a subject of intense study and numerous formulations have been proposed. Most of these can be described as separable, meaning that they apply a penalty to each element (or singular value) based on size, without considering the joint distribution. In this paper we present a class of non-separable penalties and give a recipe for computing strong relaxations suitable for optimization. In our analysis of this formulation we first give conditions that ensure that the globally optimal solution of the relaxation is the same as that of the original (unrelaxed) objective. We then show how a stationary point can be guaranteed to be unique under the restricted isometry property (RIP) assumption.",
    "code_link": ""
  },
  "iccv2021_rslcv_transblastself-supervisedlearningusingaugmentedsubspacewithtransformerforbackground/foregroundseparation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "TransBlast: Self-Supervised Learning Using Augmented Subspace With Transformer for Background/Foreground Separation",
    "authors": [
      "Islam Osman",
      "Mohamed Abdelpakey",
      "Mohamed S. Shehata"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Osman_TransBlast_Self-Supervised_Learning_Using_Augmented_Subspace_With_Transformer_for_BackgroundForeground_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Osman_TransBlast_Self-Supervised_Learning_Using_Augmented_Subspace_With_Transformer_for_BackgroundForeground_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Background/Foreground separation is a fundamental and challenging task of many computer vision applications. The F-measure performance of state-of-the-art models is limited due to the lack of fine details in the predicted output (i.e., the foreground object) and the limited labeled data. In this paper, we propose a background/foreground separation model based on a transformer that has a higher learning capacity than the convolutional neural networks. The model is trained using self-supervised learning to leverage the limited data and learn a strong object representation that is invariant to changes. The proposed method, dubbed TransBlast, reformulates the background/foreground separation problem in self-supervised learning using the augmented subspace loss function. The augmented subspace loss function consists of two components: 1) the cross-entropy loss function and 2) the subspace that depends on Singular Value Decomposition (SVD). The proposed model is evaluated using three benchmarks, namely CDNet, DAVIS, and SegTrackV2. The performance of TransBlast outperforms state-of-the-art background/foreground separation models in terms of F-measure.",
    "code_link": ""
  },
  "iccv2021_rslcv_graphcnnformovingobjectdetectionincomplexenvironmentsfromunseenvideos": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Graph CNN for Moving Object Detection in Complex Environments From Unseen Videos",
    "authors": [
      "Jhony H. Giraldo",
      "Sajid Javed",
      "Naoufel Werghi",
      "Thierry Bouwmans"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Giraldo_Graph_CNN_for_Moving_Object_Detection_in_Complex_Environments_From_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Giraldo_Graph_CNN_for_Moving_Object_Detection_in_Complex_Environments_From_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Moving Object Detection (MOD) is a fundamental step for many computer vision applications. MOD becomes very challenging when a video sequence captured from a static or moving camera suffers from the challenges: camouflage, shadow, dynamic backgrounds, and lighting variations, to name a few. Deep learning methods have been successfully applied to address MOD with competitive performance. However, in order to handle the overfitting problem, deep learning methods require a large amount of labeled data which is a laborious task as exhaustive annotations are always not available. Moreover, some MOD deep learning methods show performance degradation in the presence of unseen video sequences because the testing and training splits of the same sequences are involved during the network learning process. In this work, we pose the problem of MOD as a node classification problem using Graph Convolutional Neural Networks (GCNNs). Our algorithm, dubbed as GraphMOD-Net, encompasses instance segmentation, background initialization, feature extraction, and graph construction. GraphMOD-Net is tested on unseen videos and outperforms state-of-the-art methods in unsupervised, semi-supervised, and supervised learning in several challenges of the Change Detection 2014 (CDNet2014) and UCSD background subtraction datasets.",
    "code_link": ""
  },
  "iccv2021_rslcv_background/foregroundseparationguidedattentionbasedadversarialmodeling(gaam)versusrobustsubspacelearningmethods": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Background/Foreground Separation: Guided Attention Based Adversarial Modeling (GAAM) Versus Robust Subspace Learning Methods",
    "authors": [
      "Maryam Sultana",
      "Arif Mahmood",
      "Thierry Bouwmans",
      "Muhammad Haris Khan",
      "Soon Ki Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Sultana_BackgroundForeground_Separation_Guided_Attention_Based_Adversarial_Modeling_GAAM_Versus_Robust_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Sultana_BackgroundForeground_Separation_Guided_Attention_Based_Adversarial_Modeling_GAAM_Versus_Robust_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Background-Foreground separation and appearance generation is a fundamental step in many computer vision applications. Existing methods like Robust Subspace Learning (RSL) suffer performance degradation in the presence of challenges like bad weather, illumination variations, occlusion, dynamic backgrounds and intermittent object motion. In the current work we propose a more accurate deep neural network based model for background-foreground separation and complete appearance generation of the foreground objects. Our proposed model, Guided Attention based Adversarial Model (GAAM), can efficiently extract pixel-level boundaries of the foreground objects for improved appearance generation. Unlike RSL methods our model extracts the binary information of foreground objects labeled as attention map which guides our generator network to segment the foreground objects from the complex background information. Wide range of experiments performed on the benchmark CDnet2014 dataset demonstrate the excellent performance of our proposed model.",
    "code_link": ""
  },
  "iccv2021_rslcv_double-weightedlow-rankmatrixrecoverybasedonrankestimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Double-Weighted Low-Rank Matrix Recovery Based on Rank Estimation",
    "authors": [
      "Zhengqin Xu",
      "Huasong Xing",
      "Shun Fang",
      "Shiqian Wu",
      "Shoulie Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Xu_Double-Weighted_Low-Rank_Matrix_Recovery_Based_on_Rank_Estimation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Xu_Double-Weighted_Low-Rank_Matrix_Recovery_Based_on_Rank_Estimation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Robust principal component analysis (RPCA) has widely application in computer vision and data mining. However, the various RPCA algorithms in practical applications need to know the rank of low-rank matrix in advance, or manually adjust parameters. To overcome these limitations,an adaptive double-weighted RPCA algorithm is proposed to recover low-rank matrix accurately based on the estimated rank of the low-rank matrix and the reweighting strategy in this paper. More specifically, the Gerschgorin's disk theorem is introduced to estimate the rank of the low-rank matrix first. Then a double-weighted optimization model through two weighting factors for the low-rankness and sparsity is presented. Finally an adaptive double weighted algorithm based on rank estimation is proposed, which can reweight the singular values of low-rank matrix and the sparsity of sparse matrix iteratively. Experimental results show that the proposed double-weighted RPCA algorithm outperforms the state-of-the-art RPCA methods.",
    "code_link": ""
  },
  "iccv2021_rslcv_fastrobusttensorprincipalcomponentanalysisviafibercurdecomposition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Fast Robust Tensor Principal Component Analysis via Fiber CUR Decomposition",
    "authors": [
      "HanQin Cai",
      "Zehan Chao",
      "Longxiu Huang",
      "Deanna Needell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Cai_Fast_Robust_Tensor_Principal_Component_Analysis_via_Fiber_CUR_Decomposition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Cai_Fast_Robust_Tensor_Principal_Component_Analysis_via_Fiber_CUR_Decomposition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We study the problem of tensor robust principal component analysis (TRPCA), that aims to separate an underlying low-multilinear-rank tensor and a sparse outlier tensor from their sum. In this work, we propose a fast non-convex algorithm, coined Robust Tensor CUR (RTCUR), for large-scale TRPCA problems. RTCUR considers a framework of alternating projections and utilizes the recently developed tensor Fiber CUR decomposition to dramatically lower its computational complexity. The speed advantage of RTCUR is empirically verified against the state-of-the-art on both synthetic and real-world datasets.",
    "code_link": ""
  },
  "iccv2021_rslcv_convolutionalauto-encoderwithtensor-trainfactorization": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Convolutional Auto-Encoder With Tensor-Train Factorization",
    "authors": [
      "Manish Sharma",
      "Panos P. Markopoulos",
      "Eli Saber",
      "M. Salman Asif",
      "Ashley Prater-Bennette"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Sharma_Convolutional_Auto-Encoder_With_Tensor-Train_Factorization_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Sharma_Convolutional_Auto-Encoder_With_Tensor-Train_Factorization_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Convolutional auto-encoders (CAEs) are extensively used for general purpose feature extraction, image reconstruction, image denoising, and other machine learning tasks. Despite their many successes, similar to other convolutional networks, CAEs often suffer from over-parameterization when trained with small or moderate-sized datasets. In such cases, CAEs suffer from excess computational and memory overhead as well as decreased performance due to parameter over-fitting. In this work we introduce CAE-TT: a CAE with tunable tensor-train (TT) structure to its convolution and transpose-convolution filters. By tuning the TT-ranks, CAE-TT can adjust the number of its learning parameters without changing the network architecture. In our numerical studies, we demonstrate the performance of the proposed method and compare it with alternatives, in both batch and online learning settings.",
    "code_link": ""
  },
  "iccv2021_rslcv_synthetictemporalanomalyguidedend-to-endvideoanomalydetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RSLCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Synthetic Temporal Anomaly Guided End-to-End Video Anomaly Detection",
    "authors": [
      "Marcella Astrid",
      "Muhammad Zaigham Zaheer",
      "Seung-Ik Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Astrid_Synthetic_Temporal_Anomaly_Guided_End-to-End_Video_Anomaly_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RSLCV/papers/Astrid_Synthetic_Temporal_Anomaly_Guided_End-to-End_Video_Anomaly_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Due to the limited availability of anomaly examples, video anomaly detection is often seen as one-class classification (OCC) problem. A popular way to tackle this problem is by utilizing an autoencoder (AE) trained only on normal data. At test time, the AE is then expected to reconstruct the normal input well while reconstructing the anomalies poorly. However, several studies show that, even with normal data only training, AEs can often start reconstructing anomalies as well which depletes their anomaly detection performance. To mitigate this, we propose a temporal pseudo anomaly synthesizer that generates fake-anomalies using only normal data. An AE is then trained to maximize the reconstruction loss on pseudo anomalies while minimizing this loss on normal data. This way, the AE is encouraged to produce distinguishable reconstructions for normal and anomalous frames. Extensive experiments and analysis on three challenging video anomaly datasets demonstrate the effectiveness of our approach to improve the basic AEs in achieving superiority against several existing state-of-the-art models.",
    "code_link": ""
  },
  "iccv2021_dsc_self-attentionagreementamongcapsules": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "Self-Attention Agreement Among Capsules",
    "authors": [
      "Rita Pucci",
      "Christian Micheloni",
      "Niki Martinel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Pucci_Self-Attention_Agreement_Among_Capsules_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Pucci_Self-Attention_Agreement_Among_Capsules_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "At the state of the art, Capsule Networks (CapsNets) have shown to be a promising alternative to Convolutional Neural Networks (CNNs) in many computer vision tasks, due to their ability to encode object viewpoint variations. Network capsules provide maps of votes that focus on entities presence in the image and their pose. Each map is the point of view of a given capsule. To compute such votes, CapsNets rely on the routing-by-agreement mechanism. This computationally costly iterative algorithm selects the most appropriate parent capsule to have nodes in a parse tree for all the active capsules but this behaviour is not ensured by the routing, hence it possibly causes vanishing weights during training. We hypothesise that an attention-like mechanism will help capsules to select the predominant regions among the maps to focus on, hence introducing a more reliable way of learning the agreement between the capsules in a single pass. We propose the Attention Agreement Capsule Networks (AA-Caps) architecture that builds upon CapsNet by introducing a self-attention layer to suppress irrelevant capsule votes thus keeping only the ones that are useful for capsules agreements on a specific entity. The generated capsule attention map is then assigned to classification layer responsible of emitting the predicted image class. The proposed AA-Caps model has been evaluated on five benchmark datasets to validate its ability in dealing with the diverse and complex data that CapsNet often fails with. The achieved results demonstrate that AA-Caps outperforms existing methods without the need of more complex architectures or model ensembles.",
    "code_link": ""
  },
  "iccv2021_dsc_panoptopaframeworkforgeneratingviewpoint-invarianthumanposeestimationdatasets": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "PanopTOP: A Framework for Generating Viewpoint-Invariant Human Pose Estimation Datasets",
    "authors": [
      "Nicola Garau",
      "Giulia Martinelli",
      "Piotr Br\u00f3dka",
      "Niccol\u00f2 Bisagno",
      "Nicola Conci"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Garau_PanopTOP_A_Framework_for_Generating_Viewpoint-Invariant_Human_Pose_Estimation_Datasets_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Garau_PanopTOP_A_Framework_for_Generating_Viewpoint-Invariant_Human_Pose_Estimation_Datasets_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Human pose estimation (HPE) from RGB and depth images has recently experienced a push for viewpoint-invariant and scale-invariant pose retrieval methods. In fact, current methods fail to generalise to unconventional viewpoints due to the lack of viewpoint-invariant data at training time. Existing datasets do not provide multiple-viewpoint observations, and mostly focus on frontal views. In this work, we introduce PanopTOP, a fully automatic framework for the generation of semi-synthetic RGB and depth samples with 2D and 3D ground truth of pedestrian poses from multiple arbitrary viewpoints. Starting from the Panoptic Dataset, we use the PanopTOP framework to generate the PanopTOP31K dataset, consisting of 31K images from 23 different subjects recorded from diverse and challenging viewpoints, also including the top-view. Finally, we provide baseline results and cross-validation tests for our dataset, demonstrating how it is possible to generalise from the semi-synthetic to the real world domain. The dataset and the code will be made publicly available upon acceptance.",
    "code_link": ""
  },
  "iccv2021_dsc_infrareddatasetgenerationforpeopledetectionthroughsuperimpositionofdifferentcamerasensors": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "Infrared Dataset Generation for People Detection Through Superimposition of Different Camera Sensors",
    "authors": [
      "Alessandro Avi",
      "Matteo Zuccatti",
      "Matteo Nardello",
      "Nicola Conci",
      "Davide Brunelli"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Avi_Infrared_Dataset_Generation_for_People_Detection_Through_Superimposition_of_Different_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Avi_Infrared_Dataset_Generation_for_People_Detection_Through_Superimposition_of_Different_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Infra-red (IR) cameras have found widespread use in many different fields. The most common ones are generally related to industrial applications, particularly maintenance and inspections activities. In the domain of surveillance, instead, they are mostly used for threat detection and security purposes. Pushed by cost reduction and the availability of compact sensors, intelligent IR cameras are gaining popularity in the field of Internet-of-Things, in light of the valuable information made available by the acquired data. Unfortunately, the achievable overall quality is not always satisfactory. For example, low-resolution devices, noise, or harsh environmental conditions, like high temperatures on sunny days, can degrade the quality of the thermal images. This paper presents the development of a portable, low-cost, and low-power thermal scanner prototype consisting of a thermal sensor assisted by a grayscale camera. The prototype is completely made using COTS components and provides 80x60 IR and 160x120 grayscale images, mostly used to collect and validate the IR-based data. Our application focuses on people detection, for which we present a suitable learning framework together with the corresponding IR dataset, collected and annotated via the paired grayscale images.",
    "code_link": ""
  },
  "iccv2021_dsc_resolutionbasedfeaturedistillationforcrossresolutionpersonre-identification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "Resolution Based Feature Distillation for Cross Resolution Person Re-Identification",
    "authors": [
      "Asad Munir",
      "Chengjin Lyu",
      "Bart Goossens",
      "Wilfried Philips",
      "Christian Micheloni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Munir_Resolution_Based_Feature_Distillation_for_Cross_Resolution_Person_Re-Identification_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Munir_Resolution_Based_Feature_Distillation_for_Cross_Resolution_Person_Re-Identification_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Person re-identification (re-id) aims to retrieve images of same identities across different camera views. Resolution mismatch occurs due to varying distances between person of interest and cameras, this significantly degrades the performance of re-id in real world scenarios. Most of the existing approaches resolve the re-id task as low resolution problem in which a low resolution query image is searched in a high resolution images gallery. Several approaches apply image super resolution techniques to produce high resolution images but ignore the multiple resolutions of gallery images which is a better realistic scenario. In this paper, we introduce channel correlations to improve the learning of features from the degraded data. In addition, to overcome the problem of multiple resolutions we propose a Resolution based Feature Distillation (RFD) approach. Such an approach learns resolution invariant features by filtering the resolution related features from the final feature vectors that are used to compute the distance matrix. We tested the proposed approach on two synthetically created datasets and on one original multi resolution dataset with real degradation. Our approach improves the performance when multiple resolutions occur in the gallery and have comparable results in case of single resolution (low resolution re-id).",
    "code_link": ""
  },
  "iccv2021_dsc_deepquaternionposeproposalsfor6dobjectposetracking": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "Deep Quaternion Pose Proposals for 6D Object Pose Tracking",
    "authors": [
      "Mateusz Majcher",
      "Bogdan Kwolek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Majcher_Deep_Quaternion_Pose_Proposals_for_6D_Object_Pose_Tracking_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Majcher_Deep_Quaternion_Pose_Proposals_for_6D_Object_Pose_Tracking_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this work we study quaternion pose distributions for tracking in RGB image sequences the 6D pose of an object selected from a set of objects, for which common models were trained in advance. We propose an unit quaternion representation of the rotational state space for a particle filter, which is then integrated with the particle swarm optimization to shift samples toward local maximas. Owing to k-means++ we better maintain multimodal probability distributions. We train convolutional neural networks to estimate the 2D positions of fiducial points and then to determine PnP-based object pose hypothesis. A CNN is utilized to estimate the positions of fiducial points in order to calculate PnP-based object pose hypothesis. A common Siamese neural network for all objects, which is trained on keypoints from current and previous frame is employed to guide the particles towards predicted pose of the object. Such a keypoint based pose hypothesis is injected into the probability distribution that is recursively updated in a Bayesian framework. The 6D object pose tracker is evaluated on Nvidia Jetson AGX Xavier both on synthetic and real sequences of images acquired from a calibrated RGB camera.",
    "code_link": ""
  },
  "iccv2021_dsc_anembeddeddeeplearning-basedpackagefortrafficlawenforcement": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "An Embedded Deep Learning-Based Package for Traffic Law Enforcement",
    "authors": [
      "Abbas Omidi",
      "Amirhossein Heydarian",
      "Aida Mohammadshahi",
      "Behnam Asghari Beirami",
      "Farzan Haddadi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Omidi_An_Embedded_Deep_Learning-Based_Package_for_Traffic_Law_Enforcement_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Omidi_An_Embedded_Deep_Learning-Based_Package_for_Traffic_Law_Enforcement_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Crossing Heavy Good Vehicles (HGVs) from the overtaking lane in highways is not only a traffic violation but may also cause severe casualties in case of an accident happening in such velocities. Currently, the only way to prevent this violation is to identify the violating vehicles by the highway police, so in this paper, a violation detection system using an embedded camera is introduced using algorithms based on deep learning and image processing techniques. The embedded system benefits of a multi-stage deep system based on the YOLO network, which consists of four stages of cascaded detection, including overtaking lane detection, HGV detection, license plate detection, and character recognition. In this research, the developed deep learning models, after some initial training, are fine-tuned on a local Persian dataset collected with distributed cameras. The accuracy obtained on the test dataset of each of the four separate stages was above 85% and the results show the efficiency of the proposed smart system with 70% accuracy in the union of all stages. All data including local datasets, implementations, codes, and results are available on the project's GitHub (https://github.com/NEFTeam/Traffic-Law-Enforcement).",
    "code_link": ""
  },
  "iccv2021_dsc_domain-basedsemi-supervisedlearningexploitinglabelinvarianceinunlabeleddatafromdistributedcameras": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "Domain-Based Semi-Supervised Learning: Exploiting Label Invariance in Unlabeled Data From Distributed Cameras",
    "authors": [
      "Leonardo Taccari"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Taccari_Domain-Based_Semi-Supervised_Learning_Exploiting_Label_Invariance_in_Unlabeled_Data_From_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Taccari_Domain-Based_Semi-Supervised_Learning_Exploiting_Label_Invariance_in_Unlabeled_Data_From_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In several practical supervised learning problems where we have a large amount of data from distributed cameras or sensors, we can use domain knowledge to identify subsets of unlabeled examples with the same (unknown) label. Under this assumption, we propose a straightforward way to exploit label invariance in unlabeled data within a domain-aware semi-supervised learning framework (DSSL). Our approach exploits such invariance to generate higher quality pseudolabels to be used in a consistency loss term. We report experiments and ablation studies on three practical cases on data from real-world fleets of connected vehicles that naturally exhibit the required assumption: an image classification problem, a semantic segmentation task, and a time series classification one. We show that our approach is extremely effective, especially when few labeled samples are available, and can be easily adapted to tasks of different nature.",
    "code_link": ""
  },
  "iccv2021_dsc_wheredidiseeit?objectinstancere-identificationwithattention": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "Where Did I See It? Object Instance Re-Identification With Attention",
    "authors": [
      "Vaibhav Bansal",
      "Gian Luca Foresti",
      "Niki Martinel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Bansal_Where_Did_I_See_It_Object_Instance_Re-Identification_With_Attention_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Bansal_Where_Did_I_See_It_Object_Instance_Re-Identification_With_Attention_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Existing methods dealing with object instance re-identification (OIRe-ID) look for the best visual features match of a target object within a set of frames. Due to the nature of the problem, relying only on the visual appearance of object instances is likely to provide many false matches when there are multiple objects with similar appearance or multiple instances of same object class present in the scene. We focus on a rigid scene setup and to limit the negative effects of the aforementioned cases, we propose to exploit the background information. We believe that this would be particularly helpful in a rigid environment with a lot of reoccurring identical models of objects since it would provide rich context information. We introduce an attention-based mechanism to the existing Mask R-CNN architecture such that we learn to encode the important and distinct information in the background jointly with the foreground features relevant to rigid real-world scenarios. To evaluate the proposed approach, we run compelling experiments on the ScanNet dataset. Results demonstrate that we outperform significantly compared to different baselines and SOTA methods.",
    "code_link": ""
  },
  "iccv2021_dsc_pedestriantrackingthroughcoordinatedminingofmultiplemovingcameras": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DSC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Distributed Smart Cameras",
    "title": "Pedestrian Tracking Through Coordinated Mining of Multiple Moving Cameras",
    "authors": [
      "Yanting Zhang",
      "Qingxiang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Zhang_Pedestrian_Tracking_Through_Coordinated_Mining_of_Multiple_Moving_Cameras_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Zhang_Pedestrian_Tracking_Through_Coordinated_Mining_of_Multiple_Moving_Cameras_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Multiple object tracking has attracted great interest in the computer vision community. Most researchers focus on the applications under a single static or moving camera. In intelligent cities, tracking across multiple static cameras is also investigated due to the need for surveillance purposes. With the growing development of autonomous driving, it is critical to correlate all the vehicles' vision systems on the road to achieve a global perception. However, tracking across multiple moving cameras has not been well studied yet. We observe a lack of such a publicly available dataset for coordinated mining of multiple moving cameras. In this paper, we aim to bridge the gap and propose a new dataset of multiple moving cameras, called \"DHU-MTMMC\", in which the videos are collected from several cameras mounted on the moving cars. The dataset contains fourteen sequences in different scenarios with annotated pedestrians. We propose a baseline MTMMC workflow to deal with tracking pedestrians across cameras. When the joint detection and embedding are performed, the association algorithm can run online under single-camera settings. We treat multi-camera tracking as a linear assignment problem that can be solved efficiently. The overall IDF1 of the proposed MTMMC tracking on the dataset is 57.8%.",
    "code_link": "https://github.com/open-mmlab/mmtracking"
  },
  "iccv2021_neurarch_convolutionalfilterapproximationusingfractionalcalculus": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "Convolutional Filter Approximation Using Fractional Calculus",
    "authors": [
      "Julio Zamora",
      "Jesus A. Cruz Vargas",
      "Anthony Rhodes",
      "Lama Nachman",
      "Narayan Sundararajan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Zamora_Convolutional_Filter_Approximation_Using_Fractional_Calculus_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Zamora_Convolutional_Filter_Approximation_Using_Fractional_Calculus_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We introduce a generalized fractional convolutional filter (FF) with the flexibility to behave as any novel, customized, or well-known filter (e.g. Gaussian, Sobel, and Laplacian). Our method can be trained using only five parameters - regardless of the kernel size. Furthermore, these kernels can be used in place of traditional kernels in any CNN topology. We demonstrate a nominal 5X parameter compression per kernel as compared to a traditional (5x5) convolutional kernel, and in the generalized case, a compression from NxN to 6 trainable parameters per kernel. We furthermore achieve 3X compression for 3D convolutional filters compared with conventional (7x7x7)3D filters. Using fractional filters, we set a new MNIST record for the fewest number of parameters required to achieve above99% classification accuracy with only3,750 trainable parameters. In addition to providing a generalizable method for CNN model compression, FFs present a compelling use case for the compression of CNNs that require large kernel sizes (e.g. medical imaging, semantic segmentation)",
    "code_link": ""
  },
  "iccv2021_neurarch_russiandollnetworklearningnestednetworksforsample-adaptivedynamicinference": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "Russian Doll Network: Learning Nested Networks for Sample-Adaptive Dynamic Inference",
    "authors": [
      "Borui Jiang",
      "Yadong Mu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Jiang_Russian_Doll_Network_Learning_Nested_Networks_for_Sample-Adaptive_Dynamic_Inference_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Jiang_Russian_Doll_Network_Learning_Nested_Networks_for_Sample-Adaptive_Dynamic_Inference_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This work bridges recent advances in once-for-all (OFA) networks and sample-adaptive dynamic networks. We propose a novel neural architecture dubbed as Russian doll network (RDN). Key differentiators of RDN are two-folds: first, a RDN topologically consists of a few nested sub-networks. Any smaller sub-network is completely embedded in all larger ones in a parameter-sharing manner. The computation flow of a RDN starts from the inner-most (and smallest) sub-network and sequentially executes larger ones according to the nesting order. A larger sub-network can re-use all intermediate features calculated at their inner sub-networks. This crucially ensures that each sub-network can conduct inference independently. Secondly, the nesting order of RDNs naturally plots the sequential neural path of a sample in the network. For an easy sample, much computation can be saved without much sacrifice of accuracy if an early-termination point can be intelligently determined. To this end, we formulate satisfying a specific accuracy-complexity tradeoff as a constrained optimization problem, solved via the Lagrangian multiplier theory. Comprehensive experiments of transforming several base models into RDN on ImageNet clearly demonstrate the superior accuracy-complexity balance of RDN.",
    "code_link": ""
  },
  "iccv2021_neurarch_tiledsqueeze-and-excitechannelattentionwithlocalspatialcontext": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "Tiled Squeeze-and-Excite: Channel Attention With Local Spatial Context",
    "authors": [
      "Niv Vosco",
      "Alon Shenkler",
      "Mark Grobman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Vosco_Tiled_Squeeze-and-Excite_Channel_Attention_With_Local_Spatial_Context_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Vosco_Tiled_Squeeze-and-Excite_Channel_Attention_With_Local_Spatial_Context_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper we investigate the amount of spatial context required for channel attention. To this end we study the popular squeeze-and-excite (SE) block which is a simple and lightweight channel attention mechanism. SE blocks and its numerous variants commonly use global average pooling (GAP) to create a single descriptor for each channel. Here, we empirically analyze the amount of spatial context needed for effective channel attention and find that limited local-context on the order of seven rows or columns of the original image is sufficient to match the performance of global context. We propose tiled squeeze-and-excite (TSE), which is a framework for building SE-like blocks that employ several descriptors per channel, with each descriptor based on local context only. We further show that TSE is a drop-in replacement for the SE block and can be used in existing SE networks without re-training. This implies that local context descriptors are similar both to each other and to the global context descriptor. Finally, we show that TSE has important practical implications for deployment of SE-networks to dataflow AI accelerators due to their reduced pipeline buffering requirements. For example, using TSE reduces the amount of activation pipeline buffering in EfficientDet-D2 by 90% compared to SE (from 50M to 4.77M) without loss of accuracy. Our code and pre-trained models will be publicly available.",
    "code_link": ""
  },
  "iccv2021_neurarch_scarlet-nasbridgingthegapbetweenstabilityandscalabilityinweight-sharingneuralarchitecturesearch": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "SCARLET-NAS: Bridging the Gap Between Stability and Scalability in Weight-Sharing Neural Architecture Search",
    "authors": [
      "Xiangxiang Chu",
      "Bo Zhang",
      "Qingyuan Li",
      "Ruijun Xu",
      "Xudong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Chu_SCARLET-NAS_Bridging_the_Gap_Between_Stability_and_Scalability_in_Weight-Sharing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Chu_SCARLET-NAS_Bridging_the_Gap_Between_Stability_and_Scalability_in_Weight-Sharing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "To discover powerful yet compact models is an important goal of neural architecture search. Previous two-stage one-shot approaches are limited by search space with a fixed depth. It seems handy to include an additional skip connection in the search space to make depths variable. However, it creates a large range of perturbation during supernet training and it has difficulty giving a confident ranking for subnetworks. In this paper, we discover that skip connections bring about significant feature inconsistency compared with other operations, which potentially degrades the supernet performance. Based on this observation, we tackle the problem by imposing an equivariant learnable stabilizer to homogenize such disparities. Experiments show that our proposed stabilizer helps to improve the supernet's convergence as well as ranking performance. With an evolutionary search backend that incorporates the stabilized supernet as an evaluator, we derive a family of state-of-the-art architectures, the SCARLET series of several depths, especially SCARLET-A obtains 76.9% top-1 accuracy on ImageNet.",
    "code_link": ""
  },
  "iccv2021_neurarch_leveragingbatchnormalizationforvisiontransformers": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "Leveraging Batch Normalization for Vision Transformers",
    "authors": [
      "Zhuliang Yao",
      "Yue Cao",
      "Yutong Lin",
      "Ze Liu",
      "Zheng Zhang",
      "Han Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Transformer-based vision architectures have attracted great attention because of the strong performance over the convolutional neural networks (CNNs). Inherited from the NLP tasks, the architectures take Layer Normalization (LN) as a default normalization technique. On the other side, previous vision models, i.e., CNNs, treat Batch Normalization (BN) as a de facto standard, with the merits of faster inference than other normalization layers due to an avoidance of calculating the mean and variance statistics during inference, as well as better regularization effects during training. In this paper, we aim to introduce Batch Normalization to Transformer-based vision architectures. Our initial exploration reveals frequent crashes in model training when directly replacing all LN layers with BN, contributing to the un-normalized feed forward network (FFN) blocks. We therefore propose to add a BN layer in-between the two linear layers in the FFN block where stabilized training statistics are observed, resulting in a pure BN-based architecture. Our experiments proved that our resulting approach is as effective as the LN-based counterpart and is about 20% faster.",
    "code_link": ""
  },
  "iccv2021_neurarch_contextualconvolutionalneuralnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "Contextual Convolutional Neural Networks",
    "authors": [
      "Ionut Cosmin Duta",
      "Mariana Iuliana Georgescu",
      "Radu Tudor Ionescu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Duta_Contextual_Convolutional_Neural_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Duta_Contextual_Convolutional_Neural_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose contextual convolution (CoConv) for visual recognition. CoConv is a direct replacement of the standard convolution, which is the core component of convolutional neural networks. CoConv is implicitly equipped with the capability of incorporating contextual information while maintaining a similar number of parameters and computational cost compared to the standard convolution. CoConv is inspired by neuroscience studies indicating that (i) neurons, even from the primary visual cortex (V1 area), are involved in detection of contextual cues and that (ii) the activity of a visual neuron can be influenced by the stimuli placed entirely outside of its theoretical receptive field. On the one hand, we integrate CoConv in the widely-used residual networks and show improved recognition performance over baselines on the core tasks and benchmarks for visual recognition, namely image classification on the ImageNet data set and object detection on the MS COCO data set. On the other hand, we introduce CoConv in the generator of a state-of-the-art Generative Adversarial Network, showing improved generative results on CIFAR-10 and CelebA. Our code is available at https://github.com/iduta/coconv.",
    "code_link": "https://github.com/iduta/coconv"
  },
  "iccv2021_neurarch_single-dartstowardsstablearchitecturesearch": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "Single-DARTS: Towards Stable Architecture Search",
    "authors": [
      "Pengfei Hou",
      "Ying Jin",
      "Yukang Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Hou_Single-DARTS_Towards_Stable_Architecture_Search_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Hou_Single-DARTS_Towards_Stable_Architecture_Search_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Differentiable architecture search (DARTS) marks a milestone in Neural Architecture Search (NAS), boasting simplicity and small search costs. However, DARTS still suffers from frequent performance collapse, which happens when some operations, such as skip connections, zeroes and poolings, dominate the architecture. In this paper, we are the first to point out that the phenomenon is attributed to bi-level optimization. We propose Single-DARTS which merely uses single-level optimization, updating network weights and architecture parameters simultaneously with the same data batch. Even single-level optimization has been previously attempted, no literature provides a systematic explanation on this essential point. Experiment results show that Single-DARTS achieves state-of-the-art performance on mainstream search spaces. For instance, on NAS-Benchmark-201, the searched architectures are nearly optimal ones. We also validate that the single-level optimization framework is much more stable than the bi-level one. We hope that this simple yet effective method will give some insights on differential architecture search.",
    "code_link": "https://github.com/PencilAndBike/Single-DARTS.git"
  },
  "iccv2021_neurarch_pp-nassearchingforplug-and-playblocksonconvolutionalneuralnetwork": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "PP-NAS: Searching for Plug-and-Play Blocks on Convolutional Neural Network",
    "authors": [
      "Biluo Shen",
      "Anqi Xiao",
      "Jie Tian",
      "Zhenhua Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Shen_PP-NAS_Searching_for_Plug-and-Play_Blocks_on_Convolutional_Neural_Network_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Shen_PP-NAS_Searching_for_Plug-and-Play_Blocks_on_Convolutional_Neural_Network_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Multi-scale features are of great importance in modern convolutional neural networks and show consistent performance gains on many vision tasks. Therefore, many plug-and-play blocks are introduced to upgrade existing convolutional neural networks for stronger multi-scale representation ability. However, the design of plug-and-play blocks is getting more complex and these manually designed blocks are not optimal. In this work, we propose PP-NAS to develop plug-and-play blocks based on neural architecture search. Specifically, we design a new search space and develop the corresponding search algorithm. Extensive experiments on CIFAR10, CIFAR100, and ImageNet show that PP-NAS can find a series of novel blocks that outperform manually designed ones. Transfer learning results on representative computer vision tasks including object detection and semantic segmentation further verify the superiority of the PP-NAS over the state-of-the-art CNNs (e.g., ResNet, Res2Net). Our code will be made avaliable at https://github.com/sbl1996/PP-NAS.",
    "code_link": "https://github.com/sbl1996/PP-NAS"
  },
  "iccv2021_neurarch_conetchanneloptimizationforconvolutionalneuralnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "CONet: Channel Optimization for Convolutional Neural Networks",
    "authors": [
      "Mahdi S. Hosseini",
      "Jia Shu Zhang",
      "Zhe Liu",
      "Andre Fu",
      "Jingxuan Su",
      "Mathieu Tuli",
      "Konstantinos N. Plataniotis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Hosseini_CONet_Channel_Optimization_for_Convolutional_Neural_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Hosseini_CONet_Channel_Optimization_for_Convolutional_Neural_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Neural Architecture Search (NAS) has shifted network design from using human intuition to leveraging search algorithms guided by evaluation metrics. We study channel size optimization in convolutional neural networks (CNN) and identify the role it plays in model accuracy and complexity. Current channel size selection methods are generally limited by discrete sample spaces while suffering from manual iteration and simple heuristics. To solve this, we introduce an efficient dynamic scaling algorithm -- CONet -- that automatically optimizes channel sizes across network layers for a given CNN. Two metrics -- \"Rank\" and \"Rank Average Slope\" -- are introduced to identify the information accumulated in training. The algorithm dynamically scales channel sizes up or down over a fixed searching phase. We conduct experiments on CIFAR10/100 and ImageNet datasets and show that CONet can find efficient and accurate architectures searched in ResNet, DARTS, and DARTS+ spaces that outperform their baseline models.",
    "code_link": "https://github.com/mahdihosseini/CONet"
  },
  "iccv2021_neurarch_ddunetdensedenseu-netwithapplicationsinimagedenoising": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "DDUNet: Dense Dense U-Net With Applications in Image Denoising",
    "authors": [
      "Fan Jia",
      "Wing Hong Wong",
      "Tieyong Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Jia_DDUNet_Dense_Dense_U-Net_With_Applications_in_Image_Denoising_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Jia_DDUNet_Dense_Dense_U-Net_With_Applications_in_Image_Denoising_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The investigation of CNN for image denoising has arrived at a serious bottleneck and it is extremely difficult to design an efficient network for image denoising with better performance and fewer parameters. A nice starting point for this is the cascading U-Nets architecture which has been successfully applied in numerous image-to-image tasks such as image denoising and segmentation. However, the previous related models often focused on the local architecture in each U-Net rather than the connection between U-Nets, which strictly limits their performances. To further improve the connection between U-Nets, we propose a novel cascading U-Nets architecture with multi-scale dense processing, named Dense Dense U-Net (DDUNet). The multi-scale dense processing connects the feature maps in each level cross cascading U-Nets, which has several compelling advantages: they alleviate the vanishing gradient problem, strengthen feature propagation and encourage feature reuse. Furthermore, we develop a series of related important techniques to improve model performance with fewer parameters. Extensive experimental results on both synthetic and real noisy datasets demonstrate that the proposed model achieves outstanding results with fewer parameters. Meanwhile, experimental results show clearly that the proposed DDUNet is good at edge recovery and structure preservation in real noisy image denoising.",
    "code_link": ""
  },
  "iccv2021_neurarch_graph-basedneuralarchitecturesearchwithoperationembeddings": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Neural Architectures: Past, Present and Future",
    "title": "Graph-Based Neural Architecture Search With Operation Embeddings",
    "authors": [
      "Michail Chatzianastasis",
      "George Dasoulas",
      "Georgios Siolas",
      "Michalis Vazirgiannis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/html/Chatzianastasis_Graph-Based_Neural_Architecture_Search_With_Operation_Embeddings_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Chatzianastasis_Graph-Based_Neural_Architecture_Search_With_Operation_Embeddings_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Neural Architecture Search (NAS) has recently gained increased attention, as a class of approaches that automatically searches in an input space of network architectures. A crucial part of the NAS pipeline is the encoding of the architecture that consists of the applied computational blocks, namely the operations and the links between them. Most of the existing approaches either fail to capture the structural properties of the architectures or use hand-engineered vector to encode the operator information. In this paper, we propose the replacement of fixed operator encoding with learnable representations in the optimization process. This approach, which effectively captures the relations of different operations, leads to smoother and more accurate representations of the architectures and consequently to improved performance of the end task. Our extensive evaluation in ENAS benchmark demonstrates the effectiveness of the proposed operation embeddings to the generation of highly accurate models, achieving state-of-the-art performance. Finally, our method produces top-performing architectures that share similar operation and graph patterns, highlighting a strong correlation between the structural properties of the architecture and its performance",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_ahierarchicalclassificationsystemforthedetectionofcovid-19fromchestx-rayimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "A Hierarchical Classification System for the Detection of COVID-19 From Chest X-Ray Images",
    "authors": [
      "Meghna P Ayyar",
      "Jenny Benois-Pineau",
      "Akka Zemmari"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Ayyar_A_Hierarchical_Classification_System_for_the_Detection_of_COVID-19_From_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Ayyar_A_Hierarchical_Classification_System_for_the_Detection_of_COVID-19_From_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With the ever-increasing cases of the Covid-19 pandemic, it is important to leverage deep learning methods to create tools that can aid in relieving the pressure that is put on the limited resources in most developing countries. In this work, we propose a hierarchical classification system for the classification of Covid-19 from Chest X-Ray (CXR) images following a recent proposal of massive use of this modality instead of CT. The system composed of multiple binary classifiers outperforms a tailor-made multi-class classifier COVID-Net. We also show that using well-known established deep learning frameworks combined with a global attention mechanism outperforms the baseline COVID-Net specifically designed for the classification of Covid-19 from CXR images. Our method shows approximately a 4% improvement in the sensitivity to Covid-19 detection from 91% of COVID-Net to 96%. Using popular networks with the possibility of cross-domain transfer learning ensures that the designing and training times are reduced. Furthermore, well-established frameworks can be faster adapted into an application in clinical practice.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_brainmidlineshiftdetectionandquantificationbyacascadeddeepnetworkpipelineonnon-contrastcomputedtomographyscans": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "Brain Midline Shift Detection and Quantification by a Cascaded Deep Network Pipeline on Non-Contrast Computed Tomography Scans",
    "authors": [
      "Nguyen P. Nguyen",
      "Youngjin Yoo",
      "Andrei Chekkoury",
      "Eva Eibenberger",
      "Thomas J. Re",
      "Jyotipriya Das",
      "Abishek Balachandran",
      "Yvonne W. Lui",
      "Pina C. Sanelli",
      "Thomas J. Schroeppel",
      "Uttam Bodanapally",
      "Savvas Nicolaou",
      "Tommi A. White",
      "Filiz Bunyak",
      "Dorin Comaniciu",
      "Eli Gibson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Nguyen_Brain_Midline_Shift_Detection_and_Quantification_by_a_Cascaded_Deep_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Nguyen_Brain_Midline_Shift_Detection_and_Quantification_by_a_Cascaded_Deep_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Brain midline shift (MLS), demonstrated by imaging, is a qualitative and quantitative radiological feature which measures the extent of lateral shift of brain midline structures in response to mass effect caused by hematomas, tumors, abscesses or any other space occupying intracranial lesions. It can be used, with other parameters, to determine the urgency of neurosurgical interventions and to predict clinical outcome in patients with space occupying lesions. However, precisely detecting and quantifying MLS can be challenging due to the great variability in clinically relevant brain structures across cases. In this study, we investigated a cascaded network pipeline consisting of case-level MLS detection and initial localization and refinement of brain landmark locations by using classification and segmentation network architectures. We used a 3D U-Net for initial localization and subsequently a 2D U-Net to estimate exact landmark points at finer resolution. In the refinement step, we fused the prediction from multiple slices to calculate the final location for each landmark. We trained these two U- Nets with the Gaussian heatmap targets generated from the brain's anatomical markers. The case-level ground-truth labels and landmark annotation were generated by multiple trained annotators and reviewed by radiology technologists and radiologists. Our proposed pipeline achieved the case- level MLS detection performance of 95.3% in AUC using a testing dataset from 2,545 head non-contrast computed tomography cases and quantify MLS with a mean absolute error of 1.20 mm on 228 MLS positive cases.",
    "code_link": "https://github.com/qubvel/segmentation"
  },
  "iccv2021_mia-cov19d_advanced3ddeepnon-localembeddedsystemforself-augmentedx-ray-basedcovid-19assessment": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "Advanced 3D Deep Non-Local Embedded System for Self-Augmented X-Ray-Based COVID-19 Assessment",
    "authors": [
      "Francesco Rundo",
      "Angelo Genovese",
      "Roberto Leotta",
      "Fabio Scotti",
      "Vincenzo Piuri",
      "Sebastiano Battiato"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Rundo_Advanced_3D_Deep_Non-Local_Embedded_System_for_Self-Augmented_X-Ray-Based_COVID-19_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Rundo_Advanced_3D_Deep_Non-Local_Embedded_System_for_Self-Augmented_X-Ray-Based_COVID-19_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "COVID-19 diagnosis using chest x-ray (CXR) imaging has a greater sensitivity and faster acquisition procedures than the Real-Time Polimerase Chain Reaction (RT-PCR) test, also requiring radiology machinery that is cheap and widely available. To process the CXR images, methods based on Deep Learning (DL) are being increasingly used, often in combination with data augmentation techniques. However, no method in the literature performs data augmentation in which the augmented training samples are processed collectively as a multi-channel image. Furthermore, no approach has yet considered a combination of attention-based networks with Convolutional Neural Networks (CNN) for COVID-19 detection. In this paper, we propose the first method for COVID-19 detection from CXR images that uses an innovative self-augmentation scheme based on reinforcement learning, which combines all the augmented images in a 3D deep volume and processes them together using a novel non-local deep CNN, which integrates convolutional and attention layers based on non-local blocks. Results on publicly-available databases exhibit a greater accuracy than the state of the art, also showing that the regions of CXR images influencing the decision are consistent with radiologists' observations.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_intelligentradiomicanalysisofq-spect/ctimagestooptimizepulmonaryembolismdiagnosisincovid-19patients": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "Intelligent Radiomic Analysis of Q-SPECT/CT Images To Optimize Pulmonary Embolism Diagnosis in COVID-19 Patients",
    "authors": [
      "Debora Gil",
      "Sonia Baeza",
      "Carles Sanchez",
      "Guillermo Torres",
      "Ignasi Garc\u00eda-Oliv\u00e9",
      "Gloria Moragas",
      "Jordi Deport\u00f3s",
      "Maite Salcedo",
      "Antoni Rosell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Gil_Intelligent_Radiomic_Analysis_of_Q-SPECTCT_Images_To_Optimize_Pulmonary_Embolism_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Gil_Intelligent_Radiomic_Analysis_of_Q-SPECTCT_Images_To_Optimize_Pulmonary_Embolism_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Coronavirus disease 2019 (COVID-19) pneumonia is associated with a high rate of pulmonary embolism (PE). In patients with contraindications for CT pulmonary angiography (CTPA) or non-diagnostic on CTPA, perfusion single photon emission computed tomography/computed tomography (Q-SPECT/CT) is a diagnosis option. The goal of this work is to develop an Intelligent Radiomic system for the detection of PE in COVID-19 patients from the analysis of Q-SPECT/CT scans. Our Intelligent Radiomic System for identification of patients with PE (with/without pneumonia) is based on a local analysis of SPECT-CT volumes that considers both CT and SPECT values for each volume point. We present an hybrid approach that uses radiomic features extracted from each scan as input to a siamese classification network trained to discriminate among 4 different types of tissue: no pneumonia without PE (control group), no pneumonia with PE, pneumonia without PE and pneumonia with PE. The proposed radiomic system has been tested on 133 patients, 63 with COVID-19 (26 with PE, 22 without PE, 15 indeterminate-PE) and 70 without COVID-19 (31 healthy/control, 39 with PE). The per-patient recall for the detection of COVID-19 pneumonia and COVID-19 pneumonia with PE was, respectively, 91% and 81% with an area under the receiver operating characteristic curves equal to 0.99 and 0.87.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_residualdilatedu-netforthesegmentationofcovid-19infectionfromctimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "Residual Dilated U-Net for the Segmentation of COVID-19 Infection From CT Images",
    "authors": [
      "Alyaa Amer",
      "Xujiong Ye",
      "Faraz Janan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Amer_Residual_Dilated_U-Net_for_the_Segmentation_of_COVID-19_Infection_From_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Amer_Residual_Dilated_U-Net_for_the_Segmentation_of_COVID-19_Infection_From_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Medical imaging such as computed tomography (CT) plays a critical role in the global fight against COVID-19. Computer-aided platforms have emerged to help radiologists diagnose and track disease prognosis. In this paper, we introduce an automated deep-learning segmentation model, which builds upon the current U-net model, however, leverages the strengths of long and short skip connections. We complemented the long skip connections with a cascaded dilated convolution module that learns multiscale context information, compensates the reduction in receptive fields, and reduces the disparity between encoded and decoded features. The short connections are considered in utilizing residual blocks as the basic building blocks for our model. They ease the training process, reduce the degradation problem, and propagate the low fine details. This enables the model to perform well in capturing smaller regions of interest. Furthermore, each residual block is followed by a squeeze and excitation unit, which stimulates informative features and suppresses less important ones, thus improving the overall feature representation. After extensive experimentation with a dataset of 1705 COVID-19 axial CT images, we demonstrate that performance gains can be achieved when deep learning modules are integrated with the basic U- net model. Experimental results show that our model outperformed the basic U-net and ResDUnet model by 8.1% and 1.9% in dice similarity, respectively. Our model provided a dice similarity measure of 85.3%, with a slight increase in trainable parameters, thus demonstrating a huge potential for use in the clinical domain.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_visualinterpretabilityanalysisofdeepcnnsusinganadaptivethresholdmethodondiabeticretinopathyimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "Visual Interpretability Analysis of Deep CNNs Using an Adaptive Threshold Method on Diabetic Retinopathy Images",
    "authors": [
      "George Ioannou",
      "Tasos Papagiannis",
      "Thanos Tagaris",
      "Georgios Alexandridis",
      "Andreas Stafylopatis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Ioannou_Visual_Interpretability_Analysis_of_Deep_CNNs_Using_an_Adaptive_Threshold_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Ioannou_Visual_Interpretability_Analysis_of_Deep_CNNs_Using_an_Adaptive_Threshold_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep neural networks have been dominating the field of computer vision, achieving exceptional performance on object detection and pattern recognition. However, despite the highly accurate predictions of these models, the continuous increase in depth and complexity comes at the cost of interpretability, making the task of explaining the reasoning behind these predictions very challenging. In this paper, an analysis of state-of-the-art approaches towards the direction of interpreting the networks' representations, is carried out over two Diabetic Retinopathy image datasets, IDRiD and DDR. Furthermore, these techniques are compared in the task of image segmentation of the same datasets. This is to discover which method can produce the better attention maps that can solve the problem of segmentation without actually training the network for the specific task. To accomplish that we propose an adaptive threshold method that transforms the attention masks in a more suitable representation for segmentation. Experiments over multiple architectures were conducted to ensure the robustness of the results.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_a3dcnnnetworkwithbertforautomaticcovid-19diagnosisfromct-scanimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "A 3D CNN Network With BERT for Automatic COVID-19 Diagnosis From CT-Scan Images",
    "authors": [
      "Weijun Tan",
      "Jingfeng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Tan_A_3D_CNN_Network_With_BERT_for_Automatic_COVID-19_Diagnosis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Tan_A_3D_CNN_Network_With_BERT_for_Automatic_COVID-19_Diagnosis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice images. In this framework, the slice images of a CT-scan volume are first preprocessed using segmentation techniques to filter out images of closed lung, and to remove the useless background. Then a resampling method is used to select a set of fixed number of slice images for training and validation. A 3D CNN network with BERT is used to classify this set of selected slice images. In this network, an embedding feature is also extracted. In cases where there are more than one set of slice images in a volume, the features of all sets are extracted and pooled into a feature vector for the whole CT-scan volume. A simple multiple-layer perceptron (MLP) network is used to further classify the aggregated feature vector. The models are trained and evaluated on the provided training and validation datasets. On the validation dataset, the precision is 0.9278 and the F1 score is 0.9261. On the test dataset, the F1 score is 0.8822.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_atransformer-basedframeworkforautomaticcovid19diagnosisinchestcts": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "A Transformer-Based Framework for Automatic COVID19 Diagnosis in Chest CTs",
    "authors": [
      "Lei Zhang",
      "Yan Wen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Zhang_A_Transformer-Based_Framework_for_Automatic_COVID19_Diagnosis_in_Chest_CTs_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Zhang_A_Transformer-Based_Framework_for_Automatic_COVID19_Diagnosis_in_Chest_CTs_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Automated diagnosis of covid19 in chest CTs is becoming a clinically important technique to support precision and efficient diagnosis and treatment planning. A few efforts have been made to automatically diagnose the COVID-19 in CTs using CNNs, and the task still remains a challenge. In this paper, we present a transformer-based framework for COVID19 classification. We attempt to expand the adaption of vision transformer as a robust feature learner to the 3D CTs to diagnose the COVID-19. The framework consists of two main stages: lung segmentation using UNet followed by the classification, in which the features extracted from each CT slice using Swin transformer in a CT scan are aggregated into 3D volume level feature. We also investigated the performance of using the robust CNNs (BiT and EfficientNetV2) as backbones in the framework. The dataset from the ICCV workshop: MIA-COV19D, is used in our experiments. The evaluation results show that the method with the backbone of Swin transformer gain the best F1 score of 0.935 on the validation dataset, while the CNN based backbone of EfficientNetV2 has the competitive classification performance with the best precision of 93.7%. The final prediction model with Swin transformer achieves the F1 score of 0.84 on the test dataset, which doesn't require an additional post-processing stage.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_evaluatingvolumetricandslice-basedapproachesforcovid-19detectioninchestcts": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "Evaluating Volumetric and Slice-Based Approaches for COVID-19 Detection in Chest CTs",
    "authors": [
      "Radu Miron",
      "Cosmin Moisii",
      "Sergiu Dinu",
      "Mihaela Elena Breaban"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Miron_Evaluating_Volumetric_and_Slice-Based_Approaches_for_COVID-19_Detection_in_Chest_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Miron_Evaluating_Volumetric_and_Slice-Based_Approaches_for_COVID-19_Detection_in_Chest_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The paper presents a comparative analysis of several distinct approaches based on deep learning for identifying COVID-19 cases in chest CTs. A first approach is a volumetric one, involving 3D convolutions, while other two approaches perform at first slice-wise classification and then aggregate the results at the volume level. The experiments are carried on the COV19-CT-DB dataset, with the aim of addressing the challenge raised by the MIA-COV19D Competition within ICCV 2021. Our best results reach a macro F1 score of 92.34% on the validation subset and 90.06% on the test set, obtained with the volumetric approach which was ranked second in the competition. Its performance can be further increased by a simple trick, using semi-supervised training in the form of self-training, technique which proved to bring a consistent increase over the reported F1-score on the validation subset.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_covid19diagnosisusingautomlfrom3dctscans": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "COVID19 Diagnosis Using AutoML From 3D CT Scans",
    "authors": [
      "Talha Anwar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Anwar_COVID19_Diagnosis_Using_AutoML_From_3D_CT_Scans_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Anwar_COVID19_Diagnosis_Using_AutoML_From_3D_CT_Scans_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Coronavirus is a pandemic that affects the respiratory system causing cough, shortness of breath, and death in severe cases. Polymerase chain reaction (PCR) tests are used to diagnose coronavirus. The false-negative rate of these tests is high, so there needs a supporting method for an accurate diagnosis. CT scan provides a detailed examination of the chest to diagnose COVID, but a single CT scan comprises hundreds of slices. Expert and experienced radiologists and pulmonologists can diagnose COVID from these hundreds of slices, but this is very time-consuming. So an automatic artificial intelligence (AI) based method is required to diagnose coronavirus with high accuracy. Developing this AI-based technique requires a lot of resources and time, but once it is developed, it can significantly help the clinicians. This paper used an Automated machine learning (AutoML) technique that requires fewer resources (optimal architecture trials) and time to develop, resulting in the best diagnosis. The AutoML models are trained on 2D slices instead of 3D CT scans, and the predictions on unknown data (slices of CT scan) are aggregated to form a prediction of 3D CT scan. The aggregation process picked the most occurred case, whether COVID or non-COVID from all CT scan slices and labeled the 3D CT scan accordingly. Different thresholds are also used to label COVID or non-COVID 3D CT scans from 2D slices. The approach resulted in accuracy and F1-score of 89% and 88%, respectively. Implementation is available at github.com/talhaanwarch/mia-covid19",
    "code_link": "https://github.com/talhaanwarch/mia-covid19"
  },
  "iccv2021_mia-cov19d_cmc-cov19dcontrastivemixupclassificationforcovid-19diagnosis": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "CMC-COV19D: Contrastive Mixup Classification for COVID-19 Diagnosis",
    "authors": [
      "Junlin Hou",
      "Jilan Xu",
      "Rui Feng",
      "Yuejie Zhang",
      "Fei Shan",
      "Weiya Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep learning methods have been extensively investigated for rapid and precise computer-aided diagnosis during the outbreak of the COVID-19 epidemic. However, there are still remaining issues to be addressed, such as distinguishing COVID-19 in the complex scenario of multi-type pneumonia classification. In this paper, we aim to boost the COVID-19 diagnostic performance with more discriminative deep representations of COVID and non-COVID categories. We propose a novel COVID-19 diagnosis approach with contrastive representation learning to effectively capture the intra-class similarity and inter-class difference. Besides, we design an adaptive joint training strategy to integrate the classification loss, mixup loss, and contrastive loss. Through the joint loss function, we obtain the high-level representations which are highly discriminative in COVID-19 screening. Extensive experiments on two chest CT image datasets, i.e., CC-CCII dataset and COV19-CT-DB database, demonstrate the effectiveness of our proposed approach in COVID-19 diagnosis. Our method won the first prize in the ICCV 2021 Covid-19 Diagnosis Competition of AI-enabled Medical Image Analysis Workshop. Our code is publicly available at https://github.com/houjunlin/Team-FDVTS-COVID-Solution.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_ahybridandfastdeeplearningframeworkforcovid-19detectionvia3dchestctimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "A Hybrid and Fast Deep Learning Framework for COVID-19 Detection via 3D Chest CT Images",
    "authors": [
      "Shuang Liang",
      "Weicun Zhang",
      "Yu Gu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Liang_A_Hybrid_and_Fast_Deep_Learning_Framework_for_COVID-19_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Liang_A_Hybrid_and_Fast_Deep_Learning_Framework_for_COVID-19_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we present a hybrid deep learning framework named CTNet which combines convolutional neural network (CNN) and transformer together for the detection of COVID-19 via 3D chest CT images. It consists of a CNN feature extractor module with SE attention to extract sufficient features from CT scans, together with a transformer model to model the discriminative features of the 3D CT scans. Compared to previous works, CTNet provides an effective and efficient method to perform COVID-19 diagnosis via 3D CT scans with data resampling strategy. Advanced results on a large and public benchmarks, COV19-CT-DB database, was achieved by the proposed CTNet with a macro F1 score of 88.21% on the validation set, which lead ten percentage over the state-of-the-art baseline approach proposed together with the dataset. Notably, the inference speed of the proposed framework is about ten times faster than that of the typical CNN frameworks which make it more promising in actual applications.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_adaptivedistributionlearningwithstatisticalhypothesistestingforcovid-19ctscanclassification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "Adaptive Distribution Learning With Statistical Hypothesis Testing for COVID-19 CT Scan Classification",
    "authors": [
      "Guan-Lin Chen",
      "Chih-Chung Hsu",
      "Mei-Hsuan Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Chen_Adaptive_Distribution_Learning_With_Statistical_Hypothesis_Testing_for_COVID-19_CT_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Chen_Adaptive_Distribution_Learning_With_Statistical_Hypothesis_Testing_for_COVID-19_CT_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With the massive damage in the world caused by Coronavirus Disease 2019 SARS-CoV-2 (COVID-19), many related research topics have been proposed in the past two years. The Chest Computed Tomography (CT) scan is the most valuable materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19 classification of Chest CT scan are based on single slice-level schemes, implying that the most critical CT slice should be selected from the original CT volume manually. In this paper, a statistical hypothesis test is adopted to the deep neural network to learn the implicit representation of CT slices. Specifically, we propose an Adaptive Distribution Learning with Statistical hypothesis Testing (ADLeaST) for COVID-19 CT scan classification can be used to judge the importance of each slice in CT scan and followed by adopting the nonparametric statistics method, Wilcoxon signed-rank test, to make predicted result explainable and stable. In this way, the impact of out-of-distribution (OOD) samples can be significantly reduced. Meanwhile, a self-attention mechanism without statistical analysis is also introduced into the backbone network to learn the importance of the slices explicitly. The extensive experiments show that both the proposed schemes are stable and superior. Our experiments also demonstrated that the proposed ADLeaST significantly outperforms the state-of-the-art methods.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_thevalueofvisualattentionforcovid-19classificationinctscans": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "The Value of Visual Attention for COVID-19 Classification in CT Scans",
    "authors": [
      "Adrit Rao",
      "Jongchan Park",
      "Oliver Aalami"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Rao_The_Value_of_Visual_Attention_for_COVID-19_Classification_in_CT_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Rao_The_Value_of_Visual_Attention_for_COVID-19_Classification_in_CT_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Detecting COVID-19 in early stages is crucial in order to initiate timely treatment of disease. COVID-19 screening with chest CT scans has been utilized due to the rapidity of results and robustness. Computer vision aided medical diagnosis with deep learning models can improve accuracy and efficiency of screening. When developing models for high-risk medical classification tasks, it is important to aim to reach radiologist level interpretation in terms of cognition. When the human brain analyzes visual information, cognitive visual attention is applied in order to apply more focus onto higher frequency regions of interest. Using attention mechanisms in order to infer channel and spatial attention maps within convolutional neural networks can improve the performance in classification of COVID-19 changes. Through performing a compact study with a quantitative accuracy measure along with a qualitative visualization of activation heat-maps, we study the benefits of visual self-attention for the classification of COVID-19.",
    "code_link": ""
  },
  "iccv2021_mia-cov19d_telinetclassifyingctscanimagesforcovid-19diagnosis": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "TeliNet: Classifying CT Scan Images for COVID-19 Diagnosis",
    "authors": [
      "Mohammad Nayeem Teli"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Teli_TeliNet_Classifying_CT_Scan_Images_for_COVID-19_Diagnosis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Teli_TeliNet_Classifying_CT_Scan_Images_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "COVID-19 has led to hundreds of millions of cases and millions of deaths worldwide since its onset. The fight against this pandemic is on-going on multiple fronts. While vaccinations are picking up speed, there are still billions of unvaccinated people. In this fight against the virus, di- agnosis of the disease and isolation of the patients to pre- vent any spread play a huge role. Machine Learning ap- proaches have assisted in the diagnosis of COVID-19 cases by analyzing chest X-rays and CT-scan images of patients. To push algorithm development and research in this direc- tion of radiological diagnosis, a challenge to classify CT- scan series was organized in conjunction with ICCV, 2021. In this research we present a simple and shallow Convo- lutional Neural Network based approach, TeliNet, to clas- sify these CT-scan images of COVID-19 patients presented as part of this competition. Our results outperform the F1 'macro' score of the competition benchmark and VGGNet approaches. Our proposed solution is also more lightweight in comparison to the other methods.",
    "code_link": "https://github.com/nayeemmz/TeliNet"
  },
  "iccv2021_mia-cov19d_mia-cov19dcovid-19detectionthrough3-dchestctimageanalysis": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MIA-COV19D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI-Enabled Medical Image Analysis and COVID-19 Diagnosis",
    "title": "MIA-COV19D: COVID-19 Detection Through 3-D Chest CT Image Analysis",
    "authors": [
      "Dimitrios Kollias",
      "Anastasios Arsenos",
      "Levon Soukissian",
      "Stefanos Kollias"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Kollias_MIA-COV19D_COVID-19_Detection_Through_3-D_Chest_CT_Image_Analysis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Kollias_MIA-COV19D_COVID-19_Detection_Through_3-D_Chest_CT_Image_Analysis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist medical specialists in vital circumstances. Deep learning methodologies constitute a main approach for chest CT scan analysis and disease prediction. However, large annotated databases are necessary for developing deep learning models that are able to provide COVID-19 diagnosis across various medical environments in different countries. Due to privacy issues, publicly available COVID-19 CT datasets are highly difficult to obtain, which hinders the research and development of AI-enabled diagnosis methods of COVID-19 based on CT scans. In this paper we present the COV19-CT-DB database which is annotated for COVID-19, consisting of about 5,000 3-D CT scans, We have split the database in training, validation and test datasets. The former two datasets can be used for training and validation of machine learning models, while the latter will be used for evaluation of the developed models. We present a deep learning approach, based on a CNN-RNN network and report its performance on the COVID19-CT-DB database. Moreover, we present the results of all main techniques that were developed and used in the ICCV COV19D Competition.",
    "code_link": ""
  },
  "iccv2021_cdpath_aquadtreeimagerepresentationforcomputationalpathology": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "A QuadTree Image Representation for Computational Pathology",
    "authors": [
      "Robert Jewsbury",
      "Abhir Bhalerao",
      "Nasir M. Rajpoot"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Jewsbury_A_QuadTree_Image_Representation_for_Computational_Pathology_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Jewsbury_A_QuadTree_Image_Representation_for_Computational_Pathology_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The field of computational pathology presents many challenges for computer vision algorithms due to the sheer size of pathology images. Histopathology images are large and need to be split up into image tiles or patches so modern convolutional neural networks (CNNs) can process them. In this work, we present a method to generate an interpretable image representation of computational pathology images using quadtrees and a pipeline to use these representations for highly accurate downstream classification. To the best of our knowledge, this is the first attempt to use quadtrees for pathology image data. We show it is highly accurate, able to achieve as good results as the currently widely adopted tissue mask patch extraction methods all while using over 38% less data.",
    "code_link": ""
  },
  "iccv2021_cdpath_probeabledartswithapplicationtocomputationalpathology": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Probeable DARTS With Application to Computational Pathology",
    "authors": [
      "Sheyang Tang",
      "Mahdi S. Hosseini",
      "Lina Chen",
      "Sonal Varma",
      "Corwyn Rowsell",
      "Savvas Damaskinos",
      "Konstantinos N. Plataniotis",
      "Zhou Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Tang_Probeable_DARTS_With_Application_to_Computational_Pathology_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Tang_Probeable_DARTS_With_Application_to_Computational_Pathology_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "AI technology has made remarkable achievements in computational pathology (CPath), especially with the help of deep neural networks. However, the network performance is highly related to architecture design, which commonly requires human experts with domain knowledge. In this paper, we combat this challenge with the recent advance in neural architecture search (NAS) to find an optimal network for CPath applications. In particular, we use differentiable architecture search (DARTS) for its efficiency. We first adopt a probing metric to show that the original DARTS lacks proper hyperparameter tuning on the CIFAR dataset, and how the generalization issue can be addressed using an adaptive optimization strategy. We then apply our searching framework on CPath applications by searching for the optimum network architecture on a histological tissue type dataset (ADP). Results show that the searched network outperforms state-of-the-art networks in terms of prediction accuracy and computation complexity. We further conduct extensive experiments to demonstrate the transferability of the searched network to new CPath applications, the robustness against downscaled inputs, as well as the reliability of predictions.",
    "code_link": "https://github.com/mahdihosseini/DARTS-ADP"
  },
  "iccv2021_cdpath_albrtcellularcompositionpredictioninroutinehistologyimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "ALBRT: Cellular Composition Prediction in Routine Histology Images",
    "authors": [
      "Muhammad Dawood",
      "Kim Branson",
      "Nasir M. Rajpoot",
      "Fayyaz Minhas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Dawood_ALBRT_Cellular_Composition_Prediction_in_Routine_Histology_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Dawood_ALBRT_Cellular_Composition_Prediction_in_Routine_Histology_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Cellular composition prediction, i.e., predicting the presence and counts of different types of cells in the tumor microenvironment from a digitized image of a Hematoxylin and Eosin (H&E) stained tissue section can be used for various tasks in computational pathology such as the analysis of cellular topology and interactions, subtype prediction, survival analysis, etc. In this work, we propose an image-based cellular composition predictor (ALBRT) which can accurately predict the presence and counts of different types of cells in a given image patch. ALBRT, by its contrastive-learning inspired design, learns a compact and rotation-invariant feature representation that is then used for cellular composition prediction of different cell types. It offers significant improvement over existing state-of-the-art approaches for cell classification and counting. The patch-level feature representation learned by ALBRT is transferrable for cellular composition analysis over novel datasets and can also be utilized for downstream prediction tasks in CPath as well. The code and the inference webserver for the proposed method are available at the URL: https://github.com/engrodawood/ALBRT.",
    "code_link": ""
  },
  "iccv2021_cdpath_self-supervisedrepresentationlearningusingvisualfieldexpansionondigitalpathology": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Self-Supervised Representation Learning Using Visual Field Expansion on Digital Pathology",
    "authors": [
      "Joseph Boyd",
      "Mykola Liashuha",
      "Eric Deutsch",
      "Nikos Paragios",
      "Stergios Christodoulidis",
      "Maria Vakalopoulou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Boyd_Self-Supervised_Representation_Learning_Using_Visual_Field_Expansion_on_Digital_Pathology_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Boyd_Self-Supervised_Representation_Learning_Using_Visual_Field_Expansion_on_Digital_Pathology_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The examination of histopathology images is considered to be the gold standard for the diagnosis and stratification of cancer patients. A key challenge in the analysis of such images is their size, which can run into the gigapixels and can require tedious screening by clinicians. With the recent advances in computational medicine, automatic tools have been proposed to assist clinicians in their everyday practice. Such tools typically process these large images by slicing them into tiles that can then be encoded and utilized for different clinical models. In this study, we propose a novel generative framework that can learn powerful representations for such tiles by learning to plausibly expand their visual field. In particular, we developed a progressively grown generative model with the objective of visual field expansion. Thus trained, our model learns to generate different tissue types with fine details, while simultaneously learning powerful representations that can be used for different clinical endpoints, all in a self-supervised way. To evaluate the performance of our model, we conducted classification experiments on CAMELYON17 and CRC benchmark datasets, comparing favorably to other self-supervised and pre-trained strategies that are commonly used in digital pathology. Our code is available at https://github.com/jcboyd/cdpath21-gan.",
    "code_link": ""
  },
  "iccv2021_cdpath_iterativecross-scannerregistrationforwholeslideimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Iterative Cross-Scanner Registration for Whole Slide Images",
    "authors": [
      "Luisa Theelke",
      "Frauke Wilm",
      "Christian Marzahl",
      "Christof A. Bertram",
      "Robert Klopfleisch",
      "Andreas Maier",
      "Marc Aubreville",
      "Katharina Breininger"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Theelke_Iterative_Cross-Scanner_Registration_for_Whole_Slide_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Theelke_Iterative_Cross-Scanner_Registration_for_Whole_Slide_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The successful registration of digitized microscopic images is required for many applications in digital pathology. In particular, the registration of specimens scanned by different slide scanning systems may be beneficial to transfer expert annotations from one image domain to another and thereby reduce labeling effort. We present an iterative approach to register microscopic specimens digitized with multiple scanning systems, aiming to compute an optimal global transformation for the images at highest resolution. For this purpose, an initial registration based on a down-scaled version of the images is followed by a patch-based iterative update scheme. We make use of the hierarchical structure of digitized whole slide images to gradually approximate the optimal transformation. By using kernel density estimation to weight local transformation estimates, the influence of registration errors can be further mitigated. We validate our method on five histologic and five cytologic samples, each scanned with four different scanning systems. Furthermore, we perform first experiments on samples stained with different stain combinations. Our experiments demonstrate the potential of the proposed method for a variety of datasets and application fields.",
    "code_link": "https://github.com/DeepPathology/CrossScannerRegistration.git"
  },
  "iccv2021_cdpath_jointsemi-supervisedandactivelearningforsegmentationofgigapixelpathologyimageswithcost-effectivelabeling": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Joint Semi-Supervised and Active Learning for Segmentation of Gigapixel Pathology Images With Cost-Effective Labeling",
    "authors": [
      "Zhengfeng Lai",
      "Chao Wang",
      "Luca Cerny Oliveira",
      "Brittany N. Dugger",
      "Sen-Ching Cheung",
      "Chen-Nee Chuah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Lai_Joint_Semi-Supervised_and_Active_Learning_for_Segmentation_of_Gigapixel_Pathology_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Lai_Joint_Semi-Supervised_and_Active_Learning_for_Segmentation_of_Gigapixel_Pathology_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The need for manual and detailed annotations limits the applicability of supervised deep learning algorithms in medical image analyses, specifically in the field of pathology. Semi-supervised learning (SSL) provides an effective way for leveraging unlabeled data to relieve the heavy reliance on the amount of labeled samples when training a model. Although SSL has shown good performance, the performance of recent state-of-the-art SSL methods on pathology images is still under study. The problem for selecting the most optimal data to label for SSL is not fully explored. To tackle this challenge, we propose a semi-supervised active learning framework with a region-based selection criterion. This framework iteratively selects regions for annotation query to quickly expand the diversity and volume of the labeled set. We evaluate our framework on a grey-matter/white-matter segmentation problem using gigapixel pathology images from autopsied human brain tissues. With only 0.1% regions labeled, our proposed algorithm can reach a competitive IoU score compared to fully-supervised learning and outperform the current state-of-the-art SSL by more than 10% of IoU score and DICE coefficient.",
    "code_link": ""
  },
  "iccv2021_cdpath_simultaneousnuclearinstanceandlayersegmentationinoralepithelialdysplasia": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Simultaneous Nuclear Instance and Layer Segmentation in Oral Epithelial Dysplasia",
    "authors": [
      "Adam J. Shephard",
      "Simon Graham",
      "Saad Bashir",
      "Mostafa Jahanifar",
      "Hanya Mahmood",
      "Ali Khurram",
      "Nasir M. Rajpoot"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Shephard_Simultaneous_Nuclear_Instance_and_Layer_Segmentation_in_Oral_Epithelial_Dysplasia_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Shephard_Simultaneous_Nuclear_Instance_and_Layer_Segmentation_in_Oral_Epithelial_Dysplasia_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Oral epithelial dysplasia (OED) is a pre-malignant histopathological diagnosis given to lesions of the oral cavity. Predicting OED grade or whether a case will transition to malignancy is critical for early detection and appropriate treatment. OED typically begins in the lower third of the epithelium before progressing upwards with grade severity, thus we have suggested that segmenting intra-epithelial layers, in addition to individual nuclei, may enable researchers to evaluate important layer-specific morphological features for grade/malignancy prediction. We present HoVer-Net+, a deep learning framework to simultaneously segment (and classify) nuclei and (intra-)epithelial layers in H&E stained slides from OED cases. The proposed architecture consists of an encoder branch and four decoder branches for simultaneous instance segmentation of nuclei and semantic segmentation of the epithelial layers. We show that the proposed model achieves the state-of-the-art (SOTA) performance in both tasks, with no additional costs when compared to previous SOTA methods for each task. To the best of our knowledge, ours is the first method for simultaneous nuclear instance segmentation and semantic tissue segmentation, with potential for use in computational pathology for other similar simultaneous tasks and for future studies into malignancy prediction.",
    "code_link": ""
  },
  "iccv2021_cdpath_guidedrepresentationlearningfortheclassificationofhematopoieticcells": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Guided Representation Learning for the Classification of Hematopoietic Cells",
    "authors": [
      "Philipp Gr\u00e4bel",
      "Martina Crysandt",
      "Barbara M. Klinkhammer",
      "Peter Boor",
      "Tim H. Br\u00fcmmendorf",
      "Dorit Merhof"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Grabel_Guided_Representation_Learning_for_the_Classification_of_Hematopoietic_Cells_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Grabel_Guided_Representation_Learning_for_the_Classification_of_Hematopoietic_Cells_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Cell classification in human bone marrow microscopy images is a challenging image analysis task due to the number and inter-connection of cell types. While machine learning techniques have vastly higher throughput and could thus be more reliable, humans are intrinsically capable of understanding relations between cell types. In this paper, we propose methods to incorporate such intrinsic model knowledge based on representation learning. To this end, we construct a manually defined, two-dimensional reference embedding, coined embedding guide, which we use together with inverse dimensionality reduction, a distance-based loss and a growing embedding technique. Results show improved classification scores as well as a visually interpretable and clearly defined embedding space.",
    "code_link": ""
  },
  "iccv2021_cdpath_improvingself-supervisedlearningwithhardness-awaredynamiccurriculumlearninganapplicationtodigitalpathology": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Improving Self-Supervised Learning With Hardness-Aware Dynamic Curriculum Learning: An Application to Digital Pathology",
    "authors": [
      "Chetan L. Srinidhi",
      "Anne L. Martel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Srinidhi_Improving_Self-Supervised_Learning_With_Hardness-Aware_Dynamic_Curriculum_Learning_An_Application_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Srinidhi_Improving_Self-Supervised_Learning_With_Hardness-Aware_Dynamic_Curriculum_Learning_An_Application_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Self-supervised learning (SSL) has recently shown tremendous potential to learn generic visual representations useful for many image analysis tasks. Despite their notable success, the existing SSL methods fail to generalize to downstream tasks when the number of labeled training instances is small or if the domain shift between the transfer domains is significant. In this paper, we attempt to improve self-supervised pretrained representations through the lens of curriculum learning by proposing a hardness-aware dynamic curriculum learning (HaDCL) approach. To improve the robustness and generalizability of SSL, we dynamically leverage progressive harder examples via easy-to-hard and hard-to-very-hard samples during mini-batch downstream fine-tuning. We discover that by progressive stage-wise curriculum learning, the pretrained representations are significantly enhanced and adaptable to both in-domain and out-of-domain distribution data. We performed extensive validation on three histology benchmark datasets on both patch-wise and slide-level classification problems. Our curriculum based fine-tuning yields a significant improvement over standard fine-tuning, with a minimum improvement in area-under-the-curve (AUC) score of 1.7% and 2.2% on in-domain and out-of-domain distribution data, respectively. Further, we empirically show that our approach is more generic and adaptable to any SSL methods and does not impose any additional overhead complexity. Besides, we also outline the role of patch-based versus slide-based curriculum learning in histopathology to provide practical insights into the success of curriculum based fine-tuning of SSL methods.",
    "code_link": "https://github.com/srinidhiPY/ICCVCDPATH2021-ID-8"
  },
  "iccv2021_cdpath_aninvestigationofattentionmechanismsinhistopathologywhole-slide-imageanalysisforregressionobjectives": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "An Investigation of Attention Mechanisms in Histopathology Whole-Slide-Image Analysis for Regression Objectives",
    "authors": [
      "Philippe Weitz",
      "Yinxi Wang",
      "Johan Hartman",
      "Mattias Rantalainen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Weitz_An_Investigation_of_Attention_Mechanisms_in_Histopathology_Whole-Slide-Image_Analysis_for_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Weitz_An_Investigation_of_Attention_Mechanisms_in_Histopathology_Whole-Slide-Image_Analysis_for_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Analysis of whole-slide-images (WSIs) of histopathology tissue sections remains challenging due to the gigapixel scale of these images, which often necessitates their division into smaller image tiles. Recently, attention mechanisms have been successfully applied to alleviate the tile-to-slide challenges for classification tasks based on WSIs. In this study, we explore the potential of attention mechanisms in regression settings, by comparing four modelling approaches, two of which use attention mechanisms. We evaluate these models both in a simulated experiment using the MNIST data set, and in real histopathology data sets focused on prediction of gene expression levels from WSIs, including an analysis of the local prediction performance using spatial transcriptomics. The MNIST simulation demonstrates that if only a small proportion of instances in a set of images contribute to the set-level regression label, attention mechanisms may be preferable to commonly applied weakly supervised models. When predicting gene expression from WSIs, the differences in performance between the models that we investigated were small. Nevertheless, we found some evidence that attention mechanisms may be more sensitive to domain shifts. In the regression-based task of gene expression prediction, the prediction performance in the present study appears to be limited by other factors rather than by the choice of modelling approach. Nevertheless, attention mechanisms appear promising for regression objectives and warrant further investigation.",
    "code_link": ""
  },
  "iccv2021_cdpath_lizardalarge-scaledatasetforcolonicnuclearinstancesegmentationandclassification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Lizard: A Large-Scale Dataset for Colonic Nuclear Instance Segmentation and Classification",
    "authors": [
      "Simon Graham",
      "Mostafa Jahanifar",
      "Ayesha Azam",
      "Mohammed Nimir",
      "Yee-Wah Tsang",
      "Katherine Dodd",
      "Emily Hero",
      "Harvir Sahota",
      "Atisha Tank",
      "Ksenija Benes",
      "Noorul Wahab",
      "Fayyaz Minhas",
      "Shan E. Ahmed Raza",
      "Hesham El Daly",
      "Kishore Gopalakrishnan",
      "David Snead",
      "Nasir M. Rajpoot"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Graham_Lizard_A_Large-Scale_Dataset_for_Colonic_Nuclear_Instance_Segmentation_and_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Graham_Lizard_A_Large-Scale_Dataset_for_Colonic_Nuclear_Instance_Segmentation_and_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The development of deep segmentation models for computational pathology (CPath) can help foster the investigation of interpretable morphological biomarkers. Yet, there is a major bottleneck in the success of such approaches because supervised deep learning models require an abundance of accurately labelled data. This issue is exacerbated in the field of CPath because the generation of detailed annotations usually demands the input of a pathologist to be able to distinguish between different tissue constructs and nuclei. Manually labelling nuclei may not be a feasible approach for collecting large-scale annotated datasets, especially when a single image region can contain thousands of different cells. However, solely relying on automatic generation of annotations will limit the accuracy and reliability of ground truth. Therefore, to help overcome the above challenges, we propose a multi-stage annotation pipeline to enable the collection of large-scale datasets for histology image analysis, with pathologist-in-the-loop refinement steps. Using this pipeline, we generate the largest known nuclear instance segmentation and classification dataset, containing nearly half a million labelled nuclei in H&E stained colon tissue. We have released the dataset and encourage the research community to utilise it to drive forward the development of downstream cell-based models in CPath.",
    "code_link": ""
  },
  "iccv2021_cdpath_h&e-adversarialnetworkaconvolutionalneuralnetworktolearnstain-invariantfeaturesthroughhematoxylin&eosinregression": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "H&E-Adversarial Network: A Convolutional Neural Network To Learn Stain-Invariant Features Through Hematoxylin & Eosin Regression",
    "authors": [
      "Niccol\u00f2 Marini",
      "Manfredo Atzori",
      "Sebastian Ot\u00e1lora",
      "Stephane Marchand-Maillet",
      "Henning M\u00fcller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Marini_HE-Adversarial_Network_A_Convolutional_Neural_Network_To_Learn_Stain-Invariant_Features_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Marini_HE-Adversarial_Network_A_Convolutional_Neural_Network_To_Learn_Stain-Invariant_Features_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Computational pathology is a domain that aims to develop algorithms to automatically analyze large digitized histopathology images, called whole slide images (WSI). WSIs are produced scanning thin tissue samples that are stained to make specific structures visible. They show stain colour heterogeneity due to different preparation and scanning settings applied across medical centers. Stain colour heterogeneity is a problem to train convolutional neural networks (CNN), the state-of-the-art algorithms for most computational pathology tasks, since CNNs usually underperform when tested on images including different stain variations than those within data used to train the CNN. Despite several methods that were developed, stain colour heterogeneity is still an unsolved challenge that limits the development of CNNs that can generalize on data from several medical centers. This paper aims to present a novel method to train CNNs that better generalize on data including several colour variations. The method, called H&E-adversarial CNN, exploits H&E matrix information to learn stain-invariant features during the training. The method is evaluated on the classification of colon and prostate histopathology images, involving eleven heterogeneous datasets, and compared with five other techniques used to handle stain colour heterogeneity. H&E-adversarial CNNs show an improvement in performance compared to the other algorithms, demonstrating that it can help to better deal with stain colour heterogeneous images.",
    "code_link": "https://github.com/ilmaro8/HE"
  },
  "iccv2021_cdpath_deepordinalfocusassessmentforwholeslideimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Deep Ordinal Focus Assessment for Whole Slide Images",
    "authors": [
      "Tom\u00e9 Albuquerque",
      "Ana Moreira",
      "Jaime S. Cardoso"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Albuquerque_Deep_Ordinal_Focus_Assessment_for_Whole_Slide_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Albuquerque_Deep_Ordinal_Focus_Assessment_for_Whole_Slide_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Medical image quality assessment plays an important role not only in the design and manufacturing processes of image acquisition but also in the optimization of decision support systems. This work introduces a new deep ordinal learning approach for focus assessment in whole slide images. From the blurred image to the focused image there is an ordinal progression that contains relevant knowledge for more robust learning of the models. With this new method, it is possible to infer quality without losing ordinal information about focus since instead of using the nominal cross-entropy loss for training, ordinal losses were used. Our proposed model is contrasted against other state-of-the-art methods present in the literature. A first conclusion is a benefit of using data-driven methods instead of knowledge-based methods. Additionally, the proposed model is found to be the top-performer in several metrics. The best performing model scores an accuracy of 94.4% for a 12 classes classification problem in the FocusPath database.",
    "code_link": ""
  },
  "iccv2021_cdpath_multi-prototypefew-shotlearninginhistopathology": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Multi-Prototype Few-Shot Learning in Histopathology",
    "authors": [
      "Jessica Deuschel",
      "Daniel Firmbach",
      "Carol I. Geppert",
      "Markus Eckstein",
      "Arndt Hartmann",
      "Volker Bruns",
      "Petr Kuritcyn",
      "Jakob Dexl",
      "David Hartmann",
      "Dominik Perrin",
      "Thomas Wittenberg",
      "Michaela Benz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Deuschel_Multi-Prototype_Few-Shot_Learning_in_Histopathology_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Deuschel_Multi-Prototype_Few-Shot_Learning_in_Histopathology_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The ability to adapt quickly to a new task or data distribution based on only a few examples is a challenge in AI and highly relevant for various domains. In digital pathology, slight variations in the scanning and staining process can lead to a distribution shift that provokes significant performance degradation of classical neural networks for tasks like tissue cartography where a reliable classification is essential. To overcome this problem, we propose a few-shot learning technique, specifically a k-means extension of Prototypical Networks, to train a highly flexible model that adapts to new, unseen scanner data based on only a few examples. We evaluate our approach on a multi-scanner database comprising a total amount of 356 annotated whole slide images digitized by a base scanner for training and additional five different scanners for evaluation. We verify our method's effectiveness by comparing it to a classically trained benchmark and Prototypical Networks, both trained on the same data. A particular focus for us is to investigate the support set, used for adapting the prototypes, to provide recommended actions for digital pathology. The best results are obtained by employing multiple prototypes per class, calculated from a distributed support set, and domain-specific data augmentation. This results in 86.9 - 88.2% accuracy for a classification task of seven tissue classes on unseen, shifted data from the automated scanners, which is almost equal to the accuracy on the in-distribution data of 89.2%.",
    "code_link": ""
  },
  "iccv2021_cdpath_apathologydeeplearningsystemcapableoftriageofmelanomaspecimensutilizingdermatopathologistconsensusasgroundtruth": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "A Pathology Deep Learning System Capable of Triage of Melanoma Specimens Utilizing Dermatopathologist Consensus As Ground Truth",
    "authors": [
      "Sivaramakrishnan Sankarapandian",
      "Saul Kohn",
      "Vaughn Spurrier",
      "Sean Grullon",
      "Rajath E. Soans",
      "Kameswari D. Ayyagari",
      "Ramachandra V. Chamarthi",
      "Kiran Motaparthi",
      "Jason B. Lee",
      "Wonwoo Shon",
      "Michael Bonham",
      "Julianna D. Ianni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Sankarapandian_A_Pathology_Deep_Learning_System_Capable_of_Triage_of_Melanoma_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Sankarapandian_A_Pathology_Deep_Learning_System_Capable_of_Triage_of_Melanoma_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Although melanoma occurs more rarely than several other skin cancers, patients' long term survival rate is extremely low if the diagnosis is missed. Diagnosis is complicated by a high discordance rate among pathologists when distinguishing between melanoma and benign melanocytic lesions. A tool that allows pathology labs to sort and prioritize melanoma cases in their workflow could improve turnaround time by prioritizing challenging cases and routing them directly to the appropriate subspecialist. We present a pathology deep learning system (PDLS) that performs hierarchical classification of digitized whole slide image (WSI) specimens into six classes defined by their morphological characteristics, including classification of \"Melanocytic Suspect\" specimens likely representing melanoma or severe dysplastic nevi . We trained the system on 7,685 images from a single lab (the reference lab), including the the largest set of triple-concordant melanocytic specimens compiled to date, and tested the system on 5,099 images from two distinct validation labs. We achieved Area Underneath the ROC Curve (AUC) values of 0.93 classifying Melanocytic Suspect specimens on the reference lab, 0.95 on the first validation lab, and 0.82 on the second validation lab. We demonstrate that the PDLS is capable of automatically sorting and triaging skin specimens with high sensitivity to Melanocytic Suspect cases and that a pathologist would only need between 30% and 60% of the caseload to address all melanoma specimens.",
    "code_link": ""
  },
  "iccv2021_cdpath_real-timecellcountinginunlabeledmicroscopyimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Real-Time Cell Counting in Unlabeled Microscopy Images",
    "authors": [
      "Yuang Zhu",
      "Zhao Chen",
      "Yuxin Zheng",
      "Qinghua Zhang",
      "Xuan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Zhu_Real-Time_Cell_Counting_in_Unlabeled_Microscopy_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Zhu_Real-Time_Cell_Counting_in_Unlabeled_Microscopy_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep learning is largely applied to cell counting in microscopy images. However, most of the existing cell counting models are fully supervised and trained off-line. They adopt the usual training-testing framework, whereas the models are trained in advance to infer numbers of cells in test images. They require large amounts of manually labeled data for training but lack the ability to adapt to newlycollected unlabeled images that are fed to processing systems dynamically. To solve these problems, we propose a novel framework for real-time (RT) cell counting with density maps (DM). It is a semisupervised system which enables training with upcoming unlabeled images and predicting their cell counts simultaneously. It is also flexible enough to allow almost any cell counting model to be embedded within it. With a reliable and automatic training set renewing mechanism, it ensures counting accuracy while optimizing the models by both historical data and new images. To deal with cell variability and image complexity, we propose a Semisupervised Graph-Based Network (SGN) for within the RT counting framework. It leverages a count-sensitive measurement to construct dynamic graphs of DM patches. With the graph constraint, it regularizes an encoder-decoder to represent underlying data structures and gain robustness for cell counting. We have realized SGN along with several baseline networks and state-of-the-art methods within the RT counting framework. Experimental results validate the effectiveness and robustness of SGN. They also demonstrate the feasibility, efficacy and generalizability of the proposed framework for cell counting in unlabeled images.",
    "code_link": "https://github.com/Yihouyihou/SGN"
  },
  "iccv2021_cdpath_robustinteractivesemanticsegmentationofpathologyimageswithminimaluserinput": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CDPath",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computational Challenges in Digital Pathology",
    "title": "Robust Interactive Semantic Segmentation of Pathology Images With Minimal User Input",
    "authors": [
      "Mostafa Jahanifar",
      "Neda Zamani Tajeddin",
      "Navid Alemi Koohbanani",
      "Nasir M. Rajpoot"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/html/Jahanifar_Robust_Interactive_Semantic_Segmentation_of_Pathology_Images_With_Minimal_User_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Jahanifar_Robust_Interactive_Semantic_Segmentation_of_Pathology_Images_With_Minimal_User_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "From the simple measurement of tissue attributes in pathology workflow to designing an explainable diagnostic/prognostic AI tool, access to accurate semantic segmentation of tissue regions in histology images is a prerequisite. However, delineating different tissue regions manually is a laborious, time-consuming and costly task that requires expert knowledge. On the other hand, the state-of-the-art automatic deep learning models for semantic segmentation require lots of annotated training data and there are only a limited number of tissue region annotated images publicly available. To obviate this issue in computational pathology projects and collect large-scale region annotations efficiently, we propose an efficient interactive segmentation network that requires minimum input from the user to accurately annotate different tissue types in the histology image. The user is only required to draw a simple squiggle inside each region of interest so it will be used as the guiding signal for the model. To deal with the complex appearance and amorph geometry of different tissue regions we introduce several automatic and minimalistic guiding signal generation techniques that help the model to become robust against the variation in the user input. By experimenting on a dataset of breast cancer images, we show that not only does our proposed method speed up the interactive annotation process, it can also outperform the existing automatic and interactive region segmentation models.",
    "code_link": ""
  },
  "iccv2021_luai_getbetter1pixelpckladderscalescorrespondenceflownetworksforremotesensingimagematchinginhigherresolution": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LUAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning To Understand Aerial Images",
    "title": "Get Better 1 Pixel PCK: Ladder Scales Correspondence Flow Networks for Remote Sensing Image Matching in Higher Resolution",
    "authors": [
      "Weitao Chen",
      "Zhibin Wang",
      "Hao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Chen_Get_Better_1_Pixel_PCK_Ladder_Scales_Correspondence_Flow_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Chen_Get_Better_1_Pixel_PCK_Ladder_Scales_Correspondence_Flow_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recently, remote sensing image matching by deep learning reaches competitive performance evaluated by Probability of Correct Keypoints(PCK). Percentage of image size is often used as the threshold of PCK. Even though it can achieve a good 1% PCK in high resolution by regression of transformer parameters,the value will be reduced by using the absolute 1 pixel as threshold in the higher resolution. Inspired by the flow-based methods used in natural image matching tasks, we convert the transformer to correspondence flow and propose ladder scales correspondence flow networks(LSCFN) to get better 1 pixel PCK in higher resolution.Input images are resized to multi scales and then sent to network backbone to generate multi feature pyramids. These pyramids are linked and effectively pull up the highest resolution of original backbone just like a ladder when the global correlation scale is fixed.LSCFN regress correspondence flow in ladder scales by a dense cascade way.We build LSCFN-b and LSCFN-s based on the degree of semantic change between compared images. One with only global correlation is used for the big change, another with global and local correlation is used for the opposite one.The proposed LSCFN achieve state-of-the-art performance evaluated by 1% of image size PCK and absolute 1 pixel PCK on google earth dataset.",
    "code_link": ""
  },
  "iccv2021_luai_doubleheadpredictorbasedfew-shotobjectdetectionforaerialimagery": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LUAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning To Understand Aerial Images",
    "title": "Double Head Predictor Based Few-Shot Object Detection for Aerial Imagery",
    "authors": [
      "Stefan Wolf",
      "Jonas Meier",
      "Lars Sommer",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Wolf_Double_Head_Predictor_Based_Few-Shot_Object_Detection_for_Aerial_Imagery_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Wolf_Double_Head_Predictor_Based_Few-Shot_Object_Detection_for_Aerial_Imagery_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Many applications based on aerial imagery rely on accurate object detection, which requires a high number of annotated training data. However, the number of annotated training data is often limited. In this paper, we propose a novel few-shot detection method for aerial imagery that aims at detecting objects of unseen classes with only a few annotated examples. For this purpose, we extend the Two-Stage Fine-Tuning Approach (TFA), which achieves state-of-the-art results on common benchmark datasets. We propose a novel annotation sampling and pre-processing strategy to yield a better exploitation of base class annotations and a more stable training. We further apply a modified fine-tuning scheme to reduce the number of missed detections. To prevent loss of knowledge learned during the base training, we introduce a novel double head predictor, yielding the best trade-off in detection accuracy between the novel and base classes. Our proposed Double Head Few-Shot Detection (DH-FSDet) method outperforms state-of-the-art baselines on publicly available aerial imagery datasets. Finally, ablation experiments are performed in order to get better insight how few-shot detection in aerial imagery is affected by the selection of base and novel classes. We provide the source code at https://github.com/Jonas-Meier/FrustratinglySimpleFsDet.",
    "code_link": "https://github.com/CAPTAIN-WHU/iSAID"
  },
  "iccv2021_luai_luaichallenge2021onlearningtounderstandaerialimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LUAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning To Understand Aerial Images",
    "title": "LUAI Challenge 2021 on Learning To Understand Aerial Images",
    "authors": [
      "Gui-Song Xia",
      "Jian Ding",
      "Ming Qian",
      "Nan Xue",
      "Jiaming Han",
      "Xiang Bai",
      "Michael Ying Yang",
      "Shengyang Li",
      "Serge Belongie",
      "Jiebo Luo",
      "Mihai Datcu",
      "Marcello Pelillo",
      "Liangpei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Xia_LUAI_Challenge_2021_on_Learning_To_Understand_Aerial_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Xia_LUAI_Challenge_2021_on_Learning_To_Understand_Aerial_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This report summarizes the results of Learning to Understand Aerial Images (LUAI) 2021 challenge held on ICCV'2021, which focuses on object detection and seman tic segmentation in aerial images. Using DOTA-v2.0 [7]and GID-15 [35] datasets, this challenge proposes three tasks for oriented object detection, horizontal object detec-tion, and semantic segmentation of common categories in aerial images. This challenge received a total of 146 registrations on the three tasks. Through the challenge, we hope to draw attention from a wide range of communities and call for more efforts on the problems of learning to understand aerial images.",
    "code_link": "https://github.com/csuhan/ReDet"
  },
  "iccv2021_luai_convolutionalneuralnetworksbasedremotesensingsceneclassificationunderclearandcloudyenvironments": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LUAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning To Understand Aerial Images",
    "title": "Convolutional Neural Networks Based Remote Sensing Scene Classification Under Clear and Cloudy Environments",
    "authors": [
      "Huiming Sun",
      "Yuewei Lin",
      "Qin Zou",
      "Shaoyue Song",
      "Jianwu Fang",
      "Hongkai Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Sun_Convolutional_Neural_Networks_Based_Remote_Sensing_Scene_Classification_Under_Clear_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Sun_Convolutional_Neural_Networks_Based_Remote_Sensing_Scene_Classification_Under_Clear_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Remote sensing (RS) scene classification has wide applications in the environmental monitoring and geological survey. In the real-world applications, the RS scene images taken by the satellite might have two scenarios: clear and cloudy environments. However, most of existing methods did not consider these two environments simultaneously. In this paper, we assume that the global and local features are discriminative in either clear or cloudy environments. Many existing Convolution Neural Networks (CNN) based models have made excellent achievements in the image classification, however they somewhat ignored the global and local features in their network structure. In this paper, we propose a new CNN based network (named GLNet) with the Global Encoder and Local Encoder to extract the discriminative global and local features for the RS scene classification, where the constraints for inter-class dispersion and intra-class compactness are embedded in the GLNet training. The experimental results on two publicized RS scene classification datasets show that the proposed GLNet could achieve better performance based on many existing CNN backbones under both clear and cloudy environments.",
    "code_link": "https://github.com/wuchangsheng951/GLNET"
  },
  "iccv2021_luai_self-supervisedpretrainingandcontrolledaugmentationimproverarewildliferecognitioninuavimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LUAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning To Understand Aerial Images",
    "title": "Self-Supervised Pretraining and Controlled Augmentation Improve Rare Wildlife Recognition in UAV Images",
    "authors": [
      "Xiaochen Zheng",
      "Benjamin Kellenberger",
      "Rui Gong",
      "Irena Hajnsek",
      "Devis Tuia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Zheng_Self-Supervised_Pretraining_and_Controlled_Augmentation_Improve_Rare_Wildlife_Recognition_in_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Zheng_Self-Supervised_Pretraining_and_Controlled_Augmentation_Improve_Rare_Wildlife_Recognition_in_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Automated animal censuses with aerial imagery are a vital ingredient towards wildlife conservation. Recent models are generally based on deep learning and thus require vast amounts of training data. Due to their scarcity and minuscule size, annotating animals in aerial imagery is a highly tedious process. In this project, we present a methodology to reduce the amount of required training data by resorting to self-supervised pretraining. In detail, we examine a combination of recent contrastive learning methodologies like Momentum Contrast (MoCo) and Cross-Level Instance-Group Discrimination (CLD) to condition our model on the aerial images without the requirement for labels. We show that a combination of MoCo, CLD, and geometric augmentations outperforms conventional models pretrained on ImageNet by a large margin. Crucially, our method still yields favorable results even if we reduce the number of training animals to just 10%, at which point our best model scores double the recall of the baseline at similar precision. This effectively allows reducing the number of required annotations to a fraction while still being able to train high-accuracy models in such highly challenging settings.",
    "code_link": ""
  },
  "iccv2021_luai_progressiveunsuperviseddeeptransferlearningforforestmappinginsatelliteimage": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LUAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning To Understand Aerial Images",
    "title": "Progressive Unsupervised Deep Transfer Learning for Forest Mapping in Satellite Image",
    "authors": [
      "Nouman Ahmed",
      "Sudipan Saha",
      "Muhammad Shahzad",
      "Muhammad Moazam Fraz",
      "Xiao Xiang Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Ahmed_Progressive_Unsupervised_Deep_Transfer_Learning_for_Forest_Mapping_in_Satellite_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Ahmed_Progressive_Unsupervised_Deep_Transfer_Learning_for_Forest_Mapping_in_Satellite_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Automated forest mapping is important to understand our forests that play a key role in ecological system. However, efforts towards forest mapping is impeded by difficulty to collect labeled forest images that show large intraclass variation. Recently unsupervised learning has shown promising capability when exploiting limited labeled data. Motivated by this, we propose a progressive unsupervised deep transfer learning method for forest mapping. The proposed method exploits a pre-trained model that is subsequently fine-tuned over the target forest domain. We propose two different fine-tuning mechanism, one works in a totally unsupervised setting by jointly learning the parameters of CNN and the k-means based cluster assignments of the resulting features and the other one works in a semi-supervised setting by exploiting the extracted knearest neighbor based pseudo labels. The proposed progressive scheme is evaluated on publicly available EuroSAT dataset using the relevant base model trained on BigEarthNet labels. The results show that the proposed method greatly improves the forest regions classification accuracy as compared to the unsupervised baseline, nearly approaching the supervised classification approach.",
    "code_link": ""
  },
  "iccv2021_luai_aframeworkforsemi-automaticcollectionoftemporalsatelliteimageryforanalysisofdynamicregions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LUAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning To Understand Aerial Images",
    "title": "A Framework for Semi-Automatic Collection of Temporal Satellite Imagery for Analysis of Dynamic Regions",
    "authors": [
      "Nicholas Kashani Motlagh",
      "Aswathnarayan Radhakrishnan",
      "Jim Davis",
      "Roman Ilin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Motlagh_A_Framework_for_Semi-Automatic_Collection_of_Temporal_Satellite_Imagery_for_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Motlagh_A_Framework_for_Semi-Automatic_Collection_of_Temporal_Satellite_Imagery_for_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Analyzing natural and anthropogenic activities using remote sensing data has become a problem of increasing interest. However, this generally involves tediously labeling extensive imagery, perhaps on a global scale. The lack of a streamlined method to collect and label imagery over time makes it challenging to tackle these problems using popular, supervised deep learning approaches. We address this need by presenting a framework to semi-automatically collect and label dynamic regions in satellite imagery using crowd-sourced OpenStreetMap data and available satellite imagery resources. The generated labels can be quickly verified to ease the burden of full manual labeling. We leverage this framework for the ability to gather image sequences of areas that have label reclassification over time. One possible application of our framework is demonstrated to collect and classify construction vs. non-construction sites. Overall, the proposed framework can be adapted for similar change detection or classification tasks in various remote sensing applications.",
    "code_link": "https://github.com/geopandas/geopandas"
  },
  "iccv2021_lpcv_post-trainingdeepneuralnetworkpruningvialayer-wisecalibration": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Low-Power Computer Vision",
    "title": "Post-Training Deep Neural Network Pruning via Layer-Wise Calibration",
    "authors": [
      "Ivan Lazarevich",
      "Alexander Kozlov",
      "Nikita Malinin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/html/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present a post-training weight pruning method for deep neural networks that achieves accuracy levels tolerable for the production setting and that is sufficiently fast to be run on commodity hardware such as desktop CPUs or edge devices. We propose a data-free extension of the approach for computer vision models based on automatically-generated synthetic fractal images. We obtain state-of-the-art results for data-free neural network pruning, with1.5% top@1 accuracy drop for a ResNet50 on ImageNet at 50% sparsity rate. When using real data, we are able to get a ResNet50 model on ImageNet with 65% sparsity rate in 8-bit precision in a post-training setting with a1% top@1 accuracy drop. We release the code as a part of the OpenVINO(TM) Post-Training Optimization tool.",
    "code_link": ""
  },
  "iccv2021_lpcv_knowledgedistillationforlow-powerobjectdetectionasimpletechniqueanditsextensionsfortrainingcompactmodelsusingunlabeleddata": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Low-Power Computer Vision",
    "title": "Knowledge Distillation for Low-Power Object Detection: A Simple Technique and Its Extensions for Training Compact Models Using Unlabeled Data",
    "authors": [
      "Amin Banitalebi-Dehkordi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/html/Banitalebi-Dehkordi_Knowledge_Distillation_for_Low-Power_Object_Detection_A_Simple_Technique_and_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Banitalebi-Dehkordi_Knowledge_Distillation_for_Low-Power_Object_Detection_A_Simple_Technique_and_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The existing solutions for object detection distillation rely on the availability of both a teacher model and ground-truth labels. We propose a new perspective to relax this constraint. In our framework, a student is first trained with pseudo labels generated by the teacher, and then fine-tuned using labeled data, if any available. Extensive experiments demonstrate improvements over existing object detection distillation algorithms. In addition, decoupling the teacher and ground-truth distillation in this framework provides interesting properties such as: 1) using unlabeled data to further improve the student's performance, 2) combining multiple teacher models of different architectures, even with different object categories, and 3) reducing the need for labeled data (with only 20% of COCO labels, this method achieves the same performance as the model trained on the entire set of labels). Furthermore, a by-product of this approach is the potential usage for domain adaptation. We verify these properties through extensive experiments.",
    "code_link": ""
  },
  "iccv2021_lpcv_fox-nasfast,on-deviceandexplainableneuralarchitecturesearch": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Low-Power Computer Vision",
    "title": "FOX-NAS: Fast, On-Device and Explainable Neural Architecture Search",
    "authors": [
      "Chia-Hsiang Liu",
      "Yu-Shin Han",
      "Yuan-Yao Sung",
      "Yi Lee",
      "Hung-Yueh Chiang",
      "Kai-Chiang Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/html/Liu_FOX-NAS_Fast_On-Device_and_Explainable_Neural_Architecture_Search_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Liu_FOX-NAS_Fast_On-Device_and_Explainable_Neural_Architecture_Search_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Neural architecture search can discover neural networks with good performance, and One-Shot approaches are prevalent. One-Shot approaches typically require a supernet with weight sharing and predictors that predict the performance of architecture. However, the previous methods take much time to generate performance predictors thus are inefficient. To this end, we propose FOX-NAS that consists of fast and explainable predictors based on simulated annealing and multivariate regression. Our method is quantization-friendly and can be efficiently deployed to the edge. The experiments on different hardware show that FOX-NAS models outperform some other popular neural network architectures. For example, FOX-NAS matches MobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on the edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer Vision Challenge (LPCVC), DSP classification track. See all evaluation results at https://lpcv.ai/competitions/2020. Search code and pre-trained models are released at https://github.com/great8nctu/FOX-NAS.",
    "code_link": ""
  },
  "iccv2021_lpcv_exploringthepoweroflightweightyolov4": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Low-Power Computer Vision",
    "title": "Exploring the Power of Lightweight YOLOv4",
    "authors": [
      "Chien-Yao Wang",
      "Hong-Yuan Mark Liao",
      "I-Hau Yeh",
      "Yung-Yu Chuang",
      "Youn-Long Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/html/Wang_Exploring_the_Power_of_Lightweight_YOLOv4_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Wang_Exploring_the_Power_of_Lightweight_YOLOv4_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Research on deep learning has always had two main streams: (1) design a powerful network architecture and train it with existing learning methods to achieve the best results, and (2) design better learning methods so that the existing network architecture can achieve the best capbility after training. In recent years, because mobile device has become popular, the requirement of low power consumption becomes a must. Under the requirement of low power consumption, we hope to design low-cost lightweight networks that can be effectively deployed at the edge, while it must have enough resources to be used and the inference speed must be fast enough. In this work, we set a very ambitious goal of exploring the power of lightweight neural networks. We utilize the analysis of data space, model's representational capacity, and knowledge projection space to construct an automated machine learning pipeline. Through this mechanism, we systematically derive the most suitable knowledge projection space between the data and the model. Our method can indeed automatically find learning strategies suitable for the target model and target application through exploration. Experiment results show that the proposed method can significantly enhance the accuracy of lightweight neural networks for object detection. We directly apply the lightweight model trained by our proposed method to a Jetson Xavier NX embedded module and a Kneron KL720 edge AI SoC as system solutions.",
    "code_link": ""
  },
  "iccv2021_mmvra_themulti-modalvideoreasoningandanalyzingcompetition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MMVRA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Multi-Modal Video Reasoning and Analyzing",
    "title": "The Multi-Modal Video Reasoning and Analyzing Competition",
    "authors": [
      "Haoran Peng",
      "He Huang",
      "Li Xu",
      "Tianjiao Li",
      "Jun Liu",
      "Hossein Rahmani",
      "Qiuhong Ke",
      "Zhicheng Guo",
      "Cong Wu",
      "Rongchang Li",
      "Mang Ye",
      "Jiahao Wang",
      "Jiaxu Zhang",
      "Yuanzhong Liu",
      "Tao He",
      "Fuwei Zhang",
      "Xianbin Liu",
      "Tao Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MMVRA/html/Peng_The_Multi-Modal_Video_Reasoning_and_Analyzing_Competition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MMVRA/papers/Peng_The_Multi-Modal_Video_Reasoning_and_Analyzing_Competition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we introduce the Multi-Modal Video Reasoning and Analyzing Competition (MMVRAC) workshop in conjunction with ICCV 2021. This competition is composed of four different tracks, namely, video question answering, skeleton-based action recognition, fisheye video-based action recognition, and person re-identification, which are based on two datasets: SUTD-TrafficQA and UAV-Human. We summarize the top performing methods submitted by the participants in this competition and show their results achieved in the competition.",
    "code_link": "https://github.com/jayleicn/ClipBERT"
  },
  "iccv2021_fas_adual-streamframeworkfor3dmaskfacepresentationattackdetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "FAS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Chalearn Face Anti-Spoofing",
    "title": "A Dual-Stream Framework for 3D Mask Face Presentation Attack Detection",
    "authors": [
      "Shen Chen",
      "Taiping Yao",
      "Keyue Zhang",
      "Yang Chen",
      "Ke Sun",
      "Shouhong Ding",
      "Jilin Li",
      "Feiyue Huang",
      "Rongrong Ji"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/html/Chen_A_Dual-Stream_Framework_for_3D_Mask_Face_Presentation_Attack_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/papers/Chen_A_Dual-Stream_Framework_for_3D_Mask_Face_Presentation_Attack_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face presentation attack detection (PAD) plays a vital role in face recognition systems. Many previous face anti-spoofing methods mainly focus on the 2D face representation attacks, which however, suffer from great performance degradation when facing high-fidelity 3D mask attacks. To address this issue, we propose a novel dual-stream framework consisting of the vanilla convolution stream and the central difference convolution stream. These two streams complement each other and learn more comprehensive features for 3D mask attacks detection. Moreover, we extend 3D PAD to a multi-classification task that contains real face, plaster attack and transparent attack, and utilize various data augmentations and label smoothing techniques to improve the generalizability on unseen attacks. The proposed method achieved the second place in the Chalearn 3D High-Fidelity Mask Face Presentation Attack Detection Challenge@ICCV2021 with a score of 3.15 (ACER).",
    "code_link": ""
  },
  "iccv2021_fas_onimprovingtemporalconsistencyforonlinefacelivenessdetectionsystem": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "FAS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Chalearn Face Anti-Spoofing",
    "title": "On Improving Temporal Consistency for Online Face Liveness Detection System",
    "authors": [
      "Xiang Xu",
      "Yuanjun Xiong",
      "Wei Xia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/html/Xu_On_Improving_Temporal_Consistency_for_Online_Face_Liveness_Detection_System_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/papers/Xu_On_Improving_Temporal_Consistency_for_Online_Face_Liveness_Detection_System_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we focus on improving the online face liveness detection system to enhance the security of the downstream face recognition system. Most of the existing frame-based methods are suffering from the prediction inconsistency across time. To address the issue, a simple yet effective solution based on temporal consistency is proposed. Specifically, in the training stage, to integrate the temporal consistency constraint, a temporal self-supervision loss and a class consistency loss are proposed in addition to the softmax cross-entropy loss. In the deployment stage, a training-free non-parametric uncertainty estimation module is developed to smooth the predictions adaptively. Beyond the common evaluation approach, a video segment-based evaluation is proposed to accommodate more practical scenarios. Extensive experiments demonstrated that our solution is more robust against several presentation attacks in various scenarios, and significantly outperformed the state-of-the-art on multiple public datasets by at least 40% in terms of ACER.",
    "code_link": ""
  },
  "iccv2021_fas_3dhigh-fidelitymaskfacepresentationattackdetectionchallenge": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "FAS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Chalearn Face Anti-Spoofing",
    "title": "3D High-Fidelity Mask Face Presentation Attack Detection Challenge",
    "authors": [
      "Ajian Liu",
      "Chenxu Zhao",
      "Zitong Yu",
      "Anyang Su",
      "Xing Liu",
      "Zijian Kong",
      "Jun Wan",
      "Sergio Escalera",
      "Hugo Jair Escalante",
      "Zhen Lei",
      "Guodong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/html/Liu_3D_High-Fidelity_Mask_Face_Presentation_Attack_Detection_Challenge_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/papers/Liu_3D_High-Fidelity_Mask_Face_Presentation_Attack_Detection_Challenge_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The threat of 3D mask to face recognition systems is increasing serious, and has been widely concerned by researchers. To facilitate the study of the algorithms, a large-scale High-Fidelity Mask dataset, namely CASIA-SURF HiFiMask (briefly HiFiMask) has been collected. Specifically, it consists of total amount of 54,600 videos which are recorded from 75 subjects with 225 realistic masks under 7 new kinds of sensors. Based on this dataset and Protocol 3 which evaluates both the discrimination and generalization ability of the algorithm under the open set scenarios, we organized a 3D High-Fidelity Mask Face Presentation Attack Detection Challenge to boost the research of 3D mask based attack detection. It attracted more than 200 teams for the development phase with a total of 18 teams qualifying for the final round. All the results were verified and re-ran by the organizing team, and the results were used for the final ranking. This paper presents an overview of the challenge, including the introduction of the dataset used, the definition of the protocol, the calculation of the evaluation criteria, and the summary and publication of the competition results. Finally, we focus on introducing and analyzing the top ranked algorithms, the conclusion summary, and the research ideas for mask attack detection provided by this competition.",
    "code_link": "https://github.com/open-mmlab/mmclassification"
  },
  "iccv2021_fas_singlepatchbased3dhigh-fidelitymaskfaceanti-spoofing": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "FAS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Chalearn Face Anti-Spoofing",
    "title": "Single Patch Based 3D High-Fidelity Mask Face Anti-Spoofing",
    "authors": [
      "Samuel Huang",
      "Wen-Huang Cheng",
      "Robert Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/html/Huang_Single_Patch_Based_3D_High-Fidelity_Mask_Face_Anti-Spoofing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/papers/Huang_Single_Patch_Based_3D_High-Fidelity_Mask_Face_Anti-Spoofing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face anti-spoofing is rapidly increasing in importance as facial recognition systems have become common in the financial and security fields. Among all kinds of attack, 3D high-fidelity masks are especially hard to defend. Recently, CASIA introduced a large scale dataset CASIA-SURF HiFiMask, which comprises of 54,600 videos recorded from 75 subjects with 225 high-fidelity masks. In this paper, we design a lightweight network with single patch input on the basis of CDCN++, and supervise it by focal loss. The proposed method achieves the Average Classification Error Rate (ACER) of 3.215 on the Protocol 3 of CASIA-SURF HiFiMask dataset and ranks the third best model in the Chalearn 3D High-Fidelity Mask Face Presentation Attack Detection Challenge at ICCV 2021.",
    "code_link": ""
  },
  "iccv2021_fas_3dmaskpresentationattackdetectionviahighresolutionfaceparts": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "FAS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Chalearn Face Anti-Spoofing",
    "title": "3D Mask Presentation Attack Detection via High Resolution Face Parts",
    "authors": [
      "Oleg Grinchuk",
      "Aleksandr Parkin",
      "Evgenija Glazistova"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/html/Grinchuk_3D_Mask_Presentation_Attack_Detection_via_High_Resolution_Face_Parts_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ChaLearn_FAS/papers/Grinchuk_3D_Mask_Presentation_Attack_Detection_via_High_Resolution_Face_Parts_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "3D mask presentation attack detection (PAD) is a long standing challenge in face anti-spoofing due to the high fidelity of attack artifacts and a limited number of samples available for training and evaluation. With the recent release of the large-scale and diverse CASIA-SURF HiFiMask dataset, it now becomes possible to address 3D mask PAD with deep neural networks. This paper introduces a new one-shot method for 3D mask PAD that extracts fine-grained information from appropriate parts of the human face and uses it to identify subtle differences between real and fake samples. The proposed method achieves state-of-the-art results of 3% ACER on the CASIA-SURF HiFiMask test set.",
    "code_link": "https://github.com/AlexanderParkin/chalearn_3d_hifi"
  },
  "iccv2021_gsp-cv_unsupervisedlearningofgeometricsamplinginvariantrepresentationsfor3dpointclouds": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "GSP-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - When Graph Signal Processing Meets Computer Vision",
    "title": "Unsupervised Learning of Geometric Sampling Invariant Representations for 3D Point Clouds",
    "authors": [
      "Haolan Chen",
      "Shitong Luo",
      "Xiang Gao",
      "Wei Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/html/Chen_Unsupervised_Learning_of_Geometric_Sampling_Invariant_Representations_for_3D_Point_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/papers/Chen_Unsupervised_Learning_of_Geometric_Sampling_Invariant_Representations_for_3D_Point_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Point clouds consist of a discrete set of points irregularly sampled from continuous 3D objects. Most existing approaches for point cloud learning are in (semi)-supervised fashions, which nevertheless require costly human annotations. To this end, we propose a novel unsupervised learning of geometric sampling invariant representations, aiming to learn intrinsic feature representations of point clouds based on that the geometry of one object can be sampled in various patterns and densities into different forms of point clouds. In particular, we exploit invariant representations at multiple hierarchies: the low-resolution invariance and original-resolution invariance. To learn invariance at a lower resolution, we subsample the input point cloud in distinct patterns, and maximize the mutual information among the subsampled variants. Further, to learn invariance at the original resolution, we increase the resolution of the subsampled point clouds to the original resolution of the input based on the learned features, and minimize the distance between the input and each of the upsampled versions. In experiments, we apply the learned representations to representative downstream tasks of point clouds, and results on point cloud classification, segmentation and upsampling demonstrate the superiority of the proposed model.",
    "code_link": ""
  },
  "iccv2021_gsp-cv_zero-shotlearningviacontrastivelearningondualknowledgegraphs": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "GSP-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - When Graph Signal Processing Meets Computer Vision",
    "title": "Zero-Shot Learning via Contrastive Learning on Dual Knowledge Graphs",
    "authors": [
      "Jin Wang",
      "Bo Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/html/Wang_Zero-Shot_Learning_via_Contrastive_Learning_on_Dual_Knowledge_Graphs_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/papers/Wang_Zero-Shot_Learning_via_Contrastive_Learning_on_Dual_Knowledge_Graphs_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Graph Convolutional Networks (GCNs), which can integrate both explicit knowledge and implicit knowledge together, have shown effectively for zero-shot learning problems. Previous GCN-based methods generally leverage a single category (relationship) knowledge graph for zero-shot learning. However, in practical scenarios, multiple types of relationships among categories are usually available which can be represented as multiple knowledge graphs. To this end, we propose a novel dual knowledge graph contrastive learning framework to perform zero-shot learning. The proposed model fully exploits multiple relationships among different categories for zero-shot learning by employing graph convolutional representation and contrastive learning techniques. The main benefit of the proposed contrastive learning module is that it can effectively encourage the consistency of the category representations from different knowledge graphs while enhancing the discriminability of the generated category classifiers. We perform extensive experiments on several benchmark datasets and the experimental results show the superior performance of our approach.",
    "code_link": ""
  },
  "iccv2021_gsp-cv_parameterizedpseudo-differentialoperatorsforgraphconvolutionalneuralnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "GSP-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - When Graph Signal Processing Meets Computer Vision",
    "title": "Parameterized Pseudo-Differential Operators for Graph Convolutional Neural Networks",
    "authors": [
      "Kevin Potter",
      "Steven Sleder",
      "Matthew Smith",
      "Shehan Perera",
      "Alper Yilmaz",
      "John Tencer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/html/Potter_Parameterized_Pseudo-Differential_Operators_for_Graph_Convolutional_Neural_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/papers/Potter_Parameterized_Pseudo-Differential_Operators_for_Graph_Convolutional_Neural_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present a novel graph convolutional layer that is fast, conceptually simple, and provides high accuracy with reduced overfitting. Based on pseudo-differential operators, our layer operates on graphs with relative position information available for each pair of connected nodes. We evaluate our method on a variety of supervised learning tasks, including 2D graph classification using the MNIST and CIFAR-100 datasets and 3D node correspondence using the FAUST dataset. We also introduce a superpixel graph version of the lesion classification task using the ISIC 2016 challenge dataset and evaluate our layer versus other state-of-the-art graph convolutional network architectures. The new layer outperforms multiple recent architectures on graph classification tasks using the MNIST and CIFAR-100 datasets. When compared to the best published results, the new layer achieves greater than 15% reduction in error rate on the MNIST dataset and greater than 8% reduction in error rate for the CIFAR-100 dataset. For the FAUST node correspondence task, our layer is competitive with other recent results without extensive hyperparameter tuning. For the ISIC dataset, we outperform all other graph neural networks examined as well as all of the submissions to the original ISIC challenge despite the best of those models having more than 200 times as many parameters as our model.",
    "code_link": ""
  },
  "iccv2021_gsp-cv_movingobjectdetectionforevent-basedvisionusinggraphspectralclustering": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "GSP-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - When Graph Signal Processing Meets Computer Vision",
    "title": "Moving Object Detection for Event-Based Vision Using Graph Spectral Clustering",
    "authors": [
      "Anindya Mondal",
      "Shashant R",
      "Jhony H. Giraldo",
      "Thierry Bouwmans",
      "Ananda S. Chowdhury"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/html/Mondal_Moving_Object_Detection_for_Event-Based_Vision_Using_Graph_Spectral_Clustering_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/papers/Mondal_Moving_Object_Detection_for_Event-Based_Vision_Using_Graph_Spectral_Clustering_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Moving object detection has been a central topic of discussion in computer vision for its wide range of applications like in self-driving cars, video surveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are bio-inspired sensors that mimic the working of the human eye. Unlike conventional frame-based cameras, these sensors capture a stream of asynchronous 'events' that pose multiple advantages over the former, like high dynamic range, low latency, low power consumption, and reduced motion blur. However, these advantages come at a high cost, as the event camera data typically contains more noise and has low resolution. Moreover, as event-based cameras can only capture the relative changes in brightness of a scene, event data do not contain usual visual information (like texture and color) as available in video data from normal cameras. So, moving object detection in event-based cameras becomes an extremely challenging task. In this paper, we present an unsupervised Graph Spectral Clustering technique for Moving Object Detection in Event-based data (GSCEventMOD). We additionally show how the optimum number of moving objects can be automatically determined. Experimental comparisons on publicly available datasets show that the proposed GSCEventMOD algorithm outperforms a number of state-of-the-art techniques by a maximum margin of 30%.",
    "code_link": ""
  },
  "iccv2021_gsp-cv_border-seggcnimprovingsemanticsegmentationbyrefiningtheborderoutlineusinggraphconvolutionalnetwork": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "GSP-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - When Graph Signal Processing Meets Computer Vision",
    "title": "Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline Using Graph Convolutional Network",
    "authors": [
      "Naina Dhingra",
      "George Chogovadze",
      "Andreas Kunz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/html/Dhingra_Border-SegGCN_Improving_Semantic_Segmentation_by_Refining_the_Border_Outline_Using_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/papers/Dhingra_Border-SegGCN_Improving_Semantic_Segmentation_by_Refining_the_Border_Outline_Using_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present Border-SegGCN, a novel architecture to improve semantic segmentation by refining the border outline using graph convolutional networks (GCN). The semantic segmentation network such as UNet or DeepLabV3+ is used as a base network to have pre-segmented output. This output is converted into a graphical structure and fed into the GCN to improve the border pixel prediction of the pre-segmented output. We explored and studied the factors such as border thickness, number of edges for a node, and the number of features to be fed into the GCN by performing experiments. We demonstrate the effectiveness of the Border-SegGCN on the CamVid and Carla dataset, achieving a test set performance of 81.96% without any post-processing on CamVid dataset. It is higher than the reported state of the art mIoU achieved on CamVid dataset by 0.404%.",
    "code_link": "https://github.com/qubvel/segmentation"
  },
  "iccv2021_gsp-cv_skeletongraphscatteringnetworksfor3dskeleton-basedhumanmotionprediction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "GSP-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - When Graph Signal Processing Meets Computer Vision",
    "title": "Skeleton Graph Scattering Networks for 3D Skeleton-Based Human Motion Prediction",
    "authors": [
      "Maosen Li",
      "Siheng Chen",
      "Zihui Liu",
      "Zijing Zhang",
      "Lingxi Xie",
      "Qi Tian",
      "Ya Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/html/Li_Skeleton_Graph_Scattering_Networks_for_3D_Skeleton-Based_Human_Motion_Prediction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/GSP-CV/papers/Li_Skeleton_Graph_Scattering_Networks_for_3D_Skeleton-Based_Human_Motion_Prediction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "To achieve 3D skeleton-based human motion prediction, many graph-convolution-based methods are proposed for promising results; however, due to only preserving low-pass information over graphs, those graph convolution methods suffer from over-smoothing, causing the predicted poses staying the same in the long term. To resolve the over-smoothing issue, we propose a novel skeleton graph scattering network (SGSN), which leverages graph scattering to extract comprehensive motion information from multiple graph spectrum bands. The core of the proposed SGSN is the adaptive graph scattering block (AGSB), including two key modules: i) graph scattering decomposition, which decomposes information into various graph spectrum bands and updates the trainable features in each band, as well as ii) graph spectrum attention, which aggregates those features in various graph spectrum bands via trainable attention weights. Extensive experiments reveal that SGSN outperforms state-of-the-art methods by 8.5%, 9.0% and 3.9% of 3D mean per joint position error (MPJPE) in average on Human3.6M, CMU Mocap and 3DPW datasets, respectively. We also test the mean angle error (MAE) on Human3.6M, which is lower by 3.3% than previous methods. Moreover, SGSN outperforms even more in the long-term prediction because of the alleviation of the over-smoothing.",
    "code_link": ""
  },
  "iccv2021_3dodi_monociniscameraindependentmonocular3dobjectdetectionusinginstancesegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "3DODI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - 3D Object Detection From Images",
    "title": "MonoCInIS: Camera Independent Monocular 3D Object Detection Using Instance Segmentation",
    "authors": [
      "Jonas Heylen",
      "Mark De Wolf",
      "Bruno Dawagne",
      "Marc Proesmans",
      "Luc Van Gool",
      "Wim Abbeloos",
      "Hazem Abdelkawy",
      "Daniel Olmeda Reino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/html/Heylen_MonoCInIS_Camera_Independent_Monocular_3D_Object_Detection_Using_Instance_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Heylen_MonoCInIS_Camera_Independent_Monocular_3D_Object_Detection_Using_Instance_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Monocular 3D object detection has recently shown promising results, however there remain challenging problems. One of those is the lack of invariance to different camera intrinsic parameters, which can be observed across different 3D object datasets. Little effort has been made to exploit the combination of heterogeneous 3D object datasets. In contrast to general intuition, we show that more data does not automatically guarantee a better performance, but rather, methods need to have a degree of 'camera independence' in order to benefit from large and heterogeneous training data. In this paper we propose a category-level pose estimation method based on instance segmentation, using camera independent geometric reasoning to cope with the varying camera viewpoints and intrinsics of different datasets. Every pixel of an instance predicts the object dimensions, the 3D object reference points projected in 2D image space and, optionally, the local viewing angle. Camera intrinsics are only used outside of the learned network to lift the predicted 2D reference points to 3D. We surpass camera independent methods on the challenging KITTI3D benchmark and show the key benefits compared to camera dependent methods.",
    "code_link": ""
  },
  "iccv2021_3dodi_bridgingtherealitygapforposeestimationnetworksusingsensor-baseddomainrandomization": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "3DODI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - 3D Object Detection From Images",
    "title": "Bridging the Reality Gap for Pose Estimation Networks Using Sensor-Based Domain Randomization",
    "authors": [
      "Frederik Hagelskj\u00e6r",
      "Anders Glent Buch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/html/Hagelskjaer_Bridging_the_Reality_Gap_for_Pose_Estimation_Networks_Using_Sensor-Based_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Hagelskjaer_Bridging_the_Reality_Gap_for_Pose_Estimation_Networks_Using_Sensor-Based_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Since the introduction of modern deep learning methods for object pose estimation, test accuracy and efficiency has increased significantly. For training, however, large amounts of annotated training data are required for good performance. While the use of synthetic training data prevents the need for manual annotation, there is currently a large performance gap between methods trained on real and synthetic data. This paper introduces a new method, which bridges this gap. Most methods trained on synthetic data use 2D images, as domain randomization in 2D is more developed. To obtain precise poses, many of these methods perform a final refinement using 3D data. Our method integrates the 3D data into the network to increase the accuracy of the pose estimation. To allow for domain randomization in 3D, a sensor-based data augmentation has been developed. Additionally, we introduce the SparseEdge feature, which uses a wider search space during point cloud propagation to avoid relying on specific features without increasing run-time. Experiments on three large pose estimation benchmarks show that the presented method outperforms previous methods trained on synthetic data and achieves comparable results to existing methods trained on real data.",
    "code_link": "https://github.com/matterport/Mask_RCNN"
  },
  "iccv2021_3dodi_fcos3dfullyconvolutionalone-stagemonocular3dobjectdetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "3DODI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - 3D Object Detection From Images",
    "title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection",
    "authors": [
      "Tai Wang",
      "Xinge Zhu",
      "Jiangmiao Pang",
      "Dahua Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/html/Wang_FCOS3D_Fully_Convolutional_One-Stage_Monocular_3D_Object_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Wang_FCOS3D_Fully_Convolutional_One-Stage_Monocular_3D_Object_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Monocular 3D object detection is an important task for autonomous driving considering its advantage of low cost. It is much more challenging than conventional 2D cases due to its inherent ill-posed property, which is mainly reflected in the lack of depth information. Recent progress on 2D detection offers opportunities to better solving this problem. However, it is non-trivial to make a general adapted 2D detector work in this 3D task. In this paper, we study this problem with a practice built on a fully convolutional single-stage detector and propose a general framework FCOS3D. Specifically, we first transform the commonly defined 7-DoF 3D targets to the image domain and decouple them as 2D and 3D attributes. Then the objects are distributed to different feature levels with consideration of their 2D scales and assigned only according to the projected 3D-center for the training procedure. Furthermore, the center-ness is redefined with a 2D Gaussian distribution based on the 3D-center to fit the 3D target formulation. All of these make this framework simple yet effective, getting rid of any 2D detection or 2D-3D correspondence priors. Our solution achieves 1st place out of all the vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020. Code and models are released at https://github.com/open-mmlab/mmdetection3d.",
    "code_link": "https://github.com/open-mmlab/mmdetection3d"
  },
  "iccv2021_ercvad_deploymentofdeepneuralnetworksforobjectdetectiononedgeaideviceswithruntimeoptimization": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "Deployment of Deep Neural Networks for Object Detection on Edge AI Devices With Runtime Optimization",
    "authors": [
      "Lukas St\u00e4cker",
      "Juncong Fei",
      "Philipp Heidenreich",
      "Frank Bonarens",
      "Jason Rambach",
      "Didier Stricker",
      "Christoph Stiller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Stacker_Deployment_of_Deep_Neural_Networks_for_Object_Detection_on_Edge_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Stacker_Deployment_of_Deep_Neural_Networks_for_Object_Detection_on_Edge_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep neural networks have proven increasingly important for automotive scene understanding with new algorithms offering constant improvements of the detection performance. However, there is little emphasis on experiences and needs for deployment in embedded environments. We therefore perform a case study of the deployment of two representative object detection networks on an edge AI platform. In particular, we consider RetinaNet for image-based 2D object detection and PointPillars for LiDAR-based 3D object detection. We describe the modifications necessary to convert the algorithms from a PyTorch training environment to the deployment environment taking into account the available tools. We evaluate the runtime of the deployed DNN using two different libraries, TensorRT and TorchScript. In our experiments, we observe slight advantages of TensorRT for convolutional layers and TorchScript for fully connected layers. We also study the trade-off between runtime and performance, when selecting an optimized setup for deployment, and observe that quantization significantly reduces the runtime while having only little impact on the detection performance.",
    "code_link": ""
  },
  "iccv2021_ercvad_descriptionofcornercasesinautomateddrivinggoalsandchallenges": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "Description of Corner Cases in Automated Driving: Goals and Challenges",
    "authors": [
      "Daniel Bogdoll",
      "Jasmin Breitenstein",
      "Florian Heidecker",
      "Maarten Bieshaar",
      "Bernhard Sick",
      "Tim Fingscheidt",
      "Marius Z\u00f6llner"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Bogdoll_Description_of_Corner_Cases_in_Automated_Driving_Goals_and_Challenges_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Bogdoll_Description_of_Corner_Cases_in_Automated_Driving_Goals_and_Challenges_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Scaling the distribution of automated vehicles requires handling various unexpected and possibly dangerous situations, termed corner cases (CC). Since many modules of automated driving systems are based on machine learning (ML), CC are an essential part of the data for their development. However, there is only a limited amount of CC data in large-scale data collections, which makes them challenging in the context of ML. With a better understanding of CC, offline applications, e.g., dataset analysis, and online methods, e.g., improved performance of automated driving systems, can be improved. While there are knowledge-based descriptions and taxonomies for CC, there is little research on machine-interpretable descriptions. In this extended abstract, we will give a brief overview of the challenges and goals of such a description.",
    "code_link": ""
  },
  "iccv2021_ercvad_semanticconcepttestinginautonomousdrivingbyextractionofobject-levelannotationsfromcarla": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "Semantic Concept Testing in Autonomous Driving by Extraction of Object-Level Annotations From CARLA",
    "authors": [
      "Sujan Gannamaneni",
      "Sebastian Houben",
      "Maram Akila"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Gannamaneni_Semantic_Concept_Testing_in_Autonomous_Driving_by_Extraction_of_Object-Level_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Gannamaneni_Semantic_Concept_Testing_in_Autonomous_Driving_by_Extraction_of_Object-Level_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With the growing use of Deep Neural Networks (DNNs) in various safety-critical applications comes an increasing need for Verification and Validation (V&V) of these DNNs. Unlike testing in software engineering, where several established methods exist for V&V, DNN testing is still at an early stage. The data-driven nature of DNNs adds to the complexity of testing them. In the scope of autonomous driving, we showcase our validation method by leveraging object-level annotations (object metadata) to test DNNs on a more granular level using human-understandable semantic concepts like gender, shirt colour, age, and illumination. Such an enhanced granularity, as we detail, can prove useful in the construction of closed-loop testing or the investigation of dataset coverage/completeness. Our add-on sensor to the CARLA simulator enables us to generate datasets with this granular metadata. For the task of semantic segmentation for pedestrian detection using DeepLabv3+, we highlight potential insights and challenges that become apparent on this level of granularity. For instance, imbalances within a CARLA generated dataset w.r.t. the pedestrian distribution do not directly carry over into weak spots of the DNN performances and vice versa.",
    "code_link": ""
  },
  "iccv2021_ercvad_abouttheambiguityofdataaugmentationfor3dobjectdetectioninautonomousdriving": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "About the Ambiguity of Data Augmentation for 3D Object Detection in Autonomous Driving",
    "authors": [
      "Matthias Reuse",
      "Martin Simon",
      "Bernhard Sick"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Reuse_About_the_Ambiguity_of_Data_Augmentation_for_3D_Object_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Reuse_About_the_Ambiguity_of_Data_Augmentation_for_3D_Object_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Although data augmentation is considered an important step in the training strategy of 3D object detectors on point clouds to increase the overall performance and robustness, in almost all publications the topic of augmentation and the choice of the individual augmentation methods used are only addressed very briefly with reference to previous work and are not backed up with sufficient experiments. The question therefore arises as to the impact and the transferability of different augmentation policies. Through a series of elaborate experiments with four networks on two datasets, this paper shows that the positive effects of different data augmentation methods are not so clear-cut and instead depend strongly on the network architecture and the dataset.",
    "code_link": "https://github.com/open-mmlab/OpenPCDet"
  },
  "iccv2021_ercvad_instancesegmentationincarlamethodologyandanalysisforpedestrian-orientedsyntheticdatagenerationincrowdedscenes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "Instance Segmentation in CARLA: Methodology and Analysis for Pedestrian-Oriented Synthetic Data Generation in Crowded Scenes",
    "authors": [
      "Maria Lyssenko",
      "Christoph Gladisch",
      "Christian Heinzemann",
      "Matthias Woehrle",
      "Rudolph Triebel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Lyssenko_Instance_Segmentation_in_CARLA_Methodology_and_Analysis_for_Pedestrian-Oriented_Synthetic_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Lyssenko_Instance_Segmentation_in_CARLA_Methodology_and_Analysis_for_Pedestrian-Oriented_Synthetic_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The evaluation of camera-based perception functions in automated driving (AD) is a significant challenge and requires large-scale high-quality datasets. Recently proposed metrics for safety evaluation additionally require detailed per-instance annotations of dynamic properties such as distance and velocities that may not be available in openly accessible AD datasets. Synthetic data from 3D simulators like CARLA may provide a solution to this problem as labeled data can be produced in a structured manner. However, CARLA currently lacks instance segmentation ground truth. In this paper, we present a back projection pipeline that allows us to obtain accurate instance segmentation maps for CARLA, which is necessary for precise per-instance ground truth information. Our evaluation results show that per-pedestrian depth aggregation obtained from our instance segmentation is more precise than previously available approximations based on bounding boxes especially in the context of crowded scenes in urban automated driving.",
    "code_link": "https://github.com/carla-simulator/carla"
  },
  "iccv2021_ercvad_visualdomainadaptationformonoculardepthestimationonresource-constrainedhardware": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware",
    "authors": [
      "Julia Hornauer",
      "Lazaros Nalpantidis",
      "Vasileios Belagiannis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Hornauer_Visual_Domain_Adaptation_for_Monocular_Depth_Estimation_on_Resource-Constrained_Hardware_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Hornauer_Visual_Domain_Adaptation_for_Monocular_Depth_Estimation_on_Resource-Constrained_Hardware_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Real-world perception systems in many cases build on hardware with limited resources to adhere to cost and power limitations of their carrying system. Deploying deep neural networks on resource-constrained hardware became possible with model compression techniques, as well as efficient and hardware-aware architecture design. However, model adaptation is additionally required due to the diverse operation environments. In this work, we address the problem of training deep neural networks on resource-constrained hardware in the context of visual domain adaptation. We select the task of monocular depth estimation where our goal is to transform a pre-trained model to the target's domain data. While the source domain includes labels, we assume an unlabelled target domain, as it happens in real-world applications. Then, we present an adversarial learning approach that is adapted for training on the device with limited resources. Since visual domain adaptation, i.e. neural network training, has not been previously explored for resource-constrained hardware, we present the first feasibility study for image-based depth estimation. Our experiments show that visual domain adaptation is relevant only for efficient network architectures and training sets at the order of a few hundred samples. Models and code are publicly available.",
    "code_link": "https://github.com/jhornauer/embedded_domain"
  },
  "iccv2021_ercvad_perf4sightatoolflowtomodelcnntrainingperformanceonedgegpus": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "perf4sight: A Toolflow To Model CNN Training Performance on Edge GPUs",
    "authors": [
      "Aditya Rajagopal",
      "Christos-Savvas Bouganis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Rajagopal_perf4sight_A_Toolflow_To_Model_CNN_Training_Performance_on_Edge_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Rajagopal_perf4sight_A_Toolflow_To_Model_CNN_Training_Performance_on_Edge_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The increased memory and processing capabilities of today's edge devices create opportunities for greater edge intelligence. In the domain of vision, the ability to adapt a Convolutional Neural Network's (CNN) structure and parameters to the input data distribution leads to systems with lower memory footprint, latency and power consumption. However, due to the limited compute resources and memory budget on edge devices, it is necessary for the system to be able to predict the latency and memory footprint of the training process in order to identify favourable training configurations of the network topology and device combination for efficient network adaptation. This work proposes perf4sight, an automated methodology for developing accurate models that predict CNN training memory footprint and latency given a target device and network. This enables rapid identification of network topologies that can be retrained on the edge device with low resource consumption. With PyTorch as the framework and NVIDIA Jetson TX2 as the target device, the developed models predict training memory footprint and latency with 95% and 91% accuracy respectively for a wide range of networks, opening the path towards efficient network adaptation on edge GPUs.",
    "code_link": "https://github.com/ICIdsl/performance"
  },
  "iccv2021_ercvad_boostinginstancesegmentationwithsyntheticdataastudytoovercomethelimitsofrealworlddatasets": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "Boosting Instance Segmentation With Synthetic Data: A Study To Overcome the Limits of Real World Data Sets",
    "authors": [
      "Florentin Poucin",
      "Andrea Kraus",
      "Martin Simon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Poucin_Boosting_Instance_Segmentation_With_Synthetic_Data_A_Study_To_Overcome_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Poucin_Boosting_Instance_Segmentation_With_Synthetic_Data_A_Study_To_Overcome_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A major issue related to computer vision for the automotive industry is that real-world perception models require huge amount of well-annotated data to achieve decent performance. While this data is very expensive to collect and annotate, synthetically generated images seem to be an efficient alternative to solve this problem. More and more public data sets, composed of synthetic data, are available in various domains, however, there is too little concrete methodology to use them properly. In this paper, we propose a simple approach combining the use of synthetic and real images to boost instance segmentation. We mention some pre-processing requirements as harmonizing instance labeling and removing non-valuable instances from synthetic images. We present our training strategy based on data set mixing, and show that it overcomes the domain shift between real and synthetic data sets. A comparison study with other training approaches, such as fine-tuning techniques, highlights the benefits of our method, which boosts network performances on both real and synthetic image inferences.",
    "code_link": ""
  },
  "iccv2021_ercvad_mealmanifoldembedding-basedactivelearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "MEAL: Manifold Embedding-Based Active Learning",
    "authors": [
      "Deepthi Sreenivasaiah",
      "Johannes Otterbach",
      "Thomas Wollmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Sreenivasaiah_MEAL_Manifold_Embedding-Based_Active_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Sreenivasaiah_MEAL_Manifold_Embedding-Based_Active_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Image segmentation is a common and challenging task in autonomous driving. Availability of sufficient pixel-level annotations for the training data is a hurdle. Active learning helps learning from small amounts of data by suggesting the most promising samples for labeling. In this work, we propose a new pool-based method for active learning, which proposes promising patches extracted from full image, in each acquisition step. The problem is framed in an exploration-exploitation framework by combining an embedding based on Uniform Manifold Approximation to model representativeness with entropy as uncertainty measure to model informativeness. We applied our proposed method to the autonomous driving datasets CamVid and Cityscapes and performed a quantitative comparison with state-of-the-art baselines. We find that our active learning method achieves better performance compared to previous methods.",
    "code_link": ""
  },
  "iccv2021_ercvad_proaianefficientembeddedaihardwareforautomotiveapplications-abenchmarkstudy": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "ProAI: An Efficient Embedded AI Hardware for Automotive Applications - A Benchmark Study",
    "authors": [
      "Sven Mantowsky",
      "Falk Heuer",
      "Saqib Bukhari",
      "Michael Keckeisen",
      "Georg Schneider"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Mantowsky_ProAI_An_Efficient_Embedded_AI_Hardware_for_Automotive_Applications_-_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Mantowsky_ProAI_An_Efficient_Embedded_AI_Hardware_for_Automotive_Applications_-_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Development in the field of Single Board Computers (SBC) have been increasing for several years. They provide a good balance between computing performance and power consumption which is usually required for mobile platforms, like application in vehicles for Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD). However, there is an ever-increasing need of more powerful and efficient SBCs which can run power intensive Deep Neural Networks (DNNs) in real-time and can also satisfy necessary functional safety requirements such as Automotive Safety Integrity Level (ASIL). \"ProAI\" is being developed by ZF mainly to run powerful and efficient applications such as multitask DNNs and on top of that it also has the required safety certification for AD. In this work, we compare and discuss state of the art SBC on the basis of power intensive multitask DNN architecture called Multitask-CenterNet with respect to performance measures such as, FPS and power efficiency. \"ProAI\" is being developed by ZF mainly to run powerful and efficient applications such as multitask DNNs and on top of that it also has the required safety certification for AD. As an automotive supercomputer, ProAI delivers an excellent combination of performance and efficiency, managing nearly twice the number of FPS per watt than a modern workstation laptop and almost four times compared to the Jetson Nano. Furthermore, it was also shown that there is still power in reserve for further and more complex tasks on the ProAI, based on the CPU/GPU utilization during the benchmark.",
    "code_link": ""
  },
  "iccv2021_ercvad_multitask-centernet(mcn)efficientanddiversemultitasklearningusingananchorfreeapproach": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ERCVAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Embedded and Real-World Computer Vision in Autonomous Driving",
    "title": "MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning Using an Anchor Free Approach",
    "authors": [
      "Falk Heuer",
      "Sven Mantowsky",
      "Saqib Bukhari",
      "Georg Schneider"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/html/Heuer_MultiTask-CenterNet_MCN_Efficient_and_Diverse_Multitask_Learning_Using_an_Anchor_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ERCVAD/papers/Heuer_MultiTask-CenterNet_MCN_Efficient_and_Diverse_Multitask_Learning_Using_an_Anchor_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Multitask learning is a common approach in machine learning, which allows to train multiple objectives with a shared architecture. It has been shown that by training multiple tasks together inference time and compute resources can be saved, while the objectives performance remains on a similar or even higher level. However, in perception related multitask networks only closely related tasks can be found, such as object detection, instance and semantic segmentation or depth estimation. Multitask networks with diverse tasks and their effects with respect to efficiency on one another are not well studied. In this paper we augment the CenterNet anchor-free approach for training multiple diverse perception related tasks together, including the task of object detection and semantic segmentation as well as human pose estimation. We refer to this DNN as Multitask-CenterNet (MCN). Additionally, we study different MCN settings for efficiency. The MCN can perform several tasks at once while maintaining, and in some cases even exceeding, the performance values of its corresponding single task networks. More importantly, the MCN architecture decreases inference time and reduces network size when compared to a composition of single task networks.",
    "code_link": ""
  },
  "iccv2021_vipriors_lsd-clinearlyseparabledeepclusters": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "LSD-C: Linearly Separable Deep Clusters",
    "authors": [
      "Sylvestre-Alvise Rebuffi",
      "Sebastien Ehrhardt",
      "Kai Han",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Rebuffi_LSD-C_Linearly_Separable_Deep_Clusters_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Rebuffi_LSD-C_Linearly_Separable_Deep_Clusters_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present LSD-C, a novel method to identify clusters in an unlabeled dataset. Our algorithm first establishes pairwise connections in the feature space between the samples of the minibatch based on a similarity metric. Then it regroups in clusters the connected samples and enforces a linear separation between clusters. This is achieved by using the pairwise connections as targets together with a binary cross-entropy loss on the predictions that the associated pairs of samples belong to the same cluster. This way, the feature representation of the network will evolve such that similar samples in this feature space will belong to the same linearly separated cluster. Our method draws inspiration from recent semi-supervised learning practice and proposes to combine our clustering algorithm with self-supervised pretraining and strong data augmentation. We show that our approach significantly outperforms competitors on popular public image benchmarks including CIFAR 10/100, STL 10 and MNIST, as well as the document classification dataset Reuters 10K. Our code is available at https://github.com/srebuffi/lsd-clusters.",
    "code_link": ""
  },
  "iccv2021_vipriors_relationalpriorformulti-objecttracking": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "Relational Prior for Multi-Object Tracking",
    "authors": [
      "Artem Moskalev",
      "Ivan Sosnovik",
      "Arnold Smeulders"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Moskalev_Relational_Prior_for_Multi-Object_Tracking_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Moskalev_Relational_Prior_for_Multi-Object_Tracking_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Tracking multiple objects individually differs from tracking groups of related objects. When an object is a part of the group, its trajectory is conditioned on the trajectories of the other group members. Most of the current state-of-the-art trackers follow the approach of tracking each object independently, with the mechanism to handle the overlapping trajectories where necessary. Such an approach does not take inter-object relations into account, which may cause unreliable tracking for the members of the groups, especially in crowded scenarios, where individual cues become unreliable. To overcome these limitations, we propose a plug-in Relation Encoding Module (REM). REM encodes relations between tracked objects by running a message passing over a spatio-temporal graph of tracked instances, computing the relation embeddings. The relation embeddings then serve as a prior for predicting future positions of the objects. Our experiments on MOT17 and MOT20 benchmarks demonstrate that extending a tracker with relational prior improves tracking quality.",
    "code_link": ""
  },
  "iccv2021_vipriors_self-supervisedvisualattributelearningforfashioncompatibility": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "Self-Supervised Visual Attribute Learning for Fashion Compatibility",
    "authors": [
      "Donghyun Kim",
      "Kuniaki Saito",
      "Samarth Mishra",
      "Stan Sclaroff",
      "Kate Saenko",
      "Bryan A. Plummer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Kim_Self-Supervised_Visual_Attribute_Learning_for_Fashion_Compatibility_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Kim_Self-Supervised_Visual_Attribute_Learning_for_Fashion_Compatibility_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Many self-supervised learning (SSL) methods have been successful in learning semantically meaningful visual representations by solving pretext tasks. However, prior work in SSL focuses on tasks like object recognition or detection, which aim to learn object shapes and assume that the features should be invariant to concepts like colors and textures. Thus, these SSL methods perform poorly on downstream tasks where these concepts provide critical information. In this paper, we present an SSL framework that enables us to learn color and texture-aware features without requiring any labels during training. Our approach consists of three self-supervised tasks designed to capture different concepts that are neglected in prior work that we can select from depending on the needs of our downstream tasks. Our tasks include learning to predict color histograms and discriminate shapeless local patches and textures from each instance. We evaluate our approach on fashion compatibility using Polyvore Outfits and In-Shop Clothing Retrieval using Deepfashion, improving upon prior SSL methods by 9.5-16%, and even outperforming some supervised approaches on Polyvore Outfits despite using no labels. We also show that our approach can be used for transfer learning, demonstrating that we can train on one dataset while achieving high performance on a different dataset.",
    "code_link": ""
  },
  "iccv2021_vipriors_few-shotlearningwithonlineself-distillation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "Few-Shot Learning With Online Self-Distillation",
    "authors": [
      "Sihan Liu",
      "Yue Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Liu_Few-Shot_Learning_With_Online_Self-Distillation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Liu_Few-Shot_Learning_With_Online_Self-Distillation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Few-shot learning has been a long-standing problem in learning to learn. This problem typically involves training a model on an extremely small amount of data and testing the model on the out-of-distribution data. The focus of recent few-shot learning research has been on the development of good representation models that can quickly adapt to test tasks. To that end, we come up with a model that learns representation through online self-distillation. Our model combines supervised training with knowledge distillation via a continuously updated teacher. We also identify that data augmentation plays an important role in producing robust features. Our final model is trained with CutMix augmentation and online self-distillation. On the commonly used benchmark miniImageNet, our model achieves 67.07% and 83.03% under the 5-way 1-shot setting and the 5-way 5-shot setting, respectively. It outperforms counterparts of its kind by 2.25% and 0.89%.",
    "code_link": ""
  },
  "iccv2021_vipriors_scatsimclrself-supervisedcontrastivelearningwithpretexttaskregularizationforsmall-scaledatasets": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "ScatSimCLR: Self-Supervised Contrastive Learning With Pretext Task Regularization for Small-Scale Datasets",
    "authors": [
      "Vitaliy Kinakh",
      "Olga Taran",
      "Svyatoslav Voloshynovskiy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Kinakh_ScatSimCLR_Self-Supervised_Contrastive_Learning_With_Pretext_Task_Regularization_for_Small-Scale_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Kinakh_ScatSimCLR_Self-Supervised_Contrastive_Learning_With_Pretext_Task_Regularization_for_Small-Scale_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we consider a problem of self-supervised learning for small-scale datasets based on contrastive loss between multiple views of the data, which demonstrates the state-of-the-art performance in classification task. Despite the reported results, such factors as the complexity of training requiring complex architectures, the needed number of views produced by data augmentation, and their impact on the classification accuracy are understudied problems. To establish the role of these factors, we consider an architecture of contrastive loss system such as SimCLR, where baseline model is replaced by geometrically invariant \"hard-crafted\" network ScatNet with small trainable adapter network and argue that the number of parameters of the whole system and the number of views can be considerably reduced while practically preserving the same classification accuracy. In addition, we investigate the impact of regularization strategies using pretext task learning based on an estimation of parameters of augmentation transform such as rotation and jigsaw permutation for both traditional baseline models and ScatNet based models. Finally, we demonstrate that the proposed architecture with pretext task learning regularization achieves the state-of-the-art classification performance with a smaller number of trainable parameters and with reduced number of views.",
    "code_link": "https://github.com/vkinakh/scatsimclr"
  },
  "iccv2021_vipriors_deepmanifoldprior": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "Deep Manifold Prior",
    "authors": [
      "Matheus Gadelha",
      "Rui Wang",
      "Subhransu Maji"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Gadelha_Deep_Manifold_Prior_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Gadelha_Deep_Manifold_Prior_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present a prior for manifold structured data, such as surfaces of 3D shapes, where deep neural networks are adopted to reconstruct a target shape using gradient descent starting from a random initialization. We show that surfaces generated this way are smooth, with limiting behavior characterized by Gaussian processes, and we mathematically derive such properties for fully-connected as well as convolutional networks. We demonstrate our method in a variety of manifold reconstruction applications, such as point cloud denoising and interpolation, achieving considerably better results against competitive baselines while requiring no training data. We also show that when training data is available, our method allows developing alternate parametrizations of surfaces under the framework of AtlasNet, leading to a compact network architecture and better reconstruction results on standard image to shape reconstruction benchmarks.",
    "code_link": ""
  },
  "iccv2021_vipriors_multimodalcontinuousvisualattentionmechanisms": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "Multimodal Continuous Visual Attention Mechanisms",
    "authors": [
      "Ant\u00f3nio Farinhas",
      "Andr\u00e9 F. T. Martins",
      "Pedro M. Q. Aguiar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Farinhas_Multimodal_Continuous_Visual_Attention_Mechanisms_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Farinhas_Multimodal_Continuous_Visual_Attention_Mechanisms_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Visual attention mechanisms are a key component of neural network models for computer vision. By focusing on a discrete set of objects or image regions, these mechanisms identify the most relevant features and use them to build more powerful representations. Recently, continuous-domain alternatives to discrete attention models have been proposed, which exploit the continuity of images. These approaches model attention as simple unimodal densities (e.g. a Gaussian), making them less suitable to deal with images whose region of interest has a complex shape or is composed of multiple non-contiguous patches. In this paper, we introduce a new continuous attention mechanism that produces multimodal densities, in the form of mixtures of Gaussians. We use the EM algorithm to obtain a clustering of relevant regions in the image, and a description length penalty to select the number of components in the mixture. Our densities decompose as a linear combination of unimodal attention mechanisms, enabling closed-form Jacobians for the backpropagation step. Experiments on visual question answering in the VQA-v2 dataset show competitive accuracies and a selection of regions that mimics human attention more closely in VQA-HAT, substantiating the impact of the structure induced by our visual prior. We present several examples that suggest how multimodal attention maps are naturally more interpretable than their unimodal counterparts, showing the ability of our model to automatically segregate objects from ground in complex scenes.",
    "code_link": "https://github.com/MILVLG/mcan-vqa"
  },
  "iccv2021_vipriors_howtotransformkernelsforscale-convolutions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "How To Transform Kernels for Scale-Convolutions",
    "authors": [
      "Ivan Sosnovik",
      "Artem Moskalev",
      "Arnold Smeulders"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Sosnovik_How_To_Transform_Kernels_for_Scale-Convolutions_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Sosnovik_How_To_Transform_Kernels_for_Scale-Convolutions_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Scale is often seen as a given, disturbing factor in many vision tasks. When doing so it is one of the factors why we need more data during learning. In recent work scale equivariance was added to convolutional neural networks. It was shown to be effective for a range of tasks. We aim for accurate scale-equivariant convolutional neural networks (SE-CNNs) applicable for problems where high granularity of scale and small kernel sizes are required. Current SE-CNNs rely on weight sharing and kernel rescaling, the latter of which is accurate for integer scales only. To reach accurate scale equivariance, we derive general constraints under which scale-convolution remains equivariant to discrete rescaling. We find the exact solution for all cases where it exists, and compute the approximation for the rest. The discrete scale-convolution pays off, as demonstrated in a new state-of-the-art classification on MNIST-scale and on STL-10 in the supervised learning setting.",
    "code_link": ""
  },
  "iccv2021_vipriors_tuneitordontuseitbenchmarkingdata-efficientimageclassification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "Tune It or Don't Use It: Benchmarking Data-Efficient Image Classification",
    "authors": [
      "Lorenzo Brigato",
      "Bj\u00f6rn Barz",
      "Luca Iocchi",
      "Joachim Denzler"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Brigato_Tune_It_or_Dont_Use_It_Benchmarking_Data-Efficient_Image_Classification_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Brigato_Tune_It_or_Dont_Use_It_Benchmarking_Data-Efficient_Image_Classification_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Data-efficient image classification using deep neural networks in settings, where only small amounts of labeled data are available, has been an active research area in the recent past. However, an objective comparison between published methods is difficult, since existing works use different datasets for evaluation and often compare against untuned baselines with default hyper-parameters. We design a benchmark for data-efficient image classification consisting of six diverse datasets spanning various domains (e.g., natural images, medical imagery, satellite data) and data types (RGB, grayscale, multispectral). Using this benchmark, we re-evaluate the standard cross-entropy baseline and eight methods for data-efficient deep learning published between 2017 and 2021 at renowned venues. For a fair and realistic comparison, we carefully tune the hyper-parameters of all methods on each dataset. Surprisingly, we find that tuning learning rate, weight decay, and batch size on a separate validation split results in a highly competitive baseline, which outperforms all but one specialized method and performs competitively to the remaining one.",
    "code_link": "https://github.com/cvjena/deic"
  },
  "iccv2021_vipriors_predictivecodingwithtopographicvariationalautoencoders": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VIPriors",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Inductive Priors for Data-Efficient Deep Learning",
    "title": "Predictive Coding With Topographic Variational Autoencoders",
    "authors": [
      "T. Anderson Keller",
      "Max Welling"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/html/Keller_Predictive_Coding_With_Topographic_Variational_Autoencoders_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Keller_Predictive_Coding_With_Topographic_Variational_Autoencoders_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Predictive coding is a model of visual processing which suggests that the brain is a generative model of input, with prediction error serving as a signal for both learning and attention. In this work, we show how the equivariant capsules learned by a Topographic Variational Autoencoder can be extended to fit within the predictive coding framework by treating the slow rolling of capsule activations as the forward prediction operator. We demonstrate quantitatively that such an extension leads to improved sequence modeling compared with both topographic and non-topographic baselines, and that the resulting forward predictions are qualitatively more coherent with the provided partial input transformations.",
    "code_link": "https://github.com/akandykeller/PCTVAE"
  },
  "iccv2021_pbdl_multi-leveladaptiveseparableconvolutionforlarge-motionvideoframeinterpolation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Multi-Level Adaptive Separable Convolution for Large-Motion Video Frame Interpolation",
    "authors": [
      "Ruth Wijma",
      "Shaodi You",
      "Yu Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Wijma_Multi-Level_Adaptive_Separable_Convolution_for_Large-Motion_Video_Frame_Interpolation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Wijma_Multi-Level_Adaptive_Separable_Convolution_for_Large-Motion_Video_Frame_Interpolation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Current state-of-the-art methods within Video Frame In-terpolation (VI) fail at synthesizing interpolated frames incertain problem areas, such as when the video containslarge motion. This work aims at improving performanceon frame sequences containing large displacements by ex-tending the Adaptive Separable Convolution model in twoways. First of all, we increase the receptive field of themodel by utilizing spatial pyramids, which efficiently in-crease the interpolation kernel size. We additionally adaptthe network to accommodate for four frames, as opposedto just two, which should give it the ability to learn morecomplex motion patterns. This work also introduces theLarge-Motion Video Interpolation Dataset (LMD), whichcontains extracted frames from videos containing large dis-placements and highly non-linear movements. Our analy-sis shows that applying the model changes, together withthe use of our new dataset, does indeed result in improvedperformance on large displacement videos. We also showthat the increase in performance generalizes to frame se-quences of all sorts by outperforming other models in ourbenchmark on most tasks, and almost setting the new state-of-the-art on the Vimeo-90K dataset.",
    "code_link": "https://github.com/rwq/MSC_AI_Thesis_FI"
  },
  "iccv2021_pbdl_preciseforecastingofskyimagesusingspatialwarping": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Precise Forecasting of Sky Images Using Spatial Warping",
    "authors": [
      "Leron Julian",
      "Aswin C. Sankaranarayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Julian_Precise_Forecasting_of_Sky_Images_Using_Spatial_Warping_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Julian_Precise_Forecasting_of_Sky_Images_Using_Spatial_Warping_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The intermittency of solar power, due to occlusion from cloud cover, is one of the key factors inhibiting its widespread use in both commercial and residential settings. Hence, real-time forecasting of solar irradiance for grid-connected photovoltaic systems is necessary to schedule and allocate resources across the grid. Ground-based imagers that capture wide field-of-view images of the sky are commonly used to monitor cloud movement around a particular site in an effort to forecast solar irradiance. However, these wide FOV imagers capture a distorted image of sky image, where regions near the horizon are heavily compressed. This hinders the ability to precisely predict cloud motion near the horizon which especially affects prediction over longer time horizons. In this work, we combat the aforementioned constraint by introducing a deep learning method to predict a future sky image frame with higher resolution than previous methods. Our main contribution is to derive an optimal warping method to counter the adverse affects of clouds at the horizon, and learn a framework for future sky image prediction which better determines cloud evolution for longer time horizons.",
    "code_link": ""
  },
  "iccv2021_pbdl_delieve-netdeblurringlow-lightimageswithlightstreaksandlocalevents": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "DeLiEve-Net: Deblurring Low-Light Images With Light Streaks and Local Events",
    "authors": [
      "Chu Zhou",
      "Minggui Teng",
      "Jin Han",
      "Chao Xu",
      "Boxin Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Zhou_DeLiEve-Net_Deblurring_Low-Light_Images_With_Light_Streaks_and_Local_Events_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Zhou_DeLiEve-Net_Deblurring_Low-Light_Images_With_Light_Streaks_and_Local_Events_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Modern blind deblurring methods usually show degenerate performance when handling images captured in low-light conditions because these images often contain saturated regions of light sources, and the image contents and details in dark regions are poorly visible. In contrast, event cameras can faithfully record the positions and polarities of intensity changes with a very high dynamic range and low latency, which suffer less in the dark than conventional cameras. However, existing event-based deblurring methods require guidance from global events with the same spatial resolution as the blurry image (typically 346 * 260 pixels), which significantly limits the spatial resolution of images they can process. In this paper, we address this problem in a two-stage way by proposing a neural network named DeLiEve-Net, which learns to Deblur low-Light images with light streaks and local Events. An RGB-DAVIS hybrid camera system is built to validate that our method can deblur high-resolution RGB images with events in low-light conditions.",
    "code_link": ""
  },
  "iccv2021_pbdl_hypermixnethyperspectralimagereconstructionwithdeepmixednetworkfromasnapshotmeasurement": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "HyperMixNet: Hyperspectral Image Reconstruction With Deep Mixed Network From a Snapshot Measurement",
    "authors": [
      "Kouhei Yorimoto",
      "Xian-Hua Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Yorimoto_HyperMixNet_Hyperspectral_Image_Reconstruction_With_Deep_Mixed_Network_From_a_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Yorimoto_HyperMixNet_Hyperspectral_Image_Reconstruction_With_Deep_Mixed_Network_From_a_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Many hyperspectral imaging systems resort to computational photography technique for capturing spectral information of the dynamic world in recent decades of years. Therein, Coded aperture snapshot spectral imaging encodes the 3D hyperspectral image as a 2D compressive image (snapshot) and then employs an inverse optimization algorithm embedded in the imaging system to reconstruct the underlying HSI. This study proposes a novel HyperMixNet to reconstruct an underlying HSI from the single snapshot image. Specifically, to reduce the size of the reconstruction model for being handy embedded in the real imaging system, we integrate the MixConv block instead of the conventional convolutional layers in our proposed HyperMixNet, which can not only greatly decrease the network parameter amount but also learn multi-level context for more representative feature extraction. Simultaneously, we employ a mixed spatial and spectral convolutional module to effectively learn the spatial structure and spectral attribute for more robust HSI reconstruction. We further design a mixed loss function for network training, which incorporates not only spatial fidelity but also spectral fidelity aiming at recovering the hyperspectral signature with small spectral distortion. Experimental results on three benchmark HSI datasets validate that our proposed method outperforms the state-of-the-art methods in quantitative values, visual effect, and reconstruction model scale.",
    "code_link": ""
  },
  "iccv2021_pbdl_weakly-supervisedsemanticsegmentationincityscapeviahyperspectralimage": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Weakly-Supervised Semantic Segmentation in Cityscape via Hyperspectral Image",
    "authors": [
      "Yuxing Huang",
      "Qiu Shen",
      "Ying Fu",
      "Shaodi You"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Huang_Weakly-Supervised_Semantic_Segmentation_in_Cityscape_via_Hyperspectral_Image_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Huang_Weakly-Supervised_Semantic_Segmentation_in_Cityscape_via_Hyperspectral_Image_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Hyperspectral images (HSIs) contain the response of each pixel in different spectral bands, which can be used to effectively distinguish various objects in complex scenes. While HSI cameras have become low cost, algorithms based on it have not been well exploited. In this paper, we focus on a novel topic, weakly-supervised semantic segmentation in cityscape via HSIs. It is based on the idea that high-resolution HSIs in city scenes contain rich spectral information, which can be easily associated to semantics without manual labeling. Therefore, it enables low cost, highly reliable semantic segmentation in complex scenes. Specifically, in this paper, we theoretically analyze the HSIs and introduce a weakly-supervised HSI semantic segmentation framework, which utilizes spectral information to improve the coarse labels to a finer degree. The experimental results show that our method can obtain highly competitive labels and even have higher edge fineness than artificial fine labels in some classes. At the same time, the results also show that the refined labels can effectively improve the performance of existing semantic segmentation algorithms. The combination of HSIs and semantic segmentation proves that HSIs have great potential in high-level visual tasks for automatic driving.",
    "code_link": ""
  },
  "iccv2021_pbdl_deepsinglefisheyeimagecameracalibrationforover180-degreeprojectionoffieldofview": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Deep Single Fisheye Image Camera Calibration for Over 180-Degree Projection of Field of View",
    "authors": [
      "Nobuhiko Wakai",
      "Takayoshi Yamashita"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Wakai_Deep_Single_Fisheye_Image_Camera_Calibration_for_Over_180-Degree_Projection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Wakai_Deep_Single_Fisheye_Image_Camera_Calibration_for_Over_180-Degree_Projection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a learning-based calibration method for trigonometric function models that represent distortion with over 180-degree projection of field of views. Unlike perspective projection for less than 180-degree projection of field of views, fisheye projection such as equisolid angle projection is valid for whole world coordinates. To calibrate fisheye camera models, we define a new loss function based on camera projection effectively to optimize fisheye camera extrinsic (tilt and roll angles) and intrinsic (focal length) parameters. Our loss achieves small prediction errors throughout the ranges of parameters. Our results show that our method predicts precise fisheye camera parameters compared with conventional polynomial function models for radial distortion. This work is the first to calibrate a fisheye camera model including extrinsic and intrinsic parameters for over 180-degree projection of field of views from a single image to our knowledge.",
    "code_link": ""
  },
  "iccv2021_pbdl_generativemodelsformulti-illuminationcolorconstancy": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Generative Models for Multi-Illumination Color Constancy",
    "authors": [
      "Partha Das",
      "Yang Liu",
      "Sezer Karaoglu",
      "Theo Gevers"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Das_Generative_Models_for_Multi-Illumination_Color_Constancy_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Das_Generative_Models_for_Multi-Illumination_Color_Constancy_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, the aim is multi-illumination color constancy. However, most of the existing color constancy methods are designed for single light sources. Furthermore, datasets for learning multiple illumination color constancy are largely missing. We propose a seed (physics driven) based multi-illumination color constancy method. GANs are exploited to model the illumination estimation problem as an image-to-image domain translation problem. Additionally, a novel multi-illumination data augmentation method is proposed. Experiments on single and multi-illumination datasets show that our methods outperform sota methods.",
    "code_link": ""
  },
  "iccv2021_pbdl_efficientlighttransportacquisitionbycodedilluminationandrobustphotometricstereobydualphotographyusingdeepneuralnetwork": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Efficient Light Transport Acquisition by Coded Illumination and Robust Photometric Stereo by Dual Photography Using Deep Neural Network",
    "authors": [
      "Takafumi Iwaguchi",
      "Hiroshi Kawasaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Iwaguchi_Efficient_Light_Transport_Acquisition_by_Coded_Illumination_and_Robust_Photometric_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Iwaguchi_Efficient_Light_Transport_Acquisition_by_Coded_Illumination_and_Robust_Photometric_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Light transport is fundamental information to describe both photometric and geometric information of a scene, however, a costly process is required for its acquisition. In this paper, to reduce the sampling time without losing SNR, we propose a technique to use a diffuser as well as a video projector, which projects special patterns designed by deep neural network (DNN). By using the light transport, the scene lit by an arbitrary lighting condition can be synthesized, which will be utilized for various purposes. Among them, photometric stereo (PS) is one important application, which requires multiple images captured under different lighting positions. Although simple PS algorithm cannot be applied to complicated BRDF, we propose a robust PS achieved by using dual photography, which can recover the shape of complicated BRDF, i.e., glitter surfaces of objects. In the experiment, comprehensive evaluations of LT acquisition as well as surface normal estimation using simulation data.",
    "code_link": ""
  },
  "iccv2021_pbdl_enforcingtemporalconsistencyinvideodepthestimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Enforcing Temporal Consistency in Video Depth Estimation",
    "authors": [
      "Siyuan Li",
      "Yue Luo",
      "Ye Zhu",
      "Xun Zhao",
      "Yu Li",
      "Ying Shan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/html/Li_Enforcing_Temporal_Consistency_in_Video_Depth_Estimation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Li_Enforcing_Temporal_Consistency_in_Video_Depth_Estimation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Most existing monocular depth estimation methods are trained on single images and have unsatisfactory temporal stability in video prediction. They may rely on post processing to solve this issue. A few video based depth estimation methods use reconstruction framework like structure-from-motion or sequential modeling. These methods have assumptions in the scenarios that they can apply thus limits their real applications. In this work, we present a simple method for improving temporal consistency in video depth estimation. Specifically, we learn a prior from video data and this prior can be imposed directly into any single image monocular depth method. During testing, our method just performs end-to-end forward inference frame by frame without any sequential module or multi-frame module. In the mean while, we propose an evaluation metric that quantitatively measures temporal consistency of video depth predictions. It does not require labelled depth ground truths and only assesses flickering between consecutive frames. Experiments show our method can achieve improved temporal consistency in both standard benchmark and general cases without any post processing and extra computational cost. A subjective study indicates that our proposed metric is consistent with the visual perception of users, and our results with higher consistency scores are indeed preferred. These features make our method a practical video depth estimator to predict dense depth of real scenes and enable several video depth based applications.",
    "code_link": ""
  },
  "iccv2021_antiuav_unmannedaerialvehiclevisualdetectionandtrackingusingdeepneuralnetworksaperformancebenchmark": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AntiUAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Catch UAVs That Want To Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild and Anti-UAV Challenge",
    "title": "Unmanned Aerial Vehicle Visual Detection and Tracking Using Deep Neural Networks: A Performance Benchmark",
    "authors": [
      "Brian K. S. Isaac-Medina",
      "Matt Poyser",
      "Daniel Organisciak",
      "Chris G. Willcocks",
      "Toby P. Breckon",
      "Hubert P. H. Shum"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/html/Isaac-Medina_Unmanned_Aerial_Vehicle_Visual_Detection_and_Tracking_Using_Deep_Neural_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/papers/Isaac-Medina_Unmanned_Aerial_Vehicle_Visual_Detection_and_Tracking_Using_Deep_Neural_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due to both negligent and malicious use. For this reason, the automated detection and tracking of UAV is a fundamental task in aerial security systems. Common technologies for UAV detection include visible-band and thermal infrared imaging, radio frequency and radar. Recent advances in deep neural networks (DNNs) for image-based object detection open the possibility to use visual information for this detection and tracking task. Furthermore, these detection architectures can be implemented as backbones for visual tracking systems, thereby enabling persistent tracking of UAV incursions. To date, no comprehensive performance benchmark exists that applies DNNs to visible-band imagery for UAV detection and tracking. To this end, three datasets with varied environmental conditions for UAV detection and tracking, comprising a total of 241 videos (331,486 images), are assessed using four detection architectures and three tracking frameworks. The best performing detector architecture obtains an mAP of 98.6% and the best performing tracking framework obtains a MOTA of 98.7%. Cross-modality evaluation is carried out between visible and infrared spectrums, achieving a maximal 82.8% mAP on visible images when training in the infrared modality. These results provide the first public multi-approach benchmark for state-of-the-art deep learning-based methods and give insight into which detection and tracking architectures are effective in the UAV domain.",
    "code_link": ""
  },
  "iccv2021_antiuav_semi-automaticannotationforvisualobjecttracking": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AntiUAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Catch UAVs That Want To Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild and Anti-UAV Challenge",
    "title": "Semi-Automatic Annotation for Visual Object Tracking",
    "authors": [
      "Kutalmis Gokalp Ince",
      "Aybora Koksal",
      "Arda Fazla",
      "A. Aydin Alatan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/html/Ince_Semi-Automatic_Annotation_for_Visual_Object_Tracking_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/papers/Ince_Semi-Automatic_Annotation_for_Visual_Object_Tracking_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a semi-automatic bounding box annotation method for visual object tracking by utilizing temporal information with a tracking-by-detection approach. For detection, we use an off-the-shelf object detector which is trained iteratively with the annotations generated by the proposed method, and we perform object detection on each frame independently. We employ Multiple Hypothesis Tracking (MHT) to exploit temporal information and to reduce the number of false-positives which makes it possible to use lower objectness thresholds for detection to increase recall. The tracklets formed by MHT are evaluated by human operators to enlarge the training set. This novel incremental learning approach helps to perform annotation iteratively. The experiments performed on AUTH Multidrone Dataset reveal that the annotation workload can be reduced up to 96% by the proposed approach.",
    "code_link": ""
  },
  "iccv2021_antiuav_areal-timeanti-distractorinfrareduavtrackerwithchannelfeaturerefinementmodule": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AntiUAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Catch UAVs That Want To Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild and Anti-UAV Challenge",
    "title": "A Real-Time Anti-Distractor Infrared UAV Tracker With Channel Feature Refinement Module",
    "authors": [
      "Houzhang Fang",
      "Xiaolin Wang",
      "Zikai Liao",
      "Yi Chang",
      "Luxin Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/html/Fang_A_Real-Time_Anti-Distractor_Infrared_UAV_Tracker_With_Channel_Feature_Refinement_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/papers/Fang_A_Real-Time_Anti-Distractor_Infrared_UAV_Tracker_With_Channel_Feature_Refinement_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The unmanned aerial vehicles (UAVs) have been widely used in various application fields, yet unauthorized use of UAVs raises great threats for restricted areas and public security. Therefore, it is urgently necessary to develop a practical anti-UAV target tracking technique. In this paper, we propose a real-time anti-distractor infrared UAV tracker for infrared anti-UAV tasks, which employs a global real-time perception mechanism to find candidate targets, then utilizes spatial-temporal information to obtain the real UAV target. Moreover, we integrate a channel feature refinement module into multi-scale feature fusion to better enhance the representation of the finer features of the UAV targets channel-wisely, thus improving the tracking performance. We test the performance of the proposed method and the other competitive ones on the constructed UAV dataset from ourselves, and eventually verify the validity of the proposed method as the best performing method with a better balance between tracking accuracy and speed.",
    "code_link": ""
  },
  "iccv2021_antiuav_aunifiedapproachfortrackinguavsininfrared.": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AntiUAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Catch UAVs That Want To Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild and Anti-UAV Challenge",
    "title": "A Unified Approach for Tracking UAVs in Infrared.",
    "authors": [
      "Jinjian Zhao",
      "Xiaohan Zhang",
      "Pengyu Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/html/Zhao_A_Unified_Approach_for_Tracking_UAVs_in_Infrared._ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/papers/Zhao_A_Unified_Approach_for_Tracking_UAVs_in_Infrared._ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With complex camera and object movement, the tracked object often suffers camera motion, out of view, dramatic scale variation, etc., which severely influence tracking performance. Due to the fast speed and tiny size of unmanned aerial vehicles(UAV), it is crucial to design a robust framework for tracking UAVs. This paper carefully designs a unified framework, including a local tracker, camera motion estimation module, bounding box refinement module,re-detection module, and model updater. The camera motion estimation module achieves motion compensation for the local tracker. Then, the bounding box refinement module aims to measure an accurate bounding box. If the target is missing, we switch to the re-detection module to relocalize the target when it reappears. We also adopt a model updater to control the updating process and filter out unreliable samples. Numerous experimental results on 9 visual/thermal datasets show the effectiveness and generalization of our framework.",
    "code_link": "https://github.com/visionml/pytracking"
  },
  "iccv2021_antiuav_siamstaspatio-temporalattentionbasedsiamesetrackerfortrackinguavs": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AntiUAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Catch UAVs That Want To Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild and Anti-UAV Challenge",
    "title": "SiamSTA: Spatio-Temporal Attention Based Siamese Tracker for Tracking UAVs",
    "authors": [
      "Bo Huang",
      "Junjie Chen",
      "Tingfa Xu",
      "Ying Wang",
      "Shenwang Jiang",
      "Yuncheng Wang",
      "Lei Wang",
      "Jianan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/html/Huang_SiamSTA_Spatio-Temporal_Attention_Based_Siamese_Tracker_for_Tracking_UAVs_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AntiUAV/papers/Huang_SiamSTA_Spatio-Temporal_Attention_Based_Siamese_Tracker_for_Tracking_UAVs_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With the growing threat of unmanned aerial vehicle (UAV) intrusion, anti-UAV techniques are becoming increasingly demanding. Object tracking, especially in thermal infrared (TIR) videos, though provides a promising solution, struggles with challenges like small scale and fast movement that commonly occur in anti-UAV scenarios. To mitigate this, we propose a simple yet effective spatio-temporal attention based Siamese network, dubbed SiamSTA, to track UAV robustly by performing reliable local tracking and wide-range re-detection alternatively. Concretely, tracking is carried out by posing spatial and temporal constraints on generating candidate proposals within local neighborhoods, hence eliminating background distractors to better perceive small targets. Complementarily, in case of target lost from local regions due to fast movement, a three-stage re-detection mechanism is introduced to re-detect targets from a global view by exploiting valuable motion cues through a correlation filter based on change detection. Finally, a state-aware switching policy is adopted to adaptively integrate local tracking and global re-detection and take their complementary strengths for robust tracking. Extensive experiments on the 1st and 2nd anti-UAV datasets well demonstrate the superiority of SiamSTA over other competing counterparts. Notably, SiamSTA is the foundation of the 1st-place winning entry in the 2nd Anti-UAV Challenge.",
    "code_link": ""
  },
  "iccv2021_cvppa_enlisting3dcropmodelsandgansformoredataefficientandgeneralizablefruitdetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Enlisting 3D Crop Models and GANs for More Data Efficient and Generalizable Fruit Detection",
    "authors": [
      "Zhenghao Fei",
      "Alexander G. Olenskyj",
      "Brian N. Bailey",
      "Mason Earles"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Fei_Enlisting_3D_Crop_Models_and_GANs_for_More_Data_Efficient_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Fei_Enlisting_3D_Crop_Models_and_GANs_for_More_Data_Efficient_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Training real-world neural network models to achieve high performance and generalizability typically requires a substantial amount of labeled data, spanning a broad range of variation. This data-labeling process can be both labor and cost intensive. To achieve desirable predictive performance, a trained model is typically applied into a domain where the data distribution is similar to the training dataset. However, for many agricultural machine learning problems, training datasets are collected at a specific location, during a specific period in time of the growing season. As a result, since agricultural systems exhibit substantial variability in terms of crop type, cultivar, management, seasonal growth dynamics, lighting condition, sensor type, etc, and a model trained from one dataset often does not generalize across domains. To enable more data efficient and generalizable neural network models in agriculture, we propose a method that generates photorealistic agricultural images from a synthetic 3D crop model domain into realworld crop domains. The method uses a semantically constrained GAN (Generative adversarial network) network to preserve the fruit position and geometry. We observe that a baseline CycleGAN method can generate visually realistic target domain images but does not preserve fruit position information while our method maintains fruit positions well. Image generation results in vineyard grape day and night images show the visual outputs of our network are much better compared to a baseline network. Incremental training experiments in vineyard grape detection tasks show that the images generated from our method can significantly speed the domain adaption process, increase performance for a given number of labeled images (i.e. data efficiency), and decrease the labeling needs.",
    "code_link": ""
  },
  "iccv2021_cvppa_semi-superviseddryherbagemassestimationusingautomaticdataandsyntheticimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Semi-Supervised Dry Herbage Mass Estimation Using Automatic Data and Synthetic Images",
    "authors": [
      "Paul Albert",
      "Mohamed Saadeldin",
      "Badri Narayanan",
      "Brian Mac Namee",
      "Deirdre Hennessy",
      "Aisling O'Connor",
      "Noel O'Connor",
      "Kevin McGuinness"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Albert_Semi-Supervised_Dry_Herbage_Mass_Estimation_Using_Automatic_Data_and_Synthetic_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Albert_Semi-Supervised_Dry_Herbage_Mass_Estimation_Using_Automatic_Data_and_Synthetic_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Monitoring species-specific dry herbage biomass is an important aspect of pasture-based milk production systems. Being aware of the herbage biomass in the field enables farmers to manage surpluses and deficits in herbage supply, as well as using targeted nitrogen fertilization when necessary. Deep learning for computer vision is a powerful tool in this context as it can accurately estimate the dry biomass of a herbage parcel using images of the grass canopy taken using a portable device. However, the performance of deep learning comes at the cost of an extensive, and in this case destructive, data gathering process. Since accurate species-specific biomass estimation is labor intensive and destructive for the herbage parcel, we propose in this paper to study low supervision approaches to dry biomass estimation using computer vision. Our contributions include: a synthetic data generation algorithm to generate data for a herbage height aware semantic segmentation task, an automatic process to label data using semantic segmentation maps, and a robust regression network trained to predict dry biomass using approximate biomass labels and a small trusted dataset with gold standard labels. We design our approach on a herbage mass estimation dataset collected in Ireland and also report state-of-the-art results on the publicly released Grass-Clover biomass estimation dataset from Denmark. Our code is available at https://git.io/J0L2a.",
    "code_link": ""
  },
  "iccv2021_cvppa_visualizingfeaturemapsformodelselectioninconvolutionalneuralnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Visualizing Feature Maps for Model Selection in Convolutional Neural Networks",
    "authors": [
      "Sakib Mostafa",
      "Debajyoti Mondal",
      "Michael Beck",
      "Christopher Bidinosti",
      "Christopher Henry",
      "Ian Stavness"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Mostafa_Visualizing_Feature_Maps_for_Model_Selection_in_Convolutional_Neural_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Mostafa_Visualizing_Feature_Maps_for_Model_Selection_in_Convolutional_Neural_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Convolutional neural networks (CNN) are increasingly being used to achieve state-of-the-art performance for various plant phenotyping and agricultural tasks. While constructing such CNN models, a common problem is over-parameterization, which may lead to a model becoming overfit on a training dataset. This problem is particularly relevant for plant datasets with limited variation and/or small samples sizes. Inspection of the loss and accuracy curves is a common way to detect overfitting in a CNN model, but it provides little insight into how the model could be improved. There are several reasons contributing to the overfitting of a CNN model; however, in this paper, we aim at explaining overfitting in a CNN classification model by analyzing the features learned at various depths of the model. We use three plant phenotyping datasets in our experimental studies. Our comparative analysis between the visualizations of the feature maps obtained from overfit and balanced models reveals that the image background often influences an overfit model's behavior. Researchers with limited deep learning domain knowledge often attempt to build deeper layer models with the hope of improving performance. Using Guided Backpropagation, we show how the pairwise similarity matrix between the visualization of the features learned at different depths can be leveraged to pave a new way to potentially select a better CNN model by removing redundant layers.",
    "code_link": "https://github.com/SakibMostafa/CVPPA_PID"
  },
  "iccv2021_cvppa_asemi-self-supervisedlearningapproachforwheatheaddetectionusingextremelysmallnumberoflabeledsamples": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "A Semi-Self-Supervised Learning Approach for Wheat Head Detection Using Extremely Small Number of Labeled Samples",
    "authors": [
      "Keyhan Najafian",
      "Alireza Ghanbari",
      "Ian Stavness",
      "Lingling Jin",
      "Gholam Hassan Shirdel",
      "Farhad Maleki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Najafian_A_Semi-Self-Supervised_Learning_Approach_for_Wheat_Head_Detection_Using_Extremely_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Najafian_A_Semi-Self-Supervised_Learning_Approach_for_Wheat_Head_Detection_Using_Extremely_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Most of the success of deep learning is owed to supervised learning, where a large-scale annotated dataset is used for model training. However, developing such datasets is challenging. In this paper, we develop a semi-self-supervised learning approach for wheat head detection. The proposed method utilized a few short video clips and only one annotated image from each video clip of wheat fields to simulate a large computationally annotated dataset used for model building. Considering the domain gap between the simulated and real images, we applied two domain adaptation steps to alleviate the challenge of distributional shift. The resulting model achieved high performance when applied to real unannotated datasets. When fine-tuned on the dataset from the Global Wheat Head Detection Challenge, the performance was further improved. The model achieved a mean average precision of 0.827, where an overlap of 50% or more between a predicted bounding box and ground truth was considered as a correct prediction. Although the utility of the proposed methodology was shown by applying it to wheat head detection, the proposed method is not limited to this application and could be used for other domains, such as detecting different crop types, alleviating the barrier of lack of large-scale annotated datasets in those domains.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "iccv2021_cvppa_leafmasktowardsgreateraccuracyonleafsegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "LeafMask: Towards Greater Accuracy on Leaf Segmentation",
    "authors": [
      "Ruohao Guo",
      "Liao Qu",
      "Dantong Niu",
      "Zhenbo Li",
      "Jun Yue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Guo_LeafMask_Towards_Greater_Accuracy_on_Leaf_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Guo_LeafMask_Towards_Greater_Accuracy_on_Leaf_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Leaf segmentation is the most direct and effective way for high-throughput plant phenotype data analysis and quantitative researches of complex traits. Currently, the primary goal of plant phenotyping is to raise the accuracy of the autonomous phenotypic measurement. In this work, we present the LeafMask neural network, a new end-to-end model to delineate each leaf region and count the number of leaves, with two main components: 1) the mask assembly module merging position-sensitive bases of each predicted box after non-maximum suppression (NMS) and corresponding coefficients to generate original masks; 2) the mask refining module elaborating leaf boundaries from the mask assembly module by the point selection strategy and predictor. In addition, we also design a novel and flexible multi-scale attention module for the dual attention-guided mask (DAG-Mask) branch to effectively enhance information expression and produce more accurate bases. Our main contribution is to generate the final improved masks by combining the mask assembly module with the mask refining module under the anchor-free instance segmentation paradigm. We validate our LeafMask through extensive experiments on Leaf Segmentation Challenge (LSC) dataset. Our proposed model achieves the 90.09% BestDice score outperforming other state-of-the-art approaches.",
    "code_link": ""
  },
  "iccv2021_cvppa_leafareaestimationbysemanticsegmentationofpointcloudoftomatoplants": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Leaf Area Estimation by Semantic Segmentation of Point Cloud of Tomato Plants",
    "authors": [
      "Takeshi Masuda"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Masuda_Leaf_Area_Estimation_by_Semantic_Segmentation_of_Point_Cloud_of_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Masuda_Leaf_Area_Estimation_by_Semantic_Segmentation_of_Point_Cloud_of_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Growth monitoring is an essential task in agriculture for obtaining good crops and sustainable management of cultivation. Though it is essential, it is also a hard task requiring much labor and working time, and many automation approaches have been proposed. We present an attempt to estimate the leaf area of the tomatoes grown in a sunlight-type plant factory. We scanned tomato plants by an RGB-D sensor that moves vertically to scan one side of the plants from the pathway. We built a point cloud by merging the scanned data, and we segmented it into four classes (Stem, Leaf, Fruit, and Other) based on annotation. With a limited amount of data, we estimated the stem from Stem points, and from the number of Leaf points around the stem, we estimate the leaf area of a specific tomato plant in a plant factory with the relative error of about 20%.",
    "code_link": "https://github.com/yanx27/Pointnet"
  },
  "iccv2021_cvppa_dynamiccolortransformforwheatheaddetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Dynamic Color Transform for Wheat Head Detection",
    "authors": [
      "Chengxin Liu",
      "Kewei Wang",
      "Hao Lu",
      "Zhiguo Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Liu_Dynamic_Color_Transform_for_Wheat_Head_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Liu_Dynamic_Color_Transform_for_Wheat_Head_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Developing accurate algorithms for wheat head detection is challenging due to the variability of observation circumstances and the uncertainty of wheat head appearances. In this work, we propose a simple but effective idea-dynamic color transform (DCT)-for accurate wheat head detection. This idea is based on an observation that modifying the color channel of an input image can significantly alleviate false negatives and therefore improve detection results. DCT follows a linear color transform and can be easily implemented as a dynamic network. A key property of DCT is that the transform parameters are data-dependent such that illumination variations can be corrected adaptively. The DCT network can be incorporated into any existing object detectors. For example, DCT plays an important role in our solution participating in the Global Wheat Head Detection (GWHD) Challenge 2021, where our solution ranks the first on the initial public leaderboard, with an Average Domain Accuracy (ADA) of 0.821, and obtains the runner-up reward on the final complete testing set, with an ADA of 0.695.",
    "code_link": ""
  },
  "iccv2021_cvppa_identificationandmeasurementofindividualrootsinminirhizotronimagesofdenserootsystems": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Identification and Measurement of Individual Roots in Minirhizotron Images of Dense Root Systems",
    "authors": [
      "Alexander Gillert",
      "Bo Peters",
      "Uwe Freiherr von Lukas",
      "J\u00fcrgen Kreyling"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Gillert_Identification_and_Measurement_of_Individual_Roots_in_Minirhizotron_Images_of_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Gillert_Identification_and_Measurement_of_Individual_Roots_in_Minirhizotron_Images_of_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Semantic segmentation networks are prone to oversegmentation in areas where objects are tightly clustered. In minirhizotron images with densely packed plant root systems this can lead to a failure to separate individual roots, thereby skewing the root length and width measurements. We propose to deal with this problem by adding additional output heads to the segmentation model, one of which is used with a ridge detection algorithm as an intermediate step and a second one that directly estimates root width. With this method we are able to improve detection and width measurements in densely packed roots systems without negative effects on sparse root systems.",
    "code_link": ""
  },
  "iccv2021_cvppa_wheatnet-liteanovellightweightnetworkforwheatheaddetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "WheatNet-Lite: A Novel Light Weight Network for Wheat Head Detection",
    "authors": [
      "Sandesh Bhagat",
      "Manesh Kokare",
      "Vineet Haswani",
      "Praful Hambarde",
      "Ravi Kamble"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Bhagat_WheatNet-Lite_A_Novel_Light_Weight_Network_for_Wheat_Head_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Bhagat_WheatNet-Lite_A_Novel_Light_Weight_Network_for_Wheat_Head_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recently, the potential for wheat head detection has been significantly enhanced using deep learning techniques. However, the significant challenges are variation in growth stages of wheat heads, canopy, genotype, and wheat head orientation. Furthermore, the wheat head detection task gets even more complex due to the overlapping density of wheat heads and the blur image due to the wind. For real-time wheat head detection, designing lightweight deep learning models for edge devices is also challenging. This paper proposes a lightweight WheatNet-Lite architecture to enhance the efficiency and accuracy of wheat head detection. The proposed method utilizes Mixed Depthwise Conv (MDWConv) with an inverted residual bottleneck in the backbone. Additionally, the Modified Spatial Pyramidal Polling (MSPP) effectively extracts the multi-scale features. The final wheat head bounding box prediction is achieved using WheatNet-lite Neck by utilizing Depthwise Convolution (DWConv) with a Feature Pyramid structure. It reduces 54.2 M network parameters in comparison to YOLOV3. The proposed approach outperforms the existing state-of-the-art methods with mean average precision (mAP) of 91.32 mAP@0.5 and 86.10 mAP@0.5 on GWHD and SPIKE datasets, respectively, with only 8.2 M parameters. Also, the new ACID dataset is proposed with bounding box annotation with 76.32 mAP@0.5. The experimental results are demonstrated on three different datasets viz. Global Wheat Head Detection (GWHD), SPIKE dataset, and Annotated Crop Image Dataset (ACID) showing a significant improvement in the wheat head detection with speed and accuracy.",
    "code_link": ""
  },
  "iccv2021_cvppa_multi-domainfew-shotlearninganddatasetforagriculturalapplications": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Multi-Domain Few-Shot Learning and Dataset for Agricultural Applications",
    "authors": [
      "Sai Vidyaranya Nuthalapati",
      "Anirudh Tunga"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Nuthalapati_Multi-Domain_Few-Shot_Learning_and_Dataset_for_Agricultural_Applications_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Nuthalapati_Multi-Domain_Few-Shot_Learning_and_Dataset_for_Agricultural_Applications_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Automatic classification of pests and plants (both healthy and diseased) is of paramount importance in agriculture to improve yield. Conventional deep learning models based on convolutional neural networks require thousands of labeled examples per category. In this work we propose a method to learn from a few samples to automatically classify different pests, plants, and their diseases, using Few-Shot Learning (FSL). We learn a feature extractor to generate embeddings and then update the embeddings using Transformers. Using Mahalanobis distance, a class-covariance-based metric, we then calculate the similarity of the transformed embeddings with the embedding of the image to be classified. Using our proposed architecture, we conduct extensive experiments on multiple datasets showing the effectiveness of our proposed model. We conduct 42 experiments in total to comprehensively analyze the model and it achieves up to 14% and 24% performance gains on few-shot image classification benchmarks on two datasets. We also compile a new FSL dataset containing images of healthy and diseased plants taken in real-world settings. Using our proposed architecture which has been shown to outperform several existing FSL architectures in agriculture, we provide strong baselines on our newly proposed dataset.",
    "code_link": ""
  },
  "iccv2021_cvppa_classificationandvisualizationofgenotypexphenotypeinteractionsinbiomasssorghum": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Classification and Visualization of Genotype x Phenotype Interactions in Biomass Sorghum",
    "authors": [
      "Abby Stylianou",
      "Robert Pless",
      "Nadia Shakoor",
      "Todd Mockler"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Stylianou_Classification_and_Visualization_of_Genotype_x_Phenotype_Interactions_in_Biomass_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Stylianou_Classification_and_Visualization_of_Genotype_x_Phenotype_Interactions_in_Biomass_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We introduce a simple approach to understanding the relationship between single nucleotide polymorphisms (SNPs), or groups of related SNPs, and the phenotypes they control. The pipeline involves training deep convolutional neural networks (CNNs) to differentiate between images of plants with reference and alternate versions of various SNPs, and then using visualization approaches to highlight what the classification networks key on. We demonstrate the capacity of deep CNNs at performing this classification task, and show the utility of these visualizations on RGB imagery of biomass sorghum captured by the TERRA-REF gantry. We focus on several different genetic markers with known phenotypic expression, and discuss the possibilities of using this approach to uncover genotype x phenotype relationships.",
    "code_link": ""
  },
  "iccv2021_cvppa_tip-burnstressdetectionoflettucecanopygrowninplantfactories": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Tip-Burn Stress Detection of Lettuce Canopy Grown in Plant Factories",
    "authors": [
      "Riccardo Gozzovelli",
      "Benjamin Franchetti",
      "Malik Bekmurat",
      "Fiora Pirri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Gozzovelli_Tip-Burn_Stress_Detection_of_Lettuce_Canopy_Grown_in_Plant_Factories_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Gozzovelli_Tip-Burn_Stress_Detection_of_Lettuce_Canopy_Grown_in_Plant_Factories_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A compelling effort has been made in recent years to face several kinds of plant stresses using a variety of sensors and deep learning methods. Yet most of the datasets are based on single leaves or on single plants, exhibiting explicit diseases. In this work we present a new method for stress detection which can deal with a dense canopy of plants, grown in Plant Factories under artificial lights. Our approach combining both classification and segmentation with self supervised masks, and WGAN based data augmentation, has the significant advantage of using normal rgb low cost cameras, simple data aquisition for training and it can both localize and detect the tip-burn stress on the plant canopy with very good accuracy as shown in the results. We have tested our results also on datasets available on tensorflow.org.",
    "code_link": ""
  },
  "iccv2021_cvppa_fromrgbtonirpredictingofnearinfraredreflectancefromvisiblespectrumaerialimagesofcrops": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "From RGB to NIR: Predicting of Near Infrared Reflectance From Visible Spectrum Aerial Images of Crops",
    "authors": [
      "Masoomeh Aslahishahri",
      "Kevin G. Stanley",
      "Hema Duddu",
      "Steve Shirtliffe",
      "Sally Vail",
      "Kirstin Bett",
      "Curtis Pozniak",
      "Ian Stavness"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Aslahishahri_From_RGB_to_NIR_Predicting_of_Near_Infrared_Reflectance_From_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Aslahishahri_From_RGB_to_NIR_Predicting_of_Near_Infrared_Reflectance_From_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Near infrared spectroscopy (NIR) provides rich information in agricultural operations and experiments to determine crop parameters which are not visible to the human eye. Collecting the NIR spectral band requires a multispectral camera which is typically more expensive and has lower resolution than a comparable RGB camera. We investigate image-to-image translation as a means to generate an NIR spectral band from an RGB image alone in aerial crop imagery. Aerial images were captured via a multispectral sensor mounted on an unmanned aerial vehicle (UAV) flown over canola, lentil, dry bean, and wheat breeding trials. A software workflow was created to preprocess raw aerial images creating a dataset suitable for training and evaluating deep learning based band inferencing algorithms. Two different experiments over different crop types in our dataset were conducted to evaluate efficacy in an agricultural context.",
    "code_link": "https://github.com/p2irc/rgb2nir"
  },
  "iccv2021_cvppa_machinelearningmeetsdistinctnessinvarietytesting": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Machine Learning Meets Distinctness in Variety Testing",
    "authors": [
      "Geoffroy Couasnet",
      "Mouad Zine el Abidine",
      "Fran\u00e7ois Laurens",
      "Helin Dutagaci",
      "David Rousseau"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Couasnet_Machine_Learning_Meets_Distinctness_in_Variety_Testing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Couasnet_Machine_Learning_Meets_Distinctness_in_Variety_Testing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Distinctness is a binary trait used in variety testing to determine if a new plant variety can be considered distinct or not from a set of already existing varieties. Currently distinctness is mostly based on human perception. This communication considers distinctness with a machine learning perspective where distinctness is evaluated through an identification process based on information extraction from machine vision. Illustrations are provided on apple variety testing to perform distinctness based on color. An automated pipeline of image acquisition, processing and supervised learning is proposed. A feature space based on the 3D color histogram of a set of apples is built. This feature space is built using optimal transport, fractal dimension, mutual entropy and fractional anisotropy and it provides results in accordance with human expertise when applied to a set of varieties highly contrasted in color and another one with low contrast. These results open new research directions for achieving higher-throughput, higher reproducibility and higher statistical confidence in variety testing",
    "code_link": ""
  },
  "iccv2021_cvppa_field-basedplotextractionusinguavrgbimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Field-Based Plot Extraction Using UAV RGB Images",
    "authors": [
      "Changye Yang",
      "Sriram Baireddy",
      "Enyu Cai",
      "Melba Crawford",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Yang_Field-Based_Plot_Extraction_Using_UAV_RGB_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Yang_Field-Based_Plot_Extraction_Using_UAV_RGB_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Unmanned Aerial Vehicles (UAVs) have become popular for use in plant phenotyping of field based crops, such as maize and sorghum, due to their ability to acquire high resolution data over field trials. Field experiments, which may comprise thousands of plants, are planted according to experimental designs to evaluate varieties or management practices. For many types of phenotyping analysis, we examine smaller groups of plants known as \"plots.\" In this paper, we propose a new plot extraction method that will segment a UAV image into plots. We will demonstrate that our method achieves higher plot extraction accuracy than existing approaches.",
    "code_link": ""
  },
  "iccv2021_cvppa_analysisofarabidopsisrootimages--studiesoncnnsandskeleton-basedroottopology": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Analysis of Arabidopsis Root Images -- Studies on CNNs and Skeleton-Based Root Topology",
    "authors": [
      "Birgit M\u00f6ller",
      "Berit Schreck",
      "Stefan Posch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Moller_Analysis_of_Arabidopsis_Root_Images_--_Studies_on_CNNs_and_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Moller_Analysis_of_Arabidopsis_Root_Images_--_Studies_on_CNNs_and_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Roots and their temporal development play an important role in plant research. Over the decades image-based monitoring of root growth has become a key methodology in this research field. The growing amount of image data is often tackled with automatic image analysis approaches. In particular convolutional neural networks (CNNs) recently gained increasing interest for root segmentation. This segmentation of roots is usually only the first step of an analysis pipeline and needs to be supplemented by topological reconstruction of the complete root system architecture. In this paper we present a comprehensive study of different CNN architectures, loss functions and parameter settings for root image segmentation. In addition, we show how main and lateral roots can be identified based on the skeletons of segmented root components as a first step towards topological reconstruction of root system architecture. We present quantitative and qualitative results on data released in the course of the CVPPA Arabidopsis Root Segmentation Challenge 2021.",
    "code_link": ""
  },
  "iccv2021_cvppa_whatdoesterra-refshighresolution,multisensorplantsensingpublicdomaindataofferthecomputervisioncommunity?": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?",
    "authors": [
      "David LeBauer",
      "Max Burnette",
      "Noah Fahlgren",
      "Rob Kooper",
      "Kenton McHenry",
      "Abby Stylianou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/LeBauer_What_Does_TERRA-REFs_High_Resolution_Multi_Sensor_Plant_Sensing_Public_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/LeBauer_What_Does_TERRA-REFs_High_Resolution_Multi_Sensor_Plant_Sensing_Public_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A core objective of the TERRA-REF project was to generate an open-access reference dataset for the evaluation of sensing technologies to study plants under field conditions. The TERRA-REF program deployed a suite of high-resolution, cutting edge technology sensors on a gantry system with the aim of scanning 1 hectare (10\\textsuperscript 4 m) at around 1 mm\\textsuperscript 2spatial resolution multiple times per week. The system contains co-located sensors including a stereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D structure, and two hyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is provided alongside over sixty types of traditional plant phenotype measurements that can be used to train new machine learning models. Associated weather and environmental measurements, information about agronomic management and experimental design, and the genomic sequences of hundreds of plant varieties have been collected and are available alongside the sensor and plant phenotype data. Over the course of four years and ten growing seasons, the TERRA-REF system generated over 1 PB of sensor data and almost 45 million files. The subset that has been released to the public domain accounts for two seasons and about half of the total data volume. This provides an unprecedented opportunity for investigations far beyond the core biological scope of the project. The focus of this paper is to provide the Computer Vision and Machine Learning communities an overview of the available data and some potential applications of this one of a kind data.",
    "code_link": ""
  },
  "iccv2021_cvppa_predictingproteincontentingrainusinghyperspectraldeeplearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVPPA",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Plant Phenotyping and Agriculture",
    "title": "Predicting Protein Content in Grain Using Hyperspectral Deep Learning",
    "authors": [
      "Ole-Christian Galbo Engstr\u00f8m",
      "Erik Schou Dreier",
      "Kim Steenstrup Pedersen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Engstrom_Predicting_Protein_Content_in_Grain_Using_Hyperspectral_Deep_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Engstrom_Predicting_Protein_Content_in_Grain_Using_Hyperspectral_Deep_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We assess the possibility of performing regression analysis on hyperspectral images utilizing the entire spatio-spectral data cube in convolutional neural networks using protein regression analysis of bulk wheat grain kernels as a test case. By introducing novel modifications of the well-known convolutional neural network, ResNet-18, we are able to significantly increase its performance on hyperspectral images. Our modifications consist of firstly applying a 3D convolution layer enabling learning of spectral derivatives that 2D spatial convolution is unable to learn, and secondly, the application of a (1 x 1) 2D convolution layer that downsamples the spectral dimension. Analysis of the responses learned by the convolution kernels in our modifications reveals meaningful representations of the input data cube that reduce noise and enable the subsequent ResNet-18 to perform more accurate regression analysis.",
    "code_link": ""
  },
  "iccv2021_diff3d_ssrsemi-supervisedsoftrasterizerforsingle-view2dto3dreconstruction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "Diff3D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Differentiable 3D Vision and Graphics",
    "title": "SSR: Semi-Supervised Soft Rasterizer for Single-View 2D to 3D Reconstruction",
    "authors": [
      "Issam Laradji",
      "Pau Rodr\u00edguez",
      "David Vazquez",
      "Derek Nowrouzezahrai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/Diff3D/html/Laradji_SSR_Semi-Supervised_Soft_Rasterizer_for_Single-View_2D_to_3D_Reconstruction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/Diff3D/papers/Laradji_SSR_Semi-Supervised_Soft_Rasterizer_for_Single-View_2D_to_3D_Reconstruction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recent work has made significant progress in learning object meshes with weak supervision. Soft Rasterization methods have achieved accurate 3D reconstruction from 2D images with viewpoint supervision only. In this work, we further reduce the labeling effort by allowing such 3D reconstruction methods leverage unlabeled images. In order to obtain the viewpoints for these unlabeled images, we propose to use a Siamese network that takes two images as input and outputs whether they correspond to the same viewpoint. During training, we minimize the cross entropy loss to maximize the probability of predicting whether a pair of images belong to the same viewpoint or not. To get the viewpoint of a new image, we compare it against different viewpoints obtained from the training samples and select the viewpoint with the highest matching probability. We finally label the unlabeled images with the most confident predicted viewpoint and train a deep network that has a differentiable rasterization layer. Our experiments show that even labeling only two objects yields significant improvement in IoU for ShapeNet when leveraging unlabeled examples. Code is available at https://github.com/IssamLaradji/SSR.",
    "code_link": "https://github.com/ShichenLiu/SoftRas"
  },
  "iccv2021_diff3d_deepdraperfastandaccurate3dgarmentdrapingovera3dhumanbody": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "Diff3D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Differentiable 3D Vision and Graphics",
    "title": "DeepDraper: Fast and Accurate 3D Garment Draping Over a 3D Human Body",
    "authors": [
      "Lokender Tiwari",
      "Brojeshwar Bhowmick"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/Diff3D/html/Tiwari_DeepDraper_Fast_and_Accurate_3D_Garment_Draping_Over_a_3D_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/Diff3D/papers/Tiwari_DeepDraper_Fast_and_Accurate_3D_Garment_Draping_Over_a_3D_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Draping a 3D human mesh has garnered broad interest due to its wide applicability in virtual try-on, animations, etc. The 3D garment deformations produced by the existing methods are often inconsistent with the body shape, pose, and measurements. This paper proposes a single unified learning-based framework (DeepDraper) to predict garment deformation as a function of body shape, pose, measurements, and garment styles. We train the DeepDraper with coupled geometric and multi-view perceptual losses. Unlike existing methods, we additionally model garment deformations as a function of standard body measurements, which generally a buyer or a designer uses to buy or design perfect fit clothes. As a result, DeepDraper significantly outperforms the state-of-the-art deep network-based approaches in terms of fitness and realism and generalizes well to the unseen style of the garments. In addition to that, DeepDraper is10 times smaller in size and23 times faster than the closest state-of-the-art method (TailorNet), which favors its use in real-time applications with less computational power. Despite being trained on the static poses of the TailorNet dataset, DeepDraper generalizes well to unseen body shapes, poses, and garment styles and produces temporally coherent garment deformations on the pose sequences even from the unseen AMASS dataset.",
    "code_link": ""
  },
  "iccv2021_mfr_towardsmask-robustfacerecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Towards Mask-Robust Face Recognition",
    "authors": [
      "Tao Feng",
      "Liangpeng Xu",
      "Hangjie Yuan",
      "Yongfei Zhao",
      "Mingqian Tang",
      "Mang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Feng_Towards_Mask-Robust_Face_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Feng_Towards_Mask-Robust_Face_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we focus on the problem of mask-robust face recognition. Facial mask usually covers a major part of face, causing a significant reduction in extracting effective features. Due to such restriction, even the most advanced face recognition models are confronted with significant challenges. In light of this, this paper attempts to provide a reliable solution. Specifically, we introduce a mask-to-face image blending approach based on UV texture mapping, and a self-learning based cleaning pipeline for processing noisy training datasets. Then, considering the impacts of the long-tail distribution and hard faces samples, a loss function named Balanced Curricular Loss is introduced. Together with a bag of tricks is briefly presented. Experimental results show that the proposed solution separately achieved 84.528% @ Mask and 88.355% @ MR-ALL in InsightFace ms1m Track, which ranks 3rd when the paper submitted.",
    "code_link": ""
  },
  "iccv2021_mfr_maskawarenetworkformaskedfacerecognitioninthewild": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Mask Aware Network for Masked Face Recognition in the Wild",
    "authors": [
      "Kai Wang",
      "Shuo Wang",
      "Jianfei Yang",
      "Xiaobo Wang",
      "Baigui Sun",
      "Hao Li",
      "Yang You"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Wang_Mask_Aware_Network_for_Masked_Face_Recognition_in_the_Wild_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Wang_Mask_Aware_Network_for_Masked_Face_Recognition_in_the_Wild_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face recognition is one of the most important research topics for intelligence security system, especially in the COVID-19 era. Medical research has proven that wearing a mask is the most efficient way to avoid the risk of COVID-19. Nevertheless, classic face recognition systems often fail when dealing with the masked faces, so it is very essential to design a method that is robust to Masked Face Recognition (MFR). In this paper, to relieve the degraded performance of MFR, we propose Mask Aware Network (MAN) including a mask generation module and a loss function searching module. The mask generation module utilizes the face landmarks to obtain more realistic and reliable masked faces for training. The loss function searching module tries to match the most suitable loss for face recognition. On ICCV MFR challenge, our team victor-2021 achieves 5 first places (including 3 champions in standard face recognition and 2 champions in masked face recognition) and 1 third place by 3rd August 2021. These results demonstrate the robustness and generalization of our method no matter in standard or masked face recognition task.",
    "code_link": ""
  },
  "iccv2021_mfr_maskedfacerecognitiondatasetsandvalidation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Masked Face Recognition Datasets and Validation",
    "authors": [
      "Baojin Huang",
      "Zhongyuan Wang",
      "Guangcheng Wang",
      "Kui Jiang",
      "Zheng He",
      "Hua Zou",
      "Qin Zou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Huang_Masked_Face_Recognition_Datasets_and_Validation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Huang_Masked_Face_Recognition_Datasets_and_Validation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In order to effectively prevent the spread of COVID-19 virus, almost everyone wears a mask during coronavirus epidemic. This nearly makes conventional facial recognition technology ineffective in many scenarios, such as face authentication, security check, community visit check-in, etc. Therefore, it is very urgent to boost performance of existing face recognition systems on masked faces. Most current advanced face recognition approaches are based on deep learning, which heavily depend on a large number of training samples. However, there are presently no publicly available masked face recognition datasets. To this end, this work proposes three types of masked face datasets, including Masked Face Detection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD) and Synthetic Masked Face Recognition Dataset (SMFRD). As far as we know, we are the first to publicly release large-scale masked face recognition datasets that can be downloaded for free at https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset.",
    "code_link": "https://github.com/X-zhangyang/Rea"
  },
  "iccv2021_mfr_partialfctraining10millionidentitiesonasinglemachine": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Partial FC: Training 10 Million Identities on a Single Machine",
    "authors": [
      "Xiang An",
      "Xuhan Zhu",
      "Yuan Gao",
      "Yang Xiao",
      "Yongle Zhao",
      "Ziyong Feng",
      "Lan Wu",
      "Bin Qin",
      "Ming Zhang",
      "Debing Zhang",
      "Ying Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/An_Partial_FC_Training_10_Million_Identities_on_a_Single_Machine_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/An_Partial_FC_Training_10_Million_Identities_on_a_Single_Machine_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memory is gradually becoming irreconcilable. In this work, we theoretically analyze the upper limit of model parallelism in face recognition in the first place. Then we propose a load-balanced sparse distributed classification training method, Partial FC, which is capable of using a machine with only 8 Nvidia Tesla V100 GPUs to implement training on a face recognition data set with up to 29 million IDs. Furthermore, we are able to train on data set with 100 million IDs in 64 RTX2080Ti GPUs. We have verified the effectiveness of Partial FC in 8 mainstream face recognition trainsets, and find that Partial FC is effective in all face recognition training sets. The code of this paper has been made available at https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.",
    "code_link": "https://github.com/deepinsight/insightface"
  },
  "iccv2021_mfr_boostingfairnessformaskedfacerecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Boosting Fairness for Masked Face Recognition",
    "authors": [
      "Jun Yu",
      "Xinlong Hao",
      "Zeyu Cui",
      "Peng He",
      "Tongliang Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Yu_Boosting_Fairness_for_Masked_Face_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Yu_Boosting_Fairness_for_Masked_Face_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face recognition achieved excellent performance in recent years. However, its potential for unfairness is raising alarm. For example, the recognition rate for the special group of East Asian is quite low. Many efforts have spent to improve the fairness of face recognition. During the COVID-19 pandemic, masked face recognition is becoming a hot topic but brings new challenging for fair face recognition. For example, the mouth and nose are important to recognizing faces of Asian groups. Masks would further reduce the recognition rate of Asian faces. To this end, this paper proposes a fair masked face recognition system. First, an appropriate masking method is used to generate masked faces. Then, a data re-sampling approach is employed to balance the data distribution and reduce the bias based on the analysis of training data. Moreover, we propose an asymmetric-arc-loss which is a combination of arc-face loss and circle-loss, it is useful for increasing recognition rate and reducing bias. Integrating these techniques, this paper obtained fairer and better face recognition results on masked faces.",
    "code_link": ""
  },
  "iccv2021_mfr_explainablefacerecognitionbasedonaccuratefacialcompositions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Explainable Face Recognition Based on Accurate Facial Compositions",
    "authors": [
      "Haoran Jiang",
      "Dan Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Jiang_Explainable_Face_Recognition_Based_on_Accurate_Facial_Compositions_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Jiang_Explainable_Face_Recognition_Based_on_Accurate_Facial_Compositions_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With impressive advances made in face recognition, the explainability has attracted more and more attentions in the community, which delves into traceable and well-founded clues behind the identifications in addition to the confidence scores. However, the current Explainable Face Recognition (XFR) methods are difficult to balance the explainability and the recognition performance. In this paper, we propose a framework based on Accurate Facial Compositions, namely AFC-XFR. The framework consists of three modules: the Backbone for feature extraction, the Local Feature Refine Module (LFRM) for semantic feature refining, and the Self-Attention based Reconstruction Module (SARM) for serialized feature interaction. Fifteen semantic features, which are accurately captured from local facial components via the proposed acquisition scheme, are conveyed in the latter two modules. Moreover, the LFRM allows us to verify three significant insights experimentally, obtaining the explainability from the perspective of model decisions. Inspired by the insight \"Facial features are processed holistically\", the SARM's internal feature interaction mechanism facilitates performance increase. Extensive experiments on varying loss functions and network architectures accomplish consistent advances on evaluation benchmarks.",
    "code_link": ""
  },
  "iccv2021_mfr_balancedmaskedandstandardfacerecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Balanced Masked and Standard Face Recognition",
    "authors": [
      "Delong Qi",
      "Kangli Hu",
      "Weijun Tan",
      "Qi Yao",
      "Jingfeng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Qi_Balanced_Masked_and_Standard_Face_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Qi_Balanced_Masked_and_Standard_Face_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present the improved network architecture, data augmentation, and training strategies for the Webface track and Insightface/Glint360K track of the masked face recognition challenge of ICCV2021. One of the key goals is how to have a balanced performance of masked and standard face recognition. In order to prevent the overfitting for the masked face recognition, we balance the total number of masked faces by not more than 10% of the total face recognition in the training dataset. We propose a few key changes to the face recognition network including a new stem unit, drop block, and face alignment using YOLO5Face. With this strategy, we achieve good and balanced performance for both masked and standard face recognition.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "iccv2021_mfr_anefficientnetworkdesignforfacevideosuper-resolution": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "An Efficient Network Design for Face Video Super-Resolution",
    "authors": [
      "Feng Yu",
      "He Li",
      "Sige Bian",
      "Yongming Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Yu_An_Efficient_Network_Design_for_Face_Video_Super-Resolution_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Yu_An_Efficient_Network_Design_for_Face_Video_Super-Resolution_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face video super-resolution algorithm aims to reconstruct realistic face details through continuous input video sequences. However, existing video processing algorithms usually contain redundant parameters to guarantee different super-resolution scenes. In this work, we focus on super-resolution of face areas in original video scenes, while rest areas are interpolated. This specific super-resolved task makes it possible to cut redundant parameters in general video super-resolution networks. We construct a dataset consisting entirely of face video sequences for network training and evaluation, and conduct hyper-parameter optimization in our experiments. We use three combined strategies to optimize the network parameters with a simultaneous train-evaluation method to accelerate optimization process. Results show that simultaneous train-evaluation method improves the training speed and facilitates the generation of efficient networks. The generated network can reduce at least 52.4% parameters and 20.7% FLOPs, achieve better performance on PSNR, SSIM compared with state-of-art video super-resolution algorithms. When processing 36x36x1x3 input video frame sequences, the efficient network provides 47.62 FPS real-time processing performance.",
    "code_link": "https://github.com/yphone/efficient-network-for-face-VSR"
  },
  "iccv2021_mfr_maskoutadataaugmentationmethodformaskedfacerecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "MaskOut: A Data Augmentation Method for Masked Face Recognition",
    "authors": [
      "Weiqiu Wang",
      "Zhicheng Zhao",
      "Hongyuan Zhang",
      "Zhaohui Wang",
      "Fei Su"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Wang_MaskOut_A_Data_Augmentation_Method_for_Masked_Face_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Wang_MaskOut_A_Data_Augmentation_Method_for_Masked_Face_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep learning methods have achieved great performances in face recognition. However, the performances of deep learning methods deteriorate in case of wearing a mask. Recently, due to the world-wide COVID-19 pandemic, masked face recognition attracts more attention. It is non-trivial and urgent to improve the performances in masked face recognition. In this work, a simple and effective data augmentation method, named MaskOut, is proposed. MaskOut replaces a random region below the nose of a face with a random mask template to mask out original face features. Our method is computing and memory efficient and convenient to combine with other methods. The experimental results show that the performances in masked face recognition are improved by a large margin with MaskOut. Besides, we construct a real-life masked face dataset, named MCPRL-Mask, to evaluate the performance of masked face recognition models.",
    "code_link": ""
  },
  "iccv2021_mfr_ressanetahybridbackboneofresidualblockandself-attentionmoduleformaskedfacerecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "ResSaNet: A Hybrid Backbone of Residual Block and Self-Attention Module for Masked Face Recognition",
    "authors": [
      "Wei-Yi Chang",
      "Ming-Ying Tsai",
      "Shih-Chieh Lo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Chang_ResSaNet_A_Hybrid_Backbone_of_Residual_Block_and_Self-Attention_Module_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Chang_ResSaNet_A_Hybrid_Backbone_of_Residual_Block_and_Self-Attention_Module_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In recent years, the performances of face recognition have been improved significantly by using convolution neural networks (CNN) as the feature extractors. On the other hands, in order to avoid spreading COVID-19 virus, people would wear mask even when they want to pass the face recognition system. Thus, it is necessary to improve the performance of masked face recognition so that users could utilize face recognition methods more easily. In this paper, we propose a feature extraction backbone named ResSaNet that integrates CNN (especially Residual block) and Self-attention module into the same network. By capturing the local and global information of face area simultaneously, our proposed ResSaNet could achieve promising results on both masked and non-masked testing data.",
    "code_link": ""
  },
  "iccv2021_mfr_rectifyingthedatabiasinknowledgedistillation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Rectifying the Data Bias in Knowledge Distillation",
    "authors": [
      "Boxiao Liu",
      "Shenghan Zhang",
      "Guanglu Song",
      "Haihang You",
      "Yu Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Liu_Rectifying_the_Data_Bias_in_Knowledge_Distillation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Liu_Rectifying_the_Data_Bias_in_Knowledge_Distillation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Knowledge distillation is a representative technique for model compression and acceleration, which is important for deploying neural networks on resource limited devices. The knowledge transferred from teacher to student is the mapping of teacher model, or represented by all the input-output pairs. However, in practice the student model only learns from data pairs of the dataset that may be biased, and we think this limits the performance of knowledge distillation. In this paper, we first quantitatively define the uniformity of the sampled data for training, providing a unified view for methods that learn from biased data. Then we evaluate the uniformity on real world dataset and show that existing methods actually improve the uniformity of data. We further introduce two uniformity-oriented methods for rectifying the bias of data for knowledge distillation. Extensive experiments conducted on Face Recognition and Person Re-identification have shown the effectiveness of our method. Moreover, we analyze the sampled data on Face Recognition and show that better balance is achieved between races and between easy and hard samples. And this effect can be also confirmed in training the student model from scratch, resulting in a comparable performance with standard knowledge distillation.",
    "code_link": ""
  },
  "iccv2021_mfr_improvingrepresentationconsistencywithpairwiselossformaskedfacerecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Improving Representation Consistency With Pairwise Loss for Masked Face Recognition",
    "authors": [
      "Hanjie Qian",
      "Panpan Zhang",
      "Sijie Ji",
      "Shuxin Cao",
      "Yuecong Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Qian_Improving_Representation_Consistency_With_Pairwise_Loss_for_Masked_Face_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Qian_Improving_Representation_Consistency_With_Pairwise_Loss_for_Masked_Face_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Given the coronavirus disease (COVID-19) pandemic,people need to wear masks to protect themselves and reduce the spread of COVID, which bring new challenge to traditional face recognition task. Since features like the nose andmouth, which are well distinguishable, are hidden under themask, traditional methods are no longer simply applicable,even though they once achieved a high degree of accuracy.In response to this problem, the Masked Face RecognitionChallenge&Workshop (MFR) was held in conjunction withthe International Conference on Computer Vision (ICCV)2021. This article details a method that combining the classic ArcFace and pairwise loss to target the new masked facerecognition task. So far, our method has achieved the second place in the competition.",
    "code_link": ""
  },
  "iccv2021_mfr_revistingquantizationerrorinfacealignment": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Revisting Quantization Error in Face Alignment",
    "authors": [
      "Xing Lan",
      "Qinghao Hu",
      "Jian Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Lan_Revisting_Quantization_Error_in_Face_Alignment_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Lan_Revisting_Quantization_Error_in_Face_Alignment_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recently, heatmap regression models have become the mainstream in locating facial landmarks. To keep computation affordable and reduce memory usage, the whole procedure involves downsampling from the raw image to the output heatmap. However, how much impact will the quantization error introduced by downsampling bring? The problem is hardly systematically investigated among previous works. This work fills the blank and we are the first to quantitatively analyze the negative gain. The statistical results show the NME generated by quantization error is even larger than 1/3 of the SOTA item, which is a serious obstacle for making a new breakthrough in face alignment. To compensate for the impact of quantization effect, we propose a novel method, called Heatmap In Heatmap(HIH), which leverages two categories of heatmaps as label representation to encode coordinate. And in HIH, the range of one heatmap represents a pixel of the other category of heatmap. Also, we even combine the face alignment with solutions of other fields to make a comparison. Extensive experiments on various benchmarks show the feasibility of HIH and superior performance than other solutions. Moreover, the mean error reaches to 4.18 on WFLW, which exceeds SOTA a lot.",
    "code_link": ""
  },
  "iccv2021_mfr_maskedfacerecognitionchallengetheinsightfacetrackreport": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MFR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Face Bio-Metrics Under COVID\u2014Masked Face Recognition",
    "title": "Masked Face Recognition Challenge: The InsightFace Track Report",
    "authors": [
      "Jiankang Deng",
      "Jia Guo",
      "Xiang An",
      "Zheng Zhu",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/html/Deng_Masked_Face_Recognition_Challenge_The_InsightFace_Track_Report_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Deng_Masked_Face_Recognition_Challenge_The_InsightFace_Track_Report_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "During the COVID-19 coronavirus epidemic, almost everyone wears a facial mask, which poses a huge challenge to deep face recognition. In this workshop, we organize Masked Face Recognition (MFR) challenge and focus on bench-marking deep face recognition methods under the existence of facial masks. In the MFR challenge, there are two main tracks: the InsightFace track and the WebFace260M track. For the InsightFace track, we manually collect a large-scale masked face test set with 7K identities. In addition, we also collect a children test set including 14K identities and a multi-racial test set containing 242K identities. By using these three test sets, we build up an online model testing system, which can give a comprehensive evaluation of face recognition models. To avoid data privacy problems, no test image is released to the public. As the challenge is still under-going, we will keep on updating the top-ranked solutions as well as this report on the arxiv.",
    "code_link": ""
  },
  "iccv2021_ildav_dataaugmentationforscenetextrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Data Augmentation for Scene Text Recognition",
    "authors": [
      "Rowel Atienza"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Atienza_Data_Augmentation_for_Scene_Text_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Atienza_Data_Augmentation_for_Scene_Text_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Scene text recognition (STR) is a challenging task in computer vision due to the large number of possible text appearances in natural scenes. Most STR models rely on synthetic datasets for training since there are no sufficiently big and publicly available labelled real datasets. Since STR models are evaluated using real data, the mismatch between training and testing data distributions results into poor performance of models especially on challenging text that are affected by noise, artifacts, geometry, structure, etc. In this paper, we introduce STRAug which is made of 36 image augmentation functions designed for STR. Each function mimics certain text image properties that can be found in natural scenes, caused by camera sensors, or induced by signal processing operations but poorly represented in the training dataset. When applied to strong baseline models using RandAugment, STRAug significantly increases the overall absolute accuracy of STR models across regular and irregular test datasets by as much as 2.10% on Rosetta, 1.48% on R2AM, 1.30% on CRNN, 1.35% on RARE, 1.06% on TRBA and 0.89% on GCRNN. The diversity and simplicity of API provided by STRAug functions enable easy replication and validation of existing data augmentation methods for STR. STRAug is available at https://github.com/roatienza/straug.",
    "code_link": "https://github.com/roatienza/straug"
  },
  "iccv2021_ildav_nuisance-labelsupervisionrobustnessimprovementbyfreelabels": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Nuisance-Label Supervision: Robustness Improvement by Free Labels",
    "authors": [
      "Xinyue Wei",
      "Weichao Qiu",
      "Yi Zhang",
      "Zihao Xiao",
      "Alan Yuille"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Wei_Nuisance-Label_Supervision_Robustness_Improvement_by_Free_Labels_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Wei_Nuisance-Label_Supervision_Robustness_Improvement_by_Free_Labels_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we present a Nuisance-label Supervision (NLS) module, which can make models more robust to nuisance factor variations. Nuisance factors are those irrelevant to a task, and an ideal model should be invariant to them. For example, an activity recognition model should perform consistently regardless of the change of clothes and background. But our experiments show existing models are far from this capability. So we explicitly supervise a model with nuisance labels to make extracted features less dependent on nuisance factors. Although the values of nuisance factors are rarely annotated, we demonstrate that besides existing annotations, nuisance labels can be acquired freely from data augmentation and synthetic data. Experiments show consistent improvement in robustness towards image corruption and appearance change in action recognition.",
    "code_link": ""
  },
  "iccv2021_ildav_interactivelabelingforhumanposeestimationinsurveillancevideos": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Interactive Labeling for Human Pose Estimation in Surveillance Videos",
    "authors": [
      "Mickael Cormier",
      "Fabian R\u00f6pke",
      "Thomas Golda",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Cormier_Interactive_Labeling_for_Human_Pose_Estimation_in_Surveillance_Videos_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Cormier_Interactive_Labeling_for_Human_Pose_Estimation_in_Surveillance_Videos_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Automatically detecting and estimating the movement of persons in real-world uncooperative scenarios is very challenging in great part due to limited and unreliably annotated data. For instance annotating a single human body pose for activity recognition requires 40-60 seconds in complex sequences, leading to long-winded and costly annotation processes. Therefore increasing the sizes of annotated datasets through crowdsourcing or automated annotation is often used at a great financial costs, without reliable validation processes and inadequate annotation tools greatly impacting the annotation quality. In this work we combine multiple techniques into a single web-based general-purpose annotation application. Pre-trained machine learning models enable annotators to interactively detect pedestrians, re-identify them throughout the sequence, estimate their poses, and correct annotation suggestions in the same interface. Annotations are then inter- and extrapolated between frames. The application is evaluated through several user studies and the results are extensively analyzed. Experiments demonstrate a 55% reduction in annotation time for less complex scenarios while simultaneously decreasing perceived annotator workload.",
    "code_link": "https://github.com/openvinotoolkit/cvat"
  },
  "iccv2021_ildav_usingsyntheticdatagenerationtoprobemulti-viewstereonetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Using Synthetic Data Generation To Probe Multi-View Stereo Networks",
    "authors": [
      "Pranav Acharya",
      "Daniel Lohn",
      "Vivian Ross",
      "Maya Ha",
      "Alexander Rich",
      "Ehsan Sayyad",
      "Tobias H\u00f6llerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Acharya_Using_Synthetic_Data_Generation_To_Probe_Multi-View_Stereo_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Acharya_Using_Synthetic_Data_Generation_To_Probe_Multi-View_Stereo_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Synthetic data is highly useful for training machine learning systems performing image-based 3D reconstruction, as synthetic data has applications in both extending existing generalizable datasets and being tailored to train neural networks for specific learning tasks of interest. In this paper, we introduce and utilize a synthetic data generation suite capable of generating data given existing 3D scene models as input. Specifically, we use our tool to generate image sequences for use with Multi-View Stereo (MVS), moving a camera through the virtual space according to user-chosen camera parameters. We evaluate how the given camera parameters and type of 3D environment affect how applicable the generated image sequences are to the MVS task using five pre-trained neural networks on image sequences generated from three different 3D scene datasets. We obtain generated predictions for each combination of parameter value and input image sequence, using standard error metrics to analyze the differences in depth predictions on image sequences across 3D datasets, parameters, and networks. Among other results, we find that camera height and vertical camera viewing angle are the parameters that cause the most variation in depth prediction errors on these image sequences.",
    "code_link": ""
  },
  "iccv2021_ildav_boundingboxdatasetaugmentationforlong-rangeobjectdistanceestimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Bounding Box Dataset Augmentation for Long-Range Object Distance Estimation",
    "authors": [
      "Marten Franke",
      "Vaishnavi Gopinath",
      "Chaitra Reddy",
      "Danijela Risti\u0107-Durrant",
      "Kai Michels"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Franke_Bounding_Box_Dataset_Augmentation_for_Long-Range_Object_Distance_Estimation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Franke_Bounding_Box_Dataset_Augmentation_for_Long-Range_Object_Distance_Estimation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Autonomous long-range obstacle detection and distance estimation plays an important role in numerous applications such as railway applications when it comes to locomotive drivers support or developments towards driverless trains. To overcome the problem of small training datasets, this paper presents two data augmentation methods for training the ANN DisNet to perform reliable longrange distance estimation.",
    "code_link": ""
  },
  "iccv2021_ildav_weakly-supervisedsemanticsegmentationbylearninglabeluncertainty": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Weakly-Supervised Semantic Segmentation by Learning Label Uncertainty",
    "authors": [
      "Robby Neven",
      "Davy Neven",
      "Bert De Brabandere",
      "Marc Proesmans",
      "Toon Goedem\u00e9"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Neven_Weakly-Supervised_Semantic_Segmentation_by_Learning_Label_Uncertainty_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Neven_Weakly-Supervised_Semantic_Segmentation_by_Learning_Label_Uncertainty_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Since the rise of deep learning, many computer vision tasks have seen significant advancements. However, the downside of deep learning is that it is very data-hungry. Especially for segmentation problems, training a deep neural net requires dense supervision in the form of pixel-perfect image labels, which are very costly. In this paper, we present a new loss function to train a segmentation network with only a small subset of pixel-perfect labels, but take the advantage of weakly-annotated training samples in the form of cheap bounding-box labels. Unlike recent works which make use of box-to-mask proposal generators, our loss trains the network to learn a label uncertainty within the bounding-box, which can be leveraged to perform online bootstrapping (i.e. transforming the boxes to segmentation masks), while training the network. We evaluated our method on binary segmentation tasks, as well as a multi-class segmentation task (CityScapes vehicles and persons). We trained each task on a dataset comprised of only 18% pixel-perfect and 82% bounding-box labels, and compared the results to a baseline model trained on a completely pixel-perfect dataset. For the binary segmentation tasks, our method achieves an IoU score which is98.33% as good as our baseline model, while for the multi-class task, our method is 97.12% as good as our baseline model (77.5 vs. 79.8 mIoU).",
    "code_link": ""
  },
  "iccv2021_ildav_reducinglabeleffortself-supervisedmeetsactivelearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Reducing Label Effort: Self-Supervised Meets Active Learning",
    "authors": [
      "Javad Zolfaghari Bengar",
      "Joost van de Weijer",
      "Bartlomiej Twardowski",
      "Bogdan Raducanu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Bengar_Reducing_Label_Effort_Self-Supervised_Meets_Active_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Bengar_Reducing_Label_Effort_Self-Supervised_Meets_Active_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Active learning is a paradigm aimed at reducing the annotation effort by training the model on actively selected informative and/or representative samples. Another paradigm to reduce the annotation effort is self-training that learns from a large amount of unlabeled data in an unsupervised way and fine-tunes on few labeled samples. Recent developments in self-training have achieved very impressive results rivaling supervised learning on some datasets. The current work focuses on whether the two paradigms can benefit from each other. We studied object recognition datasets including CIFAR10, CIFAR100 and Tiny ImageNet with several labeling budgets for the evaluations. Our experiments reveal that self-training is remarkably more efficient than active learning at reducing the labeling effort, that for a low labeling budget, active learning offers no benefit to self-training, and finally that the combination of active learning and self-training is fruitful when the labeling budget is high. The performance gap between active learning trained either with self-training or from scratch diminishes as we approach to the point where almost half of the dataset is labeled.",
    "code_link": ""
  },
  "iccv2021_ildav_multi-domainconditionalimagetranslationtranslatingdrivingdatasetsfromclear-weathertoadverseconditions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Multi-Domain Conditional Image Translation: Translating Driving Datasets From Clear-Weather to Adverse Conditions",
    "authors": [
      "Vishal Vinod",
      "K. Ram Prabhakar",
      "R. Venkatesh Babu",
      "Anirban Chakraborty"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Vinod_Multi-Domain_Conditional_Image_Translation_Translating_Driving_Datasets_From_Clear-Weather_to_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Vinod_Multi-Domain_Conditional_Image_Translation_Translating_Driving_Datasets_From_Clear-Weather_to_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Vision systems for fully autonomous navigation must perform well even in unstructured and degraded scenarios. In most driving datasets today, there is a bias toward clear-weather conditions as compared with extreme-weather owing to the difficulty in capturing and annotating large-scale image datasets degraded by adverse weather. While there has been extensive research on techniques such as deraining, dehazing and on tasks such as segmentation and domain adaptation, there has been minimal attention toward methods to effectively translate clear-weather driving datasets to extreme-weather domains. To address this, we present a method that builds on recent advances in Generative Networks and Self-Supervised Learning to perform conditional multi-domain image translation. We evaluate our method on the semantic scene understanding task and demonstrate quantitatively superior translation results from clear-weather conditions to adverse-weather shifted domains such as Rain, Night and Fog conditions. From our experiments, we show improved domain invariant content disentanglement, and segmentation methods trained with datasets translated using the proposed method have improved performance over single and multi-domain image translation baselines on real-world adverse weather data.",
    "code_link": ""
  },
  "iccv2021_ildav_edgeflowachievingpracticalinteractivesegmentationwithedge-guidedflow": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "EdgeFlow: Achieving Practical Interactive Segmentation With Edge-Guided Flow",
    "authors": [
      "Yuying Hao",
      "Yi Liu",
      "Zewu Wu",
      "Lin Han",
      "Yizhou Chen",
      "Guowei Chen",
      "Lutao Chu",
      "Shiyu Tang",
      "Zhiliang Yu",
      "Zeyu Chen",
      "Baohua Lai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Hao_EdgeFlow_Achieving_Practical_Interactive_Segmentation_With_Edge-Guided_Flow_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Hao_EdgeFlow_Achieving_Practical_Interactive_Segmentation_With_Edge-Guided_Flow_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "High-quality training data play a key role in image segmentation tasks. Usually, pixel-level annotations are expensive, laborious and time-consuming for the large volume of training data. To reduce labelling cost and improve segmentation quality, interactive segmentation methods have been proposed, which provide the result with just a few clicks. However, their performance does not meet the requirements of practical segmentation tasks in terms of speed and accuracy. In this work, we propose EdgeFlow, a novel architecture that fully utilizes interactive information of user clicks with edge-guided flow. Our method achieves state-of-the-art performance without any post-processing or iterative optimization scheme. Comprehensive experiments on benchmarks also demonstrate the superiority of our method. In addition, with the proposed method, we develop an efficient interactive segmentation tool for practical data annotation tasks. The source code and tool is avaliable at \\href https://github.com/PaddlePaddle/PaddleSeghttps://github.com/PaddlePaddle/PaddleSeg .",
    "code_link": ""
  },
  "iccv2021_ildav_learningtolocaliseandcountwithincompletedot-annotations": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Learning to Localise and Count With Incomplete Dot-Annotations",
    "authors": [
      "Feng Chen",
      "Michael P. Pound",
      "Andrew P. French"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Chen_Learning_to_Localise_and_Count_With_Incomplete_Dot-Annotations_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Chen_Learning_to_Localise_and_Count_With_Incomplete_Dot-Annotations_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Annotating training data is a time consuming and labor intensive process in deep learning, especially for images with many objects present. In this paper, we propose a method to allow deep networks to be trained on data with reduced numbers of annotations per image in heatmap regression tasks (e.g. object localisation and counting), by applying an asymmetric loss function. This reduction of annotations can be imposed by the researchers by asking annotators to intentionally label only 50% of what they see in each image - a form of 'few-click' annotation. Our method also has a secondary benefit of counteracting unintentionally missing labels from the annotators. We conduct experiments on wheat spikelet localisation and crowd counting to assess the effectiveness and robustness of our method. Results show that an asymmetric loss function is effective across different models and datasets, even in very extreme cases with limited annotations provided (e.g. 90% of the original annotations reduced). Whilst tuning of the key parameters is required, we find that setting conservative parameter values can help more realistic situations, where only small amounts of data have been missed by annotators.",
    "code_link": ""
  },
  "iccv2021_ildav_class-agnosticsegmentationlossanditsapplicationtosalientobjectdetectionandsegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Class-Agnostic Segmentation Loss and Its Application to Salient Object Detection and Segmentation",
    "authors": [
      "Angira Sharma",
      "Naeemullah Khan",
      "Muhammad Mubashar",
      "Ganesh Sundaramoorthi",
      "Philip Torr"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Sharma_Class-Agnostic_Segmentation_Loss_and_Its_Application_to_Salient_Object_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Sharma_Class-Agnostic_Segmentation_Loss_and_Its_Application_to_Salient_Object_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper we present a novel loss function, called class-agnostic segmentation (CAS) loss. With CAS loss the class descriptors are learned during training of the network. We don't require to define the label of a class a-priori, rather the CAS loss clusters regions with similar appearance together in a weakly-supervised manner. Furthermore, we show that the CAS loss function is sparse, bounded, and robust to class-imbalance. We first apply our CAS loss function with fully-convolutional ResNet101 and DeepLab-v3 architectures to the binary segmentation problem of salient object detection. We investigate the performance against the state-of-the-art methods in two settings of low and high-fidelity training data on seven salient object detection datasets. For low-fidelity training data (incorrect class label) class-agnostic segmentation loss outperforms the state-of-the-art methods on salient object detection datasets by staggering margins of around 50%. For high-fidelity training data (correct class labels) class-agnostic segmentation models perform as good as the state-of-the-art approaches while beating the state-of-the-art methods on most datasets. In order to show the utility of the loss function across different domains we then also test on general segmentation dataset, where class-agnostic segmentation loss outperforms competing losses by huge margins.",
    "code_link": "https://github.com/sofmonk/class"
  },
  "iccv2021_ildav_allyouneedareafewpixelssemanticsegmentationwithpixelpick": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "All You Need Are a Few Pixels: Semantic Segmentation With PixelPick",
    "authors": [
      "Gyungin Shin",
      "Weidi Xie",
      "Samuel Albanie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_With_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_With_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A central challenge for the task of semantic segmentation is the prohibitive cost of obtaining dense pixel-level annotations to supervise model training. In this work, we show that in order to achieve a good level of segmentation performance, all you need are a few well-chosen pixel labels. We make the following contributions: (i) We investigate the semantic segmentation setting in which labels are supplied only at sparse pixel locations, and show that deep neural networks can use a handful of such labels to good effect; (ii) We demonstrate how to exploit this phenomenon within an active learning framework, termed PixelPick, to radically reduce labelling cost, and propose an efficient \"mouse-free\" annotation strategy to implement our approach; (iii) We conduct extensive experiments to study the influence of annotation diversity under a fixed budget, model pretraining, model capacity and the sampling mechanism for picking pixels in this low annotation regime; (iv) We provide comparisons to the existing state of the art in semantic segmentation with active learning, and demonstrate comparable performance with up to two orders of magnitude fewer pixel annotations on the CamVid, Cityscapes and PASCAL VOC 2012 benchmarks; (v) Finally, we evaluate the efficiency of our annotation pipeline and its sensitivity to annotator error to demonstrate its practicality.",
    "code_link": ""
  },
  "iccv2021_ildav_object-basedaugmentationforbuildingsemanticsegmentationventuraandsantarosacasestudy": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Object-Based Augmentation for Building Semantic Segmentation: Ventura and Santa Rosa Case Study",
    "authors": [
      "Svetlana Illarionova",
      "Sergey Nesteruk",
      "Dmitrii Shadrin",
      "Vladimir Ignatiev",
      "Mariia Pukalchik",
      "Ivan Oseledets"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Illarionova_Object-Based_Augmentation_for_Building_Semantic_Segmentation_Ventura_and_Santa_Rosa_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Illarionova_Object-Based_Augmentation_for_Building_Semantic_Segmentation_Ventura_and_Santa_Rosa_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Today deep convolutional neural networks (CNNs) push the limits for most computer vision problems, define trends, and set state-of-the-art results. In remote sensing tasks such as object detection and semantic segmentation, CNNs reach the SotA performance. However, for precise performance, CNNs require much high-quality training data. Rare objects and the variability of environmental conditions strongly affect prediction stability and accuracy. To overcome these data restrictions, it is common to consider various approaches including data augmentation techniques. This study focuses on the development and testing of object-based augmentation. The practical usefulness of the developed augmentation technique is shown in the remote sensing domain, being one of the most demanded in effective augmentation techniques. We propose a novel pipeline for georeferenced image augmentation that enables a significant increase in the number of training samples. The presented pipeline is called object-based augmentation (OBA) and exploits objects' segmentation masks to produce new realistic training scenes using target objects and various label-free backgrounds. We test the approach on the buildings segmentation dataset with different CNN architectures (U-Net, FPN, HRNet) and show that the proposed method benefits for all the tested models. We also show that further augmentation strategy optimization can improve the results. The proposed method leads to the meaningful improvement of U-Net model predictions from 0.78 to 0.83 F1-score.",
    "code_link": "https://github.com/LanaLana/satellite"
  },
  "iccv2021_ildav_metaself-learningformulti-sourcedomainadaptationabenchmark": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark",
    "authors": [
      "Shuhao Qiu",
      "Chuang Zhu",
      "Wenli Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Qiu_Meta_Self-Learning_for_Multi-Source_Domain_Adaptation_A_Benchmark_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Qiu_Meta_Self-Learning_for_Multi-Source_Domain_Adaptation_A_Benchmark_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In recent years, deep learning-based methods have shown promising results in computer vision area. However, a common deep learning model requires a large amount of labeled data, which is labor-intensive to collect and label. What's more, the model can be ruined due to the domain shift between training data and testing data. Text recognition is a broadly studied field in computer vision and suffers from the same problems noted above due to the diversity of fonts and complicated backgrounds. In this paper, we focus on the text recognition problem and mainly make three contributions toward these problems. First, we collect a multi-source domain adaptation dataset for text recognition, including five different domains with over five million images, which is the first multi-domain text recognition dataset to our best knowledge. Secondly, we propose a new method called Meta Self-Learning, which combines the self-learning method with the meta-learning paradigm and achieves a better recognition result under the scene of multi-domain adaptation. Thirdly, extensive experiments are conducted on the dataset to provide a benchmark and also show the effectiveness of our method. The code of our work and dataset are available soon at https://bupt-ai-cz.github.io/Meta-SelfLearning.",
    "code_link": "https://github.com/YCG09/chinese"
  },
  "iccv2021_ildav_inaugmentimprovingclassifiersviainternalaugmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "InAugment: Improving Classifiers via Internal Augmentation",
    "authors": [
      "Moab Arar",
      "Ariel Shamir",
      "Amit Bermano"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Arar_InAugment_Improving_Classifiers_via_Internal_Augmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Arar_InAugment_Improving_Classifiers_via_Internal_Augmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Image augmentation techniques apply transformation functions such as rotation, shearing, or color distortion on an input image. These augmentations were proven useful in improving neural networks' generalization ability. In this paper, we present a novel augmentation operation, InAugment, that exploits image internal statistics. The key idea is to copy patches from the image itself, apply augmentation operations on them, and paste them back at random positions on the same image. This method is simple and easy to implement and can be incorporated with existing augmentation techniques. We test InAugment on two popular datasets -- CIFAR and ImageNet. We show improvement over state-of-the-art augmentation techniques. Incorporating InAugment with Auto Augment yields a significant improvement over other augmentation techniques (e.g., +1% improvement over multiple architectures trained on the CIFAR dataset). We also demonstrate an increase for ResNet50 and EfficientNet-B3 top-1's accuracy on the ImageNet dataset compared to prior augmentation methods. Finally, our experiments suggest that training convolutional neural network using InAugment not only improves the model's accuracy and confidence but its performance on out-of-distribution images.",
    "code_link": ""
  },
  "iccv2021_ildav_self-improvingclassificationperformancethroughgandistillation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Self-Improving Classification Performance Through GAN Distillation",
    "authors": [
      "Matteo Pennisi",
      "Simone Palazzo",
      "Concetto Spampinato"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Pennisi_Self-Improving_Classification_Performance_Through_GAN_Distillation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Pennisi_Self-Improving_Classification_Performance_Through_GAN_Distillation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The availability of a large dataset can be a key factor in achieving good generalization capabilities when training deep learning models. Unfortunately, dataset collection is an expensive and time-consuming task, especially in specific application domains (e.g., medicine). In this paper, we present an approach for overcoming dataset size limitations by combining a classifier with a generative adversarial network (GAN) trained to synthesize \"\"hard\"\" samples through a triplet loss, to encourage the model to learn class features which may be under-represented or ambiguous in a small dataset. We evaluate the proposed approach on subsets of CIFAR-10 in order to simulate a low data availability, and compare the results achieved by our method with those obtained when training in a standard supervised setting over the same reduced set of data. Performance analysis shows a significant improvement in accuracy when training the model on GAN-generated hard samples: our GAN distillation approach improves accuracy in the reduced dataset scenario by about 5 percent points, compared to standard supervised training. Ablation studies and feature visualization confirm that our generative approach is able to consistently produce synthetic images that allow the model to improve its performance even with low data availability.",
    "code_link": ""
  },
  "iccv2021_ildav_localizinghumankeypointsbeyondtheboundingbox": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ILDAV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Interactive Labeling and Data Augmentation for Vision",
    "title": "Localizing Human Keypoints Beyond the Bounding Box",
    "authors": [
      "Soonchan Park",
      "Jinah Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Park_Localizing_Human_Keypoints_Beyond_the_Bounding_Box_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Park_Localizing_Human_Keypoints_Beyond_the_Bounding_Box_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Since human pose is one of the most effective and popular sources for understanding human in various applications, there have been numerous researches on detecting keypoints of human body from the image source. However, when a human body is shown partially in the source image, estimation range is also restricted causing performance degradation in locating keypoints of human body. In this paper, we propose `Position Puzzle' network and augmentation to leverage the performance of detecting keypoints including those outside the bounding box. Specifically, Position Puzzle Network expands the spatial range of keypoint localization by refining the position and the scale of the target's bounding box, and Position Puzzle Augmentation improves the performance of keypoint detector using the partial image in training. We prepare data by cropping COCO dataset and utilize them in training and evaluation. Under the prepared dataset, the proposed method enhances the performance of baseline network up to 37.6% and 30.6% in mAP and mAR, respectively, and effectively localizes keypoints positioned not only inside but also outside the bounding box. We also verify that the proposed method can localize keypoints beyond the bounding box in the original COCO dataset.",
    "code_link": "https://github.com/leoxiaobin/deep-high-resolutionnet.pytorch"
  },
  "iccv2021_acvr_frankmocapamonocular3dwhole-bodyposeestimationsystemviaregressionandintegration": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration",
    "authors": [
      "Yu Rong",
      "Takaaki Shiratori",
      "Hanbyul Joo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Rong_FrankMocap_A_Monocular_3D_Whole-Body_Pose_Estimation_System_via_Regression_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Rong_FrankMocap_A_Monocular_3D_Whole-Body_Pose_Estimation_System_via_Regression_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Most existing monocular 3D pose estimation approaches only focus on a single body part, neglecting the fact that the essential nuance of human motion is conveyed through a concert of subtle movements of face, hands, and body. In this paper, we present FrankMocap, a fast and accurate whole-body 3D pose estimation system that can produce 3D face, hands, and body simultaneously from in-the-wild monocular images. The core idea of FrankMocap is its modular design: We first run 3D pose regression methods for face, hands, and body independently, followed by composing the regression outputs via an integration module. The separate regression modules allow us to take full advantage of their state-of-the-art performances without compromising the original accuracy and reliability in practice. We develop three different integration modules that trade off between latency and accuracy. All of them are capable of providing simple yet effective solutions to unify the separate outputs into seamless whole-body pose estimation results. We quantitatively and qualitatively demonstrate that our modularized system outperforms both the optimization-based and end-to-end methods of estimating whole-body pose.",
    "code_link": ""
  },
  "iccv2021_acvr_opticalbraillerecognitionusingobjectdetectionneuralnetwork": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "Optical Braille Recognition Using Object Detection Neural Network",
    "authors": [
      "Ilya G. Ovodov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Ovodov_Optical_Braille_Recognition_Using_Object_Detection_Neural_Network_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Ovodov_Optical_Braille_Recognition_Using_Object_Detection_Neural_Network_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Optical Braille recognition methods generally rely heavily on a Braille text's geometric structure. They run into problems if this structure is distorted. Thus, they find it difficult to cope with images of book pages taken with a smartphone. We propose an optical Braille recognition method that uses an object detection convolutional neural network to detect whole Braille characters at once. The proposed algorithm is robust to deformations and perspective distortions of a Braille page displayed on an image. The algorithm is suitable for recognizing braille texts captured with a smartphone camera in domestic conditions. It can handle curved pages and images with perspective distortion. The proposed algorithm shows high performance and accuracy compared to existing methods. Additionally, we produced a new dataset containing 240 photos of Braille texts with annotation for each Braille letter. Both the proposed algorithm and the dataset are available at GitHub.",
    "code_link": ""
  },
  "iccv2021_acvr_virtualtouchcomputervisionaugmentedtouch-freesceneexplorationfortheblindorvisuallyimpaired": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "Virtual Touch: Computer Vision Augmented Touch-Free Scene Exploration for the Blind or Visually Impaired",
    "authors": [
      "Xixuan Julie Liu",
      "Yi Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Liu_Virtual_Touch_Computer_Vision_Augmented_Touch-Free_Scene_Exploration_for_the_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Liu_Virtual_Touch_Computer_Vision_Augmented_Touch-Free_Scene_Exploration_for_the_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The Blind or Visually Impaired (BVI) individuals usehaptics much more frequently than the healthy-sighted intheir everyday lives to locate objects and acquire object de-tails. This consequently puts them at higher risk of contract-ing the virus through close contact during a pandemic cri-sis (e.g. COVID-19). Traditional canes only give the BVIslimited perceptive range. Our project develops a wearablesolution named Virtual Touch to augment the BVI's per-ceptive power so they can perceive objects near and farin their surrounding environment in a touch-free mannerand consequently carry out activities of daily living dur-ing pandemics more intuitively, safely, and independently.The Virtual Touch feature contains a camera with a novelpoint-based neural network TouchNet tailored for real-timeblind-centered object detection, and a headphone telling theBVI the semantic labels. Through finger pointing, the BVIend user indicates where he or she is paying attention to rel-ative to their egocentric coordinate system, based on whichwe build attention-driven spatial intelligence.",
    "code_link": ""
  },
  "iccv2021_acvr_hidatowardsholisticindoorunderstandingforthevisuallyimpairedviasemanticinstancesegmentationwithawearablesolid-statelidarsensor": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "HIDA: Towards Holistic Indoor Understanding for the Visually Impaired via Semantic Instance Segmentation With a Wearable Solid-State LiDAR Sensor",
    "authors": [
      "Huayao Liu",
      "Ruiping Liu",
      "Kailun Yang",
      "Jiaming Zhang",
      "Kunyu Peng",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Liu_HIDA_Towards_Holistic_Indoor_Understanding_for_the_Visually_Impaired_via_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Liu_HIDA_Towards_Holistic_Indoor_Understanding_for_the_Visually_Impaired_via_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Independently exploring unknown spaces or finding objects in an indoor environment is a daily but challenging task for visually impaired people. However, common 2D assistive systems lack depth relationships between various objects, resulting in difficulty to obtain accurate spatial layout and relative positions of objects. To tackle these issues, we propose HIDA, a lightweight assistive system based on 3D point cloud instance segmentation with a solid-state LiDAR sensor, for holistic indoor detection and avoidance. Our entire system consists of three hardware components, two interactive functions (obstacle avoidance and object finding) and a voice user interface. Based on voice guidance, the point cloud from the most recent state of the changing indoor environment is captured through an on-site scanning performed by the user. In addition, we design a point cloud segmentation model with dual lightweight decoders for semantic and offset predictions, which satisfies the efficiency of the whole system. After the 3D instance segmentation, we post-process the segmented point cloud by removing outliers and projecting all points onto a top-view 2D map representation. The system integrates the information above and interacts with users intuitively by acoustic feedback. The proposed 3D instance segmentation model has achieved state-of-the-art performance on ScanNet v2 dataset. Comprehensive field tests with various tasks in a user study verify the usability and effectiveness of our system for assisting visually impaired people in holistic indoor understanding, obstacle avoidance and object search.",
    "code_link": ""
  },
  "iccv2021_acvr_trans4transefficienttransformerfortransparentobjectsegmentationtohelpvisuallyimpairedpeoplenavigateintherealworld": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "Trans4Trans: Efficient Transformer for Transparent Object Segmentation To Help Visually Impaired People Navigate in the Real World",
    "authors": [
      "Jiaming Zhang",
      "Kailun Yang",
      "Angela Constantinescu",
      "Kunyu Peng",
      "Karin M\u00fcller",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Zhang_Trans4Trans_Efficient_Transformer_for_Transparent_Object_Segmentation_To_Help_Visually_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Zhang_Trans4Trans_Efficient_Transformer_for_Transparent_Object_Segmentation_To_Help_Visually_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Common fully glazed facades and transparent objects present architectural barriers and impede the mobility of people with low vision or blindness, for instance, a path detected behind a glass door is inaccessible unless it is correctly perceived and reacted. However, segmenting these safety-critical objects is rarely covered by conventional assistive technologies. To tackle this issue, we construct a wearable system with a novel dual-head Transformer for Transparency (Trans4Trans) model, which is capable of segmenting general and transparent objects and performing real-time wayfinding to assist people walking alone more safely. Especially, both decoders created by our proposed Transformer Parsing Module (TPM) enable effective joint learning from different datasets. Besides, the efficient Trans4Trans model composed of symmetric transformer-based encoder and decoder, requires little computational expenses and is readily deployed on portable GPUs. Our Trans4Trans model outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various pre-tests and a user study conducted in indoor and outdoor scenarios, the usability and reliability of our assistive system have been extensively verified.",
    "code_link": ""
  },
  "iccv2021_acvr_efficientsearchinapanoramicimagedatabaseforlong-termvisuallocalization": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "Efficient Search in a Panoramic Image Database for Long-Term Visual Localization",
    "authors": [
      "Semih Orhan",
      "Yal\u0131n Ba\u015ftanlar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Orhan_Efficient_Search_in_a_Panoramic_Image_Database_for_Long-Term_Visual_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Orhan_Efficient_Search_in_a_Panoramic_Image_Database_for_Long-Term_Visual_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this work, we focus on a localization technique that is based on image retrieval. In this technique, database images are kept with GPS coordinates and the geographic location of the retrieved database image serves as an approximate position of the query image. In our scenario, database consists of panoramic images (e.g. Google Street View) and query images are collected with a standard field-of-view camera in a different time. While searching the match of a perspective query image in a panoramic image database, unlike previous studies, we do not generate a number of perspective images from the panoramic image. Instead, taking advantage of CNNs, we slide a search window in the last convolutional layer belonging to the panoramic image and compute the similarity with the descriptor extracted from the query image. In this way, more locations are visited in less amount of time. We conducted experiments with state-of-the-art descriptors and results reveal that the proposed sliding window approach reaches higher accuracy than generating 4 or 8 perspective images.",
    "code_link": ""
  },
  "iccv2021_acvr_deepembeddings-basedplacerecognitionrobusttomotionblur": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "Deep Embeddings-Based Place Recognition Robust to Motion Blur",
    "authors": [
      "Piotr Wozniak",
      "Bogdan Kwolek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Wozniak_Deep_Embeddings-Based_Place_Recognition_Robust_to_Motion_Blur_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Wozniak_Deep_Embeddings-Based_Place_Recognition_Robust_to_Motion_Blur_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this work we present an algorithm for severe (unknown) blur detection on RGB images. On salient CNN-based regional representations we calculate local features that are then fed to calibrated classifiers in order to estimate blur intensity. We perform scene classification and show that considerable gain in classification performance can be obtained owing to information on blur presence. We calculate global descriptors of the scene that are then fed to image retrieval engine that uses blur detection, scene category and minimum spanning tree to decide if current query image is relevant or irrelevant in context of place recognition. We show that information about blur and scene category improves mean average performance. We introduce a freely available challenging dataset both for blur detection and place recognition. It contains both images with severe blurs and sharp images with 6-DOF viewpoint variations, which were recorded using a humanoid robot.",
    "code_link": ""
  },
  "iccv2021_acvr_audi-exchangeai-guidedhand-basedactionstoassisthuman-humaninteractionsfortheblindandthevisuallyimpaired": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "Audi-Exchange: AI-Guided Hand-Based Actions To Assist Human-Human Interactions for the Blind and the Visually Impaired",
    "authors": [
      "Daohan Lu",
      "Yi Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Lu_Audi-Exchange_AI-Guided_Hand-Based_Actions_To_Assist_Human-Human_Interactions_for_the_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Lu_Audi-Exchange_AI-Guided_Hand-Based_Actions_To_Assist_Human-Human_Interactions_for_the_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Vision loss or low vision poses significant challenges to blind-or-visually-impaired (BVI) individuals when interacting with humans and objects. Although many apps and assistive devices can help them better interact with the environment and objects, the current state of assistive technology leaves human-human interaction needs of the BVI largely unaddressed. Because of this, we introduce a new wearable mobile assistive platform, named Audi-Exchange, to address part of the problem. Developed with mobile-optimized computer vision and audio engineering techniques, Audi-Exchange facilitates a specific area of human-human interaction by helping the BVI user accurately locate another person's hand with spatial audio in order to pass objects over to or receive objects from the other person. Audi-Exchange differs from existing academic and commercial assistive technologies in that it is intuitive to use and non-intrusive when worn. We conduct several experiments to investigate Audi-Exchange's effectiveness as an assistive human-human interaction tool and discover encouraging results.",
    "code_link": ""
  },
  "iccv2021_acvr_orb-slamwithnear-infraredimagesandopticalflowdata": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "ORB-SLAM With Near-Infrared Images and Optical Flow Data",
    "authors": [
      "Antonio Buemi",
      "Arcangelo Bruna",
      "Sylvain Petinot",
      "Nicolas Roux"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Buemi_ORB-SLAM_With_Near-Infrared_Images_and_Optical_Flow_Data_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Buemi_ORB-SLAM_With_Near-Infrared_Images_and_Optical_Flow_Data_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The algorithms designed to solve the Simultaneous Localization And Mapping (SLAM) problem have to be often executed on embedded platforms in order to become part of complex robotics systems. Despite the continuous growth of their computational capabilities, the embedded devices still have considerable limitations, especially in terms of memory. This paper presents a modified version of the well known ORB-SLAM algorithm which improves its performance thanks to the use of Hardware-generated Optical Flow (HW-OF). The ORB-SLAM has been modified in order to run into the Stereo-cam embedded system by STMicroelectronics. The Stereo-cam includes the VD56G3 sensor, able to provide Near Infrared (NIR) images and OF data computed by a hardware accelerator. The experiments showed an improvement of the ORB-SLAM performances in terms of memory consumption and frame rate.",
    "code_link": ""
  },
  "iccv2021_acvr_tofnestefficientnormalestimationfortime-of-flightdepthcameras": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "ToFNest: Efficient Normal Estimation for Time-of-Flight Depth Cameras",
    "authors": [
      "Szil\u00e1rd Moln\u00e1r",
      "Benjamin Kel\u00e9nyi",
      "Levente Tam\u00e1s"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Molnar_ToFNest_Efficient_Normal_Estimation_for_Time-of-Flight_Depth_Cameras_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Molnar_ToFNest_Efficient_Normal_Estimation_for_Time-of-Flight_Depth_Cameras_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this work, we propose an efficient normal estimation method for depth images acquired by Time-of-Flight (ToF) cameras based on feature pyramid networks (FPN). We perform the normal estimation starting from the 2D depth images, projecting the measured data into the 3D space and computing the loss function for the point cloud normal. Despite the simplicity of our method, which we call ToFNest, it proves to be efficient in terms of robustness and runtime. In order to validate ToFNest we performed extensive evaluations using both public and custom outdoor datasets. Compared with the state of the art methods, our algorithm is faster by an order of magnitude without losing precision on public datasets. The demo code, custom datasets and videos are available on the project website.",
    "code_link": "https://github.com/molnarszilard/ToFNest"
  },
  "iccv2021_acvr_exploitingegocentricvisiononshoppingcartforout-of-stockdetectioninretailenvironments": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Assistive Computer Vision and Robotics",
    "title": "Exploiting Egocentric Vision on Shopping Cart for Out-of-Stock Detection in Retail Environments",
    "authors": [
      "Dario Allegra",
      "Mattia Litrico",
      "Maria Ausilia Napoli Spatafora",
      "Filippo Stanco",
      "Giovanni Maria Farinella"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Allegra_Exploiting_Egocentric_Vision_on_Shopping_Cart_for_Out-of-Stock_Detection_in_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Allegra_Exploiting_Egocentric_Vision_on_Shopping_Cart_for_Out-of-Stock_Detection_in_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Continuous detection and efficient monitoring of Out-Of-Stock (OOS) of products in retail environments is a key factor to improve stores profits. Traditional methods require labour-intensive human work dedicated to checking for products to refill raising the requirement of automatic solutions to detect OOS. In this work, we focus on the problem of OOS detection from an egocentric perspective proposing a new weak annotation of the EgoCart dataset. We benchmark the considered challenge employing a deep learning approach for the detection of OOS areas. Specifically, we train a Convolutional Neural Network (CNN) to predict attention maps useful to find OOS in retail areas and hence suggest the retail employers where to intervene. We evaluate results with both objective measures and a subjective analysis provided by human which has reviewed the obtained OOS attention maps. The achieved performance demonstrates that the proposed pipeline is promising to help the refilling process in the retail domain.",
    "code_link": ""
  },
  "iccv2021_aim_improvingkeyhumanfeaturesforposetransfer": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Improving Key Human Features for Pose Transfer",
    "authors": [
      "Victor-Andrei Ivan",
      "Ionut Mistreanu",
      "Andrei Leica",
      "Sung-Jun Yoon",
      "Manri Cheon",
      "Junwoo Lee",
      "Jinsoo Oh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Ivan_Improving_Key_Human_Features_for_Pose_Transfer_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Ivan_Improving_Key_Human_Features_for_Pose_Transfer_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "It is still a great challenge in the Pose Transfer task to generate visually coherent images, to preserve the texture of clothes, to maintain the source identity and to realistically generate key human features such as the face or the hands. To tackle these challenges, we first conduct a study to obtain the most robust conditioning labels for this task and the baseline method [??] that we choose. We then improve upon the baseline by including deep source features from an Auto-encoder through an Attention mechanism. Finally we add region discriminators that are focused on key human features, thus obtaining results competitive with the state-of-the-art.",
    "code_link": ""
  },
  "iccv2021_aim_efficientwaveletboostlearning-basedmulti-stageprogressiverefinementnetworkforunderwaterimageenhancement": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Efficient Wavelet Boost Learning-Based Multi-Stage Progressive Refinement Network for Underwater Image Enhancement",
    "authors": [
      "Fushuo Huo",
      "Bingheng Li",
      "Xuegui Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Huo_Efficient_Wavelet_Boost_Learning-Based_Multi-Stage_Progressive_Refinement_Network_for_Underwater_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Huo_Efficient_Wavelet_Boost_Learning-Based_Multi-Stage_Progressive_Refinement_Network_for_Underwater_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Raw underwater images suffer from low contrast and color cast due to wavelength-selective light scattering and attenuation. The distortions in color and luminance mainly appear at the low frequency while that in edge and texture are mainly at the high frequency. However, the hybrid distortions are difficult to simultaneously recover for existing methods, which mainly focus on the spatial domain. To tackle these issues, we propose a novel deep learning network to progressively refine underwater images by wavelet boost learning strategy (PRWNet), both in spatial and frequency domains. Specifically, the Multi-stage refinement strategy is adopted to efficiently enhance the spatial-varying degradations in a coarse-to-fine way. For each refinement procedure, Wavelet Boost Learning (WBL) unit decomposes the hierarchical features into high and low frequency and enhances them respectively by normalization and attention mechanisms. The modified boosting strategy is also adopted in WBL to further enhance the feature representations. Extensive experiments show that our method achieves state-of-the-art results. Our network is efficient and has the potential for real-world applications. The code is available at: https://github.com/huofushuo/PRWNet.",
    "code_link": "https://github.com/huofushuo/PRWNet"
  },
  "iccv2021_aim_generalizedreal-worldsuper-resolutionthroughadversarialrobustness": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Generalized Real-World Super-Resolution Through Adversarial Robustness",
    "authors": [
      "Angela Castillo",
      "Mar\u00eda Escobar",
      "Juan C. P\u00e9rez",
      "Andr\u00e9s Romero",
      "Radu Timofte",
      "Luc Van Gool",
      "Pablo Arbelaez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Castillo_Generalized_Real-World_Super-Resolution_Through_Adversarial_Robustness_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Castillo_Generalized_Real-World_Super-Resolution_Through_Adversarial_Robustness_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Real-world Super-Resolution (SR) has been traditionally tackled by first learning a specific degradation model that resembles the noise and corruption artifacts in low-resolution imagery. Thus, current methods lack generalization and lose their accuracy when tested on unseen types of corruption. In contrast to the traditional proposal, we present Robust Super-Resolution (RSR), a method that leverages the generalization capability of adversarial attacks to tackle real-world SR. Our novel framework poses a paradigm shift in the development of real-world SR methods. Instead of learning a dataset-specific degradation, we employ adversarial attacks to create difficult examples that target the model's weaknesses. Afterward, we use these adversarial examples during training to improve our model's capacity to process noisy inputs. We perform extensive experimentation on synthetic and real-world images and empirically demonstrate that our RSR method generalizes well across datasets without re-training for specific noise priors. By using a single robust model, we outperform state-of-the-art specialized methods on real-world benchmarks.",
    "code_link": "https://github.com/BCV-Uniandes/RSR"
  },
  "iccv2021_aim_rethinkingcontentandstyleexploringbiasforunsuperviseddisentanglement": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Rethinking Content and Style: Exploring Bias for Unsupervised Disentanglement",
    "authors": [
      "Xuanchi Ren",
      "Tao Yang",
      "Yuwang Wang",
      "Wenjun Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Ren_Rethinking_Content_and_Style_Exploring_Bias_for_Unsupervised_Disentanglement_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Ren_Rethinking_Content_and_Style_Exploring_Bias_for_Unsupervised_Disentanglement_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Content and style (C-S) disentanglement intends to decompose the underlying explanatory factors of objects into two independent subspaces. From the unsupervised disentanglement perspective, we rethink content and style and propose a formulation for unsupervised C-S disentanglement based on our assumption that different factors are of different importance and popularity for image reconstruction, which serves as a data bias. The corresponding model inductive bias is introduced by our proposed C-S disentanglement Module (C-S DisMo), which assigns different and independent roles to content and style when approximating the real data distributions. Specifically, each content embedding from the dataset, which encodes the most dominant factors for image reconstruction, is assumed to be sampled from a shared distribution across the dataset. The style embedding for a particular image, encoding the remaining factors, is used to customize the shared distribution through an affine transformation. The experiments on several popular datasets demonstrate that our method achieves the state-of-the-art unsupervised C-S disentanglement, which is comparable or even better than supervised methods. We verify the effectiveness of our method by downstream tasks: image-to-image translation and single-view 3D reconstruction.",
    "code_link": ""
  },
  "iccv2021_aim_sparsetodensemotiontransferforfaceimageanimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Sparse to Dense Motion Transfer for Face Image Animation",
    "authors": [
      "Ruiqi Zhao",
      "Tianyi Wu",
      "Guodong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Zhao_Sparse_to_Dense_Motion_Transfer_for_Face_Image_Animation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Zhao_Sparse_to_Dense_Motion_Transfer_for_Face_Image_Animation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face image animation from a single image has achieved remarkable progress. However, it remains challenging when only sparse landmarks are available as the driving signal. Given a source face image and a sequence of sparse face landmarks, our goal is to generate a video of the face imitating the motion of landmarks. We develop an efficient and effective method for motion transfer from sparse landmarks to the face image. We then combine global and local motion estimation in a unified model to faithfully transfer the motion. The model can learn to segment the moving foreground from the background and generate not only global motion, such as rotation and translation of the face, but also subtle local motion such as the gaze change. We further improve face landmark detection on videos. With temporally better aligned landmark sequences for training, our method can generate temporally coherent videos with higher visual quality. Experiments suggest we achieve results comparable to the state-of-the-art image driven method on the same identity testing and better results on cross identity testing.",
    "code_link": ""
  },
  "iccv2021_aim_test-timeadaptationforsuper-resolutionyouonlyneedtooverfitonafewmoreimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Test-Time Adaptation for Super-Resolution: You Only Need to Overfit on a Few More Images",
    "authors": [
      "Mohammad Saeed Rad",
      "Thomas Yu",
      "Behzad Bozorgtabar",
      "Jean-Philippe Thiran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Rad_Test-Time_Adaptation_for_Super-Resolution_You_Only_Need_to_Overfit_on_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Rad_Test-Time_Adaptation_for_Super-Resolution_You_Only_Need_to_Overfit_on_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Existing reference (RF)-based super-resolution (SR) models try to improve perceptual quality in SR under the assumption of the availability of high-resolution RF images paired with low-resolution (LR) inputs at testing. As the RF images should be similar in terms of content, colors, contrast, etc. to the test image, this hinders the applicability in a real scenario. Other approaches to increase the perceptual quality of images, including perceptual loss and adversarial losses, tend to dramatically decrease fidelity to the ground-truth through significant decreases in PSNR/SSIM. Addressing both issues, we propose a simple yet universal approach to improve the perceptual quality of the HR prediction from a pre-trained SR network on a given LR input by further fine-tuning the SR network on a subset of images from the training dataset with similar patterns of activation as the initial HR prediction, with respect to the filters of a feature extractor. In particular, we show the effects of fine-tuning on these images in terms of the perceptual quality and PSNR/SSIM values. Contrary to perceptually driven approaches, we demonstrate that the fine-tuned network produces a HR prediction with both greater perceptual quality and minimal changes to the PSNR/SSIM with respect to the initial HR prediction. Further, we present novel numerical experiments concerning the filters of SR networks, where we show through filter correlation, that the filters of the fine-tuned network from our method are closer to \"ideal\" filters, than those of the baseline network or a network fine-tuned on random images.",
    "code_link": ""
  },
  "iccv2021_aim_asystemforfusingcolorandnear-infraredimagesinradiancedomain": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "A System for Fusing Color and Near-Infrared Images in Radiance Domain",
    "authors": [
      "Kim C Ng",
      "Jinglin Shen",
      "Chiu Man Ho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Ng_A_System_for_Fusing_Color_and_Near-Infrared_Images_in_Radiance_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Ng_A_System_for_Fusing_Color_and_Near-Infrared_Images_in_Radiance_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We designed and demonstrated a system that fused color and near-infrared (NIR) images in the radiance domain. The system is designed to enhance image quality captured in outdoor environments, especially in hazy weather conditions. Previous dehazing methods based on RGB-NIR fusion exist but have rarely addressed the issue of color fidelity and potential see-through effect of fusing with NIR image. The proposed system can dehaze and enhance image details while maintaining the color fidelity and protect privacy. By working in the radiance domain, the system could handle large brightness differences among the color and NIR images and achieve High Dynamic Range (HDR). We proposed two methods to correct the fusion color: linear scalings when raw images were used and color swapping with base-detail image decomposition in the presence of nonlinearity in the ISP pipeline. The system also had two clothing see-through prevention mechanisms to avoid ethical issue arising from the see-through effect of NIR image.",
    "code_link": ""
  },
  "iccv2021_aim_graph2pixagraph-basedimagetoimagetranslationframework": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Graph2Pix: A Graph-Based Image to Image Translation Framework",
    "authors": [
      "Dilara Gokay",
      "Enis Simsar",
      "Efehan Atici",
      "Alper Ahmetoglu",
      "Atif Emre Yuksel",
      "Pinar Yanardag"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Gokay_Graph2Pix_A_Graph-Based_Image_to_Image_Translation_Framework_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Gokay_Graph2Pix_A_Graph-Based_Image_to_Image_Translation_Framework_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we propose a graph-based image-to-image translation framework for generating images. We use rich data collected from the popular creativity platform Artbreeder (http://artbreeder.com), where users interpolate multiple GAN-generated images to create artworks. This unique approach of creating new images leads to a tree-like structure where one can track historical data about the creation of a particular image. Inspired by this structure, we propose a novel graph-to-image translation model called Graph2Pix, which takes a graph and corresponding images as input and generates a single image as output. Our experiments show that Graph2Pix is able to outperform several image-to-image translation frameworks on benchmark metrics, including LPIPS (with a 25% improvement) and human perception studies (n=60), where users preferred the images generated by our method 81.5% of the time. Our source code and dataset are publicly available at https://github.com/catlab-team/graph2pix.",
    "code_link": "https://github.com/catlab-team/graph2pix"
  },
  "iccv2021_aim_highperceptualqualityimagedenoisingwithaposteriorsamplingcgan": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "High Perceptual Quality Image Denoising With a Posterior Sampling CGAN",
    "authors": [
      "Guy Ohayon",
      "Theo Adrai",
      "Gregory Vaksman",
      "Michael Elad",
      "Peyman Milanfar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Ohayon_High_Perceptual_Quality_Image_Denoising_With_a_Posterior_Sampling_CGAN_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Ohayon_High_Perceptual_Quality_Image_Denoising_With_a_Posterior_Sampling_CGAN_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The vast work in Deep Learning (DL) has led to a leap in image denoising research. Most DL solutions for this task have chosen to put their efforts on the denoiser's architecture while maximizing distortion performance. However, distortion driven solutions lead to blurry results with sub-optimal perceptual quality, especially in immoderate noise levels. In this paper we propose a different perspective, aiming to produce sharp and visually pleasing denoised images that are still faithful to their clean sources. Formally, our goal is to achieve high perceptual quality with acceptable distortion. This is attained by a stochastic denoiser that samples from the posterior distribution, trained as a generator in the framework of conditional generative adversarial networks (CGAN). Contrary to distortion-based regularization terms that conflict with perceptual quality, we introduce to the CGAN objective a theoretically founded penalty term that does not force a distortion requirement on individual samples, but rather on their mean. We showcase our proposed method with a novel denoiser architecture that achieves the reformed denoising goal and produces vivid and diverse outcomes in immoderate noise levels.",
    "code_link": "https://github.com/mseitzer/pytorch-fid"
  },
  "iccv2021_aim_stochasticimagedenoisingbysamplingfromtheposteriordistribution": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Stochastic Image Denoising by Sampling From the Posterior Distribution",
    "authors": [
      "Bahjat Kawar",
      "Gregory Vaksman",
      "Michael Elad"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Kawar_Stochastic_Image_Denoising_by_Sampling_From_the_Posterior_Distribution_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Kawar_Stochastic_Image_Denoising_by_Sampling_From_the_Posterior_Distribution_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Image denoising is a well-known and well studied problem, commonly targeting a minimization of the mean squared error (MSE) between the outcome and the original image. Unfortunately, especially for severe noise levels, such Minimum MSE (MMSE) solutions may lead to blurry output images. In this work we propose a novel stochastic denoising approach that produces viable and high perceptual quality results, while maintaining a small MSE. Our method employs Langevin dynamics that relies on a repeated application of any given MMSE denoiser, obtaining the reconstructed image by effectively sampling from the posterior distribution. Due to its stochasticity, the proposed algorithm can produce a variety of high-quality outputs for a given noisy input, all shown to be legitimate denoising results. In addition, we present an extension of our algorithm for handling the inpainting problem, recovering missing pixels while removing noise from partially given data.",
    "code_link": ""
  },
  "iccv2021_aim_saliency-guidedtransformernetworkcombinedwithlocalembeddingforno-referenceimagequalityassessment": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Saliency-Guided Transformer Network Combined With Local Embedding for No-Reference Image Quality Assessment",
    "authors": [
      "Mengmeng Zhu",
      "Guanqun Hou",
      "Xinjia Chen",
      "Jiaxing Xie",
      "Haixian Lu",
      "Jun Che"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Zhu_Saliency-Guided_Transformer_Network_Combined_With_Local_Embedding_for_No-Reference_Image_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Zhu_Saliency-Guided_Transformer_Network_Combined_With_Local_Embedding_for_No-Reference_Image_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "No-Reference Image Quality Assessment (NR-IQA) methods based on Vision Transformer have recently drawn much attention for their superior performance. Unfortunately, being a crude combination of NR-IQA and Transformer, they can hardly take the advantage of their attributes. In this paper, we propose a novel Saliency-Guided Transformer Network combined with Local Embedding (TranSLA) for No-Reference Image Quality Assessment. Our TranSLA integrates multi-level information for a robust representation. Existed researches have shown that the human vision system concentrates more on the Region-of-interest (RoI) when assessing the image quality. Thus we combine saliency prediction with Transformer to guide the model highlight the RoI when aggregating the global information. Besides, we import local embedding for Transformer with gradient map. Since the gradient map focuses on extracting structured feature in detail, it can be used as a supplement to offer local information for Transformer. Then, the local and non-local information can be utilized. Moreover, to accelerate the aggregation of information from all tokens, we introduce a Boosting Interaction Module (BIM) to enhance feature aggregation. BIM forces patch tokens to interact better with class tokens at all levels. Experiments on two large-scale NR-IQA benchmarks demonstrate that our method significantly outperforms the state-of-the-arts.",
    "code_link": ""
  },
  "iccv2021_aim_reducingnoisepixelsandmetricbiasinsemanticinpaintingonsegmentationmap": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Reducing Noise Pixels and Metric Bias in Semantic Inpainting on Segmentation Map",
    "authors": [
      "Jianfeng He",
      "Bei Xiao",
      "Xuchao Zhang",
      "Shuo Lei",
      "Shuhui Wang",
      "Chang-Tien Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/He_Reducing_Noise_Pixels_and_Metric_Bias_in_Semantic_Inpainting_on_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/He_Reducing_Noise_Pixels_and_Metric_Bias_in_Semantic_Inpainting_on_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Semantic Inpainting on Segmentation Map (SISM) aims to manipulate segmentation maps by semantics. Providing structural assistance, segmentation maps have been broadly used as intermediate interfaces to achieve better image manipulation. We improve the SISM by considering the unique characteristics of segmentation maps in the both training and testing processes.First, to improve SISM training process, we reduce the noise pixels, which are pixel artifacts from the generation. Because each pixel in the segmentation maps has a much smaller value range in comparison to pixels in natural images, we propose a novel denoise activation (DA) by estimating the possible pixel values for an inpainted area in advance. Second, we improve SISM testing process by reducing the metric bias. The bias is caused by the ignore of latent ground truths in the current metrics in SISM. Based on the analysis of possible latent ground truths, we then propose a novel metric, Semantic Similarity (Sem), to quantify the semantic divergence between the generated and ground-truth target objects. Sem is calculated by a pre-trained semantic classifier using object shapes as training data. Since the classifier is pre-trained on PS-COCO dataset, with a large number of training samples and relatively general classes, Sem is also applicable to other datasets. Our experiments show impressive results of DA and Sem on three datasets.",
    "code_link": ""
  },
  "iccv2021_aim_real-esrgantrainingreal-worldblindsuper-resolutionwithpuresyntheticdata": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution With Pure Synthetic Data",
    "authors": [
      "Xintao Wang",
      "Liangbin Xie",
      "Chao Dong",
      "Ying Shan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Wang_Real-ESRGAN_Training_Real-World_Blind_Super-Resolution_With_Pure_Synthetic_Data_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Wang_Real-ESRGAN_Training_Real-World_Blind_Super-Resolution_With_Pure_Synthetic_Data_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly. Code: https://github.com/xinntao/Real-ESRGAN",
    "code_link": "https://github.com/xinntao/Real-ESRGAN"
  },
  "iccv2021_aim_deepfakemnist+adeepfakefacialanimationdataset": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "DeepFake MNIST+: A DeepFake Facial Animation Dataset",
    "authors": [
      "Jiajun Huang",
      "Xueyu Wang",
      "Bo Du",
      "Pei Du",
      "Chang Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Huang_DeepFake_MNIST_A_DeepFake_Facial_Animation_Dataset_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Huang_DeepFake_MNIST_A_DeepFake_Facial_Animation_Dataset_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The DeepFakes, which are the facial manipulation techniques, is the emerging threat to digital society. Various DeepFake detection methods and datasets are proposed for detecting such data, especially for face-swapping. However, recent researches less consider facial animation, which is also important in the DeepFake attack side. It tries to animate a face image with actions provided by a driving video, which also leads to a concern about the security of recent payment systems that reply on liveness detection to authenticate real users via recognising a sequence of user facial actions. However, our experiments show that the existed datasets are not sufficient to develop reliable detection methods. While the current liveness detector cannot defend such videos as the attack. As a response, we propose a new human face animation dataset, called DeepFake MNIST+, generated by a SOTA image animation generator. It includes 10,000 facial animation videos in ten different actions, which can spoof the recent liveness detectors. A baseline detection method and a comprehensive analysis of the method is also included in this paper. In addition, we analyze the proposed dataset's properties and reveal the difficulty and importance of detecting animation datasets under different types of motion and compression quality.",
    "code_link": ""
  },
  "iccv2021_aim_unsupervisedgenerativeadversarialnetworkswithcross-modelweighttransfermechanismforimage-to-imagetranslation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Unsupervised Generative Adversarial Networks With Cross-Model Weight Transfer Mechanism for Image-to-Image Translation",
    "authors": [
      "Xuguang Lai",
      "Xiuxiu Bai",
      "Yongqiang Hao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Lai_Unsupervised_Generative_Adversarial_Networks_With_Cross-Model_Weight_Transfer_Mechanism_for_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Lai_Unsupervised_Generative_Adversarial_Networks_With_Cross-Model_Weight_Transfer_Mechanism_for_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Image-to-image translation covers a variety of application scenarios in reality, and is one of the key research directions in computer vision. However, due to the defects of GAN, current translation frameworks may encounter model collapse and low quality of generated images. To solve the above problems, this paper proposes a new model CWT-GAN, which introduces the cross-model weight transfer mechanism. The discriminator of CWT-GAN has the same encoding module structure as the generator's. In the training process, the discriminator will transmit the weight of its encoding module to the generator in a certain proportion after each weight update. CWT-GAN can generate diverse and higher-quality images with the aid of the weight transfer mechanism, since features learned by discriminator tend to be more expressive than those learned by generator trained via maximum likelihood. Extensive experiments demonstrate that our CWT-GAN performs better than the state-of-the-art methods in a single translation direction for several datasets.",
    "code_link": "https://github.com/lxg0387/CWT-GAN"
  },
  "iccv2021_aim_distillingreflectiondynamicsforsingle-imagereflectionremoval": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Distilling Reflection Dynamics for Single-Image Reflection Removal",
    "authors": [
      "Quanlong Zheng",
      "Xiaotian Qiao",
      "Ying Cao",
      "Shi Guo",
      "Lei Zhang",
      "Rynson W.H. Lau"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Zheng_Distilling_Reflection_Dynamics_for_Single-Image_Reflection_Removal_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Zheng_Distilling_Reflection_Dynamics_for_Single-Image_Reflection_Removal_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Single-image reflection removal (SIRR) aims to restore the transmitted image given a single image shot through glass or window. Existing methods rely mainly on information extracted from a single image along with some pre-defined priors, and fail to give satisfying results on real-world images, due to inherent ambiguity and lack of large and diverse real-world training data. In this paper, instead of reasoning about a single image only, we propose to distill a representation of reflection dynamics from multi-view images (i.e., the motions of reflection and transmission layers over time), and transfer the learned knowledge for the SIRR problem. In particular, we propose a teacher-student framework where the teacher network learns a representation of reflection dynamics by watching a sequence of multi-view images of a scene captured by a moving camera and teaches a student network to remove reflection from a single input image. In addition, we collect a large real-world multi-view reflection image dataset for reflection dynamics knowledge distillation. Extensive experiments show that our model yields state-of-the-art performances.",
    "code_link": ""
  },
  "iccv2021_aim_manipulatingimagestyletransformationvialatent-spacesvm": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Manipulating Image Style Transformation via Latent-Space SVM",
    "authors": [
      "Qiudan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Wang_Manipulating_Image_Style_Transformation_via_Latent-Space_SVM_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Wang_Manipulating_Image_Style_Transformation_via_Latent-Space_SVM_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep Neural Networks have been proved as the go-to approach in modeling data distribution in a latent space, especially in Neural Style Transfer (NST), which casts a specific style extracted from a source image to another target image by calibrating the style and content information in a latent space. While existing methods focuses on different ways to extract features that more precisely describe style or content information to improve existing NST pipelines, the latent space of the NST model has not been well-explored. In this paper, we show that different half-spaces in the latent space are actually associated with particular styles of a network's generated images. The corresponding constraints of these half-spaces can be computed by using linear classifiers, e.g. a Support Vector Machines (SVM). Leveraging the understanding of the relation between half-spaces in the latent space and output style, we propose the Linear Modification for Latent Representations (LMLR), a method that effectively increases or decreases the level of stylizing in the output image for any given NST model. We empirically evaluate our method on several state-of-the-art NST models and show that LMLR can manipulate the level of stylizing in the output image.",
    "code_link": ""
  },
  "iccv2021_aim_simpleandefficientunpairedreal-worldsuper-resolutionusingimagestatistics": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Simple and Efficient Unpaired Real-World Super-Resolution Using Image Statistics",
    "authors": [
      "Kwangjin Yoon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Yoon_Simple_and_Efficient_Unpaired_Real-World_Super-Resolution_Using_Image_Statistics_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Yoon_Simple_and_Efficient_Unpaired_Real-World_Super-Resolution_Using_Image_Statistics_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Learning super-resolution (SR) network without the paired low resolution (LR) and high resolution (HR) image is difficult because direct supervision through the corresponding HR counterpart is unavailable. Recently, many real-world SR researches take advantage of the unpaired image-to-image translation technique. That is, they used two or more generative adversarial networks (GANs), each of which translates images from one domain to another domain, e.g., translates images from the HR domain to the LR domain. However, it is not easy to stably learn such a translation with GANs using unpaired data. In this study, we present a simple and efficient method of training of real-world SR network. To stably train the network, we use statistics of an image patch, such as means and variances. Our real-world SR framework consists of two GANs, one for translating HR images to LR images (degradation task) and the other for translating LR to HR (SR task). We argue that the unpaired image translation using GANs can be learned efficiently with our proposed data sampling strategy, namely, variance matching. We test our method on the NTIRE 2020 real-world SR dataset. Our method outperforms the current state-of-the-art method in terms of the SSIM metric as well as produces comparable results on the LPIPS metric.",
    "code_link": ""
  },
  "iccv2021_aim_underwaterimagecolorcorrectionusingensemblecolorizationnetwork": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Underwater Image Color Correction Using Ensemble Colorization Network",
    "authors": [
      "Arpit Pipara",
      "Urvi Oza",
      "Srimanta Mandal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Pipara_Underwater_Image_Color_Correction_Using_Ensemble_Colorization_Network_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Pipara_Underwater_Image_Color_Correction_Using_Ensemble_Colorization_Network_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Underwater image color correction has been gaining traction due to its usage in marine biology and surveillance. Color corrected images also help marine archaeologists in locating objects. The underwater image suffers from various degradation with respect to the depth at which the image is taken. In this paper, we propose an alternate path to correct the color of the underwater images. We address the problem of underwater image color correction as a colorization task. For this purpose, we propose a deep learning architecture that comprises of an ensemble encoder and a decoder. The ensemble encoder part uses pre-trained networks to extract multi-level features. These features are then fused together and are used up by the decoder to generate the color corrected output. We evaluate the performance of our model using reference-based as well as no reference-based metrics. The metrics indicate that the produced results are inline with the human perceptual system.",
    "code_link": ""
  },
  "iccv2021_aim_contrastivefeaturelossforimageprediction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "Contrastive Feature Loss for Image Prediction",
    "authors": [
      "Alex Andonian",
      "Taesung Park",
      "Bryan Russell",
      "Phillip Isola",
      "Jun-Yan Zhu",
      "Richard Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Andonian_Contrastive_Feature_Loss_for_Image_Prediction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Andonian_Contrastive_Feature_Loss_for_Image_Prediction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Training supervised image synthesis models requires a critic to compare two images: the ground truth to the result. Yet, this basic functionality remains an open problem. A popular line of approaches uses the L1 (mean absolute error) loss, either in the pixel or the feature space of pretrained deep networks. However, we observe that these losses tend to produce overly blurry and grey images, and other techniques such as GANs need to be employed to fight these artifacts. In this work, we introduce an information theory based approach to measuring similarity between two images. We argue that a good reconstruction should have high mutual information with the ground truth. This view enables learning a lightweight critic to \"calibrate\" a feature space in a contrastive manner, such that reconstructions of corresponding spatial patches are brought together, while other patches are repulsed. We show that our formulation immediately boosts the perceptual realism of output images when used as a drop-in replacement for the L1 loss, with or without an additional GAN loss.",
    "code_link": ""
  },
  "iccv2021_aim_sdwnetastraightdilatednetworkwithwavelettransformationforimagedeblurring": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "SDWNet: A Straight Dilated Network With Wavelet Transformation for Image Deblurring",
    "authors": [
      "Wenbin Zou",
      "Mingchao Jiang",
      "Yunchen Zhang",
      "Liang Chen",
      "Zhiyong Lu",
      "Yi Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Zou_SDWNet_A_Straight_Dilated_Network_With_Wavelet_Transformation_for_Image_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Zou_SDWNet_A_Straight_Dilated_Network_With_Wavelet_Transformation_for_Image_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Image deblurring is a classical computer vision problem that aims to recover a sharp image from a blurred image. To solve this problem, existing methods apply the Encode-Decode architecture to design the complex networks to make a good performance. However, most of these methods use repeated up-sampling and down-sampling structures to expand the receptive field, which results in texture information loss during the sampling process and some of them design the multiple stages that lead to difficulties with convergence. Therefore, our model uses dilated convolution to enable the obtainment of the large receptive field with high spatial resolution. Through making full use of the different receptive fields, our method can achieve better performance. On this basis, we reduce the number of up-sampling and down-sampling and design a simple network structure. Besides, we propose a novel module using the wavelet transform, which effectively helps the network to recover clear high-frequency texture details. Qualitative and quantitative evaluations of real and synthetic datasets show that our deblurring method is comparable to existing algorithms in terms of performance with much lower training requirements. The source code and pre-trained models are available at https://github.com/FlyEgle/SDWNet.",
    "code_link": "https://github.com/FlyEgle/SDWNet"
  },
  "iccv2021_aim_smilesemantically-guidedmulti-attributeimageandlayoutediting": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "SMILE: Semantically-Guided Multi-Attribute Image and Layout Editing",
    "authors": [
      "Andr\u00e9s Romero",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Romero_SMILE_Semantically-Guided_Multi-Attribute_Image_and_Layout_Editing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Romero_SMILE_Semantically-Guided_Multi-Attribute_Image_and_Layout_Editing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Attribute image manipulation has been a very active topic since the introduction of Generative Adversarial Networks (GANs). Exploring the disentangled attribute space within a transformation is a very challenging task due to the multiple and mutually-inclusive nature of the facial images, where different labels (eyeglasses, hats, hair, identity, etc.) can co-exist at the same time. Several works address this issue either by exploiting the modality of each domain/attribute using a conditional random vector noise, or extracting the modality from an exemplary image. However, existing methods cannot handle both random and reference transformations for multiple attributes, which limits the generality of the solutions. In this paper, we successfully exploit a multimodal representation that handles all attributes, be it guided by random noise or exemplar images, while only using the underlying domain information of the target domain. We present extensive qualitative and quantitative results for facial datasets and several different attributes that show the superiority of our method. Additionally, our method is capable of adding, removing or changing either fine-grained or coarse attributes by using an image as a reference or by exploring the style distribution space, and it can be easily extended to head-swapping and face-reenactment applications without being trained on videos.",
    "code_link": ""
  },
  "iccv2021_aim_swinirimagerestorationusingswintransformer": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Advances in Image Manipulation",
    "title": "SwinIR: Image Restoration Using Swin Transformer",
    "authors": [
      "Jingyun Liang",
      "Jiezhang Cao",
      "Guolei Sun",
      "Kai Zhang",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14 0.45dB, while the total number of parameters can be reduced by up to 67%.",
    "code_link": ""
  },
  "iccv2021_struco3d_mrganmulti-rooted3dshaperepresentationlearningwithunsupervisedpartdisentanglement": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "StruCo3D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Structural and Compositional Learning on 3D Data",
    "title": "MRGAN: Multi-Rooted 3D Shape Representation Learning With Unsupervised Part Disentanglement",
    "authors": [
      "Rinon Gal",
      "Amit Bermano",
      "Hao Zhang",
      "Daniel Cohen-Or"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/StruCo3D/html/Gal_MRGAN_Multi-Rooted_3D_Shape_Representation_Learning_With_Unsupervised_Part_Disentanglement_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/StruCo3D/papers/Gal_MRGAN_Multi-Rooted_3D_Shape_Representation_Learning_With_Unsupervised_Part_Disentanglement_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We introduce MRGAN, or multi-rooted GAN, the first generative adversarial network to learn a part-disentangled 3D shape representation without any part supervision. The network fuses multiple branches of tree-structured graph convolution layers which produce point clouds in a controllable manner. Specifically, each branch learns to grow a different shape part, offering control over the shape generation at the part level. Our network encourages disentangled generation of semantic parts via two key ingredients: a root-mixing training strategy which helps decorrelate the different branches to facilitate disentanglement, and a set of loss terms designed with part disentanglement and shape semantics in mind. Of these, a novel convexity loss incentivizes the generation of parts that are more convex, as semantic parts tend to be. In addition, a root-dropping loss further ensures that each root seeds a single part, preventing the degeneration or over-growth of the point-producing branches. We evaluate the performance of our network on a number of 3D shape classes, and offer qualitative and quantitative comparisons to previous works and baseline approaches. We demonstrate the controllability offered by our part-disentangled representation through two applications for shape modeling: part mixing and individual part variation, without receiving segmented shapes as input.",
    "code_link": ""
  },
  "iccv2021_struco3d_abd-netattentionbaseddecompositionnetworkfor3dpointclouddecomposition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "StruCo3D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Structural and Compositional Learning on 3D Data",
    "title": "ABD-Net: Attention Based Decomposition Network for 3D Point Cloud Decomposition",
    "authors": [
      "Siddharth Katageri",
      "Shashidhar V Kudari",
      "Akshaykumar Gunari",
      "Ramesh Ashok Tabib",
      "Uma Mudenagudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/StruCo3D/html/Katageri_ABD-Net_Attention_Based_Decomposition_Network_for_3D_Point_Cloud_Decomposition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/StruCo3D/papers/Katageri_ABD-Net_Attention_Based_Decomposition_Network_for_3D_Point_Cloud_Decomposition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we propose Attention Based Decomposition Network (ABD-Net), for point cloud decomposition into basic geometric shapes namely, plane, sphere, cone and cylinder. We show improved performance of 3D object classification using attention features based on primitive shapes in point clouds. Point clouds, being the simple and compact representation of 3D objects have gained increasing popularity. They demand robust methods for feature extraction due to unorderness in point sets. In ABD-Net the proposed Local Proximity Encapsulator captures the local geometric variations along with spatial encoding around each point from the input point sets. The encapsulated local features are further passed to proposed Attention Feature Encoder to learn basic shapes in point cloud. Attention Feature Encoder models geometric relationship between the neighborhoods of all the points resulting in capturing global point cloud information. We demonstrate the results of our proposed ABD-Net on ANSI mechanical component and ModelNet40 datasets. We also demonstrate the effectiveness of ABD-Net over the acquired attention features by improving the performance of 3D object classification on ModelNet40 benchmark dataset and compare them with state-of-the-art techniques.",
    "code_link": ""
  },
  "iccv2021_struco3d_3dsceneanglesusinguldecompositionofplanarhomography": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "StruCo3D",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Structural and Compositional Learning on 3D Data",
    "title": "3D Scene Angles Using UL Decomposition of Planar Homography",
    "authors": [
      "Pinak Paliwal",
      "Vikas Paliwal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/StruCo3D/html/Paliwal_3D_Scene_Angles_Using_UL_Decomposition_of_Planar_Homography_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/StruCo3D/papers/Paliwal_3D_Scene_Angles_Using_UL_Decomposition_of_Planar_Homography_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Proctoring during online exams often requires students to be under surveillance from a side pose and there is a strong need to estimate the side camera's relative position with respect to student's computer screen. This work uses edge and line detectors to extract the computer screen's boundaries and estimates homography with respect to rectangular shape with corresponding aspect ratio as in a normal view. A novel Upper-Lower Decomposition of Homography (ULDH) algorithm is proposed that calculates the polar and azimuthal angles with less than 5^o mean errors and can help distinguish bad camera placements from good ones with good precision. A purpose-built dataset is created and validated for this purpose and the software for key parts of image processing pipeline is made available for remote proctoring purposes",
    "code_link": ""
  },
  "iccv2021_seai_spaceasimulatorforphysicalinteractionsandcausallearningin3denvironments": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SEAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Simulation Technology for Embodied AI",
    "title": "SPACE: A Simulator for Physical Interactions and Causal Learning in 3D Environments",
    "authors": [
      "Jiafei Duan",
      "Samson Yu",
      "Cheston Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SEAI/html/Duan_SPACE_A_Simulator_for_Physical_Interactions_and_Causal_Learning_in_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SEAI/papers/Duan_SPACE_A_Simulator_for_Physical_Interactions_and_Causal_Learning_in_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recent advancements in deep learning, computer vision, and embodied AI have given rise to synthetic causal reasoning video datasets. These datasets facilitate the development of AI algorithms that can reason about physical interactions between objects. However, datasets thus far have primarily focused on elementary physical events such as rolling or falling. There is currently a scarcity of datasets that focus on the physical interactions that humans perform daily with objects in the real world. To address this scarcity, we introduce SPACE: A Simulator for Physical Interactions and Causal Learning in 3D Environments. The SPACE simulator allows us to generate the SPACE dataset, a synthetic video dataset in a 3D environment, to systematically evaluate physics-based models on a range of physical causal reasoning tasks. Inspired by daily object interactions, the SPACE dataset comprises videos depicting three types of physical events: containment, stability and contact. These events make up the vast majority of the basic physical interactions between objects. We then further evaluate it with a state-of-the-art physics-based deep model and show that the SPACE dataset improves the learning of intuitive physics with an approach inspired by curriculum learning. Repository: https://github.com/jiafei1224/SPACE",
    "code_link": "https://github.com/jiafei1224/SPACE"
  },
  "iccv2021_dlgc_evaluationoflatentspacelearningwithprocedurally-generateddatasetsofshapes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "Evaluation of Latent Space Learning With Procedurally-Generated Datasets of Shapes",
    "authors": [
      "Sharjeel Ali",
      "Oliver van Kaick"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Ali_Evaluation_of_Latent_Space_Learning_With_Procedurally-Generated_Datasets_of_Shapes_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Ali_Evaluation_of_Latent_Space_Learning_With_Procedurally-Generated_Datasets_of_Shapes_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We compare the quality of latent spaces learned by different neural network models for organizing collections of 3D shapes. To accomplish this goal, our first contribution is to introduce a synthetic dataset of shapes with known semantic attributes. We use a procedural method to generate a dataset comprising four categories, with a total of over 10,000 shapes, providing a controlled setting for studying the properties of latent spaces. In contrast to previous work, the synthetic shapes generated with our method have a more realistic appearance, similar to objects in manually-modeled collections. We use 8,800 shapes from the generated dataset to perform a quantitative and qualitative evaluation of the latent spaces learned with a set of representative neural network models. Our second contribution is to perform the quantitative evaluation with measures that we developed for numerically assessing the properties of the latent spaces, which allow us to objectively compare different models based on statistics computed on large sets of shapes.",
    "code_link": ""
  },
  "iccv2021_dlgc_distanceandedgetransformforskeletonextraction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "Distance and Edge Transform for Skeleton Extraction",
    "authors": [
      "Xiaojun Tang",
      "Rui Zheng",
      "Yinghao Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Tang_Distance_and_Edge_Transform_for_Skeleton_Extraction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Tang_Distance_and_Edge_Transform_for_Skeleton_Extraction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The shape skeleton or medial axis, is a concise shape description, defined by the centers of the maximally inscribed circles. Skeletonization algorithms support many applications, including optical character recognition, object recognition, pose estimation, shape matching, biomedical image analysis, etc. Usually classical algorithms tend to produce redundant skeleton branches at edge noise regions and require a branch pruning post process. Recently many CNN based algorithms achieved significant performance improvements compared with classical algorithms. Most deep learning algorithms directly used the shape image as input data and it's complex for end to end learning algorithms to fit the transformation from shape to skeleton. In this work, we proposed to use Smooth Distance Estimation (SDE) and Edge transformation to preprocess the input shape. Combined with a modified U-Net model and multiple models ensemble, the proposed method achieved 0.8129 F1 score in the Pixel SkelNetOn validation set, 1.5752 symmetric chamfer distance in the Point SkelNetOn validation set and 6407.4 squared distance score in the Parametric SkelNetOn validation set.",
    "code_link": ""
  },
  "iccv2021_dlgc_3dshapeslocalgeometrycodeslearningwithsdf": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "3D Shapes Local Geometry Codes Learning With SDF",
    "authors": [
      "Shun Yao",
      "Fei Yang",
      "Yongmei Cheng",
      "Mikhail G. Mozerov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Yao_3D_Shapes_Local_Geometry_Codes_Learning_With_SDF_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Yao_3D_Shapes_Local_Geometry_Codes_Learning_With_SDF_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A signed distance function (SDF) as the 3D shape description is one of the most effective approaches to represent 3D geometry for rendering and reconstruction. Our work is inspired by the state-of-the-art method DeepSDF that learns and analyzes the 3D shape as the iso-surface of its shell and this method has shown promising results especially in the 3D shape reconstruction and compression domain. In this paper, we consider the degeneration problem of reconstruction coming from the capacity decrease of the DeepSDF model, which approximates the SDF with a neural network and a single latent code. We propose Local Geometry Code Learning (LGCL), a model that improves the original DeepSDF results by learning from a local shape geometry of the full 3D shape. We add an extra graph neural network to split the single transmittable latent code into a set of local latent codes distributed on the 3D shape. Mentioned latent codes are used to approximate the SDF in their local regions, which will alleviate the complexity of the approximation compared to the original DeepSDF. Furthermore, we introduce a new geometric loss function to facilitate the training of these local latent codes. Note that other local shape adjusting methods use the 3D voxel representation, which in turn is a problem highly difficult to solve or even is insolvable. In contrast, our architecture is based on graph processing implicitly and performs the learning regression process directly in the latent code space, thus make the proposed architecture more flexible and also simple for realization. Our experiments on 3D shape reconstruction demonstrate that our LGCL method can keep more details with a significantly smaller size of the SDF decoder and outperforms considerably the original DeepSDF method under the most important quantitative metrics.",
    "code_link": ""
  },
  "iccv2021_dlgc_skeletonnetv2adensechannelattentionblocksforskeletonextraction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "SkeletonNetV2: A Dense Channel Attention Blocks for Skeleton Extraction",
    "authors": [
      "Sabari Nathan",
      "Priya Kansal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Nathan_SkeletonNetV2_A_Dense_Channel_Attention_Blocks_for_Skeleton_Extraction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Nathan_SkeletonNetV2_A_Dense_Channel_Attention_Blocks_for_Skeleton_Extraction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Geometrical analysis of a shape through skeletonization has some of very important high- and low-level application which includes tracking, manipulation, retrieval, representation, registration, recognition, and compression. The task of skeletonization is defined as the generation of the medial axis of the shape while preserving its original topology and geometry. While the earlier approaches are mainly based on the extracting the skeleton and then pruning the unwanted branches, the present study proposes a novel convolutional neural network based method to perform this task. The proposed architecture is an encoder-decoder network which leverage the benefits of coordinated convolutional layer and multi-level supervision to prevent the loss of information between the extracted skeleton and the ground truth. The dense attention block is used as the backbone blocks in encoder and decoder block. This architecture is performing better than the state of art on not only skeletonization of image task but also skeletonization from the point cloud. This method achieved a F1 score of 0.7961 on Pixel Skeleton dataset and a Chamfer Distance (CD) score of 1.9561 on Point skeleton dataset.",
    "code_link": ""
  },
  "iccv2021_dlgc_disco-u-netbasedautoencoderarchitecturewithdualinputstreamsforskeletonimagedrawing": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "DISCO - U-Net Based Autoencoder Architecture With Dual Input Streams for Skeleton Image Drawing",
    "authors": [
      "Soonyong Song",
      "Heechul Bae",
      "Junhee Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Song_DISCO_-_U-Net_Based_Autoencoder_Architecture_With_Dual_Input_Streams_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Song_DISCO_-_U-Net_Based_Autoencoder_Architecture_With_Dual_Input_Streams_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we propose a DISCO, which is a manner of designing autoencoder architecture to process dual input streams for skeletal image generation. The DISCO was designed to be dealing with binary masks and skeletonized images concurrently at the input side. We expected the skeletonized images using traditional thinning algorithms could help to boost skeleton prediction performances. Inside the DISCO architecture, there exist two encoders and a single decoder. Each functional block is stacked with multiple logical layers. We designed that logical layer outputs of encoders transferred corresponding counterpart layers in a decoder referring to U-Net architecture. In addition, we proposed hybrid-type encoder models based on the DISCO architecture to capitalize on the effect of the model ensemble. We demonstrated performances of the DISCO-A and DISCO-B models derived from the proposed architecture in terms of f1-score and loss convergence per each epoch. We confirmed the DISCO-B had produced the best performance under symbolic label usage. In the development phase, our best score reached 0.7386 with 500 epochs.",
    "code_link": ""
  },
  "iccv2021_dlgc_u-netbasedskeletonizationandbagoftricks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "U-Net Based Skeletonization and Bag of Tricks",
    "authors": [
      "Nam Hoang Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Nguyen_U-Net_Based_Skeletonization_and_Bag_of_Tricks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Nguyen_U-Net_Based_Skeletonization_and_Bag_of_Tricks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Skeletonization is a process focused on providing a compact and simple representation of an object by extracting the skeleton pixels from the given shape in a binary image. This method has been widely applied in various image processing and computer vision applications. In addition to traditional approaches which are not robust and provide low accuracy results, many efforts have been made for creating deep learning based methods to overcome these disadvantages. However, skeletonization is still a new topic in the deep learning world. In this paper, we propose our solution for the Pixel SkelNetOn challenge in the third edition of the \"Deep Learning for Geometric Computing\" workshop at ICCV 2021, which includes (1) modification of U-Net architecture using the attention mechanism, (2) implementation of auxiliary task learning for a more effective training process and (3) application of several tricks for improving the skeletonization model's performance. Our method achieved 0.8000 on the Pixel SkelNetOn validation set and second place in the leaderboard. We also release our code to facilitate future research at https://github.com/namdvt/skeletonization.",
    "code_link": "https://github.com/namdvt/skeletonization"
  },
  "iccv2021_dlgc_investigatingtransformersinthedecompositionofpolygonalshapesaspointcollections": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "Investigating Transformers in the Decomposition of Polygonal Shapes As Point Collections",
    "authors": [
      "Andrea Alfieri",
      "Yancong Lin",
      "Jan C. van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Alfieri_Investigating_Transformers_in_the_Decomposition_of_Polygonal_Shapes_As_Point_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Alfieri_Investigating_Transformers_in_the_Decomposition_of_Polygonal_Shapes_As_Point_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Transformers can generate predictions in two approaches: 1. auto-regressively by conditioning each sequence element on the previous ones, or 2. directly produce an output sequences in parallel. While research has mostly explored upon this difference on sequential tasks in NLP, we study the difference between auto-regressive and parallel prediction on visual set prediction tasks, and in particular on polygonal shapes in images because polygons are representative of numerous types of objects, such as buildings or obstacles for aerial vehicles. This is challenging for deep learning architectures as a polygon can consist of a varying carnality of points. We provide evidence on the importance of natural orders for Transformers, and show the benefit of decomposing complex polygons into collections of points in an auto-regressive manner.",
    "code_link": ""
  },
  "iccv2021_dlgc_patchaugmentlocalneighborhoodaugmentationinpointcloudclassification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "PatchAugment: Local Neighborhood Augmentation in Point Cloud Classification",
    "authors": [
      "Shivanand Venkanna Sheshappanavar",
      "Vinit Veerendraveer Singh",
      "Chandra Kambhamettu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Sheshappanavar_PatchAugment_Local_Neighborhood_Augmentation_in_Point_Cloud_Classification_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Sheshappanavar_PatchAugment_Local_Neighborhood_Augmentation_in_Point_Cloud_Classification_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recent deep neural network models trained on smaller and less diverse datasets use data augmentation to alleviate limitations such as overfitting, reduced robustness, and lower generalization. Methods using 3D datasets are among the most common to use data augmentation techniques such as random point drop, scaling, translation, rotations, and jittering. However, these data augmentation techniques are fixed and are often applied to the entire object, ignoring the object's local geometry. Different local neighborhoods on the object surface hold a different amount of geometric complexity. Applying the same data augmentation techniques at the object level is less effective in augmenting local neighborhoods with complex structures. This paper presents PatchAugment, a data augmentation framework to apply different augmentation techniques to the local neighborhoods. Our experimental studies on PointNet++ and DGCNN models demonstrate the effectiveness of PatchAugment on the task of 3D Point Cloud Classification. We evaluated our technique against these models using four benchmark datasets, ModelNet40 (synthetic), ModelNet10 (synthetic), SHREC'16 (synthetic) and ScanObjectNN (real-world).",
    "code_link": ""
  },
  "iccv2021_dlgc_towardsefficientpointcloudgraphneuralnetworksthrougharchitecturalsimplification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification",
    "authors": [
      "Shyam A. Tailor",
      "Ren\u00e9 de Jong",
      "Tiago Azevedo",
      "Matthew Mattina",
      "Partha Maji"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Tailor_Towards_Efficient_Point_Cloud_Graph_Neural_Networks_Through_Architectural_Simplification_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Tailor_Towards_Efficient_Point_Cloud_Graph_Neural_Networks_Through_Architectural_Simplification_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In recent years graph neural network (GNN)-based approaches have become a popular strategy for processing point cloud data, regularly achieving state-of-the-art performance on a variety of tasks. To date, the research community has primarily focused on improving model expressiveness, with secondary thought given to how to design models that can run efficiently on resource constrained mobile devices including smartphones or mixed reality headsets. In this work we make a step towards improving the efficiency of these models by making the observation that these GNN models are heavily limited by the representational power of their first, feature extracting, layer. We find that it is possible to radically simplify these models so long as the feature extraction layer is retained with minimal degradation to model performance; further, we discover that it is possible to improve performance overall on ModelNet40 and S3DIS by improving the design of the feature extractor. Our approach reduces memory consumption by 20x and latency by up to 9.9x for graph layers in models such as DGCNN; overall, we achieve speed-ups of up to 4.5x and peak memory reductions of 72.5%.",
    "code_link": ""
  },
  "iccv2021_dlgc_learninglaplaciansinchebyshevgraphconvolutionalnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Learning for Geometric Computing",
    "title": "Learning Laplacians in Chebyshev Graph Convolutional Networks",
    "authors": [
      "Hichem Sahbi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Sahbi_Learning_Laplacians_in_Chebyshev_Graph_Convolutional_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Sahbi_Learning_Laplacians_in_Chebyshev_Graph_Convolutional_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Spectral graph convolutional networks (GCNs) are particular deep models which aim at extending neural networks to arbitrary irregular domains. The principle of these networks consists in projecting graph signals using the eigen-decomposition of their Laplacians, then achieving filtering in the spectral domain prior to back-project the resulting filtered signals onto the input graph domain. However, the success of these operations is highly dependent on the relevance of the used Laplacians which are mostly handcrafted and this makes GCNs clearly sub-optimal. In this paper, we introduce a novel spectral GCN that learns not only the usual convolutional parameters but also the Laplacian operators. The latter are designed \"end-to-end\" as a part of a recursive Chebyshev decomposition with the particularity of conveying both the differential and the non-differential properties of the learned representations -- with increasing order and discrimination power -- without overparametrizing the trained GCNs. Extensive experiments, conducted on the challenging task of skeleton-based action recognition, show the generalization ability and the outperformance of our proposed Laplacian design w.r.t. different baselines (built upon handcrafted and other learned Laplacians) as well as the related work.",
    "code_link": ""
  },
  "iccv2021_dyad_emotionalfeaturesofinteractionswithempathicagents": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DYAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Understanding Social Behavior in Dyadic and Small Group Interactions",
    "title": "Emotional Features of Interactions With Empathic Agents",
    "authors": [
      "Claudia Greco",
      "Carmela Buono",
      "Pau Buch-Cardona",
      "Gennaro Cordasco",
      "Sergio Escalera",
      "Anna Esposito",
      "Anais Fernandez",
      "Daria Kyslitska",
      "Maria Stylianou Kornes",
      "Cristina Palmero",
      "Jofre Tenorio Laranga",
      "Anna Torp Johansen",
      "Maria In\u00e9s Torres"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/html/Greco_Emotional_Features_of_Interactions_With_Empathic_Agents_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/papers/Greco_Emotional_Features_of_Interactions_With_Empathic_Agents_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The current study is part of the EMPATHIC project, whose aim is to develop an Empathic Virtual Coach (VC) capable of promoting healthy and independent aging. To this end, the VC needs to be capable of perceiving the emotional states of users and adjusting its behaviour during the interactions according to what the users are experiencing in terms of emotions and comfort. Thus, the present work focuses on some sessions where elderly users of three different countries interact with a simulated system. Audio and video information extracted from these sessions were examined by external observers to assess participants' emotional experience with the EMPATHIC-VC in terms of categorical and dimensional assessment of emotions. Analyses were conducted on the emotional labels assigned by the external observers while participants were engaged in two different scenarios: a generic one, where the interaction was carried out with no intention to discuss a specific topic, and a nutrition one, aimed to accomplish a conversation on users' nutritional habits.Results of analyses performed on both audio and video data revealed that the EMPATHIC coach did not elicit negative feelings in the users. Indeed, users from all countries have shown relaxed and positive behavior when interacting with the simulated VC during both scenarios.Overall, the EMPATHIC-VC was capable to offer an enjoyable experience without eliciting negative feelings in the users. This supports the hypothesis that an Empathic Virtual Coach capable of considering users' expectations and emotional states could support elderly people in daily life activities and help them to remain independent.",
    "code_link": ""
  },
  "iccv2021_dyad_dyadformeramulti-modaltransformerforlong-rangemodelingofdyadicinteractions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DYAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Understanding Social Behavior in Dyadic and Small Group Interactions",
    "title": "Dyadformer: A Multi-Modal Transformer for Long-Range Modeling of Dyadic Interactions",
    "authors": [
      "David Curto",
      "Albert Clap\u00e9s",
      "Javier Selva",
      "Sorina Smeureanu",
      "Julio C. S. Jacques Junior",
      "David Gallardo-Pujol",
      "Georgina Guilera",
      "David Leiva",
      "Thomas B. Moeslund",
      "Sergio Escalera",
      "Cristina Palmero"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/html/Curto_Dyadformer_A_Multi-Modal_Transformer_for_Long-Range_Modeling_of_Dyadic_Interactions_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/papers/Curto_Dyadformer_A_Multi-Modal_Transformer_for_Long-Range_Modeling_of_Dyadic_Interactions_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Personality computing has become an emerging topic in computer vision, due to the wide range of applications it can be used for. However, most works on the topic have focused on analyzing the individual, even when applied to interaction scenarios, and for short periods of time. To address these limitations, we present the Dyadformer, a novel multi-modal multi-subject Transformer architecture to model individual and interpersonal features in dyadic interactions using variable time windows, thus allowing the capture of long-term interdependencies. Our proposed cross-subject layer allows the network to explicitly model interactions among subjects through attentional operations. This proof-of-concept approach shows how multi-modality and joint modeling of both interactants for longer periods of time helps to predict individual attributes. With Dyadformer, we improve state-of-the-art self-reported personality inference results on individual subjects on the UDIVA v0.5 dataset.",
    "code_link": ""
  },
  "iccv2021_dyad_temporalcuesfromsociallyunacceptabletrajectoriesforanomalydetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DYAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Understanding Social Behavior in Dyadic and Small Group Interactions",
    "title": "Temporal Cues From Socially Unacceptable Trajectories for Anomaly Detection",
    "authors": [
      "Neelu Madan",
      "Arya Farkhondeh",
      "Kamal Nasrollahi",
      "Sergio Escalera",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/html/Madan_Temporal_Cues_From_Socially_Unacceptable_Trajectories_for_Anomaly_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/papers/Madan_Temporal_Cues_From_Socially_Unacceptable_Trajectories_for_Anomaly_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "State-of-the-Art (SoTA) deep learning-based approaches to detect anomalies in surveillance videos utilize limited temporal information, including basic information from motion, e.g., optical flow computed between consecutive frames. In this paper, we compliment the SoTA methods by including long-range dependencies from trajectories for anomaly detection. To achieve that, we first created trajectories by running a tracker on two SoTA datasets, namely Avenue and Shanghai-Tech. We propose a prediction-based anomaly detection method using trajectories based on Social GANs, also called in this paper as temporal-based anomaly detection. Then, we hypothesize that late fusion of the result of this temporal-based anomaly detection system with spatial-based anomaly detection systems produces SoTA results. We verify this hypothesis on two spatial-based anomaly detection systems. We show that both cases produce results better than baseline spatial-based systems, indicating the usefulness of the temporal information coming from the trajectories for anomaly detection. We observe that the proposed approach depicts the maximum improvement in micro-level Area-Under-the-Curve (AUC) by 4.1% on CUHK Avenue and 3.4% on Shanghai-Tech over one of the baseline method. We also show a high performance on cross-data evaluation, where we learn the weights to combine spatial and temporal information on Shanghai-Tech and perform evaluation on CUHK Avenue and vice-versa.",
    "code_link": ""
  },
  "iccv2021_dyad_multipleinstancetripletlossforweaklysupervisedmulti-labelactionlocalisationofinteractingpersons": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DYAD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Understanding Social Behavior in Dyadic and Small Group Interactions",
    "title": "Multiple Instance Triplet Loss for Weakly Supervised Multi-Label Action Localisation of Interacting Persons",
    "authors": [
      "Sovan Biswas",
      "J\u00fcrgen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/html/Biswas_Multiple_Instance_Triplet_Loss_for_Weakly_Supervised_Multi-Label_Action_Localisation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DYAD/papers/Biswas_Multiple_Instance_Triplet_Loss_for_Weakly_Supervised_Multi-Label_Action_Localisation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With the abundance of videos and the high cost of data annotation, weakly supervised action localisation has gained more attention. However, most of the works on weakly supervised action localisation focus on single action and single person action localisation. Recently, new approaches have been proposed to extend the weakly supervised action localisation task towards multi-label scenarios where multiple persons can interact with each other and perform multiple actions at the same time. For longer videos, these methods subdivide the training videos into very short clips and discard the temporal consistency of actions across these short clips. In this work, we address this issue and propose the Multiple Instance Triplet Loss (MITL) where consistent instances that are temporally close should be more similar than distant and inconsistent instances. It is an extension of the triplet loss to bags where a bag comprises all person detections at a keyframe. We evaluate our proposed approach on the challenging AVA dataset where it achieves state-of-the-art results when the weakly labelled training videos are longer than 1 second.",
    "code_link": "https://github.com/sovan-biswas/MITL"
  },
  "iccv2021_deepmtl_indefenseofthelearningwithoutforgettingfortaskincrementallearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DeepMTL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Multi-Task Learning in Computer Vision",
    "title": "In Defense of the Learning Without Forgetting for Task Incremental Learning",
    "authors": [
      "Guy Oren",
      "Lior Wolf"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Oren_In_Defense_of_the_Learning_Without_Forgetting_for_Task_Incremental_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Oren_In_Defense_of_the_Learning_Without_Forgetting_for_Task_Incremental_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Catastrophic forgetting is one of the major challenges on the road for continual learning systems, which are presented with an on-line stream of tasks. The field has attracted considerable interest and a diverse set of methods have been presented for overcoming this challenge. Learning without Forgetting (LwF) is one of the earliest and most frequently cited methods. It has the advantages of not requiring the storage of samples from the previous tasks, of implementation simplicity, and of being well-grounded by relying on knowledge distillation. However, the prevailing view is that while it shows a relatively small amount of forgetting when only two tasks are introduced, it fails to scale to long sequences of tasks. This paper challenges this view, by showing that using the right architecture along with a standard set of augmentations, the results obtained by LwF surpass the latest algorithms for task incremental scenario. This improved performance is demonstrated by an extensive set of experiments over CIFAR-100 and Tiny-ImageNet, where it is also shown that other methods cannot benefit as much from similar improvements. Our code is available at: https://github.com/guy-oren/In_defence_of_LWF",
    "code_link": ""
  },
  "iccv2021_deepmtl_convnetsvs.transformerswhosevisualrepresentationsaremoretransferable?": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DeepMTL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Multi-Task Learning in Computer Vision",
    "title": "ConvNets vs. Transformers: Whose Visual Representations Are More Transferable?",
    "authors": [
      "Hong-Yu Zhou",
      "Chixiang Lu",
      "Sibei Yang",
      "Yizhou Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Zhou_ConvNets_vs._Transformers_Whose_Visual_Representations_Are_More_Transferable_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Zhou_ConvNets_vs._Transformers_Whose_Visual_Representations_Are_More_Transferable_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets' features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. Given the strong correlation between the performance of pre-trained models and transfer learning, we include 2 residual ConvNets (i.e., R-101x3 and R-152x4) and 3 Transformer-based visual backbones (i.e., ViT-B, ViT-L and Swin-B), which have close error rates on ImageNet, that indicate similar transfer learning performance on downstream datasets. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.",
    "code_link": ""
  },
  "iccv2021_deepmtl_multi-modalrgb-dscenerecognitionacrossdomains": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DeepMTL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Multi-Task Learning in Computer Vision",
    "title": "Multi-Modal RGB-D Scene Recognition Across Domains",
    "authors": [
      "Andrea Ferreri",
      "Silvia Bucci",
      "Tatiana Tommasi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Ferreri_Multi-Modal_RGB-D_Scene_Recognition_Across_Domains_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Ferreri_Multi-Modal_RGB-D_Scene_Recognition_Across_Domains_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Scene recognition is one of the basic problems in computer vision research with extensive applications in robotics. When available, depth images provide helpful geometric cues that complement the RGB texture information and help to identify discriminative scene image features. Depth sensing technology developed fast in the last years and a great variety of 3D cameras have been introduced, each with different acquisition properties. However, those properties are often neglected when targeting big data collections, so multi-modal images are gathered disregarding their original nature. In this work, we put under the spotlight the existence of a possibly severe domain shift issue within multi-modality scene recognition datasets. As a consequence, a scene classification model trained on one camera may not generalize on data from a different camera, only providing a low recognition performance. Starting from the well-known SUN RGB-D dataset, we designed an experimental testbed to study this problem and we use it to benchmark the performance of existing methods. Finally, we introduce a novel adaptive scene recognition approach that leverages self-supervised translation between modalities. Indeed, learning to go from RGB to depth and vice-versa is an unsupervised procedure that can be trained jointly on data of multiple cameras and may help to bridge the gap among the extracted feature distributions. Our experimental results confirm the effectiveness of the proposed approach.",
    "code_link": ""
  },
  "iccv2021_deepmtl_milamulti-tasklearningfromvideosviaefficientinter-frameattention": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DeepMTL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Multi-Task Learning in Computer Vision",
    "title": "MILA: Multi-Task Learning From Videos via Efficient Inter-Frame Attention",
    "authors": [
      "Donghyun Kim",
      "Tian Lan",
      "Chuhang Zou",
      "Ning Xu",
      "Bryan A. Plummer",
      "Stan Sclaroff",
      "Jayan Eledath",
      "G\u00e9rard Medioni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Kim_MILA_Multi-Task_Learning_From_Videos_via_Efficient_Inter-Frame_Attention_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Kim_MILA_Multi-Task_Learning_From_Videos_via_Efficient_Inter-Frame_Attention_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Prior work in multi-task learning has mainly focused on predictions on a single image. In this work, we present a new approach for multi-task learning from videos via efficient inter-frame local attention (MILA). Our approach contains a novel inter-frame attention module which allows learning of task-specific attention across frames. We embed the attention module in a \"slow-fast\" architecture, where the slower network runs on sparsely sampled keyframes and the light-weight shallow network runs on non-keyframes at a high frame rate. We also propose an effective adversarial learning strategy to encourage the slow and fast network to learn similar features. Our approach ensures low-latency multi-task learning while maintaining high quality predictions. Experiments show competitive accuracy compared to state-of-the-art on two multi-task learning benchmarks while reducing the number of floating point operations (FLOPs) by 70%. In addition, our attention based feature propagation method (ILA) outperforms prior work in terms of task accuracy while also reducing up to 90% of FLOPs.",
    "code_link": "https://github.com/Lyken17/pytorch-OpCounter"
  },
  "iccv2021_deepmtl_concurrentdiscriminationandalignmentforself-supervisedfeaturelearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DeepMTL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Multi-Task Learning in Computer Vision",
    "title": "Concurrent Discrimination and Alignment for Self-Supervised Feature Learning",
    "authors": [
      "Anjan Dutta",
      "Massimiliano Mancini",
      "Zeynep Akata"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Dutta_Concurrent_Discrimination_and_Alignment_for_Self-Supervised_Feature_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Dutta_Concurrent_Discrimination_and_Alignment_for_Self-Supervised_Feature_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Existing self-supervised learning methods learn representation by means of pretext tasks which are either (1) discriminating that explicitly specify which features should be separated or (2) aligning that precisely indicate which features should be closed together, but ignore the fact how to jointly and principally define which features to be repelled and which ones to be attracted. In this work, we combine the positive aspects of the discriminating and aligning methods, and design a hybrid method that addresses the above issue. Our method explicitly specifies the repulsion and attraction mechanism respectively by discriminative predictive task and concurrently maximizing mutual information between paired views sharing redundant information. We qualitatively and quantitatively show that our proposed model learns better features that are more effective for the diverse downstream tasks ranging from classification to semantic segmentation. Our experiments on nine established benchmarks show that the proposed model consistently outperforms the existing state-of-the-art results of self-supervised and transfer learning protocol.",
    "code_link": ""
  },
  "iccv2021_deepmtl_uninetaunifiedsceneunderstandingnetworkandexploringmulti-taskrelationshipsthroughthelensofadversarialattacks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DeepMTL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Multi-Task Learning in Computer Vision",
    "title": "UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships Through the Lens of Adversarial Attacks",
    "authors": [
      "Naresh Kumar Gurulingan",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Gurulingan_UniNet_A_Unified_Scene_Understanding_Network_and_Exploring_Multi-Task_Relationships_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Gurulingan_UniNet_A_Unified_Scene_Understanding_Network_and_Exploring_Multi-Task_Relationships_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Scene understanding is crucial for autonomous systems which intend to operate in the real world. Single task vision networks extract information only based on some aspects of the scene. In multi-task learning (MTL), on the other hand, these single tasks are jointly learned, thereby providing an opportunity for tasks to share information and obtain a more comprehensive understanding. To this end, we develop UniNet, a unified scene understanding network that accurately and efficiently infers vital vision tasks including object detection, semantic segmentation, instance segmentation, monocular depth estimation, and monocular instance depth prediction. As these tasks look at different semantic and geometric information, they can either complement or conflict with each other. Therefore, understanding inter-task relationships can provide useful cues to enable complementary information sharing. We evaluate the task relationships in UniNet through the lens of adversarial attacks based on the notion that they can exploit learned biases and task interactions in the neural network. Extensive experiments on the Cityscapes dataset, using untargeted and targeted attacks reveal that semantic tasks strongly interact amongst themselves, and the same holds for geometric tasks. Additionally, we show that the relationship between semantic and geometric tasks is asymmetric and their interaction becomes weaker as we move towards higher-level representations.",
    "code_link": ""
  },
  "iccv2021_deepmtl_audio-visualtransformerbasedcrowdcounting": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "DeepMTL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Deep Multi-Task Learning in Computer Vision",
    "title": "Audio-Visual Transformer Based Crowd Counting",
    "authors": [
      "Usman Sajid",
      "Xiangyu Chen",
      "Hasan Sajid",
      "Taejoon Kim",
      "Guanghui Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Sajid_Audio-Visual_Transformer_Based_Crowd_Counting_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Sajid_Audio-Visual_Transformer_Based_Crowd_Counting_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.",
    "code_link": ""
  },
  "iccv2021_somof_learningdecoupledrepresentationsforhumanposeforecasting": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SoMoF",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human Trajectory and Pose Dynamics Forecasting in the Wild",
    "title": "Learning Decoupled Representations for Human Pose Forecasting",
    "authors": [
      "Behnam Parsaeifard",
      "Saeed Saadatnejad",
      "Yuejiang Liu",
      "Taylor Mordan",
      "Alexandre Alahi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Parsaeifard_Learning_Decoupled_Representations_for_Human_Pose_Forecasting_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/papers/Parsaeifard_Learning_Decoupled_Representations_for_Human_Pose_Forecasting_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Human pose forecasting involves complex spatiotemporal interactions between body parts (e.g., arms, legs, spine). State-of-the-art approaches use Long Short-Term Memories (LSTMs) or Variational AutoEncoders (VAEs) to solve the problem. Yet, they do not effectively predict human motions when both global trajectory and local pose movements exist. We propose to learn decoupled representations for the global and local pose forecasting tasks. We also show that it is better to stop the prediction when the uncertainty in human motion increases. Our forecasting model outperforms all existing methods on the pose forecasting benchmark to date by over 20%. The code is available online.",
    "code_link": "https://github.com/vita-epfl/decoupled-pose-prediction.git"
  },
  "iccv2021_somof_posetransformers(potr)humanmotionpredictionwithnon-autoregressivetransformers": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SoMoF",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human Trajectory and Pose Dynamics Forecasting in the Wild",
    "title": "Pose Transformers (POTR): Human Motion Prediction With Non-Autoregressive Transformers",
    "authors": [
      "Angel Mart\u00ednez-Gonz\u00e1lez",
      "Michael Villamizar",
      "Jean-Marc Odobez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Martinez-Gonzalez_Pose_Transformers_POTR_Human_Motion_Prediction_With_Non-Autoregressive_Transformers_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/papers/Martinez-Gonzalez_Pose_Transformers_POTR_Human_Motion_Prediction_With_Non-Autoregressive_Transformers_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose to leverage Transformer architectures for non-autoregressive human motion prediction. Our approach decodes elements in parallel from a query sequence, instead of conditioning on previous predictions such as in state-of-the-art RNN-based approaches. In such a way our approach is less computational intensive and potentially avoids error accumulation to long term elements in the sequence. In that context, our contributions are fourfold: (i) we frame human motion prediction as a sequence-to-sequence problem and propose a non-autoregressive Transformer to infer the sequences of poses in parallel; (ii) we propose to decode sequences of 3D poses from a query sequence generated in advance with elements from the input sequence; (iii) we propose to perform skeleton-based activity classification from the encoder memory, in the hope that identifying the activity can improve predictions; (iv) we show that despite its simplicity, our approach achieves competitive results in two public datasets, although surprisingly more for short term predictions rather than for long term ones.",
    "code_link": "https://github.com/idiap/potr"
  },
  "iccv2021_somof_simplebaselineforsinglehumanmotionforecasting": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SoMoF",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human Trajectory and Pose Dynamics Forecasting in the Wild",
    "title": "Simple Baseline for Single Human Motion Forecasting",
    "authors": [
      "Chenxi Wang",
      "Yunfeng Wang",
      "Zixuan Huang",
      "Zhiwen Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Wang_Simple_Baseline_for_Single_Human_Motion_Forecasting_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/papers/Wang_Simple_Baseline_for_Single_Human_Motion_Forecasting_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Global human motion forecasting is important in many fields, which is the combination of global human trajectory prediction and local human pose prediction. Visual and social information are often used to boost model performance, however, they may consume too much computational resources. In this paper, we establish a simple but effective baseline for single human motion forecasting without visual and social information, equipped with useful training tricks. Our method \"futuremotion_ICCV21\" outperforms existing methods by a large margin on SoMoF benchmark. We hope our work provide new ideas for future research.",
    "code_link": ""
  },
  "iccv2021_somof_stirnetaspatial-temporalinteraction-awarerecursivenetworkforhumantrajectoryprediction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SoMoF",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human Trajectory and Pose Dynamics Forecasting in the Wild",
    "title": "STIRNet: A Spatial-Temporal Interaction-Aware Recursive Network for Human Trajectory Prediction",
    "authors": [
      "Yusheng Peng",
      "Gaofeng Zhang",
      "Xiangyu Li",
      "Liping Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Peng_STIRNet_A_Spatial-Temporal_Interaction-Aware_Recursive_Network_for_Human_Trajectory_Prediction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/papers/Peng_STIRNet_A_Spatial-Temporal_Interaction-Aware_Recursive_Network_for_Human_Trajectory_Prediction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Pedestrian trajectory prediction is one of the important research topics in the field of computer vision and a key technology of autonomous driving system. However, it's full of challenges due to the uncertainties of crowd motions and complex interactions among pedestrians. We propose a Spatio-temporal Interaction-aware Recursive Network (STIRNet) to predict multiply socially acceptable trajectories of pedestrians. In this paper, a recursive structure is used to capture spatio-temporal interactions by spatial modeling and temporal modeling alternately. At each time-step, the spatial interactions are modeled by a graph attention network, in which the nodes feature are represented by temporal motion features. The learned spatial interaction context is used to capture temporal motion features through an LSTM model. The temporal motion features are used to infer future positions and update nodes features. Experimental results on two public pedestrian trajectory datasets (ETH and UCY) demonstrate that our proposed model achieves superior performances compared with state-of-the-art methods on ADE and FDE metrics.",
    "code_link": ""
  },
  "iccv2021_somof_scatstrideconsistencywithauto-regressiveregressorandtransformerforhandposeestimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SoMoF",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human Trajectory and Pose Dynamics Forecasting in the Wild",
    "title": "SCAT: Stride Consistency With Auto-Regressive Regressor and Transformer for Hand Pose Estimation",
    "authors": [
      "Daiheng Gao",
      "Bang Zhang",
      "Qi Wang",
      "Xindi Zhang",
      "Pan Pan",
      "Yinghui Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Gao_SCAT_Stride_Consistency_With_Auto-Regressive_Regressor_and_Transformer_for_Hand_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/papers/Gao_SCAT_Stride_Consistency_With_Auto-Regressive_Regressor_and_Transformer_for_Hand_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The current state-of-the-art monocular 3D hand pose estimation methods are mostly model-based. For instance, MANO is one of the most popular hand parametric models, which can depict hand shapes and poses. It is widely adopted for estimating hand poses in images and videos. However, MANO is a parametric model derived from scanned hand data with limited shapes and poses which constrains its capability in depicting in-the-wild shape and pose variations. In this paper, we propose a 3D hand pose estimation approach which does not depends on any parametric hand models yet can still accurately estimate in-the-wild hand poses. Our approach (Stride Consistency with Autoregressive regressor and Transformer, SCAT) offers a new representation for measuring hand poses. The new representation includes a mean shape hand template and its 21 hand joint offsets depicting the 3D distances between the hand template and the hand that needs to be estimated. Besides, SCAT can generate a robust and smooth linear mapping between visual feature maps and the target 3D offsets, ensuring inter-frame smoothness and removing motion jittering. We also introduce an auto-regressive refinement procedure for iteratively refining the hand pose estimation. Extensive experiments show that our SCAT can generate more accurate and smoother 3D hand pose estimation results compared with the state-of-the-art methods.",
    "code_link": ""
  },
  "iccv2021_somof_multi-inputfusionforpracticalpedestrianintentionprediction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SoMoF",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human Trajectory and Pose Dynamics Forecasting in the Wild",
    "title": "Multi-Input Fusion for Practical Pedestrian Intention Prediction",
    "authors": [
      "Ankur Singh",
      "Upendra Suddamalla"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Singh_Multi-Input_Fusion_for_Practical_Pedestrian_Intention_Prediction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SoMoF/papers/Singh_Multi-Input_Fusion_for_Practical_Pedestrian_Intention_Prediction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Pedestrians are the most vulnerable road users and are at a high risk of fatal accidents. Accurate pedestrian detection and effectively analyzing their intentions to cross the road are critical for autonomous vehicles and ADAS solutions to safely navigate public roads. Faster and precise estimation of pedestrian intention helps in adopting safe driving behavior. Visual pose and motion are two important cues that have been previously employed to determine pedestrian intention. However, motion patterns can give erroneous results for short-term video sequences and are thus prone to mistakes. In this work, we propose an intention prediction network that utilizes pedestrian bounding boxes, pose, bounding box coordinates, and takes advantage of global context along with the local setting. This network implicitly learns pedestrians' motion cues and location information to differentiate between a crossing and a non-crossing pedestrian. We experiment with different combinations of input features and propose multiple efficient models in terms of accuracy and inference speeds. Our best-performing model shows around 85% accuracy on the JAAD dataset.",
    "code_link": ""
  },
  "iccv2021_viral_whatmattersforad-hocvideosearch?alarge-scaleevaluationontrecvid": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ViRaL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Video Retrieval Methods and Their Limits",
    "title": "What Matters for Ad-Hoc Video Search? A Large-Scale Evaluation on TRECVID",
    "authors": [
      "Aozhu Chen",
      "Fan Hu",
      "Zihan Wang",
      "Fangming Zhou",
      "Xirong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ViRaL/html/Chen_What_Matters_for_Ad-Hoc_Video_Search_A_Large-Scale_Evaluation_on_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ViRaL/papers/Chen_What_Matters_for_Ad-Hoc_Video_Search_A_Large-Scale_Evaluation_on_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "For quantifying progress in Ad-hoc Video Search (AVS), the annual TRECVID AVS task is an important international evaluation. Solutions submitted by the task participants vary in terms of their choices of cross-modal matching models, visual features and training data. As such, what one may conclude from the evaluation is at a high level that is insufficient to reveal the influence of the individual components. In order to bridge the gap between the current solution-level comparison and the desired component-wise comparison, we propose in this paper a large-scale and systematic evaluation on TRECVID. By selected combinations of state-of-the-art matching models, visual features and (pre-)training data, we construct a set of 25 different solutions and evaluate them on the TRECVID AVS tasks 2016--2020. The presented evaluation helps answer the key question of what matters for AVS. The resultant observations and learned lessons are also instructive for developing novel AVS solutions.",
    "code_link": ""
  },
  "iccv2021_viral_instancesearchviafusinghierarchicalmulti-levelretrievalandhuman-objectinteractiondetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ViRaL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Video Retrieval Methods and Their Limits",
    "title": "Instance Search via Fusing Hierarchical Multi-Level Retrieval and Human-Object Interaction Detection",
    "authors": [
      "Wenhao Yang",
      "Yinan Song",
      "Zhicheng Zhao",
      "Fei Su"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ViRaL/html/Yang_Instance_Search_via_Fusing_Hierarchical_Multi-Level_Retrieval_and_Human-Object_Interaction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ViRaL/papers/Yang_Instance_Search_via_Fusing_Hierarchical_Multi-Level_Retrieval_and_Human-Object_Interaction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Aiming to retrieve specific persons with specific actions, instance-based video search (INS) has attracted rising attention with the development of video understanding. In this paper, a novel hierarchical multi-task INS retrieval framework is proposed. Firstly, a multi-level action recognition framework and a face matching scheme are introduced to obtain initial action and person retrieval scores separately. In particular, a novel graph-based human-object interaction (HOI) detection model, named interaction-centric graph parsing network (iCGPN), is proposed to recognize interactions between human and objects. Secondly, an improved query extension strategy is adopted to re-rank the initial person retrieval results. Thirdly, more elaborate action features are extracted to recognize complicated actions. Finally, a specially designed fusion strategy is used to integrate the retrieval results of persons and actions to generate the final INS ranking list. The experimental results show the effectiveness of the proposed framework.",
    "code_link": ""
  },
  "iccv2021_viral_hard-negativesornon-negatives?ahard-negativeselectionstrategyforcross-modalretrievalusingtheimprovedmarginalrankingloss": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ViRaL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Video Retrieval Methods and Their Limits",
    "title": "Hard-Negatives or Non-Negatives? A Hard-Negative Selection Strategy for Cross-Modal Retrieval Using the Improved Marginal Ranking Loss",
    "authors": [
      "Damianos Galanopoulos",
      "Vasileios Mezaris"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ViRaL/html/Galanopoulos_Hard-Negatives_or_Non-Negatives_A_Hard-Negative_Selection_Strategy_for_Cross-Modal_Retrieval_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ViRaL/papers/Galanopoulos_Hard-Negatives_or_Non-Negatives_A_Hard-Negative_Selection_Strategy_for_Cross-Modal_Retrieval_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Cross-modal learning has gained a lot of interest recently, and many applications of it, such as image-text retrieval, cross-modal video search, or video captioning have been proposed. In this work, we deal with the cross-modal video retrieval problem. The state-of-the-art approaches are based on deep network architectures, and rely on mining hard-negative samples during training to optimize the selection of the network's parameters. Starting from a state-of-the-art cross-modal architecture that uses the improved marginal ranking loss function, we propose a simple strategy for hard-negative mining to identify which training samples are hard-negatives and which, although presently treated as hard-negatives, are likely not negative samples at all and shouldn't be treated as such. Additionally, to take full advantage of network models trained using different design choices for hard-negative mining, we examine model combination strategies, and we design a hybrid one effectively combining large numbers of trained models.",
    "code_link": ""
  },
  "iccv2021_lffai_onlinecontinuallearningforvisualfoodclassification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LFFAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Large-Scale Fine-Grained Food AnalysIs",
    "title": "Online Continual Learning for Visual Food Classification",
    "authors": [
      "Jiangpeng He",
      "Fengqing Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LFFAI/html/He_Online_Continual_Learning_for_Visual_Food_Classification_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LFFAI/papers/He_Online_Continual_Learning_for_Visual_Food_Classification_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Food image classification is challenging for real-world applications since existing methods require static datasets for training and are not capable of learning from sequentially available new food images. Online continual learning aims to learn new classes from data stream by using each new data only once without forgetting the previously learned knowledge. However, none of the existing works target food image analysis, which is more difficult to learn incrementally due to its high intra-class variation with the unbalanced and unpredictable characteristics of future food class distribution. In this paper, we address these issues by introducing (1) a novel clustering based exemplar selection algorithm to store the most representative data belonging to each learned food for knowledge replay, and (2) an effective online learning regime using balanced training batch along with the knowledge distillation on augmented exemplars to maintain the model performance on all learned classes. Our method is evaluated on a challenging large scale food image database, Food-1K, by varying the number of newly added food classes. Our results show significant improvements compared with existing state-of-the-art online continual learning methods, showing great potential to achieve lifelong learning for food image classification in real world.",
    "code_link": ""
  },
  "iccv2021_lffai_fine-grainpredictionofstrawberryfreshnessusingsubsurfacescattering": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LFFAI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Large-Scale Fine-Grained Food AnalysIs",
    "title": "Fine-Grain Prediction of Strawberry Freshness Using Subsurface Scattering",
    "authors": [
      "Jeremy Klotz",
      "Vijay Rengarajan",
      "Aswin C. Sankaranarayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LFFAI/html/Klotz_Fine-Grain_Prediction_of_Strawberry_Freshness_Using_Subsurface_Scattering_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LFFAI/papers/Klotz_Fine-Grain_Prediction_of_Strawberry_Freshness_Using_Subsurface_Scattering_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Predicting fruit freshness before any visible decay is invaluable in the food distribution chain, spanning producers, retailers, and consumers. In this work, we leverage subsurface scattering signatures associated with strawberry tissue to perform long-term edibility predictions. Specifically, we implement various active illumination techniques with a projector-camera system to measure a strawberry's subsurface scattering and predict the time when it is likely to be inedible. We propose a learning-based approach with captures under structured illumination to perform this prediction. We study the efficacy of our method by capturing a dataset of strawberries decaying naturally over time.",
    "code_link": ""
  },
  "iccv2021_melex_analyzingandmitigatingjpegcompressiondefectsindeeplearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MELEX",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - More Exploration, Less Exploitation",
    "title": "Analyzing and Mitigating JPEG Compression Defects in Deep Learning",
    "authors": [
      "Max Ehrlich",
      "Larry Davis",
      "Ser-Nam Lim",
      "Abhinav Shrivastava"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MELEX/html/Ehrlich_Analyzing_and_Mitigating_JPEG_Compression_Defects_in_Deep_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MELEX/papers/Ehrlich_Analyzing_and_Mitigating_JPEG_Compression_Defects_in_Deep_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With the proliferation of deep learning methods, many computer vision problems which were considered academic are now viable in the consumer setting. One drawback of consumer applications is lossy compression, which is necessary from an engineering standpoint to efficiently and cheaply store and transmit user images. Despite this, there has been little study of the effect of compression on deep neural networks and benchmark datasets are often losslessly compressed or compressed at high quality. Here we present a unified study of the effects of JPEG compression on a range of common tasks and datasets. We show that there is a significant penalty on common performance metrics for high compression. We test several methods for mitigating this penalty, including a novel method based on artifact correction which requires no labels to train.",
    "code_link": ""
  },
  "iccv2021_melex_mggansolvingmodecollapseusingmanifold-guidedtraining": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MELEX",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - More Exploration, Less Exploitation",
    "title": "MGGAN: Solving Mode Collapse Using Manifold-Guided Training",
    "authors": [
      "Duhyeon Bang",
      "Hyunjung Shim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MELEX/html/Bang_MGGAN_Solving_Mode_Collapse_Using_Manifold-Guided_Training_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MELEX/papers/Bang_MGGAN_Solving_Mode_Collapse_Using_Manifold-Guided_Training_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Mode collapse is a critical problem in training generative adversarial networks. To alleviate mode collapse, several recent studies have introduced new objective functions, network architectures, or alternative training schemes. However, their achievement is often the result of sacrificing the image quality. In this paper, we propose a new algorithm, namely, the manifold-guided generative adversarial network (MGGAN), which leverages a guidance network on existing GAN architecture to induce the generator to learn the overall modes of a data distribution. The guidance network transforms an image into a learned manifold space, which is effective in representing the coverage of the overall modes. The characteristics of this guidance network helps penalize mode imbalance. Results of the experimental comparisons using various baseline GANs showed that MGGAN can be easily extended to existing GANs and resolve mode collapse without losing the image quality. Moreover, we extend the idea of manifold-guided GAN training to increase the original diversity of a data distribution. From the experiment, we confirmed that a GAN model guided by a joint manifold can sample data distribution with greater diversity. Results of the experimental analysis confirmed that MGGAN is an effective and efficient tool for improving the diversity of GANs.",
    "code_link": ""
  },
  "iccv2021_melex_addressingtargetshiftinzero-shotlearningusinggroupedadversariallearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MELEX",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - More Exploration, Less Exploitation",
    "title": "Addressing Target Shift in Zero-Shot Learning Using Grouped Adversarial Learning",
    "authors": [
      "Saneem A. Chemmengath",
      "Soumava Paul",
      "Samarth Bharadwaj",
      "Suranjana Samanta",
      "Karthik Sankaranarayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MELEX/html/Chemmengath_Addressing_Target_Shift_in_Zero-Shot_Learning_Using_Grouped_Adversarial_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MELEX/papers/Chemmengath_Addressing_Target_Shift_in_Zero-Shot_Learning_Using_Grouped_Adversarial_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Zero-shot learning (ZSL) algorithms typically work by exploiting attribute correlations to make predictions for unseen classes. However, these correlations do not remain intact at test time in most practical settings, and the resulting change in these correlations leads to adverse effects on zero-shot learning performance. In this paper, we present a new paradigm for ZSL that: (i) utilizes the class-attribute mapping of unseen classes to estimate the change in target distribution (target shift), and (ii) propose a novel technique called grouped Adversarial Learning (gAL) to reduce negative effects of this shift. Our approach is widely applicable for several existing ZSL algorithms, including those with implicit attribute predictions. We apply the proposed technique (gAL) on three popular ZSL algorithms: ALE, SJE, and DEVISE, and show performance improvements on 4 popular ZSL datasets: AwA2, aPY, CUB, and SUN.",
    "code_link": ""
  },
  "iccv2021_repss_weaklysupervisedrppgestimationforrespiratoryrateestimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RePSS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Remote Physiological Signal Sensing",
    "title": "Weakly Supervised rPPG Estimation for Respiratory Rate Estimation",
    "authors": [
      "Jingda Du",
      "Si-Qi Liu",
      "Bochao Zhang",
      "Pong C. Yuen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/html/Du_Weakly_Supervised_rPPG_Estimation_for_Respiratory_Rate_Estimation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/papers/Du_Weakly_Supervised_rPPG_Estimation_for_Respiratory_Rate_Estimation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recent studies demonstrate that respiratory rate can be estimated from skin videos through analyzing the frequency domain attributes of their remote photoplethysmography (rPPG). However, respiration is not always periodic so the frequency attributes of rPPG may not accurately estimate the respiratory rate. In this paper, we proposed an end-to-end network to estimate both rPPG signals and respiratory rates from facial videos. Since only breathing waves are available in the Remote Physiological Signal Sensing track2 competition, to preserve the respiratory pattern in rPPG estimation, rPPG signals pre-estimated by chrominace-based methods and modulated by breathing waves are used as weak labels for supervision. To adapt to the large differences between training and testing data, in terms of recording environment and subjects behavior, we also involved customized adversarial training on feature extractor to minimize the domain gap. In the competition, our model achieved 7.56 bpm MAE and ranked the second place.",
    "code_link": "https://github.com/1adrianb/face-alignment"
  },
  "iccv2021_repss_timelabsapproachtothechallengeoncomputervisionforremotephysiologicalmeasurement": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RePSS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Remote Physiological Signal Sensing",
    "title": "Time Lab's Approach to the Challenge on Computer Vision for Remote Physiological Measurement",
    "authors": [
      "Yuhang Dong",
      "Gongping Yang",
      "Yilong Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/html/Dong_Time_Labs_Approach_to_the_Challenge_on_Computer_Vision_for_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/papers/Dong_Time_Labs_Approach_to_the_Challenge_on_Computer_Vision_for_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Computer vision for remote physiological measurement is novel and uniquely challenging task, which enables non-contact monitoring of the blood volume pulse (BVP) using a commonly accessible camera. This paper introduces Time Lab's approach presented at the 2nd challenge on Remote Physiological Signal Sensing (RePSS) organized within ICCV2021. We propose an end-to-end rPPGNet for remote photoplethysmographyraphy (rPPG) signals estimation. A improved design of spatial-temporal map is also made, which is an an efficient representation of the rPPG signal by removing most of the irrelevant background content. Furthermore, our approach achieved first place on the 2nd RePSS Challenge Track 1 and has outperformed the methods of other participants as we have achieved M IBI = 117.25(4.51% improvement compared to the challenge top-2 result), R HR = 0.62(8.77% improvement). The codes are publicly available at https://github.com/yuhang1070/2nd_RePSS_Track1_Top1_Solution.",
    "code_link": "https://github.com/yuhang1070/2nd"
  },
  "iccv2021_repss_manetamotion-drivenattentionnetworkfordetectingthepulsefromafacialvideowithdrasticmotions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RePSS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Remote Physiological Signal Sensing",
    "title": "MANet: A Motion-Driven Attention Network for Detecting the Pulse From a Facial Video With Drastic Motions",
    "authors": [
      "Xuenan Liu",
      "Xuezhi Yang",
      "Ziyan Meng",
      "Ye Wang",
      "Jie Zhang",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/html/Liu_MANet_A_Motion-Driven_Attention_Network_for_Detecting_the_Pulse_From_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/papers/Liu_MANet_A_Motion-Driven_Attention_Network_for_Detecting_the_Pulse_From_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Video Photoplethysmography (VPPG) technique can detect pulse signals from facial videos, becoming increasingly popular due to its convenience and low cost. However, it fails to be sufficiently robust to drastic motion disturbances such as continuous head movements in our real life. A motion-driven attention network (MANet) is proposed in this paper to improve its motion robustness. MANet takes the frequency spectrum of a skin color signal and of a synchronous nose motion signal as the inputs, following by removing the motion features out of the skin color signal using an attention mechanism driven by the nose motion signal. Thus, it predicts frequency spectrum without components resulting from motion disturbances, which is finally transformed back to a pulse signal. MANet is tested on 1000 samples of 200 subjects provided by the 2nd Remote Physiological Signal Sensing (RePSS) Challenge. It achieves a mean inter-beat-interval (IBI) error of 122.80 milliseconds and a mean heart rate error of 7.29 beats per minute.",
    "code_link": ""
  },
  "iccv2021_repss_the2ndchallengeonremotephysiologicalsignalsensing(repss)": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RePSS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Remote Physiological Signal Sensing",
    "title": "The 2nd Challenge on Remote Physiological Signal Sensing (RePSS)",
    "authors": [
      "Xiaobai Li",
      "Haomiao Sun",
      "Zhaodong Sun",
      "Hu Han",
      "Antitza Dantcheva",
      "Shiguang Shan",
      "Guoying Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/html/Li_The_2nd_Challenge_on_Remote_Physiological_Signal_Sensing_RePSS_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/papers/Li_The_2nd_Challenge_on_Remote_Physiological_Signal_Sensing_RePSS_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Remote measurement of physiological signals from videos is an emerging topic. The topic draws great interest, but the lack of publicly available benchmark databases and a fair validation platform are hindering its further development. The RePSS Challenge is organized as an annual event for this concern. Here the 2nd RePSS is organized in conjunction with ICCV 2021. The 2nd RePSS contains two competition tracks. Track 1 is to measure inter-beat-intervals (IBI) from facial videos, which requires accurate measurement of each individual pulse peak. Track 2 is about respiration measurement from facial videos, as respiration is another important physiological index related to both health and emotional status. One new dataset is built and shared for Track 2. This paper presents an overview of the challenge, including data, protocol, results, and discussion. We highlighted the top-ranked solutions to provide insight for researchers, and we also outline future directions for this topic and this challenge.",
    "code_link": "https://github.com/TadasBaltrusaitis/OpenFace"
  },
  "iccv2021_repss_anend-to-endefficientframeworkforremotephysiologicalsignalsensing": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RePSS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Remote Physiological Signal Sensing",
    "title": "An End-to-End Efficient Framework for Remote Physiological Signal Sensing",
    "authors": [
      "Chengyang Hu",
      "Ke-Yue Zhang",
      "Taiping Yao",
      "Shouhong Ding",
      "Jilin Li",
      "Feiyue Huang",
      "Lizhuang Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/html/Hu_An_End-to-End_Efficient_Framework_for_Remote_Physiological_Signal_Sensing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RePSS/papers/Hu_An_End-to-End_Efficient_Framework_for_Remote_Physiological_Signal_Sensing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Remote photoplethysmography (rPPG) is utilized to estimate the heart activities from videos, which has drawn great interest from both researchers and companies recently. Many existing rPPG deep-learning based approaches focus on measuring the average heart rate (HR) from facial videos, which do not provide enough detailed information for many applications. To recover more detailed rPPG signals for the challenge on Remote Physiological Signal Sensing (RePSS), we propose an end-to-end efficient framework, which measures the average heart rate and estimates corresponding Blood Volume Pulse (BVP) curves simultaneously. For efficiently extracting features containing rPPG information, we adopt the temporal and spatial convolution as Feature Extractor, which alleviates the cost of calculation. Then, BVP Estimation Network estimates the frame-level BVP signal based on the feature maps via a simple 1DCNN. To improve the learning of BVP Estimation Network, we further introduce Heartbeat Measuring Network to predict the video-level HR based on global rPPG information. These two networks facilitate each other via supervising Feature Extractor from different level to promote the accuracy of BVP signal and HR. The proposed method obtains the score 168.08 (M_IBI), winning the third place in this challenge.",
    "code_link": ""
  },
  "iccv2021_she_sketchbirdlearningtogeneratebirdsketchesfromtext": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SHE",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Sketching for Human Expressivity",
    "title": "SketchBird: Learning To Generate Bird Sketches From Text",
    "authors": [
      "Shaozu Yuan",
      "Aijun Dai",
      "Zhiling Yan",
      "Zehua Guo",
      "Ruixue Liu",
      "Meng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/html/Yuan_SketchBird_Learning_To_Generate_Bird_Sketches_From_Text_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/papers/Yuan_SketchBird_Learning_To_Generate_Bird_Sketches_From_Text_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Sketch plays a critical role in the human art creation process. As one of the functions of the sketch, text-to-sketch may help the artists to catch the fleeting inspirations efficiently. Different from traditional text2image tasks, sketches consist of only a set of sparse lines and depend on very strict edge information, which requires the model to understand the text descriptions accurately and control the shape and texture in the fine-grained granularity. However, there was very rare previous research on the challenging text2sketch task. In this paper, we first construct a text2sketch image dataset by modifying the prevalent CUB dataset. Then a novel Generative Adversarial Network (GAN) based model is proposed by leveraging a Conditional Layer-Instance Normalization (CLIN) module, which can fuse the image features and sentence vector effectively and guide the sketch generation process. Extensive experiments were conducted and the results show the superiority of our proposed model compared to previous baselines. An in-depth analysis was also made to illustrate the contribution of each module and the limitation of our work.",
    "code_link": ""
  },
  "iccv2021_she_sketchydepthfromscenesketchestorgb-dimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SHE",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Sketching for Human Expressivity",
    "title": "SketchyDepth: From Scene Sketches to RGB-D Images",
    "authors": [
      "Gianluca Berardi",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/html/Berardi_SketchyDepth_From_Scene_Sketches_to_RGB-D_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/papers/Berardi_SketchyDepth_From_Scene_Sketches_to_RGB-D_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Sketch-based content generation is a creative and fun activity suited to casual and professional users that has many different applications. Today it is possible to generate the geometry and appearance of a single object by sketching it. Yet, only the appearance can be synthesized from a sketch of a whole scene. In this paper we propose the first method to generate both the depth map and image of a whole scene from a sketch. We demonstrate how generating geometrical information as a depth map is beneficial from a twofold perspective. On one hand, it improves the quality of the image synthesized from the sketch. On the other, it unlocks depth-enabled creative effects like Bokeh, fog, light variation, 3D photos and many others, which help enhancing the final output in a controlled way. We validate our method showing how generating depth maps directly from sketches produces better qualitative results with respect to alternative methods, i.e. running MiDaS after image generation. Finally we introduce depth sketching, a depth manipulation technique to further condition image generation without the need of additional annotation or training.",
    "code_link": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
  },
  "iccv2021_she_scenedesigneraunifiedmodelforscenesearchandsynthesisfromsketch": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SHE",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Sketching for Human Expressivity",
    "title": "Scene Designer: A Unified Model for Scene Search and Synthesis From Sketch",
    "authors": [
      "Leo Sampaio Ferraz Ribeiro",
      "Tu Bui",
      "John Collomosse",
      "Moacir Ponti"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/html/Ribeiro_Scene_Designer_A_Unified_Model_for_Scene_Search_and_Synthesis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/papers/Ribeiro_Scene_Designer_A_Unified_Model_for_Scene_Search_and_Synthesis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Scene Designer is a novel method for searching and generating images using free-hand sketches of scene compositions; i.e. drawings that describe both the appearance and relative positions of objects. Our core contribution is a single unified model to learn both a cross-modal search embedding for matching sketched compositions to images, and an object embedding for layout synthesis. We show that a graph neural network (GNN) followed by Transformer under our novel contrastive learning setting is required to allow learning correlations between object type, appearance and arrangement, driving a mask generation module that synthesises coherent scene layouts, whilst also delivering state of the art sketch based visual search of scenes.",
    "code_link": ""
  },
  "iccv2021_she_supportingreferenceimageryfordigitaldrawing": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "SHE",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Sketching for Human Expressivity",
    "title": "Supporting Reference Imagery for Digital Drawing",
    "authors": [
      "Josh Holinaty",
      "Alec Jacobson",
      "Fanny Chevalier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/html/Holinaty_Supporting_Reference_Imagery_for_Digital_Drawing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/SHE/papers/Holinaty_Supporting_Reference_Imagery_for_Digital_Drawing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "There is little understanding in the challenges artists face when using reference imagery while creating drawings digitally. How can this part of the creative process be better supported during the act of drawing? We conduct formative interviews with artists and reveal many adopt ad hoc strategies when integrating reference into their workflows. Interview results inform the design of a novel sketching interface in form of a technology probe to capture how artists use and access reference imagery, while also addressing opportunities to better support the use of reference, such as just-in-time presentation of imagery, automatic transparency to assist tracing, and features to mitigate design fixation. To capture how reference is used, we tasked artists to complete a series of digital drawings using our probe, with each task having particular reference needs. Artists were quick to adopt and appreciate the novel solutions provided by our probe, and we identified common strategies that can be exploited to support reference imagery in future creative tools.",
    "code_link": ""
  },
  "iccv2021_tradicv_adaptingdeepneuralnetworksforpedestrian-detectiontolow-lightconditionswithoutre-training": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Adapting Deep Neural Networks for Pedestrian-Detection to Low-Light Conditions Without Re-Training",
    "authors": [
      "Vedant Shah",
      "Anmol Agarwal",
      "Tanmay Tulsidas Verlekar",
      "Raghavendra Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Shah_Adapting_Deep_Neural_Networks_for_Pedestrian-Detection_to_Low-Light_Conditions_Without_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Shah_Adapting_Deep_Neural_Networks_for_Pedestrian-Detection_to_Low-Light_Conditions_Without_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Pedestrian detection is an integral component in many automated surveillance applications. Several state-of-the-art systems exist for pedestrian detection, however most of them are ineffective in low-light conditions. Systems specifically designed for low-light conditions require special equipment, such as depth sensing cameras. However, a lack of large publicly available depth datasets, prevents their use in training deep learning models. In this paper we propose a pre-processing pipeline, which enables any existing normal-light pedestrian detection system to operate in low-light conditions. It is based on a signal-processing and traditional computer-vision techniques, such as the use of signal strength of a depth sensing camera (amplitude images) and robust principal component analysis (RPCA). The information in an amplitude image is less noisy, and is of lower dimension than depth data, marking it computationally inexpensive to process. RPCA processes these amplitude images to generate foreground masks, which represent potential regions of interest. These masks can then be used to rectify the RGB images to increase the contrast between the foreground and background, even in low-light conditions. We show that these rectified RGB images can be used by normal-light deep learning models for pedestrian-detection, without any additional training. To test this hypothesis, we use the 'Oyla Low-Light Pedestrian Benchmark' (OLPB) dataset. Our results using two state-of-the art deep learning models (CrowdDet and CenterNet) show: a) The deep models perform poorly as pedestrian detectors in low-light conditions; b) Equipping the deep-networks with our pre-processing pipeline significantly improves the average precision for pedestrian-detection of the models without any re-training. Taken together, the results suggest that our approach could act as a useful pre-processor for deep learning models that aren't specially designed for pedestrian-detection in low-light conditions.",
    "code_link": ""
  },
  "iccv2021_tradicv_robustfacefrontalizationforvisualspeechrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Robust Face Frontalization for Visual Speech Recognition",
    "authors": [
      "Zhiqi Kang",
      "Radu Horaud",
      "Mostafa Sadeghi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Kang_Robust_Face_Frontalization_for_Visual_Speech_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Kang_Robust_Face_Frontalization_for_Visual_Speech_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face frontalization consists of synthesizing a frontally-viewed face from an arbitrarily-viewed one. The main contribution of this paper is a robust frontalization method that preserves non-rigid facial deformations, i.e. expressions, to improve lip reading. The method iteratively estimates the rigid transformation (scale, rotation, and translation) and the non-rigid deformation between 3D landmarks extracted from an arbitrarily-viewed face, and 3D vertices parameterized by a deformable shape model. An important merit of the method is its ability to deal with large Gaussian and non-Gaussian errors in the data. For that purpose, we use the generalized Student-t distribution. The associated EM algorithm assigns a weight to each observed landmark, the higher the weight the more important the landmark, thus favoring landmarks that are only affected by rigid head movements. We propose to use the zero-mean normalized cross-correlation (ZNCC) score to evaluate the ability to preserve facial expressions. Moreover, we show that the method, when incorporated into a deep lip-reading pipeline, considerably improves the word classification score on an in-the-wild benchmark.",
    "code_link": ""
  },
  "iccv2021_tradicv_dc-vinsdynamiccameravisualinertialnavigationsystemwithonlinecalibration": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "DC-VINS: Dynamic Camera Visual Inertial Navigation System With Online Calibration",
    "authors": [
      "Jason Rebello",
      "Chunshang Li",
      "Steven L. Waslander"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Rebello_DC-VINS_Dynamic_Camera_Visual_Inertial_Navigation_System_With_Online_Calibration_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Rebello_DC-VINS_Dynamic_Camera_Visual_Inertial_Navigation_System_With_Online_Calibration_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Visual-inertial (VI) sensor combinations are becoming ubiquitous in a variety of autonomous driving and aerial navigation applications due to their low cost, limited power consumption and complementary sensing capabilities. However, current VI sensor configurations assume a static rigid transformation between the camera and IMU, precluding manipulating the viewpoint of the camera independent of IMU movement which is important in situations with uneven feature distribution and for high-rate dynamic motions. Gimbal stabilized cameras, as seen on most commercially available drones, have seen limited use in SLAM due to the inability to resolve the time-varying extrinsic calibration between the IMU and camera needed in tight sensor fusion. In this paper, we present the online extrinsic calibration between a dynamic camera mounted to an actuated mechanism and an IMU mounted to the body of the vehicle integrated into a Visual Odometry pipeline. In addition, we provide a degeneracy analysis of the calibration parameters leading to a novel parameterization of the actuated mechanism used in the calibration. We build our calibration into the VINS-Fusion package and show that we are able to accurately recover the calibration parameters online while manipulating the viewpoint of the camera to feature rich areas thereby achieving an average RMSE error of 0.26m over an average trajectory length of 340m, 31.45% lower than a traditional visual inertial pipeline with a static camera.",
    "code_link": ""
  },
  "iccv2021_tradicv_finiteaperturestereo3dreconstructionofmacro-scalescenes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Finite Aperture Stereo: 3D Reconstruction of Macro-Scale Scenes",
    "authors": [
      "Matthew Bailey",
      "Adrian Hilton",
      "Jean-Yves Guillemaut"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Bailey_Finite_Aperture_Stereo_3D_Reconstruction_of_Macro-Scale_Scenes_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Bailey_Finite_Aperture_Stereo_3D_Reconstruction_of_Macro-Scale_Scenes_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "While the accuracy of multi-view stereo (MVS) has continued to advance, its performance reconstructing challenging scenes from images with a limited depth of field is generally poor. Typical implementations assume a pinhole camera model, and therefore treat defocused regions as a source of outlier. In this paper, we address these limitations by instead modelling the camera as a thick lens. Doing so allows us to exploit the complementary nature of stereo and defocus information, and overcome constraints imposed by traditional MVS methods. Using our novel reconstruction framework, we recover complete 3D models of complex macro-scale scenes. Our approach demonstrates robustness to view-dependent materials, and outperforms state-of-the-art MVS and depth from defocus across a range of real and synthetic datasets.",
    "code_link": ""
  },
  "iccv2021_tradicv_objectdetectioninclutteredenvironmentswithsparsekeypointselection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Object Detection in Cluttered Environments With Sparse Keypoint Selection",
    "authors": [
      "Viktor Seib",
      "Dietrich Paulus"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Seib_Object_Detection_in_Cluttered_Environments_With_Sparse_Keypoint_Selection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Seib_Object_Detection_in_Cluttered_Environments_With_Sparse_Keypoint_Selection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In cases such as mobile robotic applications with limited computational ressources, traditional approaches might be preferred over neural networks. However, open source solutions using traditional computer vision are harder to find than neural network implementations. In this work we address the task of object detection in cluttered environments in point clouds from RGB-D cameras. We compare several open source implementation available in the Point Cloud Library and present a novel and superior solution for this task. We further propose a novel sparse keypoint selection approach that combines the advantages of uniform sampling and a dedicated keypoint detection algorithm. Our extensive evaluation shows the validity of our approach, which also improves the results of the compared methods. All code is available on our project repository: https://github.com/vseib/point-cloud-donkey.",
    "code_link": "https://github.com/vseib/point-cloud-donkey"
  },
  "iccv2021_tradicv_absoluteandrelativeposeestimationinrefractivemultiview": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Absolute and Relative Pose Estimation in Refractive Multi View",
    "authors": [
      "Xiao Hu",
      "Fran\u00e7ois Lauze",
      "Kim Steenstrup Pedersen",
      "Jean M\u00e9lou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Hu_Absolute_and_Relative_Pose_Estimation_in_Refractive_Multi_View_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Hu_Absolute_and_Relative_Pose_Estimation_in_Refractive_Multi_View_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This paper investigates absolute and relative pose estimation under refraction, which are essential problems for refractive structure from motion. We first present an absolute pose estimation algorithm by leveraging an efficient iterative refinement. Then, we derive a novel refractive epipolar constraint for relative pose estimation. The epipolar constraint is established based on the virtual camera transformation, making it in a succinct form and can be efficiently optimized. Evaluations of the proposed algorithms on synthetic data show superior accuracy and computational efficiency to state-of-the-art methods. For further validation, we demonstrate the performance on real data and show the application in 3D reconstruction of objects under refraction.",
    "code_link": "https://github.com/itseez/opencv"
  },
  "iccv2021_tradicv_effectofparameteroptimizationonclassicalandlearning-basedimagematchingmethods": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Effect of Parameter Optimization on Classical and Learning-Based Image Matching Methods",
    "authors": [
      "Ufuk Efe",
      "Kutalmis Gokalp Ince",
      "A. Aydin Alatan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Efe_Effect_of_Parameter_Optimization_on_Classical_and_Learning-Based_Image_Matching_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Efe_Effect_of_Parameter_Optimization_on_Classical_and_Learning-Based_Image_Matching_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep learning-based image matching methods are improved significantly during the recent years. Although these methods are reported to outperform the classical techniques, the performance of the classical methods is not examined in detail. In this study, we compare classical and learning-based methods by employing mutual nearest neighbor search with ratio test and optimizing the ratio test threshold to achieve the best performance on two different performance metrics. After a fair comparison, the experimental results on HPatches dataset reveal that the performance gap between classical and learning-based methods is not that significant. Throughout the experiments, we demonstrated that SuperGlue is the state-of-the-art technique for the image matching problem on HPatches dataset. However, if a single parameter, namely ratio test threshold, is carefully optimized, a well-known traditional method SIFT performs quite close to SuperGlue and even outperforms in terms of mean matching accuracy (MMA) under 1 and 2 pixel thresholds. Moreover, a recent approach, DFM, which only uses pre-trained VGG features as descriptors and ratio test, is shown to outperform most of the well-trained learning-based methods. Therefore, we conclude that the parameters of any classical method should be analyzed carefully before comparing against a learning-based technique.",
    "code_link": ""
  },
  "iccv2021_tradicv_arobustend-to-endmethodforparametriccurvetracingviasoftcosine-similarity-basedobjectivefunction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "A Robust End-to-End Method for Parametric Curve Tracing via Soft Cosine-Similarity-Based Objective Function",
    "authors": [
      "Boran Han",
      "Jeremy Vila"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Han_A_Robust_End-to-End_Method_for_Parametric_Curve_Tracing_via_Soft_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Han_A_Robust_End-to-End_Method_for_Parametric_Curve_Tracing_via_Soft_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Parametric curve tracing enables wide applications, such as lane following in autonomous driving, volumetric reconstruction in seismic, single-molecule/protein tracking in microscopy. Most existing parametric curve tracing methods require several steps, including curve identification and parameterization. Such multi-step methods can lead to lengthy and complicated parameter optimization. Additionally, the performance of curve identification methods can be degraded by noisy or low-light images. To address these challenges, we present a novel single-step approach to trace curves parametrically via optimizing a self-defined non-linear objective function that describes several key properties of the curve. Under the assumption that signals along the curve resemble each other, our objective function will guide this pathfinding process from a seed point along the direction according to maximum cosine similarity. No pre- and post-processing step is required to measure the tangent or normal vectors. We visualize our objective function and conduct several numerical experiments. These empirical experiments demonstrate that our method outperforms other competing methods across image domains. It yields better accuracy even in low signal-to-noise ratio (SNR) conditions.",
    "code_link": "https://github.com/vonsj0210/Multi-Lane-Detection-Datasetwith-Ground-Truth"
  },
  "iccv2021_tradicv_building3dmorphablemodelsfromasinglescan": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Building 3D Morphable Models From a Single Scan",
    "authors": [
      "Skylar Sutherland",
      "Bernhard Egger",
      "Joshua Tenenbaum"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Sutherland_Building_3D_Morphable_Models_From_a_Single_Scan_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Sutherland_Building_3D_Morphable_Models_From_a_Single_Scan_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a method for constructing generative models of 3D objects from a single 3D mesh. Our method produces a 3D morphable model that represents shape and albedo in terms of Gaussian processes. We define the shape deformations in physical (3D) space and the albedo deformations as a combination of physical-space and color-space deformations. Whereas previous approaches have typically built 3D morphable models from multiple high-quality 3D scans through principal component analysis, we build 3D morphable models from a single scan or template. As we demonstrate in the face domain, these models can be used to infer 3D reconstructions from 2D data (inverse graphics) or 3D data (registration). Specifically, we show that our approach can be used to perform face recognition using only a single 3D scan (one scan total, not one per person), and further demonstrate how multiple scans can be incorporated to improve performance without requiring dense correspondence. Our approach enables the synthesis of 3D morphable models for 3D object categories where dense correspondence between multiple scans is unavailable. We demonstrate this by constructing additional 3D morphable models for fish and birds and use them to perform simple inverse rendering tasks. We share the code used to generate these models and to perform our inverse rendering and registration experiments.",
    "code_link": "https://github.com/skylar-sutherland/single-scan-3dmms"
  },
  "iccv2021_tradicv_atechnicalsurveyandevaluationoftraditionalpointcloudclusteringmethodsforlidarpanopticsegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "A Technical Survey and Evaluation of Traditional Point Cloud Clustering Methods for LiDAR Panoptic Segmentation",
    "authors": [
      "Yiming Zhao",
      "Xiao Zhang",
      "Xinming Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Zhao_A_Technical_Survey_and_Evaluation_of_Traditional_Point_Cloud_Clustering_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Zhao_A_Technical_Survey_and_Evaluation_of_Traditional_Point_Cloud_Clustering_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "LiDAR panoptic segmentation is a newly proposed technical task for autonomous driving. In contrast to popular end-to-end deep learning solutions, we propose a hybrid method with an existing semantic segmentation network to extract semantic information and a traditional LiDAR point cloud cluster algorithm to split each instance object. We argue geometry-based traditional clustering algorithms are worth being considered by showing a state-of-the-art performance among all published end-to-end deep learning solutions on the panoptic segmentation leaderboard of the SemanticKITTI dataset. To our best knowledge, we are the first to attempt the point cloud panoptic segmentation with clustering algorithms. Therefore, instead of working on new models, we give a comprehensive technical survey in this paper by implementing four typical cluster methods and report their performances on the benchmark. Those four cluster methods are the most representative ones with real-time running speed. They are implemented with C++ in this paper and then wrapped as a python function for seamless integration with the existing deep learning frameworks. We release our code for peer researchers who might be interested in this problem here: https://github.com/placeforyiming/ICCVW21-LiDAR-Panoptic-Segmentation-TradiCV-Survey-of-Point-Cloud-Cluster",
    "code_link": ""
  },
  "iccv2021_tradicv_aclosedformsolutionforviewinggraphconstructioninuncalibratedvision": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "A Closed Form Solution for Viewing Graph Construction in Uncalibrated Vision",
    "authors": [
      "Carlo Colombo",
      "Marco Fanfani"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Colombo_A_Closed_Form_Solution_for_Viewing_Graph_Construction_in_Uncalibrated_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Colombo_A_Closed_Form_Solution_for_Viewing_Graph_Construction_in_Uncalibrated_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This paper presents a closed form solution for the problem of computing a set of projective cameras from the fundamental matrices of a given viewing graph. The approach is incremental, exploits trifocal constraints, and does not rely on either image or structure points. Represented by a vector of four parameters that uniquely ensure its consistency with the local trifocal geometry, each newly computed camera is automatically coherent with the projective frame chosen as global reference, thus not needing any a posteriori synchronization. Results of experiments made under controlled conditions show that the proposed approach is relatively resilient to noise, and faster by three orders of magnitude than classical camera resectioning solutions, while reaching a comparable accuracy. This makes our closed form approach a good candidate for camera initialization in scenarios involving large-scale viewing graphs.",
    "code_link": ""
  },
  "iccv2021_tradicv_towardsrealisticsymmetry-basedcompletionofpreviouslyunseenpointclouds": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "Towards Realistic Symmetry-Based Completion of Previously Unseen Point Clouds",
    "authors": [
      "Taras Rumezhak",
      "Oles Dobosevych",
      "Rostyslav Hryniv",
      "Vladyslav Selotkin",
      "Volodymyr Karpiv",
      "Mykola Maksymenko"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Rumezhak_Towards_Realistic_Symmetry-Based_Completion_of_Previously_Unseen_Point_Clouds_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Rumezhak_Towards_Realistic_Symmetry-Based_Completion_of_Previously_Unseen_Point_Clouds_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "3D scanning is a complex multistage process that generates a point cloud of an object typically containing damaged parts due to occlusions, reflections, shadows, scanner motion, specific properties of the object surface, imperfect reconstruction algorithms, etc. Point cloud completion is specifically designed to fill in the missing parts of the object and obtain its high-quality 3D representation. The existing completion approaches perform well on the academic datasets with a predefined set of object classes and very specific types of defects; however, their performance drops significantly in the real-world settings and degrades even further on previously unseen object classes. We propose a novel framework that performs well on symmetric objects, which are ubiquitous in man-made environments. Unlike learning-based approaches, the proposed framework does not require training data and is capable of completing non-critical damages occurring in customer 3D scanning process using e.g. Kinect, time-of-flight, or structured light scanners. With thorough experiments, we demonstrate that the proposed framework achieves state-of-the-art efficiency in point cloud completion of real-world customer scans. We benchmark the framework performance on two types of datasets: properly augmented existing academic dataset and the actual 3D scans of various objects.",
    "code_link": "https://github.com/softserveincrnd/symmetry-3d-completion"
  },
  "iccv2021_tradicv_caftclassawarefrequencytransformforreducingdomaingap": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TradiCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Traditional Computer Vision in the Age of Deep Learning",
    "title": "CAFT: Class Aware Frequency Transform for Reducing Domain Gap",
    "authors": [
      "Vikash Kumar",
      "Sarthak Srivastava",
      "Rohit Lal",
      "Anirban Chakraborty"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Kumar_CAFT_Class_Aware_Frequency_Transform_for_Reducing_Domain_Gap_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Kumar_CAFT_Class_Aware_Frequency_Transform_for_Reducing_Domain_Gap_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This work explores the usage of Fourier Transform for reducing the domain gap between the Source (e.g. Synthetic Image) and Target domain (e.g. Real Image) towards solving the Domain Adaptation problem. Most of the Unsupervised Domain Adaptation (UDA) algorithms reduce the global domain shift between labelled Source and unlabelled Target domain by matching the marginal distribution. UDA performance deteriorates for the cases where the domain gap between Source and Target is significant. To improve the overall performance of the existing UDA algorithms the proposed method attempts to bring the Source domain closer to the Target domain with the help of pseudo label based class consistent low-frequency swapping. This traditional image processing technique results in computational efficiency, especially compared to the state-of-the-art deep learning methods that use complex adversarial training. The proposed method Class Aware Frequency Transformation (CAFT) can easily be plugged into any existing UDA algorithm to improve its performance. We evaluate CAFT on various domain adaptation datasets and algorithms and have achieved performance gains across all the popular benchmarks.",
    "code_link": "https://github.com/vclab-dev/CAFT"
  },
  "iccv2021_cvinhrc_3dsemanticlabeltransferinhuman-robotcollaboration": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVinHRC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Human-Robot Collaborative Factories of the Future",
    "title": "3D Semantic Label Transfer in Human-Robot Collaboration",
    "authors": [
      "D\u00e1vid Rozenberszki",
      "G\u00e1bor S\u00f6r\u00f6s",
      "Szilvia Szeier",
      "Andr\u00e1s L\u0151rincz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Rozenberszki_3D_Semantic_Label_Transfer_in_Human-Robot_Collaboration_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Rozenberszki_3D_Semantic_Label_Transfer_in_Human-Robot_Collaboration_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We tackle two practical problems in robotic scene understanding. First, the computational requirements of current semantic segmentation algorithms are prohibitive for typical robots. Second, the viewpoints of ground robots are quite different from typical human viewpoints of training datasets which may lead to misclassified objects from robot viewpoints. We present a system for sharing and reusing 3D semantic information between multiple agents with different viewpoints. We first co-localize all agents in the same coordinate system. Next, we create a 3D dense semantic model of the space from human viewpoints close to real time. Finally, by re-rendering the model's semantic labels (and/or depth maps) from the ground robots' own estimated viewpoints and sharing them over the network, we can give 3D semantic understanding to simpler agents. We evaluate the reconstruction quality and show how tiny robots can reuse knowledge about the space collected by more capable peers.",
    "code_link": "https://github.com/RozDavid/semantic"
  },
  "iccv2021_cvinhrc_multi-modalvariationalfasterr-cnnforimprovedvisualobjectdetectioninmanufacturing": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVinHRC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Human-Robot Collaborative Factories of the Future",
    "title": "Multi-Modal Variational Faster R-CNN for Improved Visual Object Detection in Manufacturing",
    "authors": [
      "Panagiotis Mouzenidis",
      "Antonios Louros",
      "Dimitrios Konstantinidis",
      "Kosmas Dimitropoulos",
      "Petros Daras",
      "Theofilos Mastos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Mouzenidis_Multi-Modal_Variational_Faster_R-CNN_for_Improved_Visual_Object_Detection_in_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Mouzenidis_Multi-Modal_Variational_Faster_R-CNN_for_Improved_Visual_Object_Detection_in_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Visual object detection is a critical task for a variety of industrial applications, such as robot navigation, quality control and product assembling. Modern industrial environments require AI-based object detection methods that can achieve high accuracy, robustness and generalization. To this end, we propose a novel object detection approach that can process and fuse information from RGB-D images for the accurate detection of industrial objects. The proposed approach utilizes a novel Variational Faster R-CNN algorithm that aims to improve the robustness and generalization ability of the original Faster R-CNN algorithm by employing a VAE encoder-decoder network and a very powerful attention layer. Experimental results on two object detection datasets, namely the well-known RGB-D Washington dataset and the QCONPASS dataset of industrial objects that is first presented in this paper, verify the significant performance improvement achieved when the proposed approach is employed.",
    "code_link": ""
  },
  "iccv2021_cvinhrc_clothmechanicalparameterestimationandsimulationforoptimizedroboticmanipulation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVinHRC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Human-Robot Collaborative Factories of the Future",
    "title": "Cloth Mechanical Parameter Estimation and Simulation for Optimized Robotic Manipulation",
    "authors": [
      "Nikolaos E. Anatoliotakis",
      "Panagiotis Koustoumpardis",
      "Konstantinos Moustakas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Anatoliotakis_Cloth_Mechanical_Parameter_Estimation_and_Simulation_for_Optimized_Robotic_Manipulation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Anatoliotakis_Cloth_Mechanical_Parameter_Estimation_and_Simulation_for_Optimized_Robotic_Manipulation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this article a method for the estimation of cloth simulation parameters is presented. We propose a method, based on already published methods from different fields, that can successfully create the mechanical model of a cloth, based only on a single monocular video source of a cloth been held and moved in the air by two hands. We propose the use of a moving graph generation method using Scale Invariant Feature Transformation (SIFT). Having the moving graph of the real cloth as the goal, a method based on genetic algorithms was designed to produce the mechanical properties of the cloth's mechanical model. This way a simulated cloth with similar mechanical properties will be created. For our experiments a mechanical model based on Provot's mass-spring-damper (MSD) cloth model with adjustable springs and dampers was used. However, we present a method that can be easily adjusted to any particlebased cloth model. The method presented was designed to be easily applicable so as to enable the broader use of cloth models in robotized cloth manipulation tasks. The use of a cloth's digital twin, enables the major part of tuning of a robot controller to be made offline. This will significantly accelerate the tuning process, enabling the broader use of robots in more delicate cloth manipulation tasks. Finally, to prove the validity of our method we provide the results of experiments with cloths of different patterns and physical parameters.",
    "code_link": ""
  },
  "iccv2021_cvinhrc_markerlessvisualtrackingofacontainercranespreader": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVinHRC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Human-Robot Collaborative Factories of the Future",
    "title": "Markerless Visual Tracking of a Container Crane Spreader",
    "authors": [
      "Manolis Lourakis",
      "Maria Pateraki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Lourakis_Markerless_Visual_Tracking_of_a_Container_Crane_Spreader_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Lourakis_Markerless_Visual_Tracking_of_a_Container_Crane_Spreader_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Crane systems play a crucial role in container transport logistics. This paper presents an approach for visually tracking the position and orientation in 3D space of a container crane spreader. An initial pose estimate is first employed to render a 3D triangle mesh model of the spreader as a wireframe with hidden lines removed. The initial pose is then refined so that the visible lines of the wireframe match the straight line segments detected in an input image. Line segment matching relies on fast, local one-dimensional searches along a segment's normal direction. Matched line segments yield constraints on the spreader motion which are processed with robust parameter estimation techniques that safeguard against outliers stemming from mismatches. The tracker automatically determines the visibility of segments, without making limiting assumptions regarding the spreader's 3D mesh model. It is also robust to parts of the tracked spreader being out of view, occluded, shadowed or simply undetected. Experimental results demonstrating the tracker's performance are additionally included.",
    "code_link": ""
  },
  "iccv2021_cvinhrc_instanceposefast6dofposeestimationformultipleobjectsfromasinglergbimage": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVinHRC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Human-Robot Collaborative Factories of the Future",
    "title": "InstancePose: Fast 6DoF Pose Estimation for Multiple Objects From a Single RGB Image",
    "authors": [
      "Lee Aing",
      "Wen-Nung Lie",
      "Jui-Chiu Chiang",
      "Guo-Shiang Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Aing_InstancePose_Fast_6DoF_Pose_Estimation_for_Multiple_Objects_From_a_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Aing_InstancePose_Fast_6DoF_Pose_Estimation_for_Multiple_Objects_From_a_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "6DoF object pose estimation depends on positional accuracy, implementation complexity and processing speed. This study presents a method to estimate 6DoF object poses for multi-instance object detection that requires less time and is accurate. The proposed method uses a deep neural network, which outputs 4 types of feature maps: the error object mask, semantic object masks, center vector maps (CVM) and 6D coordinate maps. These feature maps are combined in post processing to detect and estimate multi-object 2D-3D correspondences in parallel for PnP RANSAC estimation. The experiments show that the method can process input RGB images containing 7 different object categories/ instances at a speed of 25 frames per second with competitive accuracy, compared with current state-of-the-art methods, which focus only on some specific conditions.",
    "code_link": ""
  },
  "iccv2021_cvinhrc_ananomalydetectionsystemviamovingsurveillancerobotswithhumancollaboration": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVinHRC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in Human-Robot Collaborative Factories of the Future",
    "title": "An Anomaly Detection System via Moving Surveillance Robots With Human Collaboration",
    "authors": [
      "Muhammad Zaigham Zaheer",
      "Arif Mahmood",
      "M. Haris Khan",
      "Marcella Astrid",
      "Seung-Ik Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Zaheer_An_Anomaly_Detection_System_via_Moving_Surveillance_Robots_With_Human_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Zaheer_An_Anomaly_Detection_System_via_Moving_Surveillance_Robots_With_Human_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Autonomous anomaly detection is a fundamental step in visual surveillance systems, and so we have witnessed great progress in the form of various promising algorithms. Nonetheless, majority of prior algorithms assume static surveillance cameras that severely restricts the coverage of the system unless the number of cameras is exponentially increased, consequently increasing both the installation and monitoring costs. In the current work we propose an anomaly detection system based on mobile surveillance cameras, i.e., moving robot which continuously navigates a target area. We compare the newly acquired test images with a database of normal images using geo-tags. For anomaly detection, a Siamese network is trained which analyses two input images for anomalies while ignoring the viewpoint differences. Further, our system is capable of updating the normal images database with human collaboration. Finally, we propose a new dataset that is captured by repeated visits of the robot over a target area. Our experiments demonstrate the effectiveness of the proposed system for anomaly detection using mobile surveillance robots.",
    "code_link": ""
  },
  "iccv2021_xsanim_localize,group,andselectboostingtext-vqabyscenetextmodeling": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "XSAnim",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Crossmodal Social Animation",
    "title": "Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling",
    "authors": [
      "Xiaopeng Lu",
      "Zhen Fan",
      "Yansen Wang",
      "Jean Oh",
      "Carolyn P. Ros\u00e9"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/XSAnim/html/Lu_Localize_Group_and_Select_Boosting_Text-VQA_by_Scene_Text_Modeling_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/XSAnim/papers/Lu_Localize_Group_and_Select_Boosting_Text-VQA_by_Scene_Text_Modeling_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "As an important task in multimodal context understanding, Text-VQA aims at question answering through reading text information in images. It differentiates from the original VQA task as Text-VQA requires large amounts of scene-text relationship understanding, in addition to the cross-modal grounding capability. In this paper, we propose LOGOS (Localize, Group, and Select), a novel model which attempts to tackle this problem from multiple aspects. LOGOS leverages two grounding tasks to better localize the key information of the image, utilizes scene text clustering to group individual OCR tokens, and learns to select the best answer from different sources of OCR texts. Experiments show that LOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks without using additional OCR annotation data. Ablation studies and analysis demonstrate the capability of LOGOS to bridge different modalities and better understand scene text.",
    "code_link": ""
  },
  "iccv2021_xsanim_signposesignlanguageanimationthrough3dposelifting": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "XSAnim",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Crossmodal Social Animation",
    "title": "SignPose: Sign Language Animation Through 3D Pose Lifting",
    "authors": [
      "Shyam Krishna",
      "Vijay Vignesh P",
      "Dinesh Babu J"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/XSAnim/html/Krishna_SignPose_Sign_Language_Animation_Through_3D_Pose_Lifting_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/XSAnim/papers/Krishna_SignPose_Sign_Language_Animation_Through_3D_Pose_Lifting_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Sign Language Generation (SLG) is a challenging task in computer animation as it involves capturing intricate hand gestures accurately, for several thousand signs in each sign language. Traditional methods require expensive equipment and considerable human involvement. In this paper, we provide a method to automate this process using only plain RGB images to generate sign poses for an avatar - the first of its kind for SLG. Current state of the art models for human 3D pose estimation do not perform satisfactorily in SLG due to the large difference between tasks. The datasets they are trained on contain only tasks like walking and playing sports, which involve significantly different types of motion compared to signing. Synthetic, manually created 3D animations are available for diverse tasks including sign language performance. Modern 2D pose estimation models which work on real world images are also robust enough to work on these animations accurately. Inspired by this, we formulate a novel method of leveraging animation data, using an intermediate 2D pose representation, to train an SLG animation model that works on real world sign language performance videos. To create the dataset for training, we extend an available animated dataset of signs in the Indian Sign Language (ISL) by permuting different hand and body motions. A novel quaternion based architecture is created to perform the task of lifting the 2D keypoints to 3D. The architecture is simplified to match the requirements of our task as well as to work with our smaller dataset size. We train a model, SignPose, using this architecture on the constructed dataset and demonstrate that it matches or outperforms current models for human pose reconstruction for the Sign Language Generation task. We will release both the dataset as well the model to the public to encourage further research in this field.",
    "code_link": ""
  },
  "iccv2021_vspw_aunifiedefficientpyramidtransformerforsemanticsegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VSPW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Video Scene Parsing in the Wild",
    "title": "A Unified Efficient Pyramid Transformer for Semantic Segmentation",
    "authors": [
      "Fangrui Zhu",
      "Yi Zhu",
      "Li Zhang",
      "Chongruo Wu",
      "Yanwei Fu",
      "Mu Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VSPW/html/Zhu_A_Unified_Efficient_Pyramid_Transformer_for_Semantic_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VSPW/papers/Zhu_A_Unified_Efficient_Pyramid_Transformer_for_Semantic_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Semantic segmentation is a challenging problem due to difficulties in modeling context in complex scenes and class confusions along boundaries. Most literature either focuses on context modeling or boundary refinement, which is less generalizable in open-world scenarios. In this work, we advocate a unified framework(UN-EPT) to segment objects by considering both context information and boundary artifacts. We first adapt a sparse sampling strategy to incorporate the transformer-based attention mechanism for efficient context modeling. In addition, a separate spatial branch is introduced to capture image details for boundary refinement. The whole model can be trained in an end-to-end manner. We demonstrate promising performance on three popular benchmarks for semantic segmentation with low memory footprint.",
    "code_link": ""
  },
  "iccv2021_vspw_liteedgelightweightsemanticedgedetectionnetwork": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VSPW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Video Scene Parsing in the Wild",
    "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
    "authors": [
      "Hao Wang",
      "Hasan Mohamed",
      "Zuowen Wang",
      "Bodo Rueckauer",
      "Shih-Chii Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VSPW/html/Wang_LiteEdge_Lightweight_Semantic_Edge_Detection_Network_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VSPW/papers/Wang_LiteEdge_Lightweight_Semantic_Edge_Detection_Network_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Scene parsing is a critical component for understanding complex scenes in applications such as autonomous driving. Semantic segmentation networks are typically reported for scene parsing but semantic edge networks have also become of interest because of the sparseness of the segmented maps. This work presents an end-to-end trained lightweight deep semantic edge detection architecture called LiteEdge suitable for edge deployment. By utilizing hierarchical supervision and a new weighted multi-label loss function to balance different edge classes during training, LiteEdge predicts with high accuracy category-wise binary edges. Our LiteEdge network with only approximately 3M parameters, has a semantic edge prediction accuracy of 52.9% mean maximum F (MF) score on the Cityscapes dataset. This accuracy was evaluated on the network trained to produce a low resolution edge map. The network can be quantized to 6-bit weights and 8-bit activations and shows only a 2% drop in the mean MF score. This quantization leads to a memory footprint savings of 6X for an edge device.",
    "code_link": ""
  },
  "iccv2021_vspw_semanticsegmentationwithmultiscalespatialattentionforselfdrivingcars": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VSPW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Video Scene Parsing in the Wild",
    "title": "Semantic Segmentation With Multi Scale Spatial Attention for Self Driving Cars",
    "authors": [
      "Abhinav Sagar",
      "RajKumar Soundrapandiyan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VSPW/html/Sagar_Semantic_Segmentation_With_Multi_Scale_Spatial_Attention_for_Self_Driving_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VSPW/papers/Sagar_Semantic_Segmentation_With_Multi_Scale_Spatial_Attention_for_Self_Driving_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we present a novel neural network using multi scale feature fusion at various scales for accurate and efficient semantic image segmentation. We used ResNet based feature extractor, dilated convolutional layers in downsampling part, atrous convolutional layers in the upsampling part and used concat operation to merge them. A new attention module is proposed to encode more contextual information and enhance the receptive field of the network. We present an in depth theoretical analysis of our network with training and optimization details. Our network was trained and tested on the Camvid dataset and Cityscapes dataset using mean accuracy per class and Intersection Over Union (IOU) as the evaluation metrics. Our model outperforms previous state of the art methods on semantic segmentation achieving mean IOU value of 74.12 while running at >100 FPS.",
    "code_link": ""
  },
  "iccv2021_vot_learningspatio-appearancememorynetworkforhigh-performancevisualtracking": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Object Tracking",
    "title": "Learning Spatio-Appearance Memory Network for High-Performance Visual Tracking",
    "authors": [
      "Fei Xie",
      "Wankou Yang",
      "Kaihua Zhang",
      "Bo Liu",
      "Guangting Wang",
      "Wangmeng Zuo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Xie_Learning_Spatio-Appearance_Memory_Network_for_High-Performance_Visual_Tracking_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/papers/Xie_Learning_Spatio-Appearance_Memory_Network_for_High-Performance_Visual_Tracking_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Segmentation-based tracking is currently a promising tracking paradigm due to the robustness towards non-grid deformations, comparing to the traditional box-based tracking methods. However, existing segmentation-based trackers are insufficient in modeling and exploiting dense pixel-wise correspondence across frames. To overcome these limitations, this paper presents a novel segmentation-based tracking architecture equipped with spatio-appearance memory networks. The appearance memory bank utilizes spatio-temporal non-local similarity to propagate segmentation mask to the current frame, which can effectively capture long-range appearance variations and we further treat discriminative correlation filter as spatial memory bank to store the mapping between feature map and spatial map. Moreover, mutual promotion on dual memory networks greatly boost the overall tracking performance. We further propose a dynamic memory machine (DMM) which employs the Earth Mover's Distance (EMD) to reweight memory samples. Without bells and whistles, our simple-yet-effective tracking architecture sets a new state-of-the-art on six tracking benchmarks. Besides, our approach achieves comparable results on two video object segmentation benchmarks. Code and model are released at https://github.com/phiphiphi31/DMB.",
    "code_link": "https://github.com/phiphiphi31/DMB"
  },
  "iccv2021_vot_theninthvisualobjecttrackingvot2021challengeresults": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Object Tracking",
    "title": "The Ninth Visual Object Tracking VOT2021 Challenge Results",
    "authors": [
      "Matej Kristan",
      "Ji\u0159\u00ed Matas",
      "Ale\u0161 Leonardis",
      "Michael Felsberg",
      "Roman Pflugfelder",
      "Joni-Kristian K\u00e4m\u00e4r\u00e4inen",
      "Hyung Jin Chang",
      "Martin Danelljan",
      "Luka Cehovin",
      "Alan Luke\u017ei\u010d",
      "Ondrej Drbohlav",
      "Jani K\u00e4pyl\u00e4",
      "Gustav H\u00e4ger",
      "Song Yan",
      "Jinyu Yang",
      "Zhongqun Zhang",
      "Gustavo Fern\u00e1ndez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Kristan_The_Ninth_Visual_Object_Tracking_VOT2021_Challenge_Results_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/papers/Kristan_The_Ninth_Visual_Object_Tracking_VOT2021_Challenge_Results_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative. Results of 71 trackers are presented; many are state-of-the-art trackers published at major com- puter vision conferences or in journals in recent years. The VOT2021 challenge was composed of four sub-challenges focusing on different tracking domains: (i) VOT-ST2021 challenge focused on short-term tracking in RGB, (ii) VOT- RT2021 challenge focused on \"real-time\" short-term track- ing in RGB, (iii) VOT-LT2021 focused on long-term track- ing, namely coping with target disappearance and reap- pearance and (iv) VOT-RGBD2021 challenge focused on long-term tracking in RGB and depth imagery. The VOT- ST2021 dataset was refreshed, while VOT-RGBD2021 in- troduces a training dataset and sequestered dataset for win- ner identification. The source code for most of the trackers, the datasets, the evaluation kit and the results along with the source code for most trackers are publicly available at the challenge website.",
    "code_link": ""
  },
  "iccv2021_vot_learningtrackingrepresentationsviadual-branchfullytransformernetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Object Tracking",
    "title": "Learning Tracking Representations via Dual-Branch Fully Transformer Networks",
    "authors": [
      "Fei Xie",
      "Chunyu Wang",
      "Guangting Wang",
      "Wankou Yang",
      "Wenjun Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Xie_Learning_Tracking_Representations_via_Dual-Branch_Fully_Transformer_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/papers/Xie_Learning_Tracking_Representations_via_Dual-Branch_Fully_Transformer_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present a Siamese-like Dual-branch network based on solely Transformers for tracking. Given a template and a search image, we divide them into non-overlapping patches and extract a feature vector for each patch based on its matching results with others within an attention window. For each token, we estimate whether it contains the target object and the corresponding size. The advantage of the approach is that the features are learned from matching, and ultimately, for matching. So the features are aligned with the object tracking task. The method achieves better or comparable results as the best-performing methods which first use CNN to extract features and then use Transformer to fuse them. It outperforms the state-of-the-art methods on the GOT-10k and VOT2020 benchmarks. In addition, the method achieves real-time inference speed (about 40 fps) on one GPU. The code and models are released at https://github.com/phiphiphi31/DualTFR",
    "code_link": ""
  },
  "iccv2021_vot_isfirstpersonvisionchallengingforobjecttracking?": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Visual Object Tracking",
    "title": "Is First Person Vision Challenging for Object Tracking?",
    "authors": [
      "Matteo Dunnhofer",
      "Antonino Furnari",
      "Giovanni Maria Farinella",
      "Christian Micheloni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Dunnhofer_Is_First_Person_Vision_Challenging_for_Object_Tracking_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VOT/papers/Dunnhofer_Is_First_Person_Vision_Challenging_for_Object_Tracking_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Understanding human-object interactions is fundamental in First Person Vision (FPV). Tracking algorithms which follow the objects manipulated by the camera wearer can provide useful cues to effectively model such interactions. Visual tracking solutions available in the computer vision literature have significantly improved their performance in the last years for a large variety of target objects and tracking scenarios. However, despite a few previous attempts to exploit trackers in FPV applications, a methodical analysis of the performance of state-of-the-art trackers in this domain is still missing. In this paper, we fill the gap by presenting the first systematic study of object tracking in FPV. Our study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure. This is achieved through TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV is challenging, which suggests that more research efforts should be devoted to this problem so that tracking could benefit FPV tasks.",
    "code_link": ""
  },
  "iccv2021_v4v_automaticregion-basedheartratemeasurementusingremotephotoplethysmography": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "V4V",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision for Vitals",
    "title": "Automatic Region-Based Heart Rate Measurement Using Remote Photoplethysmography",
    "authors": [
      "Benjamin Kossack",
      "Eric Wisotzky",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/html/Kossack_Automatic_Region-Based_Heart_Rate_Measurement_Using_Remote_Photoplethysmography_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/papers/Kossack_Automatic_Region-Based_Heart_Rate_Measurement_Using_Remote_Photoplethysmography_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This paper presents a model-based approach to measure the vital signs from RGB video files focusing on the heart rate. We use the plane-orthogonal-to-skin (POS) remote photoplethysmography (rPPG) transformation performed individually at five well-defined regions of interest (ROI) in the face. We extract the heart rate information by a correlation of the different rPPG signals in these ROIs and a magnitude-based reliability calculation. This increases the robustness of the heart rate extraction from videos. With this method, we achieve a mean of all calculated mean-absolute-errors of 8.324 BPM in the V4V-Challenge data (averaged over all videos of the training and validation set).",
    "code_link": ""
  },
  "iccv2021_v4v_lcomslabsapproachtothevisionforvitals(v4v)challenge": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "V4V",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision for Vitals",
    "title": "LCOMS Lab's Approach to the Vision for Vitals (V4V) Challenge",
    "authors": [
      "Yassine Ouzar",
      "Djamaleddine Djeldjli",
      "Fr\u00e9d\u00e9ric Bousefsaf",
      "Choubeila Maaoui"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/html/Ouzar_LCOMS_Labs_Approach_to_the_Vision_for_Vitals_V4V_Challenge_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/papers/Ouzar_LCOMS_Labs_Approach_to_the_Vision_for_Vitals_V4V_Challenge_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present in this paper the LCOMS Lab's approach to the 1st Vision For Vitals (V4V) Challenge organized within ICCV2021. The V4V challenge was focused on computer vision methods for vitals signs measurement from facial videos, including pulse rate (PR) and respiratory rate. We propose a novel end-to-end architecture based on a deep spatiotemporal network for pulse rate estimation from facial video recordings. Unlike existing methods, we predict the pulse rate value directly without passing by iPPG signal extraction and without incorporating any prior knowledge or additional processing steps. We built our network using 3D Depthwise Separable Convolution layers with residual connections to extract spatial and temporal features simultaneously. This is very suitable for real-time measurement because it requires a reduced number of parameters and a short video fragment. The obtained results seem very satisfactory and promising, especially since the experiments were conducted in challenging dataset collected in uncontrolled conditions.",
    "code_link": ""
  },
  "iccv2021_v4v_beat-to-beatcardiacpulseratemeasurementfromvideo": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "V4V",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision for Vitals",
    "title": "Beat-To-Beat Cardiac Pulse Rate Measurement From Video",
    "authors": [
      "Brian L. Hill",
      "Xin Liu",
      "Daniel McDuff"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/html/Hill_Beat-To-Beat_Cardiac_Pulse_Rate_Measurement_From_Video_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/papers/Hill_Beat-To-Beat_Cardiac_Pulse_Rate_Measurement_From_Video_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Non-invasive cardiac sensing has many applications. Cameras specifically are ubiquitous, low-cost, spatial sensors that can also be used to capture context alongside physiological signals. However, sufficient precision is necessary for this technology to have an impact and for it to be trusted. Benchmark datasets and competitions have contributed significantly to advancing the state-of-the-art methods and improving transparency. We present an entry to the vision for vitals (V4V) challenge.",
    "code_link": ""
  },
  "iccv2021_v4v_thefirstvisionforvitals(v4v)challengefornon-contactvideo-basedphysiologicalestimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "V4V",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision for Vitals",
    "title": "The First Vision for Vitals (V4V) Challenge for Non-Contact Video-Based Physiological Estimation",
    "authors": [
      "Ambareesh Revanur",
      "Zhihua Li",
      "Umur A. Ciftci",
      "Lijun Yin",
      "L\u00e1szl\u00f3 A. Jeni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/html/Revanur_The_First_Vision_for_Vitals_V4V_Challenge_for_Non-Contact_Video-Based_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/papers/Revanur_The_First_Vision_for_Vitals_V4V_Challenge_for_Non-Contact_Video-Based_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Telehealth has the potential to offset the high demand for help during public health emergencies, such as the COVID-19 pandemic. Remote Photoplethysmography (rPPG) - the problem of non-invasively estimating blood volume variations in the microvascular tissue from video - would be well suited for these situations. Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the 1st Vision for Vitals Challenge (V4V) presented a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population. In this paper, we outline the evaluation protocol, the data used, and the results. V4V is to be held in conjunction with the 2021 International Conference on Computer Vision.",
    "code_link": ""
  },
  "iccv2021_v4v_estimatingheartratefromunlabelledvideo": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "V4V",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision for Vitals",
    "title": "Estimating Heart Rate From Unlabelled Video",
    "authors": [
      "John Gideon",
      "Simon Stent"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/html/Gideon_Estimating_Heart_Rate_From_Unlabelled_Video_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/V4V/papers/Gideon_Estimating_Heart_Rate_From_Unlabelled_Video_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We describe our entry for the ICCV 2021 Vision4Vitals Workshop heart rate challenge, in which the goal is to estimate the heart rate of human subjects from facial video. While the challenge dataset contains extensive training data with ground truth blood pressure and heart rate signals, and therefore affords supervised learning, we pursue a different approach. We disregard the available ground truth blood pressure data entirely and instead seek to learn the photoplethysomgraphy (PPG) signal visible in subjects' faces via a self-supervised contrastive learning technique. Since this approach does not require ground truth data, and since the challenge competition rules allow it, we therefore can train directly on test set videos. To boost performance further, we learn a supervised heart rate estimator on top of our \"discovered\" PPG signal, which more explicitly tries to match the ground truth heart rate. Our final approach ranked first on the competition test set, achieving a mean absolute error of 9.22 beats per minute.",
    "code_link": ""
  },
  "iccv2021_visdrone_giaotrackeracomprehensiveframeworkformcmotwithglobalinformationandoptimizingstrategiesinvisdrone2021": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "GIAOTracker: A Comprehensive Framework for MCMOT With Global Information and Optimizing Strategies in VisDrone 2021",
    "authors": [
      "Yunhao Du",
      "Junfeng Wan",
      "Yanyun Zhao",
      "Binyu Zhang",
      "Zhihang Tong",
      "Junhao Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Du_GIAOTracker_A_Comprehensive_Framework_for_MCMOT_With_Global_Information_and_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Du_GIAOTracker_A_Comprehensive_Framework_for_MCMOT_With_Global_Information_and_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In recent years, algorithms for multiple object tracking tasks have benefited from great progresses in deep models and video quality. However, in challenging scenarios like drone videos, they still suffer from problems, such as small objects, camera movements and view changes. In this paper, we propose a new multiple object tracker, which employs Global Information And some Optimizing strategies, named GIAOTracker. It consists of three stages, i.e., online tracking, global link and post-processing. Given detections in every frame, the first stage generates reliable tracklets using information of camera motion, object motion and object appearance. Then they are associated into trajectories by exploiting global clues and refined through four post-processing methods. With the effectiveness of the three stages, GIAOTracker achieves state-of-the-art performance on the VisDrone MOT dataset and wins the 3rd place in the VisDrone2021 MOT Challenge.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "iccv2021_visdrone_vistrongerdetstrongervisualinformationforobjectdetectioninvisdroneimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "VistrongerDet: Stronger Visual Information for Object Detection in VisDrone Images",
    "authors": [
      "Junfeng Wan",
      "Binyu Zhang",
      "Yanyun Zhao",
      "Yunhao Du",
      "Zhihang Tong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Wan_VistrongerDet_Stronger_Visual_Information_for_Object_Detection_in_VisDrone_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Wan_VistrongerDet_Stronger_Visual_Information_for_Object_Detection_in_VisDrone_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Existing methods are especially difficult to detect objects accurately in videos and images captured by UAV. In the work, we carefully analyze the characteristics of VisDrone DET 2021 dataset, and the main reasons for the low detection performance are tiny objects, wide scale span, long-tail distribution, confusion of similar classes. To mitigate the adverse influences caused thereby, we propose a novel detector named VistrongerDet, which possesses Stronger Visual Information. Our framework integrates the novel components of FPN level, ROI level and head level enhancements. Benefitted from the overall enhancements, VistrongerDet significantly improves the detection performance. Without bells and whistles, VistrongerDet is pluggable which can be used in any FPN-based two-stage detectors. It achieves 1.23 points and 1.15 points higher Average Precision (AP) than Faster R-CNN and Cascade R-CNN on VisDrone-DET test-dev set.",
    "code_link": ""
  },
  "iccv2021_visdrone_visdrone-mot2021thevisionmeetsdronemultipleobjecttrackingchallengeresults": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "VisDrone-MOT2021: The Vision Meets Drone Multiple Object Tracking Challenge Results",
    "authors": [
      "Guanlin Chen",
      "Wenguan Wang",
      "Zhijian He",
      "Lujia Wang",
      "Yixuan Yuan",
      "Dingwen Zhang",
      "Jinglin Zhang",
      "Pengfei Zhu",
      "Luc Van Gool",
      "Junwei Han",
      "Steven Hoi",
      "Qinghua Hu",
      "Ming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Chen_VisDrone-MOT2021_The_Vision_Meets_Drone_Multiple_Object_Tracking_Challenge_Results_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Chen_VisDrone-MOT2021_The_Vision_Meets_Drone_Multiple_Object_Tracking_Challenge_Results_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The Vision Meets Drone (VisDrone2021) Multiple Object Tracking (MOT) is the forth annual activity organizedby the VisDrone team focusing on evaluating UAV MOT tracking algorithms. It is held in conjunction with the International Conference on Computer Vision (ICCV 2021).The VisDrone-MOT2021 provides 96 challenging video sequences, including 56 sequences ( 24K frames) for train-ing , 7 sequences ( 3K frames) for validation and 33 sequences ( 13K frames) for testing. Each frame in the sequences are manually annotated with bounding boxes of different categories of objects. Additionally, occlusion ratio and truncation ratio are provided as extra useful annotations. The results of 8 state-of-the-art MOT algorithmsare presented and reported in detail. We hope to facilitate future research and applications in the field of UAV vision by organizing VisDrone-MOT2021 challenge. More information about the challenge can be found at http://www.aiskyeye.com/. Key words: VisDrone, multi-object tracking, drone, challenge, benchmark",
    "code_link": ""
  },
  "iccv2021_visdrone_tph-yolov5improvedyolov5basedontransformerpredictionheadforobjectdetectionondrone-capturedscenarios": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-Captured Scenarios",
    "authors": [
      "Xingkui Zhu",
      "Shuchang Lyu",
      "Xu Wang",
      "Qi Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhu_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Zhu_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Object detection on drone-captured scenarios is a recent popular task. As drones always navigate in different altitudes, the object scale varies violently, which burdens the optimization of networks. Moreover, high-speed and low-altitude flight bring in the motion blur on the densely packed objects, which leads to great challenge of object distinction. To solve the two issues mentioned above, we propose TPH-YOLOv5. Based on YOLOv5, we add one more prediction head to detect different-scale objects. Then we replace the original prediction heads with Transformer Prediction Heads (TPH) to explore the prediction potential with self-attention mechanism. We also integrate convolutional block attention model (CBAM) to find attention region on scenarios with dense objects. To achieve more improvement of our proposed TPH-YOLOv5, we provide bags of useful strategies such as data augmentation, multiscale testing, multi-model integration and utilizing extra classifier. Extensive experiments on dataset VisDrone2021 show that TPH-YOLOv5 have good performance with impressive interpretability on drone-captured scenarios. On DET-test-challenge dataset, the AP result of TPH-YOLOv5 are 39.18%, which is better than previous SOTA method (DPNetV3) by 1.81%. On VisDrone Challenge 2021, TPHYOLOv5 wins 5th place and achieves well-matched results with 1st place model (AP 39.43%). Compared to baseline model (YOLOv5), TPH-YOLOv5 improves about 7%, which is encouraging and competitive.",
    "code_link": ""
  },
  "iccv2021_visdrone_vit-yolotransformer-basedyoloforobjectdetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "ViT-YOLO:Transformer-Based YOLO for Object Detection",
    "authors": [
      "Zixiao Zhang",
      "Xiaoqiang Lu",
      "Guojin Cao",
      "Yuting Yang",
      "Licheng Jiao",
      "Fang Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhang_ViT-YOLOTransformer-Based_YOLO_for_Object_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Zhang_ViT-YOLOTransformer-Based_YOLO_for_Object_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Drone captured images have overwhelming characteristics including dramatic scale variance, complicated background filled with distractors, and flexible viewpoints, which pose enormous challenges for general object detectors based on common convolutional networks. Recently, the design of vision backbone architectures that use selfattention is an exciting topic. In this work, an improved backbone MHSA-Darknet is designed to retain sufficient global context information and extract more differentiated features for object detection via multi-head self-attention. Regarding the path-aggregation neck, we present a simpleyethighlyeffectiveweightedbi-directionalfeaturepyramid network (BiFPN) for effectively cross-scale feature fusion. In addition, other techniques including time-test augmentation (TTA) and wighted boxes fusion (WBF) help to achieve better accuracy and robustness. Our experiments demonstrate that ViT-YOLO significantly outperforms the state-of-the-art detectors and achieve one of the top resultsinVisDrone-DET2021challenge(39.41mAPfortestchallenge data set and 41 mAP for the test-dev data set).",
    "code_link": ""
  },
  "iccv2021_visdrone_visdrone-cc2021thevisionmeetsdronecrowdcountingchallengeresults": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "VisDrone-CC2021: The Vision Meets Drone Crowd Counting Challenge Results",
    "authors": [
      "Zhihao Liu",
      "Zhijian He",
      "Lujia Wang",
      "Wenguan Wang",
      "Yixuan Yuan",
      "Dingwen Zhang",
      "Jinglin Zhang",
      "Pengfei Zhu",
      "Luc Van Gool",
      "Junwei Han",
      "Steven Hoi",
      "Qinghua Hu",
      "Ming Liu",
      "Junwen Pan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Liu_VisDrone-CC2021_The_Vision_Meets_Drone_Crowd_Counting_Challenge_Results_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Liu_VisDrone-CC2021_The_Vision_Meets_Drone_Crowd_Counting_Challenge_Results_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Crowding counting research evolves quickly by the leverage of development in deep learning. Many researchers put their efforts into crowd counting tasks and have achieved many significant improvements. However, current datasets still barely satisfy this evolution and high quality evaluation data is urgent. Motivated by high quality and quantity study in crowding counting, we collect a drone-captured dataset formed by 5,468 images(images in RGB and thermal appear in pairs and 2,734 respectively). There are 1,807 pairs of images for training, and 927 pairs for testing. We manually annotate persons with points in each frame. Based on this dataset, we organized the Vision Meets Drone Crowd Counting Challenge(Visdrone-CC2021) in conjunction with the International Conference on Computer Vision (ICCV 2021). Our challenge attracts many researchers to join, which pave the road of speed up the milestone in crowding counting. To summarize the competition, we select the most remarkable algorithms from participants' submissions and provide a detailed analysis of the evaluation results. More information can be found at the website: http://www.aiskyeye.com/.",
    "code_link": ""
  },
  "iccv2021_visdrone_coarse-graineddensitymapguidedobjectdetectioninaerialimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "Coarse-Grained Density Map Guided Object Detection in Aerial Images",
    "authors": [
      "Chengzhen Duan",
      "Zhiwei Wei",
      "Chi Zhang",
      "Siying Qu",
      "Hongpeng Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Duan_Coarse-Grained_Density_Map_Guided_Object_Detection_in_Aerial_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Duan_Coarse-Grained_Density_Map_Guided_Object_Detection_in_Aerial_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Object detection in aerial images is challenging for at least two reasons: (1) most objects are small scale relative to high resolution aerial images; and (2) the object position distribution is nonuniform, making the detection inefficient. In this paper, a novel network, the coarse-grained density map network (CDMNet), is proposed to address these problems. Specifically, we format density maps into coarse-grained form and design a lightweight dual task density estimation network. The coarse-grained density map can not only describe the distribution of objects, but also cluster objects, quantify scale and reduce computing. In addition, we propose a cluster region generation algorithm guided by density maps to crop input images into multiple subregions, denoted clusters, where the objects are adjusted in a reasonable scale. Besides, we improved mosaic data augmentation to relieve foreground-background and category imbalance problems during detector training. Evaluated on two popular aerial datasets, VisDrone and UAVDT, CDMNet has achieved significant accuracy improvement compared with previous state-of-the-art methods.",
    "code_link": ""
  },
  "iccv2021_visdrone_visdrone-det2021thevisionmeetsdroneobjectdetectionchallengeresults": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "VisDrone-DET2021: The Vision Meets Drone Object Detection Challenge Results",
    "authors": [
      "Yaru Cao",
      "Zhijian He",
      "Lujia Wang",
      "Wenguan Wang",
      "Yixuan Yuan",
      "Dingwen Zhang",
      "Jinglin Zhang",
      "Pengfei Zhu",
      "Luc Van Gool",
      "Junwei Han",
      "Steven Hoi",
      "Qinghua Hu",
      "Ming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Cao_VisDrone-DET2021_The_Vision_Meets_Drone_Object_Detection_Challenge_Results_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Cao_VisDrone-DET2021_The_Vision_Meets_Drone_Object_Detection_Challenge_Results_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Object detection on the drone faces a great diversity of challenges such as small object inference, background clutter and wide viewpoint. In contrast to traditional detection problem in computer vision, object detection in bird-like angle can not be transplanted directly from common-in-use methods due to special object texture in sky`s view. However, there are few algorithms focusing on crowd counting on the drone-captured data due to the lack of comprehensive datasets. To this end, we collect a large-scale dataset and organize Vision Meets Drones: A Challenge (VisDrone2021) in conjunction with IEEE International Conference on Computer Vision (ICCV 2021) to promote the developments in the related fields. The collected dataset is formed by 3,190 images, including 1,610 images for training, and 1,580 images for testing. Specifically, teams are required to predict the bounding boxes of objects of ten predefined classes. We received results from many teams using different methods, and this article introduces the methods of the top 10 teams. We provide a detailed analysis of the evaluation results and conclude the challenge. More information can be found at the website: http://www.aiskyeye.com.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "iccv2021_visdrone_tacklingthebackgroundbiasinsparseobjectdetectionviacroppedwindows": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "VisDrone",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Vision Meets Drones: A Challenge",
    "title": "Tackling the Background Bias in Sparse Object Detection via Cropped Windows",
    "authors": [
      "Leon Amadeus Varga",
      "Andreas Zell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Varga_Tackling_the_Background_Bias_in_Sparse_Object_Detection_via_Cropped_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Varga_Tackling_the_Background_Bias_in_Sparse_Object_Detection_via_Cropped_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging task. The recordings are mostly sparse and contain only small objects. In this work, we propose a simple tiling method that improves the detection capability in the remote sensing case. We identified one core component of many tiling approaches and extracted an easy to implement preprocessing step. By reducing the background bias and enabling the usage of higher image resolutions during training, our method can improve the performance of models substantially. The procedure was validated on three different data sets and outperformed similar approaches in performance and speed.",
    "code_link": ""
  },
  "iccv2021_avvision_itsallaroundyourange-guidedcylindricalnetworkfor3dobjectdetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "It's All Around You: Range-Guided Cylindrical Network for 3D Object Detection",
    "authors": [
      "Meytal Rapoport-Lavie",
      "Dan Raviv"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Rapoport-Lavie_Its_All_Around_You_Range-Guided_Cylindrical_Network_for_3D_Object_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Rapoport-Lavie_Its_All_Around_You_Range-Guided_Cylindrical_Network_for_3D_Object_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Modern perception systems in the field of autonomous driving rely on 3D data analysis. LiDAR sensors are frequently used to acquire such data due to their increased resilience to different lighting conditions. Although rotating LiDAR scanners produce ring-shaped patterns in space, most networks analyze their data using an orthogonal voxel sampling strategy. This work presents a novel approach for analyzing 3D data produced by 360-degree depth scanners, utilizing a more suitable coordinate system, which is aligned with the scanning pattern. Furthermore, we introduce a novel notion of range-guided convolutions, adapting the receptive field by distance from the ego vehicle and the object's scale. Our network demonstrates powerful results on the competitive nuScenes 3D object detection challenge, comparable to current state-of-the-art architectures.",
    "code_link": "https://github.com/open-mmlab/mmdetection3d"
  },
  "iccv2021_avvision_ss-sfdaself-supervisedsource-freedomainadaptationforroadsegmentationinhazardousenvironments": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "SS-SFDA: Self-Supervised Source-Free Domain Adaptation for Road Segmentation in Hazardous Environments",
    "authors": [
      "Divya Kothandaraman",
      "Rohan Chandra",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Kothandaraman_SS-SFDA_Self-Supervised_Source-Free_Domain_Adaptation_for_Road_Segmentation_in_Hazardous_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Kothandaraman_SS-SFDA_Self-Supervised_Source-Free_Domain_Adaptation_for_Road_Segmentation_in_Hazardous_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present a novel approach for unsupervised road segmentation in adverse weather conditions such as rain or fog. This includes a new algorithm for source-free domain adaptation (SFDA) using self-supervised learning. Moreover, our approach uses several techniques to address various challenges in SFDA and improve performance, including online generation of pseudo-labels and self-attention as well as use of curriculum learning, entropy minimization and model distillation. We have evaluated the performance on 6 datasets corresponding to real and synthetic adverse weather conditions. Our method outperforms all prior works on unsupervised road segmentation and SFDA by atleast 10.26%, and improves the training time by 18-180x. Moreover, our self-supervised algorithm exhibits similar accuracy performance in terms of mIOU score as compared to prior supervised methods.",
    "code_link": ""
  },
  "iccv2021_avvision_occupancygridmappingwithcognitiveplausibilityforautonomousdrivingapplications": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Occupancy Grid Mapping With Cognitive Plausibility for Autonomous Driving Applications",
    "authors": [
      "Alice Plebe",
      "Julian F. P. Kooij",
      "Gastone Pietro Rosati Papini",
      "Mauro Da Lio"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Plebe_Occupancy_Grid_Mapping_With_Cognitive_Plausibility_for_Autonomous_Driving_Applications_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Plebe_Occupancy_Grid_Mapping_With_Cognitive_Plausibility_for_Autonomous_Driving_Applications_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This work investigates the validity of an occupancy grid mapping inspired by human cognition and the way humans visually perceive the environment. This query is motivated by the fact that, to date, no autonomous driving system reaches the performance of an ordinary human driver. The mechanisms behind human perception could provide cues on how to improve common techniques employed in autonomous navigation---specifically the use of occupancy grids to represent the environment. We experiment with a neural network that maps an image of the scene onto an occupancy grid representation, and we show how the model benefits from two key (and yet simple) changes: 1) a different format of occupancy grid that resembles the way the brain projects the environment into a warped representation in the cortical visual area; 2) a mechanism similar to human visual attention that filters out non-relevant information from the scene. These effective expedients can potentially be applied to any autonomous driving task requiring an abstract representation of the scenario like the occupancy grids.",
    "code_link": ""
  },
  "iccv2021_avvision_weaklysupervisedapproachforjointobjectandlanemarkingdetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Weakly Supervised Approach for Joint Object and Lane Marking Detection",
    "authors": [
      "Pranjay Shyam",
      "Kuk-Jin Yoon",
      "Kyung-Soo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Shyam_Weakly_Supervised_Approach_for_Joint_Object_and_Lane_Marking_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Shyam_Weakly_Supervised_Approach_for_Joint_Object_and_Lane_Marking_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Understanding the driving scene is critical for the safe operation of autonomous vehicles with state-of-the-art (SoTA) systems relying upon a combination of different algorithms to perform tasks for mathematically representing an environment. Amongst these tasks, lane and object detection are highly popular and have been extensively researched independently. However, their joint operation is rarely studied primarily due to the lack of a dataset that captures these attributes together, resulting in increased redundant computations that can be eliminated simply by performing these tasks together. To overcome this, we propose a weakly-supervised approach wherein, given an image from the lane detection dataset, we use a pretrained network to label different objects within a scene, generating pseudo bounding boxes used to train a network that jointly detects objects and lane lines. With an emphasis on inference speed and performance, we utilize prior works to construct two architectures based on Convolutional Neural Networks (CNNs) and Transformers. The CNN-based approach uses row-based pixel classification to detect and cluster lane lines alongside a single-stage anchor free object detector while sharing the same encoder backbone. Alternatively, using dual decoders, the transformer-based approach directly estimates bounding boxes and polynomial coefficients of lane lines. Through extensive qualitative and quantities experiments, we demonstrate the efficacy of the proposed architectures on leading datasets for object and lane detections and report state-of-the-art (SoTA) performance per GFLOPs. Codes with trained model will be available at https://github.com/PS06/JOLD",
    "code_link": "https://github.com/PS06/JOLD"
  },
  "iccv2021_avvision_visualreasoningusinggraphconvolutionalnetworksforpredictingpedestriancrossingintention": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Visual Reasoning Using Graph Convolutional Networks for Predicting Pedestrian Crossing Intention",
    "authors": [
      "Tina Chen",
      "Renran Tian",
      "Zhengming Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Chen_Visual_Reasoning_Using_Graph_Convolutional_Networks_for_Predicting_Pedestrian_Crossing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Chen_Visual_Reasoning_Using_Graph_Convolutional_Networks_for_Predicting_Pedestrian_Crossing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Autonomous vehicles being able to anticipate rather than just react to pedestrian behavior is vital for the harmonious existence of the two on the road. Previous methods for predicting pedestrian crossing intention from the ego-view relied on bounding box location, and if any, limited visual features for their prediction. However, decisions made on the road by drivers and pedestrians are heavily dependent on context, which should be taken into account when trying to predict what pedestrians on the road intend to do. In this paper, we propose using rich visual features in graph convolutional autoencoders to encode the relationship between the pedestrian and its surrounding objects to reason their crossing intention. To further improve prediction results, we also incorporate pedestrian bounding boxes and human pose estimation in the prediction module. Our model differs in that we consider the effects other road objects/agents have on the pedestrian through visual reasoning of those objects/agents. We evaluate our model's performance using balanced accuracy and F1-score to show that we are able to outperform the state-of-the-art. Our model is able to predict crossing intention with 0.79 balanced accuracy, and is able to predict particularly better for cases where the pedestrian has no crossing intention. The code for our model is released at https://github.com/chen289/Visual-GCN.",
    "code_link": "https://github.com/chen289/Visual-GCN"
  },
  "iccv2021_avvision_causalbertimprovingobjectdetectionbysearchingforchallenginggroups": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Causal BERT: Improving Object Detection by Searching for Challenging Groups",
    "authors": [
      "Cinjon Resnick",
      "Or Litany",
      "Amlan Kar",
      "Karsten Kreis",
      "James Lucas",
      "Kyunghyun Cho",
      "Sanja Fidler"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Resnick_Causal_BERT_Improving_Object_Detection_by_Searching_for_Challenging_Groups_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Resnick_Causal_BERT_Improving_Object_Detection_by_Searching_for_Challenging_Groups_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Autonomous vehicles (AV) often rely on perception modules built upon neural networks for object detection. These modules frequently have low expected error overall but high error on unknown groups due to biases inherent in the training process. When these errors cause vehicle failure, manufacturers pay humans to comb through the associated images and label what group they are from. Data from that group is then collected, annotated, and added to the training set before retraining the model to fix the issue. In other words, group errors are found and addressed in hindsight. Our main contribution is a method to find such groups in foresight, leveraging advances in simulation as well as masked language modeling in order to perform causal interventions on simulated driving scenes. We then use the found groups to improve detection, exemplified by Diamondback bikes, whose performance we improve by 30 AP points. Such a solution is of high priority because it would greatly improve the robustness and safety of AV systems. Our second contribution is the tooling to run interventions, which will benefit the causal community tremendously.",
    "code_link": ""
  },
  "iccv2021_avvision_semantics-awaremulti-modaldomaintranslationfromlidarpointcloudstopanoramiccolorimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Semantics-Aware Multi-Modal Domain Translation: From LiDAR Point Clouds to Panoramic Color Images",
    "authors": [
      "Tiago Cortinhal",
      "Fatih Kurnaz",
      "Eren Erdal Aksoy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Cortinhal_Semantics-Aware_Multi-Modal_Domain_Translation_From_LiDAR_Point_Clouds_to_Panoramic_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Cortinhal_Semantics-Aware_Multi-Modal_Domain_Translation_From_LiDAR_Point_Clouds_to_Panoramic_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this work, we present a simple yet effective framework to address the domain translation problem between different sensor modalities with unique data formats. By relying only on the semantics of the scene, our modular generative framework can, for the first time, synthesize a panoramic color image from a given full 3D LiDAR point cloud. The framework starts with semantic segmentation of the point cloud, which is initially projected onto a spherical surface. The same semantic segmentation is applied to the corresponding camera image. Next, our new conditional generative model adversarially learns to translate the predicted LiDAR segment maps to the camera image counterparts. Finally, generated image segments are processed to render the panoramic scene images. We provide a thorough quantitative evaluation on the SemanticKitti dataset and show that our proposed framework outperforms other strong baseline models. Our source code is available at https://github.com/halmstad-University/TITAN-NET.",
    "code_link": "https://github.com/Halmstad-University/TITAN-NET"
  },
  "iccv2021_avvision_syntheticdatagenerationusingimitationtraining": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Synthetic Data Generation Using Imitation Training",
    "authors": [
      "Aman Kishore",
      "Tae Eun Choe",
      "Junghyun Kwon",
      "Minwoo Park",
      "Pengfei Hao",
      "Akshita Mittel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Kishore_Synthetic_Data_Generation_Using_Imitation_Training_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Kishore_Synthetic_Data_Generation_Using_Imitation_Training_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a strategic approach to generate synthetic data in order to improve machine learning algorithms such as Deep Neural Networks (DNN). Utilization of synthetic data has shown promising results yet there are no specific rules or recipes on how to generate and cook synthetic data. We propose imitation training as a guideline of synthetic data generation to add more underrepresented entities and balance the data distribution for DNN to handle corner cases and resolve long tail problems. The proposed imitation training has a circular process with three main steps: First, the existing system is evaluated and failure cases such as false positive and false negative detections are sorted out; Secondly, synthetic data imitating such failure cases is created with domain randomization; Thirdly, we train a network with the existing data and the newly added synthetic data; We repeat these three steps until the evaluation metric converges. We validated the approach by experimenting on object detection in autonomous driving.",
    "code_link": ""
  },
  "iccv2021_avvision_sdvtrackerreal-timemulti-sensorassociationandtrackingforself-drivingvehicles": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "SDVTracker: Real-Time Multi-Sensor Association and Tracking for Self-Driving Vehicles",
    "authors": [
      "Shivam Gautam",
      "Gregory P. Meyer",
      "Carlos Vallespi-Gonzalez",
      "Brian C. Becker"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Gautam_SDVTracker_Real-Time_Multi-Sensor_Association_and_Tracking_for_Self-Driving_Vehicles_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Gautam_SDVTracker_Real-Time_Multi-Sensor_Association_and_Tracking_for_Self-Driving_Vehicles_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Accurate motion state estimation of Vulnerable Road Users (VRUs), is a critical requirement for autonomous vehicles that navigate in urban environments. Due to their computational efficiency, many traditional autonomy systems perform multi-object tracking using Kalman Filters which frequently rely on hand-engineered association. However, such methods fail to generalize to crowded scenes and multi-sensor modalities, often resulting in poor state estimates which cascade to inaccurate predictions. We present a practical and lightweight tracking system, SDVTracker, that uses a deep learned model for association and state estimation in conjunction with an Interacting Multiple Model (IMM) filter. The proposed tracking method is fast, robust and generalizes across multiple sensor modalities and different VRU classes. In this paper, we detail a model that jointly optimizes both association and state estimation with a novel loss, an algorithm for determining ground-truth supervision, and a training procedure. We show this system significantly outperforms hand-engineered methods on a real-world urban driving dataset while running in less than 2.5 ms on CPU for a scene with 100 actors, making it suitable for self-driving applications where low latency and high accuracy is critical.",
    "code_link": ""
  },
  "iccv2021_avvision_speak2labelusingdomainknowledgeforcreatingalargescaledrivergazezoneestimationdataset": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver Gaze Zone Estimation Dataset",
    "authors": [
      "Shreya Ghosh",
      "Abhinav Dhall",
      "Garima Sharma",
      "Sarthak Gupta",
      "Nicu Sebe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Ghosh_Speak2Label_Using_Domain_Knowledge_for_Creating_a_Large_Scale_Driver_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Ghosh_Speak2Label_Using_Domain_Knowledge_for_Creating_a_Large_Scale_Driver_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Labelling of human behavior analysis data is a complex and time consuming task. In this paper, a fully automatic technique for labelling an image based gaze behavior dataset for driver gaze zone estimation is proposed. Domain knowledge is added to the data recording paradigm and later labels are generated in an automatic manner using Speech To Text conversion (STT). In order to remove the noise in the STT process due to different illumination and ethnicity of subjects in our data, the speech frequency and energy are analysed. The resultant Driver Gaze in the Wild (DGW) dataset contains 586 recordings, captured during different times of the day including evenings. The large scale dataset contains 338 subjects with an age range of 18-63 years. As the data is recorded in different lighting conditions, an illumination robust layer is proposed in the Convolutional Neural Network (CNN). The extensive experiments show the variance in the dataset resembling real-world conditions and the effectiveness of the proposed CNN pipeline. The proposed network is also fine-tuned for the eye gaze prediction task, which shows the discriminativeness of the representation learnt by our network on the proposed DGW dataset. Project Page: https://sites.google.com/view/drivergazeprediction/home",
    "code_link": ""
  },
  "iccv2021_avvision_graphconvolutionalnetworksfor3dobjectdetectiononradardata": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Graph Convolutional Networks for 3D Object Detection on Radar Data",
    "authors": [
      "Michael Meyer",
      "Georg Kuschk",
      "Sven Tomforde"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Meyer_Graph_Convolutional_Networks_for_3D_Object_Detection_on_Radar_Data_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Meyer_Graph_Convolutional_Networks_for_3D_Object_Detection_on_Radar_Data_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Despite its advantages as an inexpensive, weather-robust and long-range sensor which additionally provides velocity information, radar sensors still lead a shadowy existence compared to lidar and camera when it comes to fulfilling the requirements of fully autonomous driving. In this work, we focus on fully leveraging raw radar tensor data instead of building up on human-biased point clouds which are the typical result of traditional radar signal processing techniques. Utilizing a graph neural network on the raw radar tensor we gain a significant improvement of +10% in average precision over a grid-based convolutional baseline network. The performance of both networks is evaluated on a real world dataset with dense city traffic scenarios, diverse object orientations and distances as well as occlusions up to visually fully occluded objects. Our proposed network increases the maximum range for state-of-the-art full-3D object detection on radar data from previously 20m to 100m.",
    "code_link": ""
  },
  "iccv2021_avvision_frustum-pointpillarsamulti-stageapproachfor3dobjectdetectionusingrgbcameraandlidar": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Frustum-PointPillars: A Multi-Stage Approach for 3D Object Detection Using RGB Camera and LiDAR",
    "authors": [
      "Anshul Paigwar",
      "David Sierra-Gonzalez",
      "\u00d6zg\u00fcr Erkent",
      "Christian Laugier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Paigwar_Frustum-PointPillars_A_Multi-Stage_Approach_for_3D_Object_Detection_Using_RGB_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Paigwar_Frustum-PointPillars_A_Multi-Stage_Approach_for_3D_Object_Detection_Using_RGB_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Accurate 3D object detection is a key part of the perception module for autonomous vehicles. A better understanding of the objects in 3D facilitates better decision-making and path planning. RGB Cameras and LiDAR are the most commonly used sensors in autonomous vehicles for environment perception. Many approaches have shown promising results for 2D detection with RGB Images, but efficiently localizing small objects like pedestrians in the 3D point cloud of large scenes has remained a challenging area of research. We propose a novel method, Frustum-PointPillars, for 3D object detection using LiDAR data. Instead of solely relying on point cloud features, we leverage the mature field of 2D object detection to reduce the search space in the 3D space. Then, we use the Pillar Feature Encoding network for object localization in the reduced point cloud. We also propose a novel approach for masking point clouds to further improve the localization of objects. We train our network on the KITTI dataset and perform experiments to show the effectiveness of our network. On the KITTI test set our method outperforms other multi-sensor SOTA approaches for 3D pedestrian localization (Bird's Eye View) while achieving a significantly faster runtime of 14 Hz.",
    "code_link": ""
  },
  "iccv2021_avvision_scarfasemanticconstrainedattentionrefinementnetworkforsemanticsegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "SCARF: A Semantic Constrained Attention Refinement Network for Semantic Segmentation",
    "authors": [
      "Xiaofeng Ding",
      "Chaomin Shen",
      "Zhengping Che",
      "Tieyong Zeng",
      "Yaxin Peng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Ding_SCARF_A_Semantic_Constrained_Attention_Refinement_Network_for_Semantic_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Ding_SCARF_A_Semantic_Constrained_Attention_Refinement_Network_for_Semantic_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Semantic segmentation has achieved great progress by exploiting the contextual dependencies. In this paper, we propose an end-to-end Semantic Constrained Attention ReFinement (SCARF) network, based on semantic constrained contextual dependencies, to fully utilize the semantic information across different layers. Our novelties lie in the following aspects: Firstly, we present a general framework for capturing the non-local contextual dependencies. Secondly, within the framework, we introduce an efficient Category Attention (CA) block to capture semantic-related context by using the category constraint from coarse segmentation, which reduces the computational complexity from O(n^2) to O(n) for image with n pixels. Thirdly, we overcome the contextual information confusion problem by balancing the non-local contextual dependencies and the local consistency adaptively using a category-wise learning weight. Finally, we fully utilize the multi-scale semantic-related contextual information by refining the segmentation iteratively across layers with semantic constraint. Extensive evaluations demonstrate that our SCARF network significantly improves the segmentation results and achieves superior performance 85.0% mIoU on PASCAL VOC 2012, 55.0% mIoU on PASCAL Context, and 82.1% mIoU on Cityscapes.",
    "code_link": ""
  },
  "iccv2021_avvision_multi-weathercityadverseweatherstackingforautonomousdriving": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Multi-Weather City: Adverse Weather Stacking for Autonomous Driving",
    "authors": [
      "Valentina Mu\u0219at",
      "Ivan Fursa",
      "Paul Newman",
      "Fabio Cuzzolin",
      "Andrew Bradley"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Musat_Multi-Weather_City_Adverse_Weather_Stacking_for_Autonomous_Driving_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Musat_Multi-Weather_City_Adverse_Weather_Stacking_for_Autonomous_Driving_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Autonomous vehicles make use of sensors to perceive the world around them, with heavy reliance on vision-based sensors such as RGB cameras. Unfortunately, since these sensors are affected by adverse weather, perception pipelines require extensive training on visual data under harsh conditions in order to improve the robustness of downstream tasks - data that is difficult and expensive to acquire. Based on GAN and CycleGAN architectures, we propose an overall (modular) architecture for constructing datasets, which allows one to add, swap out and combine components in order to generate images with diverse weather conditions. Starting from a single dataset with ground-truth, we generate 7 versions of the same data in diverse weather, and propose an extension to augment the generated conditions, thus resulting in a total of 14 adverse weather conditions, requiring a single ground truth. We test the quality of the generated conditions both in terms of perceptual quality and suitability for training downstream tasks, using real world, out-of-distribution adverse weather extracted from various datasets. We show improvements in both object detection and instance segmentation across all conditions, in many cases exceeding 10 percentage points increase in AP, and provide the materials and instructions needed to re-construct the multi-weather dataset, based upon the original Cityscapes dataset.",
    "code_link": "https://github.com/vnmusat/multi-weather-city"
  },
  "iccv2021_avvision_efficientuncertaintyestimationinsemanticsegmentationviadistillation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Efficient Uncertainty Estimation in Semantic Segmentation via Distillation",
    "authors": [
      "Christopher J. Holder",
      "Muhammad Shafique"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Holder_Efficient_Uncertainty_Estimation_in_Semantic_Segmentation_via_Distillation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Holder_Efficient_Uncertainty_Estimation_in_Semantic_Segmentation_via_Distillation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep neural networks typically make predictions with little regard for the probability that a prediction might be incorrect. Attempts to address this often involve input data undergoing multiple forward passes, either of multiple models or of multiple configurations of a single model, and consensus among outputs is used as a measure of confidence. This can be computationally expensive, as the time taken to process a single input sample increases linearly with the number of output samples being generated, an important consideration in real-time scenarios such as autonomous driving, and so we propose Uncertainty Distillation as a more efficient method for quantifying prediction uncertainty. Inspired by the concept of Knowledge Distillation, whereby the performance of a compact model is improved by training it to mimic the outputs of a larger model, we train a compact model to mimic the output distribution of a large ensemble of models, such that for each output there is a prediction and a predicted level of uncertainty for that prediction. We apply Uncertainty Distillation in the context of a semantic segmentation task for autonomous vehicle scene understanding and demonstrate a capability to reliably predict pixelwise uncertainty over the resultant class probability map. We also show that the aggregate pixel uncertainty across an image can be used as a metric for reliable detection of out-of-distribution data.",
    "code_link": ""
  },
  "iccv2021_avvision_sa-det3dself-attentionbasedcontext-aware3dobjectdetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection",
    "authors": [
      "Prarthana Bhattacharyya",
      "Chengjie Huang",
      "Krzysztof Czarnecki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Bhattacharyya_SA-Det3D_Self-Attention_Based_Context-Aware_3D_Object_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Bhattacharyya_SA-Det3D_Self-Attention_Based_Context-Aware_3D_Object_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Existing point-cloud based 3D object detectors use convolution-like operators to process information in a local neighbourhood with fixed-weight kernels and aggregate global context hierarchically. However, non-local neural networks and self-attention for 2D vision have shown that explicitly modeling long-range interactions can lead to more robust and competitive models. In this paper, we propose two variants of self-attention for contextual modeling in 3D object detection by augmenting convolutional features with self-attention features. We first incorporate the pairwise self-attention mechanism into the current state-of-the-art BEV, voxel and point-based detectors and show consistent improvement over strong baseline models of up to 1.5 3D AP while simultaneously reducing their parameter footprint and computational cost by 15-80% and 30-50%, respectively, on the KITTI validation set. We next propose a self-attention variant that samples a subset of the most representative features by learning deformations over randomly sampled locations. This not only allows us to scale explicit global contextual modeling to larger point-clouds, but also leads to more discriminative and informative feature descriptors. Our method can be flexibly applied to most state-of-the-art detectors with increased accuracy and parameter and compute efficiency. We show our proposed method improves 3D object detection performance on KITTI, nuScenes and Waymo Open datasets.",
    "code_link": "https://github.com/open-mmlab/OpenPCDet"
  },
  "iccv2021_avvision_autonomousvehiclevision2021iccvworkshopsummary": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Autonomous Vehicle Vision 2021: ICCV Workshop Summary",
    "authors": [
      "Rui Fan",
      "Nemanja Djuric",
      "Fisher Yu",
      "Rowan McAllister",
      "Ioannis Pitas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Fan_Autonomous_Vehicle_Vision_2021_ICCV_Workshop_Summary_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Fan_Autonomous_Vehicle_Vision_2021_ICCV_Workshop_Summary_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This paper summarizes the 2nd Autonomous Vehicle Vision (AVVision) workshop (avvision.xyz/iccv21), organized virtually in conjunction with ICCV 2021. The organizers invited seven experts from both industry and academia to deliver keynote talks, discussing the state-of-the-art and challenges in the field of autonomous driving. A total of 27 papers were accepted for publication in the ICCV 2021 proceedings (IEEE Xplore and CVF open access), resulting in an acceptance rate of 50.9%. In addition to serving as a workshop summary and a brief overview of the existing challenges, this paper also presents how these challenges were addressed by the authors through their proposed solutions.",
    "code_link": ""
  },
  "iccv2021_avvision_acomputervision-basedattentiongeneratorusingdqn": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "A Computer Vision-Based Attention Generator Using DQN",
    "authors": [
      "Jordan Chipka",
      "Shuqing Zeng",
      "Thanura Elvitigala",
      "Priyantha Mudalige"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Chipka_A_Computer_Vision-Based_Attention_Generator_Using_DQN_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Chipka_A_Computer_Vision-Based_Attention_Generator_Using_DQN_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A significant obstacle to achieving autonomous driving (AD) and advanced driver-assistance systems (ADAS) functionality in passenger vehicles is high-fidelity perception at a sufficiently low cost of computation and sensors. An area of research that aims to address this challenge takes inspiration from human foveal vision by using attention-based sensing. This work presents an end-to-end computer vision-based reinforcement learning (RL) technique that intelligently selects a priority region of an image to place greater attention to achieve better perception performance. This method is evaluated on the Berkeley Deep Drive (BDD) dataset. Results demonstrate that a substantial improvement in perception performance can be attained - compared to a baseline method - at a minimal cost in terms of time and processing.",
    "code_link": ""
  },
  "iccv2021_avvision_raidararichannotatedimagedatasetofrainystreetscenes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes",
    "authors": [
      "Jiongchao Jin",
      "Arezou Fatemi",
      "Wallace Michel Pinto Lira",
      "Fenggen Yu",
      "Biao Leng",
      "Rui Ma",
      "Ali Mahdavi-Amiri",
      "Hao Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Jin_RaidaR_A_Rich_Annotated_Image_Dataset_of_Rainy_Street_Scenes_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Jin_RaidaR_A_Rich_Annotated_Image_Dataset_of_Rainy_Street_Scenes_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We introduce RaidaR, a rich annotated image dataset of rainy street scenes, to support autonomous driving research. The new dataset contains the largest number of rainy images (58,542) to date, 5,000 of which provide semantic segmentations and 3,658 provide object instance segmentations. The RaidaR images cover a wide range of realistic rain-induced artifacts, including fog, droplets, and road reflections, which can effectively augment existing street scene datasets to improve data-driven machine perception during rainy weather. To facilitate efficient annotation of a large volume of images, we develop a semi-automatic scheme combining manual segmentation and an automated processing akin to cross validation, resulting in 10-20 fold reduction on annotation time. We demonstrate the utility of our new dataset by showing how data augmentation with RaidaR can elevate the accuracy of existing segmentation algorithms. We also present a novel unpaired image-to-image translation algorithm for adding/removing rain artifacts, which directly benefits from RaidaR.",
    "code_link": ""
  },
  "iccv2021_avvision_few-shotbatchincrementalroadobjectdetectionviadetectorfusion": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Few-Shot Batch Incremental Road Object Detection via Detector Fusion",
    "authors": [
      "Anuj Tambwekar",
      "Kshitij Agrawal",
      "Anay Majee",
      "Anbumani Subramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Tambwekar_Few-Shot_Batch_Incremental_Road_Object_Detection_via_Detector_Fusion_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Tambwekar_Few-Shot_Batch_Incremental_Road_Object_Detection_via_Detector_Fusion_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Incremental few-shot learning has emerged as a new and challenging area in deep learning, whose objective is to train deep learning models using very few samples of new class data, and none of the old class data. In this work we tackle the problem of batch incremental few-shot road object detection using data from the India Driving Dataset (IDD). Our approach, DualFusion, combines object detectors in a manner that allows us to learn to detect rare objects with very limited data, all without severely degrading the performance of the detector on the abundant classes. In the IDD OpenSet incremental few-shot detection task, we achieve a mAP50 score of 40.0 on the base classes and an overall mAP50 score of 38.8, both of which are the highest to date. In the COCO batch incremental few-shot detection task, we achieve a novel AP score of 9.9, surpassing the state-of-the-art novel class performance on the same by over 6.6 times.",
    "code_link": ""
  },
  "iccv2021_avvision_cross-modalmatchingcnnforautonomousdrivingsensordatamonitoring": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Cross-Modal Matching CNN for Autonomous Driving Sensor Data Monitoring",
    "authors": [
      "Yiqiang Chen",
      "Feng Liu",
      "Ke Pei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Chen_Cross-Modal_Matching_CNN_for_Autonomous_Driving_Sensor_Data_Monitoring_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Chen_Cross-Modal_Matching_CNN_for_Autonomous_Driving_Sensor_Data_Monitoring_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Multiple sensor types have been increasingly used in modern autonomous driving systems (ADS) to ensure safer perception. Through applications of multiple modalities of perception sensors that differ in their physical properties, obtained data complement to each other and provide a more robust view of surroundings. On the other hand, however, sensor data fault is inevitable thus lead to wrong perception results and consequently endangers the overall safety of the vehicle. In this paper, we present a cross-modal Convolutional Neural Networks (CNN) for autonomous driving sensor data monitoring functions, such as fault detection and online data quality assessment. Assuming the overlapping view of different sensors should be consistent under normal circumstances, we detect anomalies such as mis-synchronisation through matching camera image and LIDAR point cloud. A masked pixel-wise metric learning loss is proposed to improve exploration of the local structures and to build an alignment-sensitive pixel embedding. In our experiments with a selected KITTI dataset and specially tailored fault data generation methods, the approach shows a promising success for sensor fault detection and point cloud quality assessment (PCQA) results.",
    "code_link": ""
  },
  "iccv2021_avvision_ontheroadtolarge-scale3dmonocularscenereconstructionusingdeepimplicitfunctions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "On the Road to Large-Scale 3D Monocular Scene Reconstruction Using Deep Implicit Functions",
    "authors": [
      "Thomas Roddick",
      "Benjamin Biggs",
      "Daniel Olmeda Reino",
      "Roberto Cipolla"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Roddick_On_the_Road_to_Large-Scale_3D_Monocular_Scene_Reconstruction_Using_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Roddick_On_the_Road_to_Large-Scale_3D_Monocular_Scene_Reconstruction_Using_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Autonomous driving relies on building detailed models of a vehicles surroundings, including all hazards, obstacles and other road users. At present, much of the autonomous driving literature reduces the world to a collection of parametric 3D boxes. While this framework is sufficient form any driving scenarios, other important scene details (e.g. overhanging structures, open car doors, debris, potholes etc.) are not modelled. Recently deep implicit functions have been shown to be suitable for representing fine grained details at arbitrarily high resolutions using images alone. However, they have predominantly been employed in constrained situations, such as reconstructing individual objects or small-scale scenes. In this work we explore the application of deep implicit functions to larger scenes in the context of real-world autonomous driving scenarios. In particular we focus on the challenging case where only monocular images are available at test time. While most implicit function networks rely on watertight meshes for training, these are not in general available for real world scenes. We therefore propose an alternative training scheme using LiDAR to provide approximate ground truth occupancy supervision. We also show that incorporating priors such as pre-detected object bounding boxes can improve the quality of reconstruction. Our method is evaluated on a real-world autonomous driving dataset.",
    "code_link": ""
  },
  "iccv2021_avvision_centerpolyreal-timeinstancesegmentationusingboundingpolygons": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "CenterPoly: Real-Time Instance Segmentation Using Bounding Polygons",
    "authors": [
      "Hughes Perreault",
      "Guillaume-Alexandre Bilodeau",
      "Nicolas Saunier",
      "Maguelonne H\u00e9ritier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Perreault_CenterPoly_Real-Time_Instance_Segmentation_Using_Bounding_Polygons_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Perreault_CenterPoly_Real-Time_Instance_Segmentation_Using_Bounding_Polygons_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present a novel method, called CenterPoly, for real-time instance segmentation using bounding polygons. We apply it to detect road users in dense urban environments, making it suitable for applications in intelligent transportation systems like automated vehicles. CenterPoly detects objects by their center keypoint while predicting a fixed number of polygon vertices for each object, thus performing detection and segmentation in parallel. Most of the network parameters are shared by the network heads, making it fast and lightweight enough to run at real-time speed. To properly convert mask ground-truth to polygon ground-truth, we designed a vertex selection strategy to facilitate the learning of the polygons. Additionally, to better segment overlapping objects in dense urban scenes, we also train a relative depth branch to determine which instances are closer and which are further, using available weak annotations. We propose several models with different backbones to show the possible speed / accuracy trade-offs. The models were trained and evaluated on Cityscapes, KITTI and IDD and the results are reported on their public benchmark, which are state-of-the-art at real-time speeds. Code is available at https://github.com/hu64/CenterPoly.",
    "code_link": "https://github.com/hu64/CenterPoly"
  },
  "iccv2021_avvision_multi-stagefusionformulti-class3dlidardetection": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Multi-Stage Fusion for Multi-Class 3D Lidar Detection",
    "authors": [
      "Zejie Wang",
      "Zhen Zhao",
      "Zhao Jin",
      "Zhengping Che",
      "Jian Tang",
      "Chaomin Shen",
      "Yaxin Peng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Wang_Multi-Stage_Fusion_for_Multi-Class_3D_Lidar_Detection_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Wang_Multi-Stage_Fusion_for_Multi-Class_3D_Lidar_Detection_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In autonomous driving, the robust and accurate perceptions of the environment is a fundamental and challenging task. Resorting to the advancing of different sensors such as LiDAR and Camera, the autonomous systems are able to capture and process complementary perceptual information for better detection and classifying objects. In this paper, we propose a LiDAR-Camera fusion method for multi-class 3D object detection. The proposed method makes the utmost use of data from the two sensors by multiple fusion stages, and can be learned in an end-to-end manner. First, we apply a multi-level gated adaptive fusion mechanism with the feature extraction backbone. This point-wise fusion stage assiduously exploits the image and point cloud inputs, and obtains joint semantic representations of the scene. Next, given the regions of interest (RoIs) proposed based on the LiDAR features, the corresponding Camera features are selected by RoI-based feature pooling. These features are used to enrich the LiDAR features in local regions and enhance the proposal refinement. Moreover, we introduce a multi-label classification task as an auxiliary regularization to the object detection network. Without relying on extra labels, it helps the model better mine the extracted features and discover hard object instances. The experiments conducted on the KITTI dataset have proved all our fusion strategies are effective.",
    "code_link": ""
  },
  "iccv2021_avvision_cdadaacurriculumdomainadaptationfornighttimesemanticsegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "CDAda: A Curriculum Domain Adaptation for Nighttime Semantic Segmentation",
    "authors": [
      "Qi Xu",
      "Yinan Ma",
      "Jing Wu",
      "Chengnian Long",
      "Xiaolin Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Xu_CDAda_A_Curriculum_Domain_Adaptation_for_Nighttime_Semantic_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Xu_CDAda_A_Curriculum_Domain_Adaptation_for_Nighttime_Semantic_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Autonomous driving needs to ensure all-weather safety, especially in unfavorable environments such as night and rain. However, the current daytime-trained semantic segmentation networks face significant performance degradation at night because of the huge domain divergence. In this paper, we propose a novel Curriculum Domain Adaptation method (CDAda) to realize the smooth semantic knowledge transfer from daytime to nighttime. Specifically, it consists of two steps: 1) inter-domain style adaptation: fine-tune the daytime-trained model on the labeled synthetic nighttime images through the proposed frequency-based style transformation method (replace the low-frequency components of daytime images with those of nighttime images); 2) intra-domain gradual self-training: separate the nighttime domain into the easy split nighttime domain and hard split nighttime domain based on the \"entropy + illumination\" ranking principle, then gradually adapt the model to the two sub-domains through pseudo supervision on easy split data and entropy minimization on hard split data. To the best of our knowledge, we first extend the idea of intra-domain adaptation to self-training and prove different treatments on two parts can reduce the distribution divergence in the nighttime domain itself. In particular, aimed at the adopted unlabeled day-night image pairs, the prediction of the daytime images can guide the segmentation on the nighttime images by ensuring patch-level consistency. Extensive experiments on Nighttime Driving, Dark Zurich, and BDD100K-night dataset highlight the effectiveness of our approach with the more favorable performance 50.9%, 45.0%, and 33.8% Mean IoU against existing state-of-the-art approaches.",
    "code_link": ""
  },
  "iccv2021_avvision_dripeadatasetforhumanposeestimationinreal-worlddrivingsettings": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "DriPE: A Dataset for Human Pose Estimation in Real-World Driving Settings",
    "authors": [
      "Romain Guesdon",
      "Carlos Crispim-Junior",
      "Laure Tougne"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Guesdon_DriPE_A_Dataset_for_Human_Pose_Estimation_in_Real-World_Driving_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Guesdon_DriPE_A_Dataset_for_Human_Pose_Estimation_in_Real-World_Driving_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The task of 2D human pose estimation has known a significant gain of performance with the advent of deep learning. This task aims to estimate the body keypoints of people in an image or a video. However, real-life applications of such methods bring new challenges that are under-represented in the general context datasets. For instance, driver status monitoring on consumer road vehicles introduces new difficulties, like self- and background body-part occlusions, varying illumination conditions, cramped view angles, etc. These monitoring conditions are currently absent in general purposes datasets. This paper proposes two main contributions. Firstly, we introduce DriPE (Driver Pose Estimation), a new dataset to foster the development and evaluation of methods for human pose estimation of drivers in consumer vehicles. This is the first publicly available dataset depicting drivers in real scenes. It contains 10k images of 19 different driver subjects, manually annotated with human body keypoints and an object bounding box. Secondly, we propose a new keypoint-based metric for human pose estimation. This metric highlights the limitations of current metrics for HPE evaluation and of current deep neural networks on pose estimation, both on general and driving-related datasets.",
    "code_link": ""
  },
  "iccv2021_avvision_monocular3dlocalizationofvehiclesinroadscenes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "Monocular 3D Localization of Vehicles in Road Scenes",
    "authors": [
      "Haotian Zhang",
      "Haorui Ji",
      "Aotian Zheng",
      "Jenq-Neng Hwang",
      "Ren-Hung Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Zhang_Monocular_3D_Localization_of_Vehicles_in_Road_Scenes_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Zhang_Monocular_3D_Localization_of_Vehicles_in_Road_Scenes_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Sensing and perception systems for autonomous driving vehicles in road scenes are composed of three crucial components: 3D-based object detection, tracking, and localization. While all three components are important, most relevant papers tend to only focus on one single component. We propose a monocular vision-based framework for 3D-based detection, tracking, and localization by effectively integrating all three tasks in a complementary manner. Our system contains an RCNN-based Localization Network (LOCNet), which works in concert with fitness evaluation score (FES) based single-frame optimization, to get more accurate and refined 3D vehicle localization. To better utilize the temporal information, we further use a multi-frame optimization technique, taking advantage of camera ego-motion and a 3D TrackletNet Tracker (3D TNT), to improve both accuracy and consistency in our 3D localization results. Our system outperforms state-of-the-art image-based solutions in diverse scenarios and is even comparable with LiDAR-based methods.",
    "code_link": ""
  },
  "iccv2021_avvision_yolinogenericsingleshotpolylinedetectioninrealtime": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AVVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Autonomous Vehicle Vision",
    "title": "YOLinO: Generic Single Shot Polyline Detection in Real Time",
    "authors": [
      "Annika Meyer",
      "Philipp Skudlik",
      "Jan-Hendrik Pauls",
      "Christoph Stiller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Meyer_YOLinO_Generic_Single_Shot_Polyline_Detection_in_Real_Time_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Meyer_YOLinO_Generic_Single_Shot_Polyline_Detection_in_Real_Time_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The detection of polylines is usually either bound to branchless polylines or formulated in a recurrent way, prohibiting their use in real-time systems. We propose an approach that builds upon the idea of single shot object detection. Reformulating the problem of polyline detection as a bottom-up composition of small line segments allows to detect bounded, dashed and continuous polylines with a single head. This has several major advantages over previous methods. Not only is the method at 187 fps more than suited for real-time applications with virtually any restriction on the shapes of the detected polylines. By predicting multiple line segments for each cell, even branching or crossing polylines can be detected. We evaluate our approach on three different applications for road marking, lane border and center line detection. Hereby, we demonstrate the ability to generalize to different domains as well as both implicit and explicit polyline detection tasks.",
    "code_link": ""
  },
  "iccv2021_clvl_visualquestionansweringwithtextualrepresentationsforimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Closing the Loop Between Vision and Language",
    "title": "Visual Question Answering With Textual Representations for Images",
    "authors": [
      "Yusuke Hirota",
      "Noa Garcia",
      "Mayu Otani",
      "Chenhui Chu",
      "Yuta Nakashima",
      "Ittetsu Taniguchi",
      "Takao Onoye"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/html/Hirota_Visual_Question_Answering_With_Textual_Representations_for_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Hirota_Visual_Question_Answering_With_Textual_Representations_for_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "How far can we go with textual representations for understanding pictures? Deep visual features extracted by object recognition models are prevailing used in multiple tasks, and especially in visual question answering (VQA). However, conventional deep visual features may struggle to convey all the details in an image as we humans do. Meanwhile, with recent language models' progress, descriptive text may be an alternative to this problem. This paper delves into the effectiveness of textual representations for image understanding in the specific context of VQA.",
    "code_link": ""
  },
  "iccv2021_clvl_semi-autoregressivetransformerforimagecaptioning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Closing the Loop Between Vision and Language",
    "title": "Semi-Autoregressive Transformer for Image Captioning",
    "authors": [
      "Yuanen Zhou",
      "Yong Zhang",
      "Zhenzhen Hu",
      "Meng Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/html/Zhou_Semi-Autoregressive_Transformer_for_Image_Captioning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Zhou_Semi-Autoregressive_Transformer_for_Image_Captioning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Current state-of-the-art image captioning models adopt autoregressive decoders, i.e. they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. To tackle this issue, non-autoregressive image captioning models have recently been proposed to significantly accelerate the speed of inference by generating all words in parallel. However, these non-autoregressive models inevitably suffer from large generation quality degradation since they remove words dependence excessively. To make a better trade-off between speed and quality, we introduce a semi-autoregressive model for image captioning (dubbed as SATIC), which keeps the autoregressive property in global but generates words parallelly in local . Based on Transformer, there are only a few modifications needed to implement SATIC. Experimental results on the MSCOCO image captioning benchmark show that SATIC can achieve a good trade-off without bells and whistles. Code is available at\\color magenta https://github.com/YuanEZhou/satic .",
    "code_link": ""
  },
  "iccv2021_clvl_whatyousayisnotwhatyoudostudyingvisio-linguisticmodelsfortvseriessummarization": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Closing the Loop Between Vision and Language",
    "title": "What You Say Is Not What You Do: Studying Visio-Linguistic Models for TV Series Summarization",
    "authors": [
      "Alison Reboud",
      "Rapha\u00ebl Troncy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/html/Reboud_What_You_Say_Is_Not_What_You_Do_Studying_Visio-Linguistic_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Reboud_What_You_Say_Is_Not_What_You_Do_Studying_Visio-Linguistic_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we generate TV series summaries using both visual cues present in video frames and screenplay (dialogue and scenic textual descriptions). Recently, approaches relying on pre-trained vision and language representations have proven to be successful for several downstream tasks using paired text and images. For TV series summarization, we hypothesize that both scenic information and dialogues are useful to generate summaries. Visio-linguistic models being presented as task-agnostic, we explore if and how they can be used for TV series summarization by conducting experiments with varying text inputs and models fine-tuned on different datasets. We observe that such generic models, despite not being specifically designed for narrative understanding, achieve results closed to the state of the art. Our results suggest also that non aligned data also benefit from this type of visio-linguistics architecture.",
    "code_link": ""
  },
  "iccv2021_clvl_egocentricbiochemicalvideo-and-languagedataset": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Closing the Loop Between Vision and Language",
    "title": "Egocentric Biochemical Video-and-Language Dataset",
    "authors": [
      "Taichi Nishimura",
      "Kojiro Sakoda",
      "Atsushi Hashimoto",
      "Yoshitaka Ushiku",
      "Natsuko Tanaka",
      "Fumihito Ono",
      "Hirotaka Kameko",
      "Shinsuke Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/html/Nishimura_Egocentric_Biochemical_Video-and-Language_Dataset_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Nishimura_Egocentric_Biochemical_Video-and-Language_Dataset_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This paper proposes a novel biochemical video-and-language (BioVL) dataset, which consists of experimental videos, corresponding protocols, and annotations of alignment between events in the video and instructions in the protocol. The key strength of the dataset is its user-oriented design of data collection. We imagine that biochemical researchers easily take videos and share them for another researcher's replication in the future. To minimize the burden of video recording, we adopted an unedited first-person video as a visual source. As a result, we collected 16 videos from four protocols with a total length of 1.6 hours. In our experiments, we conduct two zero-shot video-and-language tasks on the BioVL dataset. Our experimental results show a large room for improvement for practical use even utilizing the state-of-the-art pre-trained video-and-language joint embedding model. We are going to release the BioVL dataset. To our knowledge, this work is the first attempt to release the biochemical video-and-language dataset.",
    "code_link": ""
  },
  "iccv2021_clvl_cigliconditionalimagegenerationfromlanguage&image": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Closing the Loop Between Vision and Language",
    "title": "CIGLI: Conditional Image Generation From Language & Image",
    "authors": [
      "Xiaopeng Lu",
      "Lynnette Ng",
      "Jared Fernandez",
      "Hao Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/html/Lu_CIGLI_Conditional_Image_Generation_From_Language__Image_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Lu_CIGLI_Conditional_Image_Generation_From_Language__Image_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Multi-modal generation has been widely explored in recent years. Current research directions involve generating text based on an image or vice versa. In this paper, we propose a new task called CIGLI: Conditional Image Generation from Language and Image. Instead of generating an image based on text as in text-image generation, this task requires the generation of an image from a textual description and an image prompt. We designed a new dataset to ensure that the text description describes information from both images, and that solely analyzing the description is insufficient to generate an image. We then propose a novel language-image fusion model which improves the performance over two established baseline methods, as evaluated by quantitative (automatic) and qualitative (human) evaluations. The code and dataset is available at https://github.com/vincentlux/CIGLI.",
    "code_link": "https://github.com/vincentlux/CIGLI"
  },
  "iccv2021_clvl_latentvariablemodelsforvisualquestionanswering": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Closing the Loop Between Vision and Language",
    "title": "Latent Variable Models for Visual Question Answering",
    "authors": [
      "Zixu Wang",
      "Yishu Miao",
      "Lucia Specia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/html/Wang_Latent_Variable_Models_for_Visual_Question_Answering_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Wang_Latent_Variable_Models_for_Visual_Question_Answering_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Current work on Visual Question Answering (VQA) explore deterministic approaches conditioned on various types of image and question features. We posit that, in addition to image and question pairs, other modalities are useful for teaching machine to carry out question answering. Hence in this paper, we propose latent variable models for VQA where extra information (e.g. captions and answer categories) are incorporated as latent variables, which are observed during training but in turn benefit question-answering performance at test time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the effectiveness of our proposed models: they improve over strong baselines, especially those that do not rely on extensive language-vision pre-training.",
    "code_link": ""
  },
  "iccv2021_clvl_language-guidedmulti-modalfusionforvideoactionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Closing the Loop Between Vision and Language",
    "title": "Language-Guided Multi-Modal Fusion for Video Action Recognition",
    "authors": [
      "Jenhao Hsiao",
      "Yikang Li",
      "Chiuman Ho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/html/Hsiao_Language-Guided_Multi-Modal_Fusion_for_Video_Action_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Hsiao_Language-Guided_Multi-Modal_Fusion_for_Video_Action_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A recent study has found that training a multi-modal network often produces a network that has not learned the proper parameters for video action recognition. These multi-modal network models perform normally during training but fall short to its single modality counterpart when testing. The main cause for this performance drop could be two-fold. First, conventional methods use a poor fusion mechanism, where each modality is trained separately and then simply combine together (e.g., late feature fusion). Second, collecting videos is much more expensive than images. The insufficient video data can hardly provide support for training a multi-modal network that has a larger and more complex weight space. In this paper, we proposed the Language-guided Multi-Modal Fusion to address the above poor fusion problem. A sophisticatedly designed bi-modal video encoder is used to fuse audio and visual signal to generate a finer video representation. To ensure the over-fitting can be avoid, we use a language-guided contrastive learning to largely augment the video data to support the learning of multi-modal network. On a large-scale benchmark video dataset, the proposed method successfully elevates the accuracy of video action recognition.",
    "code_link": ""
  },
  "iccv2021_cveu_tsptemporally-sensitivepretrainingofvideoencodersforlocalizationtasks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVEU",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI for Creative Video Editing and Understanding",
    "title": "TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks",
    "authors": [
      "Humam Alwassel",
      "Silvio Giancola",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Alwassel_TSP_Temporally-Sensitive_Pretraining_of_Video_Encoders_for_Localization_Tasks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Alwassel_TSP_Temporally-Sensitive_Pretraining_of_Video_Encoders_for_Localization_Tasks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Due to the large memory footprint of untrimmed videos, current state-of-the-art video localization methods operate atop precomputed video clip features. These features are extracted from video encoders typically trained for trimmed action classification tasks, making such features not necessarily suitable for temporal localization. In this work, we propose a novel supervised pretraining paradigm for clip features that not only trains to classify activities but also considers background clips and global video information to improve temporal sensitivity. Extensive experiments show that using features trained with our novel pretraining strategy significantly improves the performance of recent state-of-the-art methods on three tasks: Temporal Action Localization, Action Proposal Generation, and Dense Video Captioning. We also show that our pretraining approach is effective across three encoder architectures and two pretraining datasets. We believe video feature encoding is an important building block for localization algorithms, and extracting temporally-sensitive features should be of paramount importance in building more accurate models. The code and pretrained models are available on our project website.",
    "code_link": ""
  },
  "iccv2021_cveu_plotstopreviewstowardsautomaticmoviepreviewretrievalusingpubliclyavailablemeta-data": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVEU",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI for Creative Video Editing and Understanding",
    "title": "Plots to Previews: Towards Automatic Movie Preview Retrieval Using Publicly Available Meta-Data",
    "authors": [
      "Bhagyashree Gaikwad",
      "Ankita Sontakke",
      "Manasi Patwardhan",
      "Niranjan Pedanekar",
      "Shirish Karande"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Gaikwad_Plots_to_Previews_Towards_Automatic_Movie_Preview_Retrieval_Using_Publicly_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Gaikwad_Plots_to_Previews_Towards_Automatic_Movie_Preview_Retrieval_Using_Publicly_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "'Preview', a concept popularized by Netflix, is a contiguous scene of a movie or a TV show highlighting its story, characters, and tone, thus helping viewers to make quick viewing decisions. To create previews, one needs scene-level semantic annotations related to the story, characters, and tone. Soliciting such annotations is an involved exercise and these are expensive to generate automatically. Instead, we aim at creating previews by availing readily available scene meta-data, while avoiding dependency on semantic scene-level annotations. We hypothesize that movie scenes that best match publicly available IMDb plot summaries can make good previews. We use 51 movies from the MovieGraph dataset, and find that a match of the plot summaries with scene dialogues, available through subtitles, is adequate to create usable movie previews, without the need for other semantic annotations. We validate the hypothesis by comparing ratings for scenes selected by the proposed method to those for scenes selected randomly, obtained from regular viewers as well as an expert. We report that even with this 'minimalist' approach, we can select at least one good preview scene for 26 out of 51 movies, as agreed upon by a critical expert judgment. Error analysis of the scenes indicates that features related to the plot structure might be needed to further improve the results.",
    "code_link": ""
  },
  "iccv2021_cveu_face,body,voicevideoperson-clusteringwithmultiplemodalities": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVEU",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI for Creative Video Editing and Understanding",
    "title": "Face, Body, Voice: Video Person-Clustering With Multiple Modalities",
    "authors": [
      "Andrew Brown",
      "Vicky Kalogeiton",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Brown_Face_Body_Voice_Video_Person-Clustering_With_Multiple_Modalities_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Brown_Face_Body_Voice_Video_Person-Clustering_With_Multiple_Modalities_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The objective of this work is person-clustering in videos -- grouping characters according to their identity. Previous methods focus on the narrower task of face-clustering, and for the most part ignore other cues such as the person's voice, their overall appearance (hair, clothes, posture), and the editing structure of the videos. Similarly, most current datasets evaluate only the task of face-clustering, rather than person-clustering. This limits their applicability to downstream applications such as story understanding which require person-level, rather than only face-level, reasoning. In this paper we make contributions to address both these deficiencies: first, we introduce a Multi-Modal High-Precision Clustering algorithm for person-clustering in videos using cues from several modalities (face, body, and voice). Second, we introduce a Video Person-Clustering dataset, for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, face-tracks when visible, and voice-tracks when speaking, with their associated features. The dataset is by far the largest of its kind, and covers films and TV-shows representing a wide range of demographics. Finally, we show the effectiveness of using multiple-modalities for person-clustering, explore the use of this new broad task for story understanding through character co-occurrences, and achieve a new state of the art on all available datasets for face and person-clustering.",
    "code_link": ""
  },
  "iccv2021_cveu_videotransformernetwork": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVEU",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI for Creative Video Editing and Understanding",
    "title": "Video Transformer Network",
    "authors": [
      "Daniel Neimark",
      "Omri Bar",
      "Maya Zohar",
      "Dotan Asselmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Neimark_Video_Transformer_Network_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Neimark_Video_Transformer_Network_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains 16.1X faster and runs 5.1X faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring 1.5X fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md.",
    "code_link": "https://github.com/zhoubolei/moments_models"
  },
  "iccv2021_cveu_learningwheretocutfromeditedvideos": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVEU",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI for Creative Video Editing and Understanding",
    "title": "Learning Where To Cut From Edited Videos",
    "authors": [
      "Yuzhong Huang",
      "Xue Bai",
      "Oliver Wang",
      "Fabian Caba",
      "Aseem Agarwala"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Huang_Learning_Where_To_Cut_From_Edited_Videos_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Huang_Learning_Where_To_Cut_From_Edited_Videos_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this work we propose a new approach for accelerating the video editing process by identifying good moments in time to cut unedited videos. We first validate that there is indeed a consensus among human viewers about good and bad cut moments with a user study, and then formulate this problem as a classification task. In order to train for such a task, we propose a self-supervised scheme that only requires pre-existing edited videos for training, of which there is large and diverse data readily available. We then propose a contrastive learning framework to train a 3D ResNet model to predict good regions to cut. We validate our method with a second user study, which indicates that clips generated by our model are preferred over a number of baselines.",
    "code_link": ""
  },
  "iccv2021_cveu_videocontrastivelearningwithglobalcontext": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVEU",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI for Creative Video Editing and Understanding",
    "title": "Video Contrastive Learning With Global Context",
    "authors": [
      "Haofei Kuang",
      "Yi Zhu",
      "Zhi Zhang",
      "Xinyu Li",
      "Joseph Tighe",
      "S\u00f6ren Schwertfeger",
      "Cyrill Stachniss",
      "Mu Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Kuang_Video_Contrastive_Learning_With_Global_Context_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Kuang_Video_Contrastive_Learning_With_Global_Context_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Contrastive learning has revolutionized the self-supervised image representation learning field and recently been adapted to the video domain. One of the greatest advantages of contrastive learning is that it allows us to flexibly define powerful loss objectives as long as we can find a reasonable way to formulate positive and negative samples to contrast. However, existing approaches rely heavily on the short-range spatiotemporal salience to form clip-level contrastive signals, thus limit themselves from using global context. In this paper, we propose a new video-level contrastive learning method based on segments to formulate positive pairs. Our formulation is able to capture the global context in a video, thus robust to temporal content change. We also incorporate a temporal order regularization term to enforce the inherent sequential structure of videos. Extensive experiments show that our video-level contrastive learning framework (VCLR) is able to outperform previous state-of-the-arts on five video datasets for downstream action classification, action localization, and video retrieval.",
    "code_link": ""
  },
  "iccv2021_cveu_vlg-netvideo-languagegraphmatchingnetworkforvideogrounding": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVEU",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - AI for Creative Video Editing and Understanding",
    "title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding",
    "authors": [
      "Mattia Soldan",
      "Mengmeng Xu",
      "Sisi Qu",
      "Jesper Tegner",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Soldan_VLG-Net_Video-Language_Graph_Matching_Network_for_Video_Grounding_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Soldan_VLG-Net_Video-Language_Graph_Matching_Network_for_Video_Grounding_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Grounding language queries in videos aims at identifying the time interval (or moment) semantically relevant to a language query. The solution to this challenging task demands understanding videos' and queries' semantic content and the fine-grained reasoning about their multi-modal interactions. Our key idea is to recast this challenge into an algorithmic graph matching problem. Fueled by recent advances in Graph Neural Networks, we propose to leverage Graph Convolutional Networks to model video and textual information as well as their semantic alignment. To enable the mutual exchange of information across the modalities, we design a novel Video-Language Graph Matching Network (VLG-Net) to match video and query graphs. Core ingredients include representation graphs built atop video snippets and query tokens separately and used to model intra-modality relationships. A Graph Matching layer is adopted for cross-modal context modeling and multi-modal fusion. Finally, moment candidates are created using masked moment attention pooling by fusing the moment's enriched snippet features. We demonstrate superior performance over state-of-the-art grounding methods on three widely used datasets for temporal localization of moments in videos with language queries: ActivityNet-Captions, TACoS, and DiDeMo.",
    "code_link": ""
  },
  "iccv2021_cvamd_medicalimageclassificationusinggeneralizedzeroshotlearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Medical Image Classification Using Generalized Zero Shot Learning",
    "authors": [
      "Dwarikanath Mahapatra",
      "Behzad Bozorgtabar",
      "Zongyuan Ge"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Mahapatra_Medical_Image_Classification_Using_Generalized_Zero_Shot_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Mahapatra_Medical_Image_Classification_Using_Generalized_Zero_Shot_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In many real world medical image classification settings we do not have access to samples of all possible disease classes, while a robust system is expected to give high performance in recognizing novel test data. We propose a generalized zero shot learning (GZSL) method that uses self supervised learning (SSL) for: 1) selecting anchor vectors of different disease classes; and 2) training a feature generator. Our approach does not require class attribute vectors which are available for natural images but not for medical images. SSL ensures that the anchor vectors are representative of each class. SSL is also used to generate synthetic features of unseen classes. Using a simpler architecture, our method matches a state of the art SSL based GZSL method for natural images and outperforms all methods for medical images. Our method is adaptable enough to accommodate class attribute vectors when they are available for natural images.",
    "code_link": "https://github.com/hubutui/Gleason"
  },
  "iccv2021_cvamd_deepfrequencyre-calibrationu-netformedicalimagesegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Deep Frequency Re-Calibration U-Net for Medical Image Segmentation",
    "authors": [
      "Reza Azad",
      "Afshin Bozorgpour",
      "Maryam Asadi-Aghbolaghi",
      "Dorit Merhof",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Azad_Deep_Frequency_Re-Calibration_U-Net_for_Medical_Image_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Azad_Deep_Frequency_Re-Calibration_U-Net_for_Medical_Image_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The human visual cortex is biased towards shape components while CNNs produce texture biased features. This fact may explain why the performance of CNN significantly degrades with low-labeled input data scenarios. In this paper, we propose a frequency re-calibration U-Net (FRCU-Net) for medical image segmentation. Representing an object in terms of frequency rather than texture can reduce the effect of texture bias and consequently may result in better generalization for a low data regime. To do so, we apply the Laplacian pyramid in the bottleneck layer of the U-shaped structure. The Laplacian pyramid represents the object proposal in different frequency domains, where the high frequencies are responsible for the texture information and lower frequencies might be related to the shape. Adaptively re-calibrating these frequency representations can produce a more discriminative representation for describing the object of interest. To this end, we first propose to use a channel-wise attention mechanism to capture the relationship between the channels of a set of feature maps in one layer of the frequency pyramid. Second, the extracted features of each level of the pyramid are then combined through a non-linear function based on their impact on the final segmentation output. The proposed FRCU-Net is evaluated on five datasets ISIC 2017, ISIC 2018, the PH, lung segmentation, and SegPC 2021 challenge datasets and compared to existing alternatives, achieving state-of-the-art results.",
    "code_link": "https://github.com/rezazad68/FRCU-Net"
  },
  "iccv2021_cvamd_uncertainty-awareganwithadaptivelossforrobustmriimageenhancement": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Uncertainty-Aware GAN With Adaptive Loss for Robust MRI Image Enhancement",
    "authors": [
      "Uddeshya Upadhyay",
      "Viswanath P. Sudarshan",
      "Suyash P. Awate"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Upadhyay_Uncertainty-Aware_GAN_With_Adaptive_Loss_for_Robust_MRI_Image_Enhancement_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Upadhyay_Uncertainty-Aware_GAN_With_Adaptive_Loss_for_Robust_MRI_Image_Enhancement_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Image-to-image translation is an ill-posed problem as unique one-to-one mapping may not exist between the source and target images. Learning-based methods proposed in this context often evaluate the performance on test data that is similar to the training data, which may be impractical. This demands robust methods that can quantify uncertainty in the prediction for making informed decisions, especially for critical areas such as medical imaging. Recent works that employ conditional generative adversarial networks (GANs) have shown improved performance in learning photo-realistic image-to-image mappings between the source and the target images. However, these methods do not focus on (i) robustness of the models to out-of-distribution (OOD)-noisy data and (ii) uncertainty quantification. This paper proposes a GAN-based framework that (i) models an adaptive loss function for robustness to OOD-noisy data that automatically tunes the spatially varying norm for penalizing the residuals and (ii) estimates the per-voxel uncertainty in the predictions. We demonstrate our method on two key applications in medical imaging: (i) undersampled magnetic resonance imaging (MRI) reconstruction (ii) MRI modality propagation. Our experiments with two different real-world datasets show that the proposed method (i) is robust to OOD-noisy test data and provides improved accuracy and (ii) quantifies voxel-level uncertainty in the predictions.",
    "code_link": ""
  },
  "iccv2021_cvamd_medskipmedicalreportgenerationusingskipconnectionsandintegratedattention": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "MedSkip: Medical Report Generation Using Skip Connections and Integrated Attention",
    "authors": [
      "Esha Pahwa",
      "Dwij Mehta",
      "Sanjeet Kapadia",
      "Devansh Jain",
      "Achleshwar Luthra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Pahwa_MedSkip_Medical_Report_Generation_Using_Skip_Connections_and_Integrated_Attention_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Pahwa_MedSkip_Medical_Report_Generation_Using_Skip_Connections_and_Integrated_Attention_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Medical scans are extremely important for accurate diagnosis and treatment. To assist staff members in such crucial tasks, developing a computer vision model that efficiently processes a medical image and results in a generated report can be highly beneficial. Such a robust system can not only act as a helping hand for professionals but also eliminate the chances of error that might arise in the case of in-experienced staff members. However, previous studies lack focus on experimenting with the visual extractor, which is of eminent importance. Keeping this in mind, we propose a novel architecture of a modified HRNet which includes added skip connections along with convolutional block attention modules (CBAM). The entire architecture can be divided into two components, the first being the visual extractor where the pre-processed image is fed into the HRNet convolutional layers. Outputs of each down-sampled layer are concatenated after passing through the attention modules. The second component includes the use of a memory-driven transformer that generates the report. We evaluate our model on two publicly available datasets, PEIR Gross and IU X-Ray, establishing new state-of-the-art for PEIR Gross while giving competitive results for IU X-Ray.",
    "code_link": ""
  },
  "iccv2021_cvamd_multi-scannerharmonizationofpairedneuroimagingdataviastructurepreservingembeddinglearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Multi-Scanner Harmonization of Paired Neuroimaging Data via Structure Preserving Embedding Learning",
    "authors": [
      "Mahbaneh Eshaghzadeh Torbati",
      "Dana L. Tudorascu",
      "Davneet S. Minhas",
      "Pauline Maillard",
      "Charles S. DeCarli",
      "Seong Jae Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Torbati_Multi-Scanner_Harmonization_of_Paired_Neuroimaging_Data_via_Structure_Preserving_Embedding_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Torbati_Multi-Scanner_Harmonization_of_Paired_Neuroimaging_Data_via_Structure_Preserving_Embedding_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Combining datasets from multiple sites/scanners has been becoming increasingly more prevalent in modern neuroimaging studies. Despite numerous benefits from the growth in sample size, substantial technical variability associated with site/scanner-related effects exists which may inadvertently bias subsequent downstream analyses. Such a challenge calls for a data harmonization procedure which reduces the scanner effects and allows the scans to be combined for pooled analyses. In this work, we present MISPEL (Multi-scanner Image harmonization via Structure Preserving Embedding Learning), a multi-scanner harmonization framework. Unlike existing techniques, MISPEL does not assume a perfect coregistration across the scans, and the framework is naturally extendable to more than two scanners. Importantly, we incorporate our multi-scanner dataset where each subject is scanned on four different scanners. This unique paired dataset allows us to define and aim for an ideal harmonization (e.g., each subject with identical brain tissue volumes on all scanners). We extensively view scanner effects under varying metrics and demonstrate how MISPEL significantly improves them.",
    "code_link": "https://github.com/Mahbaneh/MISPEL"
  },
  "iccv2021_cvamd_improvingtuberculosis(tb)predictionusingsyntheticallygeneratedcomputedtomography(ct)images": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Improving Tuberculosis (TB) Prediction Using Synthetically Generated Computed Tomography (CT) Images",
    "authors": [
      "Ashia Lewis",
      "Evanjelin Mahmoodi",
      "Yuyue Zhou",
      "Megan Coffee",
      "Elena Sizikova"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Lewis_Improving_Tuberculosis_TB_Prediction_Using_Synthetically_Generated_Computed_Tomography_CT_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Lewis_Improving_Tuberculosis_TB_Prediction_Using_Synthetically_Generated_Computed_Tomography_CT_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The evaluation of infectious disease processes on radiologic images is an important and challenging task in medical image analysis. Pulmonary infections can often be best imaged and evaluated through computed tomography (CT) scans, which are often not available in low-resource environments and difficult to obtain for critically ill patients. On the other hand, X-ray, a different type of imaging procedure, is inexpensive, often available at the bedside and more widely available, but offers a simpler, two dimensional image. We show that by relying on a model that learns to generate CT images from X-rays synthetically, we can improve the automatic disease classification accuracy and provide clinicians with a different look at the pulmonary disease process. Specifically, we investigate Tuberculosis (TB), a deadly bacterial infectious disease that predominantly affects the lungs, but also other organ systems. We show that relying on synthetically generated CT improves TB identification by 7.50% and distinguishes TB properties up to 12.16% better than the X-ray baseline.",
    "code_link": ""
  },
  "iccv2021_cvamd_graphcutslosstoboostmodelaccuracyandgeneralizabilityformedicalimagesegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Graph Cuts Loss To Boost Model Accuracy and Generalizability for Medical Image Segmentation",
    "authors": [
      "Zhou Zheng",
      "Masahiro Oda",
      "Kensaku Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Zheng_Graph_Cuts_Loss_To_Boost_Model_Accuracy_and_Generalizability_for_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Zheng_Graph_Cuts_Loss_To_Boost_Model_Accuracy_and_Generalizability_for_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Segmentation accuracy and generalization ability are essential for deep learning models, especially in medical image segmentation. We present a novel, robust yet straightforward loss function to boost model accuracy and generalizability for medical image segmentation. We reformulate the graph cuts cost function to a loss function for supervised learning. The graph cuts loss innately focuses on a dual penalty to optimize the regional properties and boundary regularization. We benchmark the proposed loss on six public retinal vessel segmentation datasets with a comprehensive intra-dataset and cross-dataset evaluation. Results reveal that the proposed loss is more generalizable, narrowing the performance gap between different architectures. Besides, models trained with our loss show higher segmentation accuracy and better generalization ability than those trained with other mainstream losses. Moreover, we extend our loss to other segmentation tasks, e.g., left atrium and liver tumor segmentation. The proposed loss still achieves comparable performance to the state-of-the-art, demonstrating its potential for any N-D segmentation problem. The code is available at https://github.com/zzhenggit/graph_cuts_loss.",
    "code_link": ""
  },
  "iccv2021_cvamd_adualadversarialcalibrationframeworkforautomaticfetalbrainbiometry": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry",
    "authors": [
      "Yuan Gao",
      "Lokhin Lee",
      "Richard Droste",
      "Rachel Craik",
      "Sridevi Beriwal",
      "Aris Papageorghiou",
      "Alison Noble"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Gao_A_Dual_Adversarial_Calibration_Framework_for_Automatic_Fetal_Brain_Biometry_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Gao_A_Dual_Adversarial_Calibration_Framework_for_Automatic_Fetal_Brain_Biometry_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This paper presents a novel approach to automatic fetal brain biometry motivated by needs in low- and medium- income countries. Specifically, we leverage high-end (HE) ultrasound images to build a biometry solution for low-cost (LC) point-of-care ultrasound images. We propose a novel unsupervised domain adaptation approach to train deep models to be invariant to significant image distribution shift between the image types. Our proposed method, which employs a Dual Adversarial Calibration (DAC) framework, consists of adversarial pathways which enforce model invariance to; i) adversarial perturbations in the feature space derived from LC images, and ii) appearance domain discrepancy. Our Dual Adversarial Calibration method estimates transcerebellar diameter and head circumference on images from low-cost ultrasound devices with a mean absolute error (MAE) of 2.43mm and 1.65mm, compared with 7.28 mm and 5.65 mm respectively for SOTA.",
    "code_link": ""
  },
  "iccv2021_cvamd_learningtoautomaticallydiagnosemultiplediseasesinpediatricchestradiographsusingdeepconvolutionalneuralnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Learning To Automatically Diagnose Multiple Diseases in Pediatric Chest Radiographs Using Deep Convolutional Neural Networks",
    "authors": [
      "Thanh T. Tran",
      "Hieu H. Pham",
      "Thang V. Nguyen",
      "Tung T. Le",
      "Hieu T. Nguyen",
      "Ha Q. Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Tran_Learning_To_Automatically_Diagnose_Multiple_Diseases_in_Pediatric_Chest_Radiographs_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Tran_Learning_To_Automatically_Diagnose_Multiple_Diseases_in_Pediatric_Chest_Radiographs_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Chest radiograph (CXR) interpretation is critical for the diagnosis of various thoracic diseases in pediatric patients. This task, however, is error-prone and requires a high level of understanding of radiologic expertise. Recently, deep convolutional neural networks (D-CNNs) have shown remarkable performance in interpreting CXR in adults. However, there is a lack of evidence indicating that D-CNNs can recognize accurately multiple lung pathologies from pediatric CXR scans. In particular, the development of diagnostic models for the detection of pediatric chest diseases faces significant challenges such as (i) lack of physician-annotated datasets and (ii) class imbalance problems. In this paper, we retrospectively collect a large dataset of 5,017 pediatric CXR scans, for which each is manually labeled by an experienced radiologist for the presence of 10 common pathologies. A D-CNN model is then trained on 3,550 annotated scans to classify multiple pediatric lung pathologies automatically. To address the high-class imbalance issue, we propose to modify and apply \"Distribution-Balanced loss\" for training D-CNNs which reshapes the standard Binary-Cross Entropy loss (BCE) to efficiently learn harder samples by down-weighting the loss assigned to the majority classes. On an independent test set of 777 studies, the proposed approach yields an area under the receiver operating characteristic (AUC) of 0.709 (95% CI, 0.690-0.729). The sensitivity, specificity, and F1-score at the cutoff value are 0.722 (0.694-0.750), 0.579 (0.563-0.595), and 0.389 (0.373-0.405), respectively. These results significantly outperform previous state-of-the-art methods on most of the target diseases. Moreover, our ablation studies validate the effectiveness of the proposed loss function compared to other standard losses, e.g., BCE and Focal Loss, for this learning task. Overall, we demonstrate the potential of D-CNNs in interpreting pediatric CXRs.",
    "code_link": ""
  },
  "iccv2021_cvamd_generalizingfew-shotclassificationofwhole-genomedoublingacrosscancertypes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Generalizing Few-Shot Classification of Whole-Genome Doubling Across Cancer Types",
    "authors": [
      "Sherry Chao",
      "David Belanger"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Chao_Generalizing_Few-Shot_Classification_of_Whole-Genome_Doubling_Across_Cancer_Types_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Chao_Generalizing_Few-Shot_Classification_of_Whole-Genome_Doubling_Across_Cancer_Types_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The study and treatment of cancer is traditionally specialized to the cancer's primary site of origin. However, certain phenotypes are shared across cancer types and have important implications for clinical care. To date, automating the identification of these characteristics from routine clinical data - irrespective of the type of cancer - is impaired by tissue-specific variability and limited labeled data. Whole-genome doubling is one such phenotype; whole-genome doubling events occur in nearly every type of cancer and have significant prognostic implications. Using digitized histopathology slide images of primary tumor biopsies, we train a deep neural network model end-to-end to accurately generalize few-shot classification of whole-genome doubling across 17 cancer types. By taking a meta-learning approach, cancer types are treated as separate but jointly-learned tasks. This approach outperforms a traditional neural network classifier and quickly generalizes to both held-out cancer types and batch effects. These results demonstrate the unrealized potential for meta-learning to not only account for between-cancer type variability but also remedy technical variability, enabling real-time identification of cancer phenotypes that are too often costly and inefficient to obtain.",
    "code_link": "https://github.com/chsher/CAML"
  },
  "iccv2021_cvamd_styletransferbasedcoronaryarterysegmentationinx-rayangiogram": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Style Transfer Based Coronary Artery Segmentation in X-Ray Angiogram",
    "authors": [
      "Supriti Mulay",
      "Keerthi Ram",
      "Balamurali Murugesan",
      "Mohanasankar Sivaprakasam"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Mulay_Style_Transfer_Based_Coronary_Artery_Segmentation_in_X-Ray_Angiogram_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Mulay_Style_Transfer_Based_Coronary_Artery_Segmentation_in_X-Ray_Angiogram_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "X-ray coronary angiography (XCA) is a principal approach employed for identifying coronary disorders. Deep learning-based networks have recently shown tremendous promise in the diagnosis of coronary disorder from XCA scans. A deep learning-based edge adaptive instance normalization style transfer technique for segmenting the coronary arteries is presented in this paper. The proposed technique combines adaptive instance normalization style transfer with the dense extreme inception network and convolution block attention module to get the best artery segmentation performance. We tested the proposed method on two publicly available XCA datasets, and achieved a segmentation accuracy of 0.9658 and Dice coefficient of 0.71. We believe that the proposed method shows that the prediction can be completed in the fastest time with training on the natural images, and can be reliably used to diagnose and detect coronary disorders.",
    "code_link": ""
  },
  "iccv2021_cvamd_berthopaneffectivevision-and-languagemodelforchestx-raydiseasediagnosis": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "BERTHop: An Effective Vision-and-Language Model for Chest X-Ray Disease Diagnosis",
    "authors": [
      "Masoud Monajatipoor",
      "Mozhdeh Rouhsedaghat",
      "Liunian Harold Li",
      "Aichi Chien",
      "C.-C. Jay Kuo",
      "Fabien Scalzo",
      "Kai-Wei Chang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Monajatipoor_BERTHop_An_Effective_Vision-and-Language_Model_for_Chest_X-Ray_Disease_Diagnosis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Monajatipoor_BERTHop_An_Effective_Vision-and-Language_Model_for_Chest_X-Ray_Disease_Diagnosis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Vision-and-language (V&L) models take image and text as input and learn to capture the associations between them. Prior studies show that pre-trained V&L models can significantly improve the model performance for downstream tasks such as Visual Question Answering (VQA). However, V&L models are less effective when applied in the medical domain (e.g., on X-ray images and clinical notes) due to the domain gap. In this paper, we investigate the challenges of applying pre-trained V&L models in medical applications. In particular, we identify that the visual representation in general V&L models is not suitable for processing medical data. To overcome this limitation, we propose BERTHop, a transformer-based model based on PixelHop++ and VisualBERT, for better capturing the associations between the two modalities. Experiments on the OpenI dataset, a commonly used thoracic disease diagnosis benchmark, show that BERTHop achieves an average Area Under the Curve (AUC) of 98.12% which is 1.62% higher than state-of-the-art (SOTA) while it is trained on a 9x smaller dataset.",
    "code_link": ""
  },
  "iccv2021_cvamd_soodself-supervisedout-of-distributiondetectionunderdomainshiftformulti-classcolorectalcancertissuetypes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "SOoD: Self-Supervised Out-of-Distribution Detection Under Domain Shift for Multi-Class Colorectal Cancer Tissue Types",
    "authors": [
      "Behzad Bozorgtabar",
      "Guillaume Vray",
      "Dwarikanath Mahapatra",
      "Jean-Philippe Thiran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Bozorgtabar_SOoD_Self-Supervised_Out-of-Distribution_Detection_Under_Domain_Shift_for_Multi-Class_Colorectal_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Bozorgtabar_SOoD_Self-Supervised_Out-of-Distribution_Detection_Under_Domain_Shift_for_Multi-Class_Colorectal_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The goal of out-of-distribution (OoD) detection is to identify unseen categories of inputs different from those seen during training, which is an important requirement for the safe deployment of deep neural networks in computational pathology. Additionally, to make OoD detection applicable in clinical applications, one may encounter image data distribution shifts. This paper argues that practical OoD detection should handle both semantic shift and data distribution shift simultaneously. We propose a new self-supervised OoD detector for colorectal cancer tissue types based on a clustering scheme. Our work's central tenet benefits from multi-view consistency learning with a supplementary view based on style augmentation to mitigate domain shift. The learned representation is then adapted to minimize images' predictive entropy to segregate in-distribution examples from OoDs on the target data domain. We evaluated our method on two public colorectal tissue types datasets. Our method achieved state-of-the-art OoD detection performance over various self-supervised baselines. The code, data, and models are available at https://github.com/BehzadBozorgtabar/SOoD.",
    "code_link": "https://github.com/BehzadBozorgtabar/SOoD"
  },
  "iccv2021_cvamd_efficientarlimprovingskincancerdiagnosesbycombininglightweightattentiononefficientnet": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "EfficientARL: Improving Skin Cancer Diagnoses by Combining Lightweight Attention on EfficientNet",
    "authors": [
      "Miguel Nehmad Alche",
      "Daniel Acevedo",
      "Marta Mejail"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Alche_EfficientARL_Improving_Skin_Cancer_Diagnoses_by_Combining_Lightweight_Attention_on_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Alche_EfficientARL_Improving_Skin_Cancer_Diagnoses_by_Combining_Lightweight_Attention_on_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Melanoma is a very dangerous form of skin cancer. Early diagnosis is crucial to increase the chances of its cure. Based on this, computer vision algorithms can be used to analyze dermoscopic images of skin lesions and decide if these correspond to benign or malignant tumors. In this work we propose the adaptation of the attention residual learning designed for ResNets to the EfficientNet networks, and compare this mechanism with attention mechanisms that these networks already have. We maintain the efficiency of these networks since only one extra parameter per stage needs to be trained. We also test several preprocessing methods on the dataset improving the final performance.",
    "code_link": ""
  },
  "iccv2021_cvamd_dmnetdual-streammarkerguideddeepnetworkfordensecellsegmentationandlineagetracking": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "DMNet: Dual-Stream Marker Guided Deep Network for Dense Cell Segmentation and Lineage Tracking",
    "authors": [
      "Rina Bao",
      "Noor M. Al-Shakarji",
      "Filiz Bunyak",
      "Kannappan Palaniappan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Bao_DMNet_Dual-Stream_Marker_Guided_Deep_Network_for_Dense_Cell_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Bao_DMNet_Dual-Stream_Marker_Guided_Deep_Network_for_Dense_Cell_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Accurate segmentation and tracking of cells in microscopy image sequences is extremely beneficial in clinical diagnostic applications and biomedical research. A continuing challenge is the segmentation of dense touching cells and deforming cells with indistinct boundaries, in low signal-to-noise-ratio images. In this paper, we present a dual-stream marker-guided network (DMNet) for segmentation of touching cells in microscopy videos of many cell types. DMNet uses an explicit cell marker detection stream, with a separate mask-prediction stream using a distance map penalty function, which enables supervised training to focus attention on touching and nearby cells. For multi-object cell tracking we use M2Track tracking-by-detection approach with multi-step data association. Our M2Track with mask overlap includes short term trajectory-to-cell association followed by trajectory-to-trajectory association to re-link tracklets with missing segmentation masks over a short sequence of frames. Our combined detection, segmentation and tracking algorithm has proven its potential in the IEEE ISBI 2021 6th Cell Tracking Challenge (CTC-6) where we achieved multiple top three rankings for diverse cell types. Our team name is MU-Ba-US, and the implementation of DMNet is available at, http://celltrackingchallenge.net/participants/MU-Ba-US/.",
    "code_link": ""
  },
  "iccv2021_cvamd_unsupervised3dshapecoverageestimationwithapplicationstocolonoscopy": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Unsupervised 3D Shape Coverage Estimation With Applications to Colonoscopy",
    "authors": [
      "Yochai Blau",
      "Daniel Freedman",
      "Valentin Dashinsky",
      "Roman Goldenberg",
      "Ehud Rivlin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Blau_Unsupervised_3D_Shape_Coverage_Estimation_With_Applications_to_Colonoscopy_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Blau_Unsupervised_3D_Shape_Coverage_Estimation_With_Applications_to_Colonoscopy_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Reconstructing shapes from partial and noisy 3D data is a well-studied problem, which in recent years has been dominated by data-driven techniques. Yet in a low data regime, these techniques struggle to provide fine and accurate reconstructions. Here we focus on the relaxed problem of estimating shape coverage, i.e. asking \"how much of the shape was seen?\" rather than \"what was the original shape?\" We propose a method for unsupervised shape coverage estimation, and validate that this task can be performed accurately in a low data regime. Shape coverage estimation can provide valuable insights which pave the way for innovative applications, as we demonstrate for the case of deficient coverage detection in colonoscopy screenings.",
    "code_link": ""
  },
  "iccv2021_cvamd_vtgansemi-supervisedretinalimagesynthesisanddiseasepredictionusingvisiontransformers": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "VTGAN: Semi-Supervised Retinal Image Synthesis and Disease Prediction Using Vision Transformers",
    "authors": [
      "Sharif Amit Kamran",
      "Khondker Fariha Hossain",
      "Alireza Tavakkoli",
      "Stewart Lee Zuckerbrod",
      "Salah A. Baker"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Kamran_VTGAN_Semi-Supervised_Retinal_Image_Synthesis_and_Disease_Prediction_Using_Vision_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Kamran_VTGAN_Semi-Supervised_Retinal_Image_Synthesis_and_Disease_Prediction_Using_Vision_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In Fluorescein Angiography (FA), an exogenous dye is injected in the bloodstream to image the vascular structure of the retina. The injected dye can cause adverse reactions such as nausea, vomiting, anaphylactic shock, and even death. In contrast, color fundus imaging is a non-invasive technique used for photographing the retina but does not have sufficient fidelity for capturing its vascular structure. The only non-invasive method for capturing retinal vasculature is optical coherence tomography-angiography (OCTA). However, OCTA equipment is quite expensive, and stable imaging is limited to small areas on the retina. In this paper, we propose a novel conditional generative adversarial network (GAN) capable of simultaneously synthesizing FA images from fundus photographs while predicting retinal degeneration. The proposed system has the benefit of addressing the problem of imaging retinal vasculature in a non-invasive manner as well as predicting the existence of retinal abnormalities. We use a semi-supervised approach to train our GAN using multiple weighted losses on different modalities of data. Our experiments validate that the proposed architecture exceeds recent state-of-the-art generative networks for fundus-to-angiography synthesis. Moreover, our vision transformer-based discriminators generalize quite well on out-of-distribution data sets for retinal disease prediction.",
    "code_link": "https://github.com/SharifAmit/VTGAN"
  },
  "iccv2021_cvamd_end-to-endlearningoffusedimageandnon-imagefeaturesforimprovedbreastcancerclassificationfrommri": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "End-to-End Learning of Fused Image and Non-Image Features for Improved Breast Cancer Classification From MRI",
    "authors": [
      "Gregory Holste",
      "Savannah C. Partridge",
      "Habib Rahbar",
      "Debosmita Biswas",
      "Christoph I. Lee",
      "Adam M. Alessio"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Holste_End-to-End_Learning_of_Fused_Image_and_Non-Image_Features_for_Improved_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Holste_End-to-End_Learning_of_Fused_Image_and_Non-Image_Features_for_Improved_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Breast cancer diagnosis is inherently multimodal. To assess a patient's cancer status, physicians integrate imaging findings with a variety of clinical risk factor data. Despite this, deep learning approaches for automatic breast cancer classification often only utilize image data or non-image clinical data, but not both simultaneously. In this work, we implemented and compared strategies for the fusion of imaging and tabular non-image data in an end-to-end trainable manner, evaluating fusion at different stages in the model (fusing intermediate features vs. output probabilities) and with different operations (concatenation vs. addition vs. multiplication). This retrospective study utilized dynamic contrast-enhanced MRI (DCE-MRI) data from 10,185 breast MRI examinations of 5,248 women. DCE-MRIs were reduced to 2D maximum intensity projections, split into single-breast images, then linked to a set of 18 non-image features including clinical indication and mammographic breast density. We first trained unimodal baseline models on images alone and non-image data alone. We then developed three multimodal fusion models that learn jointly from image and non-image data, evaluating performance by area under the receiver operating characteristic curve (AUC) and specificity at 95% sensitivity. The image-only baseline achieved an AUC of 0.849 (95% CI: 0.834, 0.864) and specificity at 95% sensitivity of 30.1% (95% CI: 23.1%, 37.0%), while the best-performing fusion model achieved an AUC of 0.898 (95% CI: 0.885, 0.909) and specificity of 49.1% (95% CI: 38.8%, 55.3%). Further- more, all three fusion methods significantly outperformed both unimodal baselines with respect to AUC and specificity at 95% sensitivity. This work demonstrates in our dataset for breast cancer classification that incorporating non-image data with images can significantly improve predictive performance and that fusion of intermediate learned features is superior to fusion of final probabilities.",
    "code_link": "https://github.com/gholste/breast_mri_fusion"
  },
  "iccv2021_cvamd_segmentationforclassificationofscreeningpancreaticneuroendocrinetumors": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Segmentation for Classification of Screening Pancreatic Neuroendocrine Tumors",
    "authors": [
      "Zhuotun Zhu",
      "Yongyi Lu",
      "Wei Shen",
      "Elliot K. Fishman",
      "Alan L. Yuille"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Zhu_Segmentation_for_Classification_of_Screening_Pancreatic_Neuroendocrine_Tumors_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Zhu_Segmentation_for_Classification_of_Screening_Pancreatic_Neuroendocrine_Tumors_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This work presents an intuitive method to detect in the early stage the pancreatic neuroendocrine tumors (PNETs), a group of endocrine tumors arising in the pancreas, which are the second common type of pancreatic cancer, by checking the abdominal CT scans. To the best of our knowledge, this task has not been studied before as a computational task. To provide radiologists with tumor locations, we adopt a segmentation framework to classify CT volumes by checking if at least a sufficient number of voxels is segmented as tumors. To quantitatively analyze our method, we collect and voxelwisely label a new abdominal CT dataset containing 376 cases with both arterial and venous phases available for each case, in which 228 cases were diagnosed with PNETs while the remaining 148 cases are normal, which is currently the largest dataset for PNETs to the best of our knowledge. In order to incorporate rich knowledge from radiologists to our framework, we annotate dilated pancreatic duct as well, which is regarded as an abnormality indicator. Quantitatively, our approach outperforms state-of-the-art segmentation networks and achieves a sensitivity of 89.47% at a specificity of 81.08%, which indicates a potential direction to achieve a clinical impact related to cancer diagnosis by earlier tumor detection.",
    "code_link": ""
  },
  "iccv2021_epic_1000pupilsegmentationsinasecondusinghaarlikefeaturesandstatisticallearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Egocentric Perception, Interaction and Computing",
    "title": "1000 Pupil Segmentations in a Second Using Haar Like Features and Statistical Learning",
    "authors": [
      "Wolfgang Fuhl",
      "Johannes Schneider",
      "Enkelejda Kasneci"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Fuhl_1000_Pupil_Segmentations_in_a_Second_Using_Haar_Like_Features_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Fuhl_1000_Pupil_Segmentations_in_a_Second_Using_Haar_Like_Features_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper we present a new approach for pupil segmentation. It can be computed and trained very efficiently, making it ideal for online use for high speed eye trackers as well as for energy saving pupil detection in mobile eye tracking. The approach is inspired by the BORE and CBF algorithms and generalizes the binary comparison by Haar features. Since these features are intrinsically very susceptible to noise and fluctuating light conditions, we combine them with conditional pupil shape probabilities. In addition, we also rank each feature according to its importance in determining the pupil shape. Another advantage of our method is the use of statistical learning, which is very efficient and can even be used online.",
    "code_link": ""
  },
  "iccv2021_epic_seeingtheunseenpredictingthefirst-personcamerawearerslocationandposeinthird-personscenes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Seeing the Unseen: Predicting the First-Person Camera Wearer's Location and Pose in Third-Person Scenes",
    "authors": [
      "Yangming Wen",
      "Krishna Kumar Singh",
      "Markham Anderson",
      "Wei-Pang Jan",
      "Yong Jae Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Wen_Seeing_the_Unseen_Predicting_the_First-Person_Camera_Wearers_Location_and_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Wen_Seeing_the_Unseen_Predicting_the_First-Person_Camera_Wearers_Location_and_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Our goal is to predict the camera wearer's location and pose in his/her environment based on what's captured by the camera wearer's first-person wearable camera. Toward this goal, we first collect a new dataset in which the camera wearer performs various activities (e.g., opening a fridge, reading a book) in different scenes with time-synchronized first-person and stationary third-person cameras. We then propose a novel deep network architecture, which takes as input the first-person video frames and empty third-person scene image (without the camera wearer) to predict the location and pose of the camera wearer. We explore and compare our approach with several intuitive baselines and show initial promising results on this novel, challenging problem.",
    "code_link": ""
  },
  "iccv2021_epic_maadamodelanddatasetforattendedawarenessindriving": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Egocentric Perception, Interaction and Computing",
    "title": "MAAD: A Model and Dataset for \"Attended Awareness\" in Driving",
    "authors": [
      "Deepak Gopinath",
      "Guy Rosman",
      "Simon Stent",
      "Katsuya Terahata",
      "Luke Fletcher",
      "Brenna Argall",
      "John Leonard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Gopinath_MAAD_A_Model_and_Dataset_for_Attended_Awareness_in_Driving_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Gopinath_MAAD_A_Model_and_Dataset_for_Attended_Awareness_in_Driving_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a computational model to estimate a person's attended awareness of their environment. We define \"attended awareness\" to be those parts of a potentially dynamic scene which a person has attended to in recent history and which they are still likely to be physically aware of. Our model takes as input scene information in the form of a video and noisy gaze estimates, and outputs visual saliency, a refined gaze estimate and an estimate of the person's attended awareness. In order to test our model, we capture a new dataset with a high-precision gaze tracker including 24.5 hours of gaze sequences from 23 subjects attending to videos of driving scenes. The dataset also contains third-party annotations of the subjects' attended awareness based on observations of their scan path. Our results show that our model is able to reasonably estimate attended awareness in a controlled setting, and in the future could potentially be extended to real egocentric driving data to help enable more effective ahead-of-time warnings in safety systems and thereby augment driver performance. We also demonstrate our model's effectiveness on the tasks of saliency, gaze calibration and denoising, using both our dataset and an existing saliency dataset. We make our model and dataset available at https://github.com/ToyotaResearchInstitute/att-aware/.",
    "code_link": "https://github.com/ToyotaResearchInstitute/att-aware"
  },
  "iccv2021_epic_egocentricindoorlocalizationfromroomlayoutsandimageoutercorners": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Egocentric Indoor Localization From Room Layouts and Image Outer Corners",
    "authors": [
      "Xiaowei Chen",
      "Guoliang Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Chen_Egocentric_Indoor_Localization_From_Room_Layouts_and_Image_Outer_Corners_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Chen_Egocentric_Indoor_Localization_From_Room_Layouts_and_Image_Outer_Corners_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Egocentric indoor localization is an important issue for many in-home smart technologies. Room layouts have been used to characterize indoor scene images by a few typical space configurations defined by boundary lines and junctions, which are mostly detectable or inferable by deep learning methods. In this paper, we study camera pose estimation for egocentric indoor localization from room layouts that is cast as a PnL (Perspective-n-Line) problem. Specifically, image outer corners (IOCs), which are the intersecting points between image borders and room layout boundaries, are introduced to improve PnL optimization by involving additional auxiliary lines in an image. This leads to a new PnL-IOC algorithm where 3D correspondence estimation of IOCs are jointly solved with camera pose optimization in the iterative Gauss-Newton algorithm. Experiment results on both simulated and real images show the advantages of PnL-IOC on the accuracy and robustness of camera pose estimation over the existing PnL methods.",
    "code_link": ""
  },
  "iccv2021_epic_slowfastrolling-unrollinglstmsforactionanticipationinegocentricvideos": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Egocentric Perception, Interaction and Computing",
    "title": "SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos",
    "authors": [
      "Nada Osman",
      "Guglielmo Camporese",
      "Pasquale Coscia",
      "Lamberto Ballan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Osman_SlowFast_Rolling-Unrolling_LSTMs_for_Action_Anticipation_in_Egocentric_Videos_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Osman_SlowFast_Rolling-Unrolling_LSTMs_for_Action_Anticipation_in_Egocentric_Videos_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Action anticipation in egocentric videos is a difficult task due to the inherently multi-modal nature of human actions. Additionally, some actions happen faster or slower than others depending on the actor or surrounding context which could vary each time and lead to different predictions. Based on this idea, we build upon RULSTM architecture, which is specifically designed for anticipating human actions, and propose a novel attention-based technique to evaluate, simultaneously, slow and fast features extracted from three different modalities, namely RGB, optical flow and extracted objects. Two branches process information at different time scales, i.e., frame-rates, and several fusion schemes are considered to improve prediction accuracy. We perform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, and demonstrate that our technique systematically improves the results of RULSTM architecture for Top-5 accuracy metric at different anticipation times.",
    "code_link": ""
  },
  "iccv2021_rlq_llvipavisible-infraredpaireddatasetforlow-lightvision": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Real-World Computer Vision From Inputs With Limited Quality",
    "title": "LLVIP: A Visible-Infrared Paired Dataset for Low-Light Vision",
    "authors": [
      "Xinyu Jia",
      "Chuang Zhu",
      "Minzhen Li",
      "Wenqi Tang",
      "Wenli Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/html/Jia_LLVIP_A_Visible-Infrared_Paired_Dataset_for_Low-Light_Vision_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/papers/Jia_LLVIP_A_Visible-Infrared_Paired_Dataset_for_Low-Light_Vision_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "It is very challenging for various visual tasks such as image fusion, pedestrian detection, and image-to-image translation in low light conditions due to the loss of effective target areas. In this case, infrared and visible images can be used together to provide both rich detail information and effective target areas. In this paper, we present LLVIP, a visible-infrared paired dataset for low-light vision. This dataset contains 33672 images, or 16836 pairs, most of which were taken at very dark scenes, and all of the images are strictly aligned in time and space. Pedestrians in the dataset are labeled. We compare the dataset with other visible-infrared datasets and evaluate the performance of some popular visual algorithms including image fusion, pedestrian detection, and image-to-image translation on the dataset. The experimental results demonstrate the complementary effect of fusion on image information and find the deficiency of existing algorithms of the three visual tasks in very low-light conditions. We believe the LLVIP dataset will contribute to the community of computer vision by promoting image fusion, pedestrian detection, and image-to-image translation in very low-light applications. The dataset is being released in https://bupt-ai-cz.github.io/LLVIP/.",
    "code_link": ""
  },
  "iccv2021_rlq_temporalkernelconsistencyforblindvideosuper-resolution": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Real-World Computer Vision From Inputs With Limited Quality",
    "title": "Temporal Kernel Consistency for Blind Video Super-Resolution",
    "authors": [
      "Lichuan Xiang",
      "Royson Lee",
      "Mohamed S. Abdelfattah",
      "Nicholas D. Lane",
      "Hongkai Wen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/html/Xiang_Temporal_Kernel_Consistency_for_Blind_Video_Super-Resolution_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/papers/Xiang_Temporal_Kernel_Consistency_for_Blind_Video_Super-Resolution_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep learning-based blind super-resolution (SR) methods have recently achieved unprecedented performance in upscaling frames with unknown degradation. These models are able to accurately estimate the unknown downscaling kernel from a given low-resolution (LR) image in order to leverage the kernel during restoration. Although these approaches have largely been successful, they are predominantly image-based and therefore do not exploit the temporal properties of the kernels across multiple video frames. In this paper, we investigated the temporal properties of the kernels and highlighted its importance in the task of blind video super-resolution. Specifically, we measured the kernel temporal consistency of real-world videos and illustrated how the estimated kernels might change per frame in videos of varying dynamicity of the scene and its objects. With this new insight, we revisited previous popular video SR approaches, and showed that previous assumptions of using a fixed kernel throughout the restoration process can lead to visual artifacts when upscaling real-world videos. In order to counteract this, we tailored existing single-image and video SR techniques to leverage kernel consistency during both kernel estimation and video upscaling processes. Extensive experiments on synthetic and real-world videos show substantial restoration gains quantitatively and qualitatively, achieving the new state-of-the-art in blind video SR and underlining the potential of exploiting kernel temporal consistency.",
    "code_link": ""
  },
  "iccv2021_rlq_single-stagefacedetectionunderextremelylow-lightconditions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Real-World Computer Vision From Inputs With Limited Quality",
    "title": "Single-Stage Face Detection Under Extremely Low-Light Conditions",
    "authors": [
      "Jun Yu",
      "Xinlong Hao",
      "Peng He"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/html/Yu_Single-Stage_Face_Detection_Under_Extremely_Low-Light_Conditions_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/papers/Yu_Single-Stage_Face_Detection_Under_Extremely_Low-Light_Conditions_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face detection has been well studied for many years. One remaining challenge is to detect faces from low-light images. The brightness of the image captured under extremely low-light conditions could be very low and the contrast will be severely reduced. It is easy to cause confusion during feature extraction and affects the performance of face detection. In this paper, we propose a single-stage low-light face detection method. First, we design an improved MSRCR method to increase the image quality under the condition of ensuring that the colors of the image are not distorted. It shows better enhancement effect than other methods in the DARK FACE dataset, especially the low-resolution face details are well preserved. There are a number of small, blurred and partially occluded faces. To address this, the Pyramidbox algorithm is a very effective face detection algorithm. Moreover, we conduct multi-scale tests to further develop the performance of the model and integrated the results through Soft-NMS method to obtain final results. Integrating these techniques, this paper has achieved high accuracy and obtained excellent results in the face detection task of the DARK FACE dataset.",
    "code_link": ""
  },
  "iccv2021_rlq_blocksworldrevisitedtheeffectofself-occlusiononclassificationbyconvolutionalneuralnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Real-World Computer Vision From Inputs With Limited Quality",
    "title": "Blocks World Revisited: The Effect of Self-Occlusion on Classification by Convolutional Neural Networks",
    "authors": [
      "Markus D. Solbach",
      "John K. Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/html/Solbach_Blocks_World_Revisited_The_Effect_of_Self-Occlusion_on_Classification_by_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/papers/Solbach_Blocks_World_Revisited_The_Effect_of_Self-Occlusion_on_Classification_by_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Despite the recent successes in computer vision, there remain new avenues to explore. In this work, we propose a new dataset to investigate the effect of self-occlusion on deep neural networks. With TEOS (The Effect of Self-Occlusion), we propose a 3D blocks world dataset that focuses on the geometric shape of 3D objects and their omnipresent self-occlusion. We designed TEOS to investigate the role of self-occlusion in the context of object classification. In the real-world, self-occlusion of 3D objects still presents significant challenges for deep learning approaches. However, humans deal with this by deploying complex strategies, for instance, by changing the viewpoint or manipulating the scene to gather necessary information. With TEOS, we present a dataset with two subsets (L1 and L2), containing 36 and 12 objects, respectively. We provide 768 uniformly sampled views of each object, their mask, object and camera position, orientation, amount of self-occlusion, as well as the CAD model of each object. We present baseline evaluations with five well-known classification deep neural networks and show that TEOS poses a significant challenge for all of them. The dataset, as well as the pre-trained models, are made publicly available for the scientific community under https://data.nvision.eecs.yorku.ca/TEOS.",
    "code_link": ""
  },
  "iccv2021_rlq_uacanuncertainty-awarefaceclusteringalgorithm": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Real-World Computer Vision From Inputs With Limited Quality",
    "title": "UAC: An Uncertainty-Aware Face Clustering Algorithm",
    "authors": [
      "Biplob Debnath",
      "Giuseppe Coviello",
      "Yi Yang",
      "Srimat Chakradhar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/html/Debnath_UAC_An_Uncertainty-Aware_Face_Clustering_Algorithm_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/papers/Debnath_UAC_An_Uncertainty-Aware_Face_Clustering_Algorithm_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We investigate ways to leverage uncertainty in face images to improve the quality of the face clusters. We observe that popular clustering algorithms do not produce better quality clusters when clustering probabilistic face representations that implicitly model uncertainty -- these algorithms predict up to 9.6X more clusters than the ground truth for the IJB-A benchmark. We empirically analyze the causes for this unexpected behavior and identify excessive false-positives and false-negatives (when comparing face-pairs) as the main reasons for poor quality clustering. Based on this insight, we propose an uncertainty-aware \\clustering algorithm, UAC, which explicitly leverages uncertainty information during clustering to decide when a pair of faces are similar or when a predicted cluster should be discarded. UAC considers (a) uncertainty of faces in face-pairs, (b) bins face-pairs into different categories based on an uncertainty threshold, (c) intelligently varies the similarity threshold during clustering to reduce false-negatives and false-positives, and (d) discards predicted clusters that exhibit a high measure of uncertainty. Extensive experimental results on several popular benchmarks and comparisons with state-of-the-art clustering methods show that UAC produces significantly better clusters by leveraging uncertainty in face images -- predicted number of clusters is up to 0.18X more of the ground truth for the IJB-A benchmark.",
    "code_link": ""
  },
  "iccv2021_abaw_causalaffectpredictionmodelusingapastfacialimagesequence": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Causal Affect Prediction Model Using a Past Facial Image Sequence",
    "authors": [
      "Geesung Oh",
      "Euiseok Jeong",
      "Sejoon Lim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Oh_Causal_Affect_Prediction_Model_Using_a_Past_Facial_Image_Sequence_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Oh_Causal_Affect_Prediction_Model_Using_a_Past_Facial_Image_Sequence_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Among human affective behavior research, facial expression recognition research is improving in performance along with the development of deep learning. For improved performance, not only past images but also future images should be used along with corresponding facial images, but there are obstacles to the application of this technique to real-time environments. In this paper, we propose the causal affect prediction network (CAPNet), which uses only past facial images to predict corresponding affective valence and arousal. We train CAPNet to learn causal inference between past images and corresponding affective valence and arousal through supervised learning by pairing the sequence of past images with the current label using the Aff-Wild2 dataset. We show through experiments that the well-trained CAPNet outperforms the baseline of the second challenge of the Affective Behavior Analysis in-the-wild (ABAW2) Competition by predicting affective valence and arousal only with past facial images one-third of a second earlier. Therefore, in real-time application, CAPNet can reliably predict affective valence and arousal only with past data.",
    "code_link": "https://github.com/gsethan17/CAPNet_ABAW2021"
  },
  "iccv2021_abaw_continuousemotionrecognitionwithaudio-visualleader-followerattentivefusion": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Continuous Emotion Recognition With Audio-Visual Leader-Follower Attentive Fusion",
    "authors": [
      "Su Zhang",
      "Yi Ding",
      "Ziquan Wei",
      "Cuntai Guan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Zhang_Continuous_Emotion_Recognition_With_Audio-Visual_Leader-Follower_Attentive_Fusion_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Zhang_Continuous_Emotion_Recognition_With_Audio-Visual_Leader-Follower_Attentive_Fusion_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose an audio-visual spatial-temporal deep neural network with: (1) a visual block containing a pretrained 2D-CNN followed by a temporal convolutional network (TCN); (2) an aural block containing several parallel TCNs; and (3) a leader-follower attentive fusion block combining the audio-visual information. The TCN with large history coverage enables our model to exploit spatial-temporal information within a much larger window length (i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36 or 48). The fusion block emphasizes the visual modality while exploits the noisy aural modality using the inter-modality attention mechanism. To make full use of the data and alleviate over-fitting, the cross-validation is carried out on the training and validation set. The concordance correlation coefficient (CCC) centering is used to merge the results from each fold. On the test (development) set of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence and 0.492 (0.649) for arousal, which significantly outperforms the baseline method with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for valence and arousal, respectively. The code will be published upon the acceptance of the paper.",
    "code_link": "https://github.com/sucv/ABAW2"
  },
  "iccv2021_abaw_mtmsnmulti-taskandmulti-modalsequencenetworkforfacialactionunitandexpressionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "MTMSN: Multi-Task and Multi-Modal Sequence Network for Facial Action Unit and Expression Recognition",
    "authors": [
      "Yue Jin",
      "Tianqing Zheng",
      "Chao Gao",
      "Guoqiang Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Jin_MTMSN_Multi-Task_and_Multi-Modal_Sequence_Network_for_Facial_Action_Unit_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Jin_MTMSN_Multi-Task_and_Multi-Modal_Sequence_Network_for_Facial_Action_Unit_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Facial action unit (AU) and basic expression recognition are two basic tasks in the area of human affective behavior analysis. Most of the existing methods are eveloped in restricted scenarios which are not practical for in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest provides a benchmark for this in-the-wild problem. In this paper, we propose a multi-task and multi-modal sequence network (MTMSN) to mine the relationships between the above two different tasks and effectively utilize both visual and audio information of the video. We use both AU and expression annotations to train the model and apply a sequence model to further extract associations between video frames. We achieve an AU score of 0.7508 and an expression score of 0.7574 on the validation set.",
    "code_link": "https://github.com/deepinsight/insightface"
  },
  "iccv2021_abaw_publiclifeinpublicspace(plps)amulti-task,multi-groupvideodatasetforpublicliferesearch": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Public Life in Public Space (PLPS): A Multi-Task, Multi-Group Video Dataset for Public Life Research",
    "authors": [
      "Linbo Qing",
      "Lindong Li",
      "Shengyu Xu",
      "Yibo Huang",
      "Mei Liu",
      "Rulong Jin",
      "Bo Liu",
      "Tong Niu",
      "Hongqian Wen",
      "Yuchen Wang",
      "Xue Jiang",
      "Yonghong Peng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Qing_Public_Life_in_Public_Space_PLPS_A_Multi-Task_Multi-Group_Video_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Qing_Public_Life_in_Public_Space_PLPS_A_Multi-Task_Multi-Group_Video_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Human-computer interaction (HCI) is a multidisciplinary field of study focusing on the design of computer technology and, in particular, the interactions between humans and computers. Public space, between the urban buildings, is an open and accessible area to people. Public life, happening in public spaces, is about human activity, human interaction, expression of human feeling in the wild. Affective behavior analysis in the public space is the basic topic of the public life research, which is the key to achieve HCI applications through comprehensively understanding people's feelings, emotions, social behaviors and their correlations in a `human-centered' and engaging manner. However, it is a challenging task to design a robust HCI system due to the lack of multi-task datasets (including emotion, behavior, social relations, etc), collected under the uncontrolled conditions in real public spaces. In spite that existing separate datasets in computer vision can somehow meet the requirement of public life research, they are neither captured from real public spaces nor for multiple tasks, which cannot comprehensively support the joint research of public life. To tackle this issue, this paper presents a multi-task, multi-group human-oriented video dataset, namely public life in public space (PLPS). Specifically, multi-tasks in terms of activity recognition, emotion recognition and social relation recognition are integrated for each video data. Multi-group and multi-level labels in terms of individuals, groups, video clips are included in the dataset. With PLPS, more sophisticated computer vision model for comprehensive public life research can be facilitated.",
    "code_link": ""
  },
  "iccv2021_abaw_anaudiovisualandcontextualapproachforcategoricalandcontinuousemotionrecognitionin-the-wild": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "An Audiovisual and Contextual Approach for Categorical and Continuous Emotion Recognition In-the-Wild",
    "authors": [
      "Panagiotis Antoniadis",
      "Ioannis Pikoulis",
      "Panagiotis P. Filntisis",
      "Petros Maragos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Antoniadis_An_Audiovisual_and_Contextual_Approach_for_Categorical_and_Continuous_Emotion_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Antoniadis_An_Audiovisual_and_Contextual_Approach_for_Categorical_and_Continuous_Emotion_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this work we tackle the task of video-based audio-visual emotion recognition, within the premises of the 2nd Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions, head/body orientation and low image resolution constitute factors that can potentially hinder performance in case of methodologies that solely rely on the extraction and analysis of facial features. In order to alleviate this problem, we leverage both bodily and contextual features, as part of a broader emotion recognition framework. We choose to use a standard CNN-RNN cascade as the backbone of our proposed model for sequence-to-sequence (seq2seq) learning. Apart from learning through the RGB input modality, we construct an aural stream which operates on sequences of extracted mel-spectrograms. Our extensive experiments on the challenging and newly assembled Aff-Wild2 dataset verify the validity of our intuitive multi-stream and multi-modal approach towards emotion recognition in-the-wild. Emphasis is being laid on the the beneficial influence of the human body and scene context, as aspects of the emotion recognition process that have been left relatively unexplored up to this point. All the code was implemented using PyTorch and is publicly available.",
    "code_link": "https://github.com/CSAILVision/places365"
  },
  "iccv2021_abaw_emotionrecognitionwithsequentialmulti-tasklearningtechnique": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Emotion Recognition With Sequential Multi-Task Learning Technique",
    "authors": [
      "Phan Tran Dac Thinh",
      "Hoang Manh Hung",
      "Hyung-Jeong Yang",
      "Soo-Hyung Kim",
      "Guee-Sang Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Thinh_Emotion_Recognition_With_Sequential_Multi-Task_Learning_Technique_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Thinh_Emotion_Recognition_With_Sequential_Multi-Task_Learning_Technique_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The task of predicting affective information in the wild such as seven basic emotions or action units from human faces has gradually become more interesting due to the accessibility and availability of massive annotated datasets. In this study, we propose a method that utilizes the association between seven basic emotions and twelve action units from the AffWild2 dataset. The method based on the architecture of ResNet50 involves the multi-task learning technique for the incomplete labels of the two tasks. By combining the knowledge for two correlated tasks, both performances are improved by a large margin compared to those with the model employing only one kind of label.",
    "code_link": ""
  },
  "iccv2021_abaw_evaluatingtheperformanceofensemblemethodsandvotingstrategiesfordense2dpedestriandetectioninthewild": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Evaluating the Performance of Ensemble Methods and Voting Strategies for Dense 2D Pedestrian Detection in the Wild",
    "authors": [
      "Aboli Marathe",
      "Rahee Walambe",
      "Ketan Kotecha"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Marathe_Evaluating_the_Performance_of_Ensemble_Methods_and_Voting_Strategies_for_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Marathe_Evaluating_the_Performance_of_Ensemble_Methods_and_Voting_Strategies_for_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "As vehicles experience a wide variety of driving settings in the wild, 2D pedestrian detection offers a substantial barrier to autonomous vehicle navigation systems. In this work, we demonstrate the effectiveness of a lightweight ensemble architecture for pedestrian detection in the wild, which combines detectors and data augmentation techniques to improve the performance of well-established detectors. The framework uses voting strategies to increase the explainability of object detection in navigation systems while also improving the precision of bounding box predictions on the dataset. The ensemble of the best model and augmentation technique achieved 41.41 % AP in detecting pedestrians in the wild using the consensus voting strategy on the WiderPerson dataset.",
    "code_link": ""
  },
  "iccv2021_abaw_multitaskmulti-databaseemotionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Multitask Multi-Database Emotion Recognition",
    "authors": [
      "Manh Tu Vu",
      "Marie Beurton-Aimar",
      "Serge Marchand"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Vu_Multitask_Multi-Database_Emotion_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Vu_Multitask_Multi-Database_Emotion_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This work has been initiated for the 2nd Affective Behavior Analysis in-the-wild (ABAW 2021) competition. We train a unified deep learning model on multi-databases to perform two tasks: seven basic facial expressions prediction and valence-arousal estimation. Since these databases do not contain labels for all the two tasks, we have applied the distillation knowledge technique to train two networks: one teacher and one student model. Both of these models are based on CNN-RNN hybrid architecture. The student model will be trained using both ground truth labels and soft labels derived from the pretrained teacher model. During the training, we have added one more task, which is the combination of the two mentioned tasks, for better exploiting inter-task correlations. We also exploit the sharing videos between the two tasks of the AffWild2 database that is used in the competition for further improving the performance of the network. Experiment results show that with these improvements, our model has reached the performance on par with the state of the art on the test set of the competition. Code and pretrained model are publicly available at https://github.com/glmanhtu/multitask-abaw-2021",
    "code_link": ""
  },
  "iccv2021_abaw_prioraidedstreamingnetworkformulti-taskaffectiveanalysis": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Prior Aided Streaming Network for Multi-Task Affective Analysis",
    "authors": [
      "Wei Zhang",
      "Zunhu Guo",
      "Keyu Chen",
      "Lincheng Li",
      "Zhimeng Zhang",
      "Yu Ding",
      "Runze Wu",
      "Tangjie Lv",
      "Changjie Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Zhang_Prior_Aided_Streaming_Network_for_Multi-Task_Affective_Analysis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Zhang_Prior_Aided_Streaming_Network_for_Multi-Task_Affective_Analysis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Automatic affective recognition has been an important research topic in the human-computer interaction (HCI) area. With the recent development of deep learning techniques and large-scale in-the-wild annotated datasets, facial emotion analysis is now aimed at challenges in real world settings. In this paper, we introduce our submission to the 2nd Affective Behavior Analysis in-the-wild (ABAW2) Competition. In dealing with different emotion representations, including Categorical Expression (EXPR), Action Units (AU), and Valence Arousal (VA), we propose a multitask streaming network by a heuristic that the three representations are intrinsically associated with each other. Besides, we leverage an advanced facial expression embedding model as prior knowledge, which is capable of capturing identity-invariant expression features while preserving the expression similarities, to aid the down-streaming recognition tasks. In order to enhance the generalization ability of our model, we generate reliable pseudo labels for unsupervised training and adopt external datasets for fine-tuning. In the official test of ABAW2 Competition, our method ranks first in the EXPR and AU tracks and second in the VA track. The extensive quantitative evaluations, as well as ablation studies on the Aff-Wild2 dataset, prove the effectiveness of our proposed method.",
    "code_link": ""
  },
  "iccv2021_abaw_noisyannotationsrobustconsensualcollaborativeaffectexpressionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Noisy Annotations Robust Consensual Collaborative Affect Expression Recognition",
    "authors": [
      "Darshan Gera",
      "S. Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Gera_Noisy_Annotations_Robust_Consensual_Collaborative_Affect_Expression_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Gera_Noisy_Annotations_Robust_Consensual_Collaborative_Affect_Expression_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Noisy annotation of large scale facial expression datasets has been a key challenge towards Facial Expression Recognition (FER) in the wild via deep learning. During early learning stage, deep networks fit on clean data and then eventually start overfitting on noisy labels due to their memorization ability which limits FER performance. To overcome this challenge on Aff-Wild2, this paper uses a robust end-to-end Consensual Collaborative Training (CCT) framework. CCT co-trains three networks jointly using a convex combination of supervision loss and consistency loss. A dynamic balancing scheme is used to transition from supervision loss in the initial learning to consistency loss during the later stage. During the initial training, supervision loss is given higher weight thus implicitly learning from clean samples. As the training progresses, consistency loss based on the consensus of predictions among different networks is used to effectively learn from all the samples, thus preventing overfitting to noisy annotated samples. Further, CCT does not make any assumption about the noise rate. Effectiveness of CCT is demonstrated on challenging Aff-Wild2 dataset using various quantitative evaluations and various ablation studies.",
    "code_link": "https://github.com/ZhaoJ9014/face.evoLVe.PyTorch"
  },
  "iccv2021_abaw_iterativedistillationforbetteruncertaintyestimatesinmultitaskemotionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Iterative Distillation for Better Uncertainty Estimates in Multitask Emotion Recognition",
    "authors": [
      "Didan Deng",
      "Liang Wu",
      "Bertram E. Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Deng_Iterative_Distillation_for_Better_Uncertainty_Estimates_in_Multitask_Emotion_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Deng_Iterative_Distillation_for_Better_Uncertainty_Estimates_in_Multitask_Emotion_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "When recognizing emotions, subtle nuances in displays of emotion generate ambiguity or uncertainty in emotion perception. Emotion uncertainty has been previously interpreted as inter-rater disagreement among multiple annotators. In this paper, we consider a more common and challenging scenario: modeling emotion uncertainty when only single emotion labels are available. From a Bayesian perspective, we propose to use deep ensembles to capture uncertainty for multiple emotion descriptors, i.e., action units, discrete expression labels and continuous descriptors. We further apply iterative self-distillation. Iterative distillation over multiple generations significantly improves performance in both emotion recognition and uncertainty estimation. Our method generates single student models that provide accurate estimates of uncertainty for in-domain samples and a student ensemble that can detect out-of-domain samples. Our experiments on emotion recognition and uncertainty estimation using the Aff-wild2 dataset demonstrate that our algorithm gives more reliable uncertainty estimates than both Temperature Scaling and Monte Carol Dropout.",
    "code_link": ""
  },
  "iccv2021_abaw_analysingaffectivebehaviorinthesecondabaw2competition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Analysing Affective Behavior in the Second ABAW2 Competition",
    "authors": [
      "Dimitrios Kollias",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Kollias_Analysing_Affective_Behavior_in_the_Second_ABAW2_Competition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Kollias_Analysing_Affective_Behavior_in_the_Second_ABAW2_Competition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The Affective Behavior Analysis in-the-wild (ABAW2) 2021 Competition is the second Competition -following the first very successful ABAW Competition held in conjunction with IEEE Conference on Face and Gesture Recognition 2020- that aims at automatically analyzing affect. ABAW2 is split into three Challenges, each one addressing one of the three main behavior tasks of Valence-Arousal Estimation, Seven Basic Expression Classification and Twelve Action Unit Detection. All three Challenges are based on a common benchmark database, Aff-Wild2, which is a large scale in-the-wild database and the first one to be annotated for all these three tasks. In this paper, we describe this Competition, to be held in conjunction with the International Conference on Computer Vision (ICCV) 2021. We present the three Challenges, with the utilized Competition corpora. We outline the evaluation metrics and present both the baseline systems and the top-5 performing teams' per Challenge; finally we present the obtained results of the baseline systems and of all participating teams. More information regarding the Competition, the leaderboard of each Challenge and details for accessing the utilized database, are provided in the Competition website: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/.",
    "code_link": ""
  },
  "iccv2021_abaw_studentengagementdataset": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Student Engagement Dataset",
    "authors": [
      "Kevin Delgado",
      "Juan Manuel Origgi",
      "Tania Hasanpoor",
      "Hao Yu",
      "Danielle Allessio",
      "Ivon Arroyo",
      "William Lee",
      "Margrit Betke",
      "Beverly Woolf",
      "Sarah Adel Bargal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Delgado_Student_Engagement_Dataset_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Delgado_Student_Engagement_Dataset_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "A major challenge for online learning is the inability of systems to support student emotion and to maintain student engagement. In response to this challenge, computer vision has become an embedded feature in some instructional applications. In this paper, we propose a video dataset of college students solving math problems on the educational platform MathSpring.org with a front facing camera collecting visual feedback of student gestures. The video dataset is annotated to indicate whether students' attention at specific frames is engaged or wandering. In addition, we train baselines for a computer vision module that determines the extent of student engagement during remote learning. Baselines include state-of-the-art deep learning image classifiers and traditional conditional and logistic regression for head pose estimation. We then incorporate a gaze baseline into the MathSpring learning platform, and we are evaluating its performance with the currently implemented approach.",
    "code_link": ""
  },
  "iccv2021_abaw_fserdeepconvolutionalneuralnetworksforspeechemotionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition",
    "authors": [
      "Bonaventure F. P. Dossou",
      "Yeno K. S. Gbenou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Dossou_FSER_Deep_Convolutional_Neural_Networks_for_Speech_Emotion_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Dossou_FSER_Deep_Convolutional_Neural_Networks_for_Speech_Emotion_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Using mel-spectrograms over conventional MFCCs features, we assess the abilities of convolutional neural networks to accurately recognize and classify emotions from speech data. We introduce FSER, a speech emotion recognition model trained on four valid speech databases, achieving a high-classification accuracy of 95,05%, over 8 different emotion classes: anger, anxiety, calm, disgust, happiness, neutral, sadness, surprise. On each benchmark dataset, FSER outperforms the best models introduced so far, achieving a state-of-the-art performance. We show that FSER stays reliable, independently of the language, sex identity, and any other external factor. Additionally, we describe how FSER could potentially be used to improve mental and emotional health care and how our analysis and findings serve as guidelines and benchmarks for further works in the same direction.",
    "code_link": ""
  },
  "iccv2021_abaw_emotionrecognitionbasedonbodyandcontextfusioninthewild": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Emotion Recognition Based on Body and Context Fusion in the Wild",
    "authors": [
      "Yibo Huang",
      "Hongqian Wen",
      "Linbo Qing",
      "Rulong Jin",
      "Leiming Xiao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Huang_Emotion_Recognition_Based_on_Body_and_Context_Fusion_in_the_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Huang_Emotion_Recognition_Based_on_Body_and_Context_Fusion_in_the_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Emotion recognition in-the-wild under uncontrolled conditions is a challenge, because facial expression is often blurred or even missing in the public space, while the previous visual emotion recognition researches have mainly focused on facial expression. In this paper we present a learning-based algorithm for emotion recognition by utilizing posture and context information, aiming to realize emotion recognition based on video in the wild. The network is designed in a three-branch architecture, including three feature streams: body, skeleton and context streams. The three streams are then fused to predict dimensional emotion representation, valence, arousal, and dominance. In addition, a new Body and Context Emotions Dataset (BCEmotion) is captured in the wild and labeled to support the related research, to tackle the lack of datasets based on public space video including complete individuals with face blurs and occlusions. With the BCEmotion dataset, we trained the proposed model that jointly analyses body and context of videos to realize emotion recognition in the wild. Experimental results show that proposed method effectively integrates emotional information expressed by body and context, and has good generalization ability and applicability in public space video data.",
    "code_link": ""
  },
  "iccv2021_abaw_amulti-taskmeanteacherforsemi-supervisedfacialaffectivebehavioranalysis": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "A Multi-Task Mean Teacher for Semi-Supervised Facial Affective Behavior Analysis",
    "authors": [
      "Lingfeng Wang",
      "Shisen Wang",
      "Jin Qi",
      "Kenji Suzuki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/html/Wang_A_Multi-Task_Mean_Teacher_for_Semi-Supervised_Facial_Affective_Behavior_Analysis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/ABAW/papers/Wang_A_Multi-Task_Mean_Teacher_for_Semi-Supervised_Facial_Affective_Behavior_Analysis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Affective Behavior Analysis is an important part in human-computer interaction. Existing multi-task affective behavior recognition methods suffer from the problem of incomplete labeled datasets. To tackle this problem, this paper presents a semi-supervised model with a mean teacher framework to leverage additional unlabeled data. To be specific, a multi-task model is proposed to learn three different kinds of facial affective representations simultaneously. After that, the proposed model is assigned to be student and teacher networks. When training with unlabeled data, the teacher network is employed to predict pseudo labels for student network training, which allows it to learn from unlabeled data. Experimental results showed that our proposed method achieved much better performance than baseline model and ranked 4th in both competition track 1 and track 2, and 6th in track 3, which verifies that the proposed network can effectively learn from incomplete datasets.",
    "code_link": ""
  },
  "iccv2021_oceanvision_anomalydetectionforinsitumarineplanktonimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "Anomaly Detection for In Situ Marine Plankton Images",
    "authors": [
      "Yuchun Pu",
      "Zhenghui Feng",
      "Zhonglei Wang",
      "Zhenyu Yang",
      "Jianping Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Pu_Anomaly_Detection_for_In_Situ_Marine_Plankton_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Pu_Anomaly_Detection_for_In_Situ_Marine_Plankton_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Machine learning and deep learning algorithms have achieved great success in plankton image recognition, but most of them are proposed to deal with closed-set tasks, where the distribution of the test data is the same as the training one. In reality, however, we face the challenges of open-set tasks, which are also recognized as the anomaly detection problems. In these tasks, there often exist abnormal classes, which are not in the training set, and the final goal of anomaly detection is to detect the anomalies correctly so that the misclassification of them can be reduced. However, little attention has been paid to anomaly detection in marine related fields. In this paper, to help marine plankton observers to detect anomalies conveniently and efficiently, we propose an anomaly detection pipeline including both the training and the testing phases. The training phase includes two parts, the pre-training and the post-training. In the pre-training phase, we propose a new loss function to better detect the abnormal classes and classify the normal classes simultaneously, which incorporates the expected cross-entropy loss, the expected Kullback-Leibler divergence, and the Anchor loss. We conduct several experiments to show the efficacy of the proposed method and compare its performance with other competitors based on a newly released dataset of in situ marine plankton images. Numerical results show that the proposed method outperforms its competitors in terms of classification accuracy and other commonly used criteria.",
    "code_link": ""
  },
  "iccv2021_oceanvision_in-situjointlightandmediumestimationforunderwatercolorrestoration": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "In-Situ Joint Light and Medium Estimation for Underwater Color Restoration",
    "authors": [
      "David Nakath",
      "Mengkun She",
      "Yifan Song",
      "Kevin K\u00f6ser"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Nakath_In-Situ_Joint_Light_and_Medium_Estimation_for_Underwater_Color_Restoration_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Nakath_In-Situ_Joint_Light_and_Medium_Estimation_for_Underwater_Color_Restoration_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The majority of Earth's surface is situated in the deep sea and thus remains deprived of natural light. Such adverse underwater environments have to be explored with powerful camera-light systems. In order to restore the colors in images taken by such systems, we need to jointly estimate physically-meaningful optical parameters of the light as well as the water column. We thus propose an integrated in-situ estimation approach and a complementary surface texture recovery strategy, which also removes shadows as a by-product. As we operate in a scattering medium under inhomogeneous lighting conditions, the volumetric effects are difficult to capture in closed-form solutions. Hence, we leverage the latest progress in Monte Carlo-based differentiable ray tracing that becomes tractable through recent GPU RTX-hardware acceleration. Evaluations on synthetic data and in a water tank show that we can estimate physically meaningful parameters, which enables color restoration. The approaches could also be employed to other camera-light systems (AUV, robot, car, endoscope) operating either in the dark, in fog - or - underwater.",
    "code_link": "https://github.com/mitsuba-renderer/enoki"
  },
  "iccv2021_oceanvision_anewdeeplearningengineforcoralnet": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "A New Deep Learning Engine for CoralNet",
    "authors": [
      "Qimin Chen",
      "Oscar Beijbom",
      "Stephen Chan",
      "Jessica Bouwmeester",
      "David Kriegman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Chen_A_New_Deep_Learning_Engine_for_CoralNet_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Chen_A_New_Deep_Learning_Engine_for_CoralNet_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "CoralNet is a cloud-based website and platform for manual, semi-automatic and automatic analysis of coral reef images. Users access CoralNet through optimized web-based workflows for common tasks, and other systems can interface through API's. Today, marine scientists are widely using CoralNet, and nearly 3,000 registered users have uploaded 1,741,855 images from 2,040 distinct sources with over 65 million annotations. CoralNet is hosted on AWS, is free for users, and the code is open. In January 2021, we released CoralNet 1.0 which has a new machine learning engine. This paper provides an overview of that engine, and the process of choosing the particular architecture, its training, and a comparison to some of the most promising architectures. In a nutshell, CoralNet 1.0 uses transfer learning with an EfficientNet-B0 backbone that is trained on 16M labelled patches from benthic images and a hierarchical Multi-layer Perceptron classifier that is trained on source-specific labelled data. When evaluated on a holdout test set of 26 sources, the error rate of CoralNet 1.0 was 18.4% (relative) lower than CoralNet Beta.",
    "code_link": "https://github.com/beijbom/coralnet"
  },
  "iccv2021_oceanvision_underwatermarker-basedpose-estimationwithassociateduncertainty": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "Underwater Marker-Based Pose-Estimation With Associated Uncertainty",
    "authors": [
      "Petter Risholm",
      "Peter \u00d8rnulf Ivarsen",
      "Karl Henrik Haugholt",
      "Ahmed Mohammed"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Risholm_Underwater_Marker-Based_Pose-Estimation_With_Associated_Uncertainty_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Risholm_Underwater_Marker-Based_Pose-Estimation_With_Associated_Uncertainty_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a system for 6-DoF estimation of Aruco markers with associated uncertainties in the challenging underwater environment. A state-of-the-art object detection framework (EfficientDet) was adapted to predict the corner locations of Aruco markers, while dropout sampling at inference time is used to estimate the predictive 6-DoF pose uncertainty. A dataset of Aruco markers captured in a wide variety of turbidities, with ground truth position of the corner locations, was gathered and used to train the network to robustly predict the 6-DoF pose. We report median translational errors of 2.6cm at low turbidity (8.5m attenuation length) and up to 10.5cm at high turbidities (0.3m attenuation length). The respective uncertainty, reported as interquartile ranges (IQRs), range from 3.2cm up to 27.9cm. The rotational median errors varied from 5.6 (deg) to 10.7 (deg) with IQRs of 6.4 (deg) to 26.2 (deg). We also discuss how the pose uncertainty can be applied to reduce the risk in a subsea intervention operation.",
    "code_link": ""
  },
  "iccv2021_oceanvision_themarinedebrisdatasetforforward-lookingsonarsemanticsegmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "The Marine Debris Dataset for Forward-Looking Sonar Semantic Segmentation",
    "authors": [
      "Deepak Singh",
      "Matias Valdenegro-Toro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Singh_The_Marine_Debris_Dataset_for_Forward-Looking_Sonar_Semantic_Segmentation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Singh_The_Marine_Debris_Dataset_for_Forward-Looking_Sonar_Semantic_Segmentation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Accurate detection and segmentation of marine debris is important for keeping the water bodies clean. This paper presents a novel dataset for marine debris segmentation collected using a Forward Looking Sonar (FLS). The dataset consists of 1868 FLS images captured using ARIS Explorer 3000 sensor. The objects used to produce this dataset contain typical house-hold marine debris and distractor marine objects (tires, hooks, valves,etc), divided in 11 classes plus a background class. Performance of state of the art semantic segmentation architectures with a variety of encoders have been analyzed on this dataset and presented as baseline results. Since the images are grayscale, no pre-trained weights have been used. Comparisons are made using Intersection over Union (IoU). The best performing model is Unet with ResNet34 backbone at 0.7481 mIoU.",
    "code_link": ""
  },
  "iccv2021_oceanvision_super-resolutionforinsituplanktonimages": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "Super-Resolution for In Situ Plankton Images",
    "authors": [
      "Wenqi Ma",
      "Tao Chen",
      "Zhengwen Zhang",
      "Zhenyu Yang",
      "Chao Dong",
      "Jianping Qiao",
      "Jianping Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Ma_Super-Resolution_for_In_Situ_Plankton_Images_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Ma_Super-Resolution_for_In_Situ_Plankton_Images_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Being inherently limited by the wave properties of light, underwater plankton cameras compromise between their imaging resolution and field of view (FOV) for in situ observations. In order to enlarge the sampling volume in single frame acquisition, lower magnifications are usually adopted to enable larger FOV but sacrifice the resolution. In this paper, we build a real-underwater image dataset called IsPlanktonSR for in situ plankton image super-resolution (SR), in which paired low resolution (LR) and high resolution (HR) images of the same individual live planktonic organisms are captured by a customized dual-channel darkfield imaging system. An image registration algorithmic pipeline is also proposed to preprocess and align the image pairs at different scaling factors of 2x and 4x. The IsPlanktonSR dataset is used to train an enhanced deep residual network for SR through the L2, the perceptual and the contextual losses, respectively. Our extensive experimental results demonstrate that the deep learning model trained on real data through the contextual loss has delivered better visual and quantitative SR performance than those trained on simulated data or through other loss functions. The trained SR model is also proved to generalize well to images of various plankton species or captured by different instruments. The proposed SR technology is anticipated to enhance the existing darkfield plankton imageries and enable the future in situ plankton imaging instruments for better observation capability and hence deepen understanding of the plankton ecology.",
    "code_link": ""
  },
  "iccv2021_oceanvision_thevarossyntheticunderwaterdatasettowardsrealisticmulti-sensorunderwaterdatawithgroundtruth": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "The VAROS Synthetic Underwater Data Set: Towards Realistic Multi-Sensor Underwater Data With Ground Truth",
    "authors": [
      "Peder Georg Olofsson Zwilgmeyer",
      "Mauhing Yip",
      "Andreas Langeland Teigen",
      "Rudolf Mester",
      "Annette Stahl"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Zwilgmeyer_The_VAROS_Synthetic_Underwater_Data_Set_Towards_Realistic_Multi-Sensor_Underwater_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Zwilgmeyer_The_VAROS_Synthetic_Underwater_Data_Set_Towards_Realistic_Multi-Sensor_Underwater_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Underwater visual perception requires being able to deal with bad and rapidly varying illumination and with reduced visibility due to water turbidity. The verification of such algorithms is crucial for safe and efficient underwater exploration and intervention operations. Ground truth data play an important role in evaluating vision algorithms. However, obtaining ground truth from real underwater environments is in general very hard, if possible at all. In a synthetic underwater 3D environment, however, (nearly) all parameters are known and controllable, and ground truth data can be absolutely accurate in terms of geometry. In this paper, we present the VAROS environment, our approach to generating highly realistic underwater video and auxiliary sensor data with precise ground truth, built around the Blender modeling and rendering environment. VAROS allows for physically realistic motion of the simulated underwater (UW) vehicle including moving illumination. Pose sequences are created by first defining way-points for the simulated underwater vehicle which are expanded into a smooth vehicle course sampled at IMU data rate (200Hz). This expansion uses a vehicle dynamics model and a discrete-time controller algorithm that simulates the sequential following of the way-points. The scenes are rendered using the raytracing method, which generates realistic images, integrating direct light, and indirect volumetric scattering. The VAROS dataset version 1 provides images, inertial measurement unit (IMU) and depth gauge data, as well as ground truth poses, depth images and surface normal images.",
    "code_link": ""
  },
  "iccv2021_oceanvision_improvingrare-classrecognitionofmarineplanktonwithhardnegativemining": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "Improving Rare-Class Recognition of Marine Plankton With Hard Negative Mining",
    "authors": [
      "Joseph L. Walker",
      "Eric C. Orenstein"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Walker_Improving_Rare-Class_Recognition_of_Marine_Plankton_With_Hard_Negative_Mining_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Walker_Improving_Rare-Class_Recognition_of_Marine_Plankton_With_Hard_Negative_Mining_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Biological oceanographers are increasingly adopting machine learning techniques to conduct quantitative assessments of marine plankton. Most supervised plankton classifiers are trained on labeled image datasets annotated by domain experts under the closed world assumption: all object classes and their priors are the same during both training and deployment. This assumption, however, is hard to satisfy in the actual ocean where data is subject to dataset shift due to shifting populations and from the introduction of object categories not seen during training. Here we present an alternative approach for training and evaluating plankton classifiers under the more realistic open world scenario. We specifically address the problems of out-of-distribution detection and dataset shift under the class imbalance setting where downsampling is needed to reliably detect and classify relatively rare target classes. We apply a hard negative mining approach called Background Resampling to perform downsampling and compare it to other strategies. We show that Background Resampling improves detection of novel particle classes while simultaneously providing competitive classification performance under dataset shift.",
    "code_link": ""
  },
  "iccv2021_oceanvision_hyperspectral3dmappingofunderwaterenvironments": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OceanVision",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision in the Ocean",
    "title": "Hyperspectral 3D Mapping of Underwater Environments",
    "authors": [
      "Maxime Ferrera",
      "Aur\u00e9lien Arnaubec",
      "Klemen Isteni\u010d",
      "Nuno Gracias",
      "Touria Bajjouk"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/html/Ferrera_Hyperspectral_3D_Mapping_of_Underwater_Environments_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OceanVision/papers/Ferrera_Hyperspectral_3D_Mapping_of_Underwater_Environments_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Hyperspectral imaging has been increasingly used for underwater survey applications over the past years. As many hyperspectral cameras work as push-broom scanners, their use is usually limited to the creation of photo-mosaics based on a flat surface approximation and by interpolating the camera pose from dead-reckoning navigation. Yet, because of drift in the navigation and the mostly wrong flat surface assumption, the quality of the obtained photo-mosaics is often too low to support adequate analysis. In this paper we present an initial method for creating hyperspectral 3D reconstructions of underwater environments. By fusing the data gathered by a classical RGB camera, an inertial navigation system and a hyperspectral push-broom camera, we show that the proposed method creates highly accurate 3D reconstructions with hyperspectral textures. We propose to combine techniques from simultaneous localization and mapping, structure-from-motion and 3D reconstruction and advantageously use them to create 3D models with hyperspectral texture, allowing us to overcome the flat surface assumption and the classical limitation of dead-reckoning navigation.",
    "code_link": ""
  },
  "iccv2021_openeds_simplebaselinescanfool360degsaliencymetrics": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OpenEDS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Eye Tracking for AR/VR: Sensors and Applications",
    "title": "Simple Baselines Can Fool 360deg Saliency Metrics",
    "authors": [
      "Yasser Abdelaziz Dahou Djilali",
      "Kevin McGuinness",
      "Noel E. O'Connor"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OpenEDS/html/Djilali_Simple_Baselines_Can_Fool_360deg_Saliency_Metrics_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OpenEDS/papers/Djilali_Simple_Baselines_Can_Fool_360deg_Saliency_Metrics_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Evaluating a model's capacity to predict human fixations in 360deg scenes is a challenging task. 360deg saliency requires different assumptions compared to 2D as a result of the way the saliency maps are collected and pre-processed to account for the difference in statistical bias (Equator vs Center bias). However, the same classical metrics from the 2D saliency literature are typically used to evaluate 360deg models. In this paper, we show that a simple constant predictor, i.e. the average map across Salient360 and Sitzman training sets can fool existing metrics and achieve results on par with specialized models. Thus, we propose a new probabilistic metric based on the independent Bernoullis assumption that is more suited to the 360deg saliency task.",
    "code_link": ""
  },
  "iccv2021_rprmi_towardssolvingthedeepfakeproblemananalysisonimprovingdeepfakedetectionusingdynamicfaceaugmentation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RPRMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Responsible Pattern Recognition and Machine Intelligence",
    "title": "Towards Solving the DeepFake Problem: An Analysis on Improving DeepFake Detection Using Dynamic Face Augmentation",
    "authors": [
      "Sowmen Das",
      "Selim Seferbekov",
      "Arup Datta",
      "Md. Saiful Islam",
      "Md. Ruhul Amin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/html/Das_Towards_Solving_the_DeepFake_Problem_An_Analysis_on_Improving_DeepFake_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/papers/Das_Towards_Solving_the_DeepFake_Problem_An_Analysis_on_Improving_DeepFake_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we focus on identifying the limitations and shortcomings of existing deepfake detection frameworks. We identified some key problems surrounding deepfake detection through quantitative and qualitative analysis of existing methods and datasets. We found that deepfake datasets are highly oversampled, causing models to become easily overfitted. The datasets are created using a small set of real faces to generate multiple fake samples. When models are trained on these datasets, they tend to memorize the actors' faces and labels instead of learning fake features. To mitigate this problem, we propose a simple data augmentation method termed Face-Cutout. Our method dynamically cuts out regions of an image using the face landmark information. This helps the model to selectively attend to only the relevant regions of the input. Our evaluation experiments show that Face-Cutout can successfully improve the data variation and alleviate the problem of overfitting. Our method achieves a reduction in LogLoss of 15.2% to 35.3% on different datasets, compared to other occlusion-based techniques. Moreover, we also propose a general-purpose data pre-processing guideline to train and evaluate existing architectures allowing us to improve the generalizability of these models for deepfake detection.",
    "code_link": ""
  },
  "iccv2021_rprmi_thewatchlistimbalanceeffectinbiometricfaceidentificationcomparingtheoreticalestimatesandempiricmeasurements": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RPRMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Responsible Pattern Recognition and Machine Intelligence",
    "title": "The Watchlist Imbalance Effect in Biometric Face Identification: Comparing Theoretical Estimates and Empiric Measurements",
    "authors": [
      "Pawel Drozdowski",
      "Christian Rathgeb",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/html/Drozdowski_The_Watchlist_Imbalance_Effect_in_Biometric_Face_Identification_Comparing_Theoretical_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/papers/Drozdowski_The_Watchlist_Imbalance_Effect_in_Biometric_Face_Identification_Comparing_Theoretical_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Recently, different research groups have found that the gallery composition of a face database can induce performance differentials to facial identification systems in which a probe image is compared against up to all stored reference images to reach a biometric decision. This negative effect has been referred to as \"\"watchlist imbalance effect by the researchers and exhibits high relevance in real applications of biometrics, most prominently in identification searches against criminal databases and blacklists. In this work, we conduct a detailed analysis of said effect. In particular, we compare empiric observations with theoretical estimates, based on the verification performance across demographic groups and the composition of the used gallery. The experimental evaluations are conducted by systematically varying the size and demographic composition of a cleaned subset of the academic MORPH database and utilising the state-of-the-art open-source ArcFace face recognition system.",
    "code_link": "https://github.com/deepinsight/insightface"
  },
  "iccv2021_rprmi_bridgingthegapbetweendebiasingandprivacyfordeeplearning": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RPRMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Responsible Pattern Recognition and Machine Intelligence",
    "title": "Bridging the Gap Between Debiasing and Privacy for Deep Learning",
    "authors": [
      "Carlo Alberto Barbano",
      "Enzo Tartaglione",
      "Marco Grangetto"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/html/Barbano_Bridging_the_Gap_Between_Debiasing_and_Privacy_for_Deep_Learning_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/papers/Barbano_Bridging_the_Gap_Between_Debiasing_and_Privacy_for_Deep_Learning_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The broad availability of computational resources and the recent scientific progresses made deep learning the elected class of algorithms to solve complex tasks.Besides their deployment, two problems have risen: fighting biases in data and privacy preservation of sensitive attributes. Many solutions have been proposed, some of which deepen their roots in the pre-deep learning theory. There are many similarities between debiasing and privacy preserving approaches: how far apart are these two worlds, when the private information overlaps the bias? In this work we investigate the possibility of deploying debiasing strategies also to prevent privacy leakage. In particular, empirically testing on state-of-the-art datasets, we observe that there exists a subset of debiasing approaches which are also suitable for privacy preservation. We identify as the discrimen the capability of effectively hiding the biased information, rather than simply re-weighting it.",
    "code_link": ""
  },
  "iccv2021_rprmi_xaihandbooktowardsaunifiedframeworkforexplainableai": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RPRMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Responsible Pattern Recognition and Machine Intelligence",
    "title": "XAI Handbook: Towards a Unified Framework for Explainable AI",
    "authors": [
      "Sebastian Palacio",
      "Adriano Lucieri",
      "Mohsin Munir",
      "Sheraz Ahmed",
      "J\u00f6rn Hees",
      "Andreas Dengel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/html/Palacio_XAI_Handbook_Towards_a_Unified_Framework_for_Explainable_AI_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/papers/Palacio_XAI_Handbook_Towards_a_Unified_Framework_for_Explainable_AI_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The field of explainable AI (XAI) has quickly become a thriving and prolific community. However, a silent, recurrent and acknowledged issue in this area is the lack of consensus regarding its terminology. In particular, each new contribution seems to rely on its own (and often intuitive) version of terms like \"\"explanation\"\" and \"\"interpretation\"\". Such disarray encumbers the consolidation of advances in the field towards the fulfillment of scientific and regulatory demands e.g., when comparing methods or establishing their compliance w.r.t. biases and fairness constraints. We propose a theoretical framework that not only provides concrete definitions for these terms, but it also outlines all steps necessary to produce explanations and interpretations. The framework also allows for existing contributions to be re-contextualized such that their scope can be measured, thus making them comparable to other methods. We show that this framework is compliant with desiderata on explanations, on interpretability and on evaluation metrics. We present a use-case showing how the framework can be used to compare LIME, SHAP and MDNet, establishing their advantages and shortcomings. Finally, we discuss relevant trends in XAI as well as recommendations for future work, all from the standpoint of our framework.",
    "code_link": ""
  },
  "iccv2021_rprmi_unravellingtheeffectofimagedistortionsforbiasedpredictionofpre-trainedfacerecognitionmodels": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RPRMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Responsible Pattern Recognition and Machine Intelligence",
    "title": "Unravelling the Effect of Image Distortions for Biased Prediction of Pre-Trained Face Recognition Models",
    "authors": [
      "Puspita Majumdar",
      "Surbhi Mittal",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/html/Majumdar_Unravelling_the_Effect_of_Image_Distortions_for_Biased_Prediction_of_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/papers/Majumdar_Unravelling_the_Effect_of_Image_Distortions_for_Biased_Prediction_of_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Identifying and mitigating bias in deep learning algorithms has gained significant popularity in the past few years due to its impact on the society. Researchers argue that models trained on balanced datasets with good representation provide equal and unbiased performance across subgroups. However, can seemingly unbiased pre-trained model become biased when input data undergoes certain distortions? For the first time, we attempt to answer this question in the context of face recognition. We provide a systematic analysis to evaluate the performance of four state-of-the-art deep face recognition models in the presence of image distortions across different gender and race subgroups. We have observed that image distortions have a relationship with the performance gap of the model across different subgroups.",
    "code_link": "https://github.com/Puspitamajumdariiit/Unravelling-the-Effect-of-ImageDistortions"
  },
  "iccv2021_rprmi_towardaffectivexaifacialaffectanalysisforunderstandingexplainablehuman-aiinteractions": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RPRMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Responsible Pattern Recognition and Machine Intelligence",
    "title": "Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions",
    "authors": [
      "Luke Guerdan",
      "Alex Raymond",
      "Hatice Gunes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/html/Guerdan_Toward_Affective_XAI_Facial_Affect_Analysis_for_Understanding_Explainable_Human-AI_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RPRMI/papers/Guerdan_Toward_Affective_XAI_Facial_Affect_Analysis_for_Understanding_Explainable_Human-AI_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "As machine learning approaches are increasingly used to augment human decision-making, eXplainable Artificial Intelligence (XAI) research has explored methods for communicating system behavior to humans. However, these approaches often fail to account for the affective responses of humans as they interact with explanations. Facial affect analysis, which examines human facial expressions of emotions, is one promising lens for understanding how users engage with explanations. Therefore, in this work, we aim to (1) identify which facial affect features are pronounced when people interact with XAI interfaces, and (2) develop a multitask feature embedding for linking facial affect signals with participants' use of explanations. Our analyses and results show that the occurrence and values of facial AU1 and AU4, and Arousal are heightened when participants fail to use explanations effectively. This suggests that facial affect analysis should be incorporated into XAI to personalize explanations to individuals' interaction styles and to adapt explanations based on the difficulty of the task performed.",
    "code_link": ""
  },
  "iccv2021_aotw_theaircraftcontextdatasetunderstandingandoptimizingdatavariabilityinaerialdomains": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AOTW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Airborne Object Tracking",
    "title": "The Aircraft Context Dataset: Understanding and Optimizing Data Variability in Aerial Domains",
    "authors": [
      "Daniel Steininger",
      "Verena Widhalm",
      "Julia Simon",
      "Andreas Kriegler",
      "Christoph Sulzbachner"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AOTW/html/Steininger_The_Aircraft_Context_Dataset_Understanding_and_Optimizing_Data_Variability_in_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AOTW/papers/Steininger_The_Aircraft_Context_Dataset_Understanding_and_Optimizing_Data_Variability_in_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Despite their increasing demand for assistant and autonomous systems, the recent shift towards data-driven approaches has hardly reached aerial domains, partly due to a lack of specific training and test data. We introduce the Aircraft Context Dataset, a composition of two inter-compatible large-scale and versatile image datasets focusing on manned aircraft and UAVs, respectively. In addition to fine-grained annotations for multiple learning tasks, we define and apply a set of relevant meta-parameters and showcase their potential to quantify dataset variability as well as the impact of environmental conditions on model performance. Baseline experiments are conducted for detection, classification and semantic labeling on multiple dataset variants. Their evaluation clearly shows that our contribution is an essential step towards overcoming the data gap and that the proposed variability concept significantly increases the efficiency of specializing models as well as continuously and purposefully extending the dataset.",
    "code_link": "https://github.com/aircraftcontext/aircraft-context-dataset"
  },
  "iccv2021_aotw_leveragingtemporalinformationfor3dtrajectoryestimationofspaceobjects": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "AOTW",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Airborne Object Tracking",
    "title": "Leveraging Temporal Information for 3D Trajectory Estimation of Space Objects",
    "authors": [
      "Mohamed Adel Musallam",
      "Miguel Ortiz del Castillo",
      "Kassem Al Ismaeil",
      "Marcos Damian Perez",
      "Djamila Aouada"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/AOTW/html/Musallam_Leveraging_Temporal_Information_for_3D_Trajectory_Estimation_of_Space_Objects_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/AOTW/papers/Musallam_Leveraging_Temporal_Information_for_3D_Trajectory_Estimation_of_Space_Objects_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This work presents a new temporally consistent space object 3D trajectory estimation from a video taken by a single RGB camera. Understanding space objects' trajectories is an important component of Space Situational Awareness, especially for applications such as Active Debris Removal, On-orbit Servicing, and Orbital Maneuvers. Using only the information from a single image perspective gives temporally inconsistent 3D position estimation. Our approach operates in two subsequent stages. The first stage estimates the 2D location of the space object using a convolution neural network. In the next stage, the 2D locations are lifted to 3D space, using a temporal convolution neural network that enforces the temporal coherence over the estimated 3D locations. Our results show that leveraging temporal information yields smooth and accurate 3D trajectory estimations for space objects. A dedicated large realistic synthetic dataset for 3 spacecraft, under various sensing conditions, is also proposed and will be publicly shared with the research community.",
    "code_link": ""
  },
  "iccv2021_ovis_characterizingscatteredocclusionsforeffectivedense-modecrowdcounting": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OVIS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Occluded Video Instance Segmentation",
    "title": "Characterizing Scattered Occlusions for Effective Dense-Mode Crowd Counting",
    "authors": [
      "Khalid J Almalki",
      "Baek-Young Choi",
      "Yu Chen",
      "Sejun Song"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/html/Almalki_Characterizing_Scattered_Occlusions_for_Effective_Dense-Mode_Crowd_Counting_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/papers/Almalki_Characterizing_Scattered_Occlusions_for_Effective_Dense-Mode_Crowd_Counting_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a novel deep learning approach for effective dense crowd counting by characterizing scattered occlusions, named CSONet. CSONet recognizes the implications of event-induced, scene-embedded, and multitudinous obstacles such as umbrellas and picket signs to achieve an accurate crowd analysis result. CSONet is the first deep learning model for characterizing scattered occlusions of effective dense-mode crowd counting to the best of our knowledge. We have collected and annotated two new scattered occlusion object datasets, which contain crowd images occluded with umbrellas (cso-umbrellas dataset) and picket signs (cso-pickets dataset). We have designed and implemented a new crowd overfit reduction network by adding both spatial pyramid pooling and dilated convolution layers over modified VGG16 for capturing high-level features of extended receptive fields. CSONet was trained on the two new scattered occlusion datasets and the ShanghaiTech A and B datasets. We also have built an algorithm that merges scattered object maps and density heatmaps of visible humans to generate a more accurate crowd density heatmap output. Through extensive evaluations, we demonstrate that the accuracy of CSONet with scattered occlusion images outperforms over the state-of-art existing crowd counting approaches by 30% to 100% in both mean absolute error and mean square error.",
    "code_link": ""
  },
  "iccv2021_ovis_fromvistoovisatechnicalreporttopromotethedevelopmentofthefield": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OVIS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Occluded Video Instance Segmentation",
    "title": "From VIS to OVIS: A Technical Report To Promote the Development of the Field",
    "authors": [
      "Wenbo Li",
      "Xuesheng Li",
      "Qiwei Xu",
      "Chen Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/html/Li_From_VIS_to_OVIS_A_Technical_Report_To_Promote_the_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/papers/Li_From_VIS_to_OVIS_A_Technical_Report_To_Promote_the_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Occluded Video instance segmentation(OVIS) is a new vision task that has emerged in this years and is processed by video deep learning algorithms. It uses continuous video frames as input, generally ranging from a few frames to hundreds of frames. Before OVIS, there has a task called VIS. To tackle the task of OVIS and VIS, we design a new alghorithm called SimVTR, which based on DETR and VisTR. During the experiment, although we acquire the 27.66 mAP on OVIS test, 25.18m AP on OVIS val, and 31.9 mAP on VIS test, we have found a surprising phenomena that the evaluation mechanism is not sensitive to our mothod SimVTR. When we only use one frame to inference, the model can acquire the similar mAP as dozens frames. SimpleVTR trade off and optimizes the computing resources and effects of end-to-end video instance segmentation algorithm. We used one RTX1080Ti (11G) to experiment, and the batch size can change from 1 to 16 frames. We were surprised to find that only one frame can also get a very high score in inference. The VIS and OVIS cocoapi have some unreasonable place in ytvoseval.py. In this technical report, we prudently point out the phenomena that the evaluation mechanism could have some bug. If this is true, we need check our model to promote the process of the video instance segmentation.",
    "code_link": "https://github.com/arogozhnikov/einops"
  },
  "iccv2021_ovis_limitedsamplingreferenceframeformasktrackr-cnn": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OVIS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Occluded Video Instance Segmentation",
    "title": "Limited Sampling Reference Frame for MaskTrack R-CNN",
    "authors": [
      "Zhuang Li",
      "Leilei Cao",
      "Hongbin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/html/Li_Limited_Sampling_Reference_Frame_for_MaskTrack_R-CNN_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/papers/Li_Limited_Sampling_Reference_Frame_for_MaskTrack_R-CNN_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "With the great achievement for the computer vision tasks, e.g., image classification, object detection and segmenta- tion, people are diving into more complex vision tasks. Video instance segmentation is a new task which includes detection, segmentation and tracking of instances simulta- neously in a video. Occluded Video Instance Segmentation (OVIS) is used for this task, and it includes many heavily occluded scenes. Besides, there is a long range for the length of videos in this dataset. In order to track instances in videos with different lengths, we make some improvements based on MaskTrack R-CNN. Based on these optimizations, a refinement model can be well used to detect and segment instances, which acquires a better track accuracy in long videos. Furthermore, we apply Stochastic Weights Aver- aging training strategy to get a better result. Finally, The proposed method can achieve the mAP score of 28.9 for the validation set and 32.2 for the test set on the OVIS dataset.",
    "code_link": ""
  },
  "iccv2021_ovis_occludedvideoinstancesegmentationwithsetpredictionapproach": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OVIS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Occluded Video Instance Segmentation",
    "title": "Occluded Video Instance Segmentation With Set Prediction Approach",
    "authors": [
      "Heechul Bae",
      "Soonyong Song",
      "Junhee Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/html/Bae_Occluded_Video_Instance_Segmentation_With_Set_Prediction_Approach_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/papers/Bae_Occluded_Video_Instance_Segmentation_With_Set_Prediction_Approach_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Occluded Video Instance Segmentation (OVIS) is a multi-task problem performing detection, segmentation, and tracking simultaneously under severe occlusions. We propose an extended model for the OVIS task based on the real-time one-stage instance segmentation method. The proposed model was applied to the OVIS dataset hold by the ICCV 2021 - Occluded Video Instance Segmentation Workshop 2021. We also show that the occlusions can be handled efficiently through one-stage approaches.",
    "code_link": ""
  },
  "iccv2021_ovis_asingle-stage,bottom-upapproachforoccludedvisusingspatio-temporalembeddings": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OVIS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Occluded Video Instance Segmentation",
    "title": "A Single-Stage, Bottom-Up Approach for Occluded VIS Using Spatio-Temporal Embeddings",
    "authors": [
      "Ali Athar",
      "Sabarinath Mahadevan",
      "Aljos\u0306a Os\u0306ep",
      "Laura Leal-Taix\u00e9",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/html/Athar_A_Single-Stage_Bottom-Up_Approach_for_Occluded_VIS_Using_Spatio-Temporal_Embeddings_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/papers/Athar_A_Single-Stage_Bottom-Up_Approach_for_Occluded_VIS_Using_Spatio-Temporal_Embeddings_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The task of Video Instance Segmentation (VIS) involves segmenting, tracking and classifying all object instances present in a given video clip. Occluded VIS is a more challenging extension of this task which involves longer video sequences where objects undergo significant occlusions over time. Most existing approaches to VIS involve multiple networks which separately handle segmenting, tracking and classifying object instances, and potentially a set of heuristics to combine the individual network outputs. By contrast, we employ just one, single-stage network without any heuristics or post-processing for the end-to-end task. Our approach is called 'STEm-Seg', which is a bottom-up method for Segmenting object instances in videos using Spatio-Temporal Embeddings. We achieve 3rd place in the Occluded VIS challenge with an mAP score of 21.6% on the test set.",
    "code_link": ""
  },
  "iccv2021_ovis_pedestrianocclusionlevelclassificationusingkeypointdetectionand2dbodysurfaceareaestimation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "OVIS",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Occluded Video Instance Segmentation",
    "title": "Pedestrian Occlusion Level Classification Using Keypoint Detection and 2D Body Surface Area Estimation",
    "authors": [
      "Shane Gilroy",
      "Martin Glavin",
      "Edward Jones",
      "Darragh Mullins"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/html/Gilroy_Pedestrian_Occlusion_Level_Classification_Using_Keypoint_Detection_and_2D_Body_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/OVIS/papers/Gilroy_Pedestrian_Occlusion_Level_Classification_Using_Keypoint_Detection_and_2D_Body_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Effective and reliable pedestrian detection is among the most safety-critical features of semi-autonomous and autonomous vehicles. One of the most complex detection challenges is that of partial occlusion, where a target object is only partially available to the sensor due to obstruction by another foreground object. A number of current pedestrian detection benchmarks provide annotation for partial occlusion to assess algorithm performance in these scenarios, however each benchmark varies greatly in their definition of the occurrence and severity of occlusion. In addition, current occlusion level annotation methods contain a high degree of subjectivity by the human annotator. This can lead to inaccurate or inconsistent reporting of an algorithm's detection performance for partially occluded pedestrians, depending on which benchmark is used. This research presents a novel, objective method for pedestrian occlusion level classification for ground truth annotation. Occlusion level classification is achieved through the identification of visible pedestrian keypoints and through the use of a novel, effective method of 2D body surface area estimation. Experimental results demonstrate that the proposed method reflects the pixel-wise occlusion level of pedestrians in images and is effective for all forms of occlusion, including challenging edge cases such as self-occlusion, truncation and inter-occluding pedestrians.",
    "code_link": ""
  },
  "iccv2021_waami_analgorithmicapproachtoquantifyinggpstrajectoryerror": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "An Algorithmic Approach to Quantifying GPS Trajectory Error",
    "authors": [
      "Matthew Plaudis",
      "Muhammad Azam",
      "Derek Jacoby",
      "Marc-Antoine Drouin",
      "Yvonne Coady"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Plaudis_An_Algorithmic_Approach_to_Quantifying_GPS_Trajectory_Error_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Plaudis_An_Algorithmic_Approach_to_Quantifying_GPS_Trajectory_Error_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The alignment of aerial and satellite imagery with ground sensor data is an ongoing research challenge. In dense urban environments, part of this challenge is induced by the positioning error of Global Positioning System (GPS). Despite the potential for error, many studies use GPS in order to infer road networks because GPS data is inexpensive and can be acquired quickly. Major transit organizations are freely providing data on the real-time position of their buses as well as ground truth route trajectories. This work exploits geospatial open data to construct a database of historical GPS from bus roads. Using this database, the GPS error map along main arteries of major cities can be reconstructed. The extraction of error maps is highly relevant for the planning and the joint exploitation of airborne and ground-based imagery. In this work, we use bus routes in downtown Victoria, BC, Canada and Adelaide, Australia to demonstrate the extraction GPS error maps.",
    "code_link": ""
  },
  "iccv2021_waami_janusnetdetectionofmovingobjectsfromuavplatforms": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "JanusNet: Detection of Moving Objects From UAV Platforms",
    "authors": [
      "Yuxiang Zhao",
      "Khurram Shafique",
      "Zeeshan Rasheed",
      "Maoxu Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Zhao_JanusNet_Detection_of_Moving_Objects_From_UAV_Platforms_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Zhao_JanusNet_Detection_of_Moving_Objects_From_UAV_Platforms_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this paper, we present JanusNet, an efficient CNN model that can perform online background subtraction and robustly detect moving targets using resource-constrained computational hardware on-board unmanned aerial vehicles (UAVs). Most of the existing work on background subtraction either assume that the camera is stationary or make limiting assumptions about the motion of the camera, the structure of the scene under observation, or the apparent motion of the background in video. JanusNet does not have these limitations and therefore, is applicable to a variety of UAV applications. JanusNet learns to extract and combine motion and appearance features to separate background and foreground to generate accurate pixel-wise masks of the moving objects. The network is trained using a simulated video dataset (generated using Unreal Engine 4) with ground-truth labels. Results on UCF Aerial and Kaggle Drone videos datasets show that the learned model transfers well to real UAV videos and can robustly detect moving targets in a wide variety of scenarios. Moreover, experiments on CDNet dataset demonstrate that even without explicitly assuming that the camera is stationary, the performance of JanusNet is comparable to traditional background subtraction methods.",
    "code_link": ""
  },
  "iccv2021_waami_aerialcross-platformpathplanningdataset": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "Aerial Cross-Platform Path Planning Dataset",
    "authors": [
      "Md. Shahid",
      "Sumohana S."
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Shahid_Aerial_Cross-Platform_Path_Planning_Dataset_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Shahid_Aerial_Cross-Platform_Path_Planning_Dataset_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Self-localisation mechanism in an unknown territory has been an interest area for humans since ages. Image matching is an obvious contender due to advancements in imaging devices and compute technologies. Deep learning methods have proven to be state-of-art in recent times but require large volumes of relevant data. Aerial image matching remains a challenging task due to the quality of images (e.g. platform disturbances, atmospheric effects), multiple types of on-board sensors (e.g. visual, thermal), variations in scales and look angles etc. To address these challenges, we present a cross-platform path planning dataset composed of images acquired from an aircraft and the Google Earth Engine (GEE). The proposed dataset contains manually aligned frames, corresponding match region, and semantic labeling of the images. Multiple galleries representing historical and instantaneous paths are generated. Our dataset envisages several realistic scenarios in crossplatform matching and semantic segmentation. We evaluate the performance of state-of-the-art image matching and segmentation algorithms on the proposed dataset. We will make our dataset freely available at https://www.iith.ac.in/ lfovia/downloads.html. Further, a case study on utilizing an existing open-source dataset for cross-platform path planning is also presented.",
    "code_link": ""
  },
  "iccv2021_waami_appearanceandmotionbasedpersistentmultipleobjecttrackinginwideareamotionimagery": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "Appearance and Motion Based Persistent Multiple Object Tracking in Wide Area Motion Imagery",
    "authors": [
      "Lars Sommer",
      "Wolfgang Kr\u00fcger",
      "Michael Teutsch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Sommer_Appearance_and_Motion_Based_Persistent_Multiple_Object_Tracking_in_Wide_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Sommer_Appearance_and_Motion_Based_Persistent_Multiple_Object_Tracking_in_Wide_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Wide Area Motion Imagery (WAMI) data acquired by an airborne sensor for ground observation offers great potential for various applications ranging from the protection of borders and critical infrastructure to city monitoring and surveillance. Persistent multiple object tracking, which is a prerequisite for these applications, is generally based on moving object detection, as the characteristics of existing WAMI datasets, e.g. weak appearance of objects, impede the usage of appearance based features. Complex and computationally expensive strategies such as exploiting multiple trackers in parallel or classifier-based local search are typically utilized to detect slow and stopping vehicles that are missed by moving object detection. In this paper, we propose a novel and much simpler tracking-by-detection approach for persistent tracking in WAMI data, which avoids such strategies. To overcome limitations caused by image quality of existing WAMI datasets, our proposed tracker was developed on self-acquired WAMI data recorded with a state-of-the-art industrial camera. The improved image quality enables appearance based object detection by Convolutional Neural Networks (CNNs) in WAMI, which we fuse with motion detection to compensate for missed detections in image regions with partial occlusion or shadows. Our proposed tracker is an extension of Deep SORT with modified track management and data association, which is able to yield high recall even in such difficult image regions as well as for slow or stopping vehicles, outperforming state-of-the-art on our self-acquired dataset.",
    "code_link": ""
  },
  "iccv2021_waami_pointcloudobjectsegmentationusingmultielevation-layer2dbounding-boxes": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "Point Cloud Object Segmentation Using Multi Elevation-Layer 2D Bounding-Boxes",
    "authors": [
      "Tristan Brodeur",
      "Hadi AliAkbarpour",
      "Steve Suddarth"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Brodeur_Point_Cloud_Object_Segmentation_Using_Multi_Elevation-Layer_2D_Bounding-Boxes_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Brodeur_Point_Cloud_Object_Segmentation_Using_Multi_Elevation-Layer_2D_Bounding-Boxes_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Segmentation of point clouds is a necessary pre-processing technique when object discrimination is needed for scene understanding. In this paper, we propose a segmentation technique utilizing 2D bounding-box data obtained via the orthographic projection of 3D points onto a plane at multiple elevation layers. Connected components is utilized to obtain bounding-box data, and a consistency metric between bounding-boxes at various elevation layers helps determine the classification of the bounding-box to an object of the scene. The merging of point data within each 2D bounding-box results in an object-segmented point cloud. Our method conducts segmentation using only the topological information of the point data within a dataset, requiring no extra computation of normals, creation of an octree or k-d tree, nor a dependency on RGB or intensity data associated with a point. Initial experiments are run on a set of point cloud datasets obtained via photogrammetric means, as well as some open-source, LIDAR-generated point clouds, showing the method to be capture agnostic. Results demonstrate the efficacy of this method in obtaining a distinct set of objects contained within a point cloud.",
    "code_link": ""
  },
  "iccv2021_waami_simulatedphotorealisticdeeplearningframeworkandworkflowstoacceleratecomputervisionandunmannedaerialvehicleresearch": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "Simulated Photorealistic Deep Learning Framework and Workflows To Accelerate Computer Vision and Unmanned Aerial Vehicle Research",
    "authors": [
      "Brendan Alvey",
      "Derek T. Anderson",
      "Andrew Buck",
      "Matthew Deardorff",
      "Grant Scott",
      "James M. Keller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Alvey_Simulated_Photorealistic_Deep_Learning_Framework_and_Workflows_To_Accelerate_Computer_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Alvey_Simulated_Photorealistic_Deep_Learning_Framework_and_Workflows_To_Accelerate_Computer_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep learning (DL) is producing state-of-the-art results in a number of unmanned aerial vehicle (UAV) tasks from low level signal processing to object detection, 3D mapping, tracking, fusion, autonomy, control, and beyond. However, barriers exist. For example, most DL algorithms require big data, but supervised ground truth is a bottleneck, fueling topics like self-supervised learning. While it is well-known that hardware and data augmentation plays a significant role in performance, it is not well understood which data augmentations or what real data need be collected. Furthermore, existing datasets do not have sufficient ground truth nor variety to support adequate controlled experimental research into understanding and mitigating limitations in DL algorithms, models, data, and biases. In this article, we address the combination of photorealistic simulation, open source libraries, and high quality content (models, materials, and environments) to develop workflows to mitigate the above challenges and accelerate DL-enabled computer vision research. Herein, examples are provided relative to data collection, detection, passive ranging, and human-robot teaming. Online video tutorials are also provided at https://github.com/MizzouINDFUL/UEUAVSim.",
    "code_link": ""
  },
  "iccv2021_waami_learning-basedshadowdetectioninaerialimageryusingautomatictrainingsupervisionfrom3dpointclouds": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "Learning-Based Shadow Detection in Aerial Imagery Using Automatic Training Supervision From 3D Point Clouds",
    "authors": [
      "Deniz Kavzak Ufuktepe",
      "Jaired Collins",
      "Ekincan Ufuktepe",
      "Joshua Fraser",
      "Timothy Krock",
      "Kannappan Palaniappan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Ufuktepe_Learning-Based_Shadow_Detection_in_Aerial_Imagery_Using_Automatic_Training_Supervision_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Ufuktepe_Learning-Based_Shadow_Detection_in_Aerial_Imagery_Using_Automatic_Training_Supervision_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Shadows, motion parallax, and occlusions pose significant challenges to vision tasks in wide area motion imagery (WAMI) including object identification and tracking. Although there are many successful shadow detection approaches that work well in indoor scenes, close range outdoor scenes, and spaceborne satellite images, the methods tend to fail in intermediate altitude aerial WAMI. We propose an automatic shadow mask estimation approach for supervision without manual labeling to provide a large amount of training data for learning-based aerial shadow extraction. Analytical ground-truth shadow masks are generated using 3D point clouds combined with known solar angles. FSDNet, a deep network for shadow detection, is evaluated on aerial imagery. Preliminary results indicate that training using automated shadow mask supervision improves performance, and opens the door for developing new deep architectures for shadow detection and enhancement in WAMI.",
    "code_link": ""
  },
  "iccv2021_waami_robustmulti-objecttrackingusingre-identificationfeaturesandgraphconvolutionalnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "WAAMI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Analysis of Aerial Motion Imagery",
    "title": "Robust Multi-Object Tracking Using Re-Identification Features and Graph Convolutional Networks",
    "authors": [
      "Christian Lusardi",
      "Abu Md Niamul Taufique",
      "Andreas Savakis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/html/Lusardi_Robust_Multi-Object_Tracking_Using_Re-Identification_Features_and_Graph_Convolutional_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/WAAMI/papers/Lusardi_Robust_Multi-Object_Tracking_Using_Re-Identification_Features_and_Graph_Convolutional_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a graph neural network-based framework for multi-object tracking that combines detection and association along with the use of a novel re-identification feature. We explore the combination of multiple appearance features within our framework to obtain a better representation and improve tracking accuracy. Data augmentations with random erase and random noise are utilized to improve robustness during tracking. We consider various types of losses during training, including a unique application of the triplet loss to improve overall network performance. Results are presented on the UAVDT benchmark dataset for aerial-based vehicle tracking under various conditions.",
    "code_link": ""
  },
  "iccv2021_mair2_bomudanetunsupervisedadaptationforvisualsceneunderstandinginunstructureddrivingenvironments": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MAIR2",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Multi-Agent Interaction and Relational Reasoning",
    "title": "BoMuDANet: Unsupervised Adaptation for Visual Scene Understanding in Unstructured Driving Environments",
    "authors": [
      "Divya Kothandaraman",
      "Rohan Chandra",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MAIR2/html/Kothandaraman_BoMuDANet_Unsupervised_Adaptation_for_Visual_Scene_Understanding_in_Unstructured_Driving_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MAIR2/papers/Kothandaraman_BoMuDANet_Unsupervised_Adaptation_for_Visual_Scene_Understanding_in_Unstructured_Driving_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present an unsupervised adaptation approach for visual scene understanding in unstructured traffic environments. Our method is designed for unstructured real-world scenarios with dense and heterogeneous traffic consisting of cars, trucks, two-and three-wheelers, and pedestrians. We describe a new semantic segmentation technique based on unsupervised domain adaptation (DA), that can identify the class or category of each region in RGB images or videos. We also present a novel self-training algorithm for multi-source DA that improves the accuracy. Our overall approach is a deep learning-based technique and consists of an unsupervised neural network that achieves 87.18% accuracy on the challenging India Driving Dataset. Our method works well on roads that may not be well-marked or may include dirt, unidentifiable debris, potholes, etc. A key aspect of our approach is that it can also identify objects that are encountered by the model for the fist time during the testing phase. We compare our method against the state-of-the art methods and show an improvement of 5.17% - 42.9%. Furthermore, we also conduct user studies that qualitatively validate the improvements in visual scene understanding of unstructured driving environments.",
    "code_link": "https://github.com/divyakraman/BoMuDABoundless-Multi-Source-Domain-Adaptive-Segmentation-inUnstructured-Environments"
  },
  "iccv2021_mair2_cross-modalrelationalreasoningnetworkforvisualquestionanswering": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "MAIR2",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Multi-Agent Interaction and Relational Reasoning",
    "title": "Cross-Modal Relational Reasoning Network for Visual Question Answering",
    "authors": [
      "Hongyu Chen",
      "Ruifang Liu",
      "Bo Peng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/MAIR2/html/Chen_Cross-Modal_Relational_Reasoning_Network_for_Visual_Question_Answering_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/MAIR2/papers/Chen_Cross-Modal_Relational_Reasoning_Network_for_Visual_Question_Answering_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Visual Question Answering (VQA) is a challenging task that requires a cross-modal understanding of images and questions with relational reasoning leading to the correct answer. To bridge the semantic gap between these two modalities, previous works focus on the word-region alignments of all possible pairs without attending more attention to the corresponding word and object. Treating all pairs equally without consideration of relation consistency hinders the model's performance. In this paper, to align the relation-consistent pairs and integrate the interpretability of VQA systems, we propose a Cross-modal Relational Reasoning Network (CRRN), to mask the inconsistent attention map and highlight the full latent alignments of corresponding word-region pairs. Specifically, we present two relational masks for inter-modal and intra-modal highlighting, inferring the more and less important words in sentences or regions in images. The attention interrelationship of consistent pairs can be enhanced with the shift of learning focus by masking the unaligned relations. Then, we propose two novel losses LCMAM and LSMAM with explicit supervision to capture the fine-grained interplay between vision and language. We have conduct thorough experiments to prove the effectiveness and achieve the competitive performance for reaching 61.74% on GQA benchmark.",
    "code_link": ""
  },
  "iccv2021_lci_thermalimageprocessingviaphysics-inspireddeepnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "Thermal Image Processing via Physics-Inspired Deep Networks",
    "authors": [
      "Vishwanath Saragadam",
      "Akshat Dave",
      "Ashok Veeraraghavan",
      "Richard G. Baraniuk"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Saragadam_Thermal_Image_Processing_via_Physics-Inspired_Deep_Networks_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Saragadam_Thermal_Image_Processing_via_Physics-Inspired_Deep_Networks_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We introduce DeepIR, a new thermal image processing framework that combines physically accurate sensor modeling with deep network-based image representation. Our key enabling observations are that the images captured by thermal sensors can be factored into slowly changing, scene-independent sensor non-uniformities (that can be accurately modeled using physics) and a scene-specific radiance flux (that is well-represented using a deep network-based regularizer). DeepIR requires neither training data nor periodic ground-truth calibration with a known blackbody target-making it well suited for practical computer vision tasks. We demonstrate the power of going DeepIR by developing new denoising and super-resolution algorithms that exploit multiple images of the scene captured with camera jitter. Simulated and real data experiments demonstrate that DeepIR can perform high-quality non-uniformity correction with as few as three images, achieving a 10dB PSNR improvement over competing approaches.",
    "code_link": "https://github.com/vishwa91/DeepIR"
  },
  "iccv2021_lci_photon-limitedobjectdetectionusingnon-localfeaturematchingandknowledgedistillation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "Photon-Limited Object Detection Using Non-Local Feature Matching and Knowledge Distillation",
    "authors": [
      "Chengxi Li",
      "Xiangyu Qu",
      "Abhiram Gnanasambandam",
      "Omar A. Elgendy",
      "Jiaju Ma",
      "Stanley H. Chan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Li_Photon-Limited_Object_Detection_Using_Non-Local_Feature_Matching_and_Knowledge_Distillation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Li_Photon-Limited_Object_Detection_Using_Non-Local_Feature_Matching_and_Knowledge_Distillation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Robust object detection under photon-limited conditions is crucial for applications such as night vision, surveillance, and microscopy, where the number of photons per pixel is low due to a dark environment and/or a short integration time. While the mainstream \"low-light\" image enhancement methods have produced promising results that improve the image contrast between the foreground and background through advanced coloring techniques, the more challenging problem of mitigating the photon shot noise inherited from the random Poisson process remains open. In this paper, we present a photon-limited object detection framework by adding two ideas to state-of-the-art object detectors: 1) a space-time non-local module that leverages the spatial-temporal information across an image sequence in the feature space, and 2) knowledge distillation in the form of student-teacher learning to improve the robustness of the detector's feature extractor against noise. Experiments are conducted to demonstrate the improved performance of the proposed method in comparison with state-of-the-art baselines. When integrated with the latest photon counting devices, the algorithm achieves more than 50% mean average precision at a photon level of 1 photon per pixel.",
    "code_link": "https://github.com/jwyang/faster-rcnn.pytorch"
  },
  "iccv2021_lci_whatdoesyourcomputationalimagingalgorithmnotknow?aplug-and-playmodelquantifyingmodeluncertainty": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "What Does Your Computational Imaging Algorithm Not Know?: A Plug-and-Play Model Quantifying Model Uncertainty",
    "authors": [
      "Canberk Ekmekci",
      "Mujdat Cetin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Ekmekci_What_Does_Your_Computational_Imaging_Algorithm_Not_Know_A_Plug-and-Play_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Ekmekci_What_Does_Your_Computational_Imaging_Algorithm_Not_Know_A_Plug-and-Play_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Plug-and-Play is an algorithmic framework developed to solve image recovery problems. Thanks to the empirical success of convolutional neural network (CNN) denoisers, numerous Plug-and-Play algorithms utilizing CNN denoisers have been proposed to solve various image recovery tasks. Unfortunately, those Plug-and-Play algorithms lack representing the uncertainty on the parameters of CNN denoisers because their training procedure yields only a point estimate for the parameters of the CNN denoiser. In this paper, we present a novel Plug-and-Play model that quantifies the uncertainty on the parameters of the CNN denoiser. The proposed model places a probability distribution on the parameters of the CNN denoiser and carries out approximate Bayesian inference to obtain the posterior distribution of the parameters to characterize their uncertainty. The uncertainty information provided by the proposed Plug-and-Play model allows characterizing how certain the model is for a given input. The proposed Plug-and-Play model is applicable to a broad set of computational imaging problems, with the requirement that the data fidelity term is differentiable, and has a simple implementation in deep learning frameworks. We evaluate the proposed Plug-and-Play model on a magnetic resonance imaging reconstruction problem and demonstrate its uncertainty characterization capability.",
    "code_link": "https://github.com/cszn/DPIR"
  },
  "iccv2021_lci_k-spacerefinementindeeplearningmrreconstructionviaregularizingscanspecificspirit-basedselfconsistency": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "K-Space Refinement in Deep Learning MR Reconstruction via Regularizing Scan Specific SPIRiT-Based Self Consistency",
    "authors": [
      "Kanghyun Ryu",
      "Cagan Alkan",
      "Chanyeol Choi",
      "Ikbeom Jang",
      "Shreyas Vasanawala"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Ryu_K-Space_Refinement_in_Deep_Learning_MR_Reconstruction_via_Regularizing_Scan_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Ryu_K-Space_Refinement_in_Deep_Learning_MR_Reconstruction_via_Regularizing_Scan_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Deep Learning (DL) based reconstruction using unrolled neural networks has shown great potential in accelerating magnetic resonance imaging (MRI). However, one of the major drawbacks is the loss of high-frequency details and textures in the output. In this paper, we propose a novel refinement method based on SPIRiT (Iterative Self-consistent Parallel Imaging Reconstruction from Arbitrary k-Space) formulation to reduce the k-space errors and enable reconstruction of improved high-frequency image details and textures. The proposed scheme constrains the DL output to satisfy the neighborhood relationship in the frequency space (k-space) which can be easily calibrated in the auto-calibration (ACS) lines, and corrects the underestimation in the peripheral region of the k-space as well as reduce structured k-space errors. We show that our method enables the reconstruction of sharper images with significantly improved high-frequency components measured by HFEN and GMSD while maintaining overall error in the image measured by PSNR and SSIM.",
    "code_link": "https://github.com/facebookresearch/fastMRI"
  },
  "iccv2021_lci_compressedclassificationfromlearnedmeasurements": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "Compressed Classification From Learned Measurements",
    "authors": [
      "Robiulhossain Mdrafi",
      "Ali Cafer Gurbuz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Mdrafi_Compressed_Classification_From_Learned_Measurements_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Mdrafi_Compressed_Classification_From_Learned_Measurements_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "This work proposes a deep compressed learning framework inferring classification directly from the compressive measurements. While classical approaches separately sense, reconstruct signals, and apply classification on these reconstructions, we jointly learn the sensing and classification schemes utilizing a deep neural network with a novel loss function. Our approach employs a data-driven reconstruction network within the compressed learning framework utilizing a weighted loss that combines both in-network reconstruction and classification losses. The proposed network structure also learns the optimal measurement matrices for the goal of enhancing classification performance. Quantitative results demonstrated on CIFAR-10 image dataset show that the proposed framework provides better classification performance and robustness to noise compared to the tested state of the art deep compressed learning approaches.",
    "code_link": ""
  },
  "iccv2021_lci_ss-jircsself-supervisedjointimagereconstructionandcoilsensitivitycalibrationinparallelmriwithoutgroundtruth": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "SS-JIRCS: Self-Supervised Joint Image Reconstruction and Coil Sensitivity Calibration in Parallel MRI Without Ground Truth",
    "authors": [
      "Weijie Gan",
      "Yuyang Hu",
      "Cihat Eldeniz",
      "Jiaming Liu",
      "Yasheng Chen",
      "Hongyu An",
      "Ulugbek S. Kamilov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Gan_SS-JIRCS_Self-Supervised_Joint_Image_Reconstruction_and_Coil_Sensitivity_Calibration_in_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Gan_SS-JIRCS_Self-Supervised_Joint_Image_Reconstruction_and_Coil_Sensitivity_Calibration_in_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Parallel magnetic resonance imaging (MRI) is a widely-used technique that accelerates data collection by making use of the spatial encoding provided by multiple receiver coils. A key issue in parallel MRI is the estimation of coil sensitivity maps (CSMs) that are used for reconstructing a single high-quality image. This paper addresses this issue by developing SS-JIRCS, a new self-supervised model-based deep-learning (DL) method for image reconstruction that is equipped with automated CSM calibration. Our deep network consists of three types of modules: data-consistency, regularization, and CSM calibration. Unlike traditional supervised DL methods, these modules are directly trained on undersampled and noisy k-space data rather than on fully sampled high-quality ground truth. We present empirical results on simulated data that show the potential of the proposed method for achieving better performance than several baseline methods.",
    "code_link": ""
  },
  "iccv2021_lci_jointreconstructionandcalibrationusingregularizationbydenoisingwithapplicationtocomputedtomography": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "Joint Reconstruction and Calibration Using Regularization by Denoising With Application to Computed Tomography",
    "authors": [
      "Mingyang Xie",
      "Jiaming Liu",
      "Yu Sun",
      "Weijie Gan",
      "Brendt Wohlberg",
      "Ulugbek S. Kamilov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Xie_Joint_Reconstruction_and_Calibration_Using_Regularization_by_Denoising_With_Application_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Xie_Joint_Reconstruction_and_Calibration_Using_Regularization_by_Denoising_With_Application_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Regularization by denoising (RED) is a broadly applicable framework for solving inverse problems by using priors specified as denoisers. While RED has been shown to provide state-of-the-art performance in a number of imaging applications, existing RED algorithms require exact knowledge of the measurement operator characterizing the imaging system, limiting their applicability in problems where the measurement operator has parametric uncertainties. We propose a new method, called Calibrated RED (Cal-RED), that enables joint calibration of the measurement operator along with reconstruction of the unknown image. Cal-RED extends the traditional RED methodology to imaging problems that require the calibration of the measurement operator. We validate Cal-RED on the problem of image reconstruction in computerized tomography (CT) under perturbed projection angles. Our results corroborate the effectiveness of Cal-RED for joint calibration and reconstruction using pre-trained deep denoisers as image priors.",
    "code_link": ""
  },
  "iccv2021_lci_fastunsupervisedmrireconstructionwithoutfully-sampledgroundtruthdatausinggenerativeadversarialnetworks": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "Fast Unsupervised MRI Reconstruction Without Fully-Sampled Ground Truth Data Using Generative Adversarial Networks",
    "authors": [
      "Elizabeth K. Cole",
      "Frank Ong",
      "Shreyas S. Vasanawala",
      "John M. Pauly"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Cole_Fast_Unsupervised_MRI_Reconstruction_Without_Fully-Sampled_Ground_Truth_Data_Using_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Cole_Fast_Unsupervised_MRI_Reconstruction_Without_Fully-Sampled_Ground_Truth_Data_Using_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Most deep learning (DL) magnetic resonance imaging (MRI) reconstruction approaches rely on supervised training algorithms, which require access to high-quality, fully-sampled ground truth datasets. In MRI, acquiring fully-sampled data is time-consuming, expensive, and, in some cases, impossible due to limitations on data acquisition speed. We present a DL framework for MRI reconstruction that does not require any fully-sampled data using unsupervised generative adversarial networks. We test our proposed method on 2D knee MRI data and 2D+time abdominal dynamic contrast enhanced (DCE) MRI data. In the DCE-MRI dataset, as is the case with many dynamic MRI sequences, ground truth was not possible to acquire and therefore, supervised DL reconstruction was not feasible. We show that our unsupervised method produces reconstructions which are better than compressed sensing in terms of image metrics and the recovery of anatomical structure, with faster inference time. In contrast to most deep learning reconstruction techniques, which are supervised, this method does not need any fully-sampled data. With the proposed method, accelerated imaging and accurate reconstruction can be performed in applications in cases where fully-sampled datasets are difficult to obtain or unavailable.",
    "code_link": ""
  },
  "iccv2021_lci_howtocheatwithmetricsinsingle-imagehdrreconstruction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "How To Cheat With Metrics in Single-Image HDR Reconstruction",
    "authors": [
      "Gabriel Eilertsen",
      "Saghi Hajisharif",
      "Param Hanji",
      "Apostolia Tsirikoglou",
      "Rafa\u0142 K. Mantiuk",
      "Jonas Unger"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Eilertsen_How_To_Cheat_With_Metrics_in_Single-Image_HDR_Reconstruction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Eilertsen_How_To_Cheat_With_Metrics_in_Single-Image_HDR_Reconstruction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Single-image high dynamic range (SI-HDR) reconstruction has recently emerged as a problem well-suited for deep learning methods. Each successive technique demonstrates an improvement over existing methods by reporting higher image quality scores. This paper, however, highlights that such improvements in objective metrics do not necessarily translate to visually superior images. The first problem is the use of disparate evaluation conditions in terms of data and metric parameters, calling for a standardized protocol to make it possible to compare between papers. The second problem, which forms the main focus of this paper, is the inherent difficulty in evaluating SI-HDR reconstructions since certain aspects of the reconstruction problem dominate objective differences, thereby introducing a bias. Here, we reproduce a typical evaluation using existing as well as simulated SI-HDR methods to demonstrate how different aspects of the problem affect objective quality metrics. Surprisingly, we found that methods that do not even reconstruct HDR information can compete with state-of-the-art deep learning methods. We show how such results are not representative of the perceived quality and that SI-HDR reconstruction needs better evaluation protocols.",
    "code_link": ""
  },
  "iccv2021_lci_cryoposenetend-to-endsimultaneouslearningofsingle-particleorientationand3dmapreconstructionfromcryo-electronmicroscopydata": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Learning for Computational Imaging",
    "title": "CryoPoseNet: End-to-End Simultaneous Learning of Single-Particle Orientation and 3D Map Reconstruction From Cryo-Electron Microscopy Data",
    "authors": [
      "Youssef S. G. Nashed",
      "Fr\u00e9d\u00e9ric Poitevin",
      "Harshit Gupta",
      "Geoffrey Woollard",
      "Michael Kagan",
      "Chun Hong Yoon",
      "Daniel Ratner"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/html/Nashed_CryoPoseNet_End-to-End_Simultaneous_Learning_of_Single-Particle_Orientation_and_3D_Map_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/LCI/papers/Nashed_CryoPoseNet_End-to-End_Simultaneous_Learning_of_Single-Particle_Orientation_and_3D_Map_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Cryogenic electron microscopy (cryo-EM) provides im-ages from different copies of the same biomolecule in ar-bitrary orientations. Here, we present an end-to-end unsu-pervised approach that learns individual particle orienta-tions directly from cryo-EM data while reconstructing the3D map of the biomolecule following random initialization.The approach relies on an auto-encoder architecture wherethe latent space is explicitly interpreted as orientations usedby the decoder to form an image according to the physi-cal projection model. We evaluate our method on simulateddata and show that it is able to reconstruct 3D particle mapsfrom noisy- and CTF-corrupted 2D projection images of un-known particle orientations",
    "code_link": ""
  },
  "iccv2021_htcv_attentionawaredebiasingforunbiasedmodelprediction": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "Attention Aware Debiasing for Unbiased Model Prediction",
    "authors": [
      "Puspita Majumdar",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Majumdar_Attention_Aware_Debiasing_for_Unbiased_Model_Prediction_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Majumdar_Attention_Aware_Debiasing_for_Unbiased_Model_Prediction_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Due to the large applicability of AI systems in various applications, fairness in model predictions is extremely important to ensure that the systems work equally well for everyone. Biased feature representations might often lead to unfair model predictions. To address the concern, in this research, a novel method, termed as Attention Aware Debiasing (AAD) method, is proposed to learn unbiased feature representations. The proposed method uses an attention mechanism to focus on the features important for the main task while suppressing the features related to the sensitive attributes. This minimizes the model's dependency on the sensitive attribute while performing the main task. Multiple experiments are performed on two publicly available datasets, MORPH and UTKFace, to showcase the effectiveness of the proposed AAD method for bias mitigation. The proposed AAD method enhances the overall model performance and reduces the disparity in model prediction across different subgroups.",
    "code_link": ""
  },
  "iccv2021_htcv_end-to-endmodel-basedgaitrecognitionusingsynchronizedmulti-viewposeconstraint": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "End-to-End Model-Based Gait Recognition Using Synchronized Multi-View Pose Constraint",
    "authors": [
      "Xiang Li",
      "Yasushi Makihara",
      "Chi Xu",
      "Yasushi Yagi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Li_End-to-End_Model-Based_Gait_Recognition_Using_Synchronized_Multi-View_Pose_Constraint_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Li_End-to-End_Model-Based_Gait_Recognition_Using_Synchronized_Multi-View_Pose_Constraint_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose an end-to-end model-based cross-view gait recognition which employs pose sequences and shapes extracted by human model fitting. Specifically, we consider a problem setting where gait sequences from single different views are given as a pair to match in a test phase, while asynchronous multi-view gait sequences are given for each subject in a training phase. This work exploits multi-view constraint in the training phase to extract more consistent pose sequences from any views in the test phase, unlike the existing methods do not consider them. For this purpose, given asynchronous multi-view gait sequences, we introduce a phase synchronization step in the training phase so that we can impose pose consistency at each synchronized phase in a temporally up-sampled phase domain. We then train our network by minimizing a loss function based on the synchronized multi-view pose constraint as well as shape consistency, temporal pose smoothness, recognition accuracy, etc in an end-to-end manner. We also introduce the synchronization step in a test phase to reduce intra-subject variations caused by asynchronous pose features. Experimental results on the OU-MVLP and CASIA-B datasets show that the proposed method achieves the state-of-the-art performance for both gait identification and verification scenarios, especially a great improvement in terms of the pose representations.",
    "code_link": ""
  },
  "iccv2021_htcv_sveaasmall-scalebenchmarkforvalidatingtheusabilityofpost-hocexplainableaisolutionsinimageandsignalrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "SVEA: A Small-Scale Benchmark for Validating the Usability of Post-Hoc Explainable AI Solutions in Image and Signal Recognition",
    "authors": [
      "Sam Sattarzadeh",
      "Mahesh Sudhakar",
      "Konstantinos N. Plataniotis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Sattarzadeh_SVEA_A_Small-Scale_Benchmark_for_Validating_the_Usability_of_Post-Hoc_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Sattarzadeh_SVEA_A_Small-Scale_Benchmark_for_Validating_the_Usability_of_Post-Hoc_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Novel solutions in the area of Explainable AI (XAI) have made a significant breakthrough in increasing the trust of end-users in Machine Learning (ML) models. However, validating the performance of these solutions remains a challenging task. In this work, we focus on evaluating the methods that attribute a model's decision to their input features. The prior metrics on this topic fail to consider multiple properties that a usable explainability solution should satisfy. Also, conducting experiments to assess the concreteness of the explanations provided by these solutions in large-scale datasets consumes excessive time and resources. To overcome these shortcomings, we propose the Small-scale Visual Explanation Analysis (SVEA) benchmark, which comprises the recent minimal MNIST-1D dataset. Our proposed benchmarking tool aids the practitioners and researchers to perform experiments on the Explainable AI methods without the need to access expensive computational devices. Furthermore, we offer a framework to evaluate various characteristics of the state-of-the-art XAI methods and include several widely used interpretability solutions in the SVEA benchmark to perform a thorough analysis of their completeness and understandability. The results obtained from our proposed evaluation metric suggest that specific approaches lack the ability to transfer the chosen model's understanding to a second interpretable model by the explanations generated. The users can replicate our experiments within few minutes before working extensively on other larger datasets, thereby saving a lot of experimental time and effort.",
    "code_link": "https://github.com/greydanus/mnist1d"
  },
  "iccv2021_htcv_student-teacheronenessastorage-efficientapproachthatimprovesfacialexpressionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "Student-Teacher Oneness: A Storage-Efficient Approach That Improves Facial Expression Recognition",
    "authors": [
      "Zhenzhu Zheng",
      "Christopher Rasmussen",
      "Xi Peng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Zheng_Student-Teacher_Oneness_A_Storage-Efficient_Approach_That_Improves_Facial_Expression_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Zheng_Student-Teacher_Oneness_A_Storage-Efficient_Approach_That_Improves_Facial_Expression_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present Student-Teacher Oneness (STO), a simple but effective approach for online knowledge distillation improves facial expression recognition, without introducing any extra model parameters. Stochastic sub-networks are designed to replace the multi-branch architecture component in current online distillation methods. This leads to a simplified architecture, and yet competitive performances. Under the \"teacher-student\"\" framework, we construct both teacher and student within the same target network. Student network is the sub-networks which randomly skipping some portions of the full (target) network. The teacher network is the full network, can be considered as the ensemble of all possible student networks. The training process is performed in a closed-loop: (1) Forward prediction contains two passes that generate student and teacher predictions. (2) Backward distillation allows knowledge transfer from the teacher back to students. Comprehensive evaluations show that STO improves the generalization ability of a variety of deep neural networks to a significant margin. The results prove our superior performance in facial expression recognition task on FER-2013 and RAF.",
    "code_link": ""
  },
  "iccv2021_htcv_formula-drivensupervisedlearningwithrecursivetilingpatterns": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "Formula-Driven Supervised Learning With Recursive Tiling Patterns",
    "authors": [
      "Hirokatsu Kataoka",
      "Asato Matsumoto",
      "Ryosuke Yamada",
      "Yutaka Satoh",
      "Eisuke Yamagata",
      "Nakamasa Inoue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Kataoka_Formula-Driven_Supervised_Learning_With_Recursive_Tiling_Patterns_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Kataoka_Formula-Driven_Supervised_Learning_With_Recursive_Tiling_Patterns_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Can convolutional neural networks pre-trained without natural images be used to assist natural image understanding? Formula-Driven Supervised Learning (FDSL) automatically generates image patterns and their category labels by assigning a well-organized formula. Due to the characteristics of not using natural images in pre-training phase, FDSL is expected to develop a trustworthy vision-based system in terms of human-annotation-free, fairer and more transparent datasets. In this paper, we propose TileDB which consists of recursive tiling patterns in the whole image and evaluates the family of FDSL such as the datasets consist of Perlin noise and Bezier curves. Experimental results show that our proposed TileDB pre-trained model performs much better than models trained from scratch, surpasses a similar self-supervised learning (SSL), and performs similarly to the models pre-trained with 100k-order natural image datasets such as ImageNet-100 and Places-30. By comparing to the FractalDB pre-trained model, the TileDB pre-trained model achieves better performances in a compact dataset (< 1,000 categories). Moreover, the image representation trained on TileDB can extract similar features to the ImageNet pre-trained model even though the training images are non-trivially different.",
    "code_link": ""
  },
  "iccv2021_htcv_multi-perspectivefeatureslearningforfaceanti-spoofing": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "Multi-Perspective Features Learning for Face Anti-Spoofing",
    "authors": [
      "Zhuming Wang",
      "Yaowen Xu",
      "Lifang Wu",
      "Hu Han",
      "Yukun Ma",
      "Guozhang Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Wang_Multi-Perspective_Features_Learning_for_Face_Anti-Spoofing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Wang_Multi-Perspective_Features_Learning_for_Face_Anti-Spoofing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Face anti-spoofing (FAS) is important to securing face recognition. Most of the existing methods regard FAS as a binary classification problem between bona fide (real) and spoof images, training their models from only the perspective of Real vs. Spoof. It is not beneficial for a comprehensive description of real samples and leads to degraded performance after extending attack types. In fact, the spoofing clues in various attacks can be significantly different. Furthermore, some attacks have characteristics similar to the real faces but different from other attacks. For example, both real faces and video attacks have dynamic features, and both mask attacks and real faces have depth features. In this paper, a Multi-Perspective Feature Learning Network (MPFLN) is proposed to extract representative features from the perspectives of Real + Mask vs. Photo + Video and Real + Video vs. Photo + Mask. And using these features, a binary classification network is designed to perform FAS. Experimental results show that the proposed method can effectively alleviate the above issue of the decline in the discrimination of extracted features and achieve comparable performance with state-of-the-art methods.",
    "code_link": ""
  },
  "iccv2021_htcv_transformermeetspartmodeladaptivepartdivisionforpersonre-identification": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "Transformer Meets Part Model: Adaptive Part Division for Person Re-Identification",
    "authors": [
      "Shenqi Lai",
      "Zhenhua Chai",
      "Xiaolin Wei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Lai_Transformer_Meets_Part_Model_Adaptive_Part_Division_for_Person_Re-Identification_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Lai_Transformer_Meets_Part_Model_Adaptive_Part_Division_for_Person_Re-Identification_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Part model is one of the key factors to high performance person re-identification (ReID) task. In recent studies, there are mainly two streams for part model. The first one is to divide a person image into several fixed parts to obtain their local information, but it may cause performance degradation in case of misalignment. The other one is to explore external resources like pose estimation or human parsing to locate local parts, but it costs extra storage and computation. Inspired by recent successful transformers on spatial similarity modeling, we propose a novel Adaptive Part Division (APD) model to better extract local features. More specifically, APD mainly consists of two crucial modules: a Transformer-based Part Merge (TPM) module and a Part Mask Generation (PMG) module. In particular, TPM first adaptively assigns the patch tokens of the same semantic object to the identical part. Then, PMG takes these identical parts together and generates several non-overlapping masks for robust part division. We have conducted extensive evaluations on four popular benchmarks, i.e. Market-1501, CUHK03, DukeMTMC-ReID and MSMT17, and the experimental results show that our proposed method achieves the state-of-the-art performance.",
    "code_link": ""
  },
  "iccv2021_htcv_sparsefeaturerepresentationlearningfordeepfacegendertransfer": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "Sparse Feature Representation Learning for Deep Face Gender Transfer",
    "authors": [
      "Xudong Liu",
      "Ruizhe Wang",
      "Hao Peng",
      "Minglei Yin",
      "Chih-Fan Chen",
      "Xin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Liu_Sparse_Feature_Representation_Learning_for_Deep_Face_Gender_Transfer_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Liu_Sparse_Feature_Representation_Learning_for_Deep_Face_Gender_Transfer_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Why do people think Tom Hanks and Juliette Lewis look alike? Can we modify the gender appearance of a face image without changing its identity information? Is there any specific feature responsible for the perception of femininity/masculinity in a given face image? Those questions are appealing from both computer vision and visual perception perspectives. To shed light upon them, we propose to develop a GAN based approach toward face gender transfer and study the relevance of learned feature representations to face gender perception. Our key contributions include: 1) an architecture design with specially tailored loss functions in the feature space for face gender transfer; 2) the introduction of a novel probabilistic gender mask to facilitate achieving both the objectives of gender transfer and identity preservation; and 3) identification of sparse features ( approx 20 out of 256) uniquely responsible for face gender perception. Extensive experimental results are reported to demonstrate not only the superiority of the proposed face gender transfer technique (in terms of visual quality of reconstructed images) but also the effectiveness of gender feature representation learning (in terms of the high correlation between the learned sparse features and the perceived gender information). Our findings seem to corroborate a hypothesis about the independence between face recognizability and gender classifiability in the literature of psychology. We expect this work will stimulate more computational studies of face perception including race, age, attractiveness, and trustworthiness.",
    "code_link": ""
  },
  "iccv2021_htcv_fedaffectfew-shotfederatedlearningforfacialexpressionrecognition": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "FedAffect: Few-Shot Federated Learning for Facial Expression Recognition",
    "authors": [
      "Debaditya Shome",
      "Tejaswini Kar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Shome_FedAffect_Few-Shot_Federated_Learning_for_Facial_Expression_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Shome_FedAffect_Few-Shot_Federated_Learning_for_Facial_Expression_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Annotation of large-scale facial expression datasets in the real world is a major challenge because of privacy concerns of the individuals due to which traditional supervised learning approaches won't scale. Moreover, training models on large curated datasets often leads to dataset bias which reduces generalizability for real world use. Federated learning is a recent paradigm for training models collaboratively with decentralized private data on user devices. In this paper, we propose a few-shot federated learning framework which utilizes few samples of labeled private facial expression data to train local models in each training round and aggregates all the local model weights in the central server to get a globally optimal model. In addition, as the user devices are a large source of unlabeled data, we design a federated learning based self-supervised method to disjointly update the feature extractor network on unlabeled private facial data in order to learn robust and diverse face representations. Experimental results by testing the globally trained model on benchmark datasets (FER-2013 and FERG) show comparable performance with state of the art centralized approaches. To the best of author's knowledge, this is the first work on few-shot federated learning for facial expression recognition.",
    "code_link": ""
  },
  "iccv2021_htcv_rethinkingcommonassumptionstomitigateracialbiasinfacerecognitiondatasets": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "Rethinking Common Assumptions To Mitigate Racial Bias in Face Recognition Datasets",
    "authors": [
      "Matthew Gwilliam",
      "Srinidhi Hegde",
      "Lade Tinubu",
      "Alex Hanson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Gwilliam_Rethinking_Common_Assumptions_To_Mitigate_Racial_Bias_in_Face_Recognition_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Gwilliam_Rethinking_Common_Assumptions_To_Mitigate_Racial_Bias_in_Face_Recognition_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Many existing works have made great strides towards reducing racial bias in face recognition. However, most of these methods attempt to rectify bias that manifests in models during training instead of directly addressing a major source of the bias, the dataset itself. Exceptions to this are BUPT-Balancedface/RFW and Fairface, but these works assume that primarily training on a single race or not racially balancing the dataset are inherently disadvantageous. We demonstrate that these assumptions are not necessarily valid. In our experiments, training on only African faces induced less bias than training on a balanced distribution of faces and distributions skewed to include more African faces produced more equitable models. We additionally notice that adding more images of existing identities to a dataset in place of adding new identities can lead to accuracy boosts across racial categories.",
    "code_link": ""
  },
  "iccv2021_htcv_ontheimportanceofencryptingdeepfeatures": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "HTCV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Human-Centric Trustworthy Computer Vision: From Research to Applications",
    "title": "On the Importance of Encrypting Deep Features",
    "authors": [
      "Xingyang Ni",
      "Heikki Huttunen",
      "Esa Rahtu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Ni_On_the_Importance_of_Encrypting_Deep_Features_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/HTCV/papers/Ni_On_the_Importance_of_Encrypting_Deep_Features_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "In this study, we analyze model inversion attacks with only two assumptions: feature vectors of user data are known, and a black-box API for inference is provided. On the one hand, limitations of existing studies are addressed by opting for a more practical setting. Experiments have been conducted on state-of-the-art models in person re-identification, and two attack scenarios (i.e., recognizing auxiliary attributes and reconstructing user data) are investigated. Results show that an adversary could successfully infer sensitive information even under severe constraints. On the other hand, it is advisable to encrypt feature vectors, especially for a machine learning model in production. As an alternative to traditional encryption methods such as AES, a simple yet effective method termed ShuffleBits is presented. More specifically, the binary sequence of each floating-point number gets shuffled. Deployed using the one-time pad scheme, it serves as a plug-and-play module that is applicable to any neural network, and the resulting model directly outputs deep features in encrypted form. Source code is publicly available at https://github.com/nixingyang/ShuffleBits.",
    "code_link": "https://github.com/nixingyang/ShuffleBits"
  },
  "iccv2021_tag-cv_grassmanniandimensionalityreductionforoptimizeduniversalmanifoldembeddingrepresentationof3dpointclouds": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "Grassmannian Dimensionality Reduction for Optimized Universal Manifold Embedding Representation of 3D Point Clouds",
    "authors": [
      "Yuval Haitman",
      "Joseph M. Francos",
      "Louis L. Scharf"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Haitman_Grassmannian_Dimensionality_Reduction_for_Optimized_Universal_Manifold_Embedding_Representation_of_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Haitman_Grassmannian_Dimensionality_Reduction_for_Optimized_Universal_Manifold_Embedding_Representation_of_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Consider a 3-D object and the orbit of equivalent objects turned out by the rigid transformation group. The set of possible observations on these equivalent objects is generally a manifold in the ambient space of observations. It has been shown that the rigid transformation universal manifold embedding (RTUME) provides a mapping from the orbit of observations on some object to a single low dimensional linear subspace of Euclidean space. This linear subspace is invariant to the geometric transformations and hence is a representative of the orbit. In the classification set-up the RTUME subspace extracted from an experimental observation is tested against a set of subspaces representing the different object manifolds, in search for the nearest class. We clarify the way in which level-set functions, computed at each quantization level in an observation, serve as a basis for the invariant subspaces in RTUME. In the presence of observation noise and random sampling patterns of the point clouds, the observations do not lie strictly on the manifold and the resulting RTUME subspaces are noisy. Inspired by the ideas of Locality Preserving Projections and Grassmannian dimensionality reduction, we derive an optimal companding of the level-set functions yielding the Grassmannian dimensionality reduction universal manifold embedding (GDRUME). We evaluate the proposed method in a classification task on a noisy version of the ModelNet40 dataset and compare its performance to that of PointNet classification DNN. We show that in the presence of noise, GDRUME provides highly accurate classification results, while the performance of PointNet is poor.",
    "code_link": ""
  },
  "iccv2021_tag-cv_aunifiedframeworkfornon-negativematrixandtensorfactorisationswithasmoothedwassersteinloss": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "A Unified Framework for Non-Negative Matrix and Tensor Factorisations With a Smoothed Wasserstein Loss",
    "authors": [
      "Stephen Y. Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Zhang_A_Unified_Framework_for_Non-Negative_Matrix_and_Tensor_Factorisations_With_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Zhang_A_Unified_Framework_for_Non-Negative_Matrix_and_Tensor_Factorisations_With_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Non-negative matrix and tensor factorisations are a classical tool for finding low-dimensional representations of high-dimensional datasets. In applications such as imaging, datasets can be regarded as distributions supported on a space with metric structure. In such a setting, a loss function based on the Wasserstein distance of optimal transportation theory is a natural choice since it incorporates the underlying geometry of the data. We introduce a general mathematical framework for computing non-negative factorisations of both matrices and tensors with respect to an optimal transport loss. We derive an efficient computational method for its solution using a convex dual formulation, and demonstrate the applicability of this approach with several numerical illustrations with both matrix and tensor-valued data.",
    "code_link": ""
  },
  "iccv2021_tag-cv_multi-dimensionalscalingongroups": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "Multi-Dimensional Scaling on Groups",
    "authors": [
      "Mark Blumstein",
      "Henry Kvinge"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Blumstein_Multi-Dimensional_Scaling_on_Groups_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Blumstein_Multi-Dimensional_Scaling_on_Groups_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Leveraging the intrinsic symmetries in data for clear and efficient analysis is an important theme in data science. A basic example of this is the ubiquity of the discrete Fourier transform which arises from translational symmetry (i.e. time-delay/phase-shift). Particularly important in this area is understanding how symmetries inform the algorithms that we apply to our data. In this paper we explore the behavior of the dimensionality reduction algorithm multi-dimensional scaling (MDS) in the presence of symmetry. We show that understanding the properties of the underlying symmetry group allows us to make strong statements about the output of MDS even before applying the algorithm itself. In analogy to Fourier theory, we show that in some cases only a handful of fundamental \"frequencies\" (irreducible representations derived from the corresponding group) contribute information for the MDS Euclidean embedding.",
    "code_link": ""
  },
  "iccv2021_tag-cv_two-parameterpersistenceforimagesviadistancetransform": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "Two-Parameter Persistence for Images via Distance Transform",
    "authors": [
      "Chuan-Shen Hu",
      "Austin Lawson",
      "Yu-Min Chung",
      "Kaitlin Keegan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Hu_Two-Parameter_Persistence_for_Images_via_Distance_Transform_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Hu_Two-Parameter_Persistence_for_Images_via_Distance_Transform_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The distance transform of a binary image is a classic tool in computer vision and it has been widely used in the field of Topological Data Analysis (TDA) to study porous media. A common practice is to convert grayscale images to binary ones to apply the distance transform. In this work, by considering the threshold decomposition of a grayscale image, we prove that threshold decomposition and distance transform together to formulate a two-parameter filtration. This would offer the TDA community a concrete example to apply multi-parameter persistence on digital image analysis. We demonstrate our method on the firn dataset.",
    "code_link": ""
  },
  "iccv2021_tag-cv_amanifoldlearningbasedvideopredictionapproachfordeepmotiontransfer": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "A Manifold Learning Based Video Prediction Approach for Deep Motion Transfer",
    "authors": [
      "Yuliang Cai",
      "Sumit Mohan",
      "Adithya Niranjan",
      "Nilesh Jain",
      "Alex Cloninger",
      "Srinjoy Das"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Cai_A_Manifold_Learning_Based_Video_Prediction_Approach_for_Deep_Motion_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Cai_A_Manifold_Learning_Based_Video_Prediction_Approach_for_Deep_Motion_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We propose a novel manifold learning based end-to-end prediction and video synthesis framework for bandwidth reduction in motion transfer enabled applications such as video conferencing. In our workflow we use keypoint based representations of video frames where image and motion specific information are encoded in a completely unsupervised manner. Prediction of future keypoints is then performed using the manifold of a variational recurrent neural network (VRNN) following which output video frames are synthesized using an optical flow estimator and a conditional image generator in the motion transfer pipeline. The proposed architecture which combines keypoint based representation of video frames with manifold learning based prediction enables significant additional bandwidth savings over motion transfer based video conferencing systems which are implemented solely using keypoint detection. We demonstrate the superiority of our technique using two representative datasets for both video reconstruction and transfer and show that prediction using VRNN has superior performance as compared to a non manifold based technique such as RNN.",
    "code_link": ""
  },
  "iccv2021_tag-cv_theflagmanifoldasatoolforanalyzingandcomparingsetsofdatasets": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "The Flag Manifold as a Tool for Analyzing and Comparing Sets of Data Sets",
    "authors": [
      "Xiaofeng Ma",
      "Michael Kirby",
      "Chris Peterson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Ma_The_Flag_Manifold_as_a_Tool_for_Analyzing_and_Comparing_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Ma_The_Flag_Manifold_as_a_Tool_for_Analyzing_and_Comparing_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "The shape and orientation of data clouds reflect variability in observations that can confound pattern recognition systems. Subspace methods, utilizing Grassmann manifolds, have been a great aid in dealing with such variability. However, this usefulness begins to falter when the data cloud contains sufficiently many outliers corresponding to stray elements from another class or when the number of data points is larger than the number of features. We illustrate how nested subspace methods, utilizing flag manifolds, can help to deal with such additional confounding factors. Flag manifolds, which are parameter spaces for nested sequences of subspaces, are a natural geometric generalization of Grassmann manifolds. We utilize and extend known algorithms for determining the minimal length geodesic, the initial direction generating the minimal length geodesic, and the distance between any pair of points on a flag manifold. The approach is illustrated in the context of (hyper) spectral imagery showing the impact of ambient dimension, sample dimension, and flag structure.",
    "code_link": ""
  },
  "iccv2021_tag-cv_dualtransformationandmanifolddistancesvotingforoutlierrejectioninpointcloudregistration": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "Dual Transformation and Manifold Distances Voting for Outlier Rejection in Point Cloud Registration",
    "authors": [
      "Amit Efraim",
      "Joseph M. Francos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Efraim_Dual_Transformation_and_Manifold_Distances_Voting_for_Outlier_Rejection_in_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Efraim_Dual_Transformation_and_Manifold_Distances_Voting_for_Outlier_Rejection_in_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "We present a novel outlier rejection scheme for point cloud registration using SE(3) voting on local transformation estimates with a dual consensus constraint. Point cloud registration is commonly performed by matching key-points in both point clouds and estimating the transformation parameters from these matches. In the presented method, each putative matching pair of points is equipped with a local transformation estimate using the Rigid Transformation Universal Manifold Embedding. Putative matching pairs with similar local estimates are then clustered together and the global transformation between point clouds is estimated for each cluster. Finally, the cluster with the majority of the votes such that the average of local transformations agrees with its associated global transformation is selected for completing the registration. This approach successfully deals with up to 99.5% outliers where state of the art fails.",
    "code_link": ""
  },
  "iccv2021_tag-cv_sheavesasaframeworkforunderstandingandinterpretingmodelfit": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "TAG-CV",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Topology, Algebra, and Geometry in Computer Vision",
    "title": "Sheaves as a Framework for Understanding and Interpreting Model Fit",
    "authors": [
      "Henry Kvinge",
      "Brett Jefferson",
      "Cliff Joslyn",
      "Emilie Purvine"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/html/Kvinge_Sheaves_as_a_Framework_for_Understanding_and_Interpreting_Model_Fit_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/TAG-CV/papers/Kvinge_Sheaves_as_a_Framework_for_Understanding_and_Interpreting_Model_Fit_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "As data grows in size and complexity, finding frameworks which aid in interpretation and analysis has become critical. This is particularly true when data comes from complex systems where extensive structure is available, but must be drawn from peripheral sources. In this paper we argue that in such situations, sheaves can provide a natural framework to analyze how well a statistical model fits at the local level (that is, on subsets of related datapoints) vs the global level (on all the data). The sheaf-based approach that we propose is suitably general enough to be useful in a range of applications, from analyzing sensor networks to understanding the feature space of a deep learning model.",
    "code_link": ""
  },
  "iccv2021_cvamd_studyingtheeffectsofself-attentionformedicalimageanalysis": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "CVAMD",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Computer Vision for Automated Medical Diagnosis",
    "title": "Studying the Effects of Self-Attention for Medical Image Analysis",
    "authors": [
      "Adrit Rao",
      "Jongchan Park",
      "Sanghyun Woo",
      "Joon-Young Lee",
      "Oliver Aalami"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Rao_Studying_the_Effects_of_Self-Attention_for_Medical_Image_Analysis_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Rao_Studying_the_Effects_of_Self-Attention_for_Medical_Image_Analysis_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "When the trained physician interprets medical images, they understand the clinical importance of visual features. By applying cognitive attention, they apply greater focus onto clinically relevant regions while disregarding unnecessary features. The use of computer vision to automate the classification of medical images is widely studied. However, the standard convolutional neural network (CNN) does not necessarily employ subconscious feature relevancy evaluation techniques similar to the trained medical specialist and evaluates features more generally. Self-attention mechanisms enable CNNs to focus more on semantically important regions or aggregated relevant context with long-range dependencies. By using attention, medical image analysis systems can potentially become more robust by focusing on more important clinical feature regions. In this paper, we provide a comprehensive comparison of various state-of-the-art self-attention mechanisms across multiple medical image analysis tasks. Through both quantitative and qualitative evaluations along with a clinical user-centric survey study, we aim to provide a deeper understanding of the effects of self-attention in medical computer vision tasks.",
    "code_link": ""
  },
  "iccv2021_rlq_multipleganinversionforexemplar-basedimage-to-imagetranslation": {
    "conf_id": "ICCV2021",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2021_workshops - Real-World Computer Vision From Inputs With Limited Quality",
    "title": "Multiple GAN Inversion for Exemplar-Based Image-to-Image Translation",
    "authors": [
      "Taewon Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/html/Kang_Multiple_GAN_Inversion_for_Exemplar-Based_Image-to-Image_Translation_ICCVW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ICCV2021W/RLQ/papers/Kang_Multiple_GAN_Inversion_for_Exemplar-Based_Image-to-Image_Translation_ICCVW_2021_paper.pdf",
    "published": "2021-10",
    "summary": "Existing state-of-the-art techniques in exemplar-based image-to-image translation hold several critical concerns. Existing methods related to exemplar-based image-to-image translation are impossible to translate on an image tuple input (source, target) that is not aligned. Additionally, we can confirm that the existing method exhibits limited generalization ability to unseen images. In order to overcome this limitation, we propose Multiple GAN Inversion for Exemplar-based Image-to-Image Translation. Our novel Multiple GAN Inversion avoids human intervention by using a self-deciding algorithm to choose the number of layers using Frechet Inception Distance(FID), which selects more plausible image reconstruction results among multiple hypotheses without any training or supervision. Experimental results have in fact, shown the advantage of the proposed method compared to existing state-of-the-art exemplar-based image-to-image translation methods.",
    "code_link": ""
  }
}