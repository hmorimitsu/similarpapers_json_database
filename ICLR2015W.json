{
  "iclr2015_workshop_learningnon-deterministicrepresentationswithenergy-basedensembles": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Learning Non-deterministic Representations with Energy-based Ensembles",
    "authors": [
      "Maruan Al-Shedivat",
      "Emre Neftci",
      "Gert Cauwenberghs"
    ],
    "page_url": "http://arxiv.org/abs/1412.7272",
    "pdf_url": "https://arxiv.org/pdf/1412.7272",
    "published": "2015-05",
    "summary": "The goal of a generative model is to capture the distribution underlying the data, typically through latent variables. After training, these variables are often used as a new representation, more effective than the original features in a variety of learning tasks. However, the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space. Thus, even with representations robust to class-specific transformations, statistically driven models trained on them would not be able to generalize when the labeled data is scarce. Inspired by the stochasticity of the synaptic connections in the brain, we introduce Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic representations, i.e., mappings from the feature space to a family of distributions in the latent space. These mappings are encoded in a distribution over a (possibly infinite) collection of models. By conditionally sampling models from the ensemble, we obtain multiple representations for every input example and effectively augment the data. We propose an algorithm similar to contrastive divergence for training restricted Boltzmann stochastic ensembles. Finally, we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on MNIST. ",
    "code_link": ""
  },
  "iclr2015_workshop_diverseembeddingneuralnetworklanguagemodels": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Diverse Embedding Neural Network Language Models",
    "authors": [
      "Kartik Audhkhasi",
      "Abhinav Sethy",
      "Bhuvana Ramabhadran"
    ],
    "page_url": "http://arxiv.org/abs/1412.7063",
    "pdf_url": "https://arxiv.org/pdf/1412.7063",
    "published": "2015-05",
    "summary": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM. ",
    "code_link": ""
  },
  "iclr2015_workshop_hotswappingforonlineadaptationofoptimizationhyperparameters": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Hot Swapping for Online Adaptation of Optimization Hyperparameters",
    "authors": [
      "Kevin Bache",
      "Dennis Decoste",
      "Padhraic Smyth"
    ],
    "page_url": "http://arxiv.org/abs/1412.6599",
    "pdf_url": "https://arxiv.org/pdf/1412.6599",
    "published": "2015-05",
    "summary": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search. ",
    "code_link": ""
  },
  "iclr2015_workshop_representationlearningforcold-startrecommendation": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Representation Learning for cold-start recommendation",
    "authors": [
      "Gabriella Contardo",
      "Ludovic Denoyer",
      "Thierry Artieres"
    ],
    "page_url": "http://arxiv.org/abs/1412.7156",
    "pdf_url": "https://arxiv.org/pdf/1412.7156",
    "published": "2015-05",
    "summary": "A standard approach to Collaborative Filtering (CF), i.e. prediction of user ratings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunatly, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. This paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases. ",
    "code_link": ""
  },
  "iclr2015_workshop_trainingconvolutionalnetworkswithnoisylabels": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Training Convolutional Networks with Noisy Labels",
    "authors": [
      "Sainbayar Sukhbaatar",
      "Joan Bruna",
      "Manohar Paluri",
      "Lubomir Bourdev",
      "Rob Fergus"
    ],
    "page_url": "http://arxiv.org/abs/1406.2080",
    "pdf_url": "https://arxiv.org/pdf/1406.2080",
    "published": "2015-05",
    "summary": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark. ",
    "code_link": ""
  },
  "iclr2015_workshop_strivingforsimplicitytheallconvolutionalnet": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Striving for Simplicity:The All Convolutional Net",
    "authors": [
      "Alexey Dosovitskiy",
      "Jost Tobias Springenberg",
      "Thomas Brox",
      "Martin Riedmiller"
    ],
    "page_url": "http://arxiv.org/abs/1412.6806",
    "pdf_url": "https://arxiv.org/pdf/1412.6806",
    "published": "2015-05",
    "summary": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the deconvolution approach for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches. ",
    "code_link": ""
  },
  "iclr2015_workshop_learninglinearlyseparablefeaturesforspeechrecognitionusingconvolutionalneuralnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Learning linearly separable features for speech recognition using convolutional neural networks",
    "authors": [
      "Dimitri Palaz",
      "Mathew Magimai Doss",
      "Ronan Collobert"
    ],
    "page_url": "http://arxiv.org/abs/1412.7110",
    "pdf_url": "https://arxiv.org/pdf/1412.7110",
    "published": "2015-05",
    "summary": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input. ",
    "code_link": ""
  },
  "iclr2015_workshop_trainingdeepneuralnetworksonnoisylabelswithbootstrapping": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Training Deep Neural Networks on Noisy Labels with Bootstrapping",
    "authors": [
      "Scott Reed",
      "Honglak Lee",
      "Dragomir Anguelov",
      "Christian Szegedy",
      "Dumitru Erhan",
      "Andrew Rabinovich"
    ],
    "page_url": "http://arxiv.org/abs/1412.6596",
    "pdf_url": "https://arxiv.org/pdf/1412.6596",
    "published": "2015-05",
    "summary": "Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection. ",
    "code_link": ""
  },
  "iclr2015_workshop_onthestabilityofdeepnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " On the Stability of Deep Networks",
    "authors": [
      "Raja Giryes",
      "Guillermo Sapiro",
      "Alex Bronstein"
    ],
    "page_url": "http://arxiv.org/abs/1412.5896",
    "pdf_url": "https://arxiv.org/pdf/1412.5896",
    "published": "2015-05",
    "summary": "In this work we study the properties of deep neural networks (DNN) with random weights. We formally prove that these networks perform a distance-preserving embedding of the data. Based on this we then draw conclusions on the size of the training data and the networks' structure. A longer version of this paper with more results and details can be found in (Giryes et al., 2015). In particular, we formally prove in the longer version that DNN with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. ",
    "code_link": ""
  },
  "iclr2015_workshop_audiosourceseparationwithdiscriminativescatteringnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Audio source separation with Discriminative Scattering Networks ",
    "authors": [
      "Joan Bruna",
      "Yann LeCun",
      "Pablo Sprechmann"
    ],
    "page_url": "http://arxiv.org/abs/1412.7022",
    "pdf_url": "https://arxiv.org/pdf/1412.7022",
    "published": "2015-05",
    "summary": "In this report we describe an ongoing line of research for solving single-channel source separation problems. Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. The proposed representation consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms (CQT) with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations (NMF) that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discriminative training regime. We discuss several alternatives using different deep neural network architectures. ",
    "code_link": ""
  },
  "iclr2015_workshop_simpleimagedescriptiongeneratorviaalinearphrase-basedmodel": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Simple Image Description Generator via a Linear Phrase-Based Model",
    "authors": [
      "Pedro Pinheiro",
      "R\u00e9mi Lebret",
      "Ronan Collobert"
    ],
    "page_url": "http://arxiv.org/abs/1412.8419",
    "pdf_url": "https://arxiv.org/pdf/1412.8419",
    "published": "2015-05",
    "summary": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset. ",
    "code_link": ""
  },
  "iclr2015_workshop_ondistinguishabilitycriteriaforestimatinggenerativemodels": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " On Distinguishability Criteria for Estimating Generative Models",
    "authors": [
      "Ian Goodfellow"
    ],
    "page_url": "http://arxiv.org/abs/1412.6515",
    "pdf_url": "https://arxiv.org/pdf/1412.6515",
    "published": "2015-05",
    "summary": "Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classification. Noise-contrastive estimation (NCE) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples. Generative adversarial networks (GANs) are pairs of generator and discriminator networks, with the generator network learning to generate samples by attempting to fool the discriminator network into believing its samples are real data. Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum likelihood estimation (MLE). NCE corresponds to training an internal data model belonging to the {\\em discriminator} network but using a fixed generator network. We show that a variant of NCE, with a dynamic generator network, is equivalent to maximum likelihood estimation. Since pairing a learned discriminator with an appropriate dynamically selected generator recovers MLE, one might expect the reverse to hold for pairing a learned generator with a certain discriminator. However, we show that recovering MLE for a learned generator requires departing from the distinguishability game. Specifically: (i) The expected gradient of the NCE discriminator can be made to match the expected gradient of MLE, if one is allowed to use a non-stationary noise distribution for NCE, (ii) No choice of discriminator network can make the expected gradient for the GAN generator match that of MLE, and (iii) The existing theory does not guarantee that GANs will converge in the non-convex case. This suggests that the key next step in GAN research is to determine whether GANs converge, and if not, to modify their training algorithm to force convergence. ",
    "code_link": ""
  },
  "iclr2015_workshop_embeddingwordsimilaritywithneuralmachinetranslation": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Embedding Word Similarity with Neural Machine Translation",
    "authors": [
      "Felix Hill",
      "Kyunghyun Cho",
      "Sebastien Jean",
      "Coline Devin",
      "Yoshua Bengio"
    ],
    "page_url": "http://arxiv.org/abs/1412.6448",
    "pdf_url": "https://arxiv.org/pdf/1412.6448",
    "published": "2015-05",
    "summary": "Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness. ",
    "code_link": ""
  },
  "iclr2015_workshop_deepmetriclearningusingtripletnetwork": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Deep metric learning using Triplet network",
    "authors": [
      "Elad Hoffer",
      "Nir Ailon"
    ],
    "page_url": "http://arxiv.org/abs/1412.6622",
    "pdf_url": "https://arxiv.org/pdf/1412.6622",
    "published": "2015-05",
    "summary": "Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning. ",
    "code_link": ""
  },
  "iclr2015_workshop_understandingminimumprobabilityflowforrbmsundervariouskindsofdynamics": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Understanding Minimum Probability Flow for RBMs Under Various Kinds of Dynamics",
    "authors": [
      "Daniel Jiwoong Im",
      "Ethan Buchman",
      "Graham Taylor"
    ],
    "page_url": "http://arxiv.org/abs/1412.6617",
    "pdf_url": "https://arxiv.org/pdf/1412.6617",
    "published": "2015-05",
    "summary": "Energy-based models are popular in machine learning due to the elegance of their formulation and their relationship to statistical physics. Among these, the Restricted Boltzmann Machine (RBM), and its staple training algorithm contrastive divergence (CD), have been the prototype for some recent advancements in the unsupervised training of deep neural networks. However, CD has limited theoretical motivation, and can in some cases produce undesirable behavior. Here, we investigate the performance of Minimum Probability Flow (MPF) learning for training RBMs. Unlike CD, with its focus on approximating an intractable partition function via Gibbs sampling, MPF proposes a tractable, consistent, objective function defined in terms of a Taylor expansion of the KL divergence with respect to sampling dynamics. Here we propose a more general form for the sampling dynamics in MPF, and explore the consequences of different choices for these dynamics for training RBMs. Experimental results show MPF outperforming CD for various RBM configurations. ",
    "code_link": "https://github.com/jiwoongim/minimum"
  },
  "iclr2015_workshop_agrouptheoreticperspectiveonunsuperviseddeeplearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " A Group Theoretic Perspective on Unsupervised Deep Learning",
    "authors": [
      "Arnab Paul",
      "Suresh Venkatasubramanian"
    ],
    "page_url": "http://arxiv.org/abs/1504.02462",
    "pdf_url": "https://arxiv.org/pdf/1504.02462",
    "published": "2015-05",
    "summary": "Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning. One factor behind the recent resurgence of the subject is a key algorithmic step called {\\em pretraining}: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of {\\em shadow} groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the {\\em simplest}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper. ",
    "code_link": ""
  },
  "iclr2015_workshop_learninglongermemoryinrecurrentneuralnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Learning Longer Memory in Recurrent Neural Networks",
    "authors": [
      "Tomas Mikolov",
      "Armand Joulin",
      "Sumit Chopra",
      "Michael Mathieu",
      "Marc'Aurelio Ranzato"
    ],
    "page_url": "http://arxiv.org/abs/1412.7753",
    "pdf_url": "https://arxiv.org/pdf/1412.7753",
    "published": "2015-05",
    "summary": "Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory. We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997). ",
    "code_link": "https://github.com/facebook/SCRNNs"
  },
  "iclr2015_workshop_inducingsemanticrepresentationfromtextbyjointlypredictingandfactorizingrelations": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations",
    "authors": [
      "Ivan Titov",
      "Ehsan Khoddam"
    ],
    "page_url": "http://arxiv.org/abs/1412.6418",
    "pdf_url": "https://arxiv.org/pdf/1412.6418",
    "published": "2015-05",
    "summary": "In this work, we propose a new method to integrate two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language. ",
    "code_link": ""
  },
  "iclr2015_workshop_nicenon-linearindependentcomponentsestimation": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " NICE: Non-linear Independent Components Estimation",
    "authors": [
      "Laurent Dinh",
      "David Krueger",
      "Yoshua Bengio"
    ],
    "page_url": "http://arxiv.org/abs/1410.8516",
    "pdf_url": "https://arxiv.org/pdf/1410.8516",
    "published": "2015-05",
    "summary": "We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting. ",
    "code_link": ""
  },
  "iclr2015_workshop_discoveringhiddenfactorsofvariationindeepnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Discovering Hidden Factors of Variation in Deep Networks",
    "authors": [
      "Brian Cheung",
      "Jesse Livezey",
      "Arjun Bansal",
      "Bruno Olshausen"
    ],
    "page_url": "http://arxiv.org/abs/1412.6583",
    "pdf_url": "https://arxiv.org/pdf/1412.6583",
    "published": "2015-05",
    "summary": "Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classification. But there has been less exploration in learning the factors of variation apart from the classification signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly represent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate `hidden' variation in the supervised signal. ",
    "code_link": ""
  },
  "iclr2015_workshop_tailoringwordembeddingsforbilexicalpredictionsanexperimentalcomparison": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison",
    "authors": [
      "Pranava Swaroop Madhyastha",
      "Xavier Carreras",
      "Ariadna Quattoni"
    ],
    "page_url": "http://arxiv.org/abs/1412.7004",
    "pdf_url": "https://arxiv.org/pdf/1412.7004",
    "published": "2015-05",
    "summary": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks. ",
    "code_link": ""
  },
  "iclr2015_workshop_onlearningvectorrepresentationsinhierarchicallabelspaces": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " On Learning Vector Representations in Hierarchical Label Spaces",
    "authors": [
      "Jinseok Nam",
      "Johannes F\u00fcrnkranz"
    ],
    "page_url": "http://arxiv.org/abs/1412.6881",
    "pdf_url": "https://arxiv.org/pdf/1412.6881",
    "published": "2015-05",
    "summary": "An important problem in multi-label classification is to capture label patterns or underlying structures that have an impact on such patterns. This paper addresses one such problem, namely how to exploit hierarchical structures over labels. We present a novel method to learn vector representations of a label space given a hierarchy of labels and label co-occurrence patterns. Our experimental results demonstrate qualitatively that the proposed method is able to learn regularities among labels by exploiting a label hierarchy as well as label co-occurrences. It highlights the importance of the hierarchical information in order to obtain regularities which facilitate analogical reasoning over a label space. We also experimentally illustrate the dependency of the learned representations on the label hierarchy. ",
    "code_link": ""
  },
  "iclr2015_workshop_insearchoftherealinductivebiasontheroleofimplicitregularizationindeeplearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning",
    "authors": [
      "Behnam Neyshabur",
      "Ryota Tomioka",
      "Nathan Srebro"
    ],
    "page_url": "http://arxiv.org/abs/1412.6614",
    "pdf_url": "https://arxiv.org/pdf/1412.6614",
    "published": "2015-05",
    "summary": "We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning. ",
    "code_link": ""
  },
  "iclr2015_workshop_algorithmicrobustnessforsemi-supervised(\u03f5,\u03b3,\u03c4)-goodmetriclearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Algorithmic Robustness for Semi-Supervised (\u03f5, \u03b3, \u03c4)-Good Metric Learning",
    "authors": [
      "Maria-Irina Nicolae",
      "Marc Sebban",
      "Amaury Habrard",
      "\u00c9ric Gaussier",
      "Massih-Reza Amini"
    ],
    "page_url": "http://arxiv.org/abs/1412.6452",
    "pdf_url": "https://arxiv.org/pdf/1412.6452",
    "published": "2015-05",
    "summary": "The notion of metric plays a key role in machine learning problems such as classification, clustering or ranking. However, it is worth noting that there is a severe lack of theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a given metric. The theoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions (Balcan et al., 2008) has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classifier based on the algorithmic robustness framework. ",
    "code_link": ""
  },
  "iclr2015_workshop_real-worldfontrecognitionusingdeepnetworkanddomainadaptation": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Real-World Font Recognition Using Deep Network and Domain Adaptation",
    "authors": [
      "Zhangyang Wang",
      "Jianchao Yang",
      "Hailin Jin",
      "Eli Shechtman",
      "Aseem Agarwala",
      "Jon Brandt",
      "Thomas Huang"
    ],
    "page_url": "http://arxiv.org/abs/1504.00028",
    "pdf_url": "https://arxiv.org/pdf/1504.00028",
    "published": "2015-05",
    "summary": "We address a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This real-to-synthetic domain gap caused poor generalization to new real data in previous methods (Chen et al. (2014)). In this paper, we refer to Convolutional Neural Networks, and use an adaptation technique based on a Stacked Convolutional Auto-Encoder that exploits unlabeled real-world images combined with synthetic data. The proposed method achieves an accuracy of higher than 80% (top-5) on a real-world dataset. ",
    "code_link": ""
  },
  "iclr2015_workshop_scorefunctionfeaturesfordiscriminativelearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Score Function Features for Discriminative Learning",
    "authors": [
      "Majid Janzamin",
      "Hanie Sedghi",
      "Anima Anandkumar"
    ],
    "page_url": "http://arxiv.org/abs/1412.6514",
    "pdf_url": "https://arxiv.org/pdf/1412.6514",
    "published": "2015-05",
    "summary": "Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we consider a novel class of matrix and tensor-valued features, which can be pre-trained using unlabeled samples. We present efficient algorithms for extracting discriminative information, given these pre-trained features and labeled samples for any related task. Our class of features are based on higher-order score functions, which capture local variations in the probability density function of the input. We establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features, when used in conjunction with labeled samples. We employ efficient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations. Thus, we present a novel framework for employing generative models of the input for discriminative learning. ",
    "code_link": ""
  },
  "iclr2015_workshop_paralleltrainingofdnnswithnaturalgradientandparameteraveraging": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Parallel training of DNNs with Natural Gradient and Parameter Averaging",
    "authors": [
      "Daniel Povey",
      "Xioahui Zhang",
      "Sanjeev Khudanpur"
    ],
    "page_url": "http://arxiv.org/abs/1410.7455",
    "pdf_url": "https://arxiv.org/pdf/1410.7455",
    "published": "2015-05",
    "summary": "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine. ",
    "code_link": ""
  },
  "iclr2015_workshop_agenerativemodelfordeepconvolutionallearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " A Generative Model for Deep Convolutional Learning",
    "authors": [
      "Yunchen Pu",
      "Xin Yuan",
      "Lawrence Carin"
    ],
    "page_url": "http://arxiv.org/abs/1504.04054",
    "pdf_url": "https://arxiv.org/pdf/1504.04054",
    "published": "2015-05",
    "summary": "A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets. ",
    "code_link": ""
  },
  "iclr2015_workshop_randomforestscanhash": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Random Forests Can Hash",
    "authors": [
      "Qiang Qiu",
      "Guillermo Sapiro",
      "Alex Bronstein"
    ],
    "page_url": "http://arxiv.org/abs/1412.5083",
    "pdf_url": "https://arxiv.org/pdf/1412.5083",
    "published": "2015-05",
    "summary": "Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation, showing for the first time how random forest, a technique that together with deep learning have shown spectacular results in classification, can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks. ",
    "code_link": ""
  },
  "iclr2015_workshop_provablemethodsfortrainingneuralnetworkswithsparseconnectivity": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Provable Methods for Training Neural Networks with Sparse Connectivity",
    "authors": [
      "Hanie Sedghi",
      "Anima Anandkumar"
    ],
    "page_url": "http://arxiv.org/abs/1412.2693",
    "pdf_url": "https://arxiv.org/pdf/1412.2693",
    "published": "2015-05",
    "summary": "We provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage on the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be employed as effective initializers for gradient descent. ",
    "code_link": ""
  },
  "iclr2015_workshop_visualscenerepresentationssufficiency,minimality,invarianceandapproximationwithdeepconvolutionalnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Visual Scene Representations: sufficiency, minimality, invariance and approximation with deep convolutional networks",
    "authors": [
      "Stefano Soatto",
      "Alessandro Chiuso |"
    ],
    "page_url": "http://arxiv.org/abs/1411.7676",
    "pdf_url": "https://arxiv.org/pdf/1411.7676",
    "published": "2015-05",
    "summary": "Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization. ",
    "code_link": ""
  },
  "iclr2015_workshop_deeplearningwithelasticaveragingsgd": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Deep learning with Elastic Averaging SGD",
    "authors": [
      "Sixin Zhang",
      "Anna Choromanska",
      "Yann LeCun"
    ],
    "page_url": "http://arxiv.org/abs/1412.6651",
    "pdf_url": "https://arxiv.org/pdf/1412.6651",
    "published": "2015-05",
    "summary": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient. ",
    "code_link": ""
  },
  "iclr2015_workshop_exampleselectionfordictionarylearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Example Selection For Dictionary Learning",
    "authors": [
      "Tomoki Tsuchida",
      "Garrison Cottrell"
    ],
    "page_url": "http://arxiv.org/abs/1412.6177",
    "pdf_url": "https://arxiv.org/pdf/1412.6177",
    "published": "2015-05",
    "summary": "In unsupervised learning, an unbiased uniform sampling strategy is typically used, in order that the learned features faithfully encode the statistical structure of the training data. In this work, we explore whether active example selection strategies - algorithms that select which examples to use, based on the current estimate of the features - can accelerate learning. Specifically, we investigate effects of heuristic and saliency-inspired selection algorithms on the dictionary learning task with sparse activations. We show that some selection algorithms do improve the speed of learning, and we speculate on why they might work. ",
    "code_link": ""
  },
  "iclr2015_workshop_permutohedrallatticecnns": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Permutohedral Lattice CNNs",
    "authors": [
      "Martin Kiefel",
      "Varun Jampani",
      "Peter Gehler"
    ],
    "page_url": "http://arxiv.org/abs/1412.6618",
    "pdf_url": "https://arxiv.org/pdf/1412.6618",
    "published": "2015-05",
    "summary": "This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures. ",
    "code_link": ""
  },
  "iclr2015_workshop_unsuperviseddomainadaptationwithfeatureembeddings": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Unsupervised Domain Adaptation with Feature Embeddings",
    "authors": [
      "Yi Yang",
      "Jacob Eisenstein"
    ],
    "page_url": "http://arxiv.org/abs/1412.4385",
    "pdf_url": "https://arxiv.org/pdf/1412.4385",
    "published": "2015-05",
    "summary": "Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches often require the specification of pivot features that generalize across domains, which are selected by task-specific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems. ",
    "code_link": ""
  },
  "iclr2015_workshop_weaklysupervisedmulti-embeddingslearningofacousticmodels": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Weakly Supervised Multi-embeddings Learning of Acoustic Models",
    "authors": [
      "Gabriel Synnaeve",
      "Emmanuel Dupoux"
    ],
    "page_url": "http://arxiv.org/abs/1412.6645",
    "pdf_url": "https://arxiv.org/pdf/1412.6645",
    "published": "2015-05",
    "summary": "We trained a Siamese network with multi-task same/different information on a speech dataset, and found that it was possible to share a network for both tasks without a loss in performance. The first task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers. ",
    "code_link": ""
  },
  "iclr2015_workshop_learningactivationfunctionstoimprovedeepneuralnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Learning Activation Functions to Improve Deep Neural Networks",
    "authors": [
      "Forest Agostinelli",
      "Matthew Hoffman",
      "Peter Sadowski",
      "Pierre Baldi|"
    ],
    "page_url": "http://arxiv.org/abs/1412.6830",
    "pdf_url": "https://arxiv.org/pdf/1412.6830",
    "published": "2015-05",
    "summary": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes. ",
    "code_link": ""
  },
  "iclr2015_workshop_restrictedboltzmannmachineforclassificationwithhierarchicalcorrelatedprior": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Restricted Boltzmann Machine for Classification with Hierarchical Correlated Prior",
    "authors": [
      "Gang Chen",
      "Sargur Srihari"
    ],
    "page_url": "http://arxiv.org/abs/1406.3407",
    "pdf_url": "https://arxiv.org/pdf/1406.3407",
    "published": "2015-05",
    "summary": "Restricted Boltzmann machines (RBM) and its variants have become hot research topics recently, and widely applied to many classification problems, such as character recognition and document categorization. Often, classification RBM ignores the interclass relationship or prior knowledge of sharing information among classes. In this paper, we are interested in RBM with the hierarchical prior over classes. We assume parameters for nearby nodes are correlated in the hierarchical tree, and further the parameters at each node of the tree be orthogonal to those at its ancestors. We propose a hierarchical correlated RBM for classification problem, which generalizes the classification RBM with sharing information among different classes. In order to reduce the redundancy between node parameters in the hierarchy, we also introduce orthogonal restrictions to our objective function. We test our method on challenge datasets, and show promising results compared to competitive baselines. ",
    "code_link": ""
  },
  "iclr2015_workshop_learningdeepstructuredmodels": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Learning Deep Structured Models",
    "authors": [
      "Liang-Chieh Chen",
      "Alexander Schwing",
      "Alan Yuille",
      "Raquel Urtasun"
    ],
    "page_url": "http://arxiv.org/abs/1407.2538",
    "pdf_url": "https://arxiv.org/pdf/1407.2538",
    "published": "2015-05",
    "summary": "Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains. ",
    "code_link": ""
  },
  "iclr2015_workshop_n-gram-basedlow-dimensionalrepresentationfordocumentclassification": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " N-gram-Based Low-Dimensional Representation for Document Classification",
    "authors": [
      "R\u00e9mi Lebret",
      "Ronan Collobert"
    ],
    "page_url": "http://arxiv.org/abs/1412.6277",
    "pdf_url": "https://arxiv.org/pdf/1412.6277",
    "published": "2015-05",
    "summary": "The bag-of-words (BOW) model is the common approach for classifying documents, where words are used as feature for training a classifier. This generally involves a huge number of features. Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize documents in a lower dimension with the least semantic information loss. Some semantic information is nevertheless always lost, since only words are considered. Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space. Many approaches, such as the Skip-gram model, provide good word vector representations very quickly. We propose to average these representations to obtain representations of n-grams. All n-grams are thus embedded in a same semantic space. A K-means clustering can then group them into semantic concepts. The number of features is therefore dramatically reduced and documents can be represented as bag of semantic concepts. We show that this model outperforms LSA and LDA on a sentiment classification task, and yields similar results than a traditional BOW-model with far less features. ",
    "code_link": ""
  },
  "iclr2015_workshop_lowprecisionarithmeticfordeeplearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Low precision arithmetic for deep learning",
    "authors": [
      "Matthieu Courbariaux",
      "Yoshua Bengio",
      "Jean-Pierre David"
    ],
    "page_url": "http://arxiv.org/abs/1412.7024",
    "pdf_url": "https://arxiv.org/pdf/1412.7024",
    "published": "2015-05",
    "summary": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications. ",
    "code_link": "https://github.com/MatthieuCourbariaux/deep-learning-multipliers"
  },
  "iclr2015_workshop_theano-basedlarge-scalevisualrecognitionwithmultiplegpus": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Theano-based Large-Scale Visual Recognition with Multiple GPUs",
    "authors": [
      "Weiguang Ding",
      "Ruoyan Wang",
      "Fei Mao",
      "Graham Taylor"
    ],
    "page_url": "http://arxiv.org/abs/1412.2302",
    "pdf_url": "https://arxiv.org/pdf/1412.2302",
    "published": "2015-05",
    "summary": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date. ",
    "code_link": ""
  },
  "iclr2015_workshop_improvingzero-shotlearningbymitigatingthehubnessproblem": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Improving zero-shot learning by mitigating the hubness problem",
    "authors": [
      "Georgiana Dinu",
      "Marco Baroni"
    ],
    "page_url": "http://arxiv.org/abs/1412.6568",
    "pdf_url": "https://arxiv.org/pdf/1412.6568",
    "published": "2015-05",
    "summary": "The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains. ",
    "code_link": ""
  },
  "iclr2015_workshop_incorporatingbothdistributionalandrelationalsemanticsinwordrepresentations": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Incorporating Both Distributional and Relational Semantics in Word Representations",
    "authors": [
      "Daniel Fried",
      "Kevin Duh"
    ],
    "page_url": "http://arxiv.org/abs/1412.5836",
    "pdf_url": "https://arxiv.org/pdf/1412.5836",
    "published": "2015-05",
    "summary": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases. ",
    "code_link": ""
  },
  "iclr2015_workshop_variationalrecurrentauto-encoders": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Variational Recurrent Auto-Encoders",
    "authors": [
      "Otto Fabius",
      "Joost van Amersfoort"
    ],
    "page_url": "http://arxiv.org/abs/1412.6581",
    "pdf_url": "https://arxiv.org/pdf/1412.6581",
    "published": "2015-05",
    "summary": "In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state. ",
    "code_link": ""
  },
  "iclr2015_workshop_learningcompactconvolutionalneuralnetworkswithnesteddropout": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Learning Compact Convolutional Neural Networks with Nested Dropout",
    "authors": [
      "Chelsea Finn",
      "Lisa Anne Hendricks",
      "Trevor Darrell"
    ],
    "page_url": "http://arxiv.org/abs/1412.7155",
    "pdf_url": "https://arxiv.org/pdf/1412.7155",
    "published": "2015-05",
    "summary": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity. ",
    "code_link": ""
  },
  "iclr2015_workshop_compactpart-basedimagerepresentationsextremalcompetitionandovergeneralization": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Compact Part-Based Image Representations: Extremal Competition and Overgeneralization",
    "authors": [
      "Marc Goessling",
      "Yali Amit"
    ],
    "page_url": "http://arxiv.org/abs/1412.3708",
    "pdf_url": "https://arxiv.org/pdf/1412.3708",
    "published": "2015-05",
    "summary": "Learning compact and interpretable representations is a very natural task, which has not been solved satisfactorily even for simple binary datasets. In this paper, we review various ways of composing experts for binary data and argue that competitive forms of interaction are best suited to learn low-dimensional representations. We propose a new composition rule that discourages experts from focusing on similar structures and that penalizes opposing votes strongly so that abstaining from voting becomes more attractive. We also introduce a novel sequential initialization procedure, which is based on a process of oversimplification and correction. Experiments show that with our approach very intuitive models can be learned. ",
    "code_link": ""
  },
  "iclr2015_workshop_unsupervisedfeaturelearningfromtemporaldata": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Unsupervised Feature Learning from Temporal Data",
    "authors": [
      "Ross Goroshin",
      "Joan Bruna",
      "Jonathan Tompson",
      "David Eigen",
      "Yann LeCun"
    ],
    "page_url": "http://arxiv.org/abs/1504.02518",
    "pdf_url": "https://arxiv.org/pdf/1504.02518",
    "published": "2015-05",
    "summary": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric. ",
    "code_link": ""
  },
  "iclr2015_workshop_classifierwithhierarchicaltopographicalmapsasinternalrepresentation": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Classifier with Hierarchical Topographical Maps as Internal Representation",
    "authors": [
      "Pitoyo Hartono",
      "Paul Hollensen",
      "Thomas Trappenberg"
    ],
    "page_url": "http://arxiv.org/abs/1412.6567",
    "pdf_url": "https://arxiv.org/pdf/1412.6567",
    "published": "2015-05",
    "summary": "In this study we want to connect our previously proposed context-relevant topographical maps with the deep learning community. Our architecture is a classifier with hidden layers that are hierarchical two-dimensional topographical maps. These maps differ from the conventional self-organizing maps in that their organizations are influenced by the context of the data labels in a top-down manner. In this way bottom-up and top-down learning are combined in a biologically relevant representational learning setting. Compared to our previous work, we are here specifically elaborating the model in a more challenging setting compared to our previous experiments and to advance more hidden representation layers to bring our discussions into the context of deep representational learning. ",
    "code_link": ""
  },
  "iclr2015_workshop_entity-augmenteddistributionalsemanticsfordiscourserelations": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Entity-Augmented Distributional Semantics for Discourse Relations",
    "authors": [
      "Yangfeng Ji",
      "Jacob Eisenstein"
    ],
    "page_url": "http://arxiv.org/abs/1412.5673",
    "pdf_url": "https://arxiv.org/pdf/1412.5673",
    "published": "2015-05",
    "summary": "Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank. ",
    "code_link": ""
  },
  "iclr2015_workshop_flattenedconvolutionalneuralnetworksforfeedforwardacceleration": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Flattened Convolutional Neural Networks for Feedforward Acceleration",
    "authors": [
      "Jonghoon Jin",
      "Aysegul Dundar",
      "Eugenio Culurciello"
    ],
    "page_url": "http://arxiv.org/abs/1412.5474",
    "pdf_url": "https://arxiv.org/pdf/1412.5474",
    "published": "2015-05",
    "summary": "We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained. ",
    "code_link": ""
  },
  "iclr2015_workshop_gradualtrainingmethodfordenoisingautoencoders": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Gradual Training Method for Denoising Auto Encoders",
    "authors": [
      "Alexander Kalmanovich",
      "Gal Chechik"
    ],
    "page_url": "http://arxiv.org/abs/1504.02902",
    "pdf_url": "https://arxiv.org/pdf/1504.02902",
    "published": "2015-05",
    "summary": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets. ",
    "code_link": ""
  },
  "iclr2015_workshop_deepgazeiboostingsaliencypredictionwithfeaturemapstrainedonimagenet": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet",
    "authors": [
      "Matthias K\u00fcmmerer",
      "Lucas Theis",
      "Matthias Bethge"
    ],
    "page_url": "http://arxiv.org/abs/1411.1045",
    "pdf_url": "https://arxiv.org/pdf/1411.1045",
    "published": "2015-05",
    "summary": "Recent results suggest that state-of-the-art saliency models perform far from optimal in predicting fixations. This lack in performance has been attributed to an inability to model the influence of high-level image features such as objects. Recent seminal advances in applying deep neural networks to tasks like object recognition suggests that they are able to capture this kind of structure. However, the enormous amount of training data necessary to train these networks makes them difficult to apply directly to saliency prediction. We present a novel way of reusing existing neural networks that have been pretrained on the task of object recognition in models of fixation prediction. Using the well-known network of Krizhevsky et al. (2012), we come up with a new saliency model that significantly outperforms all state-of-the-art models on the MIT Saliency Benchmark. We show that the structure of this network allows new insights in the psychophysics of fixation selection and potentially their neural implementation. To train our network, we build on recent work on the modeling of saliency as point processes. ",
    "code_link": ""
  },
  "iclr2015_workshop_differencetargetpropagation": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Difference Target Propagation",
    "authors": [
      "Dong-Hyun Lee",
      "Saizheng Zhang",
      "Asja Fischer",
      "Antoine Biard",
      "Yoshua Bengio"
    ],
    "page_url": "http://arxiv.org/abs/1412.7525",
    "pdf_url": "https://arxiv.org/pdf/1412.7525",
    "published": "2015-05",
    "summary": "Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of nonlinearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks. ",
    "code_link": "https://github.com/donghyunlee/dtp"
  },
  "iclr2015_workshop_predictiveencodingofcontextualrelationshipsforperceptualinference,interpolationandprediction": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Predictive encoding of contextual relationships for perceptual inference, interpolation and prediction",
    "authors": [
      "Mingmin Zhao",
      "Chengxu Zhuang",
      "Yizhou Wang",
      "Tai Sing Lee"
    ],
    "page_url": "http://arxiv.org/abs/1411.3815",
    "pdf_url": "https://arxiv.org/pdf/1411.3815",
    "published": "2015-05",
    "summary": "We propose a new neurally-inspired model that can learn to encode the global relationship context of visual events across time and space and to use the contextual information to modulate the analysis by synthesis process in a predictive coding framework. The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes. In contrast to standard predictive coding models, the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer, and is thus more consistent with neurophysiological observations. We establish the computational feasibility of this model by demonstrating its ability in several aspects. We show that our model can outperform state-of-art performances of gated Boltzmann machines (GBM) in estimation of contextual information. Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information. We show it achieves state-of-art performances in terms of prediction accuracy in a variety of tasks and possesses the ability to interpolate missing frames, a function that is lacking in GBM. ",
    "code_link": ""
  },
  "iclr2015_workshop_purineabi-graphbaseddeeplearningframework": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Purine: A Bi-Graph based deep learning framework",
    "authors": [
      "Min Lin",
      "Shuo Li",
      "Xuan Luo",
      "Shuicheng Yan"
    ],
    "page_url": "http://arxiv.org/abs/1412.6249",
    "pdf_url": "https://arxiv.org/pdf/1412.6249",
    "published": "2015-05",
    "summary": "In this paper, we introduce a novel deep learning framework, termed Purine. In Purine, a deep network is expressed as a bipartite graph (bi-graph), which is composed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. This eases researchers from coding for various parallelization schemes, and the same dispatcher can be used for solving variant graphs. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduce the communication overhead and help us achieve approximate linear acceleration. ",
    "code_link": ""
  },
  "iclr2015_workshop_pixel-wisedeeplearningforcontourdetection": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Pixel-wise Deep Learning for Contour Detection",
    "authors": [
      "Jyh-Jing Hwang",
      "Tyng-Luh Liu"
    ],
    "page_url": "http://arxiv.org/abs/1504.01989",
    "pdf_url": "https://arxiv.org/pdf/1504.01989",
    "published": "2015-05",
    "summary": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and verify their performance on BSDS500. ",
    "code_link": ""
  },
  "iclr2015_workshop_ensembleofgenerativeanddiscriminativetechniquesforsentimentanalysisofmoviereviews": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews",
    "authors": [
      "Gr\u00e9goire Mesnil",
      "Tomas Mikolov",
      "Marc'Aurelio Ranzato",
      "Yoshua Bengio"
    ],
    "page_url": "http://arxiv.org/abs/1412.5335",
    "pdf_url": "https://arxiv.org/pdf/1412.5335",
    "published": "2015-05",
    "summary": "Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several ma- chine learning approaches to this problem, and combine them to achieve the best possible results. We show how to use for this task the standard generative lan- guage models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort. ",
    "code_link": ""
  },
  "iclr2015_workshop_fastlabelembeddingsforextremelylargeoutputspaces": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Fast Label Embeddings for Extremely Large Output Spaces",
    "authors": [
      "Paul Mineiro",
      "Nikos Karampatziakis"
    ],
    "page_url": "http://arxiv.org/abs/1503.08873",
    "pdf_url": "https://arxiv.org/pdf/1503.08873",
    "published": "2015-05",
    "summary": "Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results. ",
    "code_link": ""
  },
  "iclr2015_workshop_ananalysisofunsupervisedpre-traininginlightofrecentadvances": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " An Analysis of Unsupervised Pre-training in Light of Recent Advances",
    "authors": [
      "Tom Paine",
      "Pooya Khorrami",
      "Wei Han",
      "Thomas Huang"
    ],
    "page_url": "http://arxiv.org/abs/1412.6597",
    "pdf_url": "https://arxiv.org/pdf/1412.6597",
    "published": "2015-05",
    "summary": "Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10. ",
    "code_link": ""
  },
  "iclr2015_workshop_fullyconvolutionalmulti-classmultipleinstancelearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Fully Convolutional Multi-Class Multiple Instance Learning",
    "authors": [
      "Deepak Pathak",
      "Evan Shelhamer",
      "Jonathan Long",
      "Trevor Darrell"
    ],
    "page_url": "http://arxiv.org/abs/1412.7144",
    "pdf_url": "https://arxiv.org/pdf/1412.7144",
    "published": "2015-05",
    "summary": "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge. ",
    "code_link": ""
  },
  "iclr2015_workshop_whatdodeepcnnslearnaboutobjects?": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " What Do Deep CNNs Learn About Objects?",
    "authors": [
      "Xingchao Peng",
      "Baochen Sun",
      "Karim Ali",
      "Kate Saenko"
    ],
    "page_url": "http://arxiv.org/abs/1504.02485",
    "pdf_url": "https://arxiv.org/pdf/1504.02485",
    "published": "2015-05",
    "summary": "Deep convolutional neural networks learn extremely powerful image representations, yet most of that power is hidden in the millions of deep-layer parameters. What exactly do these parameters represent? Recent work has started to analyse CNN representations, finding that, e.g., they are invariant to some 2D transformations Fischer et al. (2014), but are confused by particular types of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how invariant are CNNs to object-class variations caused by 3D shape, pose, and photorealism? ",
    "code_link": ""
  },
  "iclr2015_workshop_representationusingtheweyltransform": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Representation using the Weyl Transform",
    "authors": [
      "Qiang Qiu",
      "Andrew Thompson",
      "Robert Calderbank",
      "Guillermo Sapiro"
    ],
    "page_url": "http://arxiv.org/abs/1412.6134",
    "pdf_url": "https://arxiv.org/pdf/1412.6134",
    "published": "2015-05",
    "summary": "The Weyl transform is introduced as a rich framework for data representation. Transform coefficients are connected to the Walsh-Hadamard transform of multiscale autocorrelations, and different forms of dyadic periodicity in a signal are shown to appear as different features in its Weyl coefficients. The Weyl transform has a high degree of symmetry with respect to a large group of multiscale transformations, which allows compact yet discriminative representations to be obtained by pooling coefficients. The effectiveness of the Weyl transform is demonstrated through the example of textured image classification. ",
    "code_link": ""
  },
  "iclr2015_workshop_denoisingautoencoderwithmodulatedlateralconnectionslearnsinvariantrepresentationsofnaturalimages": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Denoising autoencoder with modulated lateral connections learns invariant representations of natural images",
    "authors": [
      "Antti Rasmus",
      "Harri Valpola",
      "Tapani Raiko"
    ],
    "page_url": "http://arxiv.org/abs/1412.7210",
    "pdf_url": "https://arxiv.org/pdf/1412.7210",
    "published": "2015-05",
    "summary": "Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings. ",
    "code_link": ""
  },
  "iclr2015_workshop_towardsdeepneuralnetworkarchitecturesrobusttoadversarialexamples": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Towards Deep Neural Network Architectures Robust to Adversarial Examples",
    "authors": [
      "Shixiang Gu",
      "Luca Rigazio"
    ],
    "page_url": "http://arxiv.org/abs/1412.5068",
    "pdf_url": "https://arxiv.org/pdf/1412.5068",
    "published": "2015-05",
    "summary": "Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty. ",
    "code_link": ""
  },
  "iclr2015_workshop_explorationsonhighdimensionallandscapes": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Explorations on high dimensional landscapes",
    "authors": [
      "Levent Sagun",
      "Ugur Guney",
      "Yann LeCun"
    ],
    "page_url": "http://arxiv.org/abs/1412.6615",
    "pdf_url": "https://arxiv.org/pdf/1412.6615",
    "published": "2015-05",
    "summary": "Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps. ",
    "code_link": ""
  },
  "iclr2015_workshop_generativeclass-conditionalautoencoders": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Generative Class-conditional Autoencoders",
    "authors": [
      "Jan Rudy",
      "Graham Taylor"
    ],
    "page_url": "http://arxiv.org/abs/1412.7009",
    "pdf_url": "https://arxiv.org/pdf/1412.7009",
    "published": "2015-05",
    "summary": "Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efficient sampling from conditional distributions, we extend this work, both theoretically and algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets. ",
    "code_link": ""
  },
  "iclr2015_workshop_attentionforfine-grainedcategorization": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Attention for Fine-Grained Categorization",
    "authors": [
      "Pierre Sermanet",
      "Andrea Frome",
      "Esteban Real"
    ],
    "page_url": "http://arxiv.org/abs/1412.7054",
    "pdf_url": "https://arxiv.org/pdf/1412.7054",
    "published": "2015-05",
    "summary": "This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input. ",
    "code_link": ""
  },
  "iclr2015_workshop_abaselineforvisualinstanceretrievalwithdeepconvolutionalnetworks": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " A Baseline for Visual Instance Retrieval with Deep Convolutional Networks",
    "authors": [
      "Ali Sharif Razavian",
      "Josephine Sullivan",
      "Atsuto Maki",
      "Stefan Carlsson"
    ],
    "page_url": "http://arxiv.org/abs/1412.6574",
    "pdf_url": "https://arxiv.org/pdf/1412.6574",
    "published": "2015-05",
    "summary": "This paper provides an extensive study on the availability of image representations based on convolutional networks (ConvNets) for the task of visual instance retrieval. Besides the choice of convolutional layers, we present an efficient pipeline exploiting multi-scale schemes to extract local features, in particular, by taking geometric invariance into explicit account, i.e. positions, scales and spatial consistency. In our experiments using five standard image retrieval datasets, we demonstrate that generic ConvNet image representations can outperform other state-of-the-art methods if they are extracted appropriately. ",
    "code_link": ""
  },
  "iclr2015_workshop_visualscenerepresentationscalingandocclusion": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Visual Scene Representation: Scaling and Occlusion",
    "authors": [
      "Stefano Soatto",
      "Jingming Dong",
      "Nikolaos Karianakis"
    ],
    "page_url": "http://arxiv.org/abs/1412.6607",
    "pdf_url": "https://arxiv.org/pdf/1412.6607",
    "published": "2015-05",
    "summary": "We study the structure of representations, defined as approximations of minimal sufficient statistics that are maximal invariants to nuisance factors, for visual data subject to scaling and occlusion of line-of-sight. We derive analytical expressions for such representations and show that, under certain restrictive assumptions, they are related to features commonly in use in the computer vision community. This link highlights the condition tacitly assumed by these descriptors, and also suggests ways to improve and generalize them. This new interpretation draws connections to the classical theories of sampling, hypothesis testing and group invariance. ",
    "code_link": ""
  },
  "iclr2015_workshop_deepnetworkswithlargeoutputspaces": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Deep networks with large output spaces",
    "authors": [
      "Sudheendra Vijayanarasimhan",
      "Jon Shlens",
      "Jay Yagnik",
      "Rajat Monga"
    ],
    "page_url": "http://arxiv.org/abs/1412.7479",
    "pdf_url": "https://arxiv.org/pdf/1412.7479",
    "published": "2015-05",
    "summary": "Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods. ",
    "code_link": ""
  },
  "iclr2015_workshop_efficientexactgradientupdatefortrainingdeepnetworkswithverylargesparsetargets": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets",
    "authors": [
      "Pascal Vincent"
    ],
    "page_url": "http://arxiv.org/abs/1412.7091",
    "pdf_url": "https://arxiv.org/pdf/1412.7091",
    "published": "2015-05",
    "summary": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d^2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture. ",
    "code_link": ""
  },
  "iclr2015_workshop_self-informedneuralnetworkstructurelearning": {
    "conf_id": "ICLR2015",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2015_workshop",
    "title": " Self-informed neural network structure learning",
    "authors": [
      "David Warde-Farley",
      "Andrew Rabinovich",
      "Dragomir Anguelov"
    ],
    "page_url": "http://arxiv.org/abs/1412.6563",
    "pdf_url": "https://arxiv.org/pdf/1412.6563",
    "published": "2015-05",
    "summary": "We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%. ",
    "code_link": ""
  }
}