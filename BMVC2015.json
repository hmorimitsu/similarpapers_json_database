{
  "bmvc2015_main_onlinedomainadaptationformulti-objecttracking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Online Domain Adaptation for Multi-Object Tracking",
    "authors": [
      "Adrien Gaidon",
      "Eleonora Vig"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper003/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper003/paper003.pdf",
    "published": "2015-09",
    "summary": "Automatically detecting, labeling, and tracking objects in videos depends first and foremost on accurate category-level object detectors. These might, however, not always be available in practice, as acquiring high-quality large scale labeled training datasets is either too costly or impractical for all possible real-world application scenarios. A scalable solution consists in re-using object detectors pre-trained on generic datasets. This work is the first to investigate the problem of on-line domain adaptation of object detectors for causal multi-object tracking (MOT). We propose to alleviate the dataset bias by adapting detectors from category to instances, and back: (i) we jointly learn all target models by adapting them from the pre-trained one, and (ii) we also adapt the pre-trained model on-line. We introduce an on-line multi-task learning algorithm to efficiently share parameters and reduce drift, while gradually improving recall. Our approach is applicable to any linear object detector, and we evaluate both cheap 'mini-Fisher Vectors' and expensive 'off-the-shelf' ConvNet features. We quantitatively measure the benefit of our domain adaptation strategy on the KITTI tracking benchmark and on a new dataset (PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT."
  },
  "bmvc2015_main_learningoptimalparametersformulti-targettracking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Learning Optimal Parameters For Multi-target Tracking",
    "authors": [
      "Shaofei Wang",
      "Charless Fowlkes"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper004/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper004/paper004.pdf",
    "published": "2015-09",
    "summary": "We describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about co-occurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. Under our learning framework, we evaluate two different approaches to finding an optimal set of tracks under quadratic model objective based on an LP relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions. We find the greedy algorithm achieves almost equivalent accuracy to the LP relaxation while being 2-7x faster than a commercial solver. We evaluate trained models on the challenging MOT and KITTI benchmarks. Surprisingly, we find that with proper parameter learning, our simple data-association model without explicit appearance/motion reasoning is able to outperform many state-of-the-art methods that use far more complex motion features and affinity metric learning."
  },
  "bmvc2015_main_r-cnnminusr": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "R-CNN minus R",
    "authors": [
      "Karel Lenc",
      "Andrea Vedaldi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper005/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper005/paper005.pdf",
    "published": "2015-09",
    "summary": "Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding. In object category detection, however, the best results have been obtained by techniques such as R(egion)-CNN that combine CNNs with cues from image segmentation, using techniques such as selective search to propose possible object locations in images. However, the role of segmentation in CNN detectors remains controversial. On the one hand, segmentation may be a necessary modelling component, carrying essential geometric information not contained in the CNN; on the other hand, it may be merely a way of accelerating detection, by focusing the CNN classifier on promising image areas. In this paper, we answer this question by developing a detector that uses a trivial region generation scheme, constant for each image. While such region proposals approximate objects poorly, we show that a bounding box regressor using intermediate convolutional features can recover sufficiently accurate bounding boxes, demonstrating that, indeed, the required geometric information is contained in the CNN itself. Combined with convolutional feature pooling, we also obtain an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection."
  },
  "bmvc2015_main_convolutionalneuralnetworksfordirecttextdeblurring": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Convolutional Neural Networks for Direct Text Deblurring",
    "authors": [
      "Michal Hradi\u0161",
      "Jan Kotera",
      "Pavel Zem\u010d\u00edk",
      "Filip \u0160roubek"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper006/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper006/paper006.pdf",
    "published": "2015-09",
    "summary": "In this work we address the problem of blind deconvolution and denoising. We focus on restoration of text documents and we show that this type of highly structured data can be successfully restored by a convolutional neural network. The networks are trained to reconstruct high-quality images directly from blurry inputs without assuming any specific blur and noise models. We demonstrate the performance of the convolutional networks on a large set of text documents and on a combination of realistic de-focus and camera shake blur kernels. On this artificial data, the convolutional networks significantly outperform existing blind deconvolution methods, including those optimized for text, in terms of image quality and OCR accuracy. In fact, the networks outperform even state-of-the-art non-blind methods for anything but the lowest noise levels. The approach is validated on real photos taken by various devices."
  },
  "bmvc2015_main_sketch-a-netthatbeatshumans": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Sketch-a-Net that Beats Humans",
    "authors": [
      "Qian Yu",
      "Yongxin Yang",
      "Yi-Zhe Song",
      "Tao Xiang",
      "Timothy Hospedales"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper007/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper007/paper007.pdf",
    "published": "2015-09",
    "summary": "We propose a multi-scale multi-channel deep neural network framework that, for the \ufb01rst time, yields sketch recognition performance surpassing that of humans. Our superior performance is a result of explicitly embedding the unique characteristics of sketches in our model: (i) a network architecture designed for sketch rather than natural photo statistics, (ii) a multi-channel generalisation that encodes sequential ordering in the sketching process, and (iii) a multi-scale network ensemble with joint Bayesian fusion that accounts for the different levels of abstraction exhibited in free-hand sketches. We show that state-of-the art deep networks speci\ufb01cally engineered for photos of natural objects fail to perform well on sketch recognition, regardless whether they are trained using photo or sketch. Our network on the other hand not only delivers the best performance on the largest human sketch dataset to date, but also is small in size making ef\ufb01cient training possible using just CPUs."
  },
  "bmvc2015_main_learningdeeprepresentationsofappearanceandmotionforanomalouseventdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Learning Deep Representations of Appearance and Motion for Anomalous Event Detection",
    "authors": [
      "Dan Xu",
      "Elisa Ricci",
      "Yan Yan",
      "Jingkuan Song",
      "Nicu Sebe"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper008/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper008/paper008.pdf",
    "published": "2015-09",
    "summary": "We present a novel unsupervised deep learning framework for anomalous event detection in complex video scenes. While most existing works merely use hand-crafted appearance and motion features, we propose Appearance and Motion DeepNet (AMDN) which utilizes deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining both the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Based on the learned representations, multiple one-class SVM models are used to predict the anomaly scores of each input, which are then integrated with a late fusion strategy for final anomaly detection. We evaluate the proposed method on two publicly available video surveillance datasets, showing competitive performance with respect to state of the art approaches."
  },
  "bmvc2015_main_deepperceptualmappingforthermaltovisiblefacerecogntion": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Deep Perceptual Mapping for Thermal to Visible Face Recogntion",
    "authors": [
      "M. Saquib Sarfraz",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper009/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper009/paper009.pdf",
    "published": "2015-09",
    "summary": "Cross modal face matching between the thermal and visible spectrum is a much desired capability for night-time surveillance and security applications. Due to a very large modality gap, thermal-visible face recognition is one of the most challenging face matching problem. In this paper, we present an approach to bridge this modality gap by a significant margin. Our approach captures the highly non-linear relationship between the two modalities by using a deep neural network. Our model attempts to learn a non-linear mapping from visible to thermal spectrum while preserving the identity information. We show substantive performance improvement on a difficult thermal-visible face dataset. The presented approach improves the state-of-the-art by more than 10% in terms of Rank-1 identification and bridge the drop in performance due to the modality gap by more than 40%."
  },
  "bmvc2015_main_anefficientalgorithmforlearningdistancesthatobeythetriangleinequality": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "An Efficient Algorithm for Learning Distances that Obey the Triangle Inequality",
    "authors": [
      "Arijit Biswas",
      "David Jacobs"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper010/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper010/paper010.pdf",
    "published": "2015-09",
    "summary": "Semi-supervised clustering improves performance using constraints that indicate if two images belong to the same category or not. Success depends on how effectively these constraints can be propagated to the unsupervised data. Many algorithms use these constraints to learn Euclidean distances in a vector space. However, distances between images are often computed using classifiers or combinatorial algorithms that make distance learning difficult. In such a setting, we propose to use the triangle inequality to propagate constraints to unsupervised data. First, we formulate distance learning as a metric nearness problem where a brute-force Quadratic Program (QP) is used to modify the distances such that the total change in distances is minimized but the final distances obey the triangle inequality. Then we propose a much faster version of the QP that enforces only a subset of the inequalities and can be applied to real world clustering datasets. We show experimentally that this efficient QP produces stronger clustering results on face, leaf and video image datasets, outperforming state-of-the-art methods for constrained clustering. To gain insight into the effectiveness of this algorithm, we analyze a special case of the semi-supervised clustering problem, and show that the subset of constraints that we sample still preserves key properties of the distances that would be produced by enforcing all constraints."
  },
  "bmvc2015_main_diagnosingstate-of-the-artobjectproposalmethods": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Diagnosing state-of-the-art object proposal methods",
    "authors": [
      "Hongyuan Zhu",
      "Shijian Lu",
      "Jianfei Cai",
      "Guangqing Lee"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper011/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper011/paper011.pdf",
    "published": "2015-09",
    "summary": "bject proposal has become a popular paradigm to replace exhaustive sliding window search in current top-performing methods in PASCAL VOC and ImageNet. Recently, Hosang et al. [17] conduct the first unified study of existing methods\u2019 in terms of various image-level degradations. On the other hand, the vital question 'what objectlevel characteristics really affect existing methods\u2019 performance?' is not yet answered. Inspired by Hoiem et al.\u2019s work in categorical object detection, this paper conducts the first meta-analysis of various object-level characteristics\u2019 impact on state-of-the-art object proposal methods. Specifically, we examine the effects of object size, aspect ratio, iconic view, color contrast, shape regularity and texture. We also analyse existing methods\u2019 localization accuracy and latency for various PASCAL VOC object classes. Our study reveals the limitations of existing methods in terms of non-iconic view, small object size, low color contrast, shape regularity etc. Based on our observations, lessons are also learned and shared with respect to the selection of existing object proposal technologies as well as the design of the future ones"
  },
  "bmvc2015_main_wxbswidebaselinestereogeneralizations": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "WxBS: Wide Baseline Stereo Generalizations",
    "authors": [
      "Dmytro Mishkin",
      "Jiri Matas",
      "Michal Perdoch",
      "Karel Lenc"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper012/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper012/paper012.pdf",
    "published": "2015-09",
    "summary": "We present a generalization of the wide baseline two view matching problem - WxBS, where x stands for a different subset of 'wide baselines' in acquisition conditions such as geometry, illumination, sensor and appearance. We introduce a novel dataset of ground-truthed image pairs which include multiple 'wide baselines' and show that state-of-the-art matchers fail on almost all image pairs from the set. A novel matching algorithm for addressing the WxBS problem is introduced and we show experimentally that the WxBS-M matcher dominates the state-of-the-art methods both on the new and existing datasets."
  },
  "bmvc2015_main_planarshapedecompositionmadesimple": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Planar shape decomposition made simple",
    "authors": [
      "Nikos Papanelopoulos",
      "Yannis Avrithis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper013/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper013/paper013.pdf",
    "published": "2015-09",
    "summary": "We present a very simple computational model for planar shape decomposition that naturally captures most of the rules and salience measures suggested by psychophysical studies, including the minima and short-cut rules, convexity, and symmetry. It is based on a medial axis representation in ways that have not been explored before and sheds more light into the connection between existing rules like minima and convexity. In particular, vertices of the exterior medial axis directly provide the position and extent of negative minima of curvature, while a traversal of the interior medial axis directly provides a small set of candidate endpoints for part-cuts. The final selection follows a simple local convexity rule that can incorporate arbitrary salience measures. Neither global optimization nor differentiation is involved. We provide qualitative and quantitative evaluation and comparisons on ground-truth data from psychophysical experiments."
  },
  "bmvc2015_main_adaptationofsyntheticdataforcoarse-to-fineviewpointrefinement": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Adaptation of Synthetic Data for Coarse-to-Fine Viewpoint Refinement",
    "authors": [
      "Pau Panareda Busto",
      "Joerg Liebelt",
      "Juergen Gall"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper014/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper014/paper014.pdf",
    "published": "2015-09",
    "summary": "The quality of learning-based pose estimation still heavily relies on manual training data annotations. However, the manual labeling of large datasets is costly and frequently limited to a few coarse viewpoint annotations of varying accuracy. In this work, we propose to refine such coarse pose annotations with a domain adaptation approach, where the source domain consists of fine-grained pose annotations generated from synthetic computer graphics models, and the target domain of coarse manual pose annotations of a real dataset. Our domain adaptation step computes a linear map which aligns corresponding samples from the two domains and allows for the refinement of the manual pose labels using the transformed synthetic ones. Experiments show that we significantly improve pose estimation on several state-of-the-art datasets."
  },
  "bmvc2015_main_saliencypredictionwithactivesemanticsegmentation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Saliency Prediction with Active Semantic Segmentation",
    "authors": [
      "Ming Jiang",
      "Xavier Boix",
      "Juan Xu",
      "Gemma Roig",
      "Luc  Van Gool",
      "Qi Zhao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper015/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper015/paper015.pdf",
    "published": "2015-09",
    "summary": "Semantic-level features have been shown to provide a strong cue for predicting eye fixations. They are usually implemented by evaluating object classifiers everywhere in the image. As a result, extracting the semantic-level features may become a computational bottleneck that may limit the applicability of saliency prediction in real-time applications. In this paper, to reduce the computational cost at the semantic level, we introduce a saliency prediction model based on active semantic segmentation, where a set of new features are extracted during the progressive extraction of the semantic labeling. We recorded eye fixations on all the images of the popular MSRC-21 and VOC07 datasets. Experiments in this new dataset demonstrate that the semantic-level features extracted from active semantic segmentation improve the saliency prediction from low- and regional-level features, and it allows controlling the computational overhead of adding semantics to the saliency predictor."
  },
  "bmvc2015_main_usingnear-fieldlightsourcestoseparateilluminationfrombrdf": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Using Near-Field Light Sources to Separate Illumination from BRDF",
    "authors": [
      "Jeroen Put",
      "Nick Michiels",
      "Philippe Bekaert"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper016/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper016/paper016.pdf",
    "published": "2015-09",
    "summary": "Simultaneous estimation of lighting and BRDF from multi-view images is an interesting problem in computer vision. It allows for exciting applications, such as flexible relighting in post-production, without recapturing the scene. Unfortunately, the estimation problem is made difficult because lighting and BRDF have closely entangled effects in the input images. This paper presents an algorithm to support both the estimation of distant and near-field illumination. Previous techniques are limited to distant lighting. We contribute by proposing an additional factorization of the lighting, while keeping the rendering efficient and additional data compactly stored in the wavelet domain. We reduce complexity by clustering the scene geometry into a few groups of important emitters and calculate the emitting powers by alternately solving for illumination and reflectance. We demonstrate our work on a synthetic and real datasets and show that a clean separation of distant and near-field illumination leads to a more accurate estimation and separation of lighting and BRDF."
  },
  "bmvc2015_main_multi-scalegraph-basedguidedfilterforde-noisingcryo-electrontomographicdata": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multi-scale Graph-based Guided Filter for De-noising Cryo-Electron Tomographic Data",
    "authors": [
      "Shadi Albarqouni",
      "Maximilian Baust",
      "Sailesh Conjeti",
      "Asharf Al-Amoudi",
      "Nassir Navab"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper017/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper017/paper017.pdf",
    "published": "2015-09",
    "summary": "Cryo-Electron Tomography is a leading imaging technique in structural biology, which is capable of acquiring two-dimensional projections of cellular structures at high resolution and close-to-native state. Due to the limited electron dose the resulting projections exhibit extremely low SNR and contrast. The 3D structure is then reconstructed and passed through a number of post-processing steps including de-noising and sub-tomogram averaging to provide a better understanding and interpretation. As CET is mainly used for imaging fine scale structures, any denoising method applied to CET images should be scale selective and in particular be able to preserve such fine scale structures. In this context, we propose a new denoising framework based on regularized graph spectral filtering with a full control of scale-space and global consistency. Using the gold-standard metrics, we show that our denoising algorithm significantly outperforms the state-of-the-art methods such as NAD, NLM and RGF in terms of noise removal and structure preservation."
  },
  "bmvc2015_main_spotlightthenegativesageneralizeddiscriminativelatentmodel": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Spotlight the Negatives: A Generalized Discriminative Latent Model",
    "authors": [
      "Hossein Azizpour",
      "Mostafa Arefiyan",
      "Sobhan Naderi Parizi",
      "Stefan Carlsson"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper018/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper018/paper018.pdf",
    "published": "2015-09",
    "summary": "Discriminative latent variable models (LVM) are frequently applied to various visual recognition tasks. In these systems the latent (hidden) variables provide a formalism for modeling structured variation of visual features. Conventionally, latent variables are defined on the variation of the foreground (positive) class. In this work we augment LVMs to include \\textit{negative} latent variables corresponding to the background class. We formalize the scoring function of such a generalized LVM (GLVM). Then we discuss a framework for learning a model based on the GLVM scoring function. We theoretically showcase how some of the current visual recognition methods can benefit from this generalization. Finally, we experiment on a generalized form of Deformable Part Models with negative latent variables and show significant improvements on two different detection tasks."
  },
  "bmvc2015_main_perceptualdynamicrangeforin-cameraimageprocessing": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Perceptual Dynamic Range for In-Camera Image Processing",
    "authors": [
      "Praveen Cyriac",
      "David Kane",
      "Marcelo Bertalmio"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper019/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper019/paper019.pdf",
    "published": "2015-09",
    "summary": "Digital cameras apply a non-linearity to the captured sensor values prior to quantisation. This process is known as perceptual linearisation and ensures that the quantisation rate is approximately proportional to human sensitivity. We propose an adaptive in-camera non-linearity that ensures that the detail and contrast visible in the processed image match closely with the perception of the original scene. The method has been developed to emulate basic properties of the human visual system including contrast normalisation and the efficient coding of natural images via adaptive processes. Our results are validated visually and also quantitatively by two image quality metrics that model human perception. The method works for still and moving images and has a very low computational complexity, accordingly it can be implemented on any digital camera. It can also be applied off-line to RAW images or high dynamic range (HDR) images. We demonstrate the performance of the algorithm using images from digital cinema, mobile phones and amateur photography."
  },
  "bmvc2015_main_robustmultiplemodelfittingwithpreferenceanalysisandlow-rankapproximation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Robust Multiple Model Fitting with Preference Analysis and Low-rank Approximation",
    "authors": [
      "Luca Magri",
      "Andrea Fusiello"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper020/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper020/paper020.pdf",
    "published": "2015-09",
    "summary": "This paper deals with the extraction of multiple models from outlier-contaminated data. The method we present is based on preference analysis and low rank approximation. After representing points in a conceptual space, Robust PCA (Principal Component Analysis) and Symmetric NMF (Non negative Matrix Factorization) are employed to reduce the multi-model fitting problem to many single-fitting problems, which in turn are solved with a strategy that resembles MSAC (M-estimator SAmple Consensus). Experimental validation on public, real data-sets demonstrates that our method compares favourably with the state of the art."
  },
  "bmvc2015_main_robustglobalmotioncompensationinpresenceofpredominantforeground": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Robust Global Motion Compensation in Presence of Predominant Foreground",
    "authors": [
      "Seyed Morteza Safdarnejad",
      "Xiaoming Liu",
      "Lalita Udpa"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper021/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper021/paper021.pdf",
    "published": "2015-09",
    "summary": "Global motion compensation (GMC) removes intentional and unwanted camera motion. GMC is widely applicable for video stitching and, as a pre-processing module, for motion-based video analysis. While state-of-the-art GMC algorithms generally estimate homography satisfactorily between consecutive frames, their performances deteriorate on real-world unconstrained videos, for instance, videos with predominant foreground, e.g., moving objects or human, or uniform background. Since GMC transformation of frames to the global motion-compensated coordinate is done by cascading homographies, failure in GMC of a single frame drastically harms the final result. Thus, we propose a robust GMC, termed RGMC, based on homography estimation using keypoint matches. RGMC first suppresses the foreground impact by clustering the keypoint matches and removing those pertaining to the foreground, as well as erroneous matches. For homography verification, we propose a probabilistic model that combines keypoint matching error, consistency of edges after homograhy transformation, the motion history, and prior camera motion information. Experimental results on the Sports Videos in the Wild, Holleywood2, and HMDB51 datasets demonstrate the superiority of RGMC."
  },
  "bmvc2015_main_usingsegmentationtopredicttheabsenceofoccludedparts": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Using Segmentation to Predict the Absence of Occluded Parts",
    "authors": [
      "Golnaz Ghiasi",
      "Charless Fowlkes"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper022/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper022/paper022.pdf",
    "published": "2015-09",
    "summary": "Occlusion poses a significant difficulty for detecting and localizing object keypoints and subsequent fine-grained identification. We propose a part-based face detection model that utilizes bottom-up class-specific segmentation in order to jointly detect and segment out the foreground pixels belonging to the face. The model explicitly represents occlusion of parts at the detection phase, allowing for hypothesized figure-ground segmentation to suggest coherent patterns of part occlusion. We show that this bi-directional interaction between recognition and grouping results in state-of-the-art part localization accuracy for challenging benchmarks with significant occlusion and yields substantial gains in the precision of keypoint occlusion prediction."
  },
  "bmvc2015_main_learningthestructureofdeeparchitecturesusingl1regularization": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Learning the Structure of Deep Architectures Using L1 Regularization",
    "authors": [
      "Praveen Kulkarni",
      "Joaquin Zepeda",
      "Frederic Jurie",
      "Patrick P\u00e9rez",
      "Louis Chevallier"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper023/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper023/paper023.pdf",
    "published": "2015-09",
    "summary": "We present a method that formulates the selection of the structure of a deep architecture as a penalized, discriminative learning problem. Up to now, the structure of deep architectures has been fixed by hand, and only the weights are learned using discriminative learning. Our work is a first attempt towards a more formal method of deep structure selection. We consider architectures consisting only of fully-connected layers, and our approach relies on diagonal matrices inserted between subsequent layers. By including an L1 norm of the diagonal entries of said matrices as a regularization penalty, we force the diagonals to be sparse, accordingly selecting the effective number of rows (respectively, columns) of the corresponding layer's (next layer's) weights matrix. We carry out experiments on a standard dataset and show that our method succeeds in selecting the structure of deep architectures of multiple layers. One variant of our architecture results in a feature vector of size as little as $36$, while retaining very high image classification performance."
  },
  "bmvc2015_main_subspacedistributionalignmentforunsuperviseddomainadaptation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Subspace Distribution Alignment for Unsupervised Domain Adaptation",
    "authors": [
      "Baochen Sun",
      "Kate Saenko"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper024/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper024/paper024.pdf",
    "published": "2015-09",
    "summary": "We propose a novel method for unsupervised domain adaptation. Traditional machine learning algorithms often fail to generalize to new input distributions, causing reduced accuracy. Domain adaptation attempts to compensate for the performance degradation by transferring and adapting source knowledge to target domain. Existing unsupervised methods project domains into a lower-dimensional space and attempt to align the subspace bases, effectively learning a mapping from source to target points or vice versa. However, they fail to take into account the difference of the two distributions in the subspaces, resulting in misalignment even after adaptation. We present a unified view of existing subspace mapping based methods and develop a generalized approach that also aligns the distributions as well as the subspace bases. We provide a detailed evaluation of our approach on benchmark datasets and show improved results over published approaches."
  },
  "bmvc2015_main_robustspatialmatchingasensembleofweakgeometricrelations": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Robust Spatial Matching as Ensemble of Weak Geometric Relations",
    "authors": [
      "Xiaomeng Wu",
      "Kunio Kashino"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper025/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper025/paper025.pdf",
    "published": "2015-09",
    "summary": "Existing spatial matching methods permit geometrically-stable image matching, but still involve a difficult trade-off between flexibility and discriminative power. To address this issue, we regard spatial matching as an ensemble of geometric relations on a set of feature correspondences. A geometric relation is defined as a set of pairs of correspondences, in which every correspondence is associated with every other correspondence if and only if the pair satisfy a given geometric constraint. We design a novel, unified collection of weak geometric relations that fall into four fundamental classes of geometric coherences in terms of both spatial contexts and between-image transformations. The spatial similarity reduces to the cardinality of the conjunction of all geometric relations. The flexibility of weak geometric relations makes our method robust as regards incorrect rejections of true correspondences, and the conjunctive ensemble provides a high discriminative power in terms of mismatches. Extensive experiments are conducted on five datasets. Besides significant performance gain, our method yields much better scalability than existing methods, and so can be easily integrated into any image retrieval process."
  },
  "bmvc2015_main_kernelizedviewadaptivesubspacelearningforpersonre-identification": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Kernelized View Adaptive Subspace Learning for Person Re-identification",
    "authors": [
      "Qin Zhou",
      "Shibao Zheng",
      "Hang Su",
      "Hua Yang",
      "Yu Wang",
      "Shuang Wu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper026/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper026/paper026.pdf",
    "published": "2015-09",
    "summary": "Person re-identification refers to the task of recognizing the same person under different non-overlapping camera views and across different time and places. Many successful methods exploit complex feature representations or sophisticated learners. A recent trend to tackle this problem is to learn a suitable distance metric, the aim of which is to minimize the distance between true matches while maximizing the distance between mismatched pairs. However, most of the existing metric learning algorithms directly take the difference of pairwise features in the original feature space as input. By doing so, they implicitly assume that there exists a projection matrix which can map feature vectors in two different subspaces into an identical subspace where desired feature distribution(features of the same person come closely and faraway otherwise) can be achieved. In this paper, we propose to learn different projection matrices for different camera views, thereby the learned matrices are adaptive to different camera views and a common subspace satisfying the desired feature distribution is more likely to be pursued. To better adapt to the different variations encountered by different views, the kernel trick is adopted to catch more information such that nonlinear transformation is possible. During test phase, the features under different camera views are projected into the learned subspace and a simple nearest neighbor classification is performed. Extensive experiments on four challenging datasets (VIPeR, iLIDS, CAVIAR4REID and ETHZ) demonstrate the effectiveness of the proposed algorithm."
  },
  "bmvc2015_main_objectlocalizationinimagenetbylookingoutofthewindow": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Object localization in ImageNet by looking out of the window",
    "authors": [
      "Alexander Vezhnevets",
      "Vittorio Ferrari"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper027/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper027/paper027.pdf",
    "published": "2015-09",
    "summary": "We propose a method for annotating the location of objects in ImageNet. Traditionally, this is cast as an image window classification problem, where each window is considered independently and scored based on its appearance alone. Instead, we propose a method which scores each candidate window in the context of all other windows in the image, taking into account their similarity in appearance space as well as their spatial relations in the image plane. We devise a fast and exact procedure to optimize our scoring function over all candidate windows in an image, and we learn its parameters using structured output regression. We demonstrate on 92000 images from ImageNet that this significantly improves localization over recent techniques that score windows in isolation."
  },
  "bmvc2015_main_unsupervisedbehavior-specificdictionarylearningforabnormaleventdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Unsupervised Behavior-Specific Dictionary Learning for Abnormal Event Detection",
    "authors": [
      "Huamin Ren",
      "Weifeng Liu",
      "S\u00f8ren Ingvor Olsen",
      "Sergio Escalera",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper028/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper028/paper028.pdf",
    "published": "2015-09",
    "summary": "Abnormal event detection has been an important issue in video surveillance applications. Due to the huge amount of surveillance data, only a small proportion could be loaded during the training. As a result, there is a high chance of incomplete normal patterns in the training data, which makes the task very challenging. Sparse representation, as one of solutions, has shown its effectiveness. The basic principle is to find a collection (a dictionary) of atoms so that each training sample can only be represented by a few atoms. However, the relationship of atoms within the dictionary is commonly neglected, which brings a high risk of false alarm rate: if atoms from infrequent normal patterns are missing, any visual features from similar patterns can hardly be sparsely represented, hence could be wrongly detected as anomalies. In this paper, we propose Behavior-Specific Dictionaries (BSD) through unsupervised learning, in which atoms from the same dictionary representing one type of normal behavior in the training video. Moreover, 'missed atoms' that are potentially from infrequent normal features are used to refine these behavior dictionaries. To further reduce false alarms, the detection of abnormal features is not only dependent on reconstruction error from the learned dictionaries, but also on non zero distribution in coefficients. Experimental results on Anomaly Stairs dataset and UCSD Anomaly dataset show the effectiveness of our algorithm. Remarkably, our BSD algorithm can improve AUC significantly by 10% on the stricter pixel-level evaluation, compared to the best result that has been reported so far."
  },
  "bmvc2015_main_jointcalibrationforsemanticsegmentation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Joint Calibration for Semantic Segmentation",
    "authors": [
      "Holger Caesar",
      "Jasper Uijlings",
      "Vittorio Ferrari"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper029/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper029/paper029.pdf",
    "published": "2015-09",
    "summary": "Semantic segmentation is the task of assigning a class-label to each pixel in an image. We propose a region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems: (1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification. Our method outperforms the state-of-the-art on the popular SIFT Flow [17] dataset in both the fully and weakly supervised setting."
  },
  "bmvc2015_main_cameraelevationestimationfromasinglemountainlandscapephotograph": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Camera Elevation Estimation from a Single Mountain Landscape Photograph",
    "authors": [
      "Martin \u010cad\u00edk",
      "Jan Va\u0161\u00ed\u010dek",
      "Michal Hradi\u0161",
      "Filip Radenovi\u0107",
      "Ond\u0159ej Chum"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper030/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper030/paper030.pdf",
    "published": "2015-09",
    "summary": "This work addresses the problem of camera elevation estimation from a single photograph in an outdoor environment. We introduce a new benchmark dataset of one-hundred thousand images with annotated camera elevation called Alps100K. We propose and experimentally evaluate two automatic data-driven approaches to camera elevation estimation: one based on convolutional neural networks, the other on local features. To compare the proposed methods to human performance, an experiment with 100 subjects is conducted. The experimental results show that both proposed approaches outperform humans and that the best result is achieved by their combination."
  },
  "bmvc2015_main_data-freeparameterpruningfordeepneuralnetworks": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Data-free Parameter Pruning for Deep Neural Networks",
    "authors": [
      "Suraj Srinivas",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper031/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper031/paper031.pdf",
    "published": "2015-09",
    "summary": "Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85% of the total parameters in an MNIST-trained network, and about 35% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network."
  },
  "bmvc2015_main_real-timepedestriandetectionwithdeepnetworkcascades": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Real-Time Pedestrian Detection with Deep Network Cascades",
    "authors": [
      "Anelia Angelova",
      "Alex Krizhevsky",
      "Vincent Vanhoucke",
      "Abhijit Ogale",
      "Dave Ferguson"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper032/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper032/paper032.pdf",
    "published": "2015-09",
    "summary": "We present a new real-time approach to object detection that exploits the efficiency of cascade classifiers with the accuracy of deep neural networks. Deep networks have been shown to excel at classification tasks, and their ability to operate on raw pixel input without the need to design special features is very appealing. However, deep nets are notoriously slow at inference time. In this paper, we propose an approach that cascades deep nets and fast features, that is both very fast and very accurate. We apply it to the challenging task of pedestrian detection. Our algorithm runs in real-time at 15 frames per second. The resulting approach achieves a 26.2 percent average miss rate on the Caltech Pedestrian detection benchmark, which is competitive with the very best reported results. It is the first work we are aware of that achieves very high accuracy while running in real-time."
  },
  "bmvc2015_main_ruleofthumbdeepderotationforimprovedfingertipdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Rule of thumb: Deep derotation for improved fingertip detection",
    "authors": [
      "Aaron Wetzler",
      "Ron Slossberg",
      "Ron Kimmel"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper033/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper033/paper033.pdf",
    "published": "2015-09",
    "summary": "We investigate a novel global orientation regression approach for articulated objects using a deep convolutional neural network. This is integrated with an in-plane image derotation scheme, DeROT, to tackle the problem of per-frame fingertip detection in depth images. The method reduces the complexity of learning in the space of articulated poses which is demonstrated by using two distinct state-of-the-art learning based hand pose estimation methods applied to fingertip detection. Significant classification improvements are shown over the baseline implementation. Our framework involves no tracking, kinematic constraints or explicit prior model of the articulated object in hand. To support our approach we also describe a new pipeline for high accuracy magnetic annotation and labeling of objects imaged by a depth camera."
  },
  "bmvc2015_main_riesz-basedvolumelocalbinarypatternandanovelgroupexpressionmodelforgrouphappinessintensityanalysis": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Riesz-based Volume Local Binary Pattern and A Novel Group Expression Model for Group Happiness Intensity Analysis",
    "authors": [
      "Xiaohua Huang",
      "Abhinav Dhall",
      "Guoying Zhao",
      "Roland Goecke",
      "Matti Pietik\u00e4inen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper034/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper034/paper034.pdf",
    "published": "2015-09",
    "summary": "Automatic emotion analysis and understanding has received much attention over the years in affective computing. Recently, there are increasing interests in inferring the emotional intensity of a group of people. For group emotional intensity analysis, feature extraction and group expression model are two critical issues. In this paper, we propose a new method to estimate the happiness intensity of a group of people in an image. Firstly, we combine the Riesz transform and the local binary pattern descriptor, named Riesz-based volume local binary pattern, which considers neighbouring changes not only in the spatial domain of a face but also along the different Riesz faces. Secondly, we exploit the continuous conditional random fields for constructing a new group expression model, which considers global and local attributes. Intensive experiments are performed on three challenging facial expression databases to evaluate the novel feature. Furthermore, experiments are conducted on the HAPPEI database to evaluate the new group expression model with the new feature. Our experimental results demonstrate the promising performance for group happiness intensity analysis."
  },
  "bmvc2015_main_weaklysupervisedmetriclearningtowardssigneradaptationforsignlanguagerecognition": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Weakly Supervised Metric Learning towards Signer Adaptation for Sign Language Recognition",
    "authors": [
      "Fang Yin",
      "Xiujuan Chai",
      "Yu Zhou",
      "Xilin Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper035/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper035/paper035.pdf",
    "published": "2015-09",
    "summary": "Inter-signer variation is one of the challenging factors in real application of Sign Language Recognition(SLR). To tackle this problem, this paper proposes a weakly supervised metric learning framework to realize signer adaptation with some unlabeled sign data of the new signer. Concretely speaking, through the labeled data of several different signers, a generic distance metric can be learnt. Then the key step is to adapt the generic metric to the new signer according to the collected unlabeled samples. Clustering constraint and manifold constraint are considered together to realize the weakly supervised metric learning. In our implementation, a novel fragment-based feature is designed to describe each sign by fusing both trajectory and hand shape features, which is also proved more discriminative than the frame-based multi-modal feature. Extensive experiments on our collected large vocabulary datasets convincingly show that the proposed method is effective for signer adaptation and outperforms the state-of-the-art methods on signer-independent SLR."
  },
  "bmvc2015_main_hashmodahashingmethodforscalable3dobjectdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Hashmod: A Hashing Method for Scalable 3D Object Detection",
    "authors": [
      "Wadim Kehl",
      "Federico Tombari",
      "Nassir Navab",
      "Slobodan Ilic",
      "Vincent Lepetit"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper036/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper036/paper036.pdf",
    "published": "2015-09",
    "summary": "We present a scalable method for detecting objects and estimating their 3D poses in RGB-D data. To this end, we rely on an efficient representation of object views and employ hashing techniques to match these views against the input frame in a scalable way. While a similar approach already exists for 2D detection, we show how to extend it to estimate the 3D pose of the detected objects. In particular, we explore different hashing strategies and identify the one which is more suitable to our problem. We show empirically that the complexity of our method is sublinear with the number of objects and we enable detection and pose estimation of many 3D objects with high accuracy while outperforming the state-of-the-art in terms of runtime."
  },
  "bmvc2015_main_multi-tasktransfermethodstoimproveone-shotlearningformultimediaeventdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multi-Task Transfer Methods to Improve One-Shot Learning for Multimedia Event Detection",
    "authors": [
      "Wang Yan",
      "Jordan Yap",
      "Greg Mori"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper037/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper037/paper037.pdf",
    "published": "2015-09",
    "summary": "Learning a model for complex video event detection from only one positive sample is a challenging and important problem in practice, yet seldom has been addressed. This paper proposes a new one-shot learning method based on multi-task learning to address this problem. Information from external relevant events is utilized to overcome the paucity of positive samples for the given event. Relevant events are identified implicitly and are emphasized more in the training. Moreover, a new dataset focusing on personal video search is collected. Experiments on both TRECVid Multimedia Event Detection video set and the new dataset verify the efficacy of the proposed methods."
  },
  "bmvc2015_main_appearanceanddepthforrapidhumanactivityrecognitioninrealapplications": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Appearance and Depth for Rapid Human Activity Recognition in Real Applications",
    "authors": [
      "Stavros Tachos",
      "Konstantinos Avgerinakis",
      "Alexia Briasouli",
      "Ioannis Kompatsiaris"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper038/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper038/paper038.pdf",
    "published": "2015-09",
    "summary": "Human activity recognition has gained a lot of attention in the computer vision society, due to its usefulness in numerous contexts. This work focuses on the recognition of Activities of Daily Living (ADL), which involves recordings constrained to specific daily activities that are of interest in assisted living or smart home environments. We present a novel technique for spatial activity localisation and recognition from colour-depth sequences, tailored to Activities of Daily Living (ADLs), which usually take place in relatively constrained environments. The proposed method significantly reduces the computational cost of activity recognition, while at the same time achieving a competitive accuracy rate, comparable to the State of the Art (SoA). This is achieved by the introduction of appearance and depth based spatiotemporal volumes, the Spatio-Temporal Activity Cells (STACs), extracted using appearance and depth information from successive video frames. A novel adaptive background modelling method follows, to characterize the STACs as ''active'' or ''inactive'' and accumulate them into foreground or background history volumes respectively. After activity detection using the STACs, activity recognition takes place using a novel, depth-based descriptor, the Histogram of Surface Normals Projections (HONSP), in combination with well known appearance descriptors (Histograms of Oriented Gradients, HOGs). Fisher encoding aggregates them into a fixed size vector to train a multiclass SVM model, which is then used for activity recognition. Experiments on different ADL datasets recorded with elderly people verify that the suggested algorithm is very appropriate for real life scenarios."
  },
  "bmvc2015_main_robustwearablecameralocalizationasatargettrackingproblemonse(3)": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Robust Wearable Camera Localization as a Target Tracking Problem on SE(3)",
    "authors": [
      "Guillaume Bourmaud",
      "Audrey Giremus"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper039/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper039/paper039.pdf",
    "published": "2015-09",
    "summary": "This paper deals with the trajectory estimation of a wearable camera evolving in an indoor environment where a database of images has been previously annotated with map coordinates. The difficulty of this problem resides in the fact that: i) hand-held objects are frequently interposed between the camera and the environment during daily living activities; ii) strong motion blur and differences in illumination occur; iii) the environment changes between the images of the database and the video frames to localize, and the viewpoints can be significantly different. The contribution of this paper is threefold: 1) We formulate the localization problem as a target tracking problem on the Lie group of camera motions SE (3), where the measurements are map coordinates obtained by applying a Content Based Image Retrieval algorithm to the video frames. 2) In order to solve this problem, we derive a novel Rao-Blackwellized particle smoother on Lie groups, which builds upon the recently proposed Extended Kalman Filter on Lie groups and the Rauch-Tung-Striebel Smoother on Lie groups that we also derive in this paper. 3) To take into account the topology of the environment, we propose to introduce virtual measurements that guide the particles and prevent them from crossing walls. We demonstrate, on several challenging video sequences, where the person wearing the camera performs daily living activities, that the proposed method is able to efficiently deal with outlier measurements and achieves a sub-meter level accuracy while the state of the art algorithms lack in robustness."
  },
  "bmvc2015_main_jointobject-materialcategorysegmentationfromaudio-visualcues": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Joint Object-Material Category Segmentation from Audio-Visual Cues",
    "authors": [
      "Anurag Arnab",
      "Michael Sapienza",
      "Stuart Golodetz",
      "Julien Valentin",
      "Ondrej Miksik",
      "Shahram Izadi",
      "Philip H. S. Torr"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper040/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper040/paper040.pdf",
    "published": "2015-09",
    "summary": "It is not always possible to recognise objects and infer material properties for a scene from visual cues alone, since objects can look visually similar whilst being made of very different materials. In this paper, we therefore present an approach that augments the available dense visual cues with sparse auditory cues in order to estimate dense object and material labels. Since estimates of object class and material properties are mutually-informative, we optimise our multi-output labelling jointly using a random-field framework. We evaluate our system on a new dataset with paired visual and auditory data that we make publicly available. We demonstrate that this joint estimation of object and material labels significantly outperforms the estimation of either category in isolation."
  },
  "bmvc2015_main_deepfacerecognition": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Deep Face Recognition",
    "authors": [
      "Omkar M. Parkhi",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper041/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf",
    "published": "2015-09",
    "summary": "he goal of this paper is face recognition -- from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks."
  },
  "bmvc2015_main_combinatorialregularizationofdescriptormatchingforopticalflowestimation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Combinatorial Regularization of Descriptor Matching for Optical Flow Estimation",
    "authors": [
      "Benjamin Drayer",
      "Thomas Brox"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper042/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper042/paper042.pdf",
    "published": "2015-09",
    "summary": "One fundamental step in many state of the art optical flow methods is the initial estimation of reliable correspondences. It is well-established to extract and match features such as HOG to handle large displacements. We propose a combinatorial refinement of the initial matching. The (generally) sparse correspondences serve as a cue for our dense estimation. Optimization is done in the space of affine motion, where we regularize between neighboring points and similar regions. The evaluation on the MPI-Sintel dataset shows that the proposed method removes outliers from the initial matching and increases the number of reliable matches. The proposed refinement improves all optical flow algorithms that build upon pre-computed correspondences."
  },
  "bmvc2015_main_discretelightsourceestimationfromlightprobesforphotorealisticrendering": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Discrete Light Source Estimation from Light Probes for Photorealistic Rendering",
    "authors": [
      "Farshad Einabadi",
      "Oliver Grau"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper043/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper043/paper043.pdf",
    "published": "2015-09",
    "summary": "This contribution describes a new technique for estimation of discrete spot light sources. The method uses a consumer grade DSLR camera equipped with a fisheye lens to capture light probe images registered to the scene. From these probe images the geometric and radiometric properties of the dominant light sources in the scene are estimated. The first step is a robust approach to identify light sources in the light probes and to find exact positions by triangulation. Then the light direction and radiometric fall-off properties are formulated and estimated in a least square minimization approach. The new method shows quantitatively accurate estimates compared to ground truth measurements. We also tested the results in an augmented reality context by rendering a synthetic reference object scanned with a 3D scanner into an image of the scene with the estimated light properties. The rendered images give photorealistic results of the shadow and shading compared to images of the real reference object."
  },
  "bmvc2015_main_dictionarylearningwithiterativelaplacianregularisationforunsupervisedpersonre-identification": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Dictionary Learning with Iterative LaplacianRegularisation for Unsupervised Person Re-identification",
    "authors": [
      "Elyor Kodirov",
      "Tao Xiang",
      "Shaogang Gong"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper044/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper044/paper044.pdf",
    "published": "2015-09",
    "summary": "Many existing approaches to person re-identification (Re-ID) are based on supervised learning, which requires hundreds of matching pairs to be labelled for each pair of cameras. This severely limits their scalability for real-world applications. This work aims to overcome this limitation by developing a novel unsupervised Re-ID approach. The approach is based on a new dictionary learning for sparse coding formulation with a graph Laplacian regularisation term whose value is set iteratively. As an unsupervised model, the dictionary learning model is well-suited to the unsupervised task, whilst the regularisation term enables the exploitation of cross-view identity-discriminative information ignored by existing unsupervised Re-ID methods. Importantly this model is also flexible in utilising any labelled data if available. Experiments on two benchmark datasets demonstrate that the proposed approach significantly outperforms the state-of-the-arts."
  },
  "bmvc2015_main_cameraposeestimationfromlinesusingpl\u00fcckercoordinates": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Camera Pose Estimation from Lines using Pl\u00fccker Coordinates",
    "authors": [
      "Bronislav P\u0159ibyl",
      "Pavel Zem\u010d\u00edk",
      "Martin \u010cadik"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper045/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper045/paper045.pdf",
    "published": "2015-09",
    "summary": "Correspondences between 3D lines and their 2D images captured by a camera are often used to determine position and orientation of the camera in space. In this work, we propose a novel algebraic algorithm to estimate the camera pose. We parameterize 3D lines using Pl\u00fccker coordinates that allow linear projection of the lines into the image. A line projection matrix is estimated using Linear Least Squares and the camera pose is then extracted from the matrix. An algebraic approach to handle mismatched line correspondences is also included. The proposed algorithm is an order of magnitude faster yet comparably accurate and robust to the state-of-the-art, it does not require initialization, and it yields only one solution. The described method requires at least 9 lines and is particularly suitable for scenarios with 25 and more lines, as also shown in the results."
  },
  "bmvc2015_main_linearglobaltranslationestimationwithfeaturetracks": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Linear Global Translation Estimation with Feature Tracks",
    "authors": [
      "Zhaopeng Cui",
      "Nianjuan Jiang",
      "Chengzhou Tang",
      "Ping Tan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper046/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper046/paper046.pdf",
    "published": "2015-09",
    "summary": "This paper derives a novel linear position constraint for cameras seeing a common scene point, which leads to a direct linear method for global camera translation estimation. Unlike previous solutions, this method deals with collinear camera motion and weak image association at the same time. The final linear formulation does not involve the coordinates of scene points, which makes it efficient even for large scale data. We solve the linear equation based on $L_1$ norm, which makes our system more robust to outliers in essential matrices and feature correspondences. We experiment this method on both sequentially captured images and unordered Internet images. The experiments demonstrate its strength in robustness, accuracy, and efficiency."
  },
  "bmvc2015_main_learningdiscriminativevisualn-gramsfrommid-levelimagefeatures": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Learning Discriminative Visual N-grams from Mid-level Image Features",
    "authors": [
      "Raj Kumar Gupta",
      "Megha Pandey",
      "Alex YS Chia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper047/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper047/paper047.pdf",
    "published": "2015-09",
    "summary": "The task of image classification is one of the key problems in computer vision, and has inspired a variety of image representations. In this paper, we propose a method to learn discriminative combinations of mid-level visual elements that capture their spatial configurations and co-occurrence relationships. We term such combinations as visual n-grams. Our method is capable of learning combinations with different number of elements. Experiments conducted on multiple datasets demonstrate the effectiveness of our approach where we achieve high image classification accuracy. Further, on fusing our features with global image features, we outperform the state-of-the-art results."
  },
  "bmvc2015_main_minimizingthenumberofkeypointmatchingqueriesforobjectretrieval": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Minimizing the Number of Keypoint Matching Queries for Object Retrieval",
    "authors": [
      "Johannes Niedermayer",
      "Peer Kr\u00f6ger"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper048/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper048/paper048.pdf",
    "published": "2015-09",
    "summary": "To increase the efficiency of interest-point based object retrieval, researchers have put remarkable research efforts into improving the efficiency of kNN-based feature matching, pursuing to match thousands of features against a database within fractions of a second. However, due to the high-dimensional nature of image features that reduces the effectivity of index structures (curse of dimensionality) and due to the vast amount of features stored in image databases (images are often represented by up to several thousand features), this ultimate goal demanded to trade kNN query runtimes for query precision. In this paper we address an approach complementary to indexing in order to improve the efficiency of retrieval by querying only the most promising keypoint descriptors, as this affects kNN matching time linearly. As this reduction of kNN queries reduces the number of tentative correspondences, a loss of query precision is minimized by an additional image-level correspondence generation stage with a computational performance independent of the underlying indexing structure. Our experimental evaluation suggests good performance on a variety of datasets."
  },
  "bmvc2015_main_jh2rjointhomographyestimationforhighlightremoval": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "JH2R: Joint Homography Estimation for Highlight Removal",
    "authors": [
      "Sungmin Eum",
      "Hyungtae Lee",
      "David Doermann"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper049/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper049/paper049.pdf",
    "published": "2015-09",
    "summary": "This paper addresses the problem of removing highlight regions caused by the light sources reflecting off glossy surfaces in indoor environments. We devise an efficient method to detect and remove the highlights from the target scene by jointly estimating separate homographies for the target scene and the highlights. Our method is based on the observation that when given two images captured at different viewpoints, the displacement of the target scene is different from that of the highlight regions. We show the effectiveness of our method in removing the highlight reflections by comparing it with the related state-of-the-art methods. Unlike the previous methods, our method has the ability to handle saturated and relatively large highlights which completely obscure the content underneath."
  },
  "bmvc2015_main_actionrecognitionbasedonsubdivision-fusionmodel": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Action Recognition based on Subdivision-Fusion Model",
    "authors": [
      "Zongbo Hao",
      "Linlin Lu",
      "Qianni Zhang",
      "Jie Wu",
      "Ebroul Izquierdo",
      "Juanyu Yang",
      "Jun Zhao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper050/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper050/paper050.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a novel Subdivision-Fusion Model (SFM) to recognize human actions. In most action recognition tasks, overlapping feature distribution is a common problem leading to overfitting. In the subdivision stage of the proposed SFM, samples in each category are clustered. Then, such samples are grouped into multiple more concentrated subcategories. Boundaries for the subcategories are easier to find and as consequence overfitting is avoided. In the subsequent fusion stage, the multi-subcategories classification results are converted back to the original category recognition problem. Two methods to determine the number of clusters are provided. The proposed model has been thoroughly tested with four popular datasets. In the Hollywood2 dataset, an accuracy of 79.4% is achieved, outperforming the state-of-the-art accuracy of 64.3%. The performance on the YouTube Action dataset has been improved from 75.8% to 82.5%, while considerably improvements are also observed on the KTH and UCF50 datasets."
  },
  "bmvc2015_main_mixandmatchjointmodelforclothingandattributerecognition": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Mix and Match: Joint Model for Clothing and Attribute Recognition",
    "authors": [
      "Kota Yamaguchi",
      "Takayuki Okatani",
      "Kyoko Sudo",
      "Kazuhiko Murasaki",
      "Yukinobu Taniguchi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper051/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper051/paper051.pdf",
    "published": "2015-09",
    "summary": "This paper studies clothing and attribute recognition in the fashion domain. Specifically, in this paper, we turn our attention to the compatibility of clothing items and attributes. For example, people do not wear a skirt and a dress at the same time, yet a jacket and a shirt are a preferred combination. We consider such inter-object or inter-attribute compatibility in the recognition problem, and formulate a Conditional Random Field (CRF) that seeks the most probable combination in the given picture. The model takes into account the location-specific appearance with respect to a human body and the semantic correlation between clothing items and attributes, which we learn using the max-margin framework. We evaluate our model using two datasets that resemble realistic application scenarios: on-line social networks and shopping sites. The empirical evaluation shows that our model effectively improves the recognition performance over baselines including the state-of-the-art feature designed exclusively for clothing recognition. The results also suggest that our model generalizes well to different fashion-related applications."
  },
  "bmvc2015_main_describingcommonhumanvisualactionsinimages": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Describing Common Human Visual Actions in Images",
    "authors": [
      "Matteo Ruggero Ronchi",
      "Pietro Perona"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper052/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper052/paper052.pdf",
    "published": "2015-09",
    "summary": "Which common human actions and interactions are recognizable in monocular still images? Which involve objects and/or other people? How many is a person performing at a time? We address these questions by exploring the actions and interactions that are detectable in the images of the MS COCO dataset. We make two main contributions. First, a list of 140 common `visual actions', obtained by analyzing the largest on-line verb lexicon currently available for English (VerbNet) and human sentences used to describe images in MS COCO. Second, a complete set of annotations for those `visual actions', composed of subject-object and associated verb, which we call COCO-a (a for `actions'). COCO-a is larger than existing action datasets in terms of number instances of actions, and is unique because it is data-driven, rather than experimenter-biased. Other unique features are that it is exhaustive, and that all subjects and objects are localized. A statistical analysis of the accuracy of our annotations and of each action, interaction and subject-object combination is provided."
  },
  "bmvc2015_main_searchingforobjectsusingstructureinindoorscenes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Searching for Objects using Structure in Indoor Scenes",
    "authors": [
      "Varun K. Nagaraja",
      "Vlad I. Morariu",
      "Larry S. Davis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper053/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper053/paper053.pdf",
    "published": "2015-09",
    "summary": "To identify the location of objects of a particular class, a passive computer vision system generally processes all the regions in an image to finally output few regions. However, we can use structure in the scene to search for objects without processing the entire image. We propose a search technique that sequentially processes image regions such that the regions that are more likely to correspond to the query class object are explored earlier. We frame the problem as a Markov decision process and use an imitation learning algorithm to learn a search strategy. Since structure in the scene is essential for search, we work with indoor scene images as they contain both unary scene context information and object-object context in the scene. We perform experiments on the NYU-depth v2 dataset and show that the unary scene context features alone can achieve a significantly high average precision while processing only 20-25% of the regions for classes like bed and sofa. By considering object-object context along with the scene context features, the performance is further improved for classes like counter, lamp, pillow and sofa."
  },
  "bmvc2015_main_fastinversecompositionalimagealignmentwithmissingdataandre-weighting": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Fast Inverse Compositional Image Alignment with Missing Data and Re-weighting",
    "authors": [
      "Vincent Lui",
      "Dinesh Gamage",
      "Tom Drummond"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper054/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper054/paper054.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a novel method of performing inverse compositional image alignment which elegantly deals with missing data and re-weighting, and does not require the Jacobians and Hessian to be re-computed at every iteration. We show how missing data and re-weighting can be handled through preconditioning. We propose a few preconditioning techniques and analyse how each technique models the effects of missing data and re-weighting for inverse composition. We show through extensive experiments on different applications that our method improves the convergence rate of the conventional re-weighted inverse compositional method while remaining robust to outliers. We also show that the update parameters are usually underestimated and how this can be used to further speed up convergence of image alignment methods."
  },
  "bmvc2015_main_automaticageestimationfromfaceimagesviadeepranking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Automatic Age Estimation from Face Images via Deep Ranking",
    "authors": [
      "Huei-Fang Yang",
      "Bo-Yao Lin",
      "Kuang-Yu Chang",
      "Chu-Song Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper055/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper055/paper055.pdf",
    "published": "2015-09",
    "summary": "Automatic age estimation (AAE) from face images is a challenging problem because of large facial appearance variations resulting from a number of factors, e.g., aging and facial expressions. In this paper, we propose a generic, deep ranking model for AAE. Given a face image, our network first extracts features from the face through a scattering network (ScatNet), then reduces the feature dimension by principal component analysis (PCA), and finally predicts the age via category-wise rankers. The robustness of our approach comes from the following characteristics: (1) The scattering features are invariant to translation and small deformations; (2) the rank labels encoded in the network exploit the ordering relation among labels; and (3) the category-wise rankers perform age estimation within the same group. Our network achieves superior performance on a large-scale MORPH dataset and two expression ones, Lifespan and FACES."
  },
  "bmvc2015_main_featureencodingofspectralsignaturesfor3dnon-rigidshaperetrieval": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Feature Encoding of Spectral Signatures for 3D Non-Rigid Shape Retrieval",
    "authors": [
      "Frederico A. Limberger",
      "Richard C. Wilson"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper056/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper056/paper056.pdf",
    "published": "2015-09",
    "summary": "As the Internet and 3D modelling tools have led to an increasingly growth in the number of available 3D models, it becomes necessary to have a proper and smaller representation for searching purposes that captures the most important information about shapes. A large number of encoding methods have been proposed in the literature to create shape signatures from local descriptors. Two encoding methods have been receiving most attention from researchers given its informative characteristics: Fisher Vector and Super Vector. We propose to use these encoding methods combined with spectral signatures to represent 3D shapes. Although spectral signatures have many desirable properties to describe 3D shapes, for instance being invariant under rigid transformations and stable against non-rigid transformations, they do not perform so well in recent benchmarks. We propose improvements to the Wave Kernel Signature by analysing its behaviour when combined to different encoding methods for the purpose of shape retrieval and classification. At the end, we show a comparison of our method in two recent benchmarks."
  },
  "bmvc2015_main_abow-equivalentrecurrentneuralnetworkforactionrecognition": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "A BoW-equivalent Recurrent Neural Network for Action Recognition",
    "authors": [
      "Alexander Richard",
      "Juergen Gall"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper057/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper057/paper057.pdf",
    "published": "2015-09",
    "summary": "Bag-of-words (BoW) models are widely used in the field of computer vision. A BoW model consists of a visual vocabulary that is generated by unsupervised clustering the features of the training data, e.g., by using kMeans. The clustering methods, however, struggle with large amounts of data, in particular, in the context of action recognition. In this paper, we propose a transformation of the standard BoW model into a neural network, enabling discriminative training of the visual vocabulary on large action recognition datasets. We show that our model is equivalent to the original BoW model but allows for the application of supervised neural network training. Our model outperforms the conventional BoW model and sparse coding methods on recent action recognition benchmarks."
  },
  "bmvc2015_main_depthrestorationviajointtrainingofaglobalregressionmodelandcnns": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Depth Restoration via Joint Training of a Global Regression Model and CNNs",
    "authors": [
      "Gernot Riegler",
      "Ren\u00e9 Ranftl",
      "Matthias R\u00fcther",
      "Thomas Pock",
      "Horst Bischof"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper058/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper058/paper058.pdf",
    "published": "2015-09",
    "summary": "Denoising and upscaling of depth maps is a fundamental post-processing step for handling the output of depth sensors, since many applications that rely on depth data require accurate estimates to reach optimal accuracy. Adapting methods for denoising and upscaling to specific types of depth sensors is a cumbersome and error-prone task due to their complex noise characteristics. In this work we propose a model for denoising and upscaling of depth maps that adapts to the characteristics of a given sensor in a data-driven manner. We introduce a non-local Global Regression Model which models the inherent smoothness of depth maps. The Global Regression Model is parametrized by a Convolutional Neural Network, which is able to extract a rich set of features from the available input data. The structure of the model enables a complex parametrization, which can be jointly learned end-to-end and eliminates the need to explicitly model the signal formation process and the noise characteristics of a given sensor. Our experiments show that the proposed approach outperforms state-of-the-art methods, is efficient to compute and can be trained in a fully automatic way."
  },
  "bmvc2015_main_shapedetectionwithnearestneighbourcontourfragments": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Shape Detection with Nearest Neighbour Contour Fragments",
    "authors": [
      "Kasim Terzi\u0107",
      "Hussein Adnan Mohammed",
      "J.M.H. du Buf"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper059/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper059/paper059.pdf",
    "published": "2015-09",
    "summary": "We present a novel method for shape detection in natural scenes based on incomplete contour fragments and nearest neighbour search. In contrast to popular methods based on sliding windows, chamfer matching and SVMs, we characterise each contour fragment using a local descriptor and perform a fast nearest-neighbour search to find similar fragments in the training set. Based on this idea, we show how to learn robust object models from training images, generate reliable object hypotheses, and verify them. Despite its simplicity and speed, our method produces good detection results on the challenging ETHZ dataset."
  },
  "bmvc2015_main_exploitingimage-trainedcnnarchitecturesforunconstrainedvideoclassification": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Exploiting Image-trained CNN Architectures for Unconstrained Video Classification",
    "authors": [
      "Shengxin Zha",
      "Florian Luisier",
      "Walter Andrews",
      "Nitish Srivastava",
      "Ruslan Salakhutdinov"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper060/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper060/paper060.pdf",
    "published": "2015-09",
    "summary": "We conduct an in-depth exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED'14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED'14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED'14 from 34.95% to 38.74%. The fusion approach yields 89.6% classification accuracy on the challenging UCF-101 dataset."
  },
  "bmvc2015_main_changedetectionfromastreetimagepairusingcnnfeaturesandsuperpixelsegmentation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Change Detection from a Street Image Pair using CNN Features and Superpixel Segmentation",
    "authors": [
      "Ken Sakurada",
      "Takayuki Okatani"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper061/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper061/paper061.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a method for detecting changes of a scene using a pair of its vehicular, omnidirectional images. Previous approaches to the problem require the use of a 3D scene model and/or pixel-level registration between different time images. They are also computationally costly for estimating city-scale changes. We propose a novel change detection method that uses features of convolutional neural network (CNN) in combination with superpixel segmentation. Comparison of CNN features gives a low-resolution map of scene changes that is robust to illumination changes and viewpoint differences. Superpixel segmentation of the scene images is integrated with this low-resolution map to estimate precise segmentation boundaries of the changes. Our motivation is to develop a method for detecting city-scale changes, which can be used for visualization of damages of a natural disaster and subsequent recovery processes as well as for the purpose of maintaining/updating the 3D model of a city. We have created a dataset named Panoramic Change Detection Dataset, which will be made publicly available for evaluating the performances of change detection methods in these scenarios. The experimental results using the dataset show the effectiveness of our approach."
  },
  "bmvc2015_main_robustmultiviewregistrationof3dsurfacesvial_1-normminimization": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Robust Multiview Registration of 3D Surfaces via L_1-norm Minimization",
    "authors": [
      "Anil C. Raghuramu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper062/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper062/paper062.pdf",
    "published": "2015-09",
    "summary": "In this paper we present a robust method for simultaneous registration of multiple 3D scans. Rigid registration is an important task in many applications such as surface reconstruction, navigation and computer aided design. The goal of 3-D (rigid) registration is to align surfaces through a (rigid) transformation. A large number of existing registration algorithms are dependent on finding matching points between scans, but a significant number of them are spurious, and it is necessary to clean up the matches obtained. This requires a substantial amount of tuning of parameters and the final result might still contain outliers. Since the number of outliers are sparse we formulate the registration optimization using the $\\ell_1$-norm. We present experimental results to show that the performance of our algorithm is comparable to state of the art algorithms."
  },
  "bmvc2015_main_becausebetterdetectionsarestillpossiblemulti-aspectobjectdetectionwithboostedhoughforest": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Because better detections are still possible: Multi-aspect Object Detection with Boosted Hough Forest",
    "authors": [
      "Carolina Redondo-Cabrera",
      "Roberto L\u00f3pez-Sastre"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper063/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper063/paper063.pdf",
    "published": "2015-09",
    "summary": "In this work, we proceed to deconstruct the HF learning model to investigate whether a considerable better performance can be obtained detecting multi-aspect object categories. We introduce the novel Boosted Hough Forest (BHF): a HF where all the decision trees of the forest are trained in a stage-wise fashion, by optimizing a global differentiable loss function with Gradient Boosting, and using the concept of intermediate Hough voting spaces. This is in contrast to the local optimization performed in each tree node during the training of a standard HF. We also show how the multiple aspects of the object categories can be incorporated into the learning model by simply augmenting the dimensionality of the Hough voting spaces of the BHF. This allows our approach to naturally infer the pose of an object, simultaneously with the detection, for example. The experimental validation, considering four different datasets, confirms that the performance of the HF is improved by the new BHF."
  },
  "bmvc2015_main_optimizingpartitiontreesformulti-objectsegmentationwithshapeprior": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Optimizing Partition Trees for Multi-Object Segmentation with Shape Prior",
    "authors": [
      "Emmanuel Maggiori",
      "Yuliya Tarabalka",
      "Guillaume Charpiat"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper064/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper064/paper064.pdf",
    "published": "2015-09",
    "summary": "A partition tree is a hierarchical representation of an image. Once constructed, it can be repeatedly processed to extract information. Multi-object multi-class image segmentation with shape priors is one of the tasks that can be efficiently done upon an available tree. The traditional construction approach is a greedy clustering based on color similarities. However, not considering higher level cues during the construction phase leads to trees that might not accurately represent the underlying objects in the scene, inducing mistakes in the later segmentation. We propose a method to optimize a tree based both on color distributions and shape priors. It consists in pruning and regrafting tree branches in order to minimize the energy of the best segmentation that can be extracted from the tree. Theoretical guarantees help reducing the search space and make the optimization efficient. Our experiments show that we succeed in incorporating shape information to restructure a tree, which in turn enables to extract from it good quality multi-object segmentations with shape priors."
  },
  "bmvc2015_main_facepaintingqueryingartwithphotos": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Face Painting: querying art with photos",
    "authors": [
      "Elliot J. Crowley",
      "Omkar M. Parkhi",
      "Andrew Zisserman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper065/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper065/paper065.pdf",
    "published": "2015-09",
    "summary": "We study the problem of matching photos of a person to paintings of that person, in order to retrieve similar paintings given a query photo. This is challenging as paintings span many media (oil, ink, watercolor) and can vary tremendously in style (caricature, pop art, minimalist). We make the following contributions: (i) we show that, depending on the face representation used, performance can be improved substantially by learning -- either by a linear projection matrix common across identities, or by a per-identity classifier. We compare Fisher Vector and Convolutional Neural Network representations for this task; (ii) we introduce new datasets for learning and evaluating this problem; (iii) we also consider the reverse problem of retrieving photos from a large corpus given a painting; and finally, (iv) using the learnt descriptors, we show that, given a photo of a person, we are able to find their doppelg\u00e4nger in a large dataset of oil paintings, and how this result can be varied by modifying attributes (e.g. frowning, old looking)."
  },
  "bmvc2015_main_cross-domainobjectrecognitionusingobjectalignment": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Cross-Domain Object Recognition Using Object Alignment",
    "authors": [
      "Pengcheng Liu",
      "Chong Wang",
      "Peipei Yang",
      "Kaiqi Huang",
      "Tieniu Tan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper066/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper066/paper066.pdf",
    "published": "2015-09",
    "summary": "One popular solution to the problem of cross-domain object recognition is minimizing the difference between source and target distributions. Existing methods are devoted to minimizing that domain difference in a complex image space, which makes the problem hard to solve because of background influence. To discount the influence, we propose to minimize that difference using object alignment. We firstly present an algorithm to effectively align the object that appears in a set of images, and learn detectors for the aligned objects so that the detectors are robust to the influence of irrelevant background. Then we utilize the classification information from the image space to enhance our detectors. Finally, based on the detectors, we introduce a self-paced adaptation method to further reduce the domain difference. Experimental results demonstrate that the object alignment is effective to minimize the domain difference, and show the state-of-the-art recognition performance on several visual domain adaptation datasets."
  },
  "bmvc2015_main_dynamicalregularityforactionanalysis": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Dynamical Regularity for Action Analysis",
    "authors": [
      "Vinay Venkataraman",
      "Ioannis Vlachos",
      "Pavan Turaga"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper067/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper067/paper067.pdf",
    "published": "2015-09",
    "summary": "In this paper, we propose a new approach for quantification of 'dynamical regularity' as applied to modeling human actions. We use approximate entropy-based feature representation to model the dynamics in human movement to achieve temporal segmentation in untrimmed motion capture data and fine-grained quality assessment of diving actions in videos. The principle herein is to quantify regularity (frequency of typical patterns) in the dynamical space computed from trajectories of action data. We extend conventional ideas for modeling dynamics in human movement by introducing multivariate and cross approximate entropy features. Our experimental evaluation on theoretical models and two publicly available databases show that the proposed features can achieve state-of-the-art results on applications such as temporal segmentation and quality assessment of actions."
  },
  "bmvc2015_main_efficientspatio-temporaldataassociationusingmultidimensionalassignmentinmulti-cameramulti-targettracking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Efficient Spatio-Temporal Data Association Using Multidimensional Assignment in Multi-Camera Multi-Target Tracking",
    "authors": [
      "Moonsub Byeon",
      "Songhwai Oh",
      "Kikyung Kim",
      "Haan-Ju Yoo",
      "Jin Young Choi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper068/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper068/paper068.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a novel multi-target tracking method which jointly solves a data association problem using images from multiple cameras. In this work, the spatio-temporal data association problem is formulated as a multidimensional assignment problem (MDA). To achieve a fast, efficient, and easily implementable approximation algorithm, we solve the MDA problem approximately by solving a sequence of bipartite matching problems using random splitting and merging operations. In this formulation, we design a new cost function, considering the accuracy in 3D reconstruction, motion smoothness, visibility from cameras, starting/ending at entrance and exit zone, and false positive. Our approach reconstructs 3D trajectories that represent people's movement as 3D cylinders whose locations are estimated considering all adjacent frames. The experiments illustrate the proposed method shows the state-of-the-art performance in challenging multi-camera datasets and the computational efficiency with 8 times faster computation than the existing BIP approach."
  },
  "bmvc2015_main_anewfacerecognitionalgorithmbasedondictionarylearningforasingletrainingsampleperperson": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "A New Face Recognition Algorithm based on Dictionary Learning for a Single Training Sample per Person",
    "authors": [
      "Yang Liu",
      "Ian Wassell"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper069/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper069/paper069.pdf",
    "published": "2015-09",
    "summary": "The number of the training samples per person has a significant impact on face recognition (FR) performance. For the single training sample per person (STSPP) problem, most traditional FR algorithms exhibit performance degradation owing to the limited information available to predict the variance of the query sample. This paper proposes a new method for the STSPP problem in FR, namely the Learn-Generate-Classify (LGC) method. The LGC method first learns the relationship between the multiple images of a subject based on dictionary learning from a generic training set. Then it predicts the intra-class variance of the gallery set using the learned relationship. Based on the predicted information, synthetic samples can be generated, thus extending the single sample gallery set to one having multiple samples. Finally, we can classify the query samples using the traditional sparse representation classification (SRC) framework on the multi-sample gallery set. We verified the effectiveness of the new LGC method on the CMU Multi-pie database, with different illumination, expression and pose variation factors. It shows that the LGC method demonstrates a promising FR performance with only a STSPP."
  },
  "bmvc2015_main_robustdirectvisuallocalisationusingnormalisedinformationdistance": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Robust Direct Visual Localisation using Normalised Information Distance",
    "authors": [
      "Geoffrey Pascoe",
      "Will Maddern",
      "Paul Newman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper070/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper070/paper070.pdf",
    "published": "2015-09",
    "summary": "We present an information-theoretic approach for direct localisation of a monocular camera within a 3D appearance prior. In contrast to existing direct visual localisation methods based on minimising photometric error, an information-theoretic metric allows us to compare the whole image without relying on individual pixel values, yielding robustness to changes in the appearance of the scene due to lighting, camera motion, occlusions and sensor modality. Using a low-fidelity textured 3D model of the environment, we synthesise virtual images at a candidate pose within the model. We use the Normalised Information Distance (NID) metric to evaluate the appearance match between the camera image and the virtual image, and present a derivation of analytical NID derivatives for the SE(3) direct localisation problem, along with an efficient GPGPU implementation capable of online processing. We present results showing successful online visual localisation under significant appearance change both in a synthetic indoor environment and outdoors with real-world data from a vehicle-mounted camera."
  },
  "bmvc2015_main_jointclusteringandclassificationformultipleinstancelearning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Joint Clustering and Classification for Multiple Instance Learning",
    "authors": [
      "Karan Sikka",
      "Ritwik Giri",
      "Marian Bartlett"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper071/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper071/paper071.pdf",
    "published": "2015-09",
    "summary": "The multiple Instance Learning (MIL) framework has been extensively used to solve weakly labeled visual classification problems, where each image or video is treated as a bag of instances. Instance Space based MIL algorithms construct a classifier by modifying standard classifiers by defining the probability that a bag is of the target class as the maximum over the probabilities that its instances are of the target class. Although they are the most commonly used MIL algorithms, they do not account for the possibility that the instances may have multiple intermediate concepts, and that these concepts may have unequal weighting in predicting the overall target class. On the other hand, Embedding-space (ES) based MIL approaches are able to tackle this issue by defining a set of concepts, and then embedding each bag into a concept space, followed by training a standard classifier in the embedding space. In previous ES based approaches, the concepts were discovered separately from the classifier, and thus were not optimized for the final classification task. Here we propose a novel algorithm to estimate concepts and classifier parameters by jointly optimizing a classification loss. This approach discovers a small set of discriminative concepts, which yield superior classification performance. The proposed algorithm is referred to as Joint Clustering Classification for MIL data (JC2MIL) because the discovered concepts induce clusters of data instances. In comparison to previous approaches JC2MIL obtains state-of-the-art results on several MIL datasets- Corel-2000, image annotation datasets (Elephant, Tiger and Fox), and UCSB Breast Cancer dataset."
  },
  "bmvc2015_main_multi-modalityfeaturetransformaninteractiveimagesegmentationapproach": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multi-Modality Feature Transform: An Interactive Image Segmentation Approach",
    "authors": [
      "Moustafa Meshry",
      "Ahmed Taha",
      "Marwan Torki"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper072/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper072/paper072.pdf",
    "published": "2015-09",
    "summary": "Introducing suitable features in the scribble-based foreground-background (Fg/Bg) segmentation problem is crucial. In many cases, the object of interest has different regions with different color modalities. The same applies to a non-uniform background. Fg/Bg color modalities can even overlap when the appearance is solely modeled using color spaces like RGB or Lab. In this paper, we purposefully discriminate Fg scribbles from Bg scribbles for a better representation. This is achieved by learning a discriminative embedding space from the user-provided scribbles. The transformation between the original features and the embedded features is calculated. This transformation is used to project unlabeled features onto the same embedding space. The transformed features are then used in a supervised classification manner to solve the Fg/Bg segmentation problem. We further refine the results using a self-learning strategy, by expanding scribbles and recomputing the embedding and transformations. Finally, we evaluate our algorithms and compare their performance against the state-of-the-art methods on the ISEG dataset with clear improvements over competing methods."
  },
  "bmvc2015_main_multi-shothumanre-identificationusingadaptivefisherdiscriminantanalysis": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multi-Shot Human Re-Identification Using Adaptive Fisher Discriminant Analysis",
    "authors": [
      "Yang Li",
      "Ziyan Wu",
      "Srikrishna Karanam",
      "Richard J. Radke"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper073/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper073/paper073.pdf",
    "published": "2015-09",
    "summary": "While much research in human re-identification has focused on the single-shot case, in real-world applications we are likely to have an image sequence from both the person to be matched and each candidate in the gallery, extracted from automated video tracking. It is desirable to take advantage of the multiple visual aspects (states) of each subject observed during training and testing. However, since each subject may spend different amounts of time in each state, equally weighting all the images in a sequence is likely to produce suboptimal performance. To address this problem, we introduce an algorithm to hierarchically cluster image sequences and use the representative data samples to learn a feature subspace maximizing the Fisher criterion. The clustering and subspace learning processes are applied iteratively to obtain diversity-preserving discriminative features. A metric learning step is then applied to bridge the appearance difference between two cameras. The proposed method is evaluated on three multi-shot re-id datasets and the results outperform state-of-the-art methods."
  },
  "bmvc2015_main_hierarchicalhybridshaperepresentationformedicalshapes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Hierarchical Hybrid Shape Representation for Medical Shapes",
    "authors": [
      "Abhishek Kolagunda",
      "Guoyu Lu",
      "Chandra Kambhamettu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper074/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper074/paper074.pdf",
    "published": "2015-09",
    "summary": "Advances in 3D medical imaging technology have led to increase of interest in shape analysis of organs. This has in turn led to explosion of 3D medical shape data collected. The 3D shape data is also being used for simulations and to guide minimally invasive and remote surgical procedures. We present an Hierarchical-Hybrid Shape Representation (HSSR) which is compact and has both explicit and implicit forms. The compactness of the representation largely reduces storage requirement and communication overheads. The explicit and implicit forms can be used for accurate visualization of organ shapes and guide surgical procedures. The hybrid shape model proposed is a combination of Extended Superquadrics and RBF interpolation function that models, separately, the base shape and surface deformations. We also present an automatic method to fit the hybrid shape model to complex shapes by hierarchically dividing the shape into parts. Finally, we propose a technique to reconstruct shape from its compact representation by recursively blending the parts using intersection shape. Our extensive experiments show that our shape representation method significantly outperforms existing approaches in both accuracy and compactness."
  },
  "bmvc2015_main_dataseparationofl1-minimizationforreal-timemotiondetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Data Separation ofL1-minimization for Real-time Motion Detection",
    "authors": [
      "Yu Liu",
      "Huaxin Xiao",
      "Zheng Zhang",
      "Wei Xu",
      "Maojun Zhang",
      "Jianguo Zhang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper075/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper075/paper075.pdf",
    "published": "2015-09",
    "summary": "The L1-minimization used to seek the sparse solution restricts the applicability of compressive sensing theory. This paper proposes a data separation algorithm with computationally efficient strategies to achieve real-time processing for sparse model based motion detection. We regard the traditional pursuit algorithms as a pre-process step that converts the iterative optimization into linear addition and multiplication operations. A novel motion detection method is implemented to compare the difference between the current frame and the background model in terms of sparse coefficients. The influence of dynamic texture or statistical noise diminishes after the process of sparse projection; thus, enhancing the robustness of the implementation. Results of the qualitative and quantitative evaluations demonstrate the higher efficiency and effectiveness of the proposed approach compared with those of other competing methods."
  },
  "bmvc2015_main_colorconstancybydeeplearning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Color Constancy by Deep Learning",
    "authors": [
      "Zhongyu Lou",
      "Theo Gevers",
      "Ninghang Hu",
      "Marcel P. Lucassen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper076/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper076/paper076.pdf",
    "published": "2015-09",
    "summary": "Computational color constancy aims to estimate the color of the light source. The performance of many vision tasks, such as object detection and scene understanding, may benefit from color constancy by estimating the correct object colors. Since traditional color constancy methods are based on specific assumptions, none of those methods can be used as a universal predictor. Further, shallow learning schemes are used for training-based color constancy approaches, suffering from limited learning capacity. In this paper, we propose a framework using Deep Neural Networks (DNNs) to obtain an accurate light source estimator to achieve color constancy. We formulate color constancy as a DNN-based regression approach to estimate the color of the light source. The model is trained using datasets of more than a million images. Experiments show that the proposed algorithm outperforms the state-of-the-art by 9\\%. Especially in cross dataset validation, reducing the median angular error by 35\\%. Further, in our implementation, the algorithm operates at more than $100$ fps during"
  },
  "bmvc2015_main_model-based3dhandtrackingwithon-lineshapeadaptation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Model-based 3D Hand Tracking with on-line Shape Adaptation",
    "authors": [
      "Alexandros Makris",
      "Antonis Argyros"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper077/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper077/paper077.pdf",
    "published": "2015-09",
    "summary": "One of the shortcomings of the existing model-based 3D hand tracking methods is the fact that they consider a fixed hand model, i.e. one with fixed shape parameters. In this work we propose an online model-based method that tackles jointly the hand pose tracking and the hand shape estimation problems. The hand pose is estimated using a hierarchical particle filter. The hand shape is estimated by fitting the shape model parameters over the observations in a frame history. The candidate shapes required by the fitting framework are obtained by optimizing the shape parameters independently in each frame. Extensive experiments demonstrate that the proposed method tracks the pose of the hand and estimates its shape parameters accurately, even under heavy noise and inaccurate shape initialization."
  },
  "bmvc2015_main_globallyoptimaldlsmethodforpnpproblemwithcayleyparameterization": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Globally Optimal DLS Method for PnP Problem with Cayley parameterization",
    "authors": [
      "Gaku Nakano"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper078/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper078/paper078.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a globally optimal direct least squares (DLS) method for the PnP problem with Cayley parameterization. First we derive a new optimality condition without Lagrange multipliers, which is independent of any rotation representations. Then, we show that the new equation can be solved by several types of parameterizations and among them, Cayley parameterization is the most efficient. According to the experimental results, the proposed method represented by Cayley parameterization is more than three times faster than the state-of-the-art method while maintaining equivalent accuracy."
  },
  "bmvc2015_main_jointtrackingandeventanalysisforcarriedobjectdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Joint Tracking and Event Analysis for Carried Object Detection",
    "authors": [
      "Aryana Tavanai",
      "Muralikrishna Sridhar",
      "Eris Chinellato",
      "Anthony G. Cohn",
      "David C. Hogg"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper079/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper079/paper079.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a novel method for jointly estimating the track of a moving object and the events in which it participates. The method is intended for dealing with generic objects that are hard to localise and track with the performance of current detection algorithms - our focus is on events involving carried objects. The tracks for other objects with which the target object interacts (e.g. the carrying person) are assumed to be given. The method is posed as maximisation of a posterior probability defined over event sequences and temporally-disjoint subsets of the tracklets from an earlier tracking process. The probability function is a Hidden Markov Model coupled with a term that penalises non-smooth tracks and large gaps in the observed data. We evaluate the method using tracklets output by three state of the art trackers on the new created MINDSEYE2015 dataset and demonstrate improved performance."
  },
  "bmvc2015_main_occlusion-awareobjectlocalization,segmentationandposeestimation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Occlusion-Aware Object Localization, Segmentation and Pose Estimation",
    "authors": [
      "Samarth Brahmbhatt",
      "Heni Ben Amor",
      "Henrik Christensen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper080/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper080/paper080.pdf",
    "published": "2015-09",
    "summary": "We present a learning approach for localization and segmentation of objects in an image in a manner that is robust to partial occlusion. Our algorithm produces a bounding box around the full extent of the object and labels pixels in the interior that belong to the object. Like existing segmentation aware detection approaches, we learn an appearance model of the object and consider regions that do not fit this model as potential occlusions. However, in addition to the established use of pairwise potentials for encouraging local consistency, we use higher order potentials which capture information at the level of image segments. We also propose an efficient loss function that targets both localization and segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81 area under the false-positive per image vs. recall curve on average over the challenging CMU Kitchen Occlusion Dataset. This is 42.44% less segmentation error and a 16.13% increase in localization performance compared to the state-of-the-art. Finally, we show that the visibility labelling produced by our algorithm can make full 3D pose estimation from a single image robust to occlusion."
  },
  "bmvc2015_main_vesiclevolumetricevaluationofsynapticinferfacesusingcomputervisionatlargescale": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "VESICLE:Volumetric Evaluation of Synaptic Inferfaces using Computer Vision at Large Scale",
    "authors": [
      "William Gray Roncal",
      "Michael Pekala",
      "Verena Kaynig-Fittkau",
      "Dean M Kleissas",
      "Joshua T Vogelstein",
      "Hanspeter Pfister",
      "Randal Burns",
      "R Jacob Vogelstein",
      "Mark A Chevillet",
      "Gregory D Hager"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper081/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper081/paper081.pdf",
    "published": "2015-09",
    "summary": "An open challenge at the forefront of modern neuroscience is to obtain a comprehensive mapping of the neural pathways that underlie human brain function; an enhanced understanding of the wiring diagram of the brain promises to lead to new breakthroughs in diagnosing and treating neurological disorders. Inferring brain structure from image data, such as that obtained via electron microscopy (EM), entails solving the problem of identifying biological structures in large data volumes. Synapses, which are a key communication structure in the brain, are particularly difficult to detect due to their small size and limited contrast. Prior work in automated synapse detection has relied upon time-intensive, error-prone biological preparations (isotropic slicing, post-staining) in order to simplify the problem. This paper presents VESICLE, the first known approach designed for mammalian synapse detection in anisotropic, non-poststained data. Our methods explicitly leverage biological context, and the results exceed existing synapse detection methods in terms of accuracy and scalability. We provide two different approaches - a deep learning classifier (VESICLE-CNN) and a lightweight Random Forest approach (VESICLE-RF), to offer alternatives in the performance-scalability space. Addressing this synapse detection challenge enables the analysis of high-throughput imaging that is soon expected to produce petabytes of data, and provides tools for more rapid estimation of brain-graphs. Finally, to facilitate community efforts, we developed tools for large-scale object detection, and demonstrated this framework to find ~50,000 synapses in 60,000 um^3 (220 GB on disk) of electron microscopy data."
  },
  "bmvc2015_main_epipolarconsistencyinfluoroscopyforimage-basedtracking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Epipolar Consistency in Fluoroscopy for Image-Based Tracking",
    "authors": [
      "Andr\u00e9 Aichert",
      "Jian Wang",
      "Roman Schaffert",
      "Arnd D\u00f6rfler",
      "Joachim Hornegger",
      "Andreas Maier"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper082/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper082/paper082.pdf",
    "published": "2015-09",
    "summary": "Geometry and physics of absorption imaging impose certain constraints on been introduced and applied to motion correction in flat-detector computed tomography (CT). They are based on redundant information in transmission images along epipolar lines. Unlike other consistency conditions for CT scans, they act directly on an arbitrary pair of X-ray images. This paper proposes an application of ECC to 3D patient tracking in interventional radiology. We evaluate the proposed method against 2D-3D registration with a previously acquired CT. Our experiments on synthetic data based on a patient CT and phantom data from an interventional C-arm demonstrate that our method is able to compensate online for rotations of up to \u00b110\u00b0 and translations of \u00b125 mm between consecutive frames. We successfully track rotations of as much as 45\u00b0 over 45 images. The outstanding property of the approach is that no 3D scan is required for tracking a 3D object in space. We show, that small rotations of about 3\u00b0 in space and translations of about 50 mm can be tracked based on just two reference X-ray images. Since the proposed approach works directly on X-ray images, it exceeds regular 2D-3D registration with a CT in an order of magnitude in computational speed. We conclude that ECC are a simple and effective new tool for pre-aligment and online patient tracking for fluoroscopic sequences."
  },
  "bmvc2015_main_automaticaorticrootsegmentationwithshapeconstraintsandmeshregularisation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Automatic Aortic Root Segmentation with Shape Constraints and Mesh Regularisation",
    "authors": [
      "Robert Ieuan Palmer",
      "Xianghua Xie",
      "Gary Tam"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper083/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper083/paper083.pdf",
    "published": "2015-09",
    "summary": "Fully automated 3D segmentation is not only challenging due to, for instance, ambiguities in appearance, but it is also computationally demanding. We present a fully-automatic, learning-based deformable modelling method for segmenting the aortic root in CT images using a two-stage mesh deformation: a non-iterative boundary segmentation with a statistical shape model for shape constraint, followed by an iterative boundary refinement process. At both stages, we introduce a B-spline mesh regularisation technique to avoid mesh entanglement during deformation. The initialisation of the deformable model is achieved through efficient detection and localisation of the aortic root using marginal space learning, which carries out similarity parameter estimation in an incremental fashion. Quantitative comparisons are carried out against a state-of-the-art deformable model-based approach and an active shape model based segmentation. The proposed method achieves both a lower average mesh error of 1.39 \u00b1 0.29mm, and Hausdorff distance of 6.75 \u00b1 2.05mm, compared to these two approaches, and results in much more regularised mesh surfaces with no tangled mesh faces."
  },
  "bmvc2015_main_anatomicaltriangulationfromsparselandmarkstodenseannotationoftheskeletoninctimages": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Anatomical triangulation: from sparse landmarks to dense annotation of the skeleton in CT images",
    "authors": [
      "Marie Bieth",
      "Rene Donner",
      "Georg Langs",
      "Markus Schwaiger",
      "Bjoern Menze"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper084/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper084/paper084.pdf",
    "published": "2015-09",
    "summary": "The automated annotation of bones that are visible in CT images of the skeleton is a challenging task which has, so far, been approached for only certain subregions of the skeleton, such as the spine or hip. In this paper, we propose a novel annotation algorithm for automatically identifying structures and substructures in the whole skeleton. Our annotation algorithm makes use of recent advances in anatomical landmarks detection and is capable of generalising local information about landmarks to a dense label map of the full skeleton by anatomical triangulation. We follow a recognition approach that combines the use of distance-based features for measuring Euclidean and geodesic distances to a few given landmark locations, a parts-based model that is disambiguating anatomical substructures, and an iterative scheme for considering distances to the previously detected structures and, hence, to a dense set of anatomical reference points. We propose an annotation protocol for 136 substructures of the skeleton and test our annotation algorithm on 18 CT images. On average, we obtain a Dice score of 90.54."
  },
  "bmvc2015_main_primal-dualconvexoptimizationinlargedeformationdiffeomorphicregistrationwithrobustregularizers": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Primal-Dual convex optimization in large deformation diffeomorphic registration with robust regularizers",
    "authors": [
      "Monica Hernandez"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper085/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper085/paper085.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a method for primal-dual convex optimization in variational Large Deformation Diffeomorphic Metric Mapping (LDDMM) problems formulated with robust regularizers and image similarity metrics. The method is based on Chambolle and Pock primal-dual algorithm for solving general convex optimization problems. Diagonal preconditioning is used to ensure the convergence of the algorithm to the global minimum. We study three robust regularizers liable to provide acceptable results in diffeomorphic registration: Huber, V-Huber and Total Generalized Variation. Experiments in a 2D MRI data set with complex geometry show that, for all the considered regularizers, the proposed method is able to converge to diffeomorphic solutions. The method performs similarly to state of the art stationary LDDMM and log-domain diffeomorphic Demons in terms of the image similarity achieved after registration. In addition, evaluation in the 3D Non-Rigid Image Registration Project (NIREP) database shows an acceptable performance for second-order robust regularizers, close to the performance of the state of the art diffeomorphic registration methods."
  },
  "bmvc2015_main_globalminimumforcurvaturepenalizedminimalpathmethod": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Global Minimum for Curvature Penalized Minimal Path Method",
    "authors": [
      "Da Chen",
      "Jean-Marie Mirebeau",
      "Laurent D. Cohen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper086/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper086/paper086.pdf",
    "published": "2015-09",
    "summary": "Minimal path or geodesic methods have been widely applied to image analysis and medical imaging. However, traditional minimal path methods do not consider the effect of the curvature. In this paper, we propose a novel curvature penalized minimal path approach implemented via the anisotropic fast marching method and asymmetric Finsler metrics. We study the weighted Euler's elastica based geodesic energy and give an approximation to this energy by an orientation-lifted Finsler metric so that the proposed model can achieve a global minimum of this geodesic energy between the endpoint and initial source point. We also introduce a method to simplify the initialization of the proposed model. Experiments show that the proposed curvature penalized minimal path model owns several advantages comparing to the existed state-of-the-art minimal path models without curvature penalty both on synthetic and real images."
  },
  "bmvc2015_main_adaptivecontourfittingforpose-invariant3dfaceshapereconstruction": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Adaptive Contour Fitting for Pose-Invariant 3D Face Shape Reconstruction",
    "authors": [
      "Chengchao Qu",
      "Eduardo Monari",
      "Tobias Schuchert",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper087/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper087/paper087.pdf",
    "published": "2015-09",
    "summary": "Direct reconstruction of 3D face shape\u2014solely based on a sparse set of 2D feature points localized by a facial landmark detector\u2014offers an automatic, efficient and illumination-invariant alternative to the conventional analysis-by-synthesis 3D Morphable Model (3DMM) fitting. In this paper, we propose a novel algorithm that addresses the inconsistent correspondence of 2D and 3D landmarks at the facial contour due to head pose and localization ambiguity along the edge. To facilitate dynamic correspondence while fitting, a small subset of 3D vertices that serves as the contour candidates is annotated offline. During the fitting process, we employ the Levenberg-Marquardt Iterative Closest Point (LM-ICP) algorithm in combination with Distance Transform (DT) within the constrained domain, which allows for fast convergence and robust estimation of 3D face shape against pose variation. Superior evaluation results reported on ground truth 3D face scans over the state-of-the-art demonstrate the efficacy of the proposed method."
  },
  "bmvc2015_main_freehandlaserscanningusingmobilephone": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Freehand Laser Scanning Using Mobile Phone",
    "authors": [
      "Ron Slossberg",
      "Aaron Wetzler",
      "Ron Kimmel"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper088/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper088/paper088.pdf",
    "published": "2015-09",
    "summary": "3D scanners are growing in their popularity as many new applications and products are becoming a commodity. These applications are often tethered to a computer and/or require expensive and specialized hardware. In this note we demonstrate that it is possible to achieve good 3D reconstruction on a mobile device. We describe a novel approach for mobile phone scanning which utilizes a smart-phone and cheap laser line pointer attached to the phone using a 3D printed adapter. Non-linear multi-cale line filtering is used to detect the center of the projected laser beam in each frame with sub-pixel accuracy. The line location coupled with the estimated phone position and orientation in 3D space, obtained from publicly available SLAM libraries and marker tracking, permits us to perform a 3D reconstruction of a point cloud of the observed objects. Color and texture are extracted for every point along the scanned line point by projecting the reconstructed points back onto previous keyframed images. We validate the proposed method by comparing the reconstruction error to the ground truth obtained from an industrial laser scanner."
  },
  "bmvc2015_main_stochasticvisibilityinpoint-sampledscenes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Stochastic visibility in point-sampled scenes",
    "authors": [
      "Miles Hansard"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper089/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper089/paper089.pdf",
    "published": "2015-09",
    "summary": "This paper introduces a new visibility model for 3D point-clouds, such as those obtained from multiple time-of-flight or lidar scans. The scene is represented by a set of random particles, statistically distributed around the available surface-samples. Visibility is defined as the appropriate conjunction of occupancy and vacancy probabilities, along any visual ray. These probabilities are subsequently derived, in relation to the statistical scene structure. The resulting model can be used to assign probabilistic visibilities to any collection of scene-points, with respect to any camera position. Moreover, these values can be compared between different rays, and treated as functions of the camera and scene parameters. No surface mesh or volumetric discretization is required. The model is tested by decimating 3D point-clouds, and estimating the visibility of randomly selected targets. These estimates are compared to reference values, computed by standard methods, from the original full-resolution point-clouds. Applications of the new visibility model to multi-view stereo are discussed."
  },
  "bmvc2015_main_mgmasignificantlymoreglobalmatchingforstereovision": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "MGM: A Significantly More Global Matching for Stereovision",
    "authors": [
      "Gabriele Facciolo",
      "Carlo de Franchis",
      "Enric Meinhardt"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper090/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper090/paper090.pdf",
    "published": "2015-09",
    "summary": "Semi-global matching (SGM) is among the top-ranked stereovision algorithms. SGM is an efficient strategy for approximately minimizing a global energy that comprises a pixel-wise matching cost and pair-wise smoothness terms. In SGM the two-dimensional smoothness constraint is approximated as the average of one-dimensional line optimization problems. The accuracy and speed of SGM are the main reasons for its widespread adoption, even when applied to generic problems beyond stereovision. This approximate minimization, however, also produces characteristic low amplitude streaks in the final disparity image, and is clearly suboptimal with respect to more comprehensive minimization strategies. Based on a recently proposed interpretation of SGM as a min-sum Belief Propagation algorithm, we propose a new algorithm that allows to reduce by a factor five the energy gap of SGM with respect to reference algorithms for MRFs with truncated smoothness terms. The proposed method comes with no compromises with respect to the baseline SGM, no parameters and virtually no computational overhead. At the same time it attains higher quality results by removing the characteristic streaking artifacts of SGM."
  },
  "bmvc2015_main_automatedidentificationofindividualgreatwhitesharksfromunrestrictedfinimagery": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Automated Identification of Individual Great White Sharks from Unrestricted Fin Imagery",
    "authors": [
      "Benjamin Hughes",
      "Tilo Burghardt"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper092/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper092/paper092.pdf",
    "published": "2015-09",
    "summary": "The objective of this paper is automatically to identify individual great white sharks in a database of thousands of unconstrained fin images. The approach put forward appreciates shark fins in natural imagery as smooth, flexible and partially occluded objects with an individuality encoding trailing edge. In order to recover animal identities therefrom we first introduce an open contour stroke model which extends multi-scale region segmentation to achieve robust fin detection. Secondly, we show that combinatorial spectral fingerprinting can successfully encode individuality in fin boundaries. We combine both approaches in a fine-grained multi-instance recognition framework. We provide an evaluation of the system components and report their performance and properties."
  },
  "bmvc2015_main_generatingmulti-sentencenaturallanguagedescriptionsofindoorscenes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Generating Multi-sentence Natural Language Descriptions of Indoor Scenes",
    "authors": [
      "Dahua Lin",
      "Sanja Fidler",
      "Chen Kong",
      "Raquel Urtasun"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper093/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper093/paper093.pdf",
    "published": "2015-09",
    "summary": "This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account coherence among sentences. Experiments on the NYU-v2 dataset show that our framework is able to generate natural multi-sentence descriptions, outperforming those produced by a baseline."
  },
  "bmvc2015_main_overlappingdomaincoverforscalableandaccurateregressionkernelmachines": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Overlapping Domain Cover forScalableand Accurate Regression Kernel Machines",
    "authors": [
      "Mohamed Elhoseiny",
      "Ahmed Elgammal"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper094/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper094/paper094.pdf",
    "published": "2015-09",
    "summary": "In this paper, we present the Overlapping Domain Cover (ODC) notion for kernel machines, as a set of overlapping subsets of the data that covers the entire training set and optimized to be spatially cohesive as possible. We propose an efficient ODC framework, which is applicable to various regression models and in particular reduces the complexity of Twin Gaussian Processes (TGP) regression from cubic to quadratic. We also theoretically justified the idea behind our method. We validated and analyzed our method on three human pose estimation datasets and interesting findings are discussed."
  },
  "bmvc2015_main_visualcomparisonofimagesusingmultiplekernellearningforranking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Visual Comparison of Images Using Multiple Kernel Learning for Ranking",
    "authors": [
      "Amr Sharaf",
      "Mohamed E. Hussein",
      "Mohamed A. Ismail"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper095/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper095/paper095.pdf",
    "published": "2015-09",
    "summary": "Ranking is the central problem for many applications such as web search, recommendation systems, and visual comparison of images. In this paper, the multiple kernel learning framework is generalized for the learning to rank problem. This approach extends the existing learning to rank algorithms by considering multiple kernel learning and consequently improves their effectiveness. The proposed approach provides the convenience of fusing different features for describing the underlying data. As an application to our approach, the problem of visual image comparison is studied. Several visual features are used for describing the images and multiple kernel learning is adopted to find an optimal feature fusion. Experimental results on three challenging datasets show that our approach outperforms the state-of-the art and is significantly more efficient in runtime."
  },
  "bmvc2015_main_bridgingthedomainshiftbydomainadaptivedictionarylearning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Bridging the Domain Shift by Domain Adaptive Dictionary Learning",
    "authors": [
      "Hongyu Xu",
      "Jingjing Zheng",
      "Rama Chellappa"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper096/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper096/paper096.pdf",
    "published": "2015-09",
    "summary": "Domain adaptation (DA) tackles the problem where data from the training set (source domain) and test set (target domain) have different underlying distributions. In this paper, we propose a novel domain-adaptive dictionary learning framework to generate a set of intermediate domains. These intermediate domains form a smooth path and bridge the gap between the source and target domains. Specifically, we not only learn a common dictionary to encode the domain-shared features, but also learn a set of domain-specific dictionaries to model the domain shift. The separation of the common and domain-specific dictionaries enables us to learn more compact and reconstructive dictionaries for domain adaptation. These dictionaries are learned by alternating between domain-adaptive sparse coding and dictionary updating steps. Meanwhile, our approach gradually recovers the feature representations of both source and target data along the domain path. By aligning all the recovered domain data, we derive the final domain-adaptive features for recognition. Extensive experiments on cross-domain face and object recognition show that our approach significantly outperforms state-of-the-art methods."
  },
  "bmvc2015_main_jointfeatureselectionwithlow-rankdictionarylearning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Joint Feature Selection with Low-rank Dictionary Learning",
    "authors": [
      "Homa Foroughi",
      "Moein Shakeri",
      "Nilanjan Ray",
      "Hong Zhang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper097/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper097/paper097.pdf",
    "published": "2015-09",
    "summary": "Feature selection is one of the well known dimensionality reduction methods that efficiently describes the input data by removing irrelevant variables and reduces the effects of noise to provide good prediction results. In this paper, we propose a feature selection method by integrating dictionary learning and low-rank matrix approximation and apply it to image classification. The objective function finds a subset of features by preserving the reconstructive relationship of the data. This is achieved by minimizing the within-class reconstruction residual and simultaneously maximizing the between-class reconstruction residual. Simultaneously, the l_{2,1}-norm minimization on projection matrix is applied to jointly select the most relevant and discriminative features. The combination of low-rank approximation and Fisher discrimination dictionary learning, leads in more compactness within the same class and dissimilarity between different classes. As a result, even a simple classifier like KNN would perform surprisingly well and classify data accurately. Our proposed method is extensively evaluated on different benchmark image datasets and shows superior performance over several feature selection methods. The experimental results together with the theoretical analysis validate the effectiveness of our method for feature selection, and its efficacy for image classification."
  },
  "bmvc2015_main_scalablevisualinstanceminingwithinstancegraph": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Scalable Visual Instance Mining with Instance Graph",
    "authors": [
      "Wei Li",
      "Changhu Wang",
      "Lei Zhang",
      "Yong Rui",
      "Bo Zhang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper098/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper098/paper098.pdf",
    "published": "2015-09",
    "summary": "In this paper we address the problem of visual instance mining, which is to automatically discover frequently appearing visual instances from a large collection of images. We propose a scalable mining method by leveraging the graph structure with images as vertices. Different from most existing work that focused on either instance-level similarities or image-level context properties, our graph captures both information. The instance-level information is integrated during the construction of a weighted and undirected instance graph based on the similarity between augmented local features, while the image-level context is explored with a greedy breadth-first search algorithm to discover clusters of visual instances from the graph. This method is capable of mining challenging small visual instances with diverse variations. We evaluated our method on two fully annotated datasets and outperformed the state of the arts on both datasets with higher recalls. We also applied our method on a one-million Flickr dataset and proved its scalability."
  },
  "bmvc2015_main_perceptuallymotivatedbenchmarkforvideomatting": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Perceptually Motivated Benchmark for Video Matting",
    "authors": [
      "Mikhail Erofeev",
      "Yury Gitman",
      "Dmitriy Vatolin",
      "Alexey Fedorov",
      "Jue Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper099/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper099/paper099.pdf",
    "published": "2015-09",
    "summary": "Despite recent progress in the field of video matting, neither public data sets nor even a generally accepted method of measuring quality has yet emerged. In this paper we present an online benchmark for video-matting methods. Using chroma keying and a reflection-aware stop-motion capturing procedure, we prepared 12 test sequences. Then, using subjective data, we performed extensive comparative analysis of different quality metrics. The goal of our benchmark is to enable better understanding of current progress in the field of video matting and to aid in developing new methods."
  },
  "bmvc2015_main_sdicpsemi-densetrackingbasedoniterativeclosestpoints": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "SDICP: Semi-Dense Tracking based on Iterative Closest Points",
    "authors": [
      "Laurent Kneip",
      "Zhou Yi",
      "Hongdong Li"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper100/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper100/paper100.pdf",
    "published": "2015-09",
    "summary": "This paper introduces a novel strategy for real-time monocular camera tracking over the recently introduced, efficient semi-dense depth maps. We employ a geometric iterative closest point technique instead of a photometric error criterion, which has the conceptual advantage of requiring neither isotropic enlargement of the employed semi-dense regions, nor pyramidal subsampling. We outline the detailed concepts leading to robustness and efficiency even for large frame-to-frame disparities. We demonstrate successful real-time processing over very large view-point changes and significantly corrupted semi-dense depth-maps, thus underlining the validity of our geometric approach."
  },
  "bmvc2015_main_humanposeascontextforobjectdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Human Pose as Context for Object Detection",
    "authors": [
      "Abhilash Srikantha",
      "Juergen Gall"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper101/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper101/paper101.pdf",
    "published": "2015-09",
    "summary": "Detecting small objects in images is a challenging problem particularly when they are often occluded by hands or other body parts. Recently, joint modelling of human pose and objects has been proposed to improve both pose estimation as well as object detection. These approaches, however, focus on explicit interaction with an object and lack the flexibility to combine both modalities when interaction is not obvious. We therefore propose to use human pose as an additional context information for object detection. To this end, we represent an object category by a tree model and train regression forests that localize parts of an object for each modality separately. Predictions of the two modalities are then combined to detect the bounding box of the object. We evaluate our approach on three challenging datasets which vary in the amount of object interactions and the quality of automatically extracted human poses."
  },
  "bmvc2015_main_learningdepthcalibrationoftime-of-flightcameras": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Learning Depth Calibration of Time-of-Flight Cameras",
    "authors": [
      "David Ferstl",
      "Christian Reinbacher",
      "Gernot Riegler",
      "Matthias R\u00fcther",
      "Horst Bischof"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper102/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper102/paper102.pdf",
    "published": "2015-09",
    "summary": "We present a novel method for an automatic calibration of modern consumer Time-of-Flight cameras. Usually, these sensors come equipped with an integrated color camera. Albeit they deliver acquisitions at high frame rates they usually suffer from incorrect calibration and low accuracy due to multiple error sources. Using information from both cameras together with a simple planar target, we will show how to accurately calibrate both color and depth camera, and tackle most error sources inherent to Time-of-Flight technology in a unified calibration framework. Automatic feature detection minimizes user interaction during calibration. We utilize a Random Regression Forest to optimize the manufacturer supplied depth measurements. We show the improvements to commonly used depth calibration methods in a qualitative and quantitative evaluation on multiple scenes acquired by an accurate reference system for the application of dense 3D reconstruction."
  },
  "bmvc2015_main_low-rankspatio-temporalvideosegmentation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Low-Rank Spatio-Temporal Video Segmentation",
    "authors": [
      "Alasdair Newson",
      "Mariano Tepper",
      "Guillermo Sapiro"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper103/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper103/paper103.pdf",
    "published": "2015-09",
    "summary": "Robust Principal Component Analysis (RPCA) has generated a great amount of interest for background/foreground estimation in videos. The central hypothesis in this setting is that a video's background can be well-represented by a low-rank model. However, in the presence of complex lighting conditions this model is only accurate in localised spatio-temporal regions. Following this observation, we propose to model the background with a piecewise low-rank approximation. To achieve this, we introduce the piecewise low-rank segmentation problem. Starting from a carefully designed cost function which assesses the low-rank coherence of two video regions, the segmentation is obtained with an efficient graph-clustering algorithm. We show that this segmentation, when used to establish a local RPCA per segment, leads to improved quantitative and qualitative results for background/foreground estimation in challenging videos."
  },
  "bmvc2015_main_fastonlineupperbodyposeestimationfromvideo": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Fast Online Upper Body Pose Estimation from Video",
    "authors": [
      "Ming-Ching Chang",
      "Honggang Qi",
      "Xin Wang",
      "Hong Cheng",
      "Siwei Lyu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper104/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper104/paper104.pdf",
    "published": "2015-09",
    "summary": "Estimation of human body poses from video is an important problem in computer vi- sion with many applications. Most existing methods for video pose estimation are offline in nature, where all frames in the video are used in the process to estimate the body pose in each frame. In this work, we describe a fast online video upper body pose estima- tion method (CDBN-MODEC) that is based on a conditional dynamic Bayesian network model, which predicts upper body pose in a frame without using information from fu- ture frames. Our method combines fast single image based pose estimation methods with the temporal correlation of poses between frames. We collect a new high frame rate upper body pose dataset that better reflects practical scenarios calling for fast online video pose estimation. When evaluated on this dataset and the VideoPose2 benchmark dataset, CDBN-MODEC achieves improvements in both performance and running effi- ciency over several state-of-art online video pose estimation methods."
  },
  "bmvc2015_main_simultaneousinpaintingandsuper-resolutionusingself-learning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Simultaneous Inpainting and Super-resolution Using Self-learning",
    "authors": [
      "Milind G. Padalkar",
      "Manjunath V. Joshi",
      "Nilay Khatri"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper105/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper105/paper105.pdf",
    "published": "2015-09",
    "summary": "Past two decades have seen significant advancement in the techniques for scene completion and image super-resolution. Although many of the approaches solve these two problems by searching and processing of similar patches for estimating the unknown pixel values, the two problems have been addressed independently. In applications like creating immersive walkthrough systems or digital reconstruction of invaluable artwork, both inpainting and super-resolution of the given images are the preliminary steps in order to provide better visual experience. The usual practice is to solve these problems independently in a pipelined manner. In this paper we propose a unified framework to perform simultaneous inpainting and super-resolution. We construct dictionaries of image-representative low and high resolution patch pairs from the known regions in the test image and its coarser resolution. Inpainting of the missing pixels is performed using exemplars found by comparing patch details at a finer resolution, where self-learning is used to obtain the finer resolution patches by making use of the constructed dictionaries. The obtained finer resolution patches represent the super-resolved patches in the missing regions. Advantage of our approach when compared to other exemplar based inpainting techniques are (1) additional constraint in the form of finer resolution matching results in better inpainting and (2) inpainting is obtained not only in the given spatial resolution but also at higher resolution leading to super-resolution inpainting. Experiments on natural images show efficacy of the proposed method in comparison to state-of-the-art methods."
  },
  "bmvc2015_main_manitestareclassifiersreallyinvariant?": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Manitest: Are classifiers really invariant?",
    "authors": [
      "Alhussein Fawzi",
      "Pascal Frossard"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper106/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper106/paper106.pdf",
    "published": "2015-09",
    "summary": "Invariance to geometric transformations is a highly desirable property of automatic classifiers in many image recognition tasks. Nevertheless, it is unclear to which extent state-of-the-art classifiers are invariant to basic transformations such as rotations and translations. This is mainly due to the lack of general methods that properly measure such an invariance. In this paper, we propose a rigorous and systematic approach for quantifying the invariance to geometric transformations of any classifier. Our key idea is to cast the problem of assessing a classifier's invariance as the computation of geodesics along the manifold of transformed images. We propose the Manitest method, built on the efficient Fast Marching algorithm to compute the invariance of classifiers. Our new method quantifies in particular the importance of data augmentation for learning invariance from data, and the increased invariance of convolutional neural networks with depth. We foresee that the proposed generic tool for measuring invariance to a large class of geometric transformations and arbitrary classifiers will have many applications for evaluating and comparing classifiers based on their invariance, and help improving the invariance of existing classifiers."
  },
  "bmvc2015_main_mcslamamultipleconstrainedslam": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "MCSLAM : a Multiple Constrained SLAM",
    "authors": [
      "Datta Ramadasan",
      "Marc Chevaldonn\u00e9",
      "Thierry Chateau"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper107/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper107/paper107.pdf",
    "published": "2015-09",
    "summary": "The real-time localization of a camera in an unknown or partially known environment is a problem addressed by Structure From Motion algorithms and more particularly CSLAM algorithms (Constrained Simultaneous Localization And Mapping). In this paper, we propose a new algorithm, named MCSLAM (Multiple Constrained SLAM), designed to dynamically adapt each optimization to the variable number of parameters families and heterogeneous constraints. An automatic method is used to generate a dedicated optimization algorithm, from an exhaustive list of constraints. To our knowledge, this is the only implementation that combines flexibility and performance. Known objects are used to constrain the 3D structure of the reconstruction and a continuous-time representation of the trajectory is used to deal with motion constraints. A continuous trajectory provides a simple way to add heterogeneous constraints into the optimization framework like other unsynchronised sensors or an evolution model. Several experiments show the effectiveness of our approach in terms of accuracy and execution time compared to the state of the art on several public benchmarks of varying complexity."
  },
  "bmvc2015_main_incrementaldictionarylearningforunsuperviseddomainadaptation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Incremental Dictionary Learning for Unsupervised Domain Adaptation",
    "authors": [
      "Boyu Lu",
      "Rama Chellappa",
      "Nasser M. Nasrabadi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper108/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper108/paper108.pdf",
    "published": "2015-09",
    "summary": "Domain adaptation (DA) methods attempt to solve the domain mismatch problem between source and target data. In this paper, we propose an incremental dictionary learning method where some target data called supportive samples are selected to assist adaptation. Supportive samples are close to the source domain and have two properties: first, their predicted class labels are reliable and can be used for building more discriminative classification models; second, they act as a bridge to connect the two domains and reduce the domain mismatch. Theoretical analysis shows that both properties are important for adaptation, enabling the idea of adding supportive samples to the source domain. A stopping criterion is designed to guarantee that the domain mismatch decreases monotonically during adaptation. Experimental results on several widely used visual datasets show that the proposed approach performs better than many state-of-the-art methods."
  },
  "bmvc2015_main_stixelnetadeepconvolutionalnetworkforobstacledetectionandroadsegmentation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "StixelNet: A Deep Convolutional Network for Obstacle Detection and Road Segmentation",
    "authors": [
      "Dan Levi",
      "Noa Garnett",
      "Ethan Fetaya"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper109/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper109/paper109.pdf",
    "published": "2015-09",
    "summary": "General obstacle detection is a key enabler for obstacle avoidance in mobile robotics and autonomous driving. In this paper we address the task of detecting the closest obstacle in each direction from a driving vehicle. As opposed to existing methods based on 3D sensing we use a single color camera. The main novelty in our approach is the reduction of the task to a column-wise regression problem. The regression is then solved using a deep convolutional neural network (CNN). In addition, we introduce a new loss function based on a semi-discrete representation of the obstacle position probability to train the network. The network is trained using ground truth automatically generated from a laser-scanner point cloud. Using the KITTI dataset, we show that the our monocular-based approach outperforms existing camera-based methods including ones using stereo. We also apply the network on the related task of road segmentation achieving among the best results on the KITTI road segmentation challenge."
  },
  "bmvc2015_main_segmentingnaturalimageswiththeleasteffortashumans": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Segmenting natural images with the least effort as humans",
    "authors": [
      "Qiyang Zhao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper110/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper110/paper110.pdf",
    "published": "2015-09",
    "summary": "Great approaches to natural image segmentation have been made in recent years by learning from human segmentations, however little attention is paid to the behavior of human subjects in segmenting images. The paper investigates the effort made by human subjects and proposes an empirical method to estimate the boundary tracing loads, then establishes a model for natural image segmentation based on \\textit{the least effort principle}. We sort the hierarchies exhibited in human segmentation processes which use the BSDS tool, together with the monotonicity observed in the region merging processes, into two constraints on our model. Then an algorithm is established to segment natural images from scratch with pretty high efficiency thanks to the monotonic merging strategy. The experiment on BSDS500 shows our method obtains the state-of-the-art performance on both boundary and region measures. The average time consumption is only 1s and far less than those of its competitors. We also propose a new integrating evaluation measure, on which the performance of our method is noticeably worse than that of human subjects, indicating it is still a long run to build a perfect segmentation method."
  },
  "bmvc2015_main_deepfishinggradientfeaturesfromdeepnets": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Deep Fishing: Gradient Features from Deep Nets",
    "authors": [
      "Albert Gordo",
      "Adrien Gaidon",
      "Florent Perronnin"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper111/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper111/paper111.pdf",
    "published": "2015-09",
    "summary": "Convolutional Networks (ConvNets) have recently improved image recognition performance thanks to end-to-end learning of deep feed-forward models from raw pixels. Deep learning is a marked departure from the previous state of the art, the Fisher Vector (FV), which relied on gradient-based encoding of local hand-crafted features. In this paper, we discuss a novel connection between these two approaches. First, we show that one can derive gradient representations from ConvNets in a similar fashion to the FV. Second, we show that this gradient representation actually corresponds to a structured matrix that allows for efficient similarity computation. We experimentally study the benefits of transferring this representation over the outputs of ConvNet layers, and find consistent improvements on the Pascal VOC 2007 and 2012 datasets."
  },
  "bmvc2015_main_multimodalstereovisionforreconstructioninthepresenceofreflection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multimodal Stereo Vision For Reconstruction In The Presence Of Reflection",
    "authors": [
      "Scott Sorensen",
      "Philip Saponaro",
      "Stephen Rhein",
      "Chandra Kambhamettu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper112/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper112/paper112.pdf",
    "published": "2015-09",
    "summary": "Reflective and specular surfaces are problematic for traditional reconstruction techniques. Light projects non-linearly in scenes with these surfaces, and existing techniques to model this are poorly suited for real world applications. Accurately modeling the reflective surface is difficult without complete knowledge of the scene. To overcome this problem, we propose using different modalities of stereo vision to capture both the reflecting surface and the reflected scene. Using a four camera system consisting of a pair of visible wavelength cameras and a pair of long wave infrared cameras, we accurately reconstruct the reflective surface and ray trace reflected correspondences in the complementary modality. This approach allows for 3D reconstruction in the presence of a reflection, and does not require complete knowledge of the scene."
  },
  "bmvc2015_main_real-timehumandetectionbasedonpersonnessestimation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Real-time Human Detection based on Personness Estimation",
    "authors": [
      "Kyuwon Kim",
      "Kwanghoon Sohn"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper113/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper113/paper113.pdf",
    "published": "2015-09",
    "summary": "In this work, we study a real-time human detection method for mobile devices using window proposals. We find that the normed gradients, designed for generic objectness estimation, are also able to rapidly generate high quality object windows for a single-category object. We also notice that fusing the normed gradients with additional color feature improves the performance of objectness estimation for the single-category object. Based on these observations, we propose an efficient method, which we call personness estimation, to produce candidate windows that are highly likely to contain a person. The produced candidate windows are used to search over feature maps of an image so that a human detection method can achieve high detection performance within a short period of time. We further present how personness estimation can be efficiently combined into part-based human detection. Our experiments indicate that the proposed method is directly applicable to mobile devices, and allows real-time human detection."
  },
  "bmvc2015_main_semanticclassificationofboundariesofanrgbdimage": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Semantic Classification of Boundaries of an RGBD Image",
    "authors": [
      "Nishit Soni",
      "Anoop M. Namboodiri",
      "CV Jawahar",
      "Srikumar Ramalingam"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper114/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper114/paper114.pdf",
    "published": "2015-09",
    "summary": "The problem of labeling the edges present in a single color image as convex, concave, and occluding entities is one of the fundamental problems in computer vision. It has been shown that this information can contribute to segmentation, reconstruction and recognition problems. Recently, it has been shown that this classification is not straightforward even using RGBD data. This makes us wonder whether this apparent simple cue has more information than a depth map? In this paper, we propose a novel algorithm using random forest for classifying edges into convex, concave and occluding entities. We release a data set with more than 500 RGBD images with pixel-wise ground labels. Our method produces promising results and achieves an F-score of $0.84$ on the data set."
  },
  "bmvc2015_main_ellipticalasiftagglomerationinclassprototypeforlogodetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Elliptical ASIFT Agglomeration in Class Prototype for Logo Detection",
    "authors": [
      "Raluca Boia",
      "Corneliu Florea",
      "Laura Florea"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper115/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper115/paper115.pdf",
    "published": "2015-09",
    "summary": "Logo localization and recognition is difficult in natural images due to perspective deformations, varying background, possible occlusions, scaling variability; furthermore, the re-branding induces changes in the logo color palette and spatial distribution. To address this task, we locate keypoints using Affine Difference of Gaussian described by SIFT elliptical features; we construct the class prototypes by analyzing the graph of homographic matching between examples of the same class. The interconnections graph is developed for each class and the representative points from the non central examples are added to the class model. Potentially, an inverted secondary model is built for classes containing color inverted logos. Finally, each class is depicted by the reunion of the suitable keypoints and descriptors. The logo integrated detection (localization and classification) system is tested on multiple databases leading to state of the art accuracy."
  },
  "bmvc2015_main_entirereflectiveobjectsurfacestructureunderstanding": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Entire Reflective Object Surface Structure Understanding",
    "authors": [
      "Qinglin Lu",
      "Olivier Laligant",
      "Eric Fauvet",
      "Anastasia Zakharova"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper116/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper116/paper116.pdf",
    "published": "2015-09",
    "summary": "Reflection from reflective surface has been a long-standing problem for object recognition, it brings negative effects on object\u2019s color, texture and structural information. Because of that, it is not a trivial task to recognize the surface structure affected by the reflection, especially when the object is entirely reflective. Most of the time, reflection is considered as noise. In this paper, we propose a novel method for entire reflective object sub-segmentation by transforming the reflection motion into object surface label. Instead of considering the reflection as noise, our approach takes reflection as an advantage for understanding the surface structure of the entire reflective objects. The experimental results on specular and transparent objects show that the surface structures of the reflective objects can be revealed and the segmentation based on the surface structure outperforms the approaches in literature."
  },
  "bmvc2015_main_tennisvid2textfine-graineddescriptionsfordomainspecificvideos": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "TennisVid2Text: Fine-grained Descriptions for Domain Specific Videos",
    "authors": [
      "Mohak Sukhwani",
      "CV Jawahar"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper117/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper117/paper117.pdf",
    "published": "2015-09",
    "summary": "Automatically describing videos has ever been fascinating. In this work, we attempt to describe videos from a specific domain -- broadcast videos of lawn tennis matches. Given a video shot from a tennis match, we intend to generate a textual commentary similar to what a human expert would write on a sports website. Unlike many recent works that focus on generating short captions, we are interested in generating semantically richer descriptions. This demands a detailed low-level analysis of the video content, specially the actions and interactions among subjects. We address this by limiting our domain to the game of lawn tennis. By leveraging a large corpus of human created descriptions harvested from internet we generate rich descriptions. We evaluate our method on a newly created tennis video data set. Extensive analysis demonstrate that our approach addresses both semantic correctness as well as readability aspects involved in the task. It also outperforms competing baselines."
  },
  "bmvc2015_main_humanactivityrecognitioninthesemanticsimplexofelementaryactions": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Human activity recognition in the semantic simplex of elementary actions",
    "authors": [
      "Beaudry Cyrille",
      "P\u00e9teri Renaud",
      "Mascarilla Laurent"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper118/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper118/paper118.pdf",
    "published": "2015-09",
    "summary": "This paper presents an original approach for recognizing human activities in video sequences. A human activity is seen as a temporal sequence of elementary action probabilities. Actions are first generically learned using a robust action recognition method based on optical flow estimation and a cross-dataset training process. Activities are then projected as trajectories on the semantic simplex in order to be characterized and discriminated. A new trajectory attribute based on the total curvature Fourier descriptor is introduced. This attribute takes into account the induced geometry of the simplex manifold. Experiments on labelled datasets of human activities prove the efficiency of the proposed method for discriminating complex actions."
  },
  "bmvc2015_main_characteridentificationintv-seriesvianon-localcostaggregation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Character Identification in TV-series via Non-local Cost Aggregation",
    "authors": [
      "Ching-Hui Chen",
      "Rama Chellappa"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper119/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper119/paper119.pdf",
    "published": "2015-09",
    "summary": "We propose a non-local cost aggregation algorithm to recognize the identity of face and person tracks in a TV-series. In our approach, the fundamental element for identification is a track node, which is built on top of face and person tracks. Track nodes with temporal dependency are grouped into a knot. These knots then serve as the basic units in the construction of a k-knot graph for exploring the video structure. We build the minimum-distance spanning tree (MST) from the k-knot graph such that track nodes of similar appearance are adjacent to each other in MST. Non-local cost aggregation is performed on MST, which ensures information from face and person tracks is utilized as a whole to improve the identification performance. The identification task is performed by minimizing the cost of each knot, which takes into account the unique presence of a subject in a venue. Experimental results demonstrate the effectiveness of our method."
  },
  "bmvc2015_main_prototypicalpriorsfromimprovingclassificationtozero-shotlearning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Prototypical Priors: From Improving Classification to Zero-Shot Learning",
    "authors": [
      "Saumya Jetley",
      "Bernardino Romera-Paredes",
      "Sadeep Jayasumana",
      "Philip Torr"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper120/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper120/paper120.pdf",
    "published": "2015-09",
    "summary": "Recent works on zero-shot learning make use of side information such as visual attributes or natural language semantics to define the relations between output visual classes and then use these relationships to draw inference on new unseen classes at test time. In a novel extension to this idea, we propose the use of visual prototypical concepts as side information. For most real-world visual object categories, it may be difficult to establish a unique prototype. However, in cases such as traffic signs, brand logos, flags, and even natural language characters, these prototypical templates are available and can be leveraged for an improved recognition performance. The present work proposes a way to incorporate this prototypical information in a deep learning framework. Using prototypes as prior information, the deepnet pipeline learns the input image projections into the prototypical embedding space subject to minimization of the final classification loss. Based on our experiments with two different datasets of traffic signs and brand logos, prototypical embeddings incorporated in a conventional convolutional neural network improve the recognition performance. Recognition accuracy on the Belga logo dataset is especially noteworthy and establishes a new state-of-the-art. In zero-shot learning scenarios, the same system can be directly deployed to draw inference on unseen classes by simply adding the prototypical information for these new classes at test time. Thus, unlike earlier approaches, testing on seen and unseen classes is handled using the same pipeline, and the system can be tuned for a trade-off of seen and unseen class performance as per task requirement. Comparison with one of the latest works in the zero-shot learning domain yields top results on the two datasets mentioned above."
  },
  "bmvc2015_main_fastaffinetemplatematchingovergaloisfield": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Fast Affine Template Matching over Galois Field",
    "authors": [
      "Chao Zhang",
      "Takuya Akashi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper121/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper121/paper121.pdf",
    "published": "2015-09",
    "summary": "In this paper, we address the problem of template matching under affine transformations with general images. Our approach is to search an approximate affine transformation over a binary Galois field. The benefit is that we can avoid matching with huge amount of potential transformations, because they are discretely sampled. However, a Galois field of affine transformation can still be impractical for exhaustive searching. To approach the optimum solution efficiently, we introduce a level-wise adaptive sampling (LAS) method under genetic algorithm framework. In LAS, individuals converge to the global optimum depending on a level-wise selection and crossover while the population number is decreased by a population bounding scheme. In the experiment section, we analyse our method systematically and compare it against the state-of-the-art method on an evaluation data set. The results show that our method has a higher accuracy performance with fewer matching tests."
  },
  "bmvc2015_main_universalhoughdictionariesforobjecttracking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Universal Hough dictionaries for object tracking",
    "authors": [
      "Fausto Milletari",
      "Wadim Kehl",
      "Federico Tombari",
      "Slobodan Ilic",
      "Seyed-Ahmad Ahmadi",
      "Nassir Navab"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper122/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper122/paper122.pdf",
    "published": "2015-09",
    "summary": "We propose a novel approach to online visual tracking that combines the robustness of sparse coding with the flexibility of voting-based methods. Our algorithm relies on a dictionary that is learned once and for all from a large set of training patches extracted from images unrelated to the test sequences. In this way we obtain basis functions, also known as atoms, that can be sparsely combined to reconstruct local image content. In order to adapt the generic knowledge encoded in the dictionary to the specific object being tracked, we associate a set of votes and local object appearances to each atom: this is the only information being updated during online tracking. In each frame of the sequence the object's bounding box position is retrieved through a voting strategy. Our method exhibits robustness towards occlusions, sudden local and global illumination changes as well as shape changes. We test our method on 50 standard sequences obtaining results comparable or superior to the state of the art."
  },
  "bmvc2015_main_3dtrackingofhumanhandsininteractionwithunknownobjects": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "3D Tracking of Human Hands in Interaction with Unknown Objects",
    "authors": [
      "Paschalis Panteleris",
      "Nikolaos Kyriazis",
      "Antonis A. Argyros"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper123/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper123/paper123.pdf",
    "published": "2015-09",
    "summary": "The analysis and the understanding of object manipulation scenarios based on computer vision techniques can be greatly facilitated if we can gain access to the full articulation of the manipulating hands and the 3D pose of the manipulated objects. Currently, there exist methods for tracking hands in interaction with objects whose 3D models are known. There are also methods that can reconstruct 3D models of objects that are partially observable in each frame of a sequence. However, to the best of our knowledge, no method can track hands in interaction with unknown objects. In this paper we propose such a method. Experimental results show that hand tracking can be achieved with an accuracy that is comparable to the one obtained by methods that assume knowledge of the object models. Additionally, as a by-product, the proposed method delivers accurate 3D models of the manipulated objects."
  },
  "bmvc2015_main_handlingdataimbalanceinautomaticfacialactionintensityestimation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Handling Data Imbalance in Automatic Facial Action Intensity Estimation",
    "authors": [
      "Philipp Werner",
      "Frerk Saxen",
      "Ayoub Al-Hamadi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper124/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper124/paper124.pdf",
    "published": "2015-09",
    "summary": "Automatic Action Unit (AU) intensity estimation is a key problem in facial expression analysis. But limited research attention has been paid to the inherent class imbalance, which usually leads to suboptimal performance. To handle the imbalance, we propose (1) a novel multiclass under-sampling method and (2) its use in an ensemble. We compare our approach with state of the art sampling methods used for AU intensity estimation. Multiple datasets and widely varying performance measures are used in the literature, making direct comparison difficult. To address these shortcomings, we compare different performance measures for AU intensity estimation and evaluate our proposed approach on three publicly available datasets, with a comparison to state of the art methods along with a cross dataset evaluation."
  },
  "bmvc2015_main_hierarchicalrank-basedveilinglightestimationforunderwaterdehazing": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Hierarchical rank-based veiling light estimation for underwater dehazing",
    "authors": [
      "Simon Emberton",
      "Lars Chittka",
      "Andrea Cavallaro"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper125/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper125/paper125.pdf",
    "published": "2015-09",
    "summary": "Current dehazing approaches are often hindered when scenes contain bright objects which can cause veiling light and transmission estimation methods to fail. This paper introduces a single image dehazing approach for underwater images with novel veiling light and transmission estimation steps which deal with issues arising from bright objects. We use features to hierarchically rank regions of an image and to select the most likely veiling light candidate. A region-based approach is used to find optimal transmission values for areas that suffer from oversaturation. We also locate background regions through superpixel segmentation and clustering, and adapt the transmission values in these regions so to avoid artefacts. We validate the performance of our approach in comparison to the state of the art in underwater dehazing through subjective evaluation and with commonly used quantitative measures."
  },
  "bmvc2015_main_theintrinsicerrorofexposurefusionforhdrimaging,andawaytoreduceit": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "The intrinsic error of exposure fusion for HDR imaging, and a way to reduce it",
    "authors": [
      "Raquel Gil Rodr\u00edguez",
      "Javier Vazquez-Corral",
      "Marcelo Bertalm\u00edo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper126/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper126/paper126.pdf",
    "published": "2015-09",
    "summary": "In this paper we present a novel approach to the problem of exposure fusion of a stack of pictures for the generation of high dynamic range (HDR) radiance maps. All exposure fusion approaches, when applied on 8-bit non-RAW pictures, perform photometric calibration by estimating and inverting the camera response function, which is assumed to be a channelwise-independent function which does not change with the exposure. Our experiments show that these assumptions do not always hold and that the camera may automatically introduce changes (in gain, white balance, gamma correction value) from one exposure to the next when performing the non-linear operations involved in recording pictures in non-RAW formats such as JPEG. The net result is that HDR radiance maps obtained from exposure fusion of non-linear data may have substantially more error than if computed directly from the linear, RAW data. Our proposed method overcomes this problem and compensates for the changes introduced by the camera by matching the color correction and gamma correction transforms of all pictures to those of a reference picture in the stack, providing a clear improvement in terms of PSNR with respect to the classical method of Debevec and Malik."
  },
  "bmvc2015_main_detectingchangeformulti-view,long-termsurfaceinspection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Detecting Change for Multi-View, Long-Term Surface Inspection",
    "authors": [
      "Simon Stent",
      "Riccardo Gherardi",
      "Bj\u00f6rn Stenger",
      "Roberto Cipolla"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper127/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper127/paper127.pdf",
    "published": "2015-09",
    "summary": "We describe a system for the detection of changes in multiple views of a tunnel surface. From data gathered by a robotic inspection rig, we use a structure-from-motion pipeline to build panoramas of the surface and register images from different time instances. Reliably detecting changes such as hairline cracks, water ingress and other surface damage between the registered images is a challenging problem: achieving the best possible performance for a given set of data requires sub-pixel precision and careful modelling of the noise sources. The task is further complicated by factors such as unavoidable registration error and changes in image sensors, capture settings and lighting. Our contribution is a novel approach to change detection using a two-channel convolutional neural network. The network accepts pairs of approximately registered image patches taken at different times and classifies them to detect anomalous changes. To train the network, we take advantage of synthetically generated training examples and the homogeneity of the tunnel surfaces to virtually eliminate the requirement for manual labelling. We evaluate our method on field data gathered from a live tunnel over several months, demonstrating it to outperform existing approaches from recent literature and industrial practice."
  },
  "bmvc2015_main_partlocalizationusingmulti-proposalconsensusforfine-grainedcategorization": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Part Localization using Multi-Proposal Consensus for Fine-Grained Categorization",
    "authors": [
      "Kevin J. Shih",
      "Arun Mallya",
      "Saurabh Singh",
      "Derek Hoiem"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper128/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper128/paper128.pdf",
    "published": "2015-09",
    "summary": "We present a simple deep learning framework to simultaneously predict keypoint locations and their respective visibilities and use those to achieve state-of-the-art performance for fine-grained classification. We show that by conditioning the predictions on object proposals with sufficient image support, our method can do well without complicated spatial reasoning. Instead, inference methods with robustness to outliers, yield state-of-the-art for keypoint localization. We demonstrate the effectiveness of our accurate keypoint localization and visibility prediction on the fine-grained bird recognition task with and without ground truth bird bounding boxes, and outperform existing state-of-the-art methods by over 2%."
  },
  "bmvc2015_main_gestureandactionrecognitionbyevolveddynamicsubgestures": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Gesture and Action Recognition by Evolved Dynamic Subgestures",
    "authors": [
      "V\u00edctor Ponce-L\u00f3pez",
      "Hugo Jair Escalante",
      "Sergio Escalera",
      "Xavier Bar\u00f3"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper129/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper129/paper129.pdf",
    "published": "2015-09",
    "summary": "This paper introduces a framework for gesture and action recognition based on the evolution of temporal gesture primitives, or subgestures. Our work is inspired on the principle of producing genetic variations within a population of gesture subsequences, with the goal of obtaining a set of gesture units that enhance the generalization capability of standard gesture recognition approaches. In our context, gesture primitives are evolved over time using dynamic programming and generative models in order to recognize complex actions. In few generations, the proposed subgesture-based representation of actions and gestures ourperforms the state of the art results on the MSRDaily3D and MSRAction3D datasets."
  },
  "bmvc2015_main_facealignmentassistedbyheadposeestimation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Face Alignment Assisted by Head Pose Estimation",
    "authors": [
      "Heng Yang",
      "Wenxuan Mou",
      "Yichi Zhang",
      "Ioannis  Patras",
      "Hatice Gunes",
      "Peter Robinson"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper130/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper130/paper130.pdf",
    "published": "2015-09",
    "summary": "In this paper we present a supervised initialisation scheme for cascaded face alignment based on explicit head pose estimation. We first investigate the failure cases of most state of the art face alignment approaches and observe that these failures often share one common global property, i.e. the head pose variation is usually large. Inspired by this, we propose a deep convolutional network model for reliable and accurate head pose estimation. Instead of using a mean face shape, or randomly selected shapes for cascaded face alignment initialisation, we propose two schemes for generating initialisation: the first one relies on projecting a mean 3D face shape (represented by 3D facial landmarks) onto 2D image under the estimated head pose; the second one searches nearest neighbour shapes from a training set according to head pose distance. By doing so, the initialisation gets closer to the actual shape, which enhances the possibility of convergence and in turn improves the face alignment performance. We demonstrate the proposed method on the benchmark 300W dataset and show very competitive performance in both head pose estimation and face alignment."
  },
  "bmvc2015_main_scorenormalizationinmultimodalsystemsusinggeneralizedextremevaluedistribution": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Score Normalization in Multimodal Systems using Generalized Extreme Value Distribution",
    "authors": [
      "Renu Sharma",
      "Sukhendu Das",
      "Padmaja Joshi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper131/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper131/paper131.pdf",
    "published": "2015-09",
    "summary": "In multimodal biometric systems, human identification is performed by fusing information in different ways like sensor-level, feature-level, score-level, rank-level and decision-level. Score level fusion is preferred over other levels of fusion because of its low complexity and sufficient availability of information for fusion. However, the scores obtained from different unimodal systems are heterogeneous in nature and hence they all require normalization before fusion. In this paper, we propose a client-centric score normalization technique based on extreme value theory (EVT), exploiting the properties of Generalized Extreme Value (GEV) distribution. The novelty lies in the application of extreme value theory over the tail of the complete score distribution (genuine and impostor scores), assuming that the genuine scores form extreme values (tail) with respect to the entire set of scores. Normalization is then performed by estimating the cumulative density function of GEV distribution, using the parameter set obtained from genuine data. For evaluation, the proposed method is compared with state-of-the-art methods on two publicly available multimodal databases: i) NIST BSSR1 [22] multimodal biometric score database and ii) Database created from Face Recognition Grand Challenge V2.0 [23] and LG4000 iris images [24], to show the efficiency of the proposed method."
  },
  "bmvc2015_main_manifold-regularizedselectablefactorextractionforsemi-supervisedimageclassification": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Manifold-Regularized Selectable Factor Extraction for Semi-supervised Image Classification",
    "authors": [
      "Xin Shi",
      "Chao Zhang",
      "Fangyun Wei",
      "Hongyang Zhang",
      "Yiyuan She"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper132/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper132/paper132.pdf",
    "published": "2015-09",
    "summary": "In many vision analytics-based applications such as image classification, we confront explosive growth of high-dimensional data. Thus, many feature selection and extraction methods have been proposed to reduce the computational cost and avoid over-fitting. Recently, a novel selectable factor extraction (SFE) framework is proposed to simultaneously perform feature selection and extraction, and is theoretically and practically proved to be effective in handling high-dimensional data. The algorithm is also quite efficient and easy to implement. Although it is advantageous in several aspects, SFE is only designed for either supervised or unsupervised learning, and is not suitable when there are limited labeled samples and a large number of unlabeled samples, since the data distribution knowledge is likely to be poorly exploited. To tackle this problem, we propose a novel manifold regularized SFE (MRSFE) framework for semi-supervised image classification. In MRSFE, the local structures of the whole dataset are preserved, and the data distribution is well exploited. By integrating the label information, low rank property of the features and data distribution knowledge, the proposed MRSFE could select and extract reliable discriminative features when the labeled samples are scarce. An efficient and easy-to-implement algorithm is designed to find the solutions. Extensive experimental results on a real-world image dataset demonstrate the superiority of our method."
  },
  "bmvc2015_main_leafsegmentationunderlooselycontrolledconditions": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Leaf Segmentation under Loosely Controlled Conditions",
    "authors": [
      "Simone Buoncompagni",
      "Dario Maio",
      "Vincent Lepetit"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper133/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper133/paper133.pdf",
    "published": "2015-09",
    "summary": "We propose a robust and accurate method for segmenting specular objects acquired under loosely controlled conditions. We focus here on leaves because leaf segmentation plays a crucial role for plant identification, and accurately capturing the local boundary structures is critical for the success of the recognition. Popular techniques are based on Expectation-Maximization and estimate the color distributions of the background and foreground pixels of the input image. As we show, such approaches suffer in presence of shadows and reflections thus leading to inaccurate detected shapes. Classification-based methods are more robust because they can exploit prior information, however they do not adapt to the specific capturing conditions for the input image. Methods with regularization terms are prone to smooth the segments boundaries, which is undesirable. In this paper, we show we can get the best of the EM-based and classification-based methods by first segmenting the pixels around the leaf boundary, and use them to initialize the color distributions of an EM optimization. We show that this simple approach results in a robust and accurate method."
  },
  "bmvc2015_main_shapefromfocuswithadaptivefocusmeasureandhighorderderivatives": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Shape from Focus with Adaptive Focus Measure and High Order Derivatives",
    "authors": [
      "Yuval Frommer",
      "Rami Ben-Ari",
      "Nahum Kiryati"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper134/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper134/paper134.pdf",
    "published": "2015-09",
    "summary": "Shape From Focus (SFF) methods frequently use a single focus measure to obtain a depth map. Common focus measures are fixed and spatially invariant. In this paper we present a framework to create an adaptive focus measure based on ensemble of basis focus operators. Using the proposed framework we derive a new spatially variant focus measure obtained from linear combination of image derivatives. This approach effectively generalizes some of the existing measures. A new measure emerged from the proposed framework includes high order derivatives and presents a highly reliable focus measure. We rely on the focus curve standard deviation (CSTD) to determine the linear coefficients in our model. The emerged focus measure copes effectively with texture variation, strong intensity edges and depth discontinuities. Using CSTD we further suggest a new approach for aggregation in the focus volume succeeded by reconstruction based on the focus curve centroid. This different approach of aggregation and reconstruction yields improved depth maps, respecting shape smoothness and depth discontinuities for diversity of textured images. We assess the performance of our new approach by extensive experiments with highly realistic synthetic images and real images including two unique cases captured in the wild. In terms of focus measure, we significantly outperform the state-of-the-art, while presenting superior results comparing to two previously published alternatives."
  },
  "bmvc2015_main_asegmentationtechniqueforflexiblepipesindeepunderwaterenvironments": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "A Segmentation Technique for Flexible Pipes in Deep Underwater Environments",
    "authors": [
      "Saulo Pessoa",
      "Vinicius Cesar",
      "Bernardo Reis",
      "Judith Kelner",
      "Ismael Santos"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper135/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper135/paper135.pdf",
    "published": "2015-09",
    "summary": "This paper presents a segmentation technique for flexible pipes in deep underwater environments using low-light monochrome cameras. The technique relies on an alternating pattern of black and white regions marked over the pipe and is divided into three stages: a pre-processing stage for image noise-reduction; a multi-level topological binarization for collecting pipe region candidates; and a backtracking search constrained by inherent pipe characteristics for segmenting its regions. The proposed technique has been tested using video sequences from a real offshore operation and succeeded in segmenting 95.29% of the frames, while local adaptive thresholding methods achieved, at best, a rate of 68.49%."
  },
  "bmvc2015_main_structuralsymmetriesfrommotionforscenereconstructionandunderstanding": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Structural Symmetries from Motion for Scene Reconstruction and Understanding",
    "authors": [
      "Natesh Srinivasan",
      "Luca Carlone",
      "Frank Dellaert"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper136/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper136/paper136.pdf",
    "published": "2015-09",
    "summary": "The identification and description of partial symmetries in man-made structures is a powerful tool to improve the quality of 3D reconstruction from unordered images and to enable high-level understanding of scene geometry. In this work we propose an approach to identify symmetries and exploit them in Structure from Motion (SfM). Our first contribution is a symmetry detection approach that uses the 3D geometry of the scene as well as 2D appearance clues. We show that a particular parametrization of the transformation space (space in which each point represents a candidate symmetry relation) exposes the dominant symmetries in the scene. Then, we use appearance information to prune incorrect symmetry hypotheses. The second contribution is a constrained bundle adjustment (CBA) scheme that jointly optimizes for the best 3D reconstruction and the symmetry generators. Contrarily to related work on CBA, our approach models n-fold (rotational and translational) repetitions of architectural elements, and allows estimating a generative model of the 3D geometry. Experimental results confirm that our method can correctly identify and exploit partial symmetries in noisy and incomplete SfM datasets."
  },
  "bmvc2015_main_stereotrackingand3dreconstructionofunderwaterpipes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Stereo Tracking and 3D Reconstruction of Underwater Pipes",
    "authors": [
      "Vinicius Cesar",
      "Bernardo Reis",
      "Saulo Pessoa",
      "Judith Kelner",
      "Ismael Santos"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper137/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper137/paper137.pdf",
    "published": "2015-09",
    "summary": "Installing flexible oil pipes in deep underwater environments is a risky operation, which is currently monitored by manually comparing the geometric conditions to the simulated studies. In order to avoid this error-prone assessment, this paper proposes an algorithm to track and reconstruct the pipe medial axis using stereo cameras. By taking advantage of the scenario illumination and the pipe texture, it is possible to devise an energy maximization approach that efficiently tracks the pipe. The reconstruction technique is an enhancement over a state-of-the-art B-spline reconstruction algorithm, specializing it for plane curves. Experiments on synthetic and laboratory videos report a very low mean tracking and reconstruction errors, while there are strong evidences of the robustness on a real scenario."
  },
  "bmvc2015_main_nonlinearmetriclearningforalzheimer\u2019sdiseasediagnosiswithintegrationoflongitudinalneuroimagingfeatures": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Nonlinear Metric Learning for Alzheimer\u2019s Disease Diagnosis with Integration of Longitudinal Neuroimaging Features",
    "authors": [
      "Bibo Shi",
      "Yani Chen",
      "Kevin Hobbs",
      "Charles D. Smith",
      "Jundong Liu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper138/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper138/paper138.pdf",
    "published": "2015-09",
    "summary": "Identifying neuroimaging biomarkers of Alzheimer\u2019s disease (AD) is of great importance for diagnosis and prognosis of the disease. In this study, we develop a novel nonlinear metric learning method to improve biomarker identification for Alzheimer\u2019s disease and its early stage Mild Cognitive Impairment (MCI). Formulated under a constrained optimization framework, the proposed method learns a smooth nonlinear feature space transformation that pulls the samples of the same class closer to each other while pushing different classes further away. The thin-plate spline (TPS) is chosen as the geometric model due to its remarkable versatility and representation power in accounting for sophisticated deformations. In addition, a multi-resolution patch-based feature selection strategy is proposed to extract both cross-sectional and longitudinal features from MR brain images. Using the ADNI dataset, we evaluate the effectiveness of the proposed metric learning and feature extraction strategies and demonstrate the improvements over the state-of-the-art solutions within the same category."
  },
  "bmvc2015_main_boostedmetriclearningforefficientidentity-basedfaceretrieval": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Boosted Metric Learning for Efficient Identity-Based Face Retrieval",
    "authors": [
      "Romain Negrel",
      "Alexis Lechervy",
      "Frederic Jurie"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper139/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper139/paper139.pdf",
    "published": "2015-09",
    "summary": "This paper presents MLBoost, an efficient method for learning to compare face signatures, and shows its application to the hierarchical organization of large face databases. More precisely, the proposed metric learning (ML) algorithm is based on boosting so that the metric is learned iteratively by combining several weak metrics. Boosting allows our method to be free of any hyper-parameters (no cross-validation required) and to be robust with respect to overfitting. This MLBoost algorithm can be trained from constraints involving two pairs of vectors (quadruplets) with a quadratic complexity. The paper also shows how it can be included in a semi-supervised hierarchical clustering framework adapted to identity based face search. Our approach is validated on a benchmark relying on the Labelled Faces in the Wild (LFW) dataset supplemented with 1M face distractors."
  },
  "bmvc2015_main_enforcingpoint-wisepriorsonbinarysegmentation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Enforcing Point-wise Priors on Binary Segmentation",
    "authors": [
      "Feng Li",
      "Fatih Porikli"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper140/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper140/paper140.pdf",
    "published": "2015-09",
    "summary": "Non-negative point-wise priors such as saliency map, defocus field, foreground mask, object location window, and user given seeds, appear in many fundamental computer vision problems. These priors come in the form of confidence or probability values, and they are often incomplete, irregular, and noisy, which eventually makes the labelling task a challenge. Our goal is to extract image regions that are aligned on the object boundaries and also in accordance with the given point-wise priors. To this end, we define a graph Laplacian spectrum based cost function and embed it into a minimization framework. For a comprehensive understanding, we analyze five alternative formulations, and demonstrate that the robust function version produces consistently superior results."
  },
  "bmvc2015_main_sparsediscriminationbasedmultisetcanonicalcorrelationanalysisformulti-featurefusionandrecognition": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Sparse Discrimination based Multiset Canonical Correlation Analysis for Multi-Feature Fusion and Recognition",
    "authors": [
      "Hongkun Ji",
      "Xiaobo Shen",
      "Quansen Sun",
      "Zexuan Ji"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper141/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper141/paper141.pdf",
    "published": "2015-09",
    "summary": "Multiset canonical correlation analysis is a powerful technique for analyzing linear correlations among multiple representation data. However, it usually fails to discover the intrinsic sparse reconstructive relationship and discriminating structure of multiple data spaces in real-world applications. In this paper, by taking discriminative information of within-class and between-class sparse reconstruction into account, we propose a novel algorithm, called sparse discrimination based multiset canonical correlations (SDbMCCs), to explicitly consider both discriminative structure and sparse reconstructive relationship in multiple representation data. In addition to maximizing between-set cumulative correlations, SDbMCC minimizes within-class sparse reconstructive distances and maximizes between-class sparse reconstructive distances, simultaneously. The feasibility and effectiveness of the proposed method is verified on four popular databases (CMU PIE, ETH-80, AR and Extended Yale-B) with promising results."
  },
  "bmvc2015_main_towards4dcoupledmodelsofconversationalfacialexpressioninteractions": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Towards 4D Coupled Models of Conversational Facial Expression Interactions",
    "authors": [
      "Jason Vandeventer",
      "Lukas Gr\u00e4ser",
      "Magdalena Rychlowska",
      "Paul L. Rosin",
      "Dave Marshall"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper142/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper142/paper142.pdf",
    "published": "2015-09",
    "summary": "In this paper we introduce a novel approach for building 4D coupled statistical models of conversational facial expression interactions. To build these coupled models we use 3D AAMs for feature extraction, 4D polynomial fitting for sequence representation, and concatenated feature vectors of frontchannel-backchannel interactions (with offset values) for the coupled model. Using a coupled model of conversation smile interactions, we predicted each sequence\u2019s backchannel signal. In a subsequent experiment, human observers rated predicted sequences as highly similar to the originals. Our results demonstrate the usefulness of coupled models as powerful tools to analyse and synthesise key aspects of conversational interactions, including conversation timings, backchannel responses to frontchannel signals, and the spatial and temporal dynamics of conversational facial expression interactions."
  },
  "bmvc2015_main_aunifiedbayesianapproachtomulti-framesuper-resolutionandsingle-imageupsamplinginmulti-sensorimaging": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "A Unified Bayesian Approach to Multi-Frame Super-Resolution and Single-Image Upsampling in Multi-Sensor Imaging",
    "authors": [
      "Thomas K\u00f6hler",
      "Johannes Jordan",
      "Andreas Maier",
      "Joachim Hornegger"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper143/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper143/paper143.pdf",
    "published": "2015-09",
    "summary": "For a variety of multi-sensor imaging systems, there is a strong need for resolution enhancement. In this paper, we propose a unified method for single-image upsampling and multi-frame super-resolution of multi-channel images. We derive our algorithm from a Bayesian model that is formulated by a novel image prior to exploit sparsity of individual channels as well as a locally linear regression between the complementary channels. The reconstruction of high-resolution multi-channel images from low-resolution ones and the estimation of associated hyperparameters to define our prior model is formulated as a joint energy minimization. We introduce an alternating minimization scheme to solve this non-convex optimization problem efficiently. Our framework is applicable to various types of multi-sensor setups that are addressed in our experimental evaluation, including color, multispectral and 3-D range imaging. Comprehensive qualitative and quantitative comparisons demonstrate that our method outperforms state-of-the-art algorithms."
  },
  "bmvc2015_main_boostingtheperformanceofmodel-based3dtrackingbyemployinglowlevelmotioncues": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Boosting the Performance of Model-based 3D Tracking by Employing Low Level Motion Cues",
    "authors": [
      "Ammar Qammaz",
      "Nikolaos Kyriazis",
      "Antonis A. Argyros"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper144/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper144/paper144.pdf",
    "published": "2015-09",
    "summary": "3D tracking of objects and hands in an object manipulation scenario is a very interesting computer vision problem with a wide variety of applications ranging from consumer electronics to robotics and medicine. Recent advances in this research topic allow for 3D tracking of complex scenarios involving bi-manual manipulation of several rigid objects using commodity hardware and with high accuracy. The problem with these approaches is that their computational complexity is proportional to the number of objects in the scene. The problem with these approaches is that they treat tracking as a search problem whose dimensionality increases with the number of objects in the scene. In this paper we present a method that utilizes simple low level motion cues for dynamically assigning computational resources to parts of the scene where they are actually required. In a series of experiments, we show that this simple idea improves tracking performance dramatically at a cost of only a minor degradation of tracking accuracy."
  },
  "bmvc2015_main_real-timergb-dtrackingwithdepthscalingkernelisedcorrelationfiltersandocclusionhandling": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters and Occlusion Handling",
    "authors": [
      "Massimo  Camplani",
      "Sion Hannuna",
      "Majid Mirmehdi",
      "Dima Damen",
      "Adeline Paiement",
      "Lili  Tao",
      "Tilo Burghardt"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper145/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper145/paper145.pdf",
    "published": "2015-09",
    "summary": "We present a real-time RGB-D object tracker which manages occlusions and scale changes in a wide variety of scenarios. Its accuracy matches, and in many cases outperforms, state-of-the-art algorithms for precision and it far exceeds most in speed. We build our algorithm on the existing colour-only KCF tracker which uses the `kernel trick' to extend correlation filters for fast tracking. We fuse colour and depth cues as the tracker's features, and furthermore, exploit the depth data to both adjust a given target's scale, and detect and manage occlusions in such a way as to maintain real-time performance, exceeding on average 40~fps. We benchmark our approach using 2 publicly available datasets and make our easy-to-extend modularised code available to other researchers."
  },
  "bmvc2015_main_experimentalevaluationofthebag-of-featuresmodelforunsupervisedlearningofimages": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Experimental Evaluation of the Bag-of-Features Model for Unsupervised Learning of Images",
    "authors": [
      "Mariana Afonso",
      "Luis F. Teixeira"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper146/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper146/paper146.pdf",
    "published": "2015-09",
    "summary": "This paper presents the results of an experimental study of the popular Bag-of-Features (BoF) model for the application of unsupervised learning of images, or image clustering. Although this method has been extensively applied for image classification and scene recognition, there has been few works which employ it in an unsupervised way. Also, due to the fact that the BoF model requires a great amount of steps, algorithms and parameter settings, we felt like there was a lack of detailed studies about the subject. We implemented testing routines in Python which we made publicly available in GitHub. In order to assess the performance of the model, three image datasets were used, namely, Coil-20 dataset, Natural and Urban dataset and Event dataset. The results obtained indicate that the BoF method provides a good representation of simple image collections for the purpose of clustering. However, it requires fine tunning of the parameters and algorithms for each dataset and obtains poor results for more complex scene datasets. We can therefore conclude that more advanced techniques are required in order to be able to effectively extract information from large image collections."
  },
  "bmvc2015_main_indoorlocalisationwithregressionnetworksandplacecellmodels": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Indoor Localisation with Regression Networks and Place Cell Models",
    "authors": [
      "Jose Rivera-Rubio",
      "Ioannis Alexiou",
      "Anil A. Bharath"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper147/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper147/paper147.pdf",
    "published": "2015-09",
    "summary": "Animals use a variety of environmental cues in order to recognise their location. One of the key behaviours found in a certain type of biological neuron - known as place cells - is a rate-coding effect: a neuron's rate of firing decreases with distance from some landmark location. In this work, we used visual information from wearable and hand-held cameras in order to reproduce this rate-coding effect in artificial place cells (APCs). The accuracy of localisation using these APCs was evaluated using different visual descriptors and different place cell widths. Simple localisation using APCs was feasible by noting the identity of the APC yielding the maximum response. We also propose using joint coding within a number of automatically defined APCs as a population code for self-localisation. Using both approaches we were able to demonstrate good self-localisation from very small images taken in indoor settings. The error performance using APCs is favourable when compared with ground-truth and LSD-SLAM, even without the use of a motion model."
  },
  "bmvc2015_main_kinshipverificationwithdeepconvolutionalneuralnetworks": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Kinship Verification with Deep Convolutional Neural Networks",
    "authors": [
      "Kaihao Zhang",
      "Yongzhen Huang",
      "Chunfeng Song",
      "Hong Wu",
      "Liang Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper148/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper148/paper148.pdf",
    "published": "2015-09",
    "summary": "Kinship verification from facial images is an interesting and challenging problem. The current algorithms on this topic typically represent faces with multiple low-level features, followed by a shallow learning model. However, these general manual features cannot well discover information implied in facial images for kinship verification, and thus even current best algorithms are not satisfying. In this paper, we propose to extract high-level features for kinship verification based on deep convolutional neural networks. Our method is end-to-end, without complex pre-processing often used in traditional methods. The high-level features are produced from the neuron activations of the last hidden layer, and then fed into a soft-max classifier to verify the kinship of two persons. Considering the importance of facial key-points, we also extract key-points-based features for kinship verification. Experimental results demonstrate that our proposed approach is very effective even with limited training samples, largely outperforming the state-of-the-art methods. On two most widely used kinship databases, our method achieves 5.2% and 10.1% improvements compared with the previous best one, respectively."
  },
  "bmvc2015_main_exploitinglow-rankstructurefordiscriminativesub-categorization": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Exploiting Low-rank Structure for Discriminative Sub-categorization",
    "authors": [
      "Zheng Xu",
      "Xue Li",
      "Kuiyuan Yang",
      "Thomas Goldstein"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper149/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper149/paper149.pdf",
    "published": "2015-09",
    "summary": "In visual recognition, sub-categorization has been proposed to deal with large intra-class variance of samples in a category. Instead of learning a single classifier for each category, discriminant sub-categorization approaches divide a category into several sub-categories and simultaneously train classifiers for each sub-category. In this paper, we propose a novel approach for discriminative sub-categorization. Our method jointly trains the exemplar classifier for each positive sample to address the intra-variance of a category and exploits the low rank structure to preserve common information while discovering sub-categories. We formulate the problem as a convex objective function and introduce an efficient solver based on alternating direction method of multipliers.Comprehensive experiments on various datasets demonstrate the effectiveness and efficiency of the proposed method in both sub-category discovery and visual recognition."
  },
  "bmvc2015_main_sparse3dconvolutionalneuralnetworks": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Sparse 3D convolutional neural networks",
    "authors": [
      "Ben Graham"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper150/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper150/paper150.pdf",
    "published": "2015-09",
    "summary": "We have implemented a convolutional neural network designed for processing sparse three-dimensional input data. The world we live in is three dimensional so there are a large number of potential applications including 3D object recognition and analysis of space-time objects. In the quest for efficiency, we experiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice."
  },
  "bmvc2015_main_multi-taskgaussianprocessregression-basedimagesuperresolution": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multi-task Gaussian Process Regression-based Image Super Resolution",
    "authors": [
      "Xinwei Jiang",
      "Jie Yang",
      "Lei Ma",
      "Yiping Yang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper151/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper151/paper151.pdf",
    "published": "2015-09",
    "summary": "This paper presents a novel framework for image super resolution (SR) based on the multi-task gaussian process (MTGP) regression. The core idea is to treat each pixel prediction using gaussian process regression as one single task and cast recovering a high resolution image patch as a multi-task learning problem. In contrast to prior gaussian process regression-based SR approaches, our algorithm induces the inter-task correlation for considering image structures. We demonstrate the efficiency and effectiveness of the proposed method by applying it to the classic image dataset and experimental results show our approach is competitive with even outperforms the related and state-of-the-art methods."
  },
  "bmvc2015_main_spatiotemporalstereomatchingwith3ddisparityprofiles": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Spatiotemporal Stereo Matching with 3D Disparity Profiles",
    "authors": [
      "Yongho Shin",
      "Kuk-Jin Yoon"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper152/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper152/paper152.pdf",
    "published": "2015-09",
    "summary": "Adaptive support weights and over-parameterized disparity estimation truly improve the accuracy of stereo matching by enabling window-based similarity measures to handle depth discontinuities and non-fronto-parallel surfaces more effectively. Nevertheless, a disparity map sequence obtained in a frame-by-frame manner still tends to be inconsistent even with the use of state-of-the-art stereo matching methods. To solve this inconsistency problem, we propose a window-based spatiotemporal stereo matching method. We exploit the 3D disparity profile, which represents the disparities and window normals over multiple frames, and incorporate it into the PatchMatch Belief Propagation (PMBP) framework. Here, to make the 3D disparity profile more reliable, we also present the optical flow transfer method. Experimental results show the proposed method yields more consistent disparity map sequences than does the original PMBP-based method."
  },
  "bmvc2015_main_randomizedglobaltransformationapproachfordensecorrespondence": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Randomized Global Transformation Approach for Dense Correspondence",
    "authors": [
      "Kihong Park",
      "Seungryong Kim",
      "Seungchul Ryu",
      "Kwanghoon Sohn"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper153/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper153/paper153.pdf",
    "published": "2015-09",
    "summary": "This paper describes a randomized global transformation approach to estimate dense correspondence for image pairs taken under challengingly different photometric and geometric conditions. Our approach assumes that a correspondence field consists of piecewise parametric transformation model. While conventional approaches consider large search space including flow and geometric fields exhaustively, our approach is based on an inference of optimal global transformation model from transformation candidates. To build a reliable global transformation hypothesis, we build optimal global transformation candidates with a randomized manner from an initial sparse feature correspondence, followed by a transformation clustering. Furthermore, the optimal global transformation is estimated as a cost filtering scheme with fast edge-aware filtering to provide a geometrical smoothness. Experiments demonstrate outstanding performance of our approach in terms of correspondence accuracy and computational complexity."
  },
  "bmvc2015_main_achievingturbidityrobustnessonunderwaterimageslocalfeaturedetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Achieving Turbidity Robustness on Underwater Images Local Feature Detection",
    "authors": [
      "Felipe Codevilla",
      "Joel De O. Gaya",
      "Nelson Duarte Filho",
      "Silvia S. C. Costa Botelho"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper154/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper154/paper154.pdf",
    "published": "2015-09",
    "summary": "Methods to detect local features have been made to be invariant to many transformations. So far, the vast majority of feature detectors consider robustness just to over-land effects. However, when capturing pictures in underwater environments, there are media specific properties that can degrade the visual quality the captured images. Little work has been made in order to study the robustness that the popular feature detectors have to underwater environment image conditions. We develop a new dataset, called TURBID, where we produced real seabed images with different amounts of degradation. On this dataset, we search over multiple feature detectors from the literature to indicate the ones with more robust properties. We concluded that scale-invariant detectors are more robust to degradation of underwater images. Finally, we elected Center Surround Extremas, KAZE, Difference of Gaussians and the Hessian-Laplace as the best detectors for this environment on all tested scenes."
  },
  "bmvc2015_main_theself-equalizingdebruijnsequencefor3dprofilometry": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "The Self-Equalizing De Bruijn Sequence for 3D Profilometry",
    "authors": [
      "Tomislav Petkovi\u0107",
      "Tomislav Pribani\u0107",
      "Matea \u0110onli\u0107"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper155/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper155/paper155.pdf",
    "published": "2015-09",
    "summary": "Using color in 3D profilometry usually requires a tedious color calibration to mitigate the undesired effects of ambient lighting, object albedo, non-equal channel gains, and channel cross-talk. We propose a novel De Bruijn sequence for multi-channel structured light that removes the need for color calibration of a camera-projector pair. The proposed sequence has the following desirable properties: (1) it enables the extraction of ambient lighting, (2) it enables the cancellation of object albedo, and (3) it enables the equalization of channel gains."
  },
  "bmvc2015_main_afastandrobustellipsedetectorbasedontop-downleast-squarefitting": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "A fast and robust ellipse detector based on top-down least-square fitting",
    "authors": [
      "Yongtao Wang",
      "Zheqi He",
      "Xicheng Liu",
      "Zhi Tang",
      "Luyuan Li"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper156/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper156/paper156.pdf",
    "published": "2015-09",
    "summary": "Ellipse detection is a very important problem in the field of pattern recognition and computer vision. The existing algorithms often use a bottom-up strategy to combine edge points or elliptical arcs into ellipses, hence limit their robustness. In this paper, we propose a fast and robust ellipse detection algorithm which can accurately detect ellipses in the images. The main idea of the proposed algorithm is to exploit a novel top-down fitting strategy to combine edge points into ellipses and use integral chain to speed up the fitting process. Experimental results have demonstrated that our ellipse detection algorithm achieves a better performance than the state-of-the-art methods on the common evaluation measures of F1 score and average execution time."
  },
  "bmvc2015_main_multiviewreconstructionofcomplexorganicshapes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multiview Reconstruction of Complex Organic Shapes",
    "authors": [
      "Jasenko Zivanov",
      "Thomas Vetter"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper157/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper157/paper157.pdf",
    "published": "2015-09",
    "summary": "We propose a novel narrow baseline multiview stereo surface reconstruction method that is specifically aimed at complex shapes of biological origin that show many thin protrusions and curved occluding contours. Our method is built around fitting local quadrics to occluding contours and it thus avoids any planarity assumptions that are common to other state of the art methods. We describe a complete pipeline that begins with calibrated noisy images and produces a final watertight surface. We present a novel technique to detect pixel precise internal contours and to fit local quadrics to them. This procedure is designed to deal with curved occluding contours, and it is very robust to noise and to slow changes in surface radiance. Our method can even reconstruct shapes from sequences where the illumination is attached to the observer and not the scene. We demonstrate the potential of our method by reconstructing the intricate shape of a tiny insect from images taken under a scanning electron microscope."
  },
  "bmvc2015_main_manifoldregularizedtransferdistancemetriclearning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Manifold Regularized Transfer Distance Metric Learning",
    "authors": [
      "Haibo Shi",
      "Yong Luo",
      "Chao Xu",
      "Yonggang Wen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper158/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper158/paper158.pdf",
    "published": "2015-09",
    "summary": "The performance of many computer vision and machine learning algorithms are heavily depend on the distance metric between samples. It is necessary to exploit abundant of side information like pairwise constraints to learn a robust and reliable distance metric. While in real world application, large quantities of labeled data is unavailable due to the high labeling cost. Transfer distance metric learning (TDML) can be utilized to tackle this problem by leveraging different but certain related source tasks to learn a target metric. The recently proposed decomposition based TDML (DTDML) is superior to other TDML methods in that much fewer variables need to be learned. In spite of this success, the learning of the combination coefficients in DTDML still relies on the limited labeled data in the target task, and the large amounts of unlabeled data that are typically available are discarded. To utilize both the information contained in the source tasks, as well as the unlabeled data in the target task, we introduce manifold regularization in DTDML and develop the manifold regularized transfer distance metric learning (MTDML). In particular, the target metric in MTDML is learned to be close to an integration of the source metrics under the manifold regularization theme. That is, the target metric is smoothed along each data manifold that is approximated by all the labeled and unlabeled data in the target task and each source metric. In this way, more reliable target metric could be obtained given the limited labeled data in the target task. Extensive experiments on the NUS-WIDE and USPS dataset demonstrate the effectiveness of the proposed method."
  },
  "bmvc2015_main_top-downsaliencywithlocality-constrainedcontextualsparsecoding": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Top-down saliency with Locality-constrained Contextual Sparse Coding",
    "authors": [
      "Hisham Cholakkal",
      "Deepu Rajan",
      "Jubin Johnson"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper159/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper159/paper159.pdf",
    "published": "2015-09",
    "summary": "We propose a sparse coding based framework for top-down salient object detection in which three locality constraints are integrated. First is the spatial or contextual locality constraint in which features from adjacent regions have similar code, second is the feature-domain locality constraint in which similar features have similar code, and third is the category-domain locality constraint in which features are coded using similar atoms from each partition of the dictionary, where each partition corresponds to an object category. This faster coding strategy produces better saliency maps compared to conventional sparse coding. Proposed codes are max-pooled over a spatial neighborhood for saliency estimation. In spite of its simplicity, the proposed top-down saliency achieves state-of-the-art results at patch-level on two challenging datasets-Graz-02 and PASCAL VOC-07. A novel Gaussian-weighted interpolation further improves pixel-level saliency map derived from the patch-level map."
  },
  "bmvc2015_main_veryefficienttrainingofconvolutionalneuralnetworksusingfastfouriertransformandoverlap-and-add": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add",
    "authors": [
      "Tyler Highlander",
      "Andres Rodriguez"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper160/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper160/paper160.pdf",
    "published": "2015-09",
    "summary": "Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to O((N^2)(n^2)) per kernel per layer where the inputs are N x N arrays and the kernels are n x n arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of O((N^2)log_2(N)) using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when N >> n as is the case in CNNs. We mitigate this by using the ``overlap-and-add'' technique reducing the computational complexity to O((N^2)log_2(n)) per kernel. This method increases the algorithm's efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 50.4 times the traditional convolution implementation."
  },
  "bmvc2015_main_deepq-learningforactiverecognitionofgermsbaselineperformanceonastandardizeddatasetforactivelearning": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Deep Q-learning for Active Recognition of GERMS: Baseline performance on a standardized dataset for active learning",
    "authors": [
      "Mohsen Malmir",
      "Karan Sikka",
      "Deborah Forster",
      "Javier Movellan",
      "Garison Cottrell"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper161/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper161/paper161.pdf",
    "published": "2015-09",
    "summary": "In this paper, we introduce GERMS, a dataset designed to accelerate progress on active object recognition in the context of human robot interaction. GERMS consists of a collection of videos taken from the point of view of a humanoid robot that receives objects from humans and actively examines them. GERMS provides methods to simulate, evaluate, and compare active object recognition approaches that close the loop between perception and action without the need to operate physical robots. We present a benchmark system for active object recognition based on deep Q-learning (DQL). The system learns to actively examine objects by minimizing overall classification error using standard back-propagation and Q-learning. DQL learns an efficient policy that achieves high levels of accuracy with short observation periods."
  },
  "bmvc2015_main_cameraposeandfocallengthestimationusingregularizeddistanceconstraints": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Camera Pose and Focal Length Estimation Using Regularized Distance Constraints",
    "authors": [
      "Ekaterina Kanaeva",
      "Lev Gurevich",
      "Alexander Vakhitov"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper162/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper162/paper162.pdf",
    "published": "2015-09",
    "summary": "We propose a new method for camera pose estimation with unknown focal length (PnPf problem). We combine projection equations and distance constraints in a single statistically significant cost function in the form of least squares. We fix the space of the search as a linear combination of several right singular vectors of the least squares system matrix. We use linear programming techniques to find feasible solutions faster. Then we do nonlinear refinement with Levenberg-Marquardt. Numerical experiments demonstrated that the method is faster than the state-of-the-art methods for point numbers up to several hundreds, and real-life structure-from-motion demonstrates its applicability for models having $10^3$-$10^5$ points. It has the same accuracy of estimates as the the state-of-the-art methods. We show that the method offers a tradeoff between speed and accuracy, allowing the estimation to run several times faster while slightly increasing the mean reprojection error."
  },
  "bmvc2015_main_classifyingglobalscenecontextforon-linemultipletrackerselection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Classifying Global Scene Context for On-line Multiple Tracker Selection",
    "authors": [
      "Salma Moujtahid",
      "Stefan Duffner",
      "Atilla Baskurt"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper163/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper163/paper163.pdf",
    "published": "2015-09",
    "summary": "In this paper, we present a novel framework for combining several independent on-line trackers using the visual scene context. The aim of our method is to decide automatically at each point in time which specific tracking algorithm works best under the given scene or acquisition conditions. To this end, we define a set of generic global context features computed on each frame of a set of training videos. At the same time, we record the performance of each individual tracker on these videos in terms of object bounding box overlap with the ground truth. Then a classifier is trained to estimate which tracker gives the best result given the global scene context in a particular frame. We experimentally showed that such a classifier can predict the best tracker with a precision of over 80% in unknown videos with unknown environments. The proposed tracking method further filters the classifier responses temporarily using a Hidden Markov Model in order to avoid rapid oscillations between different trackers. Finally, we evaluated the overall tracking system and showed that this scene context-based tracker selection considerably improves the overall robustness and compares favourably with the state-of-the-art."
  },
  "bmvc2015_main_sketchbasedimageretrievalusinglearnedkeyshapes(lks)": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Sketch based Image Retrieval using Learned KeyShapes (LKS)",
    "authors": [
      "Jose M. Saavedra",
      "Juan Manuel Barrios"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper164/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper164/paper164.pdf",
    "published": "2015-09",
    "summary": "Sketch based image retrieval is a particular case of the image retrieval problem, in which a query is not a regular example image. Instead, the query is a hand-drawn sketch representing what the user is looking for. This kind of problem has a lot of applications, in particular when an example image is not available. For instance, in searching for design pieces in digital catalogs. The natural ambiguity of sketches as well as the poor skills of drawing make the problem very challenging, which is reflected in the low performance achieved by current methods. In this work, we present a novel method for describing sketches based on detecting mid-level patterns called learned keyshapes. Our experiments were performed in two datasets, one with 1326 images and the other with approximately 15k images. Our results show an increase of effectiveness around 17% on the smaller dataset and 98% on the larger one, which represent new state-of-the-art performance in the sketch based image retrieval domain. We also show that our method allows us to achieve good performance even when we use around 20% of the sketch content."
  },
  "bmvc2015_main_onlinevisualtrackingviacoupledobject-contextdictionary": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Online Visual Tracking via Coupled Object-Context Dictionary",
    "authors": [
      "Mingquan Ye",
      "Hong Chang",
      "Xilin Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper165/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper165/paper165.pdf",
    "published": "2015-09",
    "summary": "Sparse representation and context information have been extensively applied in visual tracking. In this paper, we make the most of context information outside the target bounding box to construct the distinct background dictionary. The pure target dictionary is then constructed by filtering out background patches from the target bounding box. At each frame, all relevant patches are encoded by the coupled dictionaries. Based on the reconstruction errors, we can efficiently compute the confidence value of each bounding box candidate. By investigating the changes of the reconstruction errors on the coupled dictionaries, we can effectively handle occlusion. Both quantitative and qualitative results demonstrate that the proposed tracker performs favorably compared with several state-of-the-art trackers on some challenging video sequences."
  },
  "bmvc2015_main_subspacealignmentbaseddomainadaptationforrcnndetector": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Subspace Alignment Based Domain Adaptation for RCNN Detector",
    "authors": [
      "Anant Raj",
      "Vinay P. Namboodiri",
      "Tinne Tuytelaars"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper166/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper166/paper166.pdf",
    "published": "2015-09",
    "summary": "In this paper, we propose subspace alignment based domain adaptation of the state of the art RCNN based object detector. The aim is to be able to achieve high quality object detection in novel, real world target scenarios without requiring labels from the target domain. While, unsupervised domain adaptation has been studied in the case of object classification, for object detection it has been relatively unexplored. In subspace based domain adaptation for objects, we need access to source and target subspaces for the bounding box features. The absence of supervision (labels and bounding boxes are absent) makes the task challenging. In this paper, we show that we can still adapt subspaces that are localized to the object by obtaining detections from the RCNN detector trained on source and applied on target. Then we form localized subspaces from the detections and show that subspace alignment based adaptation between these subspaces yields improved object detection. This evaluation is done by considering challenging real world datasets of PASCAL VOC as source and validation set of Microsoft COCO dataset as target for various categories."
  },
  "bmvc2015_main_time-slicepredictionofdyadichumanactivities": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Time-slice Prediction of Dyadic Human Activities",
    "authors": [
      "Maryam Ziaeefard",
      "Robert Bergevin",
      "Louis-Philippe Morency"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper167/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper167/paper167.pdf",
    "published": "2015-09",
    "summary": "Recognizing human activities from video data is being leveraged for surveillance and human-computer interaction applications. In this paper, we introduce the problem of time-slice activity recognition which aims to explore human activity at a smaller temporal granularity. Time-slice recognition is able to infer human behaviors from a short temporal window. It has been shown that the temporal slice analysis is helpful for motion characterization and in general for video content representation. These studies motivate us to consider time-slices for activity recognition. To this intent, we propose a new family of spatio-temporal descriptors which are optimized for early prediction with time-slice action annotations. Our predictive spatio-temporal interest point (Predict-STIP) representation is based on the intuition of temporal contingency between time-slices. Furthermore, we introduce a new dataset which is annotated at multiple short temporal windows, allowing the modeling of the inherent uncertainty in time-slice activity recognition. Our experimental results show performance comparable to human annotations."
  },
  "bmvc2015_main_adaptingransacsvmtodetectoutliersforrobustclassification": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Adapting RANSAC SVM to Detect Outliers for Robust Classification",
    "authors": [
      "Subhabrata Debnath",
      "Anjan Banerjee",
      "Vinay Namboodiri"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper168/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper168/paper168.pdf",
    "published": "2015-09",
    "summary": "Most visual classification tasks assume the authenticity of the label information. However, due to several reasons such as difficulty of annotation or inadvertently due to human error, the annotation can often be noisy. This results in examples that are wrongly annotated. In this paper, we consider the examples that are wrongly annotated to be outliers. The task of learning a robust inlier model in the presence of outliers is typically done through the RANSAC algorithm. In this paper, we show that instead of adopting RANSAC to obtain the `right' model, we could use many instances of randomly sampled sets to build lot of models. The collective decision of all these classifiers can be used to identify samples that are likely to be outliers. This results in a modification to RANSAC SVM to explicitly obtain probable outliers from the set of given samples. Once, the outliers are detected, these examples are excluded from the training set. The method can also be used to identify very hard examples from the training set. In this case, where we believe that the examples are correctly annotated, we can achieve good generalization when such examples are excluded from the training set. The method is evaluated using the standard PASCAL VOC dataset. We show that the method is particularly suited for identifying wrongly annotated examples resulting in improvement of more than 12\\% over the RANSAC SVM approach. Hard examples in PASCAL VOC dataset are also identified by this method and in fact this even results in a marginal improvement of the classification accuracy over the base classifier provided with all clean samples."
  },
  "bmvc2015_main_beyondmsermaximallystableregionsusingtreeofshapes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Beyond MSER: Maximally Stable Regions using Tree of Shapes",
    "authors": [
      "Petra Bosilj",
      "Ewa Kijak",
      "S\u00e9bastien Lef\u00e8vre"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper169/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper169/paper169.pdf",
    "published": "2015-09",
    "summary": "This article explores the application of a tree-based feature extraction algorithm for the widely-used MSER features, and proposes a Tree of Shapes based detector of Maximally Stable Regions. Changing an underlying component tree in the algorithm allows considering alternative properties and pixel orderings for extracting the Maximally Stable Regions. Differences introduced to the region structure with changing the underlying tree are discussed, as well as the spatial organization of the detected regions imposed by using a self-dual image representation for detection. Performance evaluation is carried out on a standard matching benchmark in terms of repeatability and matching score under different image transformations, as well as in a large scale image retrieval setup, measuring Mean Average Precision. The proposed descriptor is compared to the standard MSER implementation as well as a tree-based MSER implementation, achieving competitive results in the matching setup and outperforming the baseline MSER in the retrieval experiments."
  },
  "bmvc2015_main_exploringlocallyrigiddiscriminativepatchesforlearningrelativeattributes": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Exploring Locally Rigid Discriminative Patches for Learning Relative Attributes",
    "authors": [
      "Yashaswi Verma",
      "CV Jawahar"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper170/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper170/paper170.pdf",
    "published": "2015-09",
    "summary": "Relative attributes help in comparing two images based on their visual properties. These are of great interest as they have been shown to be useful in several vision related problems such as recognition, retrieval, and understanding image collections in general. In the recent past, quite a few techniques have been proposed for the relative attribute learning task that give reasonable performance. However, these have focused either on the algorithmic aspect or the representational aspect. In this work, we revisit these approaches and integrate their broader ideas to develop simple baselines. These not only take care of the algorithmic aspects, but also take a step towards analyzing a simple yet domain independent patch-based representation for this task. This representation can capture local shape in an image, as well as spatially rigid correspondences across regions in an image pair. The baselines are extensively evaluated on three challenging relative attribute datasets (OSR, LFW-10 and UT-Zap50K). Experiments demonstrate that they achieve promising results on the OSR and LFW-10 datasets, and perform better than the current state-of-the-art on the UT-Zap50K dataset. Moreover, they also provide some interesting insights about the problem, that could be helpful in developing the future techniques in this domain."
  },
  "bmvc2015_main_semanticdescriptionofmedicalimagefindingsstructuredlearningapproach": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Semantic description of medical image findings: structured learning approach",
    "authors": [
      "Pavel Kisilev",
      "Eugene Walach",
      "Sharbell Hashoul",
      "Ella Barkan",
      "Boaz Ophir",
      "Sharon Alpert"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper171/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper171/paper171.pdf",
    "published": "2015-09",
    "summary": "Computer Aided Diagnosis (CADx) systems are designed to assist doctors in medical image interpretation. However, a CADx is often thought of as a 'black box' whose diagnostic decision is not intelligible to a radiologist. Therefore, a system that uses semantic image interpretation, and mimics human image analysis, has clear benefits. We propose a new method for automatic textual description of medical image findings, such as lesions in medical images. The method performs joint estimation of semantic features of lesions from image measurements. We formalize this problem as learning to map a set of diverse medical image measurements to a set of semantic descriptor values. We use a structural learning framework to model individual semantic descriptors and their relationships. The parameters of the model are efficiently learned using the Structured Support Vector Machine (SSVM). The proposed approach generates radiological lexicon descriptors used to make a diagnosis of various diseases. This can help radiologists easily understand a diagnosis recommendation made by an automatic system, such as CADx. We apply the proposed method to publicly available and to proprietary breast and brain imaging datasets, and show that our method generates more accurate descriptions, as compared to other alternative approaches."
  },
  "bmvc2015_main_latentstructurepreservinghashing": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Latent Structure Preserving Hashing",
    "authors": [
      "Ziyun Cai",
      "Li Liu",
      "Mengyang Yu",
      "Ling Shao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper172/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper172/paper172.pdf",
    "published": "2015-09",
    "summary": "Aiming at efficient similarity search, hash functions are designed to embed high-dimensional feature descriptors to low-dimensional binary codes such that similar descriptors will lead to the binary codes with a short distance in the Hamming space. It is critical to effectively maintain the intrinsic structure and preserve the original information of data in a hashing algorithm. In this paper, we propose a novel hashing algorithm called Latent Structure Preserving Hashing (LSPH), with the target of finding a well-structured low-dimensional data representation from the original high-dimensional data through a novel objective function based on Nonnegative Matrix Factorization (NMF). Via exploiting the probabilistic distribution of data, LSPH can automatically learn the latent information and successfully preserve the structure of high-dimensional data. After finding the low-dimensional representations, the hash functions can be acquired through multi-variable logistic regression. Experimental results on two large-scale datasets, i.e., SIFT 1M and GIST 1M, show that LSPH can significantly outperform the state-of-the-art hashing techniques."
  },
  "bmvc2015_main_localfeaturebinarycodingforapproximatenearestneighborsearch": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Local Feature Binary Coding for Approximate Nearest Neighbor Search",
    "authors": [
      "Li Liu",
      "Mengyang Yu",
      "Ling Shao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper173/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper173/paper173.pdf",
    "published": "2015-09",
    "summary": "The potential value of hashing techniques has led to it becoming one of the most active research areas in computer vision and multimedia. However, most existing hashing methods for image search and retrieval are based on global representations, e.g., GIST, which lack the analysis of the intrinsic geometric property of local features and heavily limit the effectiveness of the hash code. In this paper, we propose a novel supervised hashing method called Local Feature Binary Coding (LFBC) for projecting local feature descriptors from a high-dimensional feature space to a lower-dimensional Hamming space via compact bilinear projections rather than a single large projection matrix. LFBC takes the matrix expression of local features as input and preserves the feature-to-feature and image-to-class structures simultaneously. Experimental results on challenging datasets including Caltech-256, SUN397 and NUS-WIDE demonstrate the superiority of LFBC compared with state-of-the-art hashing methods."
  },
  "bmvc2015_main_revealingsmoothstructureofvisualdatabypermutationonmanifolds": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Revealing Smooth Structure of Visual Data by Permutation on Manifolds",
    "authors": [
      "Yi-Lei Chen",
      "Chiou-Ting Hsu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper174/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper174/paper174.pdf",
    "published": "2015-09",
    "summary": "Advance in technology and commercial media has simplified the process of collecting large-scale visual data, but it also raises new challenges in data organization. In this paper, we propose to characterize data association by recovering an intrinsic order from an unorganized dataset. Our method is motivated by smooth manifold geometry. We advocate that the optimal data order should encode the shape of underlying manifold as well as the latent data association. Following the data order, we find a smooth path to visualize the latent topic of visual data with a perceptually reasonable transition. We develop an efficient algorithm Permutation on Manifolds (PoM) to solve this NP-hard permutation problem. Experiments on synthetic and real-world dataset demonstrate the potential of PoM to serve as a core technique of numerous applications."
  },
  "bmvc2015_main_normalizedautobinomialmarkovchannelsforpedestriandetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Normalized Autobinomial Markov Channels For Pedestrian Detection",
    "authors": [
      "Cosmin \u0162oca",
      "Mihai Ciuc",
      "Carmen P\u0103tra\u015fcu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper175/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper175/paper175.pdf",
    "published": "2015-09",
    "summary": "This paper brings significant contributions to the field of pedestrian detection by learning probabilistic dependencies and contextual information that draw special attention to the human body characteristics and silhouette shapes and play down other irrelevant features. More precisely, we introduce NAMC (Normalized Autobinomial Markov Channels) and study the efficiency of different configurations of cliques, providing a detailed experimental evaluation. Our proposed features outperform most of the solutions that have laid the foundations of pedestrian detection [2, 17]. Moreover, if we combine our novel features with gradient-based descriptors [15] and apply an efficient local decorrelation algorithm [30] to each channel, our results outperform the majority of the state-of-the-art solutions currently present in the Caltech Pedestrian Detection Benchmark [13]. We focus on a thorough analysis of the proposed feature model using the INRIA Pedestrian Dataset [10] as a benchmark to evaluate various parameter settings."
  },
  "bmvc2015_main_exploringpriorknowledgeforpedestriandetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Exploring Prior Knowledge for Pedestrian Detection",
    "authors": [
      "Yi Yang",
      "Zhenhua Wang",
      "Fuchao Wu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper176/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper176/paper176.pdf",
    "published": "2015-09",
    "summary": "In this paper, we aim to explore the role of prior knowledge for pedestrian detection. The main idea is to integrate human body priors into the design of features. To this end, we propose the symmetric features and cross-channel features so as to capture the specific information of human body. Experimental results demonstrate that our detector achieves state-of-the-art performance. What\u2019s more, the evaluation results on 'scale' subsets of Caltech-USA show that our detector performs best at medium scale and therefore has great potential to be integrated into real-world applications."
  },
  "bmvc2015_main_aptactionlocalizationproposalsfromdensetrajectories": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "APT: Action localization proposals from dense trajectories",
    "authors": [
      "Jan C. van Gemert",
      "Mihir Jain",
      "Ella Gati",
      "Cees G. M. Snoek"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper177/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper177/paper177.pdf",
    "published": "2015-09",
    "summary": "This paper is on action localization in video with the aid of spatio-temporal proposals. To alleviate the computational expensive video segmentation step of existing proposals, we propose bypassing the segmentations completely by generating proposals directly from the dense trajectories used to represent videos during classification. Our Action localization Proposals from dense Trajectories (APT) uses an efficient proposal generation algorithm to handle the high number of trajectories in a video. Our spatio-temporal proposals are faster than current methods and outperform the localization and classification accuracy of current proposals on UCF Sports, UCF 101, and MSR-II video datasets."
  },
  "bmvc2015_main_eventfishervectorsrobustencodingvisualdiversityofvisualstreams": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Event Fisher Vectors: Robust Encoding Visual Diversity of Visual Streams",
    "authors": [
      "Markus Nagel",
      "Thomas Mensink",
      "Cees G. M. Snoek"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper178/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper178/paper178.pdf",
    "published": "2015-09",
    "summary": "In this paper we focus on event recognition in visual image streams. More specifically, we aim to construct a compact representation which encodes the diversity of the visual stream from just a few observations. For this purpose, we introduce the Event Fisher Vector, a Fisher Kernel based representation to describe a collection of images or the sequential frames of a video. We explore different generative models beyond the Gaussian mixture model as underlying probability distribution. First, the Student's-t mixture model which captures the heavy tails of the small sample size of a collection of images. Second, Hidden Markov Models to explicitly capture the temporal ordering of the observations in a stream. For all our models we derive analytical approximations of the Fisher information matrix, which significantly improves recognition performance. We extensively evaluate the properties of our proposed method on three recent datasets for event recognition in photo collections and web videos, leading to an efficient compact image representation which achieves state-of-the-art performance on all these datasets."
  },
  "bmvc2015_main_deepstructuredmodelsforgroupactivityrecognition": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Deep Structured Models For Group Activity Recognition",
    "authors": [
      "Zhiwei Deng",
      "Mengyao Zhai",
      "Lei Chen",
      "Yuhao Liu",
      "Srikanth Muralidharan",
      "Mehrsan  Javan Roshtkhari",
      "Greg Mori"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper179/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper179/paper179.pdf",
    "published": "2015-09",
    "summary": "This paper presents a deep neural-network-based hierarchical graphical model for individual and group activity recognition in surveillance scenes. Deep networks are used to recognize the actions of individual people in a scene. Next, a neural-network-based hierarchical graphical model refines the predicted labels for each class by considering dependencies between the classes. This refinement step mimics a message-passing step similar to inference in a probabilistic graphical model. We show that this approach can be effective in group activity recognition, with the deep graphical model improving recognition rates over baseline methods."
  },
  "bmvc2015_main_fastactionretrievalfromvideosviafeaturedisaggregation": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Fast Action Retrieval from Videos via Feature Disaggregation",
    "authors": [
      "Jie Qin",
      "Li Liu",
      "Mengyang Yu",
      "Yunhong Wang",
      "Ling Shao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper180/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper180/paper180.pdf",
    "published": "2015-09",
    "summary": "Learning based hashing methods, which aim at learning similarity-preserving binary codes for efficient nearest neighbor search, have been actively studied recently. A majority of the approaches address hashing problems for image collections. However, due to the extra temporal information, videos are usually represented by much higher dimensional (thousands or even more) features compared with images, causing high computational complexity for conventional hashing schemes. In this paper, we propose a simple and efficient hashing scheme for high-dimensional video data. This method, called Disaggregation Hashing, exploits the correlations among different feature dimensions. An intuitive feature disaggregation method is first proposed, followed by a novel hashing algorithm based on different feature clusters. We demonstrate the efficiency and effectiveness of our method by theoretical analysis and exploring its application on action retrieval from video databases. Extensive experiments show the superiority of our binary coding scheme over state-of-the-art hashing methods."
  },
  "bmvc2015_main_poseestimationofkinematicchaininstancesviaobjectcoordinateregression": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Pose Estimation of Kinematic Chain Instances via Object Coordinate Regression",
    "authors": [
      "Frank Michel",
      "Alexander Krull",
      "Eric Brachmann",
      "Michael Ying Yang",
      "Stefan Gumhold",
      "Carsten Rother"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper181/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper181/paper181.pdf",
    "published": "2015-09",
    "summary": "In this paper, we address the problem of one shot pose estimation of articulated objects from an RGB-D image. In particular, we consider object instances with the topology of a kinematic chain, i.e. assemblies of rigid parts connected by prismatic or revolute joints. This object type occurs often in daily live, for instance in the form of furniture or electronic devices. Instead of treating each object part separately, we are using the relationship between parts of the kinematic chain and propose a new minimal pose sampling approach. This enables us to create a pose hypothesis for a kinematic chain consisting of K parts by sampling K 3D-3D point correspondences. To asses the quality of our method, we gathered a large dataset containing four objects and 7000+ annotated RGB-D frames. On this dataset we achieve considerably better results than a modified state-of-the-art pose estimation system for rigid objects."
  },
  "bmvc2015_main_hybridone-shot3dhandposeestimationbyexploitinguncertainties": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties",
    "authors": [
      "Georg Poier",
      "Konstantinos Roditakis",
      "Samuel Schulter",
      "Damien Michel",
      "Horst Bischof",
      "Antonis A. Argyros"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper182/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper182/paper182.pdf",
    "published": "2015-09",
    "summary": "Model-based approaches to 3D hand tracking have been shown to perform well in a wide range of scenarios. However, they require initialisation and cannot recover easily from tracking failures that occur due to fast hand motions. Data-driven approaches, on the other hand, can quickly deliver a solution, but the results often suffer from lower accuracy or missing anatomical validity compared to those obtained from model-based approaches. In this work we propose a hybrid approach for hand pose estimation from a single depth image. First, a learned regressor is employed to deliver multiple initial hypotheses for the 3D position of each hand joint. Subsequently, the kinematic parameters of a 3D hand model are found by deliberately exploiting the inherent uncertainty of the inferred joint proposals. This way, the method provides anatomically valid and accurate solutions without requiring manual initialisation or suffering from track losses. Quantitative results on several standard datasets demonstrate that the proposed method outperforms state-of-the-art representatives of the model-based, data-driven and hybrid paradigms."
  },
  "bmvc2015_main_particledynamicsandmulti-channelfeaturedictionariesforrobustvisualtracking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Particle dynamics and multi-channel feature dictionaries for robust visual tracking",
    "authors": [
      "Srikrishna Karanam",
      "Yang Li",
      "Richard J. Radke"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper183/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper183/paper183.pdf",
    "published": "2015-09",
    "summary": "We present a novel approach to solve the visual tracking problem in a particle filter framework based on sparse visual representations. Current state-of-the-art trackers use low-resolution image intensity features in target appearance modeling. Such features often fail to capture sufficient visual information about the target. Here, we demonstrate the efficacy of visually richer representation schemes by employing multi-channel feature dictionaries as part of the appearance model. To further mitigate the tracking drift problem, we propose a novel dynamic adaptive state transition model, taking into account the dynamics of the past states. Finally, we demonstrate the computational tractability of using richer appearance modeling schemes by adaptively pruning candidate particles during each sampling step, and using a fast augmented Lagrangian technique to solve the associated optimization problem. Extensive quantitative evaluations and robustness tests on several challenging video sequences demonstrate that our approach substantially outperforms the state of the art, and achieves stable results."
  },
  "bmvc2015_main_collaborativecorrelationtracking": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Collaborative Correlation Tracking",
    "authors": [
      "Guibo Zhu",
      "Jinqiao Wang",
      "Yi Wu",
      "Hanqing Lu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper184/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper184/paper184.pdf",
    "published": "2015-09",
    "summary": "Correlation filter based tracking has attracted many researchers' attention in recent years for high efficiency and robustness. Most existing works focus on exploiting different characteristics with correlation filters for visual tracking, e.g. circulant structure, kernel trick, effective feature representation and context information. However, how to handle the scale variation and the model drift is still an open problem. In this paper, we propose a unified collaborative correlation tracking framework that can handle both problems. Firstly, we extend correlation tracking filter by embedding the scale factor into the kernelized matrix to handle the scale variation. Then a novel long-term CUR filter for detection is learnt efficiently by random sampling to alleviate the model drift problem by detecting effective object candidates in the collaborative framework. In this way, the proposed approach could estimate the object state accurately and handle the model drift problem effectively. Extensive experiments show the superiority of the proposed method."
  },
  "bmvc2015_main_enablescaleandaspectratioadaptabilityinvisualtrackingwithdetectionproposals": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Enable Scale and Aspect Ratio Adaptability in Visual Tracking with Detection Proposals",
    "authors": [
      "Dafei Huang",
      "Lei Luo",
      "Mei Wen",
      "Zhaoyun Chen",
      "Chunyuan Zhang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper185/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper185/paper185.pdf",
    "published": "2015-09",
    "summary": "The newly proposed correlation filter based trackers can achieve appealing performance despite their great simplicity and superior speed. However, this kind of object trackers is not born with scale and aspect ratio adaptability. To tackle this problem, this paper integrates the class-agnostic detection proposal method, which is widely adopted in object detection area, into a correlation filter tracker. Additional optimizations such as feature integration and proposal rejection are also applied to make detection proposals more helpful. To reveal the effectiveness of our approach, two experiments are performed on 28 benchmark sequences with significant scale variation and 14 sequences with obvious aspect ratio change respectively. Among state-of-the-art trackers and existing scale-adaptive correlation filter variants, our proposed tracker reports the highest accuracy. Best accuracy is also achieved on the whole 50-sequence dataset with various challenging attributes at an average speed of 20.8 FPS, which proves the robustness and efficiency of our tracker."
  },
  "bmvc2015_main_multipleframesmatchingforobjectdiscoveryinvideo": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Multiple Frames Matching for Object Discovery in Video",
    "authors": [
      "Otilia Stretcu",
      "Marius Leordeanu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper186/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper186/paper186.pdf",
    "published": "2015-09",
    "summary": "Automatic discovery of foreground objects in video sequences is an important problem in computer vision with applications to object tracking, video segmentation and classification. We propose an efficient method for the discovery of object bounding boxes and the corresponding soft-segmentation masks across multiple video frames. We offer a graph matching formulation for bounding box selection and refinement using second and higher order terms. Our objective function takes into consideration local, frame-based information, as well as spatiotemporal and appearance consistency over multiple frames. First, we find an initial pool of candidate boxes using a novel and fast foreground estimation method in video, based on Principal Component Analysis. Then, we match the boxes across multiple frames using pairwise geometric and appearance terms. Finally, we refine their location and soft-segmentation using higher order potentials that establish appearance regularity over multiple frames. We test our method on the large scale YouTube-Objects dataset and obtain state-of-the-art results on several object classes."
  },
  "bmvc2015_main_surfacebasedobjectdetectioninrgbdimages": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Surface Based Object Detection in RGBD Images",
    "authors": [
      "Siddhartha Chandra",
      "Grigorios G. Chrysos",
      "Iasonas Kokkinos"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper187/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper187/paper187.pdf",
    "published": "2015-09",
    "summary": "Viewpoint variation is one of the main challenges for object-detection frameworks. In this work we describe strategies to improve object detection pipelines by introducing viewpoint based mixture components. We learn accurate mixtures of object detectors for RGB-Depth (RGBD) data using the latent SVM framework. Our contributions are threefold. First, we use surface-based object representations (3D mesh models) from available 3D object model repositories to learn strongly supervised viewpoint classifiers. These are used to guide the first stages of model learning, and help avoid inaccurate local minima of latent SVM training. Second, we develop a geometric dataset augmentation scheme that uses scene geometry to `take another look' at the training data, simulating the effect of camera viewpoint changes. Third, to better exploit depth information, we develop a novel depth-based dense feature extraction method that provides a robust statistical description of scene geometry. We evaluate our learned detectors on the NYU dataset, and demonstrate that each of our advances results in systematic performance improvements over the traditional HOG-based detection pipeline."
  },
  "bmvc2015_main_contextforestforobjectclassdetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Context Forest for Object Class Detection",
    "authors": [
      "Davide Modolo",
      "Alexander Vezhnevets",
      "Vittorio Ferrari"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper188/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper188/paper188.pdf",
    "published": "2015-09",
    "summary": "We present Context Forest (ConF) - a technique for predicting properties of the objects in an image based on its global appearance. Compared to standard nearest-neighbour techniques, ConF is more accurate, fast and memory efficient. We demonstrate ConF by predicting three properties: aspects of appearance, location in the image, and class memebership. In extensive experiments we show that (i) ConF can automatically select which components of a multi-component detector to run on a given test image, obtaining a considerable speed-up for detectors trained from large sets (10x for EE-SVMs [36] and 2x for DPM [21]; (ii) ConF can improve object detection performance by removing false positive detections at unlikely locations (+2% mAP), and by (iii) removing false positives produced by classes unlikely to be present in the image (+5% mAP on a 200-class dataset [2])."
  },
  "bmvc2015_main_spatiotemporaldeformableprototypesformotionanomalydetection": {
    "conf_id": "BMVC2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2015",
    "title": "Spatiotemporal Deformable Prototypes for Motion Anomaly Detection",
    "authors": [
      "Robert Bensch",
      "Thomas Brox",
      "Olaf Ronneberger"
    ],
    "page_url": "http://www.bmva.org/bmvc/2015/papers/paper189/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2015/papers/paper189/paper189.pdf",
    "published": "2015-09",
    "summary": "This paper presents an approach for motion-based anomaly detection, where a prototype pattern is detected and elastically registered against a test sample to detect anomalies in the test sample. The prototype model is learned from multiple sequences to define accepted variations. ``Supertrajectories'' based on hierarchical clustering of dense point trajectories serve as an efficient and robust representation of motion patterns. An efficient hashing approach provides transformation hypotheses that are refined by a spatiotemporal elastic registration. We propose a new method for elastic registration of 3D+time trajectory patterns that induces spatial elasticity from trajectory affinities. The method is evaluated on a new motion anomaly dataset and performs well in detecting subtle anomalies. Moreover, we demonstrate the applicability to biological motion patterns."
  }
}