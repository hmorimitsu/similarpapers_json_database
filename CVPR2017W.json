{
  "cvpr2017_w1_whatwillidonext?theintentionfrommotionexperiment": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Vision Meets Cognition: Functionality, Physics, Intentionality and Causality",
    "title": "What Will I Do Next? The Intention From Motion Experiment",
    "authors": [
      "Andrea Zunino",
      "Jacopo Cavazza",
      "Atesh Koul",
      "Andrea Cavallo",
      "Cristina Becchio",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/html/Zunino_What_Will_I_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/papers/Zunino_What_Will_I_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In computer vision, video-based approaches have been widely explored for the early classification and the prediction of actions or activities. However, it remains unclear whether this modality (as compared to 3D kinematics) can still be reliable for the prediction of human intentions, defined as the overarching goal embedded in an action sequence. Since the same action can be performed with different intentions, this problem is more challenging but yet affordable as proved by quantitative cognitive studies which exploit the 3D kinematics acquired through motion capture systems.In this paper, we bridge cognitive and computer vision studies, by demonstrating the effectiveness of video-based approaches for the prediction of human intentions. Precisely, we propose Intention from Motion, a new paradigm where, without using any contextual information, we consider instantaneous grasping motor acts involving a bottle in order to forecast why the bottle itself has been reached (to pass it or to place in a box, or to pour or to drink the liquid inside).We process only the grasping onsets casting intention prediction as a classification framework. Leveraging on our multimodal acquisition (3D motion capture data and 2D optical videos), we compare the most commonly used 3D descriptors from cognitive studies with state-of-the-art video-based techniques. Since the two analyses achieve an equivalent performance, we demonstrate that computer vision tools are effective in capturing the kinematics and facing the cognitive problem of human intention prediction.\r"
  },
  "cvpr2017_w1_theroleofsynchroniccausalconditionsinvisualknowledgelearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Vision Meets Cognition: Functionality, Physics, Intentionality and Causality",
    "title": "The Role of Synchronic Causal Conditions in Visual Knowledge Learning",
    "authors": [
      "Seng-Beng Ho"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/html/Ho_The_Role_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/papers/Ho_The_Role_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a principled approach for the learning of causal conditions from actions and activities taking place in the physical environment through visual input.Causal conditions are the preconditions that must exist before a certain effect can ensue.We propose to consider diachronic and synchronic causal conditions separately for the learning of causal knowledge. Diachronic condition captures the \"change\" aspect of the causal relationship - what change must be present at a certain time to effect a subsequent change - while the synchronic condition is the \"contextual\" aspect - what \"static\" condition must be present to enable the causal relationship involved. This paper focuses on discussing the learning of synchronic causal conditions as well as proposing a principled framework for the learning of causal knowledge including the learning of extended sequences of cause-effect and the encoding of this knowledge in the form of scripts for prediction and problem solving.\r"
  },
  "cvpr2017_w1_joint3dhumanmotioncaptureandphysicalanalysisfrommonocularvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Vision Meets Cognition: Functionality, Physics, Intentionality and Causality",
    "title": "Joint 3D Human Motion Capture and Physical Analysis From Monocular Videos",
    "authors": [
      "Petrissa Zell",
      "Bastian Wandt",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/html/Zell_Joint_3D_Human_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/papers/Zell_Joint_3D_Human_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Motion analysis is often restricted to a laboratory setup with multiple cameras and force sensors which requires expensive equipment and knowledgeable operators. Therefore it lacks in simplicity and flexibility. We propose an algorithm combining monocular 3D pose estimation with physics-based modeling to introduce a statistical framework for fast and robust 3D motion analysis from 2D video-data. We use a factorization approach to learn 3D motion coefficients and join them with physical parameters, that describe the dynamic of a mass-spring-model. Our approach does neither require additional force measurement nor torque optimization and only uses a single camera while allowing to estimate unobservable torques in the human body. We show that our algorithm improves the monocular 3D reconstruction by enforcing plausible human motion and resolving the ambiguity of camera and object motion.The performance is evaluated on different motions and multiple test data sets as well as on challenging outdoor sequences.\r"
  },
  "cvpr2017_w1_attention-basednaturallanguagepersonretrieval": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Vision Meets Cognition: Functionality, Physics, Intentionality and Causality",
    "title": "Attention-Based Natural Language Person Retrieval",
    "authors": [
      "Tao Zhou",
      "Muhao Chen",
      "Jie Yu",
      "Demetri Terzopoulos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/html/Zhou_Attention-Based_Natural_Language_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/papers/Zhou_Attention-Based_Natural_Language_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Following the recent progress in image classification and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More specifically, given the description of a person, the goal is to localize the person in an image. To this end, we first construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction while the natural language expression and attributes are input to Bidirectional Long Short-Term Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show significant improvement over state-of-the-art methods for generic object retrieval. This line of research promises to benefit search in surveillance video footage, and it may also be extended to other domains, such as human-robot interaction.\r"
  },
  "cvpr2017_w1_acfractivefacerecognitionusingconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Vision Meets Cognition: Functionality, Physics, Intentionality and Causality",
    "title": "AcFR: Active Face Recognition Using Convolutional Neural Networks",
    "authors": [
      "Masaki Nakada",
      "Han Wang",
      "Demetri Terzopoulos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/html/Nakada_AcFR_Active_Face_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/papers/Nakada_AcFR_Active_Face_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose AcFR, an active face recognition system that employs a convolutional neural network and acts consistently with human behaviors in common face recognition scenarios. AcFR comprises two main components--a recognition module and a controller module. The recognition module uses a pre-trained VGG-Face net to extract facial image features along with a nearest neighbor identity recognition algorithm. Based on the results, the controller module can make three different decisions--greet a recognized individual, disregard an unknown individual, or acquire a different viewpoint from which to reassess the subject, all of which are natural reactions when people observe passers-by. Evaluated on the PIE dataset, our recognition module yields higher accuracy on images under closer angles to those saved in memory. The accuracy is viewdependent and it also provides evidence for the proper design of the controller module.\r"
  },
  "cvpr2017_w1_automatedlayoutsynthesisandvisualizationfromimagesofinteriororexteriorspaces": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Vision Meets Cognition: Functionality, Physics, Intentionality and Causality",
    "title": "Automated Layout Synthesis and Visualization From Images of Interior or Exterior Spaces",
    "authors": [
      "Tomer Weiss",
      "Masaki Nakada",
      "Demetri Terzopoulos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/html/Weiss_Automated_Layout_Synthesis_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/papers/Weiss_Automated_Layout_Synthesis_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent work in virtual worlds has explored the synthesis of indoor spaces, with furniture, accessories, and other layout items. In this work, we bridge the gap between the real and virtual worlds---given an input image of an interior or exterior space, and a general user specification of the desired furnishings and layout constraints, our method furnishes the scene with a realistic arrangement and displays it to the user by augmenting the original image. It can deal with varying layouts and target arrangements at interactive rates, which affords the user a sense of collaboration with the design program, enabling the rapid visual assessment of various layout designs, a process which would typically be time consuming if done manually. Our method is suitable for phones and other mobile devices that have a camera.\r"
  },
  "cvpr2017_w1_inferringhiddenstatusesandactionsinvideobycausalreasoning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Vision Meets Cognition: Functionality, Physics, Intentionality and Causality",
    "title": "Inferring Hidden Statuses and Actions in Video by Causal Reasoning",
    "authors": [
      "Amy Fire",
      "Song-Chun Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/html/Fire_Inferring_Hidden_Statuses_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w1/papers/Fire_Inferring_Hidden_Statuses_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In the physical world, cause and effect are inseparable: ambient conditions trigger humans to perform actions, thereby driving status changes of objects.In video, these actions and statuses may be hidden due to ambiguity, occlusion, or because they are otherwise unobservable, but humans nevertheless perceive them. In this paper, we extend the Causal And-Or Graph (C-AOG) to a sequential model representing actions and their effects on objects over time, and we build a probability model for it.For inference, we apply a Viterbi algorithm, grounded on probabilistic detections from video, to fill in hidden and misdetected actions and statuses. We analyze our method on a new video dataset that showcases causes and effects.Our results demonstrate the effectiveness of reasoning with causality over time.\r"
  },
  "cvpr2017_w2_auto-curationandpersonalizationofsportshighlightsthroughmultimodalexcitementmeasures": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Auto-Curation and Personalization of Sports Highlights Through Multimodal Excitement Measures",
    "authors": [
      "Michele Merler",
      "Dhiraj Joshi",
      "Quoc-Bao Nguyen",
      "Stephen Hammer",
      "John Kent",
      "John R. Smith",
      "Rogerio S. Feris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Merler_Auto-Curation_and_Personalization_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Merler_Auto-Curation_and_Personalization_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The production of sports highlights packages is an essential task for broadcast media, yet it requires labor-intensive video editing. We propose a novel approach for auto-curation of sports highlights, and use it to create a real-world system for editorial aid of golf highlight reels. Our method fuses information from the player reaction (action recognition such as high-five), spectators (crowd cheering), and commentator (voice tone and word analysis) to determine the most interesting moments of a game. We identify the start and end of key shot highlights with metadata such as player name and hole number, allowing personalized content summarization and retrieval. We also introduce a zero-shot learning framework for our classifiers by exploiting the correlation of different modalities. We demonstrated our system on a major golf tournament.\r"
  },
  "cvpr2017_w2_singletsmulti-resolutionmotionsingularitiesforsoccervideoabstraction": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Singlets: Multi-Resolution Motion Singularities for Soccer Video Abstraction",
    "authors": [
      "Katy Blanc",
      "Diane Lingrand",
      "Frederic Precioso"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Blanc_Singlets_Multi-Resolution_Motion_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Blanc_Singlets_Multi-Resolution_Motion_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The burst of video production appeals for new browsing frameworks. Chiefly in sports, TV companies have years of recorded match archives to exploit and sports fans are looking for replay, summary or collection of events. In this work, we design a new multi-resolution motion feature for video abstraction. This descriptor is based on optical flow singularities tracked along the video. We use these singlets in order to detect zooms, slow-motions and salient moments in soccer games and finally to produce an automatic summarization of a game. We produce a database for soccer video summarization composed of 4 soccer matches from HDTV games for the FIFA world cup 2014 annotated with goals, fouls, corners and salient moments to make a summary. We correctly detect 88.2% of saliant moments using this database. To highlight the generalization of our approach, we test our system on the final game of the handball world championship 2015 without any retraining, refining or adaptation.\r"
  },
  "cvpr2017_w2_learningtoscoreolympicevents": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Learning to Score Olympic Events",
    "authors": [
      "Paritosh Parmar",
      "Brendan Tran Morris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Parmar_Learning_to_Score_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Parmar_Learning_to_Score_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Estimating action quality, the process of assigning a \"score\" to the execution of an action, is crucial in areas such as sports and health care. Unlike action recognition, which has millions of examples to learn from, the action quality datasets that are currently available are small -typically comprised of only a few hundred samples. This work presents three frameworks for evaluating Olympic sports which utilize spatiotemporal features learned using 3D convolutional neural networks (C3D) and perform score regression with i) SVR ii) LSTM and iii) LSTM followed by SVR. An efficient training mechanism for the limited data scenarios is presented for clip-based training with LSTM. The proposed systems show significant improvement over existing quality assessment approaches on the task of predicting scores of diving, vault, figure skating. SVR-based frameworks yield better results, LSTM-based frameworks are more natural for describing an action and can be used for improvement feedback.\r"
  },
  "cvpr2017_w2_hockeyactionrecognitionviaintegratedstackedhourglassnetwork": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Hockey Action Recognition via Integrated Stacked Hourglass Network",
    "authors": [
      "Mehrnaz Fani",
      "Helmut Neher",
      "David A. Clausi",
      "Alexander Wong",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Fani_Hockey_Action_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Fani_Hockey_Action_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A convolutional neural network (CNN) has been designed to interpret player actions in ice hockey video. The hourglass network is employed as the base to generate player pose estimation and layers are added to this network to produce action recognition. As such, the unified architecture is referred to as action recognition hourglass network, or ARHN. ARHN has three components. The first component is the latent pose estimator, the second transforms latent features to a common frame of reference, and the third performs action recognition. Since no benchmark dataset for pose estimation or action recognition is available for hockey players, we generate such an annotated dataset. Experimental results show action recognition accuracy of 65% for four types of actions in hockey. When similar poses are merged to three and two classes, the accuracy rate increases to 71% and 78%, proving the efficacy of the methodology for automated action recognition in hockey.\r"
  },
  "cvpr2017_w2_extractionandclassificationofdivingclipsfromcontinuousvideofootage": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Extraction and Classification of Diving Clips From Continuous Video Footage",
    "authors": [
      "Aiden Nibali",
      "Zhen He",
      "Stuart Morgan",
      "Daniel Greenwood"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Nibali_Extraction_and_Classification_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Nibali_Extraction_and_Classification_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The recording of video data has become a common component of athlete training programmes. However, the manual analysis of the obtained footage is time-consuming and requires domain-specific knowledge. In order to automate this kind of task, most previous work has focused on just one of the following sub-problems: 1) temporally cropping events/actions of interest from continuous video; 2) tracking the object of interest; and 3) classifying the events/actions of interest. In contrast, this paper provides a complete solution to the overall action monitoring task in the context of a challenging real-world exemplar, diving classification. The model is required to learn the temporal boundaries of a dive, even though the subject is small and other divers and bystanders may be in view, and must also be sensitive to subtle changes in body pose in order to classify the dive. We propose effective techniques which work in tandem and can be easily generalized to video footage from other sports.\r"
  },
  "cvpr2017_w2_accurateandefficient3dhumanposeestimationalgorithmusingsingledepthimagesforposeanalysisingolf": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Accurate and Efficient 3D Human Pose Estimation Algorithm Using Single Depth Images for Pose Analysis in Golf",
    "authors": [
      "Soonchan Park",
      "Ju Yong Chang",
      "Hyuk Jeong",
      "Jae-Ho Lee",
      "Ji-Young Park"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Park_Accurate_and_Efficient_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Park_Accurate_and_Efficient_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Human pose analysis has been known to be an effective means to evaluate athlete's performance. Marker-less 3D human pose estimation is one of the most practical methods to acquire human pose but lacks sufficient accuracy required to achieve precise performance analysis for sports. In this paper, we propose a human pose estimation algorithm that utilizes multiple types of random forests to enhance results for sports analysis. Random regression forest voting to localize joints of the athlete's anatomy is followed by random verification forests that evaluate and optimize the votes to improve the accuracy of clustering that determine the final position of anatomic joints. Experiential results show that the proposed algorithm enhances not only accuracy, but also efficiency of human pose estimation. We also conduct the field study to investigate feasibility of the algorithm for sports applications with developed golf swing analyzing system.\r"
  },
  "cvpr2017_w2_athleteposeestimationbyaglobal-localnetwork": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Athlete Pose Estimation by a Global-Local Network",
    "authors": [
      "Jihye Hwang",
      "Sungheon Park",
      "Nojun Kwak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Hwang_Athlete_Pose_Estimation_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Hwang_Athlete_Pose_Estimation_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Analyzing joint movements of an athlete helps to improve the pose of the athlete. In this paper, we propose a network that combines global and local information for HPE using a 2D image. Unlike previous works that have used global or local information separately, we use the combined information to enhance the performance of HPE. General information from a global network is used as aninput to a local network to refine the location of a part using a variety of regions. The global network is based on ResNet-101 [4] and trained to regress a heatmap representing parts' locations. The output features from the global network are used as input features for the local network. The local network learns spatial information using position sensitive score maps [8]. Through the end-to-end learning, the global network is affected by the local information.We demonstrate that the proposed HPE method is efficient on the LSP and UCF sports datasets. \r"
  },
  "cvpr2017_w2_continuousvideotosimplesignalsforswimmingstrokedetectionwithconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Continuous Video to Simple Signals for Swimming Stroke Detection With Convolutional Neural Networks",
    "authors": [
      "Brandon Victor",
      "Zhen He",
      "Stuart Morgan",
      "Dino Miniutti"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Victor_Continuous_Video_to_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Victor_Continuous_Video_to_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In many sports, it is useful to analyse video of an athlete in competition for training purposes. In swimming, stroke rate is a common metric used by coaches; requiring a laborious labelling of each individual stroke. We show that using a Convolutional Neural Network (CNN) we can automatically detect discrete events in continuous video (in this case, swimming strokes). We create a CNN that learns a mapping from a window of frames to a point on a smooth 1D target signal, with peaks denoting the location of a stroke, evaluated as a sliding window. To our knowledge this process of training and utilizing a CNN has not been investigated before; either in sports or fundamental computer vision research. Most research has been focused on action recognition and using it to classify many clips in continuous video for action localisation. In this paper we demonstrate our process works well on the task of detecting swimming strokes in the wild. [... more in paper]\r"
  },
  "cvpr2017_w2_applicationofcomputervisionandvectorspacemodelfortacticalmovementclassificationinbadminton": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Application of Computer Vision and Vector Space Model for Tactical Movement Classification in Badminton",
    "authors": [
      "Kokum Weeratunga",
      "Anuja Dharmaratne",
      "Khoo Boon How"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Weeratunga_Application_of_Computer_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Weeratunga_Application_of_Computer_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Performance profiling in sports allow evaluating opponents' tactics and the development of counter tactics to gain a competitive advantage. The work presented develops a comprehensive methodology to automate tactical profiling in elite badminton. The proposed approach uses computer vision techniques to automate data gathering from video footage. The image processing algorithm is validated using video footage of the highest level tournaments, including the Olympic Games. The average accuracy of player position detection is 96.03% and 97.09% on the two halves of a badminton court. Next, frequent trajectories of badminton players are extracted and classified according to their tactical relevance. The classification performs at 97.79% accuracy, 97.81% precision, 97.44% recall, and 97.62% F-score. The combination of automated player position detection, frequent trajectory extraction, and the subsequent classification can be used to automatically generate player tactical profiles. \r"
  },
  "cvpr2017_w2_automatictacticaladjustmentinreal-timemodelingadversaryformationswithradon-cumulativedistributiontransformandcanonicalcorrelationanalysis": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Automatic Tactical Adjustment in Real-Time: Modeling Adversary Formations With Radon-Cumulative Distribution Transform and Canonical Correlation Analysis",
    "authors": [
      "Amir M. Rahimi",
      "Soheil Kolouri",
      "Rajan Bhattacharyya"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Rahimi_Automatic_Tactical_Adjustment_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Rahimi_Automatic_Tactical_Adjustment_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper we introduce two fundamentally different techniques for optimizing counter formations in team sports. In the first technique, we use canonical correlation analysis (CCA) to learn an \"explicit\" relationship between offensive and defensive formations. We then use the learned CCA components to make predictions about players' spatial position. Experimenting with the basketball dataset (NBA season 2012-2013) we are able to predict players' positions with high precision. In the second technique, we create an image-based representation of the player movements relative to the ball. The mentioned representation enables coaches to assess team formations in a glance. The recently developed Radon Cumulative Distribution Transform (RCDT) was used alongside CCA to analyze the image-based representations. With these techniques, we provide real-time feedback to optimize both players' positions and team formations.\r"
  },
  "cvpr2017_w2_classificationofpuckpossessioneventsinicehockey": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Classification of Puck Possession Events in Ice Hockey",
    "authors": [
      "Moumita Roy Tora",
      "Jianhui Chen",
      "James J. Little"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Tora_Classification_of_Puck_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Tora_Classification_of_Puck_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Group activity recognition in sports is often challenging due to the complex dynamics and interaction among the players. In this paper, we propose a recurrent neural network to classify puck possession events in ice hockey. Our method extracts features from the whole frame and appearances of the players using a pre-trained convolutional neural network. In this way, our model captures the context information, individual attributes and interaction among the players. Our model requires only the player positions on the image and does not need any explicit annotations for the individual actions or player trajectories, greatly simplifying the input of the system. We evaluate our model on a new Ice Hockey Dataset. Experimental results show that our model produces competitive results on this challenging dataset with much simpler inputs compared with the previous work.\r"
  },
  "cvpr2017_w2_footballactionrecognitionusinghierarchicallstm": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Football Action Recognition Using Hierarchical LSTM",
    "authors": [
      "Takamasa Tsunoda",
      "Yasuhiro Komori",
      "Masakazu Matsugu",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Tsunoda_Football_Action_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Tsunoda_Football_Action_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": "We present a hierarchical recurrent network for understanding team sports activity in image and location sequences. In the hierarchical model, we integrate proposed multiple person-centered features over a temporal sequence based on LSTM's outputs. To achieve this scheme, we introduce the Keeping state in LSTM as one of externally controllable states, and extend the Hierarchical LSTMs to include mechanism for the integration. Experimental results demonstrate effectiveness of the proposed framework involving hierarchical LSTM and person-centered feature. In this study, we demonstrate improvement over the reference model. Specifically, by incorporating the person-centered feature with meta-information (e.g., location data) in our proposed late fusion framework, we also demonstrate increased discriminability of action categories and enhanced robustness against fluctuation in the number of observed players.\r"
  },
  "cvpr2017_w2_ball3dtrajectoryreconstructionwithoutpreliminarytemporalandgeometricalcameracalibration": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Ball 3D Trajectory Reconstruction Without Preliminary Temporal and Geometrical Camera Calibration",
    "authors": [
      "Shogo Miyata",
      "Hideo Saito",
      "Kosuke Takahashi",
      "Dan Mikami",
      "Mariko Isogawa",
      "Hideaki Kimata"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Miyata_Ball_3D_Trajectory_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Miyata_Ball_3D_Trajectory_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper proposes a method for reconstructing 3D ball trajectories by using multiple temporally and geometrically uncalibrated cameras. To use cameras to measure the trajectory of a fast-moving object, such as a ball thrown by a pitcher, the cameras must be temporally synchronized and their position and orientation should be calibrated. In some cases, these conditions cannot be met, e.g., one cannot geometrically calibrate cameras when one cannot step into a baseball stadium. The basic idea of the proposed method is to use a ball captured by multiple cameras as a corresponding point. The method first detects a ball. Then, it estimates temporal difference between cameras. After that, the ball positions are used as corresponding points for geometrically calibrating the cameras. Experiments using actual pitching videos verify the effectiveness of our method.\r"
  },
  "cvpr2017_w2_deeplearningfordomain-specificactionrecognitionintennis": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Deep Learning for Domain-Specific Action Recognition in Tennis",
    "authors": [
      "Silvia Vinyes Mora",
      "William J. Knottenbelt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Mora_Deep_Learning_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Mora_Deep_Learning_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent progress in sports analytics has been driven by the availability of spatio-temporal and high level data. Video-based action recognition in sports can significantly contribute to these advances. Good progress has been made in the field of action recognition but its application to sports mainly focuses in detecting which sport is being played. In order for action recognition to be useful in sports analytics a finer-grained action classification is needed. For this reason we focus on the fine-grained action recognition in tennis and explore the capabilities of deep neural networks for this task. In our model, videos are represented as sequences of features, extracted using the well-known Inception neural network, trained on an independent dataset. Then a 3-layered LSTM network is trained for the classification. Our main contribution is the proposed neural network architecture that achieves competitive results in the challenging THETIS dataset, comprising videos of tennis actions. \r"
  },
  "cvpr2017_w2_court-basedvolleyballvideosummarizationfocusingonrallyscene": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Court-Based Volleyball Video Summarization Focusing on Rally Scene",
    "authors": [
      "Takahiro Itazuri",
      "Tsukasa Fukusato",
      "Shugo Yamaguchi",
      "Shigeo Morishima"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Itazuri_Court-Based_Volleyball_Video_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Itazuri_Court-Based_Volleyball_Video_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we propose a video summarization system for volleyball videos. Our system automatically detects rally scenes as self-consumable video segments and evaluates rally-rank for each rally scene to decide priority. In the priority decision, features representing the contents of the game are necessary; however such features have not been considered in most previous methods. Although several visual features such as the position of a ball and players should be used, acquisition of such features is still non-robust and unreliable in low resolution or low frame rate volleyball videos. Instead, we utilize the court transition information caused by camera operation. Experimental results demonstrate the robustness of our rally scene detection and the effectiveness of our rally-rank to reflect viewers' preferences over previous methods.\r"
  },
  "cvpr2017_w2_measuringenergyexpenditureinsportsbythermalvideoanalysis": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Sports",
    "title": "Measuring Energy Expenditure in Sports by Thermal Video Analysis",
    "authors": [
      "Rikke Gade",
      "Ryan Godsk Larsen",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/html/Gade_Measuring_Energy_Expenditure_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w2/papers/Gade_Measuring_Energy_Expenditure_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Estimation of human energy expenditure in sports and exercise contributes to performance analyses and tracking of physical activity levels. The focus of this work is to develop a video-based method for estimation of energy expenditure in athletes. We propose a method using thermal video analysis to automatically extract the cyclic motion pattern, in walking and running represented as steps, and analyse the frequency. Experiments are performed with one subject in two different tests, each at 5, 8, 10, and 12 km/h. The results of our proposed video-based method is compared to concurrent measurements of oxygen uptake. These initial experiments indicate a correlation between estimated step frequency and oxygen uptake. Based on the preliminary results we conclude that the proposed method has potential as a future non-invasive approach to estimate energy expenditure during sports.\r"
  },
  "cvpr2017_w3_infraredvariationoptimizeddeepconvolutionalneuralnetworkforrobustautomaticgroundtargetrecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Infrared Variation Optimized Deep Convolutional Neural Network for Robust Automatic Ground Target Recognition",
    "authors": [
      "Sungho Kim",
      "Woo-Jin Song",
      "So-Hyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Kim_Infrared_Variation_Optimized_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Kim_Infrared_Variation_Optimized_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " ATR is a traditionally unsolved problem in military applications because of the wide range of infrared (IR) image variations and limited number of training images. Recently, deep convolutional neural network-based approaches in RGB images (RGB-CNN) showed breakthrough performance in computer vision problems. The direct use of the RGB-CNN to IR ATR problem fails to work because of the IR database problems. This paper presents a novel infrared variation-optimized deep convolutional neural network (IVO-CNN) by considering database management, such as increasing the database by a thermal simulator, controlling the image contrast automatically and suppressing the thermal noise to reduce the effects of infrared image variations in deep convolutional neural network-based automatic ground target recognition. The experimental results on the synthesized infrared images generated by the thermal simulator (OKTAL-SE) validated the feasibility of IVO-CNN for military ATR applications.\r"
  },
  "cvpr2017_w3_rgb-dscenelabelingwithmultimodalrecurrentneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "RGB-D Scene Labeling With Multimodal Recurrent Neural Networks",
    "authors": [
      "Heng Fan",
      "Xue Mei",
      "Danil Prokhorov",
      "Haibin Ling"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Fan_RGB-D_Scene_Labeling_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Fan_RGB-D_Scene_Labeling_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recurrent neural networks (RNNs) are able to capture context in an image by modeling long-range semantic dependencies among image units. However, existing methods only utilize RNNs to model dependencies of a single modality (e.g., RGB) for labeling. In this work we extend this single-modal RNNs to multimodal RNNs (MM-RNNs) and apply it to RGB-D scene labeling. Our MM-RNNs are capable of seamlessly modeling dependencies of both RGB and depth modalities, and allow 'memory' sharing across modalities. By sharing 'memory', each modality possesses multiple properties of itself and other modalities, and becomes more discriminative to distinguish pixels. Moreover, we also analyse two simple extensions of single-modal RNNs and demonstrate that our MM-RNNs perform better than both of them. Integrating with convolutional neural networks (CNNs), we build an end-to-end network for RGB-D scene labeling. Extensive experiments on NYU depth V1 and V2 demonstrate the effectiveness of MM-RNNs.\r"
  },
  "cvpr2017_w3_infraredimagecolorizationbasedonatripletdcganarchitecture": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Infrared Image Colorization Based on a Triplet DCGAN Architecture",
    "authors": [
      "Patricia L. Suarez",
      "Angel D. Sappa",
      "Boris X. Vintimilla"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Suarez_Infrared_Image_Colorization_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Suarez_Infrared_Image_Colorization_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper proposes a novel approach for colorizing near infrared (NIR) images using a Deep Convolutional Generative Adversarial Network (GAN) architecture. The proposed approach is based on the usage of a triplet model for learning each color channel independently, in a more homogeneous way. It allows a fast convergence during the training, obtaining a greater similarity between the colored NIR image and the corresponding ground truth. The proposed approach has been evaluated with a large data set of NIR images and compared with a recent approach, which is also based on a GAN architecture where all the color channels are obtained at the same time. \r"
  },
  "cvpr2017_w3_analgorithmforparallelreconstructionofjointlysparsetensorswithapplicationstohyperspectralimaging": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "An Algorithm for Parallel Reconstruction of Jointly Sparse Tensors With Applications to Hyperspectral Imaging",
    "authors": [
      "Qun Li",
      "Edgar A. Bernal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Li_An_Algorithm_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Li_An_Algorithm_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A wide range of Compressive Sensing (CS) frameworks have been proposed to address the task of color and hyperspectral image sampling and reconstruction. Methods for reconstruction of jointly sparse vectors that leverage joint sparsity constraints such as the Multiple Measurement Vector (MMV) approach have been shown to outperform Single Measurement Vector (SMV) frameworks. Recent work has shown that exploiting joint sparsity while simultaneously preserving the high-dimensional structure of the data results in further performance improvements. We introduce a parallelizable extension of a previously proposed serial tensorial MMV approach which, like its predecessor, exploits joint sparsity constraints multiple data dimensions simultaneously, but that is parallelizable in nature. We demonstrate empirically that the proposed method provides better reconstruction fidelity of hyperspectral imagery and that it is also more computationally efficient than the current state of the art.\r"
  },
  "cvpr2017_w3_deepheterogeneousfacerecognitionnetworksbasedoncross-modaldistillationandanequitabledistancemetric": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Deep Heterogeneous Face Recognition Networks Based on Cross-Modal Distillation and an Equitable Distance Metric",
    "authors": [
      "Christopher Reale",
      "Hyungtae Lee",
      "Heesung Kwon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Reale_Deep_Heterogeneous_Face_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Reale_Deep_Heterogeneous_Face_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this work we present three methods to improve a deep convolutional neural network approach to near-infrared heterogeneous face recognition. We first present a method to distill extra information from a pre-trained visible face network through the output logits of the network. Next, we put forth an altered contrastive loss function that uses the l_1 norm instead of the l_2 norm as a distance metric. Finally, we propose to improve the initialization network by training it for more iterations. We present the results of experiments of these methods on two widely used near-infrared heterogeneous face recognition datasets and compare them to the state-of-the-art.\r"
  },
  "cvpr2017_w3_aerialvehicletrackingbyadaptivefusionofhyperspectrallikelihoodmaps": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps",
    "authors": [
      "Burak Uzkent",
      "Aneesh Rangnekar",
      "Matthew Hoffman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Uzkent_Aerial_Vehicle_Tracking_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Uzkent_Aerial_Vehicle_Tracking_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Hyperspectral cameras provide unique spectral signatures that can be used to solve surveillance tasks. This paper proposes a novel real-time hyperspectral likelihood maps-aided tracking method (HLT) inspired by an adaptive hyperspectral sensor. We focus on the target detection part of a tracking system and remove the necessity to build any offline classifiers and tune large amount of hyper-parameters, instead learning a generative target model in an online manner for hyperspectral channels ranging from visible to infrared wavelengths. The key idea is that our adaptive fusion method can combine likelihood maps from multiple bands of hyperspectral imagery into one single more distinctive representation increasing the margin between mean value of foreground and background pixels in the fused map. Experimental results show that the HLT not only outperforms all established fusion methods but is on par with the current state-of-the-art hyperspectral target tracking frameworks.\r"
  },
  "cvpr2017_w3_fullyconvolutionalregionproposalnetworksformultispectralpersondetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Fully Convolutional Region Proposal Networks for Multispectral Person Detection",
    "authors": [
      "Daniel Konig",
      "Michael Adam",
      "Christian Jarvers",
      "Georg Layher",
      "Heiko Neumann",
      "Michael Teutsch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Konig_Fully_Convolutional_Region_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Konig_Fully_Convolutional_Region_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Multispectral images that combine visual-optical and infrared image information are a promising source of data for automatic person detection. Especially in automotive or surveillance applications, challenging conditions such as insufficient illumination or large distances between camera and object occur regularly and can affect image quality. This leads to weak image contrast or low object resolution. In order to detect persons under such conditions, we apply deep learning for fusing the VIS and IR information in multispectral images. We present a novel multispectral Region Proposal Network that is built up on the pre-trained very deep convolutional network VGG-16. The proposals of this network are evaluated using a Boosted Decision Trees classifier in order to reduce potential false positive detections. With a log-average miss rate of 29.83% on the reasonable test set of the KAIST Multispectral Pedestrian Detection Benchmark, we improve the current state-of-the-art by about 18%.\r"
  },
  "cvpr2017_w3_alogarithmicx-rayimagingmodelforbaggageinspectionsimulationandobjectdetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Logarithmic X-Ray Imaging Model for Baggage Inspection: Simulation and Object Detection",
    "authors": [
      "Domingo Mery",
      "Aggelos K. Katsaggelos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Mery_A_Logarithmic_X-Ray_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Mery_A_Logarithmic_X-Ray_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In baggage inspection the aim is to detect automatically threat objects. The progress in automated baggage inspection, however, is modest and very limited. In this work, we present an X-ray imaging model that can separate foreground from background in baggage screening. In our model, rather than a multiplication of foreground and background, we propose the addition of logarithmic images. This allows the use of linear strategies to superimpose images of threat objects onto X-ray images (simulation) and the use of sparse representations in order segment target objects (detection). In our experiments, we simulate new X-ray images of handguns, shuriken and razor blades, in which it is impossible to distinguish simulated and real X-ray images. In addition, we show in our experiments the effective detection of shuriken, razor blades and handguns using the proposed algorithm. \r"
  },
  "cvpr2017_w3_afastapproximatespectralunmixingalgorithmbasedonsegmentation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Fast Approximate Spectral Unmixing Algorithm Based on Segmentation",
    "authors": [
      "Jing Ke",
      "Yi Guo",
      "Arcot Sowmya"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Ke_A_Fast_Approximate_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Ke_A_Fast_Approximate_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Efficient processing of spectral unmixing is a challenging problem in high-resolution satellite data analysis. The decomposition of a pixel into a linear combination of pure spectra into their corresponding proportions is often very time-consuming. In this paper, a fast unmixing algorithm is proposed based on classifying pixels into a full unmixing group for subset selection requiring intensive computational procedures and a partial unmixing group for proportion estimation with known spectra endmembers. The classification is based on n-band spectral segmentation using the quick-shift algorithm. A subset selection algorithm applied on real satellite data evaluates accuracy and approximation, and experimental results show significant performance acceleration compared with the original algorithm. Parallelization strategies are also presented and verified on NVIDIA GTX TITAN X.\r"
  },
  "cvpr2017_w3_thefirstautomaticmethodformappingthepotholeinseagrass": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "The First Automatic Method for Mapping the Pothole in Seagrass",
    "authors": [
      "Maryam Rahnemoonfar",
      "Masoud Yari",
      "Abdullah Rahman",
      "Richard Kline"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Rahnemoonfar_The_First_Automatic_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Rahnemoonfar_The_First_Automatic_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " There is a vital need to map seagrass ecosystems in order to determine worldwide abundance and distribution. Currently there is no established method for mapping the pothole or scars in seagrass. Detection of seagrass with optical remote sensing is challenged by the fact that light is attenuated as it passes through the water column and reflects back from the benthos. Optical remote sensing of seagrass is only possible if the water is shallow and relatively clear. In reality, coastal waters are commonly turbid, and seagrasses can grow under 10 meters of water or even deeper. One of the most precise sensors to map the seagrass disturbance is side scan sonar. Underwater acoustics mapping produces a high definition, two-dimensional sonar image of seagrass ecosystems. This paper proposes a methodology which detects seagrass potholes in sonar images. Side scan sonar images usually contain speckle noise and uneven illumination across the image. Moreover, disturbance presents complex patterns where most segmentation techniques will fail. In this paper, the quality of image is improved in the first stage using adaptive thresholding and wavelet denoising techniques. In the next step, a novel level set technique is applied to identify the pothole patterns. Our method is robust to noise and uneven illumination. Moreover it can detect the complex pothole patterns. We tested our proposed approach on a collection of underwater sonar images taken from Laguna Madre in Texas. Experimental results in comparison with the ground-truth show the efficiency of the proposed method.\r"
  },
  "cvpr2017_w3_facepresentationattackwithlatexmasksinmultispectralvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Face Presentation Attack With Latex Masks in Multispectral Videos",
    "authors": [
      "Akshay Agarwal",
      "Daksha Yadav",
      "Naman Kohli",
      "Richa Singh",
      "Mayank Vatsa",
      "Afzel Noore"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Agarwal_Face_Presentation_Attack_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Agarwal_Face_Presentation_Attack_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Face recognition systems are susceptible to presentation attacks such as printed photo attacks, replay attacks, and 3D mask attacks. These attacks, primarily studied in visible spectrum, aim to obfuscate or impersonate a person's identity. This paper presents a unique multispectral video face database for face presentation attack using latex and paper masks. The proposed Multispectral Latex Mask based Video Face Presentation Attack (MLFP) database contains 1350 videos in visible, near infrared, and thermal spectrums. Since the database consists of videos of subjects without any mask as well as wearing ten different masks, the effect of identity concealment is analyzed in each spectrum using face recognition algorithms. We also present the performance of existing presentation attack detection algorithms on the proposed MLFP database. It is observed that the thermal imaging spectrum is most effective in detecting face presentation attacks.\r"
  },
  "cvpr2017_w3_privacy-preservingunderstandingofhumanbodyorientationforsmartmeetings": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Privacy-Preserving Understanding of Human Body Orientation for Smart Meetings",
    "authors": [
      "Indrani Bhattacharya",
      "Noam Eshed",
      "Richard J. Radke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Bhattacharya_Privacy-Preserving_Understanding_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Bhattacharya_Privacy-Preserving_Understanding_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a method for estimating the body orientation of seated people in a smart room by fusing low-resolution range information collected from downward pointed time-of- flight (ToF) sensors with synchronized speaker identification information from microphone recordings. The ToF sensors preserve the privacy of the occupants in that they only return the range to a small set of hit points. We propose a Bayesian estimation algorithm for the quantized body orientations in which the likelihood term is based on the observed ToF data and the prior term is based on the occupants' locations and current speakers. We evaluate our algorithm in real meeting scenarios and show that it is possible to accurately estimate seated human orientation even with very low-resolution systems.\r"
  },
  "cvpr2017_w3_selectinganoptimizedcotsfiltersetformultispectralplenopticsensing": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Selecting an Optimized COTS Filter Set for Multispectral Plenoptic Sensing",
    "authors": [
      "Timothy Doster",
      "Colin C. Olson",
      "Erin Fleet",
      "Michael Yetzbacher"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Doster_Selecting_an_Optimized_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Doster_Selecting_an_Optimized_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A 16-band plenoptic camera allows for the rapid exchange of filter sets via a 4x4 filter array on the lens's front aperture thus allowing an operator to quickly adapt to a different locale or threat intelligence. Typically, such a system incorporates a default set of 16 equally spaced, non-overlapping, flat-topped filters. Knowing the operating theater or the likely targets of interest it becomes advantageous to tune the filters; we propose a differential evolution algorithm to search over a set of commercial off-the-shelf (COTS) filters for an optimal solution. We examine two independent tasks: general spectral sensing and target detection. For general spectral sensing, we utilize compressive sensing and find filters that generate codings which minimize reconstruction error. For target detection, we select the filters to optimize the separation between the background and a set of targets. We compare our results to the default filter set and full spectral resolution hyperspectral data.\r"
  },
  "cvpr2017_w3_anoveldetectionparadigmanditscomparisontostatisticalandkernel-basedanomalydetectionalgorithmsforhyperspectralimagery": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Novel Detection Paradigm and Its Comparison to Statistical and Kernel-Based Anomaly Detection Algorithms for Hyperspectral Imagery",
    "authors": [
      "Colin C. Olson",
      "Timothy Doster"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Olson_A_Novel_Detection_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Olson_A_Novel_Detection_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Detection of anomalous pixels within hyperspectral imagery is frequently used for purposes ranging from the location of invasive plant species to the detection of military targets. The task is unsupervised because no information about target or background spectra is known or assumed. Some of the most commonly used detection algorithms assume a statistical distribution for the background and rate spectral anomalousness based on measures of deviation from the statistical model; but such assumptions can be problematic because hyperspectral data rarely meet them. More recent algorithms have employed data-driven machine learning techniques in order to improve performance. Here we investigate a novel kernel-based method and show that it achieves top detection performance relative to seven other state-of-the-art methods on a commonly tested data set.\r"
  },
  "cvpr2017_w3_learningspatiotemporalfeaturesforinfraredactionrecognitionwith3dconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Perception Beyond the Visible Spectrum",
    "title": "Learning Spatiotemporal Features for Infrared Action Recognition With 3D Convolutional Neural Networks",
    "authors": [
      "Zhuolin Jiang",
      "Viktor Rozgic",
      "Sancar Adali"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/html/Jiang_Learning_Spatiotemporal_Features_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w3/papers/Jiang_Learning_Spatiotemporal_Features_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in infrared (IR) videos is significantly less explored. Our objective is to exploit imaging data in this modality for the action recognition task. In this work, we propose a novel two-stream 3D convolutional neural network architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR images and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision performances on the InfAR dataset.\r"
  },
  "cvpr2017_w4_fast,accuratethin-structureobstacledetectionforautonomousmobilerobots": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Fast, Accurate Thin-Structure Obstacle Detection for Autonomous Mobile Robots",
    "authors": [
      "Chen Zhou",
      "Jiaolong Yang",
      "Chunshui Zhao",
      "Gang Hua"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Zhou_Fast_Accurate_Thin-Structure_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Zhou_Fast_Accurate_Thin-Structure_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Safety is paramount for mobile robotic platforms such as self-driving cars and unmanned aerial vehicles. This work is devoted to a task that is indispensable for safety yet was largely overlooked in the past -- detecting obstacles that are of very thin structures, such as wires, cables and tree branches. This is a challenging problem, as thin objects can be problematic for active sensors such as lidar and sonar and even for stereo cameras. In this work, we propose to use video sequences for thin obstacle detection. We represent obstacles with edges in the video frames, and reconstruct them in 3D using efficient edge-based visual odometry techniques. We provide both a monocular camera solution and a stereo camera solution. The former incorporates IMU data to solve scale ambiguity, while the latter enjoys a novel, purely vision-based solution. Experiments demonstrated that the proposed methods are fast and able to detect thin obstacles robustly and accurately under various conditions.\r"
  },
  "cvpr2017_w4_sparse,quantized,fullframecnnforlowpowerembeddeddevices": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Sparse, Quantized, Full Frame CNN for Low Power Embedded Devices",
    "authors": [
      "Manu Mathew",
      "Kumar Desappan",
      "Pramod Kumar Swami",
      "Soyeb Nagori"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Mathew_Sparse_Quantized_Full_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Mathew_Sparse_Quantized_Full_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper presents methods to reduce the complexity of convolutional neural networks (CNN). These include: (1) A method to quickly and easily sparsify a given network. (2) Fine tune the sparse network to obtain the lost accuracy back (3) Quantize the network to be able to implement it using 8-bit fixed point multiplications efficiently. (4) We then show how an inference engine can be designed to take advantage of the sparsity. These techniques were applied to full frame semantic segmentation and the degradation due to the sparsity and quantization is found to be negligible. We show by analysis that the complexity reduction achieved is significant. Results of implementation on Texas Instruments TDA2x SoC [17] are presented. We have modified Caffe CNN framework to do the sparse, quantized training described in this paper. The source code for the training is made available at https://github.com/tidsp/caffe-jacinto \r"
  },
  "cvpr2017_w4_reconstructingintensityimagesfrombinaryspatialgradientcameras": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Reconstructing Intensity Images From Binary Spatial Gradient Cameras",
    "authors": [
      "Suren Jayasuriya",
      "Orazio Gallo",
      "Jinwei Gu",
      "Timo Aila",
      "Jan Kautz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Jayasuriya_Reconstructing_Intensity_Images_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Jayasuriya_Reconstructing_Intensity_Images_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Binary gradient cameras extract edge and temporal information directly on the sensor, allowing for low-power, low-bandwidth, and high-dynamic-range capabilities---all critical factors for the deployment of embedded computer vision systems. However, these types of images require specialized computer vision algorithms and are not easy to interpret by a human observer. In this paper we propose to recover an intensity image from a single binary spatial gradient image with a deep auto-encoder. Extensive experimental results on both simulated and real data show the effectiveness of the proposed approach.\r"
  },
  "cvpr2017_w4_binarizedconvolutionalneuralnetworkswithseparablefiltersforefficienthardwareacceleration": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Binarized Convolutional Neural Networks With Separable Filters for Efficient Hardware Acceleration",
    "authors": [
      "Jeng-Hau Lin",
      "Tianwei Xing",
      "Ritchie Zhao",
      "Zhiru Zhang",
      "Mani Srivastava",
      "Zhuowen Tu",
      "Rajesh K. Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Lin_Binarized_Convolutional_Neural_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Lin_Binarized_Convolutional_Neural_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " State-of-the-art convolutional neural networks are enormously costly in both compute and memory, demanding massively parallel GPUs for execution. Such networks strain the computational capabilities and energy available to embedded and mobile processing platforms, restricting their use in many important applications. In this paper, we propose BCNN with Separable Filters (BCNNw/SF), which applies Singular Value Decomposition (SVD) on BCNN kernels to further reduce computational and storage complexity. We provide a closed form of the gradient over SVD to calculate the exact gradient with respect to every binarized weight in backward propagation. We verify BCNNw/SF on the MNIST, CIFAR-10, and SVHN datasets, and implement an accelerator for CIFAR10 on FPGA hardware. Our BCNNw/SF accelerator realizes memory savings of 17% and execution time reduction of 31.3% compared to BCNN with only minor accuracy sacrifices.\r"
  },
  "cvpr2017_w4_jointmobile-cloudvideostabilization": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Joint Mobile-Cloud Video Stabilization",
    "authors": [
      "Gbolahan S. Adesoye",
      "Oliver Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Adesoye_Joint_Mobile-Cloud_Video_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Adesoye_Joint_Mobile-Cloud_Video_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this work we analyze the complex trade-off between data transfer, computation time, and power consumption when a multi-stage data-intensive algorithm (in this case video stabilization) is split between a low power mobile device and high power cloud server. We evaluate design choices in terms of which intermediate representations should be transferred to the server and back to the mobile device, and present a graph-based solution that can update the optimal joint mobile-cloud computation separation as the hardware configuration or user's requirements change. The practices we employ in this work can be extended to other mobile computer vision applications.\r"
  },
  "cvpr2017_w4_embeddedrobustvisualobstacledetectiononautonomouslawnmowers": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Embedded Robust Visual Obstacle Detection on Autonomous Lawn Mowers",
    "authors": [
      "Mathias Franzius",
      "Mark Dunn",
      "Nils Einecke",
      "Roman Dirnberger"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Franzius_Embedded_Robust_Visual_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Franzius_Embedded_Robust_Visual_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Currently, the only mass-market service robots are floor cleaners and lawn mowers. Although available for more than 20 years, they mostly lack intelligent functions from modern robot research. In particular, the obstacle detection and avoidance is typically a simple physical collision detection. In this work, we discuss a prototype autonomous lawn mower with camera-based non-contact obstacle avoidance. We devised a low-cost compact module consisting of color cameras and an ARM-based processing board, which can be added to an autonomous lawn mower with minimal effort. For testing our system, we conducted a field test with 20 prototype units distributed in eight European countries with a total mowing time of 3,494 hours. The results show that our proposed system is able to work without expert interaction for a full season and strongly reduces collision events while still keeping the good mowing performance.\r"
  },
  "cvpr2017_w4_improvedcooperativestereomatchingfordynamicvisionsensorswithgroundtruthevaluation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Improved Cooperative Stereo Matching for Dynamic Vision Sensors With Ground Truth Evaluation",
    "authors": [
      "Ewa Piatkowska",
      "Jurgen Kogler",
      "Nabil Belbachir",
      "Margrit Gelautz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Piatkowska_Improved_Cooperative_Stereo_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Piatkowska_Improved_Cooperative_Stereo_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Event-based vision, as realized by bio-inspired Dynamic Vision Sensors (DVS), is gaining more and more popularity due to its advantages of high temporal resolution, wide dynamic range and power efficiency at the same time. Potential applications include surveillance, robotics, and autonomous navigation under uncontrolled environment conditions. In this paper, we deal with event-based vision for 3D reconstruction of dynamic scene content by using two stationary DVS in a stereo configuration. We focus on a cooperative stereo approach and suggest an improvement over a previously published algorithm that reduces the measured mean error by over 50 percent. An available ground truth data set for stereo event data is utilized to analyze the algorithm's sensitivity to parameter variation and for comparison with competing techniques. \r"
  },
  "cvpr2017_w4_diagnosticmechanismandrobustnessofsafetyrelevantautomotivedeepconvolutionalnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Diagnostic Mechanism and Robustness of Safety Relevant Automotive Deep Convolutional Networks",
    "authors": [
      "Robert Krutsch",
      "Rolf Schlagenhaft"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Krutsch_Diagnostic_Mechanism_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Krutsch_Diagnostic_Mechanism_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper we investigate functional safety aspects for a road labeling application, a common task in the autonomous driving algorithmic stack. We introduce computationally light safety checks that reduce the error space significantly, train a CNN on the Cityscape dataset that reaches 93% mean IU and use Monte Carlo simulations to assess the impact of single event upset random hardware faults. The results show that the networks based on convolution and ReLU have some intrinsic robustness and that together with additional constraints strong function safety claims can be made. We compare also the diagnostic coverage between floating point and fixed point implementation of CNNs and summarize key safety features needed to achieve a high diagnostic coverage.\r"
  },
  "cvpr2017_w4_handgesturebasedregionmarkingfortele-supportusingwearables": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Hand Gesture Based Region Marking for Tele-Support Using Wearables",
    "authors": [
      "Archie Gupta",
      "Shreyash Mohatta",
      "Jitender Maurya",
      "Ramakrishna Perla",
      "Ramya Hebbalaguppe",
      "Ehtesham Hassan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Gupta_Hand_Gesture_Based_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Gupta_Hand_Gesture_Based_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Wearable Augmented Reality devices are being explored in many applications for visualizing real-time contextual information. More importantly, these devices can also be used in tele-assistance from remote sites when on-field operators require off-field expert's guidance for trouble-shooting. For an effective communication, touchless hand gestures are the most intuitive to select a Region Of Interest (ROI) like defective parts in a machine, through a wearable. This paper presents a hand gestural interaction method to localise the ROI in FPV. Novelty of the proposed method include (a)touchless finger based gesture recognition algorithm that runs on smartphones, which can be used with wearable frugal modality like Google Cardboard/Wearality, (b)reducing the network latency and achieving real-time performance by on-board implementation of recognition module. We also conducted user studies that suggest the usefulness of the proposed method and evaluated it using the PASCAL VOC criteria. \r"
  },
  "cvpr2017_w4_evenmoreconfidentpredictionswithdeepmachine-learning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Even More Confident Predictions With Deep Machine-Learning",
    "authors": [
      "Matteo Poggi",
      "Fabio Tosi",
      "Stefano Mattoccia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Poggi_Even_More_Confident_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Poggi_Even_More_Confident_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Confidence measures aim at discriminating unreliable disparities inferred by a stereo vision system from reliable ones. A common and effective strategy adopted by most top-performing approaches consists in combining multiple confidence measures by means of an appropriately trained random-forest classifier. In this paper, we propose a novel approach by training an n-channel convolutional neural network on a set of feature maps, each one encoding the outcome of a single confidence measure. This strategy enables to move the confidence prediction problem from the conventional 1D feature maps domain, adopted by approaches based on random-forests, to a more distinctive 3D domain, going beyond single pixel analysis. This fact, coupled with a deep network appropriately trained on a small subset of images, enables to outperform top-performing approaches based on random-forests.\r"
  },
  "cvpr2017_w4_low-complexityglobalmotionestimationforaerialvehicles": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Low-Complexity Global Motion Estimation for Aerial Vehicles ",
    "authors": [
      "Nirmala Ramakrishnan",
      "Alok Prakash",
      "Thambipillai Srikanthan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Ramakrishnan_Low-Complexity_Global_Motion_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Ramakrishnan_Low-Complexity_Global_Motion_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Global motion estimation (GME) algorithms are typically employed on aerial videos captured by on-board UAV cameras to compensate for the artificial motion induced in these video frames due to camera motion. However, existing methods for GME have high computational complexity and are therefore not suitable for on-board processing in UAVs with limited computing capabilities. In this paper, we propose a novel low complexity technique for GME that exploits the characteristics of aerial videos to only employ the minimum, yet, well-distributed features based on the scene complexity. Experiments performed on a mobile SoC platform, similar to the ones used in UAVs, confirm that the proposed technique achieves a speedup in execution time of over 40% without compromising the accuracy of the GME step when compared to a conventional method.\r"
  },
  "cvpr2017_w4_lcdetlow-complexityfully-convolutionalneuralnetworksforobjectdetectioninembeddedsystems": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems",
    "authors": [
      "Subarna Tripathi",
      "Gokce Dane",
      "Byeongkeun Kang",
      "Vasudev Bhaskaran",
      "Truong Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Tripathi_LCDet_Low-Complexity_Fully-Convolutional_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Tripathi_LCDet_Low-Complexity_Fully-Convolutional_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Deep convolutional Neural Networks (CNN) are the state-of-the-art performers for object detection task. It is well known that object detection requires more computation and memory than image classification. Thus the consolidation of a CNN-based object detection for an embedded system is more challenging. In this work, we propose LCDet, a fully-convolutional neural network for generic object detection that aims to work in embedded systems. We design and develop an end-to-end TensorFlow(TF)-based model. Additionally, we employ 8-bit quantization on the learned weights. We use face detection as a use case. Our TF-Slim based network can predict different faces of different shapes and sizes in a single forward pass. Our experimental results show that the LCDet achieves comparative accuracy comparing with state-of-the-art CNN-based face detection methods, while reducing the model size by 12x and memory-BW by16x comparing with one of the best real-time CNN-based object detectors YOLO. \r"
  },
  "cvpr2017_w4_image-basedvisualperceptionandrepresentationforcollisionavoidance": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Image-Based Visual Perception and Representation for Collision Avoidance",
    "authors": [
      "Cevahir Cigla",
      "Roland Brockers",
      "Larry Matthies"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Cigla_Image-Based_Visual_Perception_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Cigla_Image-Based_Visual_Perception_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a novel on-board perception system for collision avoidance by micro air vehicles (MAV). An egocentric cylindrical representation is utilized to model the world using forward-looking stereo vision. This efficient representation enables a 360o field of regard, as the vehicle moves around and disparity maps are fused temporally on the cylindrical map. For this purpose, we developed a new Gaussian Mixture Models-based disparity image fusion algorithm, with an extension to handle independently moving objects (IMO). The extension improves scene models in case of moving objects, where standard temporal fusion approaches cannot detect movers and introduce errors in world models due to the common static scene assumption. The on-board implementation of the vision pipeline provides disparity maps on a 360o egocentric cylindrical surface at 10 Hz. The perception output is used in our system by real-time motion planning with collision avoidance on the MAV.\r"
  },
  "cvpr2017_w4_pruningconvnetsonlineforefficientspecialistmodels": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Pruning ConvNets Online for Efficient Specialist Models",
    "authors": [
      "Jia Guo",
      "Miodrag Potkonjak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Guo_Pruning_ConvNets_Online_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Guo_Pruning_ConvNets_Online_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Convolutional neural networks (CNNs) excel in various computer vision related tasks but are extremely computationally intensive and power hungry to run on mobile and embedded devices. Recent pruning techniques can reduce the computation and memory requirements of CNNs, but a costly retraining step is needed to restore the classification accuracy of the pruned model. In this paper, we present evidence that when only a subset of the classes need to be classified, we could prune a model and achieve reasonable classification accuracy without retraining. The resulting specialist model will require less energy and time to run than the original full model. To compensate for the pruning, we take advantage of the redundancy among filters and class-specific features. We show that even simple methods such as replacing channels with mean or with the most correlated channel can boost the accuracy of the pruned model to reasonable levels.\r"
  },
  "cvpr2017_w4_real-timedriverdrowsinessdetectionforembeddedsystemusingmodelcompressionofdeepneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Real-Time Driver Drowsiness Detection for Embedded System Using Model Compression of Deep Neural Networks",
    "authors": [
      "Bhargava Reddy",
      "Ye-Hoon Kim",
      "Sojung Yun",
      "Chanwon Seo",
      "Junik Jang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Reddy_Real-Time_Driver_Drowsiness_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Reddy_Real-Time_Driver_Drowsiness_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Driver's status is crucial because one of the main reasons for motor vehicular accidents is related to driver's inattention or drowsiness. A drowsiness detector on a car can reduce numerous accidents. Accidents occur because of a single moment of negligence, thus driver monitoring system which works in real-time is necessary. This detector should be deployable to an embedded device and perform at high accuracy. In this paper, a novel approach towards real-time drowsiness detection based on deep learning which can be implemented on a low cost embedded board and performs with a high accuracy is proposed. Main contribution of our paper is compression of heavy baseline model to a light weight model deployable to an embedded board. Moreover, minimized network structure was designed based on facial landmark input to recognize whether driver is drowsy or not. The proposed model achieved an accuracy of 89.5% on 3-class classification and speed of 14.9 frames per second (FPS) on Jetson TK1.\r"
  },
  "cvpr2017_w4_squeezedetunified,small,lowpowerfullyconvolutionalneuralnetworksforreal-timeobjectdetectionforautonomousdriving": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",
    "authors": [
      "Bichen Wu",
      "Forrest Iandola",
      "Peter H. Jin",
      "Kurt Keutzer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Wu_SqueezeDet_Unified_Small_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Wu_SqueezeDet_Unified_Small_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Object detection is a crucial task for autonomous driving. In addition to requiring high accuracy to ensure safety, object detection for autonomous driving also requires real-time inference speed to guarantee prompt vehicle control, as well as small model size and energy efficiency to enable embedded system deployment. In this work, we propose SqueezeDet, a fully convolutional neural network for object detection that aims to simultaneously satisfy all of the above constraints. In our network, we use convolutional layers to extract feature maps and compute bounding boxes and class probabilities. The detection pipeline of our model only contains a single forward pass of a neural network, thus it is extremely fast. Our model is fully convolutional, which leads to small model size and better energy efficiency. Finally, our experiments show that our model is very accurate, achieving state-of-the-art accuracy on the KITTI benchmark. The source code of SqueezeDet is open-source released.\r"
  },
  "cvpr2017_w4_trainingsparseneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "Training Sparse Neural Networks",
    "authors": [
      "Suraj Srinivas",
      "Akshayvarun Subramanya",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Srinivas_Training_Sparse_Neural_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Srinivas_Training_Sparse_Neural_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The emergence of Deep neural networks has seen human-level performance on large scale computer vision tasks such as image classification. However these deep networks typically contain large amount of parameters due to dense matrix multiplications and convolutions. As a result, these architectures are highly memory intensive, making them less suitable for embedded vision applications. Sparse Computations are known to be much more memory efficient. In this work, we train and build neural networks which implicitly use sparse computations. We introduce additional gate variables to perform parameter selection and show that this is equivalent to using a spike-and-slab prior. We experimentally validate our method on both small and large networks which result in highly sparse neural network models.\r"
  },
  "cvpr2017_w4_squeezemapfastpedestriandetectiononalow-powerautomotiveprocessorusingefficientconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Embedded Vision",
    "title": "SqueezeMap: Fast Pedestrian Detection on a Low-Power Automotive Processor Using Efficient Convolutional Neural Networks",
    "authors": [
      "Rytis Verbickas",
      "Robert Laganiere",
      "Daniel Laroche",
      "Changyun Zhu",
      "Xiaoyin Xu",
      "Ali Ors"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/html/Verbickas_SqueezeMap_Fast_Pedestrian_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w4/papers/Verbickas_SqueezeMap_Fast_Pedestrian_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Pedestrian detection for autonomous driving is a challenging task that requires careful trade-off between accuracy, storage, computation and energy requirements. In our work, we extend the recent SqueezeNet architecture to pedestrian detection. We show how this network can be modified to obtain detection performance on the Caltech USA pedestrian dataset that is comparable in overall log-average miss rate to other competing models while easily running at 30FPS on an automotive processor with a model size of 3.24MB and within a power envelope of 2W. The extension relies on the observation that precise knowledge of bounding box corners is not necessary to know the location of pedestrians and their approximate size. Rather, a coarse grid based localization is proposed here and acts as a kind of heatmap of pedestrian locations, relying on only one forward pass through the network. The number of new free parameters introduced is small relative to the original SqueezeNet model.\r"
  },
  "cvpr2017_w5_learningrobotactivitiesfromfirst-personhumanvideosusingconvolutionalfutureregression": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Learning Robot Activities From First-Person Human Videos Using Convolutional Future Regression",
    "authors": [
      "Jangwon Lee",
      "Michael S. Ryoo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Lee_Learning_Robot_Activities_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Lee_Learning_Robot_Activities_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing an activity from their own viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new fully convolutional neural network architecture to regress the intermediate scene representation corresponding to the future frame, thereby enabling explicit forecasting of future hand locations given the current frame.\r"
  },
  "cvpr2017_w5_end-to-enddrivinginarealisticracinggamewithdeepreinforcementlearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "End-To-End Driving in a Realistic Racing Game With Deep Reinforcement Learning",
    "authors": [
      "Etienne Perot",
      "Maximilian Jaritz",
      "Marin Toromanoff",
      "Raoul de Charette"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Perot_End-To-End_Driving_in_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Perot_End-To-End_Driving_in_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We address the problem of autonomous race car driving. Using a recent rally game (WRC6) with realistic physics and graphics we train an Asynchronous Actor Critic (A3C) in an end-to-end fashion and propose an improved reward function to learn faster. The network is trained simultaneously on three very different tracks (snow, mountain, and coast) with various road structures, graphics and physics. Despite the more complex environments the trained agent learns significant features and exhibits good performance while driving in a more stable way than existing end-to-end approaches.\r"
  },
  "cvpr2017_w5_automatedriskassessmentforsceneunderstandinganddomesticrobotsusingrgb-ddataand2.5dcnnsatapatchlevel": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Automated Risk Assessment for Scene Understanding and Domestic Robots Using RGB-D Data and 2.5D CNNs at a Patch Level",
    "authors": [
      "Rob Dupre",
      "Georgios Tzimiropoulos",
      "Vasileios Argyriou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Dupre_Automated_Risk_Assessment_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Dupre_Automated_Risk_Assessment_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this work the notion of automated risk assessment for 3D scenes is addressed. Using deep learning techniques smart enabled homes and domestic robots can be equipped with the functionality to detect, draw attention to, or mitigate hazards in a given scene. We extend an existing risk estimation framework that incorporates physics and shape descriptors by introducing a novel CNN architecture allowing risk detection at a patch level. Analysis is conducted on RGB-D data and is performed on a frame by frame basis, requiring no temporal information between frames.\r"
  },
  "cvpr2017_w5_semanticinstancesegmentationforautonomousdriving": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Semantic Instance Segmentation for Autonomous Driving",
    "authors": [
      "Bert De Brabandere",
      "Davy Neven",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/De_Brabandere_Semantic_Instance_Segmentation_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/De_Brabandere_Semantic_Instance_Segmentation_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation and is well-suited for real-time applications. In contrast to previous works, our method does not rely on object proposals or recurrent mechanisms and is particularly well suited for tasks with complex occlusions. A key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on-par with more complex methods. We achieve competitive performance on the Cityscapes segmentation benchmark.\r"
  },
  "cvpr2017_w5_real-timehandgrasprecognitionusingweaklysupervisedtwo-stageconvolutionalneuralnetworksforunderstandingmanipulationactions": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Real-Time Hand Grasp Recognition Using Weakly Supervised Two-Stage Convolutional Neural Networks for Understanding Manipulation Actions",
    "authors": [
      "Ji Woong Kim",
      "Sujeong You",
      "Sang Hoon Ji",
      "Hong Seok Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Kim_Real-Time_Hand_Grasp_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Kim_Real-Time_Hand_Grasp_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Understanding human hand usage is one of the richest information source to recognize human manipulation actions. Since humans use various tools during actions, grasp recognition gives important cues to figure out humans' intention and tasks. Earlier studies analyzed grasps with positions of hand joints by attaching sensors, but since these types of sensors prevent humans from naturally conducting actions, visual approaches have been focused in recent years. Convolutional neural networks require a vast annotated dataset, but, to our knowledge, no human grasping dataset includes ground truth of hand regions. In this paper, we propose a grasp recognition method only with image-level labels by the weakly supervised learning framework. In addition, we split the grasp recognition process into two stages that are hand localization and grasp classification so as to speed up. Experimental results demonstrate that the proposed method outperforms existing methods and can perform in real-time.\r"
  },
  "cvpr2017_w5_findinganomalieswithgenerativeadversarialnetworksforapatrolbot": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Finding Anomalies With Generative Adversarial Networks for a Patrolbot",
    "authors": [
      "Wallace Lawson",
      "Esube Bekele",
      "Keith Sullivan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Lawson_Finding_Anomalies_With_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Lawson_Finding_Anomalies_With_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present an anomaly detection system based on an autonomous robot performing a patrol task. Using a generative adversarial network (GAN), we compare the robot's current view with a learned model of normality. Our preliminary experimental results show that the approach is well suited for anomaly detection, providing efficient results with a low false positive rate. \r"
  },
  "cvpr2017_w5_time-contrastivenetworksself-supervisedlearningfrommulti-viewobservation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Time-Contrastive Networks: Self-Supervised Learning From Multi-View Observation",
    "authors": [
      "Pierre Sermanet",
      "Corey Lynch",
      "Jasmine Hsu",
      "Sergey Levine"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Sermanet_Time-Contrastive_Networks_Self-Supervised_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Sermanet_Time-Contrastive_Networks_Self-Supervised_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a self-supervised approach for learning representations of relationships between humans and their environment, including object interactions, attributes, and body pose, entirely from unlabeled videos recorded from multiple viewpoints. We train our representation as an embedding with a triplet loss that opposes simultaneous frames from different viewpoints against temporally adjacent and visually similar frames. This opposition allows to disambiguate the possible explanations for temporal changes in the world. We demonstrate that our model can correctly identify corresponding steps in complex object interactions, such as pouring, between different videos and with different instances. We also show what is, to the best of our knowledge, the first self-supervised results for end-to-end imitation learning of human motions with a real robot.\r"
  },
  "cvpr2017_w5_curiosity-drivenexplorationbyself-supervisedprediction": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Curiosity-Driven Exploration by Self-Supervised Prediction",
    "authors": [
      "Deepak Pathak",
      "Pulkit Agrawal",
      "Alexei A. Efros",
      "Trevor Darrell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Pathak_Curiosity-Driven_Exploration_by_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Pathak_Curiosity-Driven_Exploration_by_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward; 2) exploration with no extrinsic reward; and 3) generalization to unseen scenarios (e.g. new levels of the same game).\r"
  },
  "cvpr2017_w5_leveragingdeepreinforcementlearningforreachingrobotictasks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Leveraging Deep Reinforcement Learning for Reaching Robotic Tasks",
    "authors": [
      "Kapil Katyal",
      "I-Jeng Wang",
      "Philippe Burlina"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Katyal_Leveraging_Deep_Reinforcement_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Katyal_Leveraging_Deep_Reinforcement_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This work leverages Deep Reinforcement Learning (DRL) to make robotic control immune to changes in the robot manipulator or the environment and to perform reaching, collision avoidance and grasping without explicit, prior and fine knowledge of the human arm structure and kinematics, without careful hand-eye calibration, solely based on visual/retinal input, and in ways that are robust to environmental changes. We learn a manipulation policy which we show takes the first steps toward generalizing to changes in the environment and can scale and adapt to new manipulators. Experiments are aimed at a) comparing different DCNN network architectures b) assessing the reward prediction for two radically different manipulators and c) performing a sensitivity analysis comparing a classical visual servoing formulation of the reaching task with the proposed DRL method.\r"
  },
  "cvpr2017_w5_handmovementpredictionbasedcollision-freehuman-robotinteraction": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Hand Movement Prediction Based Collision-Free Human-Robot Interaction",
    "authors": [
      "Yiwei Wang",
      "Xin Ye",
      "Yezhou Yang",
      "Wenlong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Wang_Hand_Movement_Prediction_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Wang_Hand_Movement_Prediction_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a framework from vision based hand movement prediction in a real-world human-robot collaborative scenario for safety guarantee. We first propose a perception submodule that takes in visual data solely and predicts human collaborator's hand movement. Then a robot trajectory adaptive planning submodule is developed that takes the noisy movement prediction signal into consideration for optimization. We first collect a new human manipulation dataset that can supplement the previous publicly available dataset with motion capture data to serve as the ground truth of hand location. We then integrate the algorithm with a robot manipulator that can collaborate with human workers on a set of trained manipulation actions, and it is shown that such a robot system outperforms the one without movement prediction in terms of collision avoidance.\r"
  },
  "cvpr2017_w5_3dposeregressionusingconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "3D Pose Regression Using Convolutional Neural Networks",
    "authors": [
      "Siddharth Mahendran",
      "Haider Ali",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Mahendran_3D_Pose_Regression_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Mahendran_3D_Pose_Regression_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " 3D pose estimation is a key component of many important computer vision tasks like autonomous navigation and robot manipulation. Current state-of-the-art approaches for 3D object pose estimation, like Viewpoints & Keypoints and Render for CNN, solve this problem by discretizing the pose space into bins and solving a pose-classification task. We argue that 3D pose is continuous and can be solved in a regression framework if done with the right representation, data augmentation and loss function. We modify a standard VGG network for the task of 3D pose regression and show competitive performance compared to state-of-the-art. \r"
  },
  "cvpr2017_w5_tuningmodularnetworkswithweightedlossesforhand-eyecoordination": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Tuning Modular Networks With Weighted Losses for Hand-Eye Coordination",
    "authors": [
      "Fangyi Zhang",
      "Jurgen Leitner",
      "Michael Milford",
      "Peter I. Corke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Zhang_Tuning_Modular_Networks_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Zhang_Tuning_Modular_Networks_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuo-motor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.\r"
  },
  "cvpr2017_w5_episode-basedactivelearningwithbayesianneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Episode-Based Active Learning With Bayesian Neural Networks",
    "authors": [
      "Feras Dayoub",
      "Niko Sunderhauf",
      "Peter I. Corke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.\r"
  },
  "cvpr2017_w5_detectingandgroupingidenticalobjectsforregionproposalandclassification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Learning for Robotic Vision",
    "title": "Detecting and Grouping Identical Objects for Region Proposal and Classification",
    "authors": [
      "Wim Abbeloos",
      "Sergio Caccamo",
      "Esra Ataer-Cansizoglu",
      "Yuichi Taguchi",
      "Chen Feng",
      "Teng-Yok Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/html/Abbeloos_Detecting_and_Grouping_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w5/papers/Abbeloos_Detecting_and_Grouping_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Often multiple instances of an object occur in the same scene, for example in a warehouse. Unsupervised multi-instance object discovery algorithms are able to detect and identify such objects. We use such an algorithm to provide object proposals to a convolutional neural network (CNN) based classifier. This results in fewer regions to evaluate, compared to traditional region proposal algorithms. Additionally, it enables using the joint probability of multiple instances of an object, resulting in improved classification accuracy. The proposed technique can also split a single class into multiple sub-classes corresponding to the different object types, enabling hierarchical classification.\r"
  },
  "cvpr2017_w6_ageestimationguidedconvolutionalneuralnetworkforage-invariantfacerecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Age Estimation Guided Convolutional Neural Network for Age-Invariant Face Recognition",
    "authors": [
      "Tianyue Zheng",
      "Weihong Deng",
      "Jiani Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Zheng_Age_Estimation_Guided_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Zheng_Age_Estimation_Guided_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " While very promising results have been shown on face recognition related problems, age-invariant face recognition still remains a challenge. Facial appearance of a person changes over time, which results in significant intra-class variations. In order to address this problem, we propose a novel deep face recognition network called age estimation guided convolutional neural network (AE-CNN) to separate the variations caused by aging from the person-specific features which are stable. The carefully designed CNN model can learn age-invariant features for face recognition. To the best of our knowledge, this is the first attempt to use age estimation task for obtaining age-invariant features. Extensive results on two well-known public domain face aging datasets: MORPH Album 2 and CACD show the effectiveness of the proposed approach.\r"
  },
  "cvpr2017_w6_deeplda-prunednetsforefficientfacialgenderclassification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Deep LDA-Pruned Nets for Efficient Facial Gender Classification",
    "authors": [
      "Qing Tian",
      "Tal Arbel",
      "James J. Clark"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Tian_Deep_LDA-Pruned_Nets_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Tian_Deep_LDA-Pruned_Nets_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Many real-time tasks, such as human-computer interaction, require fast and efficient facial gender classification. Although deep CNN nets have been very effective for a multitude of classification tasks, their high space and time demands make them impractical for personal computers and mobile devices without a powerful GPU. In this paper, we develop a 16-layer, yet lightweight, neural network which boosts efficiency while maintaining high accuracy. Our net is pruned from the VGG-16 model starting from the last convolutional (conv) layer where we find neuron activations are highly uncorrelated given the gender. Through Fisher's Linear Discriminant Analysis (LDA), we show that this high decorrelation makes it safe to discard directly last conv layer neurons with high within-class variance and low between-class variance. Combined with either Support Vector Machines (SVM) or Bayesian classification, the reduced CNNs are capable of achieving comparable (or even higher) accuracies on the LFW and CelebA datasets than the original net with fully connected layers. On LFW, only four Conv5_3 neurons are able to maintain a comparably high recognition accuracy, which results in a reduction of total network size by a factor of 70X with a 11 fold speedup. Comparisons with a state-of-the-art pruning method (as well as two smaller nets) in terms of accuracy loss and convolutional layers pruning rate are also provided. \r"
  },
  "cvpr2017_w6_adaptivedeepmetriclearningforidentity-awarefacialexpressionrecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Adaptive Deep Metric Learning for Identity-Aware Facial Expression Recognition",
    "authors": [
      "Xiaofeng Liu",
      "B. V. K. Vijaya Kumar",
      "Jane You",
      "Ping Jia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Liu_Adaptive_Deep_Metric_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Liu_Adaptive_Deep_Metric_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A key challenge of facial expression recognition (FER) is to develop effective representations to balance the complex distribution of intra- and inter- class variations. The latest deep convolutional networks proposed for FER are trained by penalizing the misclassification of images via the softmax loss. In this paper, we show that better FER performance can be achieved by combining the deep metric loss and softmax loss in a unified two fully connected layer branches framework via joint optimization. A generalized adaptive (N+M)-tuplet clusters loss function together with the identity-aware hard-negative mining and online positive mining scheme are proposed for identity-invariant FER. It reduces the computational burden of deep metric learning, and alleviates the difficulty of threshold validation and anchor selection. Extensive evaluations demonstrate that our method outperforms many state-of-art approaches on the posed as well as spontaneous facial expression databases.\r"
  },
  "cvpr2017_w6_gaitganinvariantgaitfeatureextractionusinggenerativeadversarialnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "GaitGAN: Invariant Gait Feature Extraction Using Generative Adversarial Networks",
    "authors": [
      "Shiqi Yu",
      "Haifeng Chen",
      "Edel B. Garcia Reyes",
      "Norman Poh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Yu_GaitGAN_Invariant_Gait_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Yu_GaitGAN_Invariant_Gait_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The performance of gait recognition can be adversely affected by many sources of variation such as view angle, clothing, presence of and type of bag, posture, and occlusion, among others. In order to extract invariant gait features, we proposed a method named as GaitGAN which is based on generative adversarial networks (GAN). In the proposed method, a GAN model is taken as a regressor to generate invariant gait images that is side view images with normal clothing and without carrying bags. A unique advantage of this approach is that the view angle and other variations are not needed before generating invariant gait images. The most important computational challenge, however, is to address how to retain useful identity information when generating the invariant gait images. To this end, our approach differs from the traditional GAN which has only one discriminator in that GaitGAN contains two discriminators. One is a fake/real discriminator which can make the generated gait images to be realistic. Another one is an identification discriminator which ensures that the the generated gait images contain human identification information. Experimental results show that GaitGAN can achieve state-of-the-art performance. To the best of our knowledge this is the first gait recognition method based on GAN with encouraging results. Nevertheless, we have identified several research directions to further improve GaitGAN.\r"
  },
  "cvpr2017_w6_componentbiologicallyinspiredfeatureswithmovingsegmentationforageestimation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Component Biologically Inspired Features With Moving Segmentation for Age Estimation",
    "authors": [
      "Gee-Sern Jison Hsu",
      "Yi-Tseng Cheng",
      "Choon Ching Ng",
      "Moi Hoon Yap"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Hsu_Component_Biologically_Inspired_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Hsu_Component_Biologically_Inspired_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose the Component Bio-Inspired Feature (CBIF) with a moving segmentation scheme for age estimation. The CBIF defines a superset for the commonly used Bio-Inspired Feature (BIF) with more parameters and flexibility in settings, resulting in features with abundant characteristics. An in-depth study is performed for the determination of the parameters good for capturing age-related traits. The moving segmentation is proposed to better determine the age boundaries good for age grouping, and improve the overall performance. The proposed approach is evaluated on two common benchmarks, FG-NET and MORPH databases, and compared with contemporary approaches to demonstrate its efficacy. \r"
  },
  "cvpr2017_w6_facerecognitionperformanceunderaging": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Face Recognition Performance Under Aging",
    "authors": [
      "Debayan Deb",
      "Lacey Best-Rowden",
      "Anil K. Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Deb_Face_Recognition_Performance_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Deb_Face_Recognition_Performance_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " With the integration of face recognition technology into important identity applications, it is imperative that the effects of facial aging on face recognition performance are thoroughly understood. As face recognition systems evolve and improve, they should be periodically re-evaluated on large-scale longitudinal face datasets. In our study, we evaluate the performance of two state-of-the-art commercial off the shelf (COTS) face recognition systems on two large-scale longitudinal datasets of mugshots of repeat offenders. The largest of these two datasets has 147,784 images of 18,007 subjects with an average of 8 images per subject over an average time span of 8.5 years. We fit multilevel statistical models to genuine comparison scores (similarity between images of the same face) from the two COTS face matchers. This allows us to analyze the degradation in recognition performance due to elapsed time between a probe (query) and its enrollment (gallery) image. We account for face image quality to obtain a better estimate of trends due to aging, and analyze whether longitudinal trends in genuine scores differ by subject gender and race. Based on the results of our statistical model, we infer that the state-of-the-art COTS matchers can verify 99% of the subjects at a false accept rate (FAR) of 0.01% for up to 10.5 and 8.5 years of elapsed time. Beyond this time lapse of 8.5 years, there is a significant loss in face recognition accuracy. This study extends and confirms the findings of earlier longitudinal studies on face recognition.\r"
  },
  "cvpr2017_w6_predictingfacerecognitionperformanceinunconstrainedenvironments": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Predicting Face Recognition Performance in Unconstrained Environments",
    "authors": [
      "P. Jonathon Phillips",
      "Amy N. Yates",
      "J. Ross Beveridge",
      "Geof Givens"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Phillips_Predicting_Face_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Phillips_Predicting_Face_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": "While face recognition algorithms perform under many different unconstrained conditions, predicting this performance is not possible when a new location is introduced. Analyzing the impostor distribution of the videos of the Point-and-Shoot Challenge (PaSC) as well as its relationship to the genuine match distribution, we present a method for predicting the performance of an algorithm using only unlabeled data for a new location. \r"
  },
  "cvpr2017_w6_personre-identificationforimprovedmulti-personmulti-cameratrackingbycontinuousentityassociation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Person Re-Identification for Improved Multi-Person Multi-Camera Tracking by Continuous Entity Association",
    "authors": [
      "Neeti Narayan",
      "Nishant Sankaran",
      "Devansh Arpit",
      "Karthik Dantu",
      "Srirangaraj Setlur",
      "Venu Govindaraju"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Narayan_Person_Re-Identification_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Narayan_Person_Re-Identification_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a novel approach to person tracking within the context of entity association. In large-scale distributed multi-camera systems, person re-identification is a challenging computer vision task as the problem is two-fold: detecting entities through identification and recognition techniques; and connecting entities temporally by associating them in often crowded environments. Since tracking essentially involves linking detections, we can reformulate it purely as a re-identification task. The inherent advantage of such a reformulation lies in the ability of the tracking algorithm to effectively handle temporal discontinuities in multi-camera environments. To accomplish this, we model human appearance, face biometric and location constraints across cameras. We do not make restrictive assumptions such as number of people in a scene. Our approach is validated by using a simple and efficient inference algorithm. Results on two publicly available datasets, CamNeT and DukeMTMC, are significantly better compared to other existing methods.\r"
  },
  "cvpr2017_w6_towardopen-setfacerecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Toward Open-Set Face Recognition",
    "authors": [
      "Manuel Gunther",
      "Steve Cruz",
      "Ethan M. Rudd",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Gunther_Toward_Open-Set_Face_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Gunther_Toward_Open-Set_Face_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Much research has been conducted on both face identification and face verification, with greater focus on the latter. Research on face identification has mostly focused on using closed-set protocols, which assume that all probe images used in evaluation contain identities of subjects that are enrolled in the gallery. Real systems, however, where only a fraction of probe sample identities are enrolled in the gallery, cannot make this closed-set assumption. Instead, they must assume an open set of probe samples and be able to reject/ignore those that correspond to unknown identities. In this paper, we address the widespread misconception that thresholding verification-like scores is a good way to solve the open-set face identification problem, by formulating an open-set face identification protocol and evaluating different strategies for assessing similarity. Our open-set identification protocol is based on the canonical labeled faces in the wild (LFW) dataset. Additionally to the known identities, we introduce the concepts of known unknowns (known, but uninteresting persons) and unknown unknowns (people never seen before) to the biometric community. We compare three algorithms for assessing similarity in a deep feature space under an open-set protocol: thresholded verification-like scores, linear discriminant analysis (LDA) scores, and an extreme value machine (EVM) probabilities. Our findings suggest that thresholding EVM probabilities, which are open-set by design, outperforms thresholding verification-like scores.\r"
  },
  "cvpr2017_w6_investigatingnuisancefactorsinfacerecognitionwithdcnnrepresentation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Investigating Nuisance Factors in Face Recognition With DCNN Representation",
    "authors": [
      "Claudio Ferrari",
      "Giuseppe Lisanti",
      "Stefano Berretti",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Ferrari_Investigating_Nuisance_Factors_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Ferrari_Investigating_Nuisance_Factors_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Deep learning based approaches proved to be dramatically effective to address many computer vision applications, including \"face recognition in the wild\". It has been extensively demonstrated that methods exploiting Deep Convolutional Neural Networks (DCNN) are powerful enough to overcome to a great extent many problems that negatively affected computer vision algorithms based on hand-crafted features. These problems include variations in illumination, pose, expression and occlusion, to mention some. The DCNNs excellent discriminative power comes from the fact that they learn low- and high-level representations directly from the raw image data. Considering this, it can be assumed that the performance of a DCNN are influenced by the characteristics of the raw image data that are fed to the network. In this work, we evaluate the effect of different bounding box dimensions, alignment, positioning and data source on face recognition using DCNNs, and present a thorough evaluation on two well known, public DCNN architectures. \r"
  },
  "cvpr2017_w6_iarpajanusbenchmark-bfacedataset": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "IARPA Janus Benchmark-B Face Dataset",
    "authors": [
      "Cameron Whitelam",
      "Emma Taborsky",
      "Austin Blanton",
      "Brianna Maze",
      "Jocelyn Adams",
      "Tim Miller",
      "Nathan Kalka",
      "Anil K. Jain",
      "James A. Duncan",
      "Kristen Allen",
      "Jordan Cheney",
      "Patrick Grother"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Whitelam_IARPA_Janus_Benchmark-B_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Whitelam_IARPA_Janus_Benchmark-B_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Despite the importance of rigorous testing data for evaluating face recognition algorithms, all major publicly available faces-in-the-wild datasets are constrained by the use of a commodity face detector, which limits, among other conditions, pose, occlusion, expression, and illumination variations. In 2015, the NIST IJB-A dataset, which consists of 500 subjects, was released to mitigate these constraints. However, the relatively low number of impostor and genuine matches per split in the IJB-A protocol limits the evaluation of an algorithm at operationally relevant assessment points. This paper builds upon IJB-A and introduces the IARPA Janus Benchmark-B (NIST IJB-B) dataset, a superset of IJB-A. IJB-B consists of 1,845 subjects with human-labeled ground truth face bounding boxes, eye/nose locations, and covariate metadata such as occlusion, facial hair, and skintone for 21,798 still images and 55,026 frames from 7,011 videos. IJB-B was also designed to have a more uniform geographic distribution of subjects across the globe than that of IJB-A. Test protocols for IJB-B represent operational use cases including access point identification, forensic quality media searches, surveillance video searches, and clustering. Finally, all images and videos in IJB-B are published under a Creative Commons distribution license and, therefore, can be freely distributed among the research community. \r"
  },
  "cvpr2017_w6_efficientimagesetclassificationusinglinearregressionbasedimagereconstruction": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Efficient Image Set Classification Using Linear Regression Based Image Reconstruction",
    "authors": [
      "Syed A. A. Shah",
      "Uzair Nadeem",
      "Mohammed Bennamoun",
      "Ferdous Sohel",
      "Roberto Togneri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Shah_Efficient_Image_Set_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Shah_Efficient_Image_Set_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a novel image set classification technique using linear regression models. Downsampled gallery image sets are interpreted as subspaces of a high dimensional space to avoid the computationally expensive training step. We estimate regression models for each test image using the class specific gallery subspaces. Images of the test set are then reconstructed using the regression models. Based on the minimum reconstruction error between the reconstructed and the original images, a weighted voting strategy is used to classify the test set. We performed extensive evaluation on the benchmark UCSD/Honda, CMU Mobo and YouTube Celebrity datasets for face classification, and ETH-80 dataset for object classification. The results demonstrate that by using only a small amount of training data, our technique achieved competitive classification accuracy and superior computational speed compared with the state-of-the-art methods.\r"
  },
  "cvpr2017_w6_deepconvolutionalneuralnetworkusingtripletsoffaces,deepensemble,andscore-levelfusionforfacerecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Deep Convolutional Neural Network Using Triplets of Faces, Deep Ensemble, and Score-Level Fusion for Face Recognition",
    "authors": [
      "Bong-Nam Kang",
      "Yonghyun Kim",
      "Daijin Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Kang_Deep_Convolutional_Neural_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Kang_Deep_Convolutional_Neural_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper proposes a new face verification method that uses multiple deep convolutional neural networks (DCNNs) and a deep ensemble, that extracts two types of low dimensional but discriminative and high-level abstracted features from each DCNN, then combines them as a descriptor for face verification. Our DCNNs are built from stacked multi-scale convolutional layer blocks to present multi-scale abstraction. To train our DCNNs, we use different resolutions of triplets that consist of reference images, positive images, and negative images, and triplet-based loss function that maximize the ratio of distances between negative pairs and positive pairs and minimize the absolute distances between positive face images. A deep ensemble is generated from features extracted by each DCNN, and used as a descriptor to train the joint Bayesian learning and its transfer learning method. On the LFW, although we use only 198,018 images and only four different types of networks, the proposed method with the joint Bayesian learning and its transfer learning method achieved 98.33% accuracy. In addition to further increase the accuracy, we combine the proposed method and high dimensional LBP based joint Bayesian method, and achieved 99.08% accuracy on the LFW. Therefore, the proposed method helps to improve the accuracy of face verification when training data is insufficient to train DCNNs.\r"
  },
  "cvpr2017_w6_transferlearningbasedevolutionaryalgorithmforcompositefacesketchrecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Transfer Learning Based Evolutionary Algorithm for Composite Face Sketch Recognition",
    "authors": [
      "Tarang Chugh",
      "Maneet Singh",
      "Shruti Nagpal",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Chugh_Transfer_Learning_Based_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Chugh_Transfer_Learning_Based_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Matching facial sketches to digital face images has widespread application in law enforcement scenarios. Recent advancements in technology have led to the availability of sketch generation tools, minimizing the requirement of a sketch artist. While these sketches have helped in manual authentication, matching composite sketches with digital mugshot photos automatically show high modality gap. This research aims to address the task of matching a composite face sketch image to digital images by proposing a transfer learning based evolutionary algorithm. A new feature descriptor, Histogram of Image Moments, has also been presented for encoding features across modalities. Moreover, IIITD Composite Face Sketch Database of 150 subjects is presented to fill the gap due to limited availability of databases in this problem domain. Experimental evaluation and analysis on the proposed dataset show the effectiveness of the transfer learning approach for performing cross-modality recognition. \r"
  },
  "cvpr2017_w6_analysis,comparison,andassessmentoflatentfingerprintimagepreprocessing": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Analysis, Comparison, and Assessment of Latent Fingerprint Image Preprocessing",
    "authors": [
      "Haiying Guan",
      "Paul Lee",
      "Andrew Dienstfrey",
      "Mary Theofanos",
      "Curtis Lamp",
      "Brian Stanton",
      "Matthew T. Schwarz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Guan_Analysis_Comparison_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Guan_Analysis_Comparison_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Latent fingerprints obtained from crime scenes are rarely immediately suitable for identification purposes. Instead, most latent fingerprint images must be preprocessed to enhance the fingerprint information held within the digital image, while suppressing interference arising from noise and otherwise unwanted image features. In the following we present results of our ongoing research to assess this critical step in the forensic workflow. Previously we discussed the creation of a new database of latent fingerprint images to support such research. The new contributions of this paper are twofold. First, we implement a study in which a group of trained Latent Print Examiners provide Extended Feature Set markups of all images. We discuss the experimental design of this study, and its execution. Next, we propose metrics for measuring the increase of fingerprint information provided by latent fingerprint image preprocessing, and we present preliminary analysis of these metrics when applied to the images in our database. We consider formally defined quality scales (Good, Bad, Ugly), and minutiae identifications of latent fingerprint images before and after preprocessing. All analyses show that latent fingerprint image preprocessing results in a statistically significant increase in fingerprint information and quality.\r"
  },
  "cvpr2017_w6_parsimoniouscodingandverificationofofflinehandwrittensignatures": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Parsimonious Coding and Verification of Offline Handwritten Signatures",
    "authors": [
      "Elias N. Zois",
      "Ilias Theodorakopoulos",
      "Dimitrios Tsourounis",
      "George Economou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Zois_Parsimonious_Coding_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Zois_Parsimonious_Coding_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A common practice for addressing the problem of verifying the presence, or the consent of a person in many transactions is to utilize the handwritten signature. Among others, the offline or static signature is a valuable tool in forensic related studies. Thus, the importance of verifying static handwritten signatures still poses a challenging task. Throughout the literature, gray-level images, composed of handwritten signature traces are subjected to numerous processing stages; their outcome is the mapping of any input signature image in a so-called corresponding feature space. Pattern recognition techniques utilize this feature space, usually as a binary verification problem. In this work, sparse dictionary learning and coding are for the first time employed as a means to provide a feature space for offline signature verification, which intuitively adapts to a small set of randomly selected genuine reference samples, thus making it attractable for forensic cases. In this context, the K-SVD dictionary learning algorithm is employed in order to create a writer oriented lexicon. For any signature sample, sparse representation with the use of the writer's lexicon and the Orthogonal Matching Pursuit algorithm generates a weight matrix; features are then extracted by applying simple average pooling to the generated sparse codes. The performance of the proposed scheme is demonstrated using the popular CEDAR, MCYT75 and GPDS300 signature datasets, delivering state of the art results.\r"
  },
  "cvpr2017_w6_robustverificationwithsubsurfacefingerprintrecognitionusingfullfieldopticalcoherencetomography": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Robust Verification With Subsurface Fingerprint Recognition Using Full Field Optical Coherence Tomography",
    "authors": [
      "Kiran B. Raja",
      "Egidijus Auksorius",
      "R. Raghavendra",
      "A. Claude Boccara",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Raja_Robust_Verification_With_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Raja_Robust_Verification_With_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Fingerprint recognition has been extensively used in number of civilian applications ranging from border control to everyday identity verification. The threats to current systems emerge from two facts that can be attributed to loss in accuracy due to damaged external fingerprint and attacks on the sensors by creation of the artefacts simply by lifting the latent fingerprints. In the growing necessity for secure biometric systems that are based on fingerprinting, new generation sensors have been investigated which can image the subsurface fingerprint. In this work, we explore the subsurface fingerprint imaging by employing a custom-built in-house Full-Field Optical Coherent Tomography (FF-OCT) sensor for capturing the subsurface fingerprint. Further, we evaluate a newly constructed database of 200 unique fingerprint samples collected in 2 different sessions with 6 layers of fingerprints image corresponding to 6 subsurface fingerprints. We also propose a framework based on quality metrics to fuse the subsurface fingerprint images to achieve a robust verification accuracy which has resulted in Equal Error Rate (EER) of 0%. We also provide an extensive set of experiments to gauge the reliability of subsurface fingerprints and deduce the set of important conclusions for path forward in FF-OCT subsurface fingerprint imaging.\r"
  },
  "cvpr2017_w6_irissuper-resolutionusingiterativeneighborembedding": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Iris Super-Resolution Using Iterative Neighbor Embedding",
    "authors": [
      "Fernando Alonso-Fernandez",
      "Reuben A. Farrugia",
      "Josef Bigun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Alonso-Fernandez_Iris_Super-Resolution_Using_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Alonso-Fernandez_Iris_Super-Resolution_Using_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Iris recognition research is heading towards enabling more relaxed acquisition conditions. This has effects on the quality and resolution of acquired images, severely affecting the accuracy of recognition systems if not tackled appropriately. In this paper, we evaluate a super-resolution algorithm used to reconstruct iris images based on iterative neighbor embedding of local image patches which tries to represent input low-resolution patches while preserving the geometry of the original high-resolution space. To this end, the geometry of the low- and high-resolution manifolds are jointly considered during the reconstruction process. We validate the system with a database of 1,872 near-infrared iris images, while fusion of two iris comparators has been adopted to improve recognition performance. The presented approach is substantially superior to bilinear/bicubic interpolations at very low resolutions, and it also outperforms a previous PCA-based iris reconstruction approach which only considers the geometry of the low-resolution manifold during the reconstruction process.\r"
  },
  "cvpr2017_w6_irislivenessdetectionbyrelativedistancecomparisons": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Iris Liveness Detection by Relative Distance Comparisons",
    "authors": [
      "Federico Pala",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Pala_Iris_Liveness_Detection_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Pala_Iris_Liveness_Detection_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The focus of this paper is on presentation attack detection for the iris biometrics, which measures the pattern within the colored concentric circle of the subjects' eyes, to authenticate an individual to a generic user verification system. Unlike previous deep learning methods that use single convolutional neural network architectures, this paper develops a framework built upon triplet convolutional networks that takes as input two real iris patches and a fake patch or two fake patches and a genuine patch. The aim is to increase the number of training samples and to generate a representation that separates the real from the fake iris patches. The smaller architecture provides a way to do early stopping based on the liveness of single patches rather than the whole image. The matching is performed by computing the distance with respect to a reference set of real and fake examples. The proposed approach allows for real-time processing using a smaller network and provides equal or better than state-of-the-art performance on three benchmark datasets of photo-based and contact lens presentation attacks.\r"
  },
  "cvpr2017_w6_facepresentationattackdetectionbyexploringspectralsignatures": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Biometrics",
    "title": "Face Presentation Attack Detection by Exploring Spectral Signatures",
    "authors": [
      "R. Raghavendra",
      "Kiran B. Raja",
      "Sushma Venkatesh",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/html/Raghavendra_Face_Presentation_Attack_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w6/papers/Raghavendra_Face_Presentation_Attack_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Presentation attack on the face recognition systems is well studied in the biometrics community resulting in various techniques for detecting the attacks. A low-cost presentation attack (e.g. print attacks) on face recognition systems has been demonstrated for systems operating in visible, multispectral (visible and near infrared spectrum) and extended multispectral (more than two spectral bands spanning from visible to near infrared space, commonly in 500nm-1000nm). In this paper, we propose a novel method to detect the presentation attacks on the extended multispectral face recognition systems. The proposed method is based on characterising the reflectance properties of the captured image through the spectral signature. The spectral signature is further classified using the linear Support Vector Machine (SVM) to obtain the decision on presented sample as an artefact or bona-fide. Since the reflectance property of the human skin and the artefact material differ, the proposed method can efficiently detect the presentation attacks on the extended multispectral system. Extensive experiments are carried out on a publicly available extended multispectral database (EMSPAD) comprised of 50 subjects with two different Presentation Attack Instrument (PAI) generated using two different printers. The comparison analysis is presented by comparing the performance of the proposed scheme with the contemporary schemes based on the image fusion and PAD score level fusion. Based on the obtained results, the proposed method has indicated the best performance in detecting both known and unknown attacks. \r"
  },
  "cvpr2017_w7_thesquarerootvelocityframeworkforcurvesinahomogeneousspace": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "The Square Root Velocity Framework for Curves in a Homogeneous Space",
    "authors": [
      "Zhe Su",
      "Eric Klassen",
      "Martin Bauer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Su_The_Square_Root_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Su_The_Square_Root_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper we study the shape space of curves with values in a homogeneous space M = G/K, where G is a Lie group and K is a compact Lie subgroup. We generalize the square root velocity framework to obtain a reparametrization invariant metric on the space of curves in M. By identifying curves in M with their horizontal lifts in G, geodesics then can be computed. We can also mod out by reparametrizations and by rigid motions of M. In each of these quotient spaces, we can compute Karcher means, geodesics, and perform principal component analysis. We present numerical examples including the analysis of a set of hurricane paths.\r"
  },
  "cvpr2017_w7_poissondisksamplingonthegrassmannnianapplicationsinsubspaceoptimization": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Poisson Disk Sampling on the Grassmannnian: Applications in Subspace Optimization",
    "authors": [
      "Rushil Anirudh",
      "Bhavya Kailkhura",
      "Jayaraman J. Thiagarajan",
      "Peer-Timo Bremer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Anirudh_Poisson_Disk_Sampling_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Anirudh_Poisson_Disk_Sampling_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " To develop accurate inference algorithms on embedded manifolds such as the Grassmannian, we often employ several optimization tools and incorporate the characteristics of known manifolds as additional constraints. However, a direct analysis of the nature of functions on manifolds is rarely performed. In this paper, we propose an alternative approach to this inference by adopting a statistical pipeline that first generates an initial sampling of the manifold, and then performs subsequent analysis based on these samples. First, we introduce a better sampling technique based on dart throwing (called the Poisson disk sampling (PDS)) to effectively sample the Grassmannian. Next, using Grassmannian sparse coding, we demonstrate the improved coverage achieved by PDS. Finally, we develop a consensus approach, with Grassmann samples, to infer the optimal embeddings for linear dimensionality reduction, and show that the resulting solutions are nearly optimal.\r"
  },
  "cvpr2017_w7_riemannianvariancefilteringanindependentfilteringschemeforstatisticaltestsonmanifold-valueddata": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Riemannian Variance Filtering: An Independent Filtering Scheme for Statistical Tests on Manifold-Valued Data",
    "authors": [
      "Ligang Zheng",
      "Hyunwoo J. Kim",
      "Nagesh Adluru",
      "Michael A. Newton",
      "Vikas Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Zheng_Riemannian_Variance_Filtering_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Zheng_Riemannian_Variance_Filtering_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Performing large scale hypothesis testing on brain imaging data to identify group-wise differences (e.g., between healty and diseased subjects) typically leads to a large number of tests (one per voxel). Multiple testing adjustment (or correction) is necessary to control false positives, which may lead to lower detection power in detecting true positives. Motivated by the use of so-called \"independent filtering\" techniques in statistics (for genomics applications), this paper investigates the use of independent filtering for manifold-valued data (Diffusion Tensor Imaging, Cauchy Deformation Tensors) which are broadly used in neuroimaging studies Inspired by the concept of variance of a Riemannian Gaussian distribution, a type of non-specific data-dependent Riemannian variance filter is proposed. In practice, the filter will select a subset of the full set of voxels for performing the \r"
  },
  "cvpr2017_w7_measuringglide-reflectionsymmetryinhumanmovementsusingelasticshapeanalysis": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Measuring Glide-Reflection Symmetry in Human Movements Using Elastic Shape Analysis",
    "authors": [
      "Qiao Wang",
      "Chaitanya Potaraju",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Wang_Measuring_Glide-Reflection_Symmetry_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Wang_Measuring_Glide-Reflection_Symmetry_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Human bodies and movements exhibit inherent symmetry. However, an important class of everyday movements, such as walking, does not maintain symmetry at every time instance. The symmetry in these movements is a spatio-temporal glide-reflection symmetry. The ability to measure this type of symmetry will provide us opportunities for various computer-aided applications including health monitoring, rehabilitation, and athletic training. In this paper we propose a method that uses the tools from elastic shape analysis to provide continuous symmetry scores which measure the degree of glide-reflection symmetry in movements. These scores can be updated online after each frame, and easily combined to drive comprehensible feedback. Our preliminary experiment demonstrates that our symmetry scores can well distinguish between a normal gait and simulated stroke and Parkinsonian gaits. Our results also suggest that using the Riemannian elastic metric provides better scores than Euclidean approaches.\r"
  },
  "cvpr2017_w7_learningshapetrendsparameterestimationindiffusionsonshapemanifolds": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Learning Shape Trends: Parameter Estimation in Diffusions on Shape Manifolds",
    "authors": [
      "Valentina Staneva",
      "Laurent Younes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Staneva_Learning_Shape_Trends_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Staneva_Learning_Shape_Trends_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Learning the dynamics of shape is at the heart of many computer vision problems: object tracking, change detection, longitudinal shape analysis, trajectory classification, etc. In this work we address the problem of statistical inference of diffusion processes of shapes. We formulate a general It\\^o diffusion on the manifold of deformable landmarks and propose several drift models for the evolution of shapes. We derive explicit formulas for the maximum likelihood estimators of the unknown parameters in these models, and demonstrate their convergence properties on simulated sequences when true parameters are known. We further discuss how these models can be extended to a more general non-parametric approach to shape estimation.\r"
  },
  "cvpr2017_w7_ariemannianframeworkforlinearandquadraticdiscriminantanalysisonthetangentspaceofshapes": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Riemannian Framework for Linear and Quadratic Discriminant Analysis on the Tangent Space of Shapes",
    "authors": [
      "Susovan Pal",
      "Roger P. Woods",
      "Suchit Panjiyar",
      "Elizabeth Sowell",
      "Katherine L. Narr",
      "Shantanu H. Joshi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Pal_A_Riemannian_Framework_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Pal_A_Riemannian_Framework_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a Riemannian framework for linear and quadratic discriminant classification on the tangent plane of the shape space of curves. The shape space is infinite dimensional and is constructed out of square root velocity functions of curves. We introduce the idea of mean and covariance of shape-valued random variables and samples from a tangent space to the pre-shape space (invariant to translation and scaling) and then extend it to the full shape space (rotational invariance). The shape observations from the population are approximated by coefficients of a Fourier basis of the tangent space. The algorithms for linear and quadratic discriminant analysis are then defined using reduced dimensional features obtained by projecting the original shape observations on to the truncated Fourier basis. We show classification results on synthetic data and shapes of cortical sulci, corpus callosum curves, as well as facial midline curve profiles from patients with fetal alcohol syndrome (FAS).\r"
  },
  "cvpr2017_w7_signalclassificationinquotientspacesviagloballyoptimalvariationalcalculus": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Signal Classification in Quotient Spaces via Globally Optimal Variational Calculus",
    "authors": [
      "Gregory S. Chirikjian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Chirikjian_Signal_Classification_in_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Chirikjian_Signal_Classification_in_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A ubiquitous problem in pattern recognition is that of matching an observed time-evolving pattern (or signal) to a gold standard in order to recognize or characterize the meaning of a dynamic phenomenon. Examples include matching sequences of images in two videos, matching audio signals in speech recognition, or matching framed trajectories in robot action recognition. This paper shows that all of these problems can be aided by reparameterizing the temporal dependence of each signal individually to a universal standard timescale that allows pointwise comparison at each instance of time. Given two sequences, each with N timesteps, the complexity of the algorithm has a cost of O(N), which is an improvement on the most common method for matching two signals, i.e., dynamic time warping. The core of the approach presented here is that the universal standard timescale is found by solving a variational calculus problem related to differential geometry. \r"
  },
  "cvpr2017_w7_manifoldguidedlabeltransferfordeepdomainadaptation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Manifold Guided Label Transfer for Deep Domain Adaptation",
    "authors": [
      "Breton Minnehan",
      "Andreas Savakis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/html/Minnehan_Manifold_Guided_Label_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w7/papers/Minnehan_Manifold_Guided_Label_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a novel domain adaptation method for deep learning that combines adaptive batch normalization to produce a common feature-space between domains and label transfer with subspace alignment on deep features. The first step of our method automatically conditions the features from the source/target domain to have similar statistical distributions by normalizing the activations in each layer of our network using adaptive batch normalization. We then examine the clustering properties of the normalized features on a manifold to determine if the target features are well suited for the second of our algorithm, label-transfer. The second step of our method performs subspace alignment and k-means clustering on the feature manifold to transfer labels from the closest source cluster to each target cluster. The proposed manifold guided label transfer methods produce state of the art results for deep adaptation on several standard digit recognition datasets.\r"
  },
  "cvpr2017_w8_cnnbasedyeastcellsegmentationinmulti-modalfluorescentmicroscopydata": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "CNN Based Yeast Cell Segmentation in Multi-Modal Fluorescent Microscopy Data",
    "authors": [
      "Ali Selman Aydin",
      "Abhinandan Dubey",
      "Daniel Dovrat",
      "Amir Aharoni",
      "Roy Shilkrot"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Aydin_CNN_Based_Yeast_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Aydin_CNN_Based_Yeast_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a method for foreground segmentation of yeast cells in the presence of high-noise induced by intentional low illumination, where traditional approaches (e.g., threshold-based methods, specialized cell-segmentation methods) fail. To deal with these harsh conditions, we use a fully-convolutional semantic segmentation network based on the SegNet architecture. Our model is capable of segmenting patches extracted from yeast live-cell experiments with a mIOU score of 0.71 on unseen patches drawn from independent experiments. Further, we show that simultaneous multi-modal observations of bio-fluorescent markers can result in better segmentation performance than the DIC channel alone.\r"
  },
  "cvpr2017_w8_classificationandretrievalofdigitalpathologyscansanewdataset": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Classification and Retrieval of Digital Pathology Scans: A New Dataset",
    "authors": [
      "Morteza Babaie",
      "Shivam Kalra",
      "Aditya Sriram",
      "Christopher Mitcheltree",
      "Shujin Zhu",
      "Amin Khatami",
      "Shahryar Rahnamayan",
      "Hamid R. Tizhoosh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Babaie_Classification_and_Retrieval_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Babaie_Classification_and_Retrieval_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we introduce a new dataset, Kimia Path24, for image classification and retrieval in digital pathology. We use the whole scan images of 24 different tissue textures to generate 1,325 test patches of size 1000x1000 (0.5mm x 0.5mm). Training data can be generated according to preferences of algorithm designer and can range from approximately 27,000 to over 50,000 patches if the preset parameters are adopted. We propose a compound patch-and-scan accuracy measurement that makes achieving high accuracies quite challenging. In addition, we set the benchmarking line by applying LBP, dictionary approach and convolutional neural nets (CNNs) and report their results. The highest accuracy was 41.80% for CNN.\r"
  },
  "cvpr2017_w8_breastcancerhistopathologicalimageclassificationismagnificationimportant?": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Breast Cancer Histopathological Image Classification: Is Magnification Important?",
    "authors": [
      "Vibha Gupta",
      "Arnav Bhavsar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Gupta_Breast_Cancer_Histopathological_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Gupta_Breast_Cancer_Histopathological_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Breast cancer is one of the most common cancer in women worldwide. It is typically diagnosed via histopathological microscopy imaging, for which image analysis can aid physicians for more effective diagnosis. Given a large variability in tissue appearance, to better capture discriminative traits, images can be acquired at different optical magnifications. In this paper, we propose an approach which utilizes joint colour-texture features and a classifier ensemble for classifying breast histopathology images. While we demonstrate the effectiveness of the proposed framework, an important objective of this work is to study the image classification across different optical magnification levels. We provide interesting experimental results and related discussions, demonstrating a visible classification invariance with cross-magnification training-testing. Along with magnification-specific model, we also evaluate the magnification independent model, and compare the two to gain some insights.\r"
  },
  "cvpr2017_w8_delineationofskinstratainreflectanceconfocalmicroscopyimageswithrecurrentconvolutionalnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Delineation of Skin Strata in Reflectance Confocal Microscopy Images With Recurrent Convolutional Networks",
    "authors": [
      "Alican Bozkurt",
      "Trevor Gale",
      "Kivanc Kose",
      "Christi Alessi-Fox",
      "Dana H. Brooks",
      "Milind Rajadhyaksha",
      "Jennifer Dy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Bozkurt_Delineation_of_Skin_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Bozkurt_Delineation_of_Skin_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Reflectance confocal microscopy (RCM) is an effective, non-invasive pre-screening tool for cancer diagnosis. However, acquiring and reading RCM images requires extensive training and experience, and novice clinicians exhibit high variance in diagnostic accuracy. Consequently, there is a compelling need for quantitative tools to standardize image acquisition and analysis. In this study, we use deep recurrent convolutional neural networks to delineate skin strata in stacks of RCM images collected at consecutive depths. To perform diagnostic analysis, clinicians collect RCM images at 4-5 specific layers in the tissue. Our model automates this process by discriminating between RCM images of different layers. Testing our model on an expert labeled dataset of 504 RCM stacks, we achieve 87.97% classification accuracy, and a 9-fold reduction in the number of anatomically impossible errors compared to the previous state-of-the-art.\r"
  },
  "cvpr2017_w8_crowdsourcingforchromosomesegmentationanddeepclassification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Crowdsourcing for Chromosome Segmentation and Deep Classification",
    "authors": [
      "Monika Sharma",
      "Oindrila Saha",
      "Anand Sriraman",
      "Ramya Hebbalaguppe",
      "Lovekesh Vig",
      "Shirish Karande"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Sharma_Crowdsourcing_for_Chromosome_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Sharma_Crowdsourcing_for_Chromosome_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Observations of chromosomal segments or translocations during metaphase can indicate structural changes in cell genome, and is often used for diagnostic purposes. Karyotyping of the chromosomes under metaphase is done by characterizing the individual chromosomes in cell spread images. Currently considerable effort and time is spent to manually segment out overlapping chromosomes from cell images, and classifying the segmented chromosomes into one of the 24 types. There exists techniques which automate segmentation and classification of chromosomes with reasonable accuracy, but a human in the loop is often still required due to the criticality of domain. Therefore, we present a method to segment out and classify chromosomes for healthy patients using a combination of crowdsourcing for segmentation, pre-procesing and classification using deep learning. Experimental results are encouraging and promise to significantly reduce the cognitive burden of automatic karyotyping of chromosomes.\r"
  },
  "cvpr2017_w8_generativeadversariallearningforreducingmanualannotationinsemanticsegmentationonlargescalemiscroscopyimagesautomatedvesselsegmentationinretinalfundusimageastestcase": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Generative Adversarial Learning for Reducing Manual Annotation in Semantic Segmentation on Large Scale Miscroscopy Images: Automated Vessel Segmentation in Retinal Fundus Image as Test Case",
    "authors": [
      "Avisek Lahiri",
      "Kumar Ayush",
      "Prabir Kumar Biswas",
      "Pabitra Mitra"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Lahiri_Generative_Adversarial_Learning_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Lahiri_Generative_Adversarial_Learning_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Convolutional Neural Network(CNN) based semantic segmentation require extensive pixel level manual annotation which is daunting for large microscopic images. The paper is aimed towards mitigating this labeling effort by leveraging the recent concept of generative adversarial network(GAN) wherein a generator maps latent noise space to realistic images while a discriminator differentiates between samples drawn from database and generator. We extend this concept to a multi task learning wherein a discriminator-classifier network differentiates between fake/real examples and also assigns correct class labels. Though our concept is generic, we applied it for the challenging task of vessel segmentation in fundus images. We show that proposed method is more data efficient than a CNN. Specifically, with 150K, 30K and 15K training examples, proposed method achieves mean AUC of 0.962, 0.945 and 0.931 respectively, whereas the simple CNN achieves AUC of 0.960, 0.921 and 0.916 respectively.\r"
  },
  "cvpr2017_w8_microscopicbloodsmearsegmentationandclassificationusingdeepcontourawarecnnandextrememachinelearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Microscopic Blood Smear Segmentation and Classification Using Deep Contour Aware CNN and Extreme Machine Learning",
    "authors": [
      "Muhammad Imran Razzak",
      "Saeeda Naz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Razzak_Microscopic_Blood_Smear_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Razzak_Microscopic_Blood_Smear_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent advancement in genomic technologies has opened a new realm for early detection of diseases that shows potential to overcome the drawbacks of manual detection technologies. In this work, we have presented efficient contour aware segmentation approach based based on fully conventional network whereas for classification we have used extreme machine learning based on CNN features extracted from each segmented cell. We have evaluated system performance based on segmentation and classification on publicly available dataset. Experiment was conducted on 64000 blood cells and dataset is divided into 80% for training and 20% for testing. Segmentation results are compared with the manual segmentation and found that proposed approach provided with 98.12% and 98.16% for RBC and WBC respectively whereas classification accuracy is shown on publicly available dataset 94.71% and 98.68% for RBC \\& its abnormalities detection and WBC respectively. \r"
  },
  "cvpr2017_w8_applyingfasterr-cnnforobjectdetectiononmalariaimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Applying Faster R-CNN for Object Detection on Malaria Images",
    "authors": [
      "Jane Hung",
      "Anne Carpenter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Hung_Applying_Faster_R-CNN_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Hung_Applying_Faster_R-CNN_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Deep learning based models have had great success in object detection, but the state of the art models have not yet been widely applied to biological image data. We apply for the first time an object detection model previously used on natural images to identify cells and recognize their stages in brightfield microscopy images of malaria-infected blood. Many micro-organisms like malaria parasites are still studied by expert manual inspection and hand counting. This type of object detection task is challenging due to factors like variations in cell shape, density, and color, and uncertainty of some cell classes. In addition, annotated data useful for training is scarce, and the class distribution is inherently highly imbalanced due to the dominance of uninfected red blood cells. We use Faster Region-based Convolutional Neural Network (Faster R-CNN), one of the top performing object detection models in recent years, pre-trained on ImageNet but fine tuned with our data.\r"
  },
  "cvpr2017_w8_anearlyexperiencetowarddevelopingcomputeraideddiagnosisforgram-stainedsmearsimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "An Early Experience Toward Developing Computer Aided Diagnosis for Gram-Stained Smears Images",
    "authors": [
      "Johanna Carvajal",
      "Daniel F. Smith",
      "Kun Zhao",
      "Arnold Wiliem",
      "Paul Finucane",
      "Peter Hobson",
      "Anthony Jennings",
      "Rodney McDougall",
      "Brian Lovell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Carvajal_An_Early_Experience_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Carvajal_An_Early_Experience_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Gram stained direct smears test is a simple and cost effective way in early identification of infections. Unfortunately, this practice is considered time consuming and labour intensive. Most existing effort in this area is to perform high-magnification analysis of images taken from manually selected areas. In this paper, we address the problem of the automated selection of candidate areas for subsequent high-magnification analysis. We explore the possibility of selecting good candidate areas based on low-magnification images where bacteria are likely to be found when viewed in high-magnification images. To this end, we develop an approach to classify the areas according to the textural information of an image patch. We explore and study the efficacy of traditional textural features such as HOG, LBP, and 2D-DCT. Experiments show that the best variant method is able to select working areas where it is likely to find bacteria in high-powered objective images in a wide-range of images.\r"
  },
  "cvpr2017_w8_lookingunderthehooddeepneuralnetworkvisualizationtointerpretwhole-slideimageanalysisoutcomesforcolorectalpolyps": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Looking Under the Hood: Deep Neural Network Visualization to Interpret Whole-Slide Image Analysis Outcomes for Colorectal Polyps",
    "authors": [
      "Bruno Korbar",
      "Andrea M. Olofson",
      "Allen P. Miraflor",
      "Catherine M. Nicka",
      "Matthew A. Suriawinata",
      "Lorenzo Torresani",
      "Arief A. Suriawinata",
      "Saeed Hassanpour"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Korbar_Looking_Under_the_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Korbar_Looking_Under_the_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Histopathological characterization of colorectal polyps is an important principle for determining the risk of colorectal cancer and future rates of surveillance for patients. The process of characterization is time-intensive and requires years of specialized medical training. In this work, we propose a deep-learning-based image analysis approach that not only can accurately classify different types of polyps in whole-slide images, but also generates major regions and features on the slide through a model visualization approach. We argue that this visualization approach will make sense of the underlying reasons for the classification outcomes, significantly reduce the cognitive burden on clinicians, and improve the diagnostic accuracy for whole-slide image characterization tasks. Our results show the efficacy of this network visualization approach in recovering decisive regions and features for different types of polyps on whole-slide images according to the domain expert pathologists.\r"
  },
  "cvpr2017_w8_high-magnificationmulti-viewsbasedclassificationofbreastfineneedleaspirationcytologycellsamplesusingfusionofdecisionsfromdeepconvolutionalnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "High-Magnification Multi-Views Based Classification of Breast Fine Needle Aspiration Cytology Cell Samples Using Fusion of Decisions From Deep Convolutional Networks",
    "authors": [
      "Hrushikesh Garud",
      "S. P. K. Karri",
      "Debdoot Sheet",
      "Jyotirmoy Chatterjee",
      "Manjunatha Mahadevappa",
      "Ajoy K. Ray",
      "Arindam Ghosh",
      "Ashok K. Maity"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Garud_High-Magnification_Multi-Views_Based_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Garud_High-Magnification_Multi-Views_Based_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Fine needle aspiration cytology is commonly used for diagnosis of breast cancer, with traditional practice being based on the subjective visual assessment of the breast cytopathology cell samples under a microscope to evaluate the state of various cytological features. Therefore, there are many challenges in maintaining consistency and reproducibility of findings. However, digital imaging and computational aid in diagnosis can improve the diagnostic accuracy and reduce the effective workload of pathologists. This paper presents a deep convolutional neural network (CNN) based classification approach for the diagnosis of the cell samples using their microscopic high-magnification multi-views. The proposed approach has been tested using GoogLeNet architecture of CNN on an image dataset of 37 breast cytopathology samples (24 benign and 13 malignant), where the network was trained using images of54% cell samples and tested on the rest, achieving 89.7% mean accuracy in 8 fold validation.\r"
  },
  "cvpr2017_w8_nucleisegmentationoffluorescencemicroscopyimagesusingthreedimensionalconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Nuclei Segmentation of Fluorescence Microscopy Images Using Three Dimensional Convolutional Neural Networks",
    "authors": [
      "David Joon Ho",
      "Chichen Fu",
      "Paul Salama",
      "Kenneth W. Dunn",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Ho_Nuclei_Segmentation_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Ho_Nuclei_Segmentation_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Fluorescence microscopy enables one to visualize subcellular structures of living tissue or cells in three dimensions. This is especially true for two-photon microscopy using near-infrared light which can image deeper into tissue. To characterize and analyze biological structures, nuclei segmentation is a prerequisite step. Due to the complexity and size of the image data sets, manual segmentation is prohibitive. This paper describes a fully 3D nuclei segmentation method using three dimensional convolutional neural networks. To train the network, synthetic volumes with corresponding labeled volumes are automatically generated. Our results from multiple data sets demonstrate that our method can successfully segment nuclei in 3D.\r"
  },
  "cvpr2017_w8_deepxscopesegmentingmicroscopyimageswithadeepneuralnetwork": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "DeepXScope: Segmenting Microscopy Images With a Deep Neural Network",
    "authors": [
      "Philip Saponaro",
      "Wayne Treible",
      "Abhishek Kolagunda",
      "Timothy Chaya",
      "Jeffrey Caplan",
      "Chandra Kambhamettu",
      "Randall Wisser"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Saponaro_DeepXScope_Segmenting_Microscopy_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Saponaro_DeepXScope_Segmenting_Microscopy_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " High-speed confocal microscopy has shown great promise to yield insights into plant-fungal interactions by allowing for large volumes of leaf tissue to be imaged at high magnification. Currently, segmentation is performed either manually, which is infeasible for large amounts of data, or by developing separate algorithms to extract individual features within the image data. In this work, we propose the use of a single deep convolutional neural network architecture dubbed DeepXScope for automatically segmenting hyphal networks of the fungal pathogen and cell boundaries and stomata of the host plant. DeepXScope is trained on manually annotated images created for each of these structures. We describe experiments that show each individual structure can be accurately extracted automatically using DeepXScope. We anticipate that plant scientists will be able to use this network to automatically extract multiple structures of interest, and we plan to release our tool to the community.\r"
  },
  "cvpr2017_w8_transferringmicroscopyimagemodalitieswithconditionalgenerativeadversarialnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Transferring Microscopy Image Modalities With Conditional Generative Adversarial Networks",
    "authors": [
      "Liang Han",
      "Zhaozheng Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Han_Transferring_Microscopy_Image_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Han_Transferring_Microscopy_Image_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Phase Contrast (PC) and Differential Interference Contrast (DIC) microscopy are two popular noninvasive techniques for monitoring live cells. Each of these two image modalities has its own advantages and disadvantages to visualize specimens. In this paper, we investigate a conditional Generative Adversarial Network (conditional GAN) which contains one generator and two discriminators to transfer microscopy image modalities. Given a training dataset consisting of pairs of images (source and destination) captured on the same set of specimens by DIC and Phase Contrast microscopes, we can train a conditional GAN, and with this well-trained GAN, we can generate the corresponding Phase Contrast image given a new DIC image, vice versa. The preliminary experiments demonstrate that our approach outperforms one state-of-the-arts method, and can provide biologists a computational way to switch between microscopy image modalities.\r"
  },
  "cvpr2017_w8_fastneuralcelldetectionusinglight-weightssdneuralnetwork": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Fast Neural Cell Detection Using Light-Weight SSD Neural Network",
    "authors": [
      "Jingru Yi",
      "Pengxiang Wu",
      "Daniel J. Hoeppner",
      "Dimitris Metaxas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Yi_Fast_Neural_Cell_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Yi_Fast_Neural_Cell_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Identifying the lineage path of neural cells is critical for understanding the development of brain. Accurate neural cell detection is a crucial step to obtain reliable delineation of cell lineage. To solve this task, in this paper we present an efficient neural cell detection method based on SSD (single shot multibox detector) neural network model. Our method adapts the original SSD architecture and removes the unnecessary blocks, leading to a light-weight model. Moreover, we formulate the cell detection as a binary regression problem, which makes our model much simpler. Experimental results demonstrate that, with only a small training set, our method is able to accurately capture the neural cells under severe shape deformation in a fast way.\r"
  },
  "cvpr2017_w8_alevelsetmethodforglandsegmentation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "A Level Set Method for Gland Segmentation",
    "authors": [
      "Chen Wang",
      "Hong Bu",
      "Ji Bao",
      "Chunming Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/html/Wang_A_Level_Set_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w8/papers/Wang_A_Level_Set_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Histopathology plays a role as the gold standard in clinic for disease diagnosis. The identification and segmentation of histological structures are the prerequisite to disease diagnosis. With the advent of digital pathology, researchers' attention is attracted by the analysis of digital pathology images. In order to relieve the workload on pathologists, a robust segmentation method is needed in clinic for computer-assisted diagnosis. In this paper, we propose a level set framework to achieve gland image segmentation. The input image is divided into two parts, which contain glands with lumens and glands without lumens, respectively. Our experiments are performed on the clinical datasets of West China Hospital, Sichuan University. The experimental results show that our method can deal with glands without lumens, thus can obtain a better performance.\r"
  },
  "cvpr2017_w9_alargeanddiversedatasetforimprovedvehiclemakeandmodelrecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "A Large and Diverse Dataset for Improved Vehicle Make and Model Recognition",
    "authors": [
      "Faezeh Tafazzoli",
      "Hichem Frigui",
      "Keishin Nishiyama"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Tafazzoli_A_Large_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Tafazzoli_A_Large_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Vehicle Make and Model Recognition (VMMR) has evolved into a significant subject of study due to its importance in numerous Intelligent Transportation Systems (ITS) and its components such as Automated Vehicular Surveillance (AVS). A highly accurate and real-time VMMR system significantly reduces the overhead cost of resources other-wise required. The VMMR problem is a multi-class classification task with a peculiar set of issues and challenges like multiplicity, inter- and intra-make ambiguity among various classes, which need to be solved in an efficient and reliable manner to achieve a highly robust VMMR system. In this paper, facing the growing importance of make and model recognition of vehicles, we present a dataset with 9170 different classes of vehicles to advance the corresponding tasks. Extensive experiments conducted using baseline approaches yield superior results for images that were occluded, under low illumination or partial camera views, available in our VMMR dataset. \r"
  },
  "cvpr2017_w9_evaluatingstate-of-the-artobjectdetectoronchallengingtrafficlightdata": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "Evaluating State-Of-The-Art Object Detector on Challenging Traffic Light Data",
    "authors": [
      "Morten B. Jensen",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Jensen_Evaluating_State-Of-The-Art_Object_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Jensen_Evaluating_State-Of-The-Art_Object_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Traffic light detection (TLD) is a vital part of both intelligent vehicles and driving assistance systems (DAS). hard to determine the exact performance of a given method. In this paper we apply the state-of-the-art, real-time object detection system You Only Look Once, (YOLO) on the public LISA Traffic Light dataset available through the VIVA-challenge, which contain a high number of annotated traffic lights, captured in varying light and weather conditions. The YOLO object detector achieves an AUC of impressively 90.49 % for daysequence1, which is an improvement of 50.32 % compared to the latest ACF entry in the VIVA-challenge. Using the exact same training configuration as the ACF detector, the YOLO detector reaches an AUC of 58.3 %, which is in an increase of 18.13 %.\r"
  },
  "cvpr2017_w9_slotcars3dmodellingforimprovedvisualtrafficanalytics": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "Slot Cars: 3D Modelling for Improved Visual Traffic Analytics",
    "authors": [
      "Eduardo R. Corral-Soto",
      "James H. Elder"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Corral-Soto_Slot_Cars_3D_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Corral-Soto_Slot_Cars_3D_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A major challenge in visual highway traffic analytics is to disaggregate individual vehicles from clusters formed in dense traffic conditions. Here we introduce a data driven 3D generative reasoning method to tackle this segmentation problem. The method is comprised of offline (learning) and online (inference) stages. In the offline stage, we fit a mixture model for the prior distribution of vehicle dimensions to labelled data. Given camera intrinsic parameters and height, we use a parallelism method to estimate highway lane structure and camera tilt to project 3D models to the image. In the online stage, foreground vehicle cluster segments are extracted using motion and background subtraction. For each segment, we use a data-driven MCMC method to estimate the vehicles configuration and dimensions that provide the most likely account of the observed foreground pixels. We evaluate the method on two highway datasets and demonstrate a substantial improvement on the state of the art.\r"
  },
  "cvpr2017_w9_acost-effectiveframeworkforautomatedvehicle-pedestriannear-missdetectionthroughonboardmonocularvision": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "A Cost-Effective Framework for Automated Vehicle-Pedestrian Near-Miss Detection Through Onboard Monocular Vision",
    "authors": [
      "Ruimin Ke",
      "Jerome Lutin",
      "Jerry Spears",
      "Yinhai Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Ke_A_Cost-Effective_Framework_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Ke_A_Cost-Effective_Framework_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Onboard monocular cameras have been widely deployed in both public transit and personal vehicles. Obtaining vehicle-pedestrian near-miss event data from onboard monocular vision systems may be cost-effective compared with onboard multiple-sensor systems or traffic surveillance videos. But extracting near-misses from onboard monocular vision is challenging and little work has been published. This paper fills the gap by developing a framework to automatically detect vehicle-pedestrian near-misses through onboard monocular vision. The proposed framework can estimate depth and real-world motion information through monocular vision with a moving video background. The experimental results based on processing over 30-hours video data demonstrate the ability of the system to capture near-misses by comparison with the events logged by the Rosco/MobilEye Shield+ system which includes four cameras working cooperatively. The detection overlap rate reaches over 90% with the thresholds properly set.\r"
  },
  "cvpr2017_w9_edenensembleofdeepnetworksforvehicleclassification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "EDeN: Ensemble of Deep Networks for Vehicle Classification",
    "authors": [
      "Rajkumar Theagarajan",
      "Federico Pala",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Theagarajan_EDeN_Ensemble_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Theagarajan_EDeN_Ensemble_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Traffic surveillance has always been a challenging task to automate. The main difficulties arise from the high variation of the vehicles appertaining to the same category, low resolution, changes in illumination and occlusions. Due to the lack of large labeled datasets, deep learning techniques still have not shown their full potential. In this paper, we train an Ensemble of Deep Networks (EDeN) to successfully classify surveillance images into eleven different classes of vehicles. The MIO-TCD dataset consists of 786,702 images with high diversity and resembles a real-world environment. Extensive evaluation was performed using individual networks and different combinations of ensembles. Experimental results show that ensemble of networks gives better performance compared to individual networks and it is robust to noise. The ensemble of networks achieves an accuracy of 97.80%, mean precision of 94.39%, mean recall of 91.90% and Cohen kappa of 96.58%.\r"
  },
  "cvpr2017_w9_vehicletypeclassificationusingbaggingandconvolutionalneuralnetworkonmultiviewsurveillanceimage": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "Vehicle Type Classification Using Bagging and Convolutional Neural Network on Multi View Surveillance Image",
    "authors": [
      "Pyong-Kun Kim",
      "Kil-Taek Lim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Kim_Vehicle_Type_Classification_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Kim_Vehicle_Type_Classification_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper aims to introduce a new vehicle type classification scheme on the images from multi-view surveillance camera. We propose four concepts to increase the performance on the images which have various resolutions from multi-view point. The Deep Learning method is essential to multi-view point image, bagging method makes system robust, data augmentation help to grow the classification capability, and post-processing compensate for imbalanced data. We combine these schemes and build a novel vehicle type classification system. Our system shows 97.84% classification accuracy on the 103,833 images in classification challenge dataset.\r"
  },
  "cvpr2017_w9_deeplearning-basedvehicleclassificationusinganensembleoflocalexpertandglobalnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "Deep Learning-Based Vehicle Classification Using an Ensemble of Local Expert and Global Networks",
    "authors": [
      "Jong Taek Lee",
      "Yunsu Chung"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Lee_Deep_Learning-Based_Vehicle_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Lee_Deep_Learning-Based_Vehicle_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Vehicle classification has been a challenging problem because of pose variations, weather / illumination changes, inter-class similarity and insufficient training dataset. With the help of innovative deep learning algorithms and large scale traffic surveillance dataset, we are able to achieve high performance on vehicle classification. In order to improve performance, we propose an ensemble of global networks and mixture of K local expert networks. It achieved a mean accuracy of 97.92%, a mean precision of 92.98%, a mean recall of 90.24% and a Cohen Kappa score of 96.75% on unseen test dataset from the MIO-TCD classification challenge.\r"
  },
  "cvpr2017_w9_efficientscenelayoutawareobjectdetectionfortrafficsurveillance": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "Efficient Scene Layout Aware Object Detection for Traffic Surveillance",
    "authors": [
      "Tao Wang",
      "Xuming He",
      "Songzhi Su",
      "Yin Guan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Wang_Efficient_Scene_Layout_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Wang_Efficient_Scene_Layout_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present an efficient scene layout aware object detection method for traffic surveillance. Given an input image, our approach first estimates its scene layout by transferring object annotations in a large dataset to the target image based on nonparametric label transfer. The transferred annotations are then integrated with object hypotheses generated by the state-of-the-art object detectors. We propose an approximate nearest neighbor search scheme for efficient inference in the scene layout estimation. Experiments verified that this simple and efficient approach provides consistent performance improvements to the state-of-the-art object detection baselines on all object categories in the TSWC-2017 localization challenge.\r"
  },
  "cvpr2017_w9_resnet-basedvehicleclassificationandlocalizationintrafficsurveillancesystems": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Traffic Surveillance Workshop and Challenge",
    "title": "ResNet-Based Vehicle Classification and Localization in Traffic Surveillance Systems",
    "authors": [
      "Heechul Jung",
      "Min-Kook Choi",
      "Jihun Jung",
      "Jin-Hee Lee",
      "Soon Kwon",
      "Woo Young Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/html/Jung_ResNet-Based_Vehicle_Classification_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w9/papers/Jung_ResNet-Based_Vehicle_Classification_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we present ResNet-based vehicle classification and localization methods using real traffic surveillance recordings. We utilize a MIOvision traffic dataset, which comprises 11 categories including a variety of vehicles, such as bicycle, bus, car, motorcycle, and so on. To improve the classification performance, we exploit a technique called joint fine-tuning (JF). In addition, we propose a dropping CNN (DropCNN) method to create a synergy effect with the JF. For the localization, we implement basic concepts of state-of-the-art region based detector combined with a backbone convolutional feature extractor using 50 and 101 layers of residual networks and ensemble them into a single model. Finally, we achieved the highest accuracy in both classification and localization tasks using the dataset among several state-of-the-art methods, including VGG16, AlexNet, and ResNet50 for the classification, and YOLO Faster R-CNN, and SSD for the localization reported on the website.\r"
  },
  "cvpr2017_w10_scene-text-detectionmethodrobustagainstorientationanddiscontiguouscomponentsofcharacters": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Scene-Text-Detection Method Robust Against Orientation and Discontiguous Components of Characters",
    "authors": [
      "Rei Endo",
      "Yoshihiko Kawai",
      "Hideki Sumiyoshi",
      "Masanori Sano"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/html/Endo_Scene-Text-Detection_Method_Robust_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/papers/Endo_Scene-Text-Detection_Method_Robust_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Scene-text detection in natural-scene images is an important technique because scene texts contain location information such as names of places and buildings, but many difficulties still remain regarding practical use. In this paper, we tackle two problems of scene-text detection. The first is the discontiguous component problem in specific languages that contain characters consisting of discontiguous components. The second is the multi-orientation problem in all languages. To solve these two problems, we propose a connected-component-based scene-text-detection method. Our proposed method involves our novel neighbor-character search method using a synthesizable descriptor for the discontiguous-component problems and our novel region descriptor called the rotated bounding box descriptors (RBBs) for rotated characters. We also evaluated our proposed scene-text-detection method by using the well-known MSRA-TD500 dataset that includes rotated characters with discontiguous components.\r"
  },
  "cvpr2017_w10_uncertaintyquantificationoflucaskanadefeaturetrackandapplicationtovisualodometry": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Uncertainty Quantification of Lucas Kanade Feature Track and Application to Visual Odometry",
    "authors": [
      "Xue Iuan Wong",
      "Manoranjan Majji"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/html/Wong_Uncertainty_Quantification_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/papers/Wong_Uncertainty_Quantification_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " An uncertainty quantification approach to estimate the errors incurred by the Kanade Lucas Tomasi (KLT) feature tracking algorithm is presented. The covariance analysis is based on the linearized sensitivity calculations of the KLT algorithm. Track uncertainty thus computed is utilized to quantify the errors associated with feature based relative pose estimation algorithms. This paper also show that the uncertainty analysis result can serve as a mean to measures reliability of feature correspondences. Proposed technique show that a large amount of outlier can be ejected effectively, and thus improve the efficiency of iterative method such as RANSAC. \r"
  },
  "cvpr2017_w10_cluster-wiseratiotestsforfastcameralocalization": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Cluster-Wise Ratio Tests for Fast Camera Localization",
    "authors": [
      "Raul Diaz",
      "Charless C. Fowlkes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/html/Diaz_Cluster-Wise_Ratio_Tests_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/papers/Diaz_Cluster-Wise_Ratio_Tests_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Feature point matching for camera localization suffers from scalability problems. Even when feature descriptors associated with 3D scene points are locally unique, as coverage grows, similar or repeated features become increasingly common. As a result, the standard distance ratio-test used to identify reliable image feature points is overly restrictive and rejects many good candidate matches. We propose a simple coarse-to-fine strategy that uses conservative approximations to robust local ratio-tests that can be computed efficiently using global approximate k-nearest neighbor search. We treat these forward matches as votes in camera pose space and use them to prioritize back-matching within candidate camera pose clusters, exploiting feature co-visibility captured by the 3D model camera pose graph. This approach achieves state-of-the-art camera pose estimation results on a variety of benchmarks, outperforming several methods that use more complicated data structures and that make more restrictive assumptions on camera pose. We carry out diagnostic analyses on a difficult test dataset containing globally repetitive structure which suggest our approach successfully adapts to the challenges of large-scale pose estimation.\r"
  },
  "cvpr2017_w10_groundtruthaccuracyandperformanceofthematchingpipeline": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Ground Truth Accuracy and Performance of the Matching Pipeline",
    "authors": [
      "Josef Maier",
      "Martin Humenberger",
      "Oliver Zendel",
      "Markus Vincze"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/html/Maier_Ground_Truth_Accuracy_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/papers/Maier_Ground_Truth_Accuracy_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Feature matching quality strongly influences the accuracy of most computer vision tasks. This led to impressive advances in keypoint detection, descriptor calculation, and feature matching itself. To compare different approaches and evaluate their quality, datasets from related tasks are used. Unfortunately, none of these datasets actually provide ground truth (GT) feature matches. Thus, matches can only be approximated due to repeatability errors of keypoint detectors and inaccuracies of GT. In this paper, we introduce ground truth matches (GTM) for several well known datasets. Based on the provided spacial ground truth, we automatically generate them using popular feature types. Currently, feature matching evaluation is typically performed using precision and recall. The introduced GTM additionally enable evaluation with accuracy and fall-out. The datasets were manually annotated, on the one hand to evaluate the precision and unambiguousness of the GTM, and on the other hand to determine the accuracy of the ground truth provided with the datasets. Using GTM, we present an evaluation of multiple state-of-the-art keypoint-descriptor combinations as well as matching algorithms.\r"
  },
  "cvpr2017_w10_egotrackerpedestriantrackingwithre-identificationinegocentricvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "EgoTracker: Pedestrian Tracking With Re-Identification in Egocentric Videos",
    "authors": [
      "Jyoti Nigam",
      "Renu M. Rameshan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/html/Nigam_EgoTracker_Pedestrian_Tracking_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/papers/Nigam_EgoTracker_Pedestrian_Tracking_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose and analyze a novel framework for tracking a pedestrian in egocentric videos, which is needed for analyzing social gatherings recorded with a wearable camera. The constant camera and pedestrian movement makes this a challenging problem. The main challenges are natural head movement of wearer and target loss and reappearance in a later frame, due to frequent changes in field of view. By using the optical flow information specific to egocentric videos and also by modifying the learning process and sampling region of trackers which tracks by learning an SVM online, we show that re-identification is possible. The specific trackers chosen are STRUCK and MEEM.\r"
  },
  "cvpr2017_w10_probabilisticglobalscaleestimationformonoslambasedongenericobjectdetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Probabilistic Global Scale Estimation for MonoSLAM Based on Generic Object Detection",
    "authors": [
      "Edgar Sucar",
      "Jean-Bernard Hayet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/html/Sucar_Probabilistic_Global_Scale_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w10/papers/Sucar_Probabilistic_Global_Scale_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper proposes a novel method to estimate the global scale of a 3D reconstructed model within a Kalman filtering-based monocular SLAM algorithm. Our Bayesian framework integrates height priors over the detected objects belonging to a set of broad predefined classes, based on recent advances in fast generic object detection. Each observation is produced on single frames, so that we do not need a data association process along video frames. This is because we associate the height priors with the image region sizes at image places where map features projections fall within the object detection regions. We present very promising results of this approach obtained on several experiments with different object classes.\r"
  },
  "cvpr2017_w12_locallyadaptivecolorcorrectionforunderwaterimagedehazingandmatching": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Locally Adaptive Color Correction for Underwater Image Dehazing and Matching",
    "authors": [
      "Codruta O. Ancuti",
      "Cosmin Ancuti",
      "Christophe De Vleeschouwer",
      "Rafael Garcia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Ancuti_Locally_Adaptive_Color_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Ancuti_Locally_Adaptive_Color_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Underwater images are known to be strongly deterio- rated by a combination of wavelength-dependent light at- tenuation and scattering. This results in complex color casts that depend both on the scene depth map and on the light spectrum. Color transfer, which is a technique of choice to counterbalance color casts, assumes stationary casts, de- fined by global parameters, and is therefore not directly ap- plicable to the locally variable color casts encountered in underwater scenarios. To fill this gap, this paper introduces an original fusion-based strategy to exploit color transfer while tuning the color correction locally, as a function of the light attenuation level estimated from the red channel. The Dark Channel Prior (DCP) is then used to restore the color compensated image, by inverting the simplified Koschmieder light transmission model, as for outdoor de- hazing. Our technique enhances image contrast in a quite effective manner and also supports accurate transmission map estimation. Our extensive experiments also show that our color correction strongly improves the effectiveness of local keypoints matching.\r"
  },
  "cvpr2017_w12_depth-stretchenhancingdepthperceptionwithoutdepth": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Depth-Stretch: Enhancing Depth Perception Without Depth",
    "authors": [
      "Hagit Hel-Or",
      "Yacov Hel-Or",
      "Renato Keshet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Hel-Or_Depth-Stretch_Enhancing_Depth_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Hel-Or_Depth-Stretch_Enhancing_Depth_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A simple and efficient method is presented to enhance the depth perception of an image. The approach termed Depth-Stretch (D-stretch) is a tone mapping operation that is applied to the shading component of the given image. Although re-rendering a scene under geometric transformations typically requires extracting the 3D model of the scene, we show that under very simple assumptions D-stretch can be implemented without 3D reconstruction, while still providing a convincing effect of depth enhancement. \r"
  },
  "cvpr2017_w12_fastaframeworktoacceleratesuper-resolutionprocessingoncompressedvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "FAST: A Framework to Accelerate Super-Resolution Processing on Compressed Videos",
    "authors": [
      "Zhengdong Zhang",
      "Vivienne Sze"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Zhang_FAST_A_Framework_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Zhang_FAST_A_Framework_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " State-of-the-art super-resolution (SR) algorithms require significant computation resources to achieve real-time throughput (e.g., 60Mpixels/s for HD video). This paper introduces FAST (Free Adaptive Super-resolution via Transfer), a framework to accelerate any SR algorithm applied to compressed videos. FAST exploits the temporal correlation between adjacent frames such that SR is only applied to a subset of frames; SR pixels are then transferred to the other frames. The transferring process has negligible computation cost as it uses information already embedded in the compressed video (e.g., motion vectors and residual). Adaptive processing is used to retain accuracy when the temporal correlation is not present (e.g., occlusions). FAST accelerates state-of-the-art SR algorithms by up to 15x with a visual quality loss of 0.2dB. FAST is an important step towards real-time SR algorithms for ultra-HD displays and energy constrained devices (e.g., phones and tablets).\r"
  },
  "cvpr2017_w12_fastexternaldenoisingusingpre-learnedtransformations": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Fast External Denoising Using Pre-Learned Transformations",
    "authors": [
      "Shibin Parameswaran",
      "Enming Luo",
      "Charles-Alban Deledalle",
      "Truong Q. Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Parameswaran_Fast_External_Denoising_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Parameswaran_Fast_External_Denoising_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We introduce a new external denoising algorithm that utilizes pre-learned transformations to accelerate filter calculations during runtime. The proposed fast external denoising (FED) algorithm shares characteristics of the powerful Targeted Image Denoising (TID) and Expected Patch Log-Likelihood (EPLL) algorithms. By moving computationally demanding steps to an offline learning stage, the proposed approach aims to find a balance between processing speed and obtaining high quality denoising estimates. We evaluate FED on three datasets with targeted databases (text, face and license plates) and also on a set of generic images without a targeted database. We show that, like TID, the proposed approach is extremely effective when the transformations are learned using a targeted database. We also demonstrate that FED converges to competitive solutions faster than EPLL and is orders of magnitude faster than TID while providing comparable denoising performance.\r"
  },
  "cvpr2017_w12_formresnetformattedresiduallearningforimagerestoration": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "FormResNet: Formatted Residual Learning for Image Restoration",
    "authors": [
      "Jianbo Jiao",
      "Wei-Chih Tu",
      "Shengfeng He",
      "Rynson W. H. Lau"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Jiao_FormResNet_Formatted_Residual_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Jiao_FormResNet_Formatted_Residual_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we propose a deep CNN to tackle the image restoration problem by learning the structured residual. Previous deep learning based methods directly learn the mapping from corrupted images to clean images, and may suffer from the gradient exploding/vanishing problems of deep neural networks. We propose to address the image restoration problem by learning the structured details and recovering the latent clean image together, from the shared information between the corrupted image and the latent image. In addition, instead of learning the pure difference (corruption), we propose to add a \"residual formatting layer\" to format the residual to structured information, which allows the network to converge faster and boosts the performance. Furthermore, we propose a cross-level loss net to ensure both pixel-level accuracy and semantic-level visual quality. Evaluations on public datasets show that the proposed method outperforms existing approaches quantitatively and qualitatively.\r"
  },
  "cvpr2017_w12_exploitingreflectionalandrotationalinvarianceinsingleimagesuperresolution": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Exploiting Reflectional and Rotational Invariance in Single Image Superresolution",
    "authors": [
      "Simon Donne",
      "Laurens Meeus",
      "Hiep Quang Luong",
      "Bart Goossens",
      "Wilfried Philips"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Donne_Exploiting_Reflectional_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Donne_Exploiting_Reflectional_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Stationarity of reconstruction problems is the crux to enabling convolutional neural networks for many image processing tasks: the output estimate for a pixel is generally not dependent on its location within the image but only on its immediate neighbourhood. We expect other invariances, too. For most pixel-processing tasks, rigid transformations should commute with the processing: a rigid transformation of the input should result in that same transformation of the output. In existing literature this is taken into account indirectly by augmenting the training set: reflected and rotated versions of the inputs are also fed to the network when optimizing the network weights. In contrast, we enforce this invariance through the network design. Because of the encompassing nature of the proposed architecture, it can directly enhance existing CNN-based algorithms. We show how it can be applied to SRCNN and FSRCNN both, speeding up convergence in the initial training phase, and improving performance both for pretrained weights and after finetuning.\r"
  },
  "cvpr2017_w12_imagesuperresolutionbasedonfusingmultipleconvolutionneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Image Super Resolution Based on Fusing Multiple Convolution Neural Networks",
    "authors": [
      "Haoyu Ren",
      "Mostafa El-Khamy",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Ren_Image_Super_Resolution_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Ren_Image_Super_Resolution_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we focus on constructing an accurate super resolution system based on multiple Convolution Neural Networks (CNNs). Each individual CNN is trained separately with different network structure. A Context-wise Network Fusion (CNF) approach is proposed to integrate the outputs of individual networks by additional convolution layers. With fine-tuning the whole fused network, the accuracy is significantly improved compared to the individual networks. We also discuss other network fusion schemes, including Pixel-Wise network Fusion (PWF) and Progressive Network Fusion (PNF). The experimental results show that the CNF outperforms PWF and PNF. Using SRCNN as individual network, the CNF network achieves the state-of-the-art accuracy on benchmark image datasets. \r"
  },
  "cvpr2017_w12_palettenetimagerecolorizationwithgivencolorpalette": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "PaletteNet: Image Recolorization With Given Color Palette",
    "authors": [
      "Junho Cho",
      "Sangdoo Yun",
      "Kyoung Mu Lee",
      "Jin Young Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Cho_PaletteNet_Image_Recolorization_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Cho_PaletteNet_Image_Recolorization_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Image recolorization enhances the visual perception of an image for design and artistic purposes. In this work, we present a deep neural network, referred to as PaletteNet, which recolors an image according to a given target color palette that is useful to express the color concept of an image. PaletteNet takes two inputs: a source image to be recolored and a target palette. PaletteNet is then designed to change the color concept of a source image so that the palette of the output image is close to the target palette. To train PaletteNet, the proposed multi-task loss is composed of Euclidean loss and adversarial loss. The experimental results show that the proposed method outperforms the existing recolorization methods. Human experts with a commercial software take on average 18 minutes to recolor an image, while PaletteNet automatically recolors plausible results in less than a second.\r"
  },
  "cvpr2017_w12_srhrf+self-exampleenhancedsingleimagesuper-resolutionusinghierarchicalrandomforests": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "SRHRF+: Self-Example Enhanced Single Image Super-Resolution Using Hierarchical Random Forests",
    "authors": [
      "Jun-Jie Huang",
      "Tianrui Liu",
      "Pier Luigi Dragotti",
      "Tania Stathaki"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Huang_SRHRF_Self-Example_Enhanced_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Huang_SRHRF_Self-Example_Enhanced_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Example-based single image super-resolution (SISR) methods use external training datasets and have recently attracted a lot of interest. Self-example based SISR methods exploit redundant non-local self-similar patterns in natural images and because of that are more able to adapt to the image at hand to generate high quality super-resolved images. In this paper, we propose to combine the advantages of example-based SISR and self-example based SISR. A novel hierarchical random forests based super-resolution (SRHRF) method is proposed to learn statistical priors from external training images. Each layer of random forests reduce the estimation error due to variance by aggregating prediction models from multiple decision trees. The hierarchical structure further boosts the performance by pushing the estimation error due to bias towards zero. In order to further adaptively improve the super-resolved image, a self-example random forests (SERF) is learned from an image pyramid pair constructed from the down-sampled SRHRF generated result. Extensive numerical results show that the SRHRF method enhanced using SERF (SRHRF+) achieves the state-of-the-art performance on natural images and yields substantially superior performance for image with rich self-similar patterns.\r"
  },
  "cvpr2017_w12_imagedenoisingviacnnsanadversarialapproach": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Image Denoising via CNNs: An Adversarial Approach",
    "authors": [
      "Nithish Divakar",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Divakar_Image_Denoising_via_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Divakar_Image_Denoising_via_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Is it possible to recover an image from its noisy version using convolutional neural networks? This is an interesting problem as convolutional layers are generally used as feature detectors for tasks like classification, segmentation and object detection. We present a new CNN architecture for blind image denoising which synergically combines three architecture components, a multi-scale feature extraction layer which helps in reducing the effect of noise on feature maps, an l_p regularizer which helps in selecting only the appropriate feature maps for the task of reconstruction, and finally a three step training approach which leverages adversarial training to give the final performance boost to the model. The proposed model shows competitive denoising performance when compared to the state-of-the-art approaches.\r"
  },
  "cvpr2017_w12_multi-resolutiondatafusionforsuper-resolutionelectronmicroscopy": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Multi-Resolution Data Fusion for Super-Resolution Electron Microscopy",
    "authors": [
      "Suhas Sreehari",
      "S. V. Venkatakrishnan",
      "Katherine L. Bouman",
      "Jeffrey P. Simmons",
      "Lawrence F. Drummy",
      "Charles A. Bouman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Sreehari_Multi-Resolution_Data_Fusion_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Sreehari_Multi-Resolution_Data_Fusion_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Perhaps surprisingly, all electron microscopy (EM) data collected to date is less than a cubic millimeter -- presenting a huge demand in the materials and biological sciences to image at greater speed and lower dosage, while maintaining resolution. Traditional EM imaging based on homogeneous raster scanning severely limits the volume of high-resolution data that can be collected, and presents a fundamental limitation to understanding physical processes such as material deformation and crack propagation. We introduce a multi-resolution data fusion (MDF) method for super-resolution computational EM. Our method combines innovative data acquisition with novel algorithmic techniques to dramatically improve the resolution/volume/speed trade-off. The key to our approach is to collect the entire sample at low resolution, while simultaneously collecting a small fraction of data at high resolution. The high-resolution measurements are then used to create a material-specific model that is used within the \"plug-and-play\" framework to dramatically improve resolution of the low-resolution data. We present results using FEI electron microscope data that demonstrate super-resolution factors of 4x-16x, while substantially maintaining high image quality and reducing dosage.\r"
  },
  "cvpr2017_w12_fastandaccurateimagesuper-resolutionusingacombinedloss": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Fast and Accurate Image Super-Resolution Using a Combined Loss",
    "authors": [
      "Jinchang Xu",
      "Yu Zhao",
      "Yuan Dong",
      "Hongliang Bai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Xu_Fast_and_Accurate_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Xu_Fast_and_Accurate_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recently, several methods for single image super-resolution(SISR) based on deep neural networks have obtained high performance with regard to reconstruction accuracy and computational performance. This paper details the methodology and results of the New Trends in Image Restoration and Enhancement (NTIRE) challenge. The task of this challenge is to restore rich details (high frequencies) in a high resolution image for a single low resolution input image based on a set of prior examples with low and corresponding high resolution images. The challenge has two tracks. We present a super-resolution (SR) method, which uses three losses assigned with different weights to be regarded as optimization target. Meanwhile, the residual blocks are also used for obtaining significant improvement in the evaluation. The final model consists of 9 weight layers with four residual blocks and reconstructs the low resolution image with three color channels simultaneously, which shows better performance on these two tracks and benchmark datasets.\r"
  },
  "cvpr2017_w12_deepwaveletpredictionforimagesuper-resolution": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Deep Wavelet Prediction for Image Super-Resolution",
    "authors": [
      "Tiantong Guo",
      "Hojjat Seyed Mousavi",
      "Tiep Huu Vu",
      "Vishal Monga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Guo_Deep_Wavelet_Prediction_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Guo_Deep_Wavelet_Prediction_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent advances have seen a surge of deep learning approaches for image super-resolution. Invariably, a network, e.g. a deep convolutional neural network (CNN) or auto-encoder is trained to learn the relationship between low and high-resolution image patches. Recognizing that a wavelet transform provides a \"coarse\" as well as \"detail\" separation of image content, we design a deep CNN to predict the \"missing details\" of wavelet coefficients of the low-resolution images to obtain the Super-Resolution (SR) results, which we name Deep Wavelet Super-Resolution (DWSR). Out network is trained in the wavelet domain with four input and output channels respectively. The input comprises of 4 sub-bands of the low-resolution wavelet coefficients and outputs are residuals (missing details) of 4 sub-bands of high-resolution wavelet coefficients. Wavelet coefficients and wavelet residuals are used as input and outputs of our network to further enhance the sparsity of activation maps. A key benefit of such a design is that it greatly reduces the training burden of learning the network that reconstructs low frequency details. The output prediction is added to the input to form the final SR wavelet coefficients. Then the inverse 2d discrete wavelet transformation is applied to transform the predicted details and generate the SR results. We show that DWSR is computationally simpler and yet produces competitive and often better results than state-of-the-art alternatives.\r"
  },
  "cvpr2017_w12_ntire2017challengeonsingleimagesuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results",
    "authors": [
      "Radu Timofte",
      "Eirikur Agustsson",
      "Luc Van Gool",
      "Ming-Hsuan Yang",
      "Lei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Timofte_NTIRE_2017_Challenge_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Timofte_NTIRE_2017_Challenge_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper reviews the first challenge on single image super-resolution (restoration of rich details in an low resolution image) with focus on proposed solutions and results. A new DIVerse 2K resolution image dataset (DIV2K) was employed. The challenge had 6 competitions divided into 2 tracks with 3 magnification factors each. Track 1 employed the standard bicubic downscaling setup, while Track 2 had unknown downscaling operators (blur kernel and decimation) but learnable through low and high res train images. Each competition had 100 registered participants and 20 teams competed in the final testing phase. They gauge the state-of-the-art in single image super-resolution.\r"
  },
  "cvpr2017_w12_ntire2017challengeonsingleimagesuper-resolutiondatasetandstudy": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study",
    "authors": [
      "Eirikur Agustsson",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Agustsson_NTIRE_2017_Challenge_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Agustsson_NTIRE_2017_Challenge_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper introduces a novel large dataset for example-based single image super-resolution and studies the state-of-the-art as emerged from the NTIRE 2017 challenge. The challenge is the first challenge of its kind, with 6 competitions, hundreds of participants and tens of proposed solutions. Our newly collected DIVerse 2K resolution image dataset (DIV2K) was employed by the challenge. In our study we compare the solutions from the challenge to a set of representative methods from the literature and evaluate them using diverse measures on our proposed DIV2K dataset. Moreover, we conduct a number of experiments and draw conclusions on several topics of interest. We conclude that the NTIRE 2017 challenge pushes the state-of-the-art in single-image super-resolution, reaching the best results to date on the popular Set5, Set14, B100, Urban100 datasets and on our newly proposed DIV2K.\r"
  },
  "cvpr2017_w12_enhanceddeepresidualnetworksforsingleimagesuper-resolution": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
    "authors": [
      "Bee Lim",
      "Sanghyun Son",
      "Heewon Kim",
      "Seungjun Nah",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge. \r"
  },
  "cvpr2017_w12_beyonddeepresiduallearningforimagerestorationpersistenthomology-guidedmanifoldsimplification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification",
    "authors": [
      "Woong Bae",
      "Jaejun Yoo",
      "Jong Chul Ye"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Bae_Beyond_Deep_Residual_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Bae_Beyond_Deep_Residual_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The latest deep learning approaches perform better than the state-of-the-art signal processing approaches in various image restoration tasks. However, if an image contains many patterns and structures, the performance of these CNNs is still inferior. To address this issue, here we propose a novel feature space deep residual learning algorithm that outperforms the existing residual learning. The main idea is originated from the observation that the performance of a learning algorithm can be improved if the input and/or label manifolds can be made topologically simpler by an analytic mapping to a feature space. Our extensive numerical studies using denoising experiments and NTIRE single-image super-resolution (SISR) competition demonstrate that the proposed feature space residual learning outperforms the existing state-of-the-art approaches. Moreover, our algorithm was ranked high in the NTIRE competition with 5-10 times faster computational time compared to the top ranked teams.\r"
  },
  "cvpr2017_w12_adeepconvolutionalneuralnetworkwithselectionunitsforsuper-resolution": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "A Deep Convolutional Neural Network With Selection Units for Super-Resolution",
    "authors": [
      "Jae-Seok Choi",
      "Munchurl Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Choi_A_Deep_Convolutional_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Choi_A_Deep_Convolutional_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Rectified linear units (ReLU) are known to be effective in many deep learning methods. Inspired by linear-mapping technique used in other super-resolution (SR) methods, we reinterpret ReLU into point-wise multiplication of an identity mapping and a switch, and finally present a novel nonlinear unit, called a selection unit (SU). While conventional ReLU has no direct control through which data is passed, the proposed SU optimizes this on-off switching control, and is therefore capable of better handling nonlinearity functionality than ReLU in a more flexible way. Our proposed deep network with SUs, called SelNet, was top-5th ranked in NTIRE2017 Challenge, which has a much lower computation complexity compared to the top-4 entries. Further experiment results show that our proposed SelNet outperforms our baseline only with ReLU (without SUs), and other state-of-the-art deep-learning-based SR methods.\r"
  },
  "cvpr2017_w12_balancedtwo-stageresidualnetworksforimagesuper-resolution": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - New Trends in Image Restoration and Enhancement & Example-Based Single Image Super-Resolution Challenge",
    "title": "Balanced Two-Stage Residual Networks for Image Super-Resolution",
    "authors": [
      "Yuchen Fan",
      "Honghui Shi",
      "Jiahui Yu",
      "Ding Liu",
      "Wei Han",
      "Haichao Yu",
      "Zhangyang Wang",
      "Xinchao Wang",
      "Thomas S. Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/html/Fan_Balanced_Two-Stage_Residual_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w12/papers/Fan_Balanced_Two-Stage_Residual_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, balanced two-stage residual networks (BTSRN) are proposed for single image super-resolution. The deep residual design with constrained depth achieves the optimal balance between the accuracy and the speed for super-resolving images. The experiments show that the balanced two-stage structure, together with our lightweight two-layer PConv residual block design, achieves very promising results when considering both accuracy and speed. We evaluated our models on the New Trends in Image Restoration and Enhancement workshop and challenge on image super-resolution (NTIRE SR 2017). Our final model with only 10 residual blocks ranked among the best ones in terms of not only accuracy (6th among 20 final teams) but also speed (2nd among top 6 teams in terms of accuracy). The source code both for training and evaluation is available in https://github.com/ychfan/sr_ntire2017.\r"
  },
  "cvpr2017_w13_driveahead-alarge-scaledriverheadposedataset": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Vehicle Technology and Autonomous Driving Challenge",
    "title": "DriveAHead - A Large-Scale Driver Head Pose Dataset",
    "authors": [
      "Anke Schwarz",
      "Monica Haurilet",
      "Manuel Martinez",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/html/Schwarz_DriveAHead_-_A_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/papers/Schwarz_DriveAHead_-_A_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Head pose monitoring is an important task for driver assistance systems, since it is a key indicator for human attention and behavior. However, current head pose datasets either lack complexity or do not adequately represent the conditions that occur while driving. Therefore, we introduce DriveAHead, a novel dataset designed to develop and evaluate head pose monitoring algorithms in real driving conditions. We provide frame-by-frame head pose labels obtained from a motion-capture system, as well as annotations about occlusions of the driver's face. To the best of our knowledge, DriveAHead is the largest publicly available driver head pose dataset, and also the only one that provides 2D and 3D data aligned at the pixel level using the Kinect v2. Existing performance metrics are based on the mean error without any consideration of the bias towards one position or another. Here, we suggest a new performance metric, named Balanced Mean Angular Error, that addresses the bias towards the forward looking position existing in driving datasets. Finally, we present the Head Pose Network, a deep learning model that achieves better performance than current state-of-the-art algorithms, and we analyze its performance when using our dataset. \r"
  },
  "cvpr2017_w13_theonehundredlayerstiramisufullyconvolutionaldensenetsforsemanticsegmentation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Vehicle Technology and Autonomous Driving Challenge",
    "title": "The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation",
    "authors": [
      "Simon Jegou",
      "Michal Drozdzal",
      "David Vazquez",
      "Adriana Romero",
      "Yoshua Bengio"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/html/Jegou_The_One_Hundred_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/papers/Jegou_The_One_Hundred_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.\r"
  },
  "cvpr2017_w13_rear-stitchedviewpanoramaalow-powerembeddedimplementationforsmartrear-viewmirrorsonvehicles": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Vehicle Technology and Autonomous Driving Challenge",
    "title": "Rear-Stitched View Panorama: A Low-Power Embedded Implementation for Smart Rear-View Mirrors on Vehicles",
    "authors": [
      "Janice Pan",
      "Vikram Appia",
      "Jesse Villarreal",
      "Lucas Weaver",
      "Do-Kyoung Kwon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/html/Pan_Rear-Stitched_View_Panorama_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/papers/Pan_Rear-Stitched_View_Panorama_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Automobiles are currently equipped with a three-mirror system for rear-view visualization. The two side-view mirrors show close the periphery on the left and right sides of the vehicle, and the center rear-view mirror is typically adjusted to allow the driver to see through the vehicle's rear windshield. This three-mirror system, however, imposes safety concerns in requiring drivers to shift their attention and gaze to look in each mirror to obtain a full visualization of the rear-view surroundings, which takes attention off the scene in front of the vehicle. We present an alternative to the three-mirror rear-view system, which we call Rear-Stitched View Panorama (RSVP). The proposed system uses four rear-facing cameras, strategically placed to overcome the traditional blind spot problem, and stitches the feeds from each camera together to generate a single panoramic view, which can display the entire rear surroundings. We project individually captured frames onto a single virtual view using precomputed system calibration parameters. Then we determine optimal seam lines, along which the images are fused together to form the single RSVP view presented to the driver. Furthermore, we highlight techniques that enable efficient embedded implementation of the system and showcase a real-time system utilizing under 2W of power, making it suitable for in-cabin deployment in vehicles.\r"
  },
  "cvpr2017_w13_end-to-endegolaneestimationbasedonsequentialtransferlearningforself-drivingcars": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Vehicle Technology and Autonomous Driving Challenge",
    "title": "End-To-End Ego Lane Estimation Based on Sequential Transfer Learning for Self-Driving Cars",
    "authors": [
      "Jiman Kim",
      "Chanjong Park"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/html/Kim_End-To-End_Ego_Lane_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/papers/Kim_End-To-End_Ego_Lane_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Autonomous cars establish driving strategies using the positions of ego lanes. The previous methods detect lane points and select ego lanes with heuristic and complex postprocessing with strong geometric assumptions. We propose a sequential end-to-end transfer learning method to estimate left and right ego lanes directly and separately without any postprocessing. We redefined a point-detection problem as a region-segmentation problem; as a result, the proposed method is insensitive to occlusions and variations of environmental conditions, because it considers the entire content of an input image during training. Also, we constructed an extensive dataset that is suitable for a deep neural network training by collecting a variety of road conditions, annotating ego lanes, and augmenting them systematically. The proposed method demonstrated improved accuracy and stability on input variations compared with a recent method based on deep learning. Our approach does not involve postprocessing, and is therefore flexible to change of target domain.\r"
  },
  "cvpr2017_w13_robusthanddetectionandclassificationinvehiclesandinthewild": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Vehicle Technology and Autonomous Driving Challenge",
    "title": "Robust Hand Detection and Classification in Vehicles and in the Wild",
    "authors": [
      "T. Hoang Ngan Le",
      "Kha Gia Quach",
      "Chenchen Zhu",
      "Chi Nhan Duong",
      "Khoa Luu",
      "Marios Savvides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/html/Le_Robust_Hand_Detection_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/papers/Le_Robust_Hand_Detection_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Robust hand detection and classification is one of the most crucial pre-processing steps to support human computer interaction, driver behaviors monitoring, virtual reality, etc.This problem, however, is very challenging due to numerous variants of hand images in real-world scenarios. This work presents a novel approach named Multiple Scale Region-based Fully Convolutional Networks (MS-RFCN) to robustly detect and classify human hand regions under various challenging conditions, e.g. occlusions, illumination, low-resolutions. In this approach, the whole image is passed through the proposed fully convolutional network to compute score maps. Those score maps with their position-sensitive properties can help to efficiently address a dilemma between translation-invariance in classification and detection. The method is evaluated on the challenging hand databases, i.e. the Vision for Intelligent Vehicles and Applications (VIVA) Challenge, Oxford hand dataset and compared against various recent hand detection methods. The experimental results show that our proposed MS-FRCN approach consistently achieves the state-of-the-art hand detection results, i.e. Average Precision (AP) / Average Recall (AR) of 95.1% / 94.5% at level 1 and 86.0% / 83.4% at level 2, on the VIVA challenge. In addition, the proposed method achieves the state-of-the-art results for left/right hand and driver/passenger classification tasks on the VIVA database with a significant improvement on AP/AR of7% and13% for both classification tasks, respectively. The hand detection performance of MS-RFCN reaches to 75.1% of AP and77.8% of AR on Oxford database.\r"
  },
  "cvpr2017_w13_motionlanguageofstereoimagesequence": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computer Vision in Vehicle Technology and Autonomous Driving Challenge",
    "title": "Motion Language of Stereo Image Sequence",
    "authors": [
      "Tomoya Kato",
      "Hayato Itoh",
      "Atsushi Imiya"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/html/Kato_Motion_Language_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w13/papers/Kato_Motion_Language_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper proposes a method forthe symbolisation of temporal changes of the environmentsaround the autonomous robot in a work spaceusing the scene flow field for recognition of events.The proposed algorithm isa two-phase method for the accurate computation of the scene flow field.The algorithm next symbolises the motion fieldas a string of words using directional statistics.Our method extracts a string of words of motion in front of the robots from images captured by a robot-mounted stereo camera system.We evaluate performance of our method usingKITTI scene flow Dataset 2015.The results shows the efficiency of the method for the extractionof events in front of a robot. \r"
  },
  "cvpr2017_w14_deeplocalvideofeatureforactionrecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Open Domain Action Recognition Challenge",
    "title": "Deep Local Video Feature for Action Recognition",
    "authors": [
      "Zhenzhong Lan",
      "Yi Zhu",
      "Alexander G. Hauptmann",
      "Shawn Newsam"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/html/Lan_Deep_Local_Video_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/papers/Lan_Deep_Local_Video_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We investigate the problem of representing an entire video using CNN features for human action recognition. End-to-end learning of CNN/RNNs is currently not possible for whole videos due to GPU memory limitations and so a common practice is to use sampled frames as inputs along with the video labels as supervision. However, the global video labels might not be suitable for all of the temporally local samples as the videos often contain content besides the action of interest. We therefore propose to instead treat the deep networks trained on local inputs as local feature extractors. The local features are then aggregated to form global features which are used to assign video-level labels through a second classification stage. We investigate a number of design choices for this local feature approach. Experimental results on the HMDB51 and UCF101 datasets show that a simple maximum pooling on the sparsely sampled local features leads to significant performance improvement.\r"
  },
  "cvpr2017_w14_videoactionrecognitionbasedondeeperconvolutionnetworkswithpair-wiseframemotionconcatenation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Open Domain Action Recognition Challenge",
    "title": "Video Action Recognition Based on Deeper Convolution Networks With Pair-Wise Frame Motion Concatenation",
    "authors": [
      "Yamin Han",
      "Peng Zhang",
      "Tao Zhuo",
      "Wei Huang",
      "Yanning Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/html/Han_Video_Action_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/papers/Han_Video_Action_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Deep convolution networks have shown a remarkable performance in different recognition tasks. Howerver, in reality, recognition is hard especially for the videos. Challenges such as cluttered backgrounds etc. generate the problem like large inter and intra class variations. In addition, the problem of data deficiency could also make the designed model degrade during learning and update. To overcome those limitations, in this work, we proposed a deeper convolution networks based approach with pair-wise motion concatenation, which is named deep temporal convolutional networks. A temporal motion accumulation mechanism has been introduced as an effective data entry for learning of convolution networks. To handle the possible data deficiency, beneficial practices of transferring ResNet-101 weights and data variation augmentation are utilized for robust recognition. Experiments on UCF101 and ODAR dataset have verified a preferable performance when compared with state-of-art works.\r"
  },
  "cvpr2017_w14_hand-objectinteractiondetectionwithfullyconvolutionalnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Open Domain Action Recognition Challenge",
    "title": "Hand-Object Interaction Detection With Fully Convolutional Networks",
    "authors": [
      "Matthias Schroder",
      "Helge Ritter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/html/Schroder_Hand-Object_Interaction_Detection_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/papers/Schroder_Hand-Object_Interaction_Detection_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Detecting hand-object interactions is a challenging problem with many applications in the human-computer interaction domain. We present a real-time method that automatically detects hand-object interactions in RGBD sensor data and tracks the object's rigid pose over time. The detection is performed using a fully convolutional neural network, which is purposefully trained to discern the relationship between hands and objects and which predicts pixel-wise class probabilities. This output is used in a probabilistic pixel labeling strategy that explicitly accounts for the uncertainty of the prediction. Based on the labeling of object pixels, the object is tracked over time using model-based registration. We evaluate the accuracy and generalizability of our approach and make our annotated RGBD dataset as well as our trained models publicly available.\r"
  },
  "cvpr2017_w14_objectstaterecognitionforautomaticar-basedmaintenanceguidance": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Open Domain Action Recognition Challenge",
    "title": "Object State Recognition for Automatic AR-Based Maintenance Guidance",
    "authors": [
      "Pavel Dvorak",
      "Radovan Josth",
      "Elisabetta Delponte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/html/Dvorak_Object_State_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/papers/Dvorak_Object_State_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper describes a component of an Augmented Reality (AR) based system focused on supporting workers in manufacturing and maintenance industry. Particularly, it describes a component responsible for verification of performed steps. Correct handling is crucial in both manufacturing and maintenance industries and deviations may cause problems in later stages of the production and assembly. The primary aim of such support systems is making the training of new employees faster and more efficient and reducing the error rate. We present a method for automatically recognizing an object's state with the objective of verifying a set of tasks performed by a user. The novelty of our approach is that the system can automatically recognize the state of the object and provide immediate feedback to the operator using an AR visualization enabling fully automatic step-by-step instructions.\r"
  },
  "cvpr2017_w14_whenkernelmethodsmeetfeaturelearninglog-covariancenetworkforactionrecognitionfromskeletaldata": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Open Domain Action Recognition Challenge",
    "title": "When Kernel Methods Meet Feature Learning: Log-Covariance Network for Action Recognition From Skeletal Data",
    "authors": [
      "Jacopo Cavazza",
      "Pietro Morerio",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/html/Cavazza_When_Kernel_Methods_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/papers/Cavazza_When_Kernel_Methods_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Human action recognition from skeletal data is a hot research topic and important in many open domain applications of computer vision, thanks to recently introduced 3D sensors. In the literature, naive methods simply transfer off-the-shelf techniques from video to the skeletal representation. However, the current state-of-the-art is contended between to different paradigms: kernel-based methods and feature learning with (recurrent) neural networks. Both approaches show strong performances, yet they exhibit heavy, but complementary, drawbacks. Motivated by this fact, our work aims at combining together the best of the two paradigms, by proposing an approach where a shallow network is fed with a covariance representation. Our intuition is that, as long as the dynamics is effectively modeled, there is no need for the classification network to be deep nor recurrent in order to score favorably. We validate this hypothesis in a broad experimental analysis over 6 publicly available datasets.\r"
  },
  "cvpr2017_w14_fastsimplex-hmmforone-shotlearningactivityrecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Open Domain Action Recognition Challenge",
    "title": "Fast Simplex-HMM for One-Shot Learning Activity Recognition",
    "authors": [
      "Mario Rodriguez",
      "Carlos Orrite",
      "Carlos Medrano",
      "Dimitrios Makris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/html/Rodriguez_Fast_Simplex-HMM_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w14/papers/Rodriguez_Fast_Simplex-HMM_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The work presented in this paper deals with the challenging task of learning an activity class representation using a single sequence for training. Recently, Simplex-HMM framework has been shown to be an efficient representation for activity classes, however, it presents high computational costs making it impractical in several situations. A dimensionality reduction of the features spaces based on a Maximum at Posteriori adaptation combined with a fast estimation of the optimal parameters in the Expectation Maximization algorithm are presented in this paper. As confirmed by the experimental results, these two modifications not only reduce the computational cost but also maintain the performance or even improve it. The process suitability is experimentally confirmed using the human activity datasets Weizmann, KTH and IXMAS and the gesture dataset ChaLearn.\r"
  },
  "cvpr2017_w15_intelrealsensestereoscopicdepthcameras": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computational Cameras and Displays",
    "title": "Intel RealSense Stereoscopic Depth Cameras",
    "authors": [
      "Leonid Keselman",
      "John Iselin Woodfill",
      "Anders Grunnet-Jepsen",
      "Achintya Bhowmik"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/html/Keselman_Intel_RealSense_Stereoscopic_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/papers/Keselman_Intel_RealSense_Stereoscopic_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a comprehensive overview of the stereoscopic Intel RealSense RGBD imaging systems. We discuss these systems' mode-of-operation, functional behavior and include models of their expected performance, shortcomings, and limitations. We provide information about the systems' optical characteristics, their correlation algorithms, and how these properties can affect different applications, including 3D reconstruction and gesture recognition. Our discussion covers the Intel RealSense R200 and RS400. \r"
  },
  "cvpr2017_w15_compressivelightfieldreconstructionsusingdeeplearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computational Cameras and Displays",
    "title": "Compressive Light Field Reconstructions Using Deep Learning",
    "authors": [
      "Mayank Gupta",
      "Arjun Jauhari",
      "Kuldeep Kulkarni",
      "Suren Jayasuriya",
      "Alyosha Molnar",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/html/Gupta_Compressive_Light_Field_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/papers/Gupta_Compressive_Light_Field_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Single-shot light field cameras typically sacrifice spatial resolution to sample angular viewpoints, multiplexing rays onto a 2D sensor array. Using compressive sensing to recover this resolution requires long processing times for iterative solvers. We present a new, two branch neural network architecture to efficiently recover a high resolution 4D light field from a single coded 2D image. This network achieves average PSNR values of 26-32 dB on simulated data, outperforming the baseline of dictionary-based learning. In addition, reconstruction time is decreased from 35 minutes to 6.7 minutes compared to the dictionary method for equivalent visual quality at small sampling/compression ratios as low as 8%. We test our network reconstructions on synthetic light fields, simulated coded measurements of real Lytro Illum light fields, and real coded data from a custom CMOS diffractive light field camera to show the potential for real-time light field video systems in the future.\r"
  },
  "cvpr2017_w15_generating5dlightfieldsinscatteringmediaforrepresenting3dimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computational Cameras and Displays",
    "title": "Generating 5D Light Fields in Scattering Media for Representing 3D Images",
    "authors": [
      "Eri Yuasa",
      "Fumihiko Sakaue",
      "Jun Sato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/html/Yuasa_Generating_5D_Light_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/papers/Yuasa_Generating_5D_Light_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we propose a novel method for displaying 3D images based on a 5D light field representation. In our method, the light fields emitted by a light field projector are projected into 3D scattering media such as fog. The intensity of light lays projected into the scattering media decreases because of the scattering effect of the media. As a result, 5D light fields are generated in the scattering media. The proposed method models the relationship between the 5D light fields and observed images, and uses the relationship for projecting light fields so that the observed image changes according to the viewpoint of observers. In order to achieve accurate and efficient 3D image representation, we describe the relationship not by using a parametric model, but by using an observation based model obtained from a point spread function (PSF) of scattering media. The experimental results show the efficiency of the proposed method.\r"
  },
  "cvpr2017_w15_thestereoscopiczoom": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Computational Cameras and Displays",
    "title": "The Stereoscopic Zoom",
    "authors": [
      "Sergi Pujades",
      "Frederic Devernay",
      "Laurent Boiron",
      "Remi Ronfard"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/html/Pujades_The_Stereoscopic_Zoom_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w15/papers/Pujades_The_Stereoscopic_Zoom_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We study camera models to generate stereoscopic zoom shots, i.e. using very long focal length lenses. Stereoscopic images are usually generated with two cameras. However, we show that two cameras are unable to create compelling stereoscopic images for extreme focal length lenses. Inspired by the practitioners' use of the long focal length lenses we propose two different configurations: we \"get closer\" to the scene, or we create \"perspective deformations\". Both configurations are build upon state-of-the-art image-based rendering methods allowing the formal deduction of precise parameters of the cameras depending on the scene to be acquired. We present a proof of concept with the acquisition of a representative simplified scene. We discuss the advantages and drawbacks of each configuration.\r"
  },
  "cvpr2017_w16_deceivinggooglescloudvideointelligenceapibuiltforsummarizingvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Deceiving Google's Cloud Video Intelligence API Built for Summarizing Videos",
    "authors": [
      "Hossein Hosseini",
      "Baicen Xiao",
      "Radha Poovendran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Poovendran_Deceiving_Googles_Cloud_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Poovendran_Deceiving_Googles_Cloud_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Despite the rapid progress of the techniques for image classification, video annotation has remained a challenging task. Automated video annotation would be a breakthrough technology, enabling users to search within the videos. Recently, Google introduced the Cloud Video Intelligence API for video analysis. As per the website, the system can be used to \"separate signal from noise, by retrieving relevant information at the video, shot or per frame\" level. A demonstration website has been also launched, which allows anyone to select a video for annotation. The API then detects the video labels (objects within the video) as well as shot labels (description of the video events over time). In this paper, we examine the usability of the Google's Cloud Video Intelligence API in adversarial environments. In particular, we investigate whether an adversary can subtly manipulate a video in such a way that the API will return only the adversary-desired labels. For this, we select an image, which is different from the video content, and insert it, periodically and at a very low rate, into the video. We found that if we insert one image every two seconds, the API is deceived into annotating the video as if it only contained the inserted image. Note that the modification to the video is hardly noticeable as, for instance, for a typical frame rate of 25, we insert only one image per 50 video frames. We also found that, by inserting one image per second, all the shot labels returned by the API are related to the inserted image. We perform the experiments on the sample videos provided by the API demonstration website and show that our attack is successful with different videos and images.\r"
  },
  "cvpr2017_w16_simpleblack-boxadversarialattacksondeepneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Simple Black-Box Adversarial Attacks on Deep Neural Networks",
    "authors": [
      "Nina Narodytska",
      "Shiva Kasiviswanathan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Kasiviswanathan_Simple_Black-Box_Adversarial_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Kasiviswanathan_Simple_Black-Box_Adversarial_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real world. In this work, we focus on deep convolutional neural networks and demonstrate that adversaries can easily craft adversarial examples even without any internal knowledge of the target network. Our attacks treat the network as an oracle (black-box) and only assume that the output of the network can be observed on the probed inputs. Our attacks utilize a novel local-search based technique to construct numerical approximation to the network gradient, which is then carefully used to construct a small set of pixels in an image to perturb. We demonstrate how this underlying idea can be adapted to achieve several strong notions of misclassification. The simplicity and effectiveness of our proposed schemes mean that they could serve as a litmus test for designing robust networks.\r"
  },
  "cvpr2017_w16_iknowthatpersongenerativefullbodyandfacede-identificationofpeopleinimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "I Know That Person: Generative Full Body and Face De-Identification of People in Images",
    "authors": [
      "Karla Brkic",
      "Ivan Sikiric",
      "Tomislav Hrkac",
      "Zoran Kalafatic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Kalafatic_I_Know_That_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Kalafatic_I_Know_That_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a model for full body and face de-identification of humans in images. Given a segmentation of the human figure, our model generates a synthetic human image with an alternative appearance that looks natural and fits the segmentation outline. The model is usable with various levels of segmentation, from simple human figure blobs to complex garment-level segmentations. The level of detail in the de-identified output depends on the level of detail in the input segmentation. The model de-identifies not only primary biometric identifiers (e.g. the face), but also soft and non-biometric identifiers including clothing, hairstyle, etc. Quantitative and perceptual experiments indicate that our model produces de-identified outputs that thwart human and machine recognition, while preserving data utility and naturalness. \r"
  },
  "cvpr2017_w16_protectingvisualsecretsusingadversarialnets": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Protecting Visual Secrets Using Adversarial Nets",
    "authors": [
      "Nisarg Raval",
      "Ashwin Machanavajjhala",
      "Landon P. Cox"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Cox_Protecting_Visual_Secrets_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Cox_Protecting_Visual_Secrets_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Protecting visual secrets is an important problem due to the prevalence of cameras that continuously monitor our surroundings. Any viable solution to this problem should also minimize the impact on the utility of applications that use images. In this work, we build on the existing work of adversarial learning to design a perturbation mechanism that jointly optimizes privacy and utility objectives. We provide a feasibility study of the proposed mechanism and present ideas on developing a privacy framework based on the adversarial perturbation mechanism.\r"
  },
  "cvpr2017_w16_cartooningforenhancedprivacyinlifeloggingandstreamingvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Cartooning for Enhanced Privacy in Lifelogging and Streaming Videos",
    "authors": [
      "Eman T. Hassan",
      "Rakibul Hasan",
      "Patrick Shaffer",
      "David Crandall",
      "Apu Kapadia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Kapadia_Cartooning_for_Enhanced_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Kapadia_Cartooning_for_Enhanced_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We describe an object replacement approach whereby privacy-sensitive objects in videos are replaced by abstract cartoons taken from clip art. Our approach uses a combination of computer vision, deep learning, and image processing techniques to detect objects, abstract details, and replace them with cartoon clip art.We conducted a user study (N=85) to discern the utility and effectiveness of our cartoon replacement technique. The results suggest that our object replacement approach preserves a video's semantic content while improving its privacy by obscuring details of objects.\r"
  },
  "cvpr2017_w16_aseppirobustprivacyprotectionagainstde-anonymizationattacks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "ASePPI: Robust Privacy Protection Against De-Anonymization Attacks",
    "authors": [
      "Natacha Ruchaud",
      "Jean-Luc Dugelay"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Dugelay_ASePPI_Robust_Privacy_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Dugelay_ASePPI_Robust_Privacy_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The evolution of the video surveillance systems generates questions concerning protection of individual privacy. In this paper, we design ASePPI, an Adaptive Scrambling enabling Privacy Protection and Intelligibility method operating in the H.264/AVC stream with the aim to be robust against de-anonymization attacks targeting the restoration of the original image and the re-identification of people. The proposed approach automatically adapts the level of protection according to the resolution of the region of interest. Compared to existing methods, our framework provides a better trade-off between the privacy protection and the visibility of the scene with robustness against de-anonymization attacks. Moreover, the impact on the source coding stream is negligible.\r"
  },
  "cvpr2017_w16_trustingthecomputerincomputervisionaprivacy-affirmingframework": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Trusting the Computer in Computer Vision: A Privacy-Affirming Framework",
    "authors": [
      "Andrew Tzer-Yeu Chen",
      "Morteza Biglari-Abhari",
      "Kevin I-Kai Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Wang_Trusting_the_Computer_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Wang_Trusting_the_Computer_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The use of surveillance cameras continues to increase, ranging from conventional applications such as law enforcement to newer scenarios with looser requirements such as gathering business intelligence. Humans still play an integral part in using and interpreting the footage from these systems, but are also a significant factor in causing unintentional privacy breaches. As computer vision methods continue to improve, we argue in this position paper that system designers should reconsider the role of machines in surveillance, and how automation can be used to help protect privacy. We explore this by discussing the impact of the human-in-the-loop, the potential for using abstraction and distributed computing to further privacy goals, and an approach for determining when video footage should be hidden from human users. We propose that in an ideal surveillance scenario, a privacy-affirming framework causes collected camera footage to be processed by computers directly, and never shown to humans. This implicitly requires humans to establish trust, to believe that computer vision systems can generate sufficiently accurate results without human supervision, so that if information about people must be gathered, unintentional data collection is mitigated as much as possible.\r"
  },
  "cvpr2017_w16_designingamoralcompassforthefutureofcomputervisionusingspeculativeanalysis": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Designing a Moral Compass for the Future of Computer Vision Using Speculative Analysis",
    "authors": [
      "Michael Skirpan",
      "Tom Yeh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Yeh_Designing_a_Moral_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Yeh_Designing_a_Moral_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper we discuss and analyze possible futures for technologies in the field of computer vision (CV). Using a method we have coined speculative analysis we take a broad look at research trends in the field to categorize risks, analyze which ones are most threatening and likely, and ultimately summarize conclusions for how the field may attempt to stem future harms caused by CV technologies. We develop narrative case studies to provoke dialogue and deeply explore possible risk scenarios we found to be most probable and severe. We arrive at the position that there are serious potentials for CV to cause discriminatory harm and exacerbate cybersecurity issues.\r"
  },
  "cvpr2017_w16_teachingcomputervisionanditssocietaleffectsalookatprivacyandsecurityissuesfromthestudentsperspective": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Teaching Computer Vision and Its Societal Effects: A Look at Privacy and Security Issues From the Students' Perspective",
    "authors": [
      "Melissa Cote",
      "Alexandra Branzan Albu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Albu_Teaching_Computer_Vision_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Albu_Teaching_Computer_Vision_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we look at the societal effects of computer vision technologies from the perspective of the future minds in computer vision: senior year engineering students. Engineering education has traditionally focused on technical skills and knowledge. Nowadays, the need for educating engineers in socio-technical skills and reflective thinking, especially on the bright and dark sides of the technology they develop, is being recognized. We advocate for the integration of social awareness modules into computer vision courses so that the societal effects of technology can be studied together with the technology itself, as opposed to the often more generic 'impact of technology on society' courses. Such modules provide a venue for students to reflect on the real-world consequences of technology in concrete, practical contexts. In this paper, we present qualitative results of an observational study analyzing essays of senior year engineering students, who wrote about societal impacts of computer vision technologies of their choice. Privacy and security issues ranked as the top impact topics discussed by students among 50 topics. Similar social awareness modules would apply well to other advanced technical courses of the engineering curriculum where privacy and security are a major concern, such as big data courses. We believe that such modules are highly likely to enhance the reflective abilities of engineering graduates regarding societal impacts of novel technologies.\r"
  },
  "cvpr2017_w16_assistingusersinaworldfullofcamerasaprivacy-awareinfrastructureforcomputervisionapplications": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Assisting Users in a World Full of Cameras: A Privacy-Aware Infrastructure for Computer Vision Applications",
    "authors": [
      "Anupam Das",
      "Martin Degeling",
      "Xiaoyou Wang",
      "Junjue Wang",
      "Norman Sadeh",
      "Mahadev Satyanarayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Satyanarayanan_Assisting_Users_in_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Satyanarayanan_Assisting_Users_in_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Computer vision based technologies have seen widespread adoption over the recent years. This use is not limited to the rapid adoption of facial recognition technology but extends to facial expression recognition, scene recognition and more. These developments raise privacy concerns and call for novel solutions to ensure adequate user awareness, and ideally, control over the resulting collection and use of potentially sensitive data. While cameras have become ubiquitous, most of the time users are not even aware of their presence. In this paper we introduce a novel distributed privacy infrastructure for the Internet-of-Things and discuss in particular how it can help enhance user's awareness of and control over the collection and use of video data about them. The infrastructure, which has undergone early deployment and evaluation on two campuses, supports the automated discovery of IoT resources and the selective notification of users. This includes the presence of computer vision applications that collect data about users. In particular, we describe an implementation of functionality that helps users discover nearby cameras and choose whether or not they want their faces to be denatured in the video streams.\r"
  },
  "cvpr2017_w16_caughtred-handedtowardpracticalvideo-basedsubsequencesmatchinginthepresenceofreal-worldtransformations": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Caught Red-Handed: Toward Practical Video-Based Subsequences Matching in the Presence of Real-World Transformations",
    "authors": [
      "Yi Xu",
      "True Price",
      "Fabian Monrose",
      "Jan-Michael Frahm"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Frahm_Caught_Red-Handed_Toward_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Frahm_Caught_Red-Handed_Toward_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Every minute, staggering amounts of user-generated videos are uploaded to on-line social networks. These videos can generate significant advertising revenue, providing strong incentive for unscrupulous individuals that wish to capitalize on this bonanza by pirating short clips from popular content and altering the copied media in ways that might bypass detection. Unfortunately, while the challenges posed by the use of skillful transformations has been known for quite some time, current state-of-the-art methods still suffer from severe limitations. Indeed, most of today's techniques perform poorly in the face of real world copies. To address this, we propose a novel approach that leverages temporal characteristics to identify subsequences of a video that were copied from elsewhere. Our approach takes advantage of a new temporal feature to index a reference library in a manner that is robust to popular spatial and temporal transformations in pirated videos. Our experimental evaluation on 27 hours of video obtained from social networks demonstrates that our technique significantly outperforms the existing state-of-the-art approaches with respect to accuracy, resilience, and efficiency.\r"
  },
  "cvpr2017_w16_informationhidinginrgbimagesusinganimprovedmatrixpatternapproach": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Information Hiding in RGB Images Using an Improved Matrix Pattern Approach",
    "authors": [
      "Amirfarhad Nilizadeh",
      "Wojciech Mazurczyk",
      "Cliff Zou",
      "Gary T. Leavens"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/html/Leavens_Information_Hiding_in_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w16/papers/Leavens_Information_Hiding_in_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, a novel steganography algorithm based on an improved \"Matrix Pattern\" (MP) method is presented. In this process, firstly, an RGB image is divided into the non-overlapping square-sized blocks. Next, 95 dynamic-sized unique matrix patterns are automatically generated using the 4th and 5th bit layers of the green layer of each block, which are assigned to 95 English keyboard characters. Then, the blue layer of each block is used for embedding secret messages by adding matrix patterns which are assigned to the characters of the secret message. The results show that this algorithm has a high resistance against steganalysis attacks, including Regular Singular (RS), Sample Pair (SP), and PVD based attacks. Furthermore, the proposed algorithm not only improves capacity by over 27% when compared to the existing method, but also results in a slightly better transparency of the stego-image.\r"
  },
  "cvpr2017_w17_track-clusteringerrorevaluationfortrack-basedmulti-cameratrackingsystememployinghumanre-identification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "Track-Clustering Error Evaluation for Track-Based Multi-Camera Tracking System Employing Human Re-Identification",
    "authors": [
      "Chih-Wei Wu",
      "Meng-Ting Zhong",
      "Yu Tsao",
      "Shao-Wen Yang",
      "Yen-Kuang Chen",
      "Shao-Yi Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Wu_Track-Clustering_Error_Evaluation_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Wu_Track-Clustering_Error_Evaluation_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this study, we present a set of new evaluation measures for the track-based multi-camera tracking (T-MCT) task leveraging the clustering measurements. We demonstrate that the proposed evaluation measures provide notable advantages over previous ones. Moreover, a distributed and online T-MCT framework is proposed, where re-identification (Re-id) is embedded in T-MCT, to confirm the validity of the proposed evaluation measures. Experimental results reveal that with the proposed evaluation measures, the performance of T-MCT can be accurately measured, which is highly correlated to the performance of Re-id. Furthermore, it is also noted that our T-MCT framework achieves competitive score on the DukeMTMC dataset when compared to the previous work that used global optimization algorithms. Both the evaluation measures and the inter-camera tracking framework are proven to be the stepping stone for multi-camera tracking.\r"
  },
  "cvpr2017_w17_dukemtmc4reidalarge-scalemulti-camerapersonre-identificationdataset": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "DukeMTMC4ReID: A Large-Scale Multi-Camera Person Re-Identification Dataset",
    "authors": [
      "Mengran Gou",
      "Srikrishna Karanam",
      "Wenqian Liu",
      "Octavia Camps",
      "Richard J. Radke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Gou_DukeMTMC4ReID_A_Large-Scale_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Gou_DukeMTMC4ReID_A_Large-Scale_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In the past decade, research in person re-identification (re-id) has exploded due to its broad use in security and surveillance applications. Issues such as inter-camera viewpoint, illumination and pose variations make it an extremely difficult problem. Consequently, many algorithms have been proposed to tackle these issues. To validate the efficacy of re-id algorithms, numerous benchmarking datasets have been constructed. While early datasets contained relatively few identities and images, several large-scale datasets have recently been proposed, motivated by data-driven machine learning. In this paper, we introduce a new large-scale real-world re-id dataset, DukeMTMC4ReID, using 8 disjoint surveillance camera views covering parts of the Duke University campus. The dataset was created from the recently proposed fully annotated multi-target multi-camera tracking dataset DukeMTMC. A benchmark summarizing extensive experiments with many combinations of existing re-id algorithms on this dataset is also provided for an up-to-date performance analysis. \r"
  },
  "cvpr2017_w17_personre-identificationbydeeplearningattribute-complementaryinformation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "Person Re-Identification by Deep Learning Attribute-Complementary Information",
    "authors": [
      "Arne Schumann",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Schumann_Person_Re-Identification_by_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Schumann_Person_Re-Identification_by_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": "Automatic person re-identification (re-id) across camera boundaries is a challenging problem. Approaches have to be robust against many factors which influence the visual appearance of a person but are not relevant to the person's identity. Examples for such factors are pose, camera angles, and lighting conditions. Person attributes are a semantic high level information which is invariant across many such influences and contain information which is often highly relevant to a person's identity. In this work we develop a re-id approach which leverages the information contained in automatically detected attributes. We train an attribute classifier on separate data and include its responses into the training process of our person re-id model which is based on convolutional neural networks (CNNs). This allows us to learn a person representation which contains information complementary to that contained within the attributes. Our approach is able to identify attributes which perform most reliably for re-id and focus on them accordingly. We demonstrate the performance improvement gained through use of the attribute information on multiple large-scale datasets and report insights into which attributes are most relevant for person re-id.\r"
  },
  "cvpr2017_w17_towardsaprincipledintegrationofmulti-camerare-identificationandtrackingthroughoptimalbayesfilters": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "Towards a Principled Integration of Multi-Camera Re-Identification and Tracking Through Optimal Bayes Filters",
    "authors": [
      "Lucas Beyer",
      "Stefan Breuers",
      "Vitaly Kurin",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Beyer_Towards_a_Principled_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Beyer_Towards_a_Principled_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " With the rise of end-to-end learning through deep learning, person detectors and re-identification (ReID) models have recently become very strong. Multi-camera multi-target (MCMT) tracking has not fully gone through this transformation yet. We intend to take another step in this direction by presenting a theoretically principled way of integrating ReID with tracking formulated as an optimal Bayes filter. This conveniently side-steps the need for data-association and opens up a direct path from full images to the core of the tracker. While the results are still sub-par, we believe that this new, tight integration opens many interesting research opportunities and leads the way towards full end-to-end tracking from raw pixels.\r"
  },
  "cvpr2017_w17_video-basedpersonre-identificationbydeepfeatureguidedpooling": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "Video-Based Person Re-Identification by Deep Feature Guided Pooling",
    "authors": [
      "Youjiao Li",
      "Li Zhuo",
      "Jiafeng Li",
      "Jing Zhang",
      "Xi Liang",
      "Qi Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Li_Video-Based_Person_Re-Identification_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Li_Video-Based_Person_Re-Identification_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Person re-identification (re-id) aims to match a specific person across non-overlapping views of different cameras, which is currently one of the hot topics in computer vision. Compared with image-based person re-id, video-based techniques could achieve better performance by fully utilizing the space-time information. This paper presents a novel video-based person re-id method named Deep Feature Guided Pooling (DFGP), which can take full advantage of the space-time information. The contributions of the method are in the following aspects: (1) PCA-based convolutional network (PCN), a lightweight deep learning network, is trained to generate deep features of video frames. Deep features are aggregated by average pooling to obtain person deep feature vectors. The vectors are utilized to guide the generation of human appearance features, which makes the appearance features robust to the severe noise in videos. (2) Hand-crafted local features of videos are aggregated by max pooling to reinforce the motion variations of different persons. In this way, the human descriptors are more discriminative. (3) The final human descriptors are composed of deep features and hand-crafted local features to take their own advantages and the performance of identification is promoted. Experimental results show that our approach outperforms six other state-of-the-art video-based methods on the challenging PRID 2011 and iLIDS-VID video-based person re-id datasets.\r"
  },
  "cvpr2017_w17_adatasetforpersistentmulti-targetmulti-cameratrackinginrgb-d": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "A Dataset for Persistent Multi-Target Multi-Camera Tracking in RGB-D",
    "authors": [
      "Ryan Layne",
      "Sion Hannuna",
      "Massimo Camplani",
      "Jake Hall",
      "Timothy M. Hospedales",
      "Tao Xiang",
      "Majid Mirmehdi",
      "Dima Damen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Layne_A_Dataset_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Layne_A_Dataset_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Video surveillance systems are now widely deployed to improve our lives by enhancing safety, security, health monitoring and business intelligence. This has motivated extensive research into automated video analysis. Nevertheless, there is a gap between the focus of contemporary research, and the needs of end users of video surveillance systems. Many existing benchmarks and methodologies focus on narrowly defined problems in detection, tracking, re-identification or recognition. In contrast, end users face higher-level problems such as long-term monitoring of identities in order to build a picture of a person's activity across the course of a day, producing usage statistics of a particular area of space, and that these capabilities should be robust to challenges such as change of clothing. To achieve this effectively requires less widely studied capabilities such as spatio-temporal reasoning about people identities and locations within a space partially observed by multiple cameras over an extended time period. To bridge this gap between research and required capabilities, we propose a new dataset LIMA that encompasses the challenges of monitoring a typical home / office environment. LIMA contains 4.5 hours of RGB-D video from three cameras monitoring a four room house. To reflect the challenges of a realistic practical application, the dataset includes clothes changes and visitors to ensure the global reasoning is a realistic open-set problem. In addition to raw data, we provide identity annotation for benchmarking, and tracking results from a contemporary RGB-D tracker - thus allowing focus on the higher level monitoring problems.\r"
  },
  "cvpr2017_w17_trajectoryensemblemultiplepersonsconsensustrackingacrossnon-overlappingmultiplecamerasoverrandomlydroppedcameranetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "Trajectory Ensemble: Multiple Persons Consensus Tracking Across Non-Overlapping Multiple Cameras Over Randomly Dropped Camera Networks",
    "authors": [
      "Yasutomo Kawanishi",
      "Daisuke Deguchi",
      "Ichiro Ide",
      "Hiroshi Murase"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Kawanishi_Trajectory_Ensemble_Multiple_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Kawanishi_Trajectory_Ensemble_Multiple_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Multiple person tracking over a camera network is usually performed by matching person images between adjacent cameras. It easily fails by a temporal appearance change of the persons caused by environmental illumination and observation orientation of a camera. To solve this problem, matching person images across not only adjacent cameras but also cameras multiple hops away in the camera network is effective, however, such relaxation of spatio-temporal cues also cause tracking failure due to the increase of matching candidates. To avoid the failure, we introduce \"Random Camera Drop\" to generate different camera networks which relax the spatio-temporal cues partially and randomly. And we integrate tracking results over the networks to a consensus tracking result by a novel concept \"Trajectory Ensemble\", an extension of unsupervised ensemble learning for the multiple person tracking over a camera network problem. We evaluated the framework on several virtual datasets generated from a public dataset, \"Shinpuhkan 2014 dataset\" and confirmed that the proposed method achieve the highest tracking results among some comparative methods.\r"
  },
  "cvpr2017_w17_deepspatial-temporalfusionnetworkforvideo-basedpersonre-identification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Target Re-Identification and Multi-Target Multi-Camera Tracking",
    "title": "Deep Spatial-Temporal Fusion Network for Video-Based Person Re-Identification",
    "authors": [
      "Lin Chen",
      "Hua Yang",
      "Ji Zhu",
      "Qin Zhou",
      "Shuang Wu",
      "Zhiyong Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/html/Chen_Deep_Spatial-Temporal_Fusion_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w17/papers/Chen_Deep_Spatial-Temporal_Fusion_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we propose a novel deep end-to-end network to automatically learn the spatial-temporal fusion features for video-based person re-identification. Specifically, the proposed network consists of CNN and RNN to jointly learn both the spatial and the temporal features of input image sequences. The network is optimized by utilizing the siamese and softmax losses simultaneously to pull the instances of the same person closer and push the instances of different persons apart. Our network is trained on full-body and part-body image sequences respectively to learn complementary representations from holistic and local perspectives. By combining them together, we obtain more discriminative features that are beneficial to person re-identification. Experiments conducted on the PRID-2011, i-LIDS-VIS and MARS datasets show that the proposed method performs favorably against existing approaches. \r"
  },
  "cvpr2017_w18_robocodestowardsgenerativestreetaddressesfromsatelliteimagery": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Robocodes: Towards Generative Street Addresses From Satellite Imagery",
    "authors": [
      "Ilke Demir",
      "Forest Hughes",
      "Aman Raj",
      "Kleovoulos Tsourides",
      "Divyaa Ravichandran",
      "Suryanarayana Murthy",
      "Kaunil Dhruv",
      "Sanyam Garg",
      "Jatin Malhotra",
      "Barrett Doo",
      "Grace Kermani",
      "Ramesh Raskar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Demir_Robocodes_Towards_Generative_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Demir_Robocodes_Towards_Generative_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We describe our automatic generative algorithm to create street addresses (Robocodes) from satellite images by learning and labeling regions, roads, and blocks. 75% of the world lacks street addresses. According to the United Nations, this means 4 billion people are 'invisible'. Recent initiatives tend to name unknown areas by geocoding, which uses latitude and longitude information. Nevertheless settlements abut roads and such addressing schemes are not coherent with the road topology. Instead, our algorithm starts with extracting roads and junctions from satellite imagery utilizing deep learning. Then, it uniquely labels the regions, roads, and houses using some graph- and proximity-based algorithms. We present our results on both cities in mapped areas and in developing countries. We also compare productivity based on current ad-hoc and new complete addresses. We conclude with contrasting our generative addresses to current industrial and open solutions.\r"
  },
  "cvpr2017_w18_temporalvegetationmodellingusinglongshort-termmemorynetworksforcropidentificationfrommedium-resolutionmulti-spectralsatelliteimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Temporal Vegetation Modelling Using Long Short-Term Memory Networks for Crop Identification From Medium-Resolution Multi-Spectral Satellite Images",
    "authors": [
      "Marc Russwurm",
      "Marco Korner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Russwurm_Temporal_Vegetation_Modelling_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Russwurm_Temporal_Vegetation_Modelling_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Land-cover classification is one of the key problems in earth observation and extensively investigated over the recent decades. Usually, approaches concentrate on single-time and multi- or hyperspectral reflectance space- or airborne sensor measurements observed. However, land-cover classes, e.g., crops, change their reflective characteristics over time complicating classification at one observation time. Contrary, these features change in a systematic and predictive manner, which can be utilized in a multi-temporal approach. We use long short-term memory (LSTM) networks to extract temporal characteristics from a sequence of Sentinel-2 observations. We compare the performance of LSTM and other network architectures and a SVM baseline to show the effectiveness of dynamic temporal feature extraction. A large test area combined with rich ground truth labels was used for training and evaluation. Our LSTM variant achieves state-of-the art performance opening potential for further research.\r"
  },
  "cvpr2017_w18_super-resolutionofmultispectralmultiresolutionimagesfromasinglesensor": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Super-Resolution of Multispectral Multiresolution Images From a Single Sensor",
    "authors": [
      "Charis Lanaras",
      "Jose Bioucas-Dias",
      "Emmanuel Baltsavias",
      "Konrad Schindler"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Lanaras_Super-Resolution_of_Multispectral_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Lanaras_Super-Resolution_of_Multispectral_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Some remote sensing sensors, acquire multispectral images of different spatial resolutions in variable spectral ranges (e.g. Sentinel-2, MODIS). The aim of this research is to infer all the spectral bands, of multiresolution sensors, in the highest available resolution of the sensor. We formulate this problem as a minimisation of a convex objective function with an adaptive (edge-reserving) regulariser. The data-fitting term accounts for individual blur and downsampling per band, while the regulariser \"learns\" the discontinuities from the higher resolution bands and transfers them to other bands. We also observed that the data can be represented in a lower-dimensional subspace, reducing the dimensionality of the problem and significantly improving its conditioning. In a series of experiments with simulated data, we obtain results that outperform state-of-the-art, while showing competitive qualitative results on real Sentinel-2 data.\r"
  },
  "cvpr2017_w18_ontheroleofrepresentationsforreasoninginlarge-scaleurbanscenes": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "On the Role of Representations for Reasoning in Large-Scale Urban Scenes",
    "authors": [
      "Randi Cabezas",
      "Maros Blaha",
      "Sue Zheng",
      "Guy Rosman",
      "Konrad Schindler",
      "John W. Fisher III"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Cabezas_On_the_Role_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Cabezas_On_the_Role_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The advent of widely available photo collections covering broad geographic areas has spurred significant advances in large-scale urban scene modeling. While much emphasis has been placed on reconstruction and visualization, the utility of such models extends well beyond. Specifically, these models should support a wide variety of reasoning tasks (or queries), and thus enable advanced scene study. Driven by this interest, we analyze 3D representations for their utility to perform queries. Since representations as well as queries are highly heterogeneous, we build on a categorization that serves as a coupling interface between both domains. Equipped with our taxonomy and the notion of uncertainty in the representation, we quantify the utility of representations for solving three archetypal reasoning tasks in terms of accuracy, uncertainty and computational complexity. We provide an empirical analysis of these intertwined realms on challenging real and synthetic urban scenes.\r"
  },
  "cvpr2017_w18_monitoringethiopianwheatfunguswithsatelliteimageryanddeepfeaturelearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Monitoring Ethiopian Wheat Fungus With Satellite Imagery and Deep Feature Learning",
    "authors": [
      "Reid Pryzant",
      "Stefano Ermon",
      "David Lobell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Pryzant_Monitoring_Ethiopian_Wheat_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Pryzant_Monitoring_Ethiopian_Wheat_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Wheat is the most important Ethiopian crop, and rust one of its greatest antagonists. There is a need for cheap and scalable rust monitoring in the developing world, but existing methods employ costly data collection techniques. We introduce a scalable, accurate, and inexpensive method for tracking outbreaks with publicly available remote sensing data. Our approach improves existing techniques in two ways. First, we forgo the spectral features employed by the remote sensing community in favor of automatically learned features generated by Convolutional and Long Short-Term Memory Networks. Second, we aggregate data into larger geospatial regions. We evaluate our approach on nine years of agricultural outcomes, show that it outperforms competing techniques, and demonstrate its predictive foresight. This is a promising new direction in crop disease monitoring, one that has the potential to grow more powerful with time. \r"
  },
  "cvpr2017_w18_filmycloudremovalonsatelliteimagerywithmultispectralconditionalgenerativeadversarialnets": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Filmy Cloud Removal on Satellite Imagery With Multispectral Conditional Generative Adversarial Nets",
    "authors": [
      "Kenji Enomoto",
      "Ken Sakurada",
      "Weimin Wang",
      "Hiroshi Fukui",
      "Masashi Matsuoka",
      "Ryosuke Nakamura",
      "Nobuo Kawaguchi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Enomoto_Filmy_Cloud_Removal_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Enomoto_Filmy_Cloud_Removal_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper proposes a method for cloud removal from visible light RGB satellite images by extending the conditional Generative Adversarial Networks (cGANs) from RGB images to multispectral images. The networks are trained to output images that are close to the ground truth with the input images synthesized with clouds on the ground truth images. In the available dataset, the ratio of images of the forest or the sea is very high, which will cause bias of the training dataset if we uniformly sample from the dataset. Thus, we utilize the t-Distributed Stochastic Neighbor Embedding (t-SNE) to improve the bias problem of the training dataset. Finally, we confirm the feasibility of the proposed networks on the dataset of 4 bands images including three visible light bands and one near-infrared (NIR) band.\r"
  },
  "cvpr2017_w18_automatic3dreconstructionfrommulti-datesatelliteimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Automatic 3D Reconstruction From Multi-Date Satellite Images",
    "authors": [
      "Gabriele Facciolo",
      "Carlo de Franchis",
      "Enric Meinhardt-Llopis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Facciolo_Automatic_3D_Reconstruction_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Facciolo_Automatic_3D_Reconstruction_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose an algorithm for computing a 3D model from several satellite images of the same site. The method works even if the images were taken at different dates with important lighting and vegetation differences. We show that with a large number of input images the resulting 3D models can be as accurate as those obtained from a single same-date stereo pair. To deal with seasonal vegetation changes, we propose a strategy that accounts for the multi-modal nature of 3D models computed from multi-date images. Our method uses a local affine camera approximation and thus focuses on the 3D reconstruction of small areas. This is a common setup in urgent cartography for emergency management, for which abundant multi-date imagery can be immediately available to build a reference 3D model. A preliminary implementation of this method was used to win the IARPA Multi-View Stereo 3D Mapping Challenge 2016. Experiments on the challenge dataset are used to substantiate our claims.\r"
  },
  "cvpr2017_w18_jointlearningfromearthobservationandopenstreetmapdatatogetfasterbettersemanticmaps": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Joint Learning From Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps",
    "authors": [
      "Nicolas Audebert",
      "Bertrand Le Saux",
      "Sebastien Lefevre"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Audebert_Joint_Learning_From_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Audebert_Joint_Learning_From_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We investigate the use of OSM data for semantic labeling of EO images. Deep neural networks have been used in the past for remote sensing data classification from various sensors, including multispectral, hyperspectral, Radar and Lidar data. However, OSM is an abundant data source that has already been used as ground truth data, but rarely exploited as an input information layer. We study different use cases and deep network architectures to leverage this OSM data for semantic labeling of aerial and satellite images. Especially, we look into fusion based architectures and coarse-to-fine segmentation to include the OSM layer into multispectral-based deep fully convolutional networks. We illustrate how these methods can be used successfully on two public datasets: the ISPRS Potsdam and the DFC2017. We show that OSM data can efficiently be integrated into the vision-based deep learning models and that it significantly improves both the accuracy performance and the convergence.\r"
  },
  "cvpr2017_w18_densesemanticlabelingofvery-high-resolutionaerialimageryandlidarwithfully-convolutionalneuralnetworksandhigher-ordercrfs": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Dense Semantic Labeling of Very-High-Resolution Aerial Imagery and LiDAR With Fully-Convolutional Neural Networks and Higher-Order CRFs",
    "authors": [
      "Yansong Liu",
      "Sankaranarayanan Piramanayagam",
      "Sildomar T. Monteiro",
      "Eli Saber"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Liu_Dense_Semantic_Labeling_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Liu_Dense_Semantic_Labeling_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Efficient and effective multisensor fusion techniques are demanded in order to fully exploit two complementary data modalities, e.g aerial optical imagery, and the LiDAR data. Recent efforts have been mostly devoted to exploring how to properly combine both sensor data using pre-trained deep convolutional neural networks (DCNNs) at the feature level. In this paper, we propose a decision-level fusion approach with a simpler architecture for the task of dense semantic labeling. Our proposed method first obtains two initial probabilistic labeling results from a fully-convolutional neural network and a simple classifier, e.g. logistic regression exploiting spectral channels and LiDAR data, respectively. These two outcomes are then combined within a higher-order conditional random field (CRF). The CRF inference will estimate the final dense semantic labeling results. The proposed method generates the state-of-the-art semantic labeling results.\r"
  },
  "cvpr2017_w18_nonrigidregistrationofhyperspectralandcolorimageswithvastlydifferentspatialandspectralresolutionsforspectralunmixingandpansharpening": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Nonrigid Registration of Hyperspectral and Color Images With Vastly Different Spatial and Spectral Resolutions for Spectral Unmixing and Pansharpening",
    "authors": [
      "Yuan Zhou",
      "Anand Rangarajan",
      "Paul D. Gader"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Zhou_Nonrigid_Registration_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Zhou_Nonrigid_Registration_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we propose a framework to register images with very large scale differences by utilizing the point spread function (PSF), and apply it to register hyperspectral and hi-resolution color images. The algorithm minimizes a least-squares (LSQ) objective function with an incorporated spectral response function (SRF), a nonrigid freeform deformation applied on the hyperspectral image and a rigid transformation on the color image. The optimization problem is solved by updating the two transformations and the two physical functions in an alternating fashion. We executed the framework on a simulated Pavia University dataset and a real Salton Sea dataset, by comparing the proposed algorithm with its rigid variation, and two mutual information-based algorithms. The results indicate that the LSQ freeform version has the best performance for the nonrigid simulation and real datasets, with less than 0.15 pixel error given 1 pixel nonrigid distortion in the hyperspectral domain.\r"
  },
  "cvpr2017_w18_earthobservationusingsarandsocialmediaimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Earth Observation Using SAR and Social Media Images",
    "authors": [
      "Yuanyuan Wang",
      "Xiao Xiang Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/html/Wang_Earth_Observation_Using_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w18/papers/Wang_Earth_Observation_Using_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Earth Observation (EO) is mostly carried out through centralized optical and synthetic aperture radar (SAR) missions. Despite the controlled quality of their products, such observation is restricted by the characteristics of the sensor platform, e.g. the revisit time. Over the last decade, the rapid development of social media has accumulated vast amount of online images. Despite their uncontrolled quality, the sheer volume may contain useful information that can complement the EO missions, especially the SAR missions. This paper presents a preliminary work of fusing social media and SAR images. They have distinct imaging geometries, which are nearly impossible to even coregister without a precise 3-D model. We describe a general approach to coregister them without using external 3-D model. We demonstrate that, one can obtain a new kind of 3-D city model that includes the optical texture for better scene understanding and the precise deformation retrieved from SAR interferometry. \r"
  },
  "cvpr2017_w19_humanactivityrecognitionusingcombinatorialdeepbeliefnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Understanding of Humans in Crowd Scene and the 1st Look Into Person Challenge",
    "title": "Human Activity Recognition Using Combinatorial Deep Belief Networks",
    "authors": [
      "Shreyank N. Gowda"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w19/html/Gowda_Human_Activity_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w19/papers/Gowda_Human_Activity_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Human activity recognition is a field undergoing a great amount of research. The main reason for that is the number of practical applications that are developed using activity recognition as the base. This paper proposes an approach to human activity recognition using a combination of deep belief networks. One network is used to obtain features from motion and to do this we propose a modified Weber descriptor. Another network is used to obtain features from images and to do this we propose the modification of the standard local binary patterns descriptor to obtain a concatenated histogram of lower dimensions. This helps to encode spatial and temporal information of various actions happening in a frame. This further helps to overcome the dimensionality problem that occurs with LBP. The features extracted are then passed onto a CNN that classifies the activity. Few standard activities are considered such as walking, sprinting, hugging etc. \r"
  },
  "cvpr2017_w19_self-supervisedneuralaggregationnetworksforhumanparsing": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Visual Understanding of Humans in Crowd Scene and the 1st Look Into Person Challenge",
    "title": "Self-Supervised Neural Aggregation Networks for Human Parsing",
    "authors": [
      "Jian Zhao",
      "Jianshu Li",
      "Xuecheng Nie",
      "Fang Zhao",
      "Yunpeng Chen",
      "Zhecan Wang",
      "Jiashi Feng",
      "Shuicheng Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w19/html/Zhao_Self-Supervised_Neural_Aggregation_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w19/papers/Zhao_Self-Supervised_Neural_Aggregation_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we present a Self-Supervised Neural Aggregation Network (SS-NAN) for human parsing. SS-NAN adaptively learns to aggregate the multi-scale features at each pixel \"address\". In order to further improve the feature discriminative capacity, a self-supervised joint loss is adopted as an auxiliary learning strategy, which imposes human joint structures into parsing results without resorting to extra supervision. The proposed SS-NAN is end-to-end trainable. SS-NAN can be integrated into any advanced neural networks to help aggregate features regarding the importance at different positions and scales and incorporate rich high-level knowledge regarding human joint structures from a global perspective, which in turn improve the parsing results. Comprehensive evaluations on the recent Look into Person (LIP) and the PASCAL-Person-Part benchmark datasets demonstrate the significant superiority of our method over other state-of-the-arts.\r"
  },
  "cvpr2017_w20_unsupervisedhumanactiondetectionbyactionmatching": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Brave New Ideas for Motion and Spatio-Temporal Representations",
    "title": "Unsupervised Human Action Detection by Action Matching",
    "authors": [
      "Basura Fernando",
      "Sareh Shirazi",
      "Stephen Gould"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/html/Fernando_Unsupervised_Human_Action_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/papers/Fernando_Unsupervised_Human_Action_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a new task of unsupervised action detection by action matching. Given two long videos, the objective is to temporally detect all pairs of matching video segments. A pair of video segments are matched if they share the same human action. The task is category independent---it does not matter what action is being performed---and no supervision is used to discover such video segments. Unsupervised action detection by action matching allows us to align videos in a meaningful manner. As such, it can be used to discover new action categories or as an action proposal technique within, say, an action detection pipeline. We solve this new task using an effective and efficient method. We use an unsupervised temporal encoding method and exploit the temporal consistency in human actions to obtain candidate action segments. We evaluate our method on this challenging task using three activity recognition benchmarks.\r"
  },
  "cvpr2017_w20_ratmrecurrentattentivetrackingmodel": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Brave New Ideas for Motion and Spatio-Temporal Representations",
    "title": "RATM: Recurrent Attentive Tracking Model",
    "authors": [
      "Samira Ebrahimi Kahou",
      "Vincent Michalski",
      "Roland Memisevic",
      "Christopher Pal",
      "Pascal Vincent"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/html/Kahou_RATM_Recurrent_Attentive_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/papers/Kahou_RATM_Recurrent_Attentive_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present an attention-based modular neural framework for computer vision. The framework uses a soft attention mechanism allowing models to be trained with gradient descent. It consists of three modules: a recurrent attention module controlling where to look in an image or video frame, a feature-extraction module providing a representation of what is seen, and an objective module formalizing why the model learns its attentive behavior. The attention module allows the model to focus computation on task-related information in the input. We apply the framework to several object tracking tasks and explore various design choices. We experiment with three data sets, bouncing ball, moving digits and the real-world KTH data set. The proposed RATM performs well on all three tasks and can generalize to related but previously unseen sequences from a challenging tracking data set.\r"
  },
  "cvpr2017_w20_interpretable3dhumanactionanalysiswithtemporalconvolutionalnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Brave New Ideas for Motion and Spatio-Temporal Representations",
    "title": "Interpretable 3D Human Action Analysis With Temporal Convolutional Networks",
    "authors": [
      "Tae Soo Kim",
      "Austin Reiter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/html/Kim_Interpretable_3D_Human_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/papers/Kim_Interpretable_3D_Human_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The discriminative power of modern deep learning models for 3D human action recognition is growing ever so potent. In conjunction with the recent resurgence of 3D human action representation with 3D skeletons, the quality and the pace of recent progress have been significant. However, the inner workings of state-of-the-art learning based methods in 3D human action recognition still remain mostly black-box. In this work, we propose to use a new class of models known as Temporal Convolutional Neural Networks (TCN) for 3D human action recognition. TCN provides us a way to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition. Through this work, we wish to take a step towards a spatio-temporal model that is easier to understand, explain and interpret. The resulting model, Res-TCN, achieves state-of-the-art results on the largest 3D human action recognition dataset, NTU-RGBD.\r"
  },
  "cvpr2017_w20_learningdynamicgmmforattentiondistributiononsingle-facevideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Brave New Ideas for Motion and Spatio-Temporal Representations",
    "title": "Learning Dynamic GMM for Attention Distribution on Single-Face Videos",
    "authors": [
      "Yun Ren",
      "Zulin Wang",
      "Mai Xu",
      "Haoyu Dong",
      "Shengxi Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/html/Ren_Learning_Dynamic_GMM_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/papers/Ren_Learning_Dynamic_GMM_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The past decade has witnessed the popularity of video conferencing, such as FaceTime and Skype. Hence, it is necessary to predict attention on face videos by saliency detection, as saliency can be used as a guidance of region-of-interest (ROI) for the content-based applications. To this end, this paper proposes a novel approach for saliency detection in single-face videos. Through analysis on our database of 70 single-face videos , we investigate that most attention is attracted by face in videos, and that attention distribution within a face varies with regard to face size and mouth movement. We propose to model visual attention on face region for videos by dynamic Gaussian mixture model (DGMM), the variation of which relies on face size, mouth movement and facial landmarks. Then, we develop a long short-term memory (LSTM) neural network in estimating DGMM for saliency detection of single-face videos, so called LSTM-DGMM. \r"
  },
  "cvpr2017_w20_opticalaccelerationformotiondescriptioninvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Brave New Ideas for Motion and Spatio-Temporal Representations",
    "title": "Optical Acceleration for Motion Description in Videos",
    "authors": [
      "Anitha Edison",
      "Jiji C. V."
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/html/Edison_Optical_Acceleration_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w20/papers/Edison_Optical_Acceleration_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Modern techniques for describing motion in videos are centred around velocity descriptors based on optical flow. Realizing that acceleration is as important as velocity for describing motion information, in this paper first we propose two different algorithms to compute optical acceleration. Delving deeper into the concept of optical acceleration, we use two descriptors: histogram of optical acceleration (HOA) and histogram of spatial gradient of acceleration (HSGA), to effectively encode the motion information. To assess the effectiveness of these descriptors for motion encoding, we applied it for human action recognition and abnormal event detection in videos. In fact, we used acceleration descriptors in conjunction with velocity descriptors to get a better description of motion in videos. Our experiments reveal that acceleration descriptors could provide additional information that velocity descriptors missed and hence combining them results in a superior motion descriptor.\r"
  },
  "cvpr2017_w26_multi-modalscorefusionanddecisiontreesforexplainableautomaticjobcandidatescreeningfromvideocvs": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Multi-Modal Score Fusion and Decision Trees for Explainable Automatic Job Candidate Screening From Video CVs",
    "authors": [
      "Heysem Kaya",
      "Furkan Gurpinar",
      "Albert Ali Salah"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Kaya_Multi-Modal_Score_Fusion_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Kaya_Multi-Modal_Score_Fusion_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We describe an end-to-end system for explainable automatic job candidate screening from video CVs. In this application, audio, face and scene features are first computed from an input video CV, using rich feature sets. These multiple modalities are fed into modality-specific regressors to predict apparent personality traits and a variable that predicts whether the subject will be invited to the interview. The base learners are stacked to an ensemble of decision trees to produce the outputs of the quantitative stage, and a single decision tree, combined with a rule-based algorithm produces interview decision explanations based on the quantitative results. The proposed system in this work ranks first in both quantitative and qualitative stages of the CVPR 2017 ChaLearn Job Candidate Screening Coopetition.\r"
  },
  "cvpr2017_w26_personalitytraitsandjobcandidatescreeningviaanalyzingfacialvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Personality Traits and Job Candidate Screening via Analyzing Facial Videos",
    "authors": [
      "Salah Eddine Bekhouche",
      "Fadi Dornaika",
      "Abdelkrim Ouafi",
      "Abdelmalik Taleb-Ahmed"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Bekhouche_Personality_Traits_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Bekhouche_Personality_Traits_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we propose a novel approach for estimating the Big Five personality traits and the job candidate screening attribute through facial videos. At running time, the proposed system feeds the Pyramid Multi-Level (PML) texture features extracted from the whole video sequence to 5 Support Vector Regressors in order to estimate the personality traits. These estimated five scores are then considered as new input features to the interview score regressor. The latter is given by a Gaussian Process Regression (GPR). The experimental results on ChaLearn LAP APA2016 dataset achieve good performance. Furthermore, they demonstrate that the computational cost of both the training and the testing of the proposed framework are very competitive in terms of accuracy and computational cost.\r"
  },
  "cvpr2017_w26_human-explainablefeaturesforjobcandidatescreeningprediction": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Human-Explainable Features for Job Candidate Screening Prediction",
    "authors": [
      "Achmadnoer Sukma Wicaksana",
      "Cynthia C. S. Liem"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Wicaksana_Human-Explainable_Features_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Wicaksana_Human-Explainable_Features_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Video blogs (vlogs) are a popular media form for people to present themselves. In case a vlogger would be a job candidate, vlog content can be useful for automatically assessing the candidate's traits, as well as potential interviewability. Using a dataset from the CVPR ChaLearn competition, we build a model predicting Big Five personality trait scores and interviewability of vloggers, explicitly targeting explainability of the system output to humans without technical background. We use human-explainable features as input, and a linear model for the system's building blocks. Four multimodal feature representations are constructed to capture facial expression, movement, and linguistic usage. For each, PCA is used for dimensionality reduction and simple linear regression for the predictive model. Our system's accuracy lies in the middle of the quantitative competition chart, while we can trace back the reasoning behind each score and generate a qualitative analysis report per video.\r"
  },
  "cvpr2017_w26_explainingdistributedneuralactivationsviaunsupervisedlearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Explaining Distributed Neural Activations via Unsupervised Learning",
    "authors": [
      "Soheil Kolouri",
      "Charles E. Martin",
      "Heiko Hoffmann"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Kolouri_Explaining_Distributed_Neural_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Kolouri_Explaining_Distributed_Neural_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent work has demonstrated the emergence of semantic object-part detectors in activation patterns of convolutional neural networks (CNNs), but did not account for the distributed multi-layer neural activations in such networks. In this work, we propose a novel method to extract distributed patterns of activations from a CNN and show that such patterns correspond to high-level visual attributes. We propose an unsupervised learning module that sits above a pre-trained CNN and learns distributed activation patterns of the network. We utilize elastic non-negative matrix factorization to analyze the responses of a pretrained CNN to an input image and extract salient image regions. The corresponding patterns of neural activations for the extracted salient regions are then clustered via unsupervised deep embedding for clustering (DEC) framework. We demonstrate that these distributed activations contain high-level image features that could be explicitly used for image classification. \r"
  },
  "cvpr2017_w26_automatedscreeningofjobcandidatebasedonmultimodalvideoprocessing": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Automated Screening of Job Candidate Based on Multimodal Video Processing",
    "authors": [
      "Jelena Gorbova",
      "Iiris Lusi",
      "Andre Litvin",
      "Gholamreza Anbarjafari"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Gorbova_Automated_Screening_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Gorbova_Automated_Screening_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The selection of adequate job candidates is very long and challenging process for each employer. The system presented in this paper is aiming to decrease the time for candidate selection on the pre-employment stage using automatic personality screening based on visual, audio and lexical cues from short video-clips. The system is build to predict candidate scores of 5 Big Personality Traits and to estimate a final decision, to which degree the person from video-clip has to be invited to the job interview. For each channel a set of relevant features is extracted, which are used to train separately from each other using Deep Learning. In the final stage all three results are fused together into final scores prediction. The experiment was conducted on first impression database and achieved significant performance.\r"
  },
  "cvpr2017_w26_explainingtheunexplainedaclass-enhancedattentiveresponse(clear)approachtounderstandingdeepneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks",
    "authors": [
      "Devinder Kumar",
      "Alexander Wong",
      "Graham W. Taylor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Kumar_Explaining_the_Unexplained_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Kumar_Explaining_the_Unexplained_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.\r"
  },
  "cvpr2017_w26_decodingthedeepexploringclasshierarchiesofdeeprepresentationsusingmultiresolutionmatrixfactorization": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Decoding the Deep: Exploring Class Hierarchies of Deep Representations Using Multiresolution Matrix Factorization",
    "authors": [
      "Vamsi K. Ithapu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Ithapu_Decoding_the_Deep_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Ithapu_Decoding_the_Deep_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The necessity of depth has led to a family of designs referred to as very deep networks (e.g., GoogLeNet has 22 layers). As the depth increases even further, the need for appropriate tools to explore the space of hidden representations becomes paramount. Classical PCA or eigen-spectrum based global approaches do not model the complex inter-class relationships. In this work, we propose a novel decomposition referred to as multiresolution matrix factorization that models hierarchical and compositional structure in symmetric matrices. This new decomposition efficiently infers semantic relationships among deep representations, even when they are not explicitly trained to do so. We show that it is a valuable tool in understanding the landscape of hidden representations, in adapting existing architectures for new tasks and also for designing new architectures using interpretable, human-releatable, class-by-class relationships that we hope the network to learn. \r"
  },
  "cvpr2017_w26_interpretingcnnmodelsforapparentpersonalitytraitregression": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - ChaLearn: Explainable Computer Vision Workshop and Job Candidate Screening Competition",
    "title": "Interpreting CNN Models for Apparent Personality Trait Regression",
    "authors": [
      "Carles Ventura",
      "David Masip",
      "Agata Lapedriza"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/html/Ventura_Interpreting_CNN_Models_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w26/papers/Ventura_Interpreting_CNN_Models_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper addresses the problem of automatically inferring personality traits of people talking to a camera. As in many other computer vision problems, Convolutional Neural Networks (CNN) models have shown impressive results. However, despite of the success in terms of performance, it is unknown what internal representation emerges in the CNN. This paper presents a deep study on understanding why CNN models are performing surprisingly well in this complex problem. We use current techniques on CNN model interpretability, combined with face detection and Action Unit (AUs) recognition systems, to perform our quantitative studies. Our results show that: (1) face provides most of the discriminative information for personality trait inference, and (2) the internal CNN representations mainly analyze key face regions such as eyes, nose, and mouth. Finally, we study the contribution of AUs for personality trait inference, showing the influence of certain AUs in the facial trait judgments.\r"
  },
  "cvpr2017_w27_linearizingtheplenopticspace": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Linearizing the Plenoptic Space",
    "authors": [
      "Gregoire Nieto",
      "Frederic Devernay",
      "James Crowley"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Nieto_Linearizing_the_Plenoptic_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Nieto_Linearizing_the_Plenoptic_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The plenoptic function, also known as the light field or the lumigraph, contains the information about the radiance of all optical rays that go through all points in space in a scene. Since no camera can capture all this information, one of the main challenges in plenoptic imaging is light field reconstruction, which consists in interpolating the ray samples captured by the cameras to create a dense light field. Most existing methods perform this task by first attempting some kind of 3D reconstruction of the visible scene. Our method, in contrast, works by modeling the scene as a set of visual points, which describe how each point moves in the image when a camera moves. We compute visual point models of various degrees of complexity, and show that high-dimensional models are able to replicate complex optical effects such as reflection or refraction, and a model selection method can differentiate quasi-Lambertian from non-Lambertian areas in the scene.\r"
  },
  "cvpr2017_w27_fullbrdfreconstructionusingcnnsfrompartialphotometricstereo-lightfielddata": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Full BRDF Reconstruction Using CNNs From Partial Photometric Stereo-Light Field Data",
    "authors": [
      "Doris Antensteiner",
      "Svorad Stolc"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Antensteiner_Full_BRDF_Reconstruction_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Antensteiner_Full_BRDF_Reconstruction_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The acquisition of partial BRDF measurements using light field cameras and several illumination directions raises critical questions regarding the accuracy of inferences based on that data. Therefore, we attempt to verify the quality of the reconstruction of a full BRDF using partial input data. A dataset that provides a densely sampled BRDF was used, both in viewing and illumination directions. We show the reconstruction of dense BRDFs when the viewing angles are limited to top central regions, while the illumination angles are not reduced and are positioned in the shape of a half sphere around the material object, these properties are characteristic of data provided by plenoptic cameras paired with a photometric light dome. The partial reconstruction of the dense BRDF out of data is achieved by utilizing convolutional neural networks. We obtain a competitive full reconstruction when up to 2/3 of the BRDF is unknown. \r"
  },
  "cvpr2017_w27_surfacenormalreconstructionfromspecularinformationinlightfielddata": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Surface Normal Reconstruction From Specular Information in Light Field Data",
    "authors": [
      "Marcel Gutsche",
      "Hendrik Schilling",
      "Maximilian Diebold",
      "Christoph Garbe"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Gutsche_Surface_Normal_Reconstruction_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Gutsche_Surface_Normal_Reconstruction_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Specular highlights provide information about the shape of an object. Its characteristics are mostly unwanted in computer vision due to violation of the Lambertian assumption, which most algorithms require. Instead of neglecting this ubiquitous phenomenon we harvest it to extract surface normals with very high accuracy. Compared to photometric stereo our method works with multiple views and a fixed light source. We only require a low number of observation from a small part of the specular lobe to reconstruct the normal and reflection parameters. This is achieved by jointly optimizing the normal and the light transport of surfaces points. This work is a proof of concept to demonstrate the feasibility of acquiring highly accurate surface normals from specular reflection, which can be combined with conventional methods. The model is tested and evaluated on synthetic as well as real world data acquired by a cross light field setup.\r"
  },
  "cvpr2017_w27_datasetandpipelineformulti-viewlight-fieldvideo": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Dataset and Pipeline for Multi-View Light-Field Video",
    "authors": [
      "Neus Sabater",
      "Guillaume Boisson",
      "Benoit Vandame",
      "Paul Kerbiriou",
      "Frederic Babon",
      "Matthieu Hog",
      "Remy Gendrot",
      "Tristan Langlois",
      "Olivier Bureller",
      "Arno Schubert",
      "Valerie Allie"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Sabater_Dataset_and_Pipeline_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Sabater_Dataset_and_Pipeline_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The quantity and diversity of data in Light-Field videos makes this content valuable for many applications such as mixed and augmented reality or post-production in the movie industry. Some of such applications require a large parallax between the different views of the Light-Field, making the multi-view capture a better option than plenoptic cameras. In this paper we propose a dataset and a complete pipeline for Light-Field video. The proposed algorithms are specially tailored to process sparse and wide-baseline multi-view videos captured with a camera rig. Our pipeline includes algorithms such as geometric calibration, color homogenization, view pseudo-rectification and depth estimation. Such elemental algorithms are well known by the state-of-the-art but they must achieve high accuracy to guarantee the success of other algorithms using our data. Along this paper, we publish our Light-Field video dataset that we believe may be of special interest for the community. We provide the original sequences, the calibration parameters and the pseudo-rectified views. Finally, we propose a depth-based rendering algorithm for Dynamic Perspective Rendering. \r"
  },
  "cvpr2017_w27_lightfieldconvergencyimplicitphotometricconsistencyontransparentsurface": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Light Field Convergency: Implicit Photometric Consistency on Transparent Surface",
    "authors": [
      "Yuta Ideguchi",
      "Yuki Uranishi",
      "Shunsuke Yoshimoto",
      "Yoshihiro Kuroda",
      "Osamu Oshiro"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Ideguchi_Light_Field_Convergency_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Ideguchi_Light_Field_Convergency_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper proposes a method for estimating the surface oftransparentobjectsbasedonlightfieldconvergency. The lightfieldconvergencyrepresentsthedegreeofconvergence ofthelightfieldateachpoint. Theproposedmethodutilizes local photo consistency, which is one of characteristics of the light field convergency. Around a boundary contour, a point that is visible from viewpoints with a small convergence angle is locally consistent. In other words, the local photo consistency is implicitly maintained near the boundary contour. We use a light field camera as a camera array withsmallconvergenceanglestoestimatethesurfaceofthe transparentobject. Experimentalresultshavedemonstrated thatthedepthofthetransparentobjectisestimatedfromthe captured image by the light field camera. \r"
  },
  "cvpr2017_w27_optimizingthelensselectionprocessformulti-focusplenopticcamerasandnumericalevaluation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Optimizing the Lens Selection Process for Multi-Focus Plenoptic Cameras and Numerical Evaluation",
    "authors": [
      "Luca Palmieri",
      "Reinhard Koch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Palmieri_Optimizing_the_Lens_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Palmieri_Optimizing_the_Lens_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " The last years have seen a quick rise of digital photography. Plenoptic cameras provide extended capabilities with respect to previous models. Multi-focus cameras enlarge the depth-of-field of the pictures using different focal lengths in the lens composing the array, but questions still arise on how to select and use these lenses. In this work a further insight on the lens selection was made, and a novel method was developed in order to choose the best available lens combination for the disparity estimation. We test different lens combinations, ranking them based on the error and the number of different lenses used, creating a mapping function that relates the virtual depth with the combination that achieves the best result. The results are then organized in a look up table that can be tuned to trade off between performances and accuracy. This allows for fast and accurate lens selection. Moreover, new synthetic images with respective ground truth are provided, in order to confirm that this work performs better than the current state of the art in efficiency and accuracy of the results.\r"
  },
  "cvpr2017_w27_underwaterimagedehazingwithalightfieldcamera": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Underwater Image Dehazing With a Light Field Camera",
    "authors": [
      "Katherine A. Skinner",
      "Matthew Johnson-Roberson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Skinner_Underwater_Image_Dehazing_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Skinner_Underwater_Image_Dehazing_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Underwater vision is subject to effects of underwater light propagation that act to absorb, scatter, and attenuate light rays between the scene and the imaging platform. Backscattering has been shown to have a strong impact on underwater images. As light interacts with particulate matter in the water column, it is scattered back towards the imaging sensor, resulting in a hazy effect across the image. A similar effect occurs in terrestrial applications in images of foggy scenes due to interaction with the atmosphere. Prior work on multi-image dehazing has relied on multiple cameras, polarization filters, or moving light sources. Single image dehazing is an ill-posed problem; proposed solutions rely on strong priors of the scene. This paper presents a novel method for underwater image dehazing using a light field camera to capture both the spatial and angular distribution of light across a scene. First, a 2D dehazing method is applied to each sub-aperture image. These dehazed images are then combined to produce a smoothed central view. Lastly, the smoothed central view is used as a reference to perform guided image filtering, resulting in a 4D dehazed underwater light field image. The developed method is validated on real light field data collected in a controlled in-lab water tank, with images taken in air for reference. This dataset is made publicly available.\r"
  },
  "cvpr2017_w27_richardson-lucydeblurringformovinglightfieldcameras": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "Richardson-Lucy Deblurring for Moving Light Field Cameras",
    "authors": [
      "Donald G. Dansereau",
      "Anders Eriksson",
      "Jurgen Leitner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Dansereau_Richardson-Lucy_Deblurring_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Dansereau_Richardson-Lucy_Deblurring_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We generalize Richardson-Lucy (RL) deblurring to 4-D light fields by replacing the convolution steps with light field rendering of motion blur. The method deals correctly with blur caused by 6-degree-of-freedom camera motion in complex 3-D scenes, without performing depth estimation. We introduce a novel regularization term that maintains parallax information in the light field while reducing noise and ringing. We demonstrate the method operating effectively on rendered scenes and scenes captured using an off-the-shelf light field camera. An industrial robot arm provides repeatable and known trajectories, allowing us to establish quantitative performance in complex 3-D scenes. Qualitative and quantitative results confirm the effectiveness of the method, including commonly occurring cases for which previously published methods fail. We include mathematical proof that the algorithm converges to the maximum-likelihood estimate of the unblurred scene under Poisson noise. We expect extension to blind methods to be possible following the generalization of 2-D Richardson-Lucy to blind deconvolution. \r"
  },
  "cvpr2017_w27_ataxonomyandevaluationofdenselightfielddepthestimationalgorithms": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Light Fields for Computer Vision",
    "title": "A Taxonomy and Evaluation of Dense Light Field Depth Estimation Algorithms",
    "authors": [
      "Ole Johannsen",
      "Katrin Honauer",
      "Bastian Goldluecke",
      "Anna Alperovich",
      "Federica Battisti",
      "Yunsu Bok",
      "Michele Brizzi",
      "Marco Carli",
      "Gyeongmin Choe",
      "Maximilian Diebold",
      "Marcel Gutsche",
      "Hae-Gon Jeon",
      "In So Kweon",
      "Jaesik Park",
      "Jinsun Park",
      "Hendrik Schilling",
      "Hao Sheng",
      "Lipeng Si",
      "Michael Strecke",
      "Antonin Sulc",
      "Yu-Wing Tai",
      "Qing Wang",
      "Ting-Chun Wang",
      "Sven Wanner",
      "Zhang Xiong",
      "Jingyi Yu",
      "Shuo Zhang",
      "Hao Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/html/Johannsen_A_Taxonomy_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w27/papers/Johannsen_A_Taxonomy_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper presents the results of the depth estimation challenge for dense light fields, which took place at the second workshop on Light Fields for Computer Vision (LF4CV) in conjunction with CVPR 2017. The challenge consisted of submission to a recent light field benchmark, which allows a thorough performance analysis. While individual results are readily available on the benchmark web page http://www.lightfield-analysis.net, we take this opportunity to give a detailed overview of the current participants. Based on the algorithms submitted to our challenge, we develop a taxonomy of light field disparity estimation algorithms and give a report on the current state-of-the-art. In addition, we include more comparative metrics, and discuss the relative strengths and weaknesses of the algorithms. Thus, we obtain a snapshot of where light field algorithm development stands at the moment and identify aspects with potential for further improvement.\r"
  },
  "cvpr2017_w28_positiondeterminesperspectiveinvestigatingperspectivedistortionforimageforensicsoffaces": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Position Determines Perspective: Investigating Perspective Distortion for Image Forensics of Faces",
    "authors": [
      "Bo Peng",
      "Wei Wang",
      "Jing Dong",
      "Tieniu Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Tan_Position_Determines_Perspective_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Tan_Position_Determines_Perspective_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper points out a new telltale trace -- the characteristic of perspective distortion (CPD), for the image forensics of faces. The perspective distortion is determined by the position of image shooting, and it is often overlooked when creating a forgery, which results in the inconsistency between the claimed camera parameters and the CPD in the face image. To investigate this consistency problem, we cast it to the consistency between the claimed camera intrinsic parameters and the estimated ones from the CPD. Our parameter estimation approach is based on geometric observations that are related to CPD, like facial landmarks and contours. We analyze the estimation uncertainty caused by indeterminacy of observation to obtain a more reliable forensic decision. Experiments on synthetic datasets and real forgery examples demonstrate the effectiveness of the proposed method.\r"
  },
  "cvpr2017_w28_transferabledeep-cnnfeaturesfordetectingdigitalandprint-scannedmorphedfaceimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Transferable Deep-CNN Features for Detecting Digital and Print-Scanned Morphed Face Images",
    "authors": [
      "R. Raghavendra",
      "Kiran B. Raja",
      "Sushma Venkatesh",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Busch_Transferable_Deep-CNN_Features_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Busch_Transferable_Deep-CNN_Features_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Face biometrics is widely used in various applications including border control and facilitating the verification of travellers' identity claim with respect to his electronic passport (ePass). As in most countries, passports are issued to a citizen based on the submitted photo which allows the applicant to provide a morphed face photo to conceal his identity during the application process. In this work, we propose a novel approach leveraging the transferable features from a pre-trained Deep Convolutional Neural Networks (D-CNN) to detect both digital and print-scanned morphed face image. Thus, the proposed approach is based on the feature level fusion of the first fully connected layers of two D-CNN (VGG19 and AlexNet) that are specifically fine-tuned using the morphed face image database. The proposed method is extensively evaluated on the newly constructed database with both digital and print-scanned morphed face images corresponding to bona fide and morphed data reflecting a real-life scenario. The obtained results consistently demonstrate improved detection performance of the proposed scheme over previously proposed methods on both the digital and the print-scanned morphed face image database.\r"
  },
  "cvpr2017_w28_two-streamneuralnetworksfortamperedfacedetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Two-Stream Neural Networks for Tampered Face Detection",
    "authors": [
      "Peng Zhou",
      "Xintong Han",
      "Vlad I. Morariu",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Davis_Two-Stream_Neural_Networks_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Davis_Two-Stream_Neural_Networks_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a two-stream network for face tampering detection. We train GoogLeNet to detect tampering artifacts in a face classification stream, and train a patch based triplet network to leverage features capturing local noise residuals and camera characteristics as a second stream. In addition, we use two different online face swaping applications to create a new dataset that consists of 2010 tampered images, each of which contains a tampered face. We evaluate the proposed two-stream network on our newly collected dataset. Experimental results demonstrate the effectness of our method.\r"
  },
  "cvpr2017_w28_acounter-forensicmethodforcnn-basedcameramodelidentification": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "A Counter-Forensic Method for CNN-Based Camera Model Identification",
    "authors": [
      "David Guera",
      "Yu Wang",
      "Luca Bondi",
      "Paolo Bestagini",
      "Stefano Tubaro",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Delp_A_Counter-Forensic_Method_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Delp_A_Counter-Forensic_Method_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " An increasing number of digital images are being shared and accessed through websites, media, and social applications. Many of these images have been modified and are not authentic. Recent advances in the use of deep convolutional neural networks (CNNs) have facilitated the task of analyzing the veracity and authenticity of largely distributed image datasets. We examine in this paper the problem of identifying the camera model or type that was used to take an image and that can be spoofed. Due to the linear nature of CNNs and the high-dimensionality of images, neural networks are vulnerable to attacks with adversarial examples. These examples are imperceptibly different from correctly classified images but are misclassified with high confidence by CNNs. In this paper, we describe a counter-forensic method capable of subtly altering images to change their estimated camera model when they are analyzed by any CNN-based camera model detector. Our method can use both the Fast Gradient Sign Method (FGSM) or the Jacobian-based Saliency Map Attack (JSMA) to craft these adversarial images and does not require direct access to the CNN. Our results show that even advanced deep learning architectures trained to analyze images and obtain camera model information are still vulnerable to our proposed method.\r"
  },
  "cvpr2017_w28_camerasourceidentificationusingdiscretecosinetransformresiduefeaturesandensembleclassifier": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Camera Source Identification Using Discrete Cosine Transform Residue Features and Ensemble Classifier",
    "authors": [
      "Aniket Roy",
      "Rajat Subhra Chakraborty",
      "Udaya Sameer",
      "Ruchira Naskar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Naskar_Camera_Source_Identification_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Naskar_Camera_Source_Identification_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Machine Learning based model building and classification has proved to be extremely effective for the camera source identification problem. In this paper, we have proposed a camera source identification methodology, based on extraction of the Discrete Cosine Transform Residual features, and subsequent Random Forest based ensemble classification with AdaBoost. We improve the classification accuracy by incorporating dimensionality reduction by Principal Component Analysis. Our experimental results on 10,507 images captured by ten cameras from the Dresden Image Database gives an average classification accuracy of 99.1%, and also show low overfitting trends when the constructed classifier is applied on a different image database.\r"
  },
  "cvpr2017_w28_tamperingdetectionandlocalizationthroughclusteringofcamera-basedcnnfeatures": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Tampering Detection and Localization Through Clustering of Camera-Based CNN Features",
    "authors": [
      "Luca Bondi",
      "Silvia Lameri",
      "David Guera",
      "Paolo Bestagini",
      "Edward J. Delp",
      "Stefano Tubaro"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Tubaro_Tampering_Detection_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Tubaro_Tampering_Detection_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Due to the rapid proliferation of image capturing devices and user-friendly editing software suites, image manipulation is at everyone's hand. For this reason, the forensic community has developed a series of techniques to determine image authenticity. In this paper, we propose an algorithm for image tampering detection and localization, leveraging characteristic footprints left on images by different camera models. The rationale behind our algorithm is that all pixels of pristine images should be detected as being shot with a single device. Conversely, if a picture is obtained through image composition, traces of multiple devices can be detected. The proposed algorithm exploits a convolutional neural network (CNN) to extract characteristic camera model features from image patches. These features are then analyzed by means of iterative clustering techniques in order to detect whether an image has been forged, and localize the alien region.\r"
  },
  "cvpr2017_w28_localizationofjpegdoublecompressionthroughmulti-domainconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Localization of JPEG Double Compression Through Multi-Domain Convolutional Neural Networks",
    "authors": [
      "Irene Amerini",
      "Tiberio Uricchio",
      "Lamberto Ballan",
      "Roberto Caldelli"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Caldelli_Localization_of_JPEG_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Caldelli_Localization_of_JPEG_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " When an attacker wants to falsify an image, in most of cases she/he will perform a JPEG recompression. Different techniques have been developed based on diverse theoretical assumptions but very effective solutions have not been developed yet. Recently, machine learning based approaches have been started to appear in the field of image forensics to solve diverse tasks such as acquisition source identification and forgery detection. In this last case, the aim ahead would be to get a trained neural network able, given a to-be-checked image, to reliably localize the forged areas. With this in mind, our paper proposes a step forward in this direction by analyzing how a single or double JPEG compression can be revealed and localized using convolutional neural networks (CNNs). Different kinds of input to the CNN have been taken into consideration, and various experiments have been carried out trying also to evidence potential issues to be further investigated.\r"
  },
  "cvpr2017_w28_detectionofmetadatatamperingthroughdiscrepancybetweenimagecontentandmetadatausingmulti-taskdeeplearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Detection of Metadata Tampering Through Discrepancy Between Image Content and Metadata Using Multi-Task Deep Learning",
    "authors": [
      "Bor-Chun Chen",
      "Pallabi Ghosh",
      "Vlad I. Morariu",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Davis_Detection_of_Metadata_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Davis_Detection_of_Metadata_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Image content or metadata editing software availability and ease of use has resulted in a high demand for automatic image tamper detection algorithms. Most previous work has focused on detection of tampered image content, whereas we develop techniques to detect metadata tampering in outdoor images using sun altitude angle and other meteorological information like temperature, humidity and weather, which can be observed in most outdoor image scenes. To train and evaluate our technique, we create a large dataset of outdoor images labeled with sun altitude angle and other meteorological data (AMOS+M2), which to our knowledge, is the largest publicly available dataset of its kind. Using this dataset, we train separate regression models for sun altitude angle, temperature and humidity and a classification model for weather to detect any discrepancy between image content and its metadata. Finally, a joint multi-task network for these four features shows a relative improvement of 15.5% compared to each of them individually. We include a detailed analysis for using these networks to detect various types of modification to location and time information in image metadata.\r"
  },
  "cvpr2017_w28_detectionandlocalizationofimageforgeriesusingresamplingfeaturesanddeeplearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Detection and Localization of Image Forgeries Using Resampling Features and Deep Learning",
    "authors": [
      "Jason Bunk",
      "Jawadul H. Bappy",
      "Tajuddin Manhar Mohammed",
      "Lakshmanan Nataraj",
      "Arjuna Flenner",
      "B.S. Manjunath",
      "Shivkumar Chandrasekaran",
      "Amit K. Roy-Chowdhury",
      "Lawrence Peterson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Peterson_Detection_and_Localization_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Peterson_Detection_and_Localization_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Resampling is an important signature of manipulated images. In this paper, we propose two methods to detect and localize image manipulations based on a combination of resampling features and deep learning. In the first method, the Radon transform of resampling features are computed on overlapping image patches. Deep learning classifiers and a Gaussian conditional random field model are then used to create a heatmap. Tampered regions are located using a Random Walker segmentation method. In the second method, resampling features computed on overlapping image patches are passed through a Long short-term memory (LSTM) based network for classification and localization. We compare the performance of detection/localization of both these methods. Our experimental results show that both techniques are effective in detecting and localizing digital image forgeries.\r"
  },
  "cvpr2017_w28_forms-locksadatasetfortheevaluationofsimilaritymeasuresforforensictoolmarkimages": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "FORMS-Locks: A Dataset for the Evaluation of Similarity Measures for Forensic Toolmark Images",
    "authors": [
      "Manuel Keglevic",
      "Robert Sablatnig"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Sablatnig_FORMS-Locks_A_Dataset_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Sablatnig_FORMS-Locks_A_Dataset_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a toolmark dataset created using lock cylinders seized during criminal investigations of break-ins. A total number of 197 cylinders from 48 linked criminal cases were photographed under a comparison microscope used by forensic experts for toolmark comparisons. In order to allow an assessment of the influence of different lighting conditions, all images were captured using a ring light with 11 different lighting settings. Further, matching image regions in the toolmark images were manually annotated. In addition to the annotated toolmark images and the annotation tool, extracted toolmark patches are provided for training and testing to allow a quantitative comparison of the performance of different similarity measures. Finally, results from an evaluation using a publicly available state-of-the-art image descriptor based on deep learning are presented to provide a baseline for future publications.\r"
  },
  "cvpr2017_w28_ac3d-basedconvolutionalneuralnetworkforframedroppingdetectioninasinglevideoshot": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "A C3D-Based Convolutional Neural Network for Frame Dropping Detection in a Single Video Shot",
    "authors": [
      "Chengjiang Long",
      "Eric Smith",
      "Arslan Basharat",
      "Anthony Hoogs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Hoogs_A_C3D-Based_Convolutional_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Hoogs_A_C3D-Based_Convolutional_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Frame dropping is a type of video manipulation where consecutive frames are deleted to omit content from the original video. Automatically detecting dropped frames across a large archive of videos while maintaining a low false alarm rate is a challenging task in digital video forensics. We propose a new approach for forensic analysis by exploiting the local spatio-temporal relationships within a portion of a video to robustly detect frame removals. In this paper, we propose to adapt the Convolutional 3D Neural Network (C3D) for frame drop detection. In order to further suppress the errors due by the network, we produce a refined video-level confidence score and demonstrate that it is superior to the raw output scores from the network. We conduct experiments on two challenging video datasets containing rapid camera motion and zoom changes. The experimental results clearly demonstrate the efficacy of the proposed approach.\r"
  },
  "cvpr2017_w28_spottingaudio-visualinconsistencies(savi)inmanipulatedvideo": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Media Forensics",
    "title": "Spotting Audio-Visual Inconsistencies (SAVI) in Manipulated Video",
    "authors": [
      "Robert Bolles",
      "J. Brian Burns",
      "Martin Graciarena",
      "Andreas Kathol",
      "Aaron Lawson",
      "Mitchell McLaren",
      "Thomas Mensink"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/html/Mensink_Spotting_Audio-Visual_Inconsistencies_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w28/papers/Mensink_Spotting_Audio-Visual_Inconsistencies_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper is part of a larger effort to detect manipulations of video by searching for and combining the evidence of multiple types of inconsistencies between the audio and visual channels. Here, we focus on inconsistencies between the type of scenes detected in the audio and visual modalities (e.g., audio indoor, small room versus visual outdoor, urban), and inconsistencies in speaker identity tracking over a video given audio speaker features and visual face features (e.g., a voice change, but no talking face change). The scene inconsistency task was complicated by mismatches in the categories used in current visual scene and audio scene collections. To deal with this, we employed a novel semantic mapping method. The speaker identity inconsistency process was challenged by the complexity of comparing face tracks and audio speech clusters, requiring a novel method of fusing these two sources. Our progress on both tasks was demonstrated on two collections of tampered videos.\r"
  },
  "cvpr2017_w29_explorationofsocialandwebimagesearchresultsusingtensordecomposition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Tensor Methods in Computer Vision",
    "title": "Exploration of Social and Web Image Search Results Using Tensor Decomposition",
    "authors": [
      "Liuqing Yang",
      "Evangelos E. Papalexakis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/html/Yang_Exploration_of_Social_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/papers/Yang_Exploration_of_Social_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " How do socially popular images differ from authoritative images indexed by web search engines? Empirically, social images on e.g., Twitter often tend to look more diverse and ultimately more \"personal\", contrary to images that are returned by web image search, some of which are so-called \"stock\" images. Are there image features, that we can automatically learn, which differentiate the two types of image search results, or features that the two have in common? This paper outlines the vision towards achieving this result. We propose a tensor-based approach that learns key features of social and web image search results, and provides a comprehensive framework for analyzing and understanding the similarities and differences between the two types types of content. We demonstrate our preliminary results on a small-scale study, and conclude with future research directions for this exciting and novel application.\r"
  },
  "cvpr2017_w29_graph-regularizedgeneralizedlow-rankmodels": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Tensor Methods in Computer Vision",
    "title": "Graph-Regularized Generalized Low-Rank Models",
    "authors": [
      "Mihir Paradkar",
      "Madeleine Udell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/html/Paradkar_Graph-Regularized_Generalized_Low-Rank_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/papers/Paradkar_Graph-Regularized_Generalized_Low-Rank_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Image data is frequently extremely large and oftentimes pixel values are occluded or observed with noise. Additionally, images can be related to each other, as in images of a particular individual. This method augments the recently proposed Generalized Low Rank Model (GLRM) framework with graph regularization, which flexibly models relationships between images. For example, relationships might include images that change slowly over time (as in video or surveillance data), images of the same individual, or diagnostic images which picture the same medical condition. This paper proposes a fast optimization method to solve these graph-regularized GLRMs, which we have released as an open-source software library. We demonstrate that the method outperforms competing methods on a variety of data sets, and show how to use this method to classify and group images.\r"
  },
  "cvpr2017_w29_exploringthegranularityofsparsityinconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Tensor Methods in Computer Vision",
    "title": "Exploring the Granularity of Sparsity in Convolutional Neural Networks",
    "authors": [
      "Huizi Mao",
      "Song Han",
      "Jeff Pool",
      "Wenshuo Li",
      "Xingyu Liu",
      "Yu Wang",
      "William J. Dally"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/html/Mao_Exploring_the_Granularity_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/papers/Mao_Exploring_the_Granularity_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Granularity of sparsity affects the prediction accuracy of Deep Neural Network models. In this paper we quantitatively measure the accuracy-sparsity relationship with different grain sizes. The results validate the previous observations that larger grain size leads to worse accuracy. However, due to the index saving effect, coarse-grained sparsity is able to obtain similar or even better compression rates than fine-grained sparsity at the same accuracy threshold. Our analysis, which is based on the framework of recent sparse convolutional neural network(SCNN) accelerator, further demonstrates that it saves 30% - 35% of memory references compared with fine-grained ones. \r"
  },
  "cvpr2017_w29_humanactionrecognitionusingtensordynamicalsystemmodeling": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Tensor Methods in Computer Vision",
    "title": "Human Action Recognition Using Tensor Dynamical System Modeling",
    "authors": [
      "Chan-Su Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/html/Lee_Human_Action_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/papers/Lee_Human_Action_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper presents a new framework for human action classification using a tensor dynamical model of human action from 3-dimensional (3D) volume sequences and distance measurement on Grassmann manifold . The tensor dynamical model is an extension of linear dynamical models for multi-dimensional sequence analysis. Each subdimensional linear dynamic models are estimated from tensor sequences using an iterative expectation-maximization (EM) algorithm after projection of tensor sequence to each dimensional axis. The combination of distances on Grassmann manifold of linear dynamic systems in each dimension of the tensor dynamic model provides similarity measurement between two tensor dynamical systems. The proposed approach can be applied to 3D depth or convex hull data as well as 2D video image sequences. Experimental results show good performance in human action recognition from INRIA multiview human action database.\r"
  },
  "cvpr2017_w29_tensorcontractionlayersforparsimoniousdeepnets": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Tensor Methods in Computer Vision",
    "title": "Tensor Contraction Layers for Parsimonious Deep Nets",
    "authors": [
      "Jean Kossaifi",
      "Aran Khanna",
      "Zachary Lipton",
      "Tommaso Furlanello",
      "Anima Anandkumar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/html/Kossaifi_Tensor_Contraction_Layers_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w29/papers/Kossaifi_Tensor_Contraction_Layers_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Tensors offer a natural representation for many kinds of data frequently encountered in machine learning. Images, for example, are naturally represented as third order tensors, where the modes correspond to height, width, and channels. In particular, tensor decompositions are noted for their ability to discover multi-dimensional dependencies and produce compact low-rank approximations of data. In this paper, we explore the use of tensor contractions as neural network layers and investigate several ways to apply them to activation tensors. Specifically, we propose the Tensor Contraction Layer (TCL), the first attempt to incorporate tensor contractions as end-to-end trainable neural network layers. Applied to existing networks, TCLs reduce the dimensionality of the activation tensors and thus the number of model parameters. We evaluate the TCL on the task of image recognition, augmenting popular networks (AlexNet, VGG). The resulting models are trainable end-to-end. We evaluate TCL's performance on the task of image recognition, using the CIFAR100 and ImageNet datasets, studying the effect of parameter reduction via tensor contraction on performance. We demonstrate significant model compression without significant impact on the accuracy and, in some cases, improved performance.\r"
  },
  "cvpr2017_w33_estimationofaffectivelevelinthewildwithmultiplememorynetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Estimation of Affective Level in the Wild With Multiple Memory Networks",
    "authors": [
      "Jianshu Li",
      "Yunpeng Chen",
      "Shengtao Xiao",
      "Jian Zhao",
      "Sujoy Roy",
      "Jiashi Feng",
      "Shuicheng Yan",
      "Terence Sim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Li_Estimation_of_Affective_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Li_Estimation_of_Affective_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper presents the proposed solution to the \"affect in the wild\" challenge,which aims to estimate the affective level, i.e. the valence and arousal values, of every frame in a video. A carefully designed deep convolutional neural network (a variation of residual network) for affective level estimation of facial expressions is first implemented as a baseline. Next we use multiple memory networks to model the temporal relations between the frames. Finally ensemble models are used to combine the predictions from multiple memory networks. Our proposed solution outperforms the baseline model by a factor of 10.62% in terms of mean square error (MSE).\r"
  },
  "cvpr2017_w33_facialaffectestimationinthewildusingdeepresidualandconvolutionalnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Facial Affect Estimation in the Wild Using Deep Residual and Convolutional Networks",
    "authors": [
      "Behzad Hasani",
      "Mohammad H. Mahoor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Hasani_Facial_Affect_Estimation_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Hasani_Facial_Affect_Estimation_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Automated affective computing in the wild is a challenging task in the field of computer vision. This paper presents three neural network-based methods proposed for the task of facial affect estimation submitted to the First Affect-in-the-Wild challenge. These methods are based on Inception-ResNet modules redesigned specifically for the task of facial affect estimation. These methods are: Shallow Inception-ResNet, Deep Inception-ResNet, and Inception-ResNet with LSTMs. These networks extract facial features in different scales and simultaneously estimate both the valence and arousal in each frame. Root Mean Square Error (RMSE) rates of 0.4 and 0.3 are achieved for the valence and arousal respectively with corresponding Concordance Correlation Coefficient (CCC) rates of 0.04 and 0.29 using Deep Inception-ResNet method.\r"
  },
  "cvpr2017_w33_fatauva-netanintegrateddeeplearningframeworkforfacialattributerecognition,actionunitdetection,andvalence-arousalestimation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "FATAUVA-Net : An Integrated Deep Learning Framework for Facial Attribute Recognition, Action Unit Detection, and Valence-Arousal Estimation",
    "authors": [
      "Wei-Yi Chang",
      "Shih-Huan Hsu",
      "Jen-Hsien Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Chang_FATAUVA-Net__An_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Chang_FATAUVA-Net__An_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Facial expression recognition has been investigated for many years, and there are two popular models: Action Units (AUs) and the Valence-Arousal space (V-A space) that have been widely used. However, most of the databases for estimating V-A intensity are captured in laboratory settings, and the benchmarks \"in-the-wild\" do not exist. Thus, the First Affect-In-The-Wild Challenge released a database for V-A estimation while the videos were captured in wild condition. In this paper, we propose an integrated deep learning framework for facial attribute recognition, AU detection, and V-A estimation. The key idea is to apply AUs to estimate the V-A intensity since both AUs and V-A space could be utilized to recognize some emotion categories. Besides, the AU detector is trained based on the convolutional neural network (CNN) for facial attribute recognition. In experiments, we will show the results of the above three tasks to verify the performances of our proposed network framework.\r"
  },
  "cvpr2017_w33_recognitionofaffectinthewildusingdeepneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Recognition of Affect in the Wild Using Deep Neural Networks",
    "authors": [
      "Dimitrios Kollias",
      "Mihalis A. Nicolaou",
      "Irene Kotsia",
      "Guoying Zhao",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Kollias_Recognition_of_Affect_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Kollias_Recognition_of_Affect_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper we utilize the first large-scale \"in-the-wild\" (Aff-Wild) database, which is annotated in terms of the valence-arousal dimensions, to train and test an end-to-end deep neural architecture for the estimation of continuous emotion dimensions based on visual cues.The proposed architecture is based on jointly training convolutional (CNN) and recurrent neural network (RNN) layers, thusexploiting both the invariant properties of convolutional features, while also modelling temporal dynamics that arise in human behaviour via the recurrent layers. Various pre-trained networks are used as starting structures which are subsequently appropriately fine-tuned to the Aff-Wild database. Obtained results show premise for the utilization of deep architectures for the visual analysis of human behaviour in terms of continuous emotion dimensions and analysis of different types of affect. \r"
  },
  "cvpr2017_w33_aff-wildvalenceandarousalin-the-wildchallenge": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Aff-Wild: Valence and Arousal 'In-The-Wild'  Challenge",
    "authors": [
      "Stefanos Zafeiriou",
      "Dimitrios Kollias",
      "Mihalis A. Nicolaou",
      "Athanasios Papaioannou",
      "Guoying Zhao",
      "Irene Kotsia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Zafeiriou_Aff-Wild_Valence_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Zafeiriou_Aff-Wild_Valence_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " TheAffect-in-the-Wild (Aff-Wild) Challenge proposes a new comprehensive benchmark for assessing the performance of facial affect/behaviour analysis/understanding `in-the-wild'. The Aff-wild benchmark contains about 300 videos (over 2,000 minutes of data) annotated with regards to valence and arousal, all captured `in-the-wild' (the main source being Youtube videos). The paper presents the database description, the experimental set up, the baseline method used for the Challenge and finally the summary of the performance of the different methods submitted to the Affect-in-the-Wild Challenge for Valence and Arousal estimation. The challenge demonstrates that meticulously designed deep neural networks can achieve very good performance when trained with in-the-wild data.\r"
  },
  "cvpr2017_w33_deepanalysisoffacialbehavioraldynamics": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Deep Analysis of Facial Behavioral Dynamics",
    "authors": [
      "Lazaros Zafeiriou",
      "Stefanos Zafeiriou",
      "Maja Pantic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Zafeiriou_Deep_Analysis_of_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Zafeiriou_Deep_Analysis_of_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Modelling of facial dynamics, as well as recovering of latent dimensions that correspond to facial dynamics is of paramount importance for many tasks relevant to facial behaviour analysis. Currently, analysis of facial dynamics is performed by applying linear techniques, mainly, on sparse facial tracks. In this, paper we propose the first, to the best of our knowledge, methodology for extracting lowdimensional latent dimensions that correspond to facial dynamics (i.e., motion of facial parts). To this end we develop appropriate unsupervised and supervised deep autoencoder architectures, which are able to extract features that correspond to the facial dynamics. We demonstrate the usefulness of the proposed approach in various facial behaviour datasets.\r"
  },
  "cvpr2017_w33_agedbthefirstmanuallycollected,in-the-wildagedatabase": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "AgeDB: The First Manually Collected, In-The-Wild Age Database",
    "authors": [
      "Stylianos Moschoglou",
      "Athanasios Papaioannou",
      "Christos Sagonas",
      "Jiankang Deng",
      "Irene Kotsia",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Moschoglou_AgeDB_The_First_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Moschoglou_AgeDB_The_First_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Over the last few years, increased interest has arisen with respect to age-related tasks in the Computer Vision community. As a result, several \"in-the-wild\" databases annotated with respect to the age attribute became available in the literature. Nevertheless, one major drawback of these databases is that they are semi-automatically collected and annotated and thus they contain noisy labels. Therefore, the algorithms that are evaluated in such databases are prone to noisy estimates. In order to overcome such drawbacks, we present in this paper the first, to the best of knowledge, manually collected \"in-the-wild\" age database, dubbed AgeDB, containing images annotated with accurate to the year, noise-free labels. As demonstrated by a series of experiments utilizing state-of-the-art algorithms, this unique property renders AgeDB suitable when performing experiments on age-invariant face verification, age estimation and face age progression \"in-the-wild\".\r"
  },
  "cvpr2017_w33_marginallossfordeepfacerecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Marginal Loss for Deep Face Recognition",
    "authors": [
      "Jiankang Deng",
      "Yuxiang Zhou",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Deng_Marginal_Loss_for_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Deng_Marginal_Loss_for_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Convolutional neural networks have significantly boosted the performance of face recognition in recent years due to its high capacity in learning discriminative features. In order to enhance the discriminative power of the deeply learned features, we propose a new supervision signal named marginal loss for deep face recognition. Specifically, the marginal loss simultaneously minimises the intra-class variances as well as maximises the inter-class distances by focusing on the marginal samples. With the joint supervision of softmax loss and marginal loss, we can easily train a robust CNNs to obtain more discriminative deep features. Extensive experiments on several relevant face recognition benchmarks, Labelled Faces in the Wild (LFW), YouTube Faces (YTF), Cross-Age Celebrity Dataset (CACD), Age Database (AgeDB) and MegaFace Challenge, prove the effectiveness of the proposed marginal loss. \r"
  },
  "cvpr2017_w33_deepfacedeblurring": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Deep Face Deblurring",
    "authors": [
      "Grigorios G. Chrysos",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Chrysos_Deep_Face_Deblurring_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Chrysos_Deep_Face_Deblurring_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Blind deblurring consists a long studied task, however the outcomes of generic methods are not effective in real world blurred images. Domain-specific methods for deblurring targeted object categories, frequently outperform their generic counterparts. In this work, we develop such a domain-specific method to tackle deblurring of human faces, henceforth referred to as face deblurring. Studying faces is of tremendous significance in computer vision, however face deblurring has yet to demonstrate some convincing results. This can be partly attributed to the combination of i) poor texture and ii) highly structure shape that yield the contour/gradient priors (that are typically used) sub-optimal. We adopt a learning approach by inserting weak supervision that exploits the well-documented structure of the face. We introduce an efficient framework that allows the generation of a large dataset. We utilised this framework to create 2MF^2, a dataset of over two million frames.\r"
  },
  "cvpr2017_w33_stackedhourglassnetworkforrobustfaciallandmarklocalisation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Stacked Hourglass Network for Robust Facial Landmark Localisation",
    "authors": [
      "Jing Yang",
      "Qingshan Liu",
      "Kaihua Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Yang_Stacked_Hourglass_Network_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Yang_Stacked_Hourglass_Network_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " With the increasing number of public available training data for face alignment, the regression-based methods attracted much attention and have become the dominant methods to solve this problem.There are two main factors, the variance of the regression target and the capacity of the regression model,affecting the performance of the regression task. In this paper, we present a Stacked Hourglass Network for robust facial landmark localisation.We first adopt a supervised face transformation to remove the translation, scale and rotation variation of each face, in order to reduce the variance of the regression target.Then we employ a deep convolutional neural network named Stacked Hourglass Network to increase the capacity of the regression model.To better evaluate the proposed method, we reimplement two popular cascade shape regression models, SDM and LBF, for comparison. Extensive experiments on four challenging datasets prove the effectiveness of the proposed method. \r"
  },
  "cvpr2017_w33_deepalignmentnetworkaconvolutionalneuralnetworkforrobustfacealignment": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Deep Alignment Network: A Convolutional Neural Network for Robust Face Alignment",
    "authors": [
      "Marek Kowalski",
      "Jacek Naruniec",
      "Tomasz Trzcinski"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Kowalski_Deep_Alignment_Network_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Kowalski_Deep_Alignment_Network_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we propose Deep Alignment Network (DAN), a robust face alignment method based on a deep neural network architecture. DAN consists of multiple stages, where each stage improves the locations of the facial landmarks estimated by the previous stage. Our method uses entire face images at all stages, contrary to the recently proposed face alignment methods that rely on local patches. This is possible thanks to the use of landmark heatmaps which provide visual information about landmark locations estimated at the previous stages of the algorithm. The use of entire face images rather than patches allows DAN to handle face images with large variation in head pose and difficult initializations. An extensive evaluation on two publicly available datasets shows that DAN reduces the state-of-the-art failure rate by up to 70%. Our method has also been submitted for evaluation as part of the Menpo challenge.\r"
  },
  "cvpr2017_w33_robustfec-cnnahighaccuracyfaciallandmarkdetectionsystem": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Robust FEC-CNN: A High Accuracy Facial Landmark Detection System",
    "authors": [
      "Zhenliang He",
      "Jie Zhang",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/He_Robust_FEC-CNN_A_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/He_Robust_FEC-CNN_A_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Facial landmark detection is a typical and crucial task in computer vision widely used in face recognition, facial expression analysis, etc. In this work, we propose an effective facial landmark detection system, recorded as Robust FEC-CNN (RFC), which achieves impressive results on facial landmark detection in the wild. Considering the favorable ability of deep convolutional neural network, we resort to FEC-CNN as a basic method to characterize the complex nonlinearity from face appearance to shape. Moreover, face bounding box invariant technique is adopted to reduce the landmark localization sensitivity to the face detector while model ensemble strategy is adopted to further enhance the landmark localization performance. We participate theMenpo Facial Landmark Localisation in-the-Wild Challenge and our RFC significantly outperforms the baseline approach APS. Extensive experiments on Menpo Challenge dataset and IBUG dataset demonstrate the superior performance of the proposed RFC. \r"
  },
  "cvpr2017_w33_convolutionalexpertsconstrainedlocalmodelforfaciallandmarkdetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Convolutional Experts Constrained Local Model for Facial Landmark Detection",
    "authors": [
      "Amir Zadeh",
      "Tadas Baltrusaitis",
      "Louis-Philippe Morency"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Zadeh_Convolutional_Experts_Constrained_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Zadeh_Convolutional_Experts_Constrained_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Constrained Local Models (CLMs) are a well- established family of methods for facial landmark detection. However, they have recently fallen out of favor to cascaded regression-based approaches. This is in part due to the inability of existing CLM local detectors to model the very complex individual landmark appearance that is affected by expression, illumination, facial hair, makeup, and accessories. In our work, we present a novel local detector - Convolutional Experts Network (CEN) - that brings together the advantages of neural architectures and mixtures of experts in an end-to-end framework. We further propose a Convolutional Experts Constrained Local Model (CE-CLM) algorithm that uses CEN as local detectors. We demonstrate that our proposed CE-CLM algorithm outperforms competitive state-of-the-art baselines for facial landmark detection by a large margin. Our approach is especially accurate and robust on challenging profile images. \r"
  },
  "cvpr2017_w33_3d-assistedcoarse-to-fineextreme-posefaciallandmarkdetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "3D-Assisted Coarse-To-Fine Extreme-Pose Facial Landmark Detection",
    "authors": [
      "Shengtao Xiao",
      "Jianshu Li",
      "Yunpeng Chen",
      "Zhecan Wang",
      "Jiashi Feng",
      "Shuicheng Yan",
      "Ashraf Kassim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Xiao_3D-Assisted_Coarse-To-Fine_Extreme-Pose_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Xiao_3D-Assisted_Coarse-To-Fine_Extreme-Pose_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We propose a novel 3D-assisted coarse-to-fine extreme-pose facial landmark detection system in this work. For a given face image, our system first refines the face bounding box with landmark locations inferred from a 3D face model generated by a Recurrent 3D Regressor at coarse level. Another R3R is then employed to fit a 3D face model onto the 2D face image cropped with the refined bounding box at fine-scale.2D landmark locations inferred from the fitted 3D face are further adjusted with the popular 2D regression method, i.e. LBF. The 3D-assisted coarse-to-fine strategy and the 2D adjustment process explicitly ensure both the robustness to extreme face poses and bounding box disturbance and the accuracy towards pixel-level landmark displacement. Extensive experiments on the Menpo Challenge test sets demonstrate the superior performance of our system.\r"
  },
  "cvpr2017_w33_unconstrainedfacealignmentwithoutfacedetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Unconstrained Face Alignment Without Face Detection",
    "authors": [
      "Xiaohu Shao",
      "Junliang Xing",
      "Jiangjing Lv",
      "Chunlin Xiao",
      "Pengcheng Liu",
      "Youji Feng",
      "Cheng Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Shao_Unconstrained_Face_Alignment_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Shao_Unconstrained_Face_Alignment_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper introduces our submission to the 2nd Facial Landmark Localisation Competition. We present a deep architecture to directly detect facial landmarks without using face detection as an initialization. The architecture consists of two stages, a Basic Landmark Prediction Stage and a Whole Landmark Regression Stage. At the former stage, given an input image, the basic landmarks of all faces are detected by a sub-network of landmark heatmap and affinity field prediction. At the latter stage, the coarse canonical face and the pose can be generated by a Pose Splitting Layer based on the visible basic landmarks. According to its pose, each canonical state is distributed to the corresponding branch of the shape regression sub-networks for the whole landmark detection. Experimental results show that our method obtains promising results on the 300-W dataset, and achieves superior performances over the baselines of the semi-frontal and the profile categories in this competition.\r"
  },
  "cvpr2017_w33_multi-scalefullyconvolutionalnetworkforfacedetectioninthewild": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Multi-Scale Fully Convolutional Network for Face Detection in the Wild",
    "authors": [
      "Yancheng Bai",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Bai_Multi-Scale_Fully_Convolutional_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Bai_Multi-Scale_Fully_Convolutional_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Face detection is a classical problem in computer vision. It is still a difficult task due to many nuisances that naturally occur in the wild. In this paper, we propose a multi-scale fully convolutional network for face detection. To reduce computation, the intermediate convolutional feature maps (conv) are shared by every scale model. We up-sample and down-sample the final conv map to approximate K levels of a feature pyramid, leading to a wide range of face scales that can be detected. At each feature pyramid level, a FCN is trained end-to-end to deal with faces in a small range of scale change. Because of the up-sampling, our method can detect very small faces (10x10 pixels). We test our MS-FCN detector on four public face detection datasets, including FDDB, WIDER FACE, AFW and PASCAL FACE. Extensive experiments show that it outperforms state-of-the-art methods. Also, MS-FCN runs at 23 FPS on a GPU for images of size 640x480 with no assumption on the minimum detectable face size.\r"
  },
  "cvpr2017_w33_delvingdeepintocoarse-to-fineframeworkforfaciallandmarklocalization": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Delving Deep Into Coarse-To-Fine Framework for Facial Landmark Localization",
    "authors": [
      "Xi Chen",
      "Erjin Zhou",
      "Yuchen Mo",
      "Jiancheng Liu",
      "Zhimin Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Chen_Delving_Deep_Into_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Chen_Delving_Deep_Into_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper we proposed a 4-stage coarse-to-fine framework to tackle the facial landmark localization problem in-the-wild. In our system, we first predict the landmark key points on a coarse level of granularity, which sets a good initialization for the whole framework. Then we group the key points into several components and refine each component with local patches cropped within them. After that we further refine them separately. Each key point is further refined with multi-scale local patches cropped according to its nearest 3-, 5-, and 7-neighbors respectively. The results are fused by an attention gate network. Since a different key-point configuration is adopted in our labeled dataset, a linear transformation is finally learned with the least square approximation to adapt our predictions to the competition's task.\r"
  },
  "cvpr2017_w33_leveragingintraandinter-datasetvariationsforrobustfacealignment": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Leveraging Intra and Inter-Dataset Variations for Robust Face Alignment",
    "authors": [
      "Wenyan Wu",
      "Shuo Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Wu_Leveraging_Intra_and_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Wu_Leveraging_Intra_and_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Face alignment is a critical topic in the computer vision community. Numerous efforts have been made and various benchmark datasets have been released in recent decades. However, two significant issues remain in recent datasets, e.g., Intra-Dataset Variation and Inter-Dataset Variation. Inter-Dataset Variation refers to bias on expression, head pose, etc. inside one certain dataset, while Intra-Dataset Variation refers to different bias across different datasets. To address the mentioned problems, we proposed a novel Deep Variation Leveraging Network (DVLN), which consists of two strong coupling sub-networks, e.g., Dataset-Across Network (DA-Net) and Candidate-Decision Network (CD-Net). Extensive evaluations show that our approach demonstrates real-time performance and dramatically outperforms state-of-the-art methods on the challenging 300-W dataset.\r"
  },
  "cvpr2017_w33_facedetection,boundingboxaggregationandposeestimationforrobustfaciallandmarklocalisationinthewild": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "Face Detection, Bounding Box Aggregation and Pose Estimation for Robust Facial Landmark Localisation in the Wild",
    "authors": [
      "Zhen-Hua Feng",
      "Josef Kittler",
      "Muhammad Awais",
      "Patrik Huber",
      "Xiao-Jun Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Feng_Face_Detection_Bounding_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Feng_Face_Detection_Bounding_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present a framework for robust face detection and landmark localisation of faces in the wild, which has been evaluated as part of `the 2nd Facial Landmark Localisation Competition'. The framework has four stages: face detection, bounding box aggregation, pose estimation and landmark localisation. To achieve a high detection rate, we use two publicly available CNN-based face detectors and two proprietary detectors. We aggregate the detected face bounding boxes of each input image to reduce false positives and improve face detection accuracy. A cascaded shape regressor, trained using faces with a variety of pose variations, is then employed for pose estimation and image pre-processing. Last, we train the final cascaded shape regressor for fine-grained landmark localisation, using a large number of training samples with limited pose variations. The experimental results obtained on the 300W and Menpo benchmarks demonstrate the superiority of our framework over state-of-the-art methods.\r"
  },
  "cvpr2017_w33_themenpofaciallandmarklocalisationchallengeasteptowardsthesolution": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Faces \"In-The-Wild\" Workshop-Challenge",
    "title": "The Menpo Facial Landmark Localisation Challenge: A Step Towards the Solution",
    "authors": [
      "Stefanos Zafeiriou",
      "George Trigeorgis",
      "Grigorios Chrysos",
      "Jiankang Deng",
      "Jie Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/html/Zafeiriou_The_Menpo_Facial_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w33/papers/Zafeiriou_The_Menpo_Facial_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we present a new benchmark (Menpo benchmark) for facial landmark localisation and summarise the results of the recent competition, so-called Menpo Challenge, held in conjunction to CVPR 2017. The Menpo benchmark, contrary to the previous benchmarks such as 300-W and 300-VW, contains facial images both in (nearly) frontal, as well as in profile pose (annotated with a different markup of facial landmarks). Furthermore, we increase considerably the number of annotated images so that deep learning algorithms can be robustly applied to the problem.\r"
  },
  "cvpr2017_w34_pets2017datasetandchallenge": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - BMTT-PETS Workshop on Tracking and Surveillance",
    "title": "PETS 2017: Dataset and Challenge",
    "authors": [
      "Luis Patino",
      "Tahir Nawaz",
      "Tom Cane",
      "James Ferryman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/html/Patino_PETS_2017_Dataset_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/papers/Patino_PETS_2017_Dataset_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper indicates the dataset and challenges evaluated under PETS2017. In this edition PETS continues the evaluation theme of on-board surveillance systems for protection of mobile critical assets as set in PETS 2016. The datasets include (1) the ARENA Dataset; an RGB camera dataset, as used for PETS2014 to PETS 2016, which addresses protection of trucks; and (2) the IPATCH Dataset; a multi sensor dataset, as used in PETS2016, addressing the application of multi sensor surveillance to protect a vessel at sea from piracy. The datasets allow for performance evaluation of tracking in low-density scenarios and detection of various surveillance events ranging from innocuous abnormalities to dangerous and criminal situations. Training data for tracking algorithms is released with the dataset; tracking data is also available for authors addressing only surveillance event detection challenges but not working on tracking.\r"
  },
  "cvpr2017_w34_comaltrackingtrackingpointsattheobjectboundaries": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - BMTT-PETS Workshop on Tracking and Surveillance",
    "title": "CoMaL Tracking: Tracking Points at the Object Boundaries",
    "authors": [
      "Santhosh K. Ramakrishnan",
      "Swarna Kamlam Ravindran",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/html/Ramakrishnan_CoMaL_Tracking_Tracking_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/papers/Ramakrishnan_CoMaL_Tracking_Tracking_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Traditional point tracking algorithms such as the KLT use local 2D information aggregation for feature detection and tracking, due to which their performance degrades at the object boundaries that separate multiple objects. Recently, CoMaL Features have been proposed that handle such a case. However, they proposed a simple tracking framework where the points are re-detected in each frame and matched. This is inefficient and may also lose many points that are not re-detected in the next frame. We propose a novel tracking algorithm to accurately and efficiently track CoMaL points. For this, the level line segment associated with the CoMaL points is matched to MSER segments in the next frame using shape-based matching and the matches are further filtered using texture-based matching. Experiments show improvements over a simple re-detect-and-match framework as well as KLT in terms of speed/accuracy on different real-world applications, especially at the object boundaries. \r"
  },
  "cvpr2017_w34_enhancingdetectionmodelformultiplehypothesistracking": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - BMTT-PETS Workshop on Tracking and Surveillance",
    "title": "Enhancing Detection Model for Multiple Hypothesis Tracking",
    "authors": [
      "Jiahui Chen",
      "Hao Sheng",
      "Yang Zhang",
      "Zhang Xiong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/html/Chen_Enhancing_Detection_Model_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/papers/Chen_Enhancing_Detection_Model_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Tracking-by-detection has become a popular tracking paradigm in recent years. Due to the fact that detections within this framework are regarded as points in the tracking process, it brings data association ambiguities, especially in crowded scenarios. To cope with this issue, we extended the multiple hypothesis tracking approach by incorporating a novel enhancing detection model that included detection-scene analysis and detection-detection analysis; the former models the scene by using dense confidential detections and handles false trajectories, while the latter estimates the correlations between individual detections and improves the ability to deal with close object hypotheses in crowded scenarios. Our approach was tested on the MOT16 benchmark and achieved competitive results with current state-of-the-art trackers.\r"
  },
  "cvpr2017_w34_okutama-actionanaerialviewvideodatasetforconcurrenthumanactiondetection": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - BMTT-PETS Workshop on Tracking and Surveillance",
    "title": "Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action Detection",
    "authors": [
      "Mohammadamin Barekatain",
      "Miquel Marti",
      "Hsueh-Fu Shih",
      "Samuel Murray",
      "Kotaro Nakayama",
      "Yutaka Matsuo",
      "Helmut Prendinger"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/html/Barekatain_Okutama-Action_An_Aerial_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/papers/Barekatain_Okutama-Action_An_Aerial_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Despite significant progress in the development of human action detection datasets and algorithms, no current dataset is representative of real-world aerial view scenarios. We present Okutama-Action, a new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors. As a result, our dataset is more challenging than existing ones, and will help push the field forward to enable real-world applications.\r"
  },
  "cvpr2017_w34_abnormaleventdetectiononbmtt-pets2017surveillancechallenge": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - BMTT-PETS Workshop on Tracking and Surveillance",
    "title": "Abnormal Event Detection on BMTT-PETS 2017 Surveillance Challenge",
    "authors": [
      "Kothapalli Vignesh",
      "Gaurav Yadav",
      "Amit Sethi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/html/Vignesh_Abnormal_Event_Detection_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/papers/Vignesh_Abnormal_Event_Detection_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we have proposed a method to detect abnormal events for human group activities. Our main contribution is to develop a strategy that learns with very few videos by isolating the action and by using supervised learning. First, we subtract the background of each frame by modeling each pixel as a mixture of Gaussians(MoG) to concatenate the higher order learning only on the foreground. Next, features are extracted from each frame using a convolutional neural network (CNN) that is trained to classify between normal and abnormal frames. These feature vectors are fed into long short term memory (LSTM) network to learn the long-term dependencies between frames. The LSTM is also trained to classify abnormal frames, while extracting the temporal features of the frames. Finally, we classify the frames as abnormal or normal depending on the output of a linear SVM, whose input are the features computed by the LSTM.\r"
  },
  "cvpr2017_w34_loiteringbehaviourdetectionofboatsatsea": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - BMTT-PETS Workshop on Tracking and Surveillance",
    "title": "Loitering Behaviour Detection of Boats at Sea",
    "authors": [
      "Luis Patino",
      "James Ferryman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/html/Patino_Loitering_Behaviour_Detection_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w34/papers/Patino_Loitering_Behaviour_Detection_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We present in this paper a technique for Loitering detection based on the analysis of activity zones of the monitored area. Activity zones are learnt online employing a soft computing-based algorithm which takes as input the trajectory of object mobiles appearing on the scene. Statistical properties on zone occupancy and transition between zones makes it possible to discover abnormalities without the need to learn abnormal models beforehand. We have applied this approch to the PETS2017 IPATCH dataset and addressed the challenge on detecting skiff boats loitering around a protected ship, which eventually is attacked by the skiffs. Our results show that we can detect the suspicious behaviour on time to trigger an early warning.\r"
  },
  "cvpr2017_w37_concurrence-awarelongshort-termsub-memoriesforperson-personactionrecognition": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition",
    "authors": [
      "Xiangbo Shu",
      "Jinhui Tang",
      "Guo-Jun Qi",
      "Yan Song",
      "Zechao Li",
      "Liyan Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Shu_Concurrence-Aware_Long_Short-Term_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Shu_Concurrence-Aware_Long_Short-Term_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamics for single-person action recognition. However, existing RNN models only focus on capturing the temporal dynamics of the person-person interactions by naively combining the activity dynamics of individuals or modeling them as a whole. This neglects the inter-related dynamics of how person-person interactions change over time. To this end, we propose a novel Concurrent Long Short-Term Memories (Co-LSTM) to model the long-term inter-related dynamics between two interacting people on the bonding boxes covering people. Specifically, for each frame, two sub-memory units store individual motion information, while a concurrent LSTM unit selectively integrates and stores inter-related motion information between interacting people from these two sub-memory units via a new co-memory cell. In experiments, we show the superior performance of Co-LSTM compared with the state-of-the-arts.\r"
  },
  "cvpr2017_w37_crowd-11adatasetforfinegrainedcrowdbehaviouranalysis": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Crowd-11: A Dataset for Fine Grained Crowd Behaviour Analysis",
    "authors": [
      "Camille Dupont",
      "Luis Tobias",
      "Bertrand Luvison"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Dupont_Crowd-11_A_Dataset_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Dupont_Crowd-11_A_Dataset_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Crowd behaviour analysis is a challenging task in computer vision, mainly due to the high complexity of the interactions between groups and individuals. This task is particularly crucial given the magnitude of manual monitoring required for effective crowd management. Within this context, a key challenge is to conceive a highly generic, fine and context-independent characterisation of crowd behaviours. Since current datasets answer only partially to this problem, a new dataset is generated, with a total of 11 crowd motion patterns and over 6000 video clips with an average length of 100 frames per sequence. We establish the first baseline of crowd characterisation with an extensive evaluation on shallow and deep methods. This characterisation is expected to be useful in multiple crowd analysis circumstances, we present a new deep architecture for crowd characterisation and demonstrate its application in the context of anomaly classification.\r"
  },
  "cvpr2017_w37_temporaldomainneuralencoderforvideorepresentationlearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Temporal Domain Neural Encoder for Video Representation Learning",
    "authors": [
      "Hao Hu",
      "Zhaowen Wang",
      "Joon-Young Lee",
      "Zhe Lin",
      "Guo-Jun Qi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Hu_Temporal_Domain_Neural_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Hu_Temporal_Domain_Neural_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We address the challenge of learning good video representations by explicitly modeling the relationship between visual concepts in time space. We propose a novel Temporal Preserving Recurrent Neural Network (TPRNN) that extracts and encodes visual dynamics with frame-level features as input. The proposed network architecture captures temporal dynamics by keeping track of the ordinal relationship of co-occurring visual concepts, and constructs video representations with their temporal order patterns. The resultant video representations effectively encode temporal information of dynamic patterns, which makes them more discriminative to human actions performed with different sequences of action patterns. We evaluate the proposed model on several real video datasets, and the results show that it successfully outperforms the baseline models. In particular, we observe significant improvement on action classes that can only be distinguished by capturing the temporal orders of action patterns.\r"
  },
  "cvpr2017_w37_recurrentmemoryaddressingfordescribingvideos": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Recurrent Memory Addressing for Describing Videos",
    "authors": [
      "Arnav Kumar Jain",
      "Abhinav Agarwalla",
      "Kumar Krishna Agrawal",
      "Pabitra Mitra"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Jain_Recurrent_Memory_Addressing_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Jain_Recurrent_Memory_Addressing_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " In this paper, we introduce Key-Value Memory Networks to a multimodal setting and a novel key-addressing mechanism to deal with sequence-to-sequence models. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each frame (k) in the video, thereby creating (k, v) memory slots. We propose to find the next step attention weights conditioned on the previous attention distributions for the key-value memory slots in the memory addressing schema. Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Experiments done on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models.\r"
  },
  "cvpr2017_w37_temporallysteeredgaussianattentionforvideounderstanding": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Temporally Steered Gaussian Attention for Video Understanding",
    "authors": [
      "Shagan Sah",
      "Thang Nguyen",
      "Miguel Dominguez",
      "Felipe Petroski Such",
      "Raymond Ptucha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Sah_Temporally_Steered_Gaussian_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Sah_Temporally_Steered_Gaussian_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent advances in video understanding are enabling incredible developments in video search, summarization, automatic captioning and human computer interaction. Attention mechanisms are a powerful way to steer focus onto different sections of the video. Existing mechanisms are driven by prior training probabilities and require input instances of identical temporal duration. We introduce an intuitive video understanding framework which combines continuous attention mechanisms over a family of Gaussian distributions with a hierarchical based video representation. The hierarchical framework enables efficient abstract temporal representations of video. Video attributes steer the attention mechanism intelligently independent of video length. Our fully learnable end-to-end approach helps predict salient temporal regions of action/objects in the video. We demonstrate state-of-the-art captioning results on the popular MSVD, MSR-VTT and M-VAD video datasets. \r"
  },
  "cvpr2017_w37_sanetstructure-awarenetworkforvisualtracking": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "SANet: Structure-Aware Network for Visual Tracking",
    "authors": [
      "Heng Fan",
      "Haibin Ling"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Fan_SANet_Structure-Aware_Network_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Fan_SANet_Structure-Aware_Network_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Convolutional neural network (CNN) has drawn increasing interest in visual tracking owing to its powerfulness in feature extraction. Most existing CNN-based trackers treat tracking as a classification problem. However, these trackers are sensitive to similar distractors because their CNN models mainly focus on inter-class classification. To address this problem, we use self-structure information of object to distinguish it from distractors. Specifically, we utilize recurrent neural network (RNN) to model object structure, and incorporate it into CNN to improve its robustness to similar distractors. Considering that convolutional layers in different levels characterize the object from different perspectives, we use multiple RNNs to model object structure in different levels respectively. Extensive experiments on three benchmarks, OTB100, TC-128 and VOT2015, show that the proposed algorithm outperforms other methods. Code is released at www.dabi.temple.edu/hbling/code/SANet/SANet.html.\r"
  },
  "cvpr2017_w37_fixationpredictioninvideosusingunsupervisedhierarchicalfeatures": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Fixation Prediction in Videos Using Unsupervised Hierarchical Features",
    "authors": [
      "Julius Wang",
      "Hamed R. Tavakoli",
      "Jorma Laaksonen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Wang_Fixation_Prediction_in_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Wang_Fixation_Prediction_in_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " This paper presents a framework for saliency estimation and fixation prediction in videos. The proposed framework is based on a hierarchical feature representation obtained by stacking convolutional layers of independent subspace analysis (ISA) filters. The feature learning is thus unsupervised and independent of the task. To compute the saliency, we then employ a multiresolution saliency architecture that exploits both local and global saliency. That is, for a given image, an image pyramid is initially built. After that, for each resolution, both local and global saliency measures are computed to obtain a saliency map. The integration of saliency maps over the image pyramid provides the final video saliency. We first show that combining local and global saliency improves the results. We then compare the proposed model with several video saliency models and demonstrate that the proposed framework is capable of predicting video saliency effectively, outperforming all the other models.\r"
  },
  "cvpr2017_w37_learninglatenttemporalconnectionismofdeepresidualvisualabstractionsforidentifyingsurgicaltoolsinlaparoscopyprocedures": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Learning Latent Temporal Connectionism of Deep Residual Visual Abstractions for Identifying Surgical Tools in Laparoscopy Procedures",
    "authors": [
      "Kaustuv Mishra",
      "Rachana Sathish",
      "Debdoot Sheet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Mishra_Learning_Latent_Temporal_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Mishra_Learning_Latent_Temporal_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Surgical workflow in minimally invasive interventions like laparoscopy can be modeled with the aid of tool usage information. The video recording during the surgery primarily for viewing the surgical site using endoscope can be leveraged for this purpose without the need for additional sensors or instruments. We propose a method which learns to detect the tool presence in laparoscopy videos by leveraging the temporal information of the systematically executed surgical procedures and higher abstractions of the spatial visual features extracted from the surgical video. We propose a framework consisting of using Convolutional Neural Networks for extracting the visual features and Long Short-Term Memory network to encode the temporal information. The proposed framework has been experimentally verified using a publicly available dataset consisting of 10 training and 5 testing annotated videos with an average accuracy of 88.75% in detection of tool presence. \r"
  },
  "cvpr2017_w37_kernalisedmulti-resolutionconvnetforvisualtracking": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep-Vision: Deep Learning in Computer Vision - Temporal Deep Learning",
    "title": "Kernalised Multi-Resolution Convnet for Visual Tracking",
    "authors": [
      "Di Wu",
      "Wenbin Zou",
      "Xia Li",
      "Yong Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/html/Wu_Kernalised_Multi-Resolution_Convnet_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w37/papers/Wu_Kernalised_Multi-Resolution_Convnet_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Discriminative Correlation Filters (DCF) have demonstrated excellent performance for high-speed visual object tracking. Built upon their seminar work, there has been a plethora of recent improvements relying on convolutional neural network. In this paper, we go beyond the conventional DCF framework and propose a Kernalised Multi-resolution Convnet (KMC) formulation that utilises hierarchical response maps to directly output the target movement. The performance of this transfer learning paradigm is on par with a variety of state-of-the-art approaches and when directly deployed the learnt network to predict the unseen challenging UAV tracking dataset. Moreover, the transfered multi-reslution CNN renders it possible to be integrated into the RNN temporal learning framework, therefore opening the door on the end-to-end temporal deep learning (TDL) for visual tracking.\r"
  },
  "cvpr2017_w41_action-affect-genderclassificationusingmulti-taskrepresentationlearning": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "Action-Affect-Gender Classification Using Multi-Task Representation Learning",
    "authors": [
      "Timothy J. Shields",
      "Mohamed R. Amer",
      "Max Ehrlich",
      "Amir Tamrakar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Tamrakar_Action-Affect-Gender_Classification_Using_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Tamrakar_Action-Affect-Gender_Classification_Using_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recent work in affective computing focused on affect from facial expressions, and not as much on body. This work focuses on body affect. Affect does not occur in isolation. Humans usually couple affect with an action; for example, a person could be running and happy. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect-gender classification. For this paper we choose a probabilistic model, specifically Conditional Restricted Boltzmann Machines, to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component that enables scaling over larger number of classes without increasing the number of parameters. We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art.\r"
  },
  "cvpr2017_w41_dyadgangeneratingfacialexpressionsindyadicinteractions": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "DyadGAN: Generating Facial Expressions in Dyadic Interactions",
    "authors": [
      "Yuchi Huang",
      "Saad M. Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Khan_DyadGAN_Generating_Facial_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Khan_DyadGAN_Generating_Facial_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Generative Adversarial Networks (GANs) have been shown to produce synthetic images of compelling realism. In this work, we present a conditional GAN approach to generate contextually valid facial expressions in dyadic interactions. Contrary to previous work using conditions related to facial attributes of generated identities, we focus on dyads to model the influence of one person's facial expressions to the reaction of the other. We introduce a two level model of GANs in interviewer-interviewee interactions. In the first stage dynamic face sketches of interviewers are generated conditioned on expressions of the interviewee; in the second stage face images are synthesized from the face sketches. We demonstrate that our model is effective at synthesizing visually compelling face images in dyadic interactions. Moreover we quantitatively show that the facial expressions depicted in the generated interviewer face images reflect valid emotional reactions to the interviewee behavior. \r"
  },
  "cvpr2017_w41_exploringcontextualengagementfortraumarecovery": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "Exploring Contextual Engagement for Trauma Recovery ",
    "authors": [
      "Svati Dhamija",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Boult_Exploring_Contextual_Engagement_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Boult_Exploring_Contextual_Engagement_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " A wide range of research has used face data to estimate a person's engagement, in applications from advertising to student learning. An interesting and important question not addressed in prior work is if face-based models of engagement are generalizable and context-free, or do engagement models depend on context and task. This research shows that context-sensitive face-based engagement models are more accurate, at least in the space of web-based tools for trauma recovery. Estimating engagement is important as various psychological studies indicate that engagement is a key component to measure the effectiveness of treatment and can be predictive of behavioral outcomes in many applications. In this paper, we analyze user engagement in a trauma-recovery regime during two separate modules/tasks: relaxation and triggers. The dataset comprises of 8M+ frames from multiple videos collected from 110 subjects, with engagement data coming from 800+ subject self-reports. We build an engagement prediction model as sequence learning from facial Action Units (AUs) using Long Short Term Memory (LSTMs). Our experiments demonstrate that engagement prediction is contextual and depends significantly on the allocated task. Models trained to predict engagement on one task are only weak predictors for another and are much less accurate than context-specific models. Further, we show the interplay of subject mood and engagement using a very short version of Profile of Mood States (POMS) to extend our LSTM model. \r"
  },
  "cvpr2017_w41_facialexpressionrecognitionusingenhanceddeep3dconvolutionalneuralnetworks": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural Networks",
    "authors": [
      "Behzad Hasani",
      "Mohammad H. Mahoor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Mahoor_Facial_Expression_Recognition_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Mahoor_Facial_Expression_Recognition_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Deep Neural Networks (DNNs) have shown to outperform traditional methods in various visual recognition tasks including Facial Expression Recognition (FER). In spite of efforts made to improve the accuracy of FER systems using DNN, existing methods still are not generalizable enough in practical applications. This paper proposes a 3D Convolutional Neural Network method for FER in videos. This new network architecture consists of 3D Inception-ResNet layers followed by an LSTM unit that together extracts the spatial relations within facial images as well as the temporal relations between different frames in the video. Facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions. Our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods.\r"
  },
  "cvpr2017_w41_deepspacemood-basedimagetexturegenerationforvirtualrealityfrommusic": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "DeepSpace: Mood-Based Image Texture Generation for Virtual Reality From Music",
    "authors": [
      "Misha Sra",
      "Prashanth Vijayaraghavan",
      "Ognjen (Oggi) Rudovic",
      "Pattie Maes",
      "Deb Roy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Roy_DeepSpace_Mood-Based_Image_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Roy_DeepSpace_Mood-Based_Image_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Affective virtual spaces are of interest for many VR applications in wellbeing, art, education, and entertainment. Creating content for virtual environments is a laborious task involving skills like 3D modeling, texturing, animation, lighting, and programming. One way to facilitate content creation is to automate sub-processes like assigning textures and materials. To this end, we introduce the DeepSpace approach that automatically creates and applies textures to objects in procedurally created 3D scenes. The main novelty of our approach is that it uses music to automatically create kaleidoscopic textures for virtual environments designed to elicit emotional responses in users. Specifically, DeepSpace exploits the modeling power of deep neural networks, which have shown great performance in image generation tasks, to achieve mood-based image generation. Our study results indicate the virtual environments created by DeepSpace elicit positive emotions and achieve high presence scores.\r"
  },
  "cvpr2017_w41_itswrittenalloveryourfacefull-faceappearance-basedgazeestimation": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "It's Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation",
    "authors": [
      "Xucong Zhang",
      "Yusuke Sugano",
      "Mario Fritz",
      "Andreas Bulling"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Bulling_Its_Written_All_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Bulling_Its_Written_All_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses.\r"
  },
  "cvpr2017_w41_emoticemotionsincontextdataset": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "EMOTIC: Emotions in Context Dataset",
    "authors": [
      "Ronak Kosti",
      "Jose M. Alvarez",
      "Adria Recasens",
      "Agata Lapedriza"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Lapedriza_EMOTIC_Emotions_in_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Lapedriza_EMOTIC_Emotions_in_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Recognizing people's emotions from their frame of reference is very important in our everyday life. This capacity helps us to perceive or predict the subsequent actions of people, interact effectively with them and to be sympathetic and sensitive toward them. Hence, one should expect that a machine needs to have a similar capability of understanding people's feelings in order to correctly interact with humans. Current research on emotion recognition has focused on the analysis of facial expressions. However, recognizing emotions requires also understanding the scene in which a person is immersed. The unavailability of suitable data to study such a problem has made research in emotion recognition in context difficult. In this paper, we present the EMOTIC database (from EMOTions In Context), a database of images with people in real environments, annotated with their apparent emotions. We defined an extended list of 26 emotion categories to annotate the images, and combined these annotations with three common continuous dimensions: Valence, Arousal, and Dominance. Images in the database are annotated using the Amazon Mechanical Turk (AMT) platform. The resulting set contains 18,313 images with 23,788 annotated people. The goal of this paper is to present the EMOTIC database, detailing how it was created and the information available. We expect this dataset can help to open up new horizons on creating systems able of recognizing rich information about people's apparent emotional states. \r"
  },
  "cvpr2017_w41_personalizedautomaticestimationofself-reportedpainintensityfromfacialexpressions": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "Personalized Automatic Estimation of Self-Reported Pain Intensity From Facial Expressions",
    "authors": [
      "Daniel Lopez Martinez",
      "Ognjen (Oggi) Rudovic",
      "Rosalind Picard"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Picard_Personalized_Automatic_Estimation_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Picard_Personalized_Automatic_Estimation_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " Pain is a personal, subjective experience that is commonly evaluated through visual analog scales (VAS). While this is often convenient and useful, automatic pain detection systems can reduce pain score acquisition efforts in large-scale studies by estimating it directly from the partictipants' facial expressions. In this paper, we propose a novel two-stage learning approach for VAS estimation: first, our algorithm employs Recurrent Neural Networks (RNNs) to automatically estimate Prkachin and Solomon Pain Intensity (PSPI) levels from face images. The estimated scores are then fed into the personalized Hidden Conditional Random Fields (HCRFs), used to estimate the VAS, provided by each person. Personalization of the model is performed us- ing a newly introduced facial expressiveness score, unique for each person. To the best of our knowledge, this is the first approach to automatically estimate VAS from face images. We show the benefits of the proposed personalized over traditional non-personalized approach on a benchmark dataset for pain analysis from face images.\r"
  },
  "cvpr2017_w41_speech-driven3dfacialanimationwithimplicitemotionalawarenessadeeplearningapproach": {
    "conf_id": "CVPR2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2017_workshops - Deep Affective Learning and Context Modeling",
    "title": "Speech-Driven 3D Facial Animation With Implicit Emotional Awareness: A Deep Learning Approach",
    "authors": [
      "Hai X. Pham",
      "Samuel Cheung",
      "Vladimir Pavlovic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/html/Pavlovic_Speech-Driven_3D_Facial_CVPR_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2017_workshops/../content_cvpr_2017_workshops/w41/papers/Pavlovic_Speech-Driven_3D_Facial_CVPR_2017_paper.pdf",
    "published": "2017-07",
    "summary": " We introduce a long short-term memory recurrent neural network (LSTM-RNN) approach for real-time facial animation, which automatically estimates head rotation and facial action unit activations of a speaker from just her speech. Specifically, the time-varying contextual non-linear mapping between audio stream and visual facial movements is realized by training a LSTM neural network on a large audio-visual data corpus. In this work, we extract a set of acoustic features from input audio, including Mel-scaled spectrogram, Mel frequency cepstral coefficients and chromagram that can effectively represent both contextual progression and emotional intensity of the speech. Output facial movements are characterized by 3D rotation and blending expression weights of a blendshape model, which can be used directly for animation. Thus, even though our model does not explicitly predict the affective states of the target speaker, her emotional manifestation is recreated via expression weights of the face model. Experiments on an evaluation dataset of different speakers across a wide range of affective states demonstrate promising results of our approach in real-time speech-driven facial animation.\r"
  }
}