{
  "bmvc2020_main_bcarbeginnerclassifierasregularizationtowardsgeneralizablere-id": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "BCaR: Beginner Classifier as Regularization Towards Generalizable Re-ID",
    "authors": [
      "Masato Tamura",
      "Tomoaki Yoshinaga"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0303.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0303.pdf",
    "published": "2020-09",
    "summary": "In recent years, the performance of person re-identification has been dramatically improved by virtue of sophisticated training methods. However, most of the existing methods are based on the assumption that the statistics of a target domain can be utilized during training. This inevitably introduces huge costs for data collection each time a person re-identification system is deployed, which hinders the applicability to real-world scenarios. To mitigate this issue, we expand upon the concept of domain generalization. Typical person re-identification datasets are composed of a large amount of identities. However, examples for each identity are rather scarce. It is widely known that if examples are highly biased, over-fitting is likely to occur and degrade the performance. To alleviate this problem, we propose a novel soft-label regularization method that combines an expert feature extractor with a beginner classifier for generating soft labels. From a representation learning perspective, a convolutional neural network-based feature extractor is thought to prioritize common patterns. Therefore, the subsequent classifier typically fits common examples first, followed by rare ones. On the basis of this observation, we force the beginner classifier to remain uncertain towards rare examples by means of periodic initialization. Accordingly, the beginner classifier assigns highly confident labels to common examples and ambiguous labels to rare ones, thus enabling soft labels to mitigate over-fitting to biased examples (e.g., highly occluded ones). Extensive analysis shows that our method successfully assigns ambiguous labels to biased examples and thus increases the rank-1 accuracy by 3.4%, 1.6%, 0.9%, and 5.2% on the VIPeR, PRID, GRID, and i-LIDS datasets, respectively. To facilitate future research, the source codes will be released."
  },
  "bmvc2020_main_n2nskiplearninghighlysparsenetworksusingneuron-to-neuronskipconnections": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "N2NSkip: Learning Highly Sparse Networks using Neuron-to-Neuron Skip Connections",
    "authors": [
      "Arvind Subramaniam",
      "Avinash Sharma"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0097.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0097.pdf",
    "published": "2020-09",
    "summary": "The over-parametrized nature of Deep Neural Networks (DNNs) leads to considerable hindrances during deployment on low-end devices with time and space constraints. Network pruning strategies that sparsify DNNs using iterative prune-train schemes are often computationally expensive. As a result, techniques that prune at initialization, prior to training, have become increasingly popular. In this work, we propose neuron-to-neuron skip (N2NSkip) connections, which act as sparse weighted skip connections, to enhance the overall connectivity of pruned DNNs. Following a preliminary pruning step, N2NSkip connections are randomly added between individual neurons/channels of the pruned network, while maintaining the overall sparsity of the network. We demonstrate that introducing N2NSkip connections in pruned networks enables significantly superior performance, especially at high sparsity levels, as compared to pruned networks without N2NSkip connections. Additionally, we present a heat diffusion-based connectivity analysis to quantitatively determine the connectivity of the pruned network with respect to the reference network. We evaluate the efficacy of our approach on two different preliminary pruning methods which prune at initialization and consistently obtain superior performance by exploiting the enhanced connectivity resulting from N2NSkip connections."
  },
  "bmvc2020_main_transferringpretrainednetworkstosmalldataviacategorydecorrelation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Transferring Pretrained Networks to Small Data via Category Decorrelation",
    "authors": [
      "Ying Jin",
      "Zhangjie Cao",
      "Mingsheng Long",
      "Jianmin Wang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0090.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0090.pdf",
    "published": "2020-09",
    "summary": "Transfer learning by fine-tuning neural networks pre-trained on large-scale datasets excels at accelerating the training process and improving the model performance for the target task. Previous works have unveiled catastrophic forgetting in fine-tuning, where the model is over-transferred thus losing pre-trained knowledge, especially facing large target datasets. However, when fine-tuning pre-trained networks to small data, under transfer emerges instead, where the model sticks to the pre-trained model and learns little target knowledge. Under transfer severely restricts the wide use of fine-tuning but is still under-investigated. In this paper, we conduct an in-depth study of under transfer problem in fine-tuning and observe that when we finetune model to small data, redundant category correlation becomes stronger in the model prediction, which is a potential cause of under transfer. Based on the observation, we propose a novel regularization approach, Category Decorrelation (CatDec), to minimize category correlation in the model, which introduces a new inductive bias to strengthen the model transfer. CatDec is orthogonal to existing fine-tuning approaches and can collaborate with them to address the dilemma of catastrophic forgetting and under transfer. Experiment results demonstrate that the proposed approach can consistently improve the fine-tuning performance of various mainstream methods. Further analyses prove that CatDec alleviates redundant category correlation and helps transfer."
  },
  "bmvc2020_main_few-shotlearningwithcomplex-valuedneuralnetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Few-Shot Learning with Complex-valued Neural Networks",
    "authors": [
      "Zhen Liu",
      "Baochang Zhang",
      "Guodong Guo"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0353.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0353.pdf",
    "published": "2020-09",
    "summary": "Feature representation is fundamental and attracts much attention in few-shot learning. Convolutional neural networks (CNNs) are among the best feature extractors so far in this field, which are successfully combined with metric learning, leading to the state-of-the-art performance. However, the subtle difference among inter-class samples challenges existing CNN based methods, which only use real-valued CNNs that fail to extract more detailed information.In this paper, we introduce complex metric module (CMM) into metric learning, aiming to better measure the inter- and intra-class relations based on both amplitude and phase information. Specifically, building upon the recent episodic training mechanism, our CMM can enhance the representation capacity by extracting robust complex-valued features to facilitate modeling subtle relationships among samples, which can enhance the performance of the few-shot classification task when only few samples are available. Moreover, we introduce a new transductive method into CMM, by considering not only query and support but also query and query relationships to predict classes of unlabeled samples. Experiments on two benchmark datasets show that the proposed CMM significantly improves the performance over other approaches and achieves the state-of-the-art results."
  },
  "bmvc2020_main_attentiveactionandcontextfactorization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Attentive Action and Context Factorization",
    "authors": [
      "Yang Wang",
      "Vinh Tran",
      "Gedas Bertasius",
      "Lorenzo Torresani",
      "Minh Hoai Nguyen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0187.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0187.pdf",
    "published": "2020-09",
    "summary": "We propose a method for human action recognition, one that can localize the spatiotemporal regions that `define' the actions. This is a challenging task due to the subtlety of human actions in video and the co-occurrence of contextual elements. To address this challenge, we utilize conjugate samples of human actions, which are video clips that are contextually similar to human action samples but do not contain the action. We introduce a novel attentional mechanism that can spatially and temporally separate human actions from the co-occurring contextual factors. The separation of action and context factors is weakly supervised, eliminating the need for laboriously detailed annotation of these two factors in training samples. Our method can build human action classifiers with higher accuracy and better interpretability. Experiments on several human action recognition datasets demonstrate the quantitative and qualitative benefits of our approach."
  },
  "bmvc2020_main_boostingimageandvideocompressionvialearninglatentresidualpatterns": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Boosting Image and Video Compression via Learning Latent Residual Patterns",
    "authors": [
      "Yen-Chung Chen",
      "Keng-Jui Chang",
      "Yi-Hsuan Tsai",
      "Wei-Chen Chiu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0174.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0174.pdf",
    "published": "2020-09",
    "summary": "Reducing compression artifacts is essential for streaming videos with a better quality under a limited bandwidth. To tackle this problem, existing methods aim to directly recover details from the compressed video but do not consider learning rich information in uncompressed videos to aid this process. In this paper, we focus on utilizing the residual information, which is the difference between a compressed video and its corresponding original/uncompressed one, and propose a fairly efficient way to transmit the residual with the compressed video in order to boost the quality of video compression. Our proposed method is realized by learning to discover the patterns in the residual and storing them offline as dictionary-like patterns. During the testing stage, e.g., for video streaming, the residual is transmitted in the form of pattern indexes to reduce the cost of communication, and thus the original residual information can be easily retrieved back from the dictionary of learned residual patterns. We show the effectiveness of our framework on numerous datasets under various video compression coding methods. In addition, the proposed pipeline can be widely applied to the image compression task and reduce artifacts produced from conventional and CNN-based compression algorithms."
  },
  "bmvc2020_main_pano2scene3dindoorsemanticscenereconstructionfromasingleindoorpanoramaimage": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Pano2Scene: 3D Indoor Semantic Scene Reconstruction from a Single Indoor Panorama Image",
    "authors": [
      "Wei Zeng",
      "Sezer Karaoglu",
      "Theo Gevers"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0473.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0473.pdf",
    "published": "2020-09",
    "summary": "3D indoor semantic scene reconstruction from 2D images is challenging as it requires both scene understanding and object reconstruction. Compared to perspective images, panoramas provide larger field of view and carry more scene information. In this paper, to reconstruct the 3D indoor semantic scene from a single panorama image, we propose a pipeline that jointly learns to predict the 3D scene layout, complete the object shapes and reconstruct the full scene point cloud. Experiments on the Stanford 2D-3D dataset demonstrate the generality and suitability of the proposed method."
  },
  "bmvc2020_main_conditionalattentionforcontent-basedimageretrieval": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Conditional Attention for Content-based Image Retrieval",
    "authors": [
      "Zechao Hu",
      "Adrian Bors"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0356.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0356.pdf",
    "published": "2020-09",
    "summary": "Deep learning based feature extraction combined with visual attention mechanism is shown to provide good results in content-based image retrieval (CBIR). Ideally, CBIR should rely on regions which contain objects of interest that appear in the query image. However, most existing attention models just predict the most likely region of interest based on the knowledge learned from the training dataset regardless of the content in the query image. As a result, they may look towards contexts outside the object of interest, especially when there are multiple potential objects of interest in a given image. In this paper, we propose a conditional attention model which is sensitive to the input query image content and can generate more accurate attention maps. A key-point detection and description based method is proposed for training data generation. Consequently, our model does not require any additional attention label for training. The proposed attention model enables the spatial pooling feature extraction method (generalized mean pooling) improves image feature representation and leads to better image retrieval performance. The proposed framework is tested on a series of databases where it is shown to perform well in challenging situations."
  },
  "bmvc2020_main_axiom-basedgrad-camtowardsaccuratevisualizationandexplanationofcnns": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs",
    "authors": [
      "Ruigang Fu",
      "Qingyong Hu",
      "Xiaohu Dong",
      "Yulan Guo",
      "Yinghui Gao",
      "Biao Li"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0631.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0631.pdf",
    "published": "2020-09",
    "summary": "To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. However, in spite of the reasonable visualization, most of these methods lack clear and sufficient theoretical support. In this paper, we introduce two axioms -- Sensitivity and Conservation -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of sensitivity and conservation. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. Code is available at https://github.com/Fu0511/XGrad-CAM."
  },
  "bmvc2020_main_robustensemblemodeltrainingviarandomlayersamplingagainstadversarialattack": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Robust Ensemble Model Training via Random Layer Sampling Against Adversarial Attack",
    "authors": [
      "Hakmin Lee",
      "Hong Joo Lee",
      "Seong Tae Kim",
      "Yong Man Ro"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0532.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0532.pdf",
    "published": "2020-09",
    "summary": "Deep neural networks have achieved substantial achievements in several computer vision areas, but have vulnerabilities that are often fooled by adversarial examples that are not recognized by humans. This is an important issue for security or medical applications. In this paper, we propose an ensemble model training framework with random layer sampling to improve the robustness of deep neural networks. In the proposed training framework, we generate various sampled model through the random layer sampling and update the weight of the sampled model. After the ensemble models are trained, it can hide the gradient efficiently and avoid the gradient-based attack by the random layer sampling method. To evaluate our proposed method, comprehensive and comparative experiments have been conducted on three datasets. Experimental results show that the proposed method improves the adversarial robustness."
  },
  "bmvc2020_main_weakly-supervisedsalientinstancedetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Weakly-supervised Salient Instance Detection",
    "authors": [
      "Xin Tian",
      "Ke Xu",
      "Xin Yang",
      "Baocai Yin",
      "Rynson Lau"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0430.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0430.pdf",
    "published": "2020-09",
    "summary": "Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. We note that subitizing information provides an instant judgement on the number of salient items, which naturally relates to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this insight, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is further fused to produce salient instance maps. We conduct extensive experiments to demonstrate that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks."
  },
  "bmvc2020_main_unsupervisedandsemi-supervisednoveltydetectionusingvariationalautoencodersinopportunisticsciencemissions": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Unsupervised and Semi-supervised Novelty Detection using Variational Autoencoders in Opportunistic Science Missions",
    "authors": [
      "Lorenzo Sintini",
      "Lars Kunze"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0643.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0643.pdf",
    "published": "2020-09",
    "summary": "Scientific opportunities are missed in planetary explorations due to the lack of communication and/or long-time communication delays between rovers and ground stations. By enabling rovers to autonomously detect and explore targets the overall scientific outcome of extraterrestrial missions can be increased. In this paper, we have designed, developed, and evaluated unsupervised as well as semi-supervised approaches to novelty detection based on Variational Autoencoders (VAE). Our VAE model was trained on typical data from previous missions and tested to infer the novelty of scientific targets. In an ablation study, we investigate the effectiveness of different types of loss functions. We compare losses based on reconstruction errors, losses obtained from the VAE's latent space as well as a combination of both. In our experiments, we have evaluated both unsupervised and semi-supervised approaches on datasets obtained from NASA's Mars Curiosity rover. Results show that our VAE-based approaches are not only robust but also comparable, or better, than the state-of-the-art."
  },
  "bmvc2020_main_domainadaptationoflearnedfeaturesforvisuallocalization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Domain Adaptation of Learned Featuresfor Visual Localization",
    "authors": [
      "Sungyong Baik",
      "Hyo Jin Kim",
      "Tianwei Shen",
      "Eddy Ilg",
      "Kyoung Mu Lee",
      "Christopher Sweeney"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0316.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0316.pdf",
    "published": "2020-09",
    "summary": "We tackle the problem of visual localization under changing conditions, such as time of day, weather, and seasons. Recent learned local features based on deep neural networks have shown superior performance over classical hand-crafted local features. However, in a real-world scenario, there often exists a large domain gap between training and target images, which can significantly degrade the localization accuracy. We present an approach that mitigates the domain gap by proposing a few-shot domain adaptation framework for learned local features that deals with varying conditions in visual localization. The experimental results demonstrate the superior performance over baselines, while using only a few training examples from the target domain."
  },
  "bmvc2020_main_learningfromcountingleveragingtemporalclassificationforweaklysupervisedobjectlocalizationanddetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning from Counting: Leveraging Temporal Classification for Weakly Supervised Object Localization and Detection",
    "authors": [
      "Chia-Yu Hsu",
      "Wenwen Li"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0621.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0621.pdf",
    "published": "2020-09",
    "summary": "This paper reports a new solution of leveraging temporal classification to support weakly supervised object detection (WSOD). Specifically, we introduce raster scan-order techniques to serialize 2D images into 1D sequence data, and then leverage a combined LSTM (Long, Short-Term Memory) and CTC (Connectionist Temporal Classification) network to achieve object localization based on a total count (of interested objects). We term our proposed network LSTM-CCTC (Count-based CTC). This \u201clearning from counting\u201d strategy differs from existing WSOD methods in that our approach automatically identifies critical points on or near a target object. This strategy significantly reduces the need of generating a large number of candidate proposals for object localization. Experiments show that our method yields state-of-the-art performance based on an evaluation on PASCAL VOC datasets."
  },
  "bmvc2020_main_trippingthroughtimeefficientlocalizationofactivitiesinvideos": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Tripping through time: Efficient Localization of Activities in Videos",
    "authors": [
      "Meera Hahn",
      "Asim Kadav",
      "James Rehg",
      "Hans Peter Graf"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0549.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0549.pdf",
    "published": "2020-09",
    "summary": "Localizing moments in untrimmed videos via language queries is a new and interesting task that requires the ability to accurately ground language into video. Previous works have approached this task by processing the entire video, often more than once, to localize relevant activities. In the real world applications that this task lends itself to, such as surveillance, efficiency is a pivotal trait of a system. In this paper, we present TripNet, an end-to-end system that uses a gated attention architecture to model fine-grained textual and visual representations in order to align text and video content. Furthermore, TripNet uses reinforcement learning to efficiently localize relevant activity clips in long videos, by learning how to intelligently skip around the video. It extracts visual features for few frames to perform activity classification. In our evaluation over Charades-STA, ActivityNet Captions and the TACoS dataset, we find that TripNet achieves high accuracy and saves process- ing time by only looking at 32-41% of the entire video."
  },
  "bmvc2020_main_high-speedlight-weightcnninferenceviastridedconvolutionsonapixelprocessorarray": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "High-speed Light-weight CNN Inference via Strided Convolutions on a Pixel Processor Array",
    "authors": [
      "Yanan Liu",
      "Laurie Bose",
      "Jianing Chen",
      "Stephen Carey",
      "Piotr Dudek",
      "Walterio Mayol-Cuevas"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0126.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0126.pdf",
    "published": "2020-09",
    "summary": "Performance, storage, and power consumption are three major factors that restrict the use of machine learning algorithms on embedded systems. However, new hardware architectures designed with visual computation in mind may hold the key to solving these bottlenecks. This work makes use of a novel visual device: the pixel processor array (PPA), to embed a convolutional neural network (CNN) onto the focal plane. We present a new high-speed implementation of strided convolutions using binary weights for the CNN on PPA devices, allowing all multiplications to be replaced by more efficient addition/subtraction operations. Image convolutions, ReLU activation functions, max-pooling and a fully-connected layer are all performed directly on the PPA\u2019s imaging plane, exploiting its massive parallel computing capabilities. We demonstrate CNN inference across 4 different applications, running between 2,000 and 17,500 fps with power consumption lower than 1.5W. These tasks include identifying 8 classes of plankton, hand gesture classification and digit recognition."
  },
  "bmvc2020_main_blackmagicindeeplearninghowhumanskillimpactsnetworktraining": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Black Magic in Deep Learning: How Human Skill Impacts  Network Training",
    "authors": [
      "Kanav Anand",
      "Ziqi Wang",
      "Marco Loog",
      "Jan van Gemert"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0552.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0552.pdf",
    "published": "2020-09",
    "summary": "How does a user's prior experience with deep learning impact network performance?We present an initial study based on 31 participants with different levels of experience. Their task is to perform hyperparameter optimization for a given deep learning architecture. The results show a strong positive correlation between the participant's experience and the final performance. They additionally indicate that an experienced participant finds better solutions using fewer resources on average. The data suggests furthermore that participants with no prior experience follow random strategies in their pursuit of optimal hyperparameters. Our study reveals the human factor in scientific reproducibility and in comparisons of state of the art results in deep learning."
  },
  "bmvc2020_main_two-streamspatiotemporalcompositionalattentionnetworkforvideoqa": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Two-Stream Spatiotemporal Compositional Attention Network for VideoQA",
    "authors": [
      "Taiki Miyanishi",
      "Takuya Maekawa",
      "Motoaki Kawanabe"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0877.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0877.pdf",
    "published": "2020-09",
    "summary": "This study tackles a video question answering (VideoQA), which requires spatiotemporal video reasoning. VideoQA aims to return an appropriate answer about textual questions referring to image frames in the video. In this paper, based on the observation that multiple entities and their movements in the video can be important clues for deriving the correct answer, we propose a two-stream spatiotemporal compositional attention network that achieves sophisticated multi-step spatiotemporal reasoning by using both motion and detailed appearance features. In contrast to the existing video reasoning approach that uses frame-level or clip-level appearance and motion features, our method simultaneously attends detailed appearance features of multiple entities as well as motion features guided by attending words in the textual question. Furthermore, it progressively refines internal representation and infers the answer via multiple reasoning steps. We evaluate our method on short- and long-form VideoQA benchmarks: MSVD-QA, MSRVTT-QA, and ActivityNet-QA and achieve state-of-the-art accuracy on these datasets."
  },
  "bmvc2020_main_weaklysupervisedgenerativenetworkformultiple3dhumanposehypotheses": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Weakly Supervised Generative Network for Multiple 3D Human Pose Hypotheses",
    "authors": [
      "Chen Li",
      "Gim Hee Lee"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0330.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0330.pdf",
    "published": "2020-09",
    "summary": "3D human pose estimation from a single image is an inverse problem due to the inherent ambiguity of the missing depth. Some previous works addressed the inverse problem by generating multiple hypotheses. However, these works are strongly supervised and require ground truth 2D-to-3D correspondences which can be difficult to obtain. In this paper, we propose a weakly supervised deep generative network to address the inverse problem and circumvent the need for ground truth 2D-to-3D correspondences. To this end, we design our network to model a proposal distribution which we use to approximate the unknown multi-modal target posterior distribution. We achieve the approximation by minimizing the KL divergence between the proposal and target distributions, and this leads to a 2D reprojection error and a prior loss term that can be weakly supervised. Furthermore, we determine the most probable solution as the conditional mode of the samples using the mean-shift algorithm. We evaluate our method on three benchmark datasets -- Human3.6M, MPII and MPI-INF-3DHP. Experimental results show that our approach is capable of generating multiple feasible hypotheses and achieves state-of-the-art results compared to existing weakly supervised approaches."
  },
  "bmvc2020_main_fromsaturationtozero-shotvisualrelationshipdetectionusinglocalcontext": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "From Saturation to Zero-Shot Visual Relationship Detection Using Local Context",
    "authors": [
      "Nikolaos Gkanatsios",
      "Vassilis Pitsikalis",
      "Petros Maragos"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0772.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0772.pdf",
    "published": "2020-09",
    "summary": "Visual relationship detection has been motivated by the ``insufficiency of objects to describe rich visual knowledge''. However, we find that training and testing on current popular datasets may not support such statements; most approaches can be outperformed by a naive image-agnostic baseline that fuses language and spatial features. We visualize the errors of numerous existing detectors, to discover that most of them are caused by the coexistence and penalization of antagonizing predicates that could describe the same interaction. Such annotations hurt the dataset's causality and models tend to overfit the dataset biases, resulting in a saturation of accuracy to artificially low levels. We construct a simple architecture and explore the effect of using language on generalization. Then, we introduce adaptive local-context-aware classifiers, that are built on-the-fly based on the objects' categories. To improve context awareness, we mine and learn predicate synonyms, i.e. different predicates that could equivalently hold, and apply a distillation-like loss that forces synonyms to have similar classifiers and scores. The last also serves as a regularizer that mitigates the dominance of the most frequent classes, enabling zero-shot generalization. We evaluate predicate accuracy on existing and novel test scenarios to display state-of-the-art results over prior biased baselines."
  },
  "bmvc2020_main_theaduulm-dataset-asemanticsegmentationdatasetforsensorfusion": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "The ADUULM-Dataset - a Semantic Segmentation Dataset for Sensor Fusion",
    "authors": [
      "Andreas Pfeuffer",
      "Markus Sch\u00f6n",
      "Ditzel Carsten",
      "Klaus Dietmayer"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0474.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0474.pdf",
    "published": "2020-09",
    "summary": "One of the key challenges of today's semantic segmentation approaches is to obtain robust and reliable segmentation results not only in good weather conditions, but also in adverse weather conditions such as darkness, fog or heavy rain. For this purpose, multiple sensor data of several sensor types such as camera and lidar are required to compensate the weather sensitivity of individual sensors. Hence, a semantic segmentation dataset is necessary, which contains camera and lidar data, but until recently, no such dataset exists. Therefore, the ADUULM dataset was created, a semantic segmentation dataset which consists of fine-annotated camera data and pixel-wise labeled lidar data recorded in diverse weather conditions. Additionally, the corresponding GPS, IMU and stereo information are provided, and for each annotated data sample, a short video-sequence is available, too. Furthermore, state-of-the-art semantic segmentation and drivable area detection approaches are evaluated on the proposed dataset, and it turned out that new methods are required to obtain robust and reliable results in adverse weather conditions. The ADUULM-dataset will be available online at https://www.uni-ulm.de/en/in/driveu/projects"
  },
  "bmvc2020_main_generativeappearanceflowahybridapproachforoutdoorviewsynthesis": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Generative Appearance Flow: A Hybrid Approach for Outdoor View Synthesis",
    "authors": [
      "M. Usman Rafique",
      "Hunter Blanton",
      "Noah Snavely",
      "Nathan Jacobs"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0055.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0055.pdf",
    "published": "2020-09",
    "summary": "We address the problem of view synthesis in complex outdoor scenes. We propose a novel convolutional neural network architecture that includes flow-based and direct synthesis sub-networks. Both sub-networks introduce novel elements that greatly improve the quality of the synthesized images. These images are then adaptively fused to create the final output image. Our approach achieves state-of-the-art performance on the KITTI dataset, which is commonly used to evaluate view-synthesis methods. Unlike many recently proposed methods, ours is trained without the need for additional geometric constraints, such as a ground-truth depth map, making it more broadly applicable. Our approach also achieved the best performance on the Brooklyn Panorama Synthesis dataset, which we introduce as a new, challenging benchmark for view synthesis. Our dataset, code, and pretrained models are available at url{https://mvrl.github.io/GAF}."
  },
  "bmvc2020_main_pointcloudsuperresolutionwithadversarialresidualgraphnetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Point Cloud Super Resolution with Adversarial Residual Graph Networks",
    "authors": [
      "Huikai Wu",
      "Kaiqi Huang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0118.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0118.pdf",
    "published": "2020-09",
    "summary": "Point cloud super-resolution is a fundamental problem for 3D reconstruction and 3D data understanding. It takes a low-resolution (LR) point cloud as input and generates a high-resolution (HR) point cloud with rich details. In this paper, we present a data-driven method for point cloud super-resolution based on graph networks and adversarial losses. The key idea of the proposed network is to exploit the local similarity of point cloud and the analogy between LR input and HR output. For the former, we design a deep network with graph convolution. For the latter, we propose to add residual connections into graph convolution and introduce a skip connection between input and output. The proposed network is trained with a novel loss function, which combines Chamfer Distance (CD) and graph adversarial loss. Such a loss function captures the characteristics of HR point cloud automatically without manual design. We conduct a series of experiments to evaluate our method and validate the superiority over other methods. Results show that the proposed method achieves state-of-the-art performance and have a good generalization ability to unseen data."
  },
  "bmvc2020_main_unifiedrepresentationlearningforcrossmodelcompatibility": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Unified Representation Learning for Cross Model Compatibility",
    "authors": [
      "Chien-Yi Wang",
      "Ya-Liang Chang",
      "Shang-Ta Yang",
      "Dong Chen",
      "Shang-Hong Lai"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0195.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0195.pdf",
    "published": "2020-09",
    "summary": "We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. Cross-compatibility between different embedding models enables the visual search systems to correctly recognize and retrieve identities without re-encoding user images, which are usually not available due to privacy concerns. While there are existing approaches to address CMC in face identification, they fail to work in a more challenging setting where the distributions of embedding models shift drastically. The proposed solution improves CMC performance by introducing a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize the embedding spaces. Extensive experiments demonstrate that our proposed solution outperforms previous approaches by a large margin for various challenging visual search scenarios of face recognition and person re-identification."
  },
  "bmvc2020_main_residuallikelihoodforests": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Residual Likelihood Forests",
    "authors": [
      "Yan Zuo",
      "Tom Drummond"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0191.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0191.pdf",
    "published": "2020-09",
    "summary": "This paper presents a novel ensemble learning approach called Residual Likelihood Forests (RLF). Our weak learners produce conditional likelihoods that are sequentially optimized using global loss in the context of previous learners within a boosting-like framework (rather than probability distributions that are measured from observed data) and are combined multiplicatively (rather than additively). This increases the efficiency of our strong classifier, allowing for the design of classifiers which are more compact in terms of model capacity. We apply our method to several machine learning classification tasks, showing significant improvements in performance. When compared against several ensemble approaches including Random Forests and Gradient Boosted Trees, RLFs offer a significant improvement in performance whilst concurrently reducing the required model size."
  },
  "bmvc2020_main_advancingweaklysupervisedcross-domainalignmentwithoptimaltransport": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Advancing weakly supervised cross-domain alignment with optimal transport",
    "authors": [
      "Siyang Yuan",
      "Ke Bai",
      "Liqun Chen",
      "Yizhe Zhang",
      "Chenyang Tao",
      "Chunyuan Li",
      "Guoyin Wang",
      "Ricardo Henao",
      "Lawrence Carin Duke"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0566.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0566.pdf",
    "published": "2020-09",
    "summary": "Cross-domain alignment between image objects and text sequences is key to many visual-language tasks and it poses a fundamental challenge to both computer vision and natural language processing. This study investigates a novel approach for the identification and optimization offine-grained semantic similarities between image and text entities, under a weakly-supervised setup, improving performance over state-of-the-art solutions. Our method builds upon recent advances in optimal transport (OT) to resolve the cross-domain matching problem in a principled manner. Formulated as a drop-in regularizer, the proposed OT solution can be efficiently computed and used in combination with other existing approaches. We present empirical evidence to demonstrate the effectiveness of our approach, that enables simpler model architectures to outperform or be comparable with more sophisticated designs on a range of vision-language tasks."
  },
  "bmvc2020_main_m2kdincrementallearningviamulti-modelandmulti-levelknowledgedistillation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "M2KD: Incremental Learning via Multi-model and Multi-level Knowledge Distillation",
    "authors": [
      "Peng Zhou",
      "Long Mai",
      "Jianming Zhang",
      "Ning Xu",
      "Zuxuan Wu",
      "Larry Davis"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0131.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0131.pdf",
    "published": "2020-09",
    "summary": "Incremental learning targets at achieving good performance on new categories without forgetting old ones. Knowledge distillation has been shown critical in preserving the performance on old classes. Conventional methods, however, sequentially distill knowledge only from the last model, leading to performance degradation on the old classes in later incremental learning steps. In this paper, we propose a multi-model and multi-level knowledge distillation strategy. Instead of sequentially distilling knowledge only from the last model, we directly leverage all previous model snapshots. In addition, we incorporate an auxiliary distillation to further preserve knowledge encoded at the intermediate feature levels. To make the model more memory efficient, we adapt mask based pruning to reconstruct all previous models with a small memory footprint. Experiments on standard incremental learning benchmarks show that our method improves the overall performance over standard distillation techniques."
  },
  "bmvc2020_main_pomppomcp-basedonlinemotionplanningforactivevisualsearchinindoorenvironments": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "POMP: Pomcp-based Online Motion Planning for active visual search in indoor environments",
    "authors": [
      "Yiming Wang",
      "FrancescoGiuliari",
      "Riccardo Berra",
      "Alberto Castellini",
      "Alessio Del Bue",
      "Alessandro Farinelli",
      "Marco Cristani",
      "Francesco Setti"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0918.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0918.pdf",
    "published": "2020-09",
    "summary": "In this paper we focus on the problem of learning an optimal policy for Active Visual Search (AVS) of objects in known indoor environments with an online setup. Our POMP method uses as input the current pose of an agent (e.g. a robot) and a RGB-D frame. The task is to plan the next move that brings the agent closer to the target object. We model this problem as a Partially Observable Markov Decision Process solved by a Monte-Carlo planning approach. This allows us to make decisions on the next moves by iterating over the known scenario at hand, exploring the environment and searching for the object at the same time.Differently from the current state of the art in Reinforcement Learning, POMP does not require extensive and expensive (in time and computation) labelled data so being very agile in solving AVS in small and medium real scenarios. We only require the information of the floormap of the environment, an information usually available or that can be easily extracted from an a priori single exploration run. We validate our method on the publicly available AVD benchmark, achieving an average success rate of 0.76 with an average path length of 17.1, performing close to the state of the art but without any training needed. Additionally, we show experimentally the robustness of our method when the quality of the object detection goes from ideal to faulty."
  },
  "bmvc2020_main_stq-netsunifyingnetworkbinarizationandstructuredpruning": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "STQ-Nets: Unifying Network Binarization and Structured Pruning",
    "authors": [
      "Aurobindo Munagala",
      "Ameya Prabhu",
      "Anoop Namboodiri"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0113.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0113.pdf",
    "published": "2020-09",
    "summary": "We discuss a formulation for network compression combining two major paradigms: binarization and pruning. Past works on network binarization have demonstrated that networks are robust to the removal of activation/weight magnitude information, and can perform comparably to full-precision networks with signs alone. Pruning focuses on generating efficient and sparse networks. Both compression paradigms aid deployment in portable settings, where storage, compute and power are limited. We argue that these paradigms are complementary, and can be combined to offer high levels of compression and speedup without any significant accuracy loss. Intuitively, weights/activations closer to zero have higher binarization error making them good candidates for pruning. Our proposed formulation incorporates speedups from binary convolution algorithms through structured pruning, enabling the removal of pruned parts of the network entirely post-training, beating previous works attempting the same by a significant margin. Overall, our method brings up to 89x layer-wise compression over the corresponding full-precision networks --achieving only 0.33% loss on CIFAR-10 with ResNet-18 with a 40% PFR (Prune Factor Ratio for filters), and 0.3% on ImageNet with ResNet-18 with a 19% PFR."
  },
  "bmvc2020_main_explicitresidualdescentfor3dhumanposeestimationfrom2djointlocations": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Explicit Residual Descent for 3D Human Pose Estimation from 2D Joint Locations",
    "authors": [
      "Yangyuxuan Kang",
      "Anbang Yao",
      "Shandong Wang",
      "Ming Lu",
      "Yurong Chen",
      "Enhua Wu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0151.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0151.pdf",
    "published": "2020-09",
    "summary": "Recent studies show that the end-to-end learning paradigm based on well-designed lifting networks merely using 2D joint locations as the input can achieve impressive performance in handling 3D human pose estimation problem. However, in the viewpoint of optimization design, existing methods of this category have two drawbacks: (1) The inherent feature relation between the 2D pose input and the corresponding 3D pose estimate is not sufficiently explored. (2) The regression procedure is usually performed in a one-step manner. To address these two issues, this paper proposes an efficient yet accurate method called Explicit Residual Descent (ERD). Given an arbitrary lifting network which takes 2D joint locations in a single image as the input and generates an initial 3D pose estimate, our ERD learns a sequence of descent directions encoded with a shared lightweight differentiable structure, progressively refining the previous 3D pose estimate via adding in a 3D increment obtained from projecting the reconstructed 2D pose features onto each learnt descent direction. Extensive experiments on public benchmarks including Human3.6M dataset validate the superior performance of the proposed method against state-of-the-art methods. Code will be made publicly available."
  },
  "bmvc2020_main_delvingdeeperintoanti-aliasinginconvnets": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Delving Deeper into Anti-aliasing in ConvNets",
    "authors": [
      "Xueyan Zou",
      "Fanyi Xiao",
      "Zhiding Yu",
      "Yong Jae Lee"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0547.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0547.pdf",
    "published": "2020-09",
    "summary": "Aliasing refers to the phenomenon that high frequency signals degenerate into completely different ones after sampling. It arises as a problem in the context of deep learning as downsampling layers are widely adopted in deep architectures to reduce parameters and computation. The standard solution is to apply a low-pass filter (e.g., Gaussian blur) before downsampling [Zhang.]. However, it can be suboptimal to apply the same filter across the entire content, as the frequency of feature maps can vary across both spatial locations and feature channels. To tackle this, we propose an adaptive content-aware low-pass filtering layer, which predicts separate filter weights for each spatial location and channel group of the input feature maps. We investigate the effectiveness and generalization of the proposed method across multiple tasks including ImageNet classification, COCO instance segmentation, and Cityscapes semantic segmentation. Qualitative and quantitative results demonstrate that our approach effectively adapts to the different feature frequencies to avoid aliasing while preserving useful information for recognition."
  },
  "bmvc2020_main_anchor-freesmall-scalemultispectralpedestriandetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Anchor-free Small-scale Multispectral Pedestrian Detection",
    "authors": [
      "Alexander Wolpert",
      "Michael Teutsch",
      "Saquib Sarfraz",
      "Rainer Stiefelhagen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0534.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0534.pdf",
    "published": "2020-09",
    "summary": "Multispectral images consisting of aligned visual-optical (VIS) and thermal infrared (IR) image pairs are well-suited for practical applications like autonomous driving or visual surveillance. Such data can be used to increase the performance of pedestrian detection especially for weakly illuminated, small-scaled, or partially occluded instances. The current state-of-the-art is based on variants of Faster R-CNN and thus passes through two stages: a proposal generator network with handcrafted anchor boxes for object localization and a classification network for verifying the object category. In this paper we propose a method for effective and efficient multispectral fusion of the two modalities in an adapted single-stage anchor box free base architecture. We aim at learning pedestrian representations based on object center and scale rather than direct bounding box predictions. In this way, we can both simplify the network architecture and achieve higher detection performance, especially for pedestrians under occlusion or at low object resolution. In addition, we provide a study on well-suited multispectral data augmentation techniques that improve the commonly used augmentations. The results show our method's effectiveness in detecting small-scaled pedestrians. We achieve 5.68 % log-average miss rate in comparison to the best current state-of-the-art of 7.49 % (~25 % improvement) on the challenging KAIST Multispectral Pedestrian Benchmark."
  },
  "bmvc2020_main_asimpleandscalableshaperepresentationfor3dreconstruction.": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "A Simple and Scalable Shape Representation for 3D Reconstruction.",
    "authors": [
      "Mateusz Michalkiewicz",
      "Eugene Belilovsky",
      "Mahsa Baktashmotlagh",
      "Anders Eriksson"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0443.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0443.pdf",
    "published": "2020-09",
    "summary": "Deep learning applied to the reconstruction of 3D shapes has seen growing interest. A popular approach to 3D reconstruction and generation in recent years has been the CNN encoder-decoder model usually applied in voxel space. However, this often scales very poorly with the resolution limiting the effectiveness of these models. Several sophisticated alternatives for decoding to 3D shapes have been proposed typically relying on complex deep learning architectures for the decoder model. In this work, we show that this additional complexity is not necessary, and that we can actually obtain high quality 3D reconstruction using a linear decoder, obtained from principal component analysis on the signed distance function (SDF) of the surface. This approach allows easily scaling to larger resolutions. We show in multiple experiments that our approach is competitive with state-of-the-art methods. It also allows the decoder to be fine-tuned on the target task using a loss designed specifically for SDF transforms, obtaining further gains."
  },
  "bmvc2020_main_integratinglong-shorttermnetworkforefficientvideoobjectsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Integrating Long-Short Term Network for Efficient Video Object Segmentation",
    "authors": [
      "Jingjing Wang",
      "ZhuTeng",
      "Baopeng Zhang",
      "Jianping Fan"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0167.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0167.pdf",
    "published": "2020-09",
    "summary": "Real-world application of video object segmentation (VOS) is a very challenging problem, especially for multiple video object segmentation. The deep-learning-based approaches have recently dominated VOS by fine-tuning the networks at the first frame to seize the object dynamics, but they may result in impractical frame-rates and risk of over-fitting. To overcome this limitation, we develop an efficient and fully end-to-end model to achieve fast and accurate VOS, named Long-Short Term Network (LSTNet). It contains a long term network to encode absolute object variations and a short term network to capture relative object dynamics. The segmentation results of video objects can be directly acquired by an attentional gate operation based on these two networks. Our proposed model runs at a very high speed and can conveniently tackle multi-object segmentation without post-processing. Extensive experiments on widely used benchmarks including YouTube-VOS and DAVIS 2017 have demonstrated that our proposed model can achieve a competitive accuracy and speed in comparison to a number of state-of-the-art methods."
  },
  "bmvc2020_main_rethinkingcurriculumlearningwithincrementallabelsandadaptivecompensation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Rethinking Curriculum Learning with Incremental Labels and Adaptive Compensation",
    "authors": [
      "Madan Ravi Ganesh",
      "Jason Corso"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0581.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0581.pdf",
    "published": "2020-09",
    "summary": "Like humans, deep networks have been shown to learn better when samples are organized and introduced in a meaningful order or curriculum. Conventional curriculum learning schemes introduce samples in their order of difficulty.This forces models to begin learning from a subset of the available data while adding the external overhead of evaluating the difficulty of samples.In this work, we propose Learning with Incremental Labels and Adaptive Compensation (LILAC), a two-phase method that incrementally increases the number of unique output labels rather than the difficulty of samples while consistently using the entire dataset throughout training.In the first phase, Incremental Label Introduction, we partition data into mutually exclusive subsets, one that contains a subset of the ground-truth labels and another that contains the remaining data attached to a pseudo-label.Throughout the training process, we recursively reveal unseen ground-truth labels in fixed increments until all the labels are known to the model. In the second phase, Adaptive Compensation, we optimize the loss function using altered target vectors for previously misclassified samples. The target vectors of such samples are modified to a smoother distribution to help models learn better. On evaluating across three standard image benchmarks, CIFAR-10, CIFAR-100, and STL-10, we show that LILAC outperforms all comparable baselines. Further, we detail the importance of pacing the introduction of new labels to a model as well as the impact of using a smooth target vector."
  },
  "bmvc2020_main_comogcncoherentmotionawaretrajectorypredictionwithgraphrepresentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "CoMoGCN: Coherent Motion Aware Trajectory Prediction with Graph Representation",
    "authors": [
      "Yuying Chen",
      "Congcong LIU",
      "Bertram Shi",
      "Ming Liu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0238.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0238.pdf",
    "published": "2020-09",
    "summary": "Forecasting human trajectories is critical for tasks such as robot crowd navigation and autonomous driving. Modeling social interactions is of great importance for accurate group-wise motion prediction. However, most existing methods do not consider information about coherence within the crowd, but rather only pairwise interactions. In this work, we propose a novel framework, coherent motion aware graph convolutional network (CoMoGCN), for trajectory prediction in crowded scenes with group constraints. First, we cluster pedestrian trajectories into groups according to motion coherence. Then, we use graph convolutional networks to aggregate crowd information efficiently. The CoMoGCN also takes advantage of variational inference to capture the variability in human trajectories by modeling the distribution. Our method achieves state-of-the-art performance on several different trajectory prediction benchmarks, and the best average performance among all benchmarks considered."
  },
  "bmvc2020_main_semanticallyadaptiveimage-to-imagetranslationfordomainadaptationofsemanticsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Semantically Adaptive Image-to-image Translation for Domain Adaptation of Semantic Segmentation",
    "authors": [
      "Luigi Musto",
      "Andrea Zinelli"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0369.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0369.pdf",
    "published": "2020-09",
    "summary": "Domain shift is a very challenging problem for semantic segmentation. Any model can be easily trained on synthetic data, where images and labels are artificially generated, but it will perform poorly when deployed on real environments. In this paper, we address the problem of domain adaptation for semantic segmentation of street scenes. Many state-of-the-art approaches focus on translating the source image while imposing that the result should be semantically consistent with the input. However, we advocate that the image semantics can also be exploited to guide the translation algorithm. To this end, we rethink the generative model to enforce this assumption and strengthen the connection between pixel-level and feature-level domain alignment. We conduct extensive experiments by training common semantic segmentation models with our method and show that the results we obtain on the synthetic-to-real benchmarks surpass the state-of-the-art."
  },
  "bmvc2020_main_makingacasefor3dconvolutionsforobjectsegmentationinvideos": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Making a Case for 3D Convolutions for Object Segmentation in Videos",
    "authors": [
      "Sabarinath Mahadevan",
      "Ali Athar",
      "Aljosa Osep",
      "Laura Leal-Taix\u00e9",
      "Bastian Leibe",
      "Sebastian Hennen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0233.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0233.pdf",
    "published": "2020-09",
    "summary": "The task of object segmentation in videos is usually accomplished by processing appearance and motion information separately using standard 2D convolutional networks, followed by a learned fusion of the two sources of information. On the other hand, 3D convolutional networks have been successfully applied for video classification tasks, but have not been leveraged as effectively to problems involving dense per-pixel interpretation of videos compared to their 2D convolutional counterparts and lag behind the aforementioned networks in terms of performance. In this work, we show that 3D CNNs can be effectively applied to dense video prediction tasks such as salient object segmentation. We propose a simple yet effective encoder-decoder network architecture consisting entirely of 3D convolutions that can be trained end-to-end using a standard cross-entropy loss. To this end, we leverage an efficient 3D encoder, and propose a 3D decoder architecture, that comprises novel 3D Global Convolution layers and 3D Refinement modules. Our approach outperforms existing state-of-the-arts by a large margin on the DAVIS'16 Unsupervised, FBMS and ViSal dataset benchmarks in addition to being faster, thus showing that our architecture can efficiently learn expressive spatio-temporal features and produce high quality video segmentation masks. We have made our code and trained models publicly available at: https://github.com/sabarim/3DC-Seg"
  },
  "bmvc2020_main_high-ordergraphconvolutionalnetworksfor3dhumanposeestimation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "High-order Graph Convolutional Networks for 3D Human Pose Estimation",
    "authors": [
      "Zhiming Zou",
      "Kenkun Liu",
      "Le Wang",
      "Wei Tang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0550.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0550.pdf",
    "published": "2020-09",
    "summary": "Graph convolutional networks (GCNs) have been applied to 3D human pose estimation (HPE) from 2D body joint detections and have demonstrated promising performance. However, since the vanilla graph convolution is performed on the one-hop neighbors of each node, it is unable to capture the long-range dependencies between body joints. They can help reduce the uncertainty caused by occlusion or depth ambiguity. To resolve this issue, we propose a high-order GCN for 3D HPE. Its core building block, termed a high-order graph convolution, aggregates features of nodes at various distances. As a result, the network can model a wide range of interactions among body joints. Furthermore, we investigate different methods to fuse those multi-order features and compare how they affect the performance. Experimental results demonstrate the effectiveness of the proposed approach."
  },
  "bmvc2020_main_howtotrainyourenergy-basedmodelforregression": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "How to Train Your Energy-Based Model for Regression",
    "authors": [
      "Fredrik Gustafsson",
      "Martin Danelljan",
      "Radu Timofte",
      "Thomas Sch\u00f6n"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0154.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0154.pdf",
    "published": "2020-09",
    "summary": "Energy-based models (EBMs) have become increasingly popular within computer vision in recent years. While they are commonly employed for generative image modeling, recent work has applied EBMs also for regression tasks, achieving state-of-the-art performance on object detection and visual tracking. Training EBMs is however known to be challenging. While a variety of different techniques have been explored for generative modeling, the application of EBMs to regression is not a well-studied problem. How EBMs should be trained for best possible regression performance is thus currently unclear. We therefore accept the task of providing the first detailed study of this problem. To that end, we propose a simple yet highly effective extension of noise contrastive estimation, and carefully compare its performance to six popular methods from literature on the tasks of 1D regression and object detection. The results of this comparison suggest that our training method should be considered the go-to approach. We also apply our method to the visual tracking task, achieving state-of-the-art performance on five datasets. Notably, our tracker achieves 63.7% AUC on LaSOT and 78.7% Success on TrackingNet. Code is available at https://github.com/fregu856/ebms_regression."
  },
  "bmvc2020_main_importanceofself-consistencyinactivelearningforsemanticsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Importance of Self-Consistency in Active Learning for Semantic Segmentation",
    "authors": [
      "S. Alireza Golestaneh",
      "Kris Kitani"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0010.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0010.pdf",
    "published": "2020-09",
    "summary": "We address the task of active learning in the context of semantic segmentation and show that self-consistency can be a powerful source of self-supervision to greatly improve the performance of a data-driven model with access to only a small amount of labeled data. Self-consistency uses the simple observation that the results of semantic segmentation for a specific image should not change under transformations like horizontal flipping (i.e.,the results should only be flipped). In other words, the output of a model should be consistent under equivariant transformations. The self-supervisory signal of self-consistency is particularly helpful during active learning since the model is prone to overfitting when there is only a small amount of labeled training data. In our proposed active learning framework, we iteratively extract small image patches that need to be labeled, by selecting image patches that have high uncertainty (high entropy) under equivariant transformations. We enforce pixel-wise self-consistency between the outputs of the segmentation network for each image and its transformation (horizontally flipped) to utilize the rich self-supervisory information and reduce the uncertainty of the network. In this way, we are able to find the image patches over which the current model struggles the most to classify. By iteratively training over these difficult image patches, our experiments show that our active learning approach reaches 96% of the top performance of a model trained on all data, by using only 12% of the total data on benchmark semantic segmentation datasets (e.g., CamVid and Cityscapes)."
  },
  "bmvc2020_main_unsupervisedmonoculardepthestimationwithmulti-baselinestereo": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Unsupervised Monocular Depth Estimation with Multi-Baseline Stereo",
    "authors": [
      "Saad Imran",
      "Muhammad Umar Karim Khan",
      "Sikander Mukaram",
      "Chong-Min Kyung"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0975.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0975.pdf",
    "published": "2020-09",
    "summary": "Unsupervised deep learning methods have shown promising performance for single-image depth estimation. Since most of these methods use binocular stereo pairs for self-supervision, the depth range is generally limited. Small-baseline stereo pairs provide small depth range but handle occlusions well. On the other hand, stereo images acquired with a wide-baseline rig cause occlusions-related errors in the near range but estimate depth well in the far range. In this work, we propose to integrate the advantages of the small and wide baselines. By training the network using three horizontally aligned views, we obtain accurate depth predictions for both close and far ranges. Our strategy allows to infer multi-baseline depth from a single image. This is unlike previous multi-baseline systems which employ more than two cameras. The qualitative and quantitative results show the superior performance of multi-baseline approach over previous stereo-based monocular methods. For 0.1 to 80 meters depth range, our approach decreases the absolute relative error of depth by 24% compared to Monodepth2. Our approach provides 21 frames per second on a single Nvidia1080 GPU, making it useful for practical applications."
  },
  "bmvc2020_main_synthetictrainingforaccurate3dhumanposeandshapeestimationinthewild": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Synthetic Training for Accurate 3D Human Pose and Shape Estimation in the Wild",
    "authors": [
      "Akash Sengupta",
      "Roberto Cipolla",
      "Ignas Budvytis"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0081.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0081.pdf",
    "published": "2020-09",
    "summary": "This paper addresses the problem of monocular 3D human shape and pose estimation from an RGB image. Despite great progress in this field in terms of pose prediction accuracy, state-of-the-art methods often predict inaccurate body shapes. We suggest that this is primarily due to the scarcity of in-the-wild training data with diverse and accurate body shape labels. Thus, we propose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system that utilises proxy representations (such as silhouettes and 2D joints) as inputs to a shape and pose regression neural network, which is trained with synthetic training data (generated using the SMPL statistical body model) to overcome data scarcity. We bridge the gap between synthetic training inputs and noisy real inputs, which are predicted by keypoint detection and segmentation CNNs at test-time, by using data augmentation and corruption during training. In order to evaluate our approach, we curate and provide a challenging evaluation dataset for monocular human shape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via multi-frame optimisation. We show that STRAPS outperforms other state-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets and metrics."
  },
  "bmvc2020_main_recsaldeeprecursivesupervisionforvisualsaliencyprediction": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "RecSal : Deep Recursive Supervision for Visual Saliency Prediction",
    "authors": [
      "OindrilaSaha",
      "Sandeep Mishra"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0539.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0539.pdf",
    "published": "2020-09",
    "summary": "State-of-the-art saliency prediction methods develop upon model architectures or loss functions; while training to generate one target saliency map. However, publicly available saliency prediction datasets can be utilized to create more information for each stimulus than just a final aggregate saliency map. This information when utilized in a biologically inspired fashion can contribute in better prediction performance without the use of models with huge number of parameters. In this light, we propose to extract and use the statistics of (a) region specific saliency and (b) temporal order of fixations, to provide additional context to our network. We show that extra supervision using spatially or temporally sequenced fixations results in achieving better performance in saliency prediction. Further, we also design novel architectures for utilizing this extra information and show that it achieves superior performance over a base model which is devoid of extra supervision. We show that our best method outperforms previous state-of-the-art methods with 50-80% fewer parameters. We also show that our models perform consistently well across all evaluation metrics unlike prior methods."
  },
  "bmvc2020_main_fromquantizeddnnstoquantizablednns": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "From Quantized DNNs to Quantizable DNNs",
    "authors": [
      "Kunyuan Du",
      "Ya Zhang",
      "Haibing Guan"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0400.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0400.pdf",
    "published": "2020-09",
    "summary": "This paper proposes Quantizable DNNs, a special type of DNNs that can flexibly quantize its bit-width (denoted as `bit modes' thereafter) during execution without further re-training. To simultaneously optimize all bit modes, a combinational loss of all bit modes is proposed, which enforces consistent predictions ranging from low-bit mode to 32-bit mode. This consistency-based loss may also be viewed as certain form of regularization during training. Because outputs of matrix multiplication in different bit modes have different distributions, we introduce Bit-Specific Batch Normalization so as to reduce conflicts among different bit modes. Experiments on CIFAR100 and ImageNet have shown that compared to quantized DNNs, quantizable DNNs not only have much better flexibility, but also achieve even higher classification accuracy. Ablation studies further verify that the regularization through the consistency-based loss indeed improves the model's generalization performance. Source codes will be released in the future."
  },
  "bmvc2020_main_anovelbaselineforzero-shotlearningviaadversarialvisual-semanticembedding": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "A Novel Baseline for Zero-shot Learning via Adversarial Visual-Semantic Embedding",
    "authors": [
      "Yu Liu",
      "Tinne Tuytelaars"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0211.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0211.pdf",
    "published": "2020-09",
    "summary": "Zero-shot learning (ZSL) has been attracting ever-increasing research interest due to its capability of recognizing novel or unseen classes.A lot of studies on ZSL are based mainly on two baseline models: compatible visual-semantic embedding (CVSE) and adversarial visual feature generation (AVFG). In this work, we integrate the merits of the two baselines and propose a novel and effective baseline model, coined adversarial visual-semantic embedding (AVSE). Different from CVSE and AVFG, AVSE learns visual and semantic embeddings adversarially and jointly in a latent feature space. Additionally, AVSE integrates a classifier to make latent embeddings discriminative, and a regressor to preserve semantic consistency during the embedding procedure. Moreover, we perform embedding-to-image generation which visually exhibits the embeddings learned in AVSE.The experiments on four standard benchmarks show the advantage of AVSE over CVSE and AVFG, and empirical insights through quantitative and qualitative results. Our code is at https://github.com/Liuy8/AVSE."
  },
  "bmvc2020_main_semanticestimationof3dbodyshapeandposeusingminimalcameras": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras",
    "authors": [
      "Andrew Gilbert",
      "Matthew Trumble",
      "Adrian Hilton",
      "John Collomosse"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0324.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0324.pdf",
    "published": "2020-09",
    "summary": "We present an approach to accurately estimate high fidelity markerless 3D pose and volumetric reconstruction of human performance using only a small set of camera views ( 2). Our method utilises a dual loss in a generative adversarial network that can yield improved performance in both reconstruction and pose estimate error. We use a deep prior implicitly learnt by the network trained over a dataset of view-ablated multi-view video footage of a wide range of subjects and actions. Uniquely we use a multi-channel symmetric 3D convolutional encoder-decoder with a dual loss to enforce the learning of a latent embedding that enforces skeletal joint positions and a deep volumetric reconstruction of the performer. An extensive evaluation is performed with state of the art performance reported on three datasets; Human 3.6M, TotalCapture and TotalCaptureOutdoor. The method opens the possibility of high-end volumetric and pose performance capture in on-set and prosumer scenarios where time or cost prohibit a high witness camera count."
  },
  "bmvc2020_main_semi-supervisedsemanticsegmentationneedsstrong,variedperturbations": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Semi-supervised semantic segmentation needs strong, varied perturbations",
    "authors": [
      "Geoffrey French",
      "Samuli Laine",
      "Timo Aila",
      "Michal Mackiewicz",
      "Graham Finlayson"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0680.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0680.pdf",
    "published": "2020-09",
    "summary": "Consistency regularization describes a class of approaches that have yielded ground breaking results in semi-supervised classification problems. Prior work has established the cluster assumption - under which the data distribution consists of uniform class clusters of samples separated by low density regions - as important to its success. We analyze the problem of semantic segmentation and find that its' distribution does not exhibit low density regions separating classes and offer this as an explanation for why semi-supervised segmentation is a challenging problem, with only a few reports of success. We then identify choice of augmentation as key to obtaining reliable performance without such low-density regions. We find that adapted variants of the recently proposed CutOut and CutMix augmentation techniques yield state-of-the-art semi-supervised semantic segmentation results in standard datasets. Furthermore, given its challenging nature we propose that semantic segmentation acts as an effective acid test for evaluating semi-supervised regularizers. Implementation at: https://github.com/Britefury/cutmix-semisup-seg."
  },
  "bmvc2020_main_bihandrecoveringhandmeshwithmulti-stagebisectedhourglassnetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass Networks",
    "authors": [
      "Lixin Yang",
      "Jiasen Li",
      "Wenqiang Xu",
      "Yiqun Diao",
      "Cewu Lu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0250.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0250.pdf",
    "published": "2020-09",
    "summary": "3D hand estimation has been a long-standing research topic in computer vision. A recent trend aims not only to estimate the 3D hand joint locations but also to recover the mesh model. However, achieving those goals from a single RGB image remains challenging. In this paper, we introduce an end-to-end learnable model, BiHand, which consists of three cascaded stages, namely 2D seeding stage, 3D lifting stage, and mesh generation stage. At the output of BiHand, the full hand mesh will be recovered using the joint rotations and shape parameters predicted from the network. Inside each stage, BiHand adopts a novel bisecting design which allows the networks to encapsulate two closely related information (e.g. 2D keypoints and silhouette in 2D seeding stage, 3D joints, and depth map in 3D lifting stage, joint rotations and shape parameters in the mesh generation stage) in a single forward pass. As the information represents different geometry or structure details, bisecting the data flow can facilitate optimization and increase robustness. For quantitative evaluation, we conduct experiments on two public benchmarks, namely the Rendered Hand Dataset (RHD) and the Stereo Hand Pose Tracking Benchmark (STB). Extensive experiments show that our model can achieve superior accuracy in comparison with state-of-the-art methods, and can produce appealing 3D hand meshes in several severe conditions. The training codes, model and dataset are publicly available at https://github.com/lixiny/bihand."
  },
  "bmvc2020_main_accuratepartsvisualizationforexplainingcnnreasoningviasemanticsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Accurate Parts Visualization for Explaining CNN Reasoning via Semantic Segmentation",
    "authors": [
      "Ren Harada",
      "Antonio Tejero-de-Pablos",
      "Tatsuya Harada"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0754.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0754.pdf",
    "published": "2020-09",
    "summary": "Nowadays, neural networks are often used for image classification, but the structure of their decisions is difficult to understand because of their \"black-box\" nature. Different visualization techniques have been proposed to provide additional information on the reason of the classification results. Existing methods provide quantitative explanations by calculating heatmaps and interpretable components in the image. While the latter provides semantics on the image parts that contribute for the classification, the component areas are blurry due to the use of linear layers, which do not consider surrounding information. This makes hard to point out the specific reason for the classification and to evaluate quantitatively. In this paper, we introduce a novel method for explaining classification in neural networks, the Parts Detection Module. Unlike previous methods, ours is capable of determining the accurate position of the interpretable components in the image by performing upsampling and convolution stepwise, similarly to semantic segmentation. In addition to providing quantitative visual explanations, we also proposed a method to verify the validity of the quantitative explanations themselves. The experimental results prove the effectivity of our explanations."
  },
  "bmvc2020_main_3d-gmnetsingle-view3dshaperecoveryasagaussianmixture": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "3D-GMNet: Single-View 3D Shape Recovery as A Gaussian Mixture",
    "authors": [
      "Kohei Yamashita",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0294.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0294.pdf",
    "published": "2020-09",
    "summary": "In this paper, we introduce 3D-GMNet, a deep neural network for single-image 3D shape recovery. As the name suggests, 3D-GMNet recovers 3D shape as a Gaussian mixture model. In contrast to voxels, point clouds, or meshes, a Gaussian mixture representation requires a much smaller footprint for representing 3D shapes and, at the same time, offers a number of additional advantages including instant pose estimation, automatic level-of-detail computation, and a distance measure. The proposed 3D-GMNet is trained end-to-end with single input images and corresponding 3D models by using two novel loss functions: a 3D Gaussian mixture loss and a multi-view 2D loss. The first maximizes the likelihood of the Gaussian mixture shape representation by considering the target point cloud as samples from the true distribution, and the latter improves the consistency between the input silhouette and the projection of the Gaussian mixture shape model. Extensive quantitative evaluations with synthesized and real images demonstrate the effectiveness of the proposed method."
  },
  "bmvc2020_main_multimodalimagetranslationwithstochasticstylerepresentationsandmutualinformationloss": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Multimodal Image Translation with Stochastic Style Representations and Mutual Information Loss",
    "authors": [
      "Sanghyeon Na",
      "SeungjooYoo",
      "Jaegul Choo"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0115.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0115.pdf",
    "published": "2020-09",
    "summary": "Unpaired multimodal image-to-image translation is a task of converting a given image in a source domain into diverse images in a target domain. We propose two approaches to produce high-quality and diverse images. First, we propose to encode a source image conditioned on a given target style feature. It allows our model to generate higher-quality images than existing models, which are not based on this method. Second, we propose an information-theoretic loss function that effectively captures styles in an image. It allows our model to learn complex high-level styles rather than simple low-level styles, and generate perceptually diverse images. We show our proposed model achieves state-of-the-art performance through extensive experiments on various real-world datasets."
  },
  "bmvc2020_main_pmd-netprivilegedmodalitydistillationnetworkfor3dhandposeestimationfromasinglergbimage": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "PMD-Net: Privileged Modality Distillation Network for 3D Hand Pose Estimation from a Single RGB Image",
    "authors": [
      "Kewen Wang",
      "Xilin Chen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0413.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0413.pdf",
    "published": "2020-09",
    "summary": "3D Hand Pose Estimation from a single RGB image is a challenging task due to the significant depth ambiguities and occlusions. In this paper, we propose a Privileged Modality Distillation Network (PMD-Net), which improves the RGB-based hand pose estimation by excavating the privileged information from depth prior during training. Different from existing methods, the PMD-Net is composed of three sub-networks to regress X, Y, and Z coordinates respectively and distills the privileged information from the depth network to the RGB network by transferring constraints between corresponded layers. Furthermore, a random block replacement is adopted and a refine module is added to enhance the robustness of PMD-Net. Experiments on both synthesized and real-world hand pose estimation datasets are conducted, and extensive results demonstrate that the proposed PMD-Net achieves state-of-the-art results and is superior to existing methods."
  },
  "bmvc2020_main_deepmetriclearningmeetsdeepclusteringannovelunsupervisedapproachforfeatureembedding": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised Approach for Feature Embedding",
    "authors": [
      "Binh Nguyen",
      "Binh Nguyen",
      "Gustavo Carneiro",
      "Erman Tjiputra",
      "Quang Tran",
      "Thanh-Toan Do"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0328.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0328.pdf",
    "published": "2020-09",
    "summary": "Unsupervised Deep Distance Metric Learning (UDML) aims to learn sample similarities in the embedding space from an unlabeled dataset. Traditional UDML methods usually use the triplet loss or pairwise loss which requires the mining of positive and negative samples w.r.t. anchor data points. This is, however, challenging in an unsupervised setting as the label information is not available. In this paper, we propose a new UDML method that overcomes that challenge. In particular, we propose to use a deep clustering loss to learn centroids, i.e., pseudo labels, that represent semantic classes. During learning, these centroids are also used to reconstruct the input samples. It hence ensures the representativeness of centroids \u2014 each centroid represents visually similar samples. Therefore, the centroids give information about positive (visually similar) and negative (visually dissimilar) samples. Based on pseudo labels, we propose a novel unsupervised metric loss which enforces the positive concentration and negative separation of samples in the embedding space. Experimental results on benchmarking datasets show that the proposed approach outperforms other UDML methods."
  },
  "bmvc2020_main_daledarkregion-awarelow-lightimageenhancement": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "DALE : Dark Region-Aware Low-light Image Enhancement",
    "authors": [
      "DoKyeong Kwon",
      "Guisik Kim",
      "Junseok Kwon"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_1025.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/1025.pdf",
    "published": "2020-09",
    "summary": "In this paper, we present a novel low-light image enhancement method called dark region-aware low-light image enhancement (DALE), where dark regions are accurately recognized by the proposed visual attention module and their brightness are intensively enhanced. Our method can estimate the visual attention in an efficient manner using super-pixels without any complicated process. Thus, the method can preserve the color, tone, and brightness of original images and prevents normally illuminated areas of the images from being saturated and distorted. Experimental results show that our method accurately identifies dark regions via the proposed visual attention, and qualitatively and quantitatively outperforms state-of-the-art methods."
  },
  "bmvc2020_main_procedurecompletionbylearningfrompartialsummaries": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Procedure Completion by Learning from Partial Summaries",
    "authors": [
      "Ehsan Elhamifar",
      "Zwe Naing"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0130.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0130.pdf",
    "published": "2020-09",
    "summary": "We address the problem of procedure completion in videos, which is to find and localize all key-steps of a task given only a small observed subset of key-steps. We cast the problem as learning summarization from partial summaries that allows to incorporate prior knowledge and learn from incomplete key-steps. Given multiple pairs of (video, subset of key-steps), we address the problem by learning representations of input data and finding the remaining key-steps that generalizes well to key-step discovery in new videos. We propose a loss function on the parameters of a network that promotes to recover unseen key-steps that together with the observed key-steps optimize a desired subset selection criterion. To tackle the highly non-convex learning problem, involving both discrete and continuous variables, we develop an efficient learning algorithm that alternates between representation learning and recovering unseen key-steps while incorporating prior knowledge, via a greedy algorithm. By extensive experiments on two instructional video datasets, we show the effectiveness of our framework."
  },
  "bmvc2020_main_learningtoabstractandpredicthumanactions": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning to Abstract and Predict Human Actions",
    "authors": [
      "Romero Morais",
      "Vuong Le",
      "Truyen Tran",
      "Svetha Venkatesh"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0979.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0979.pdf",
    "published": "2020-09",
    "summary": "Human activities are naturally structured as hierarchies unrolled over time. For action prediction, temporal relations in event sequences are widely exploited by current methods while their semantic coherence across different levels of abstraction has not been well explored. In this work we model the hierarchical structure of human activities in videos and demonstrate the power of such structure in action prediction. We propose Hierarchical Encoder-Refresher-Anticipator, a multi-level neural machine that can learn the structure of human activities by observing a partial hierarchy of events and roll-out such structure into a future prediction in multiple levels of abstraction. We also introduce a new coarse-to-fine action annotation on the Breakfast Actions videos to create a comprehensive, consistent, and cleanly structured video hierarchical activity dataset. Through our experiments, we examine and rethink the settings and metrics of activity prediction tasks toward unbiased evaluation of prediction systems, and demonstrate the role of hierarchical modeling toward reliable and detailed long-term action forecasting."
  },
  "bmvc2020_main_lsd_2-jointdenoisinganddeblurringofshortandlongexposureimageswithcnns": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "LSD_2 - Joint Denoising and Deblurring of Short and Long Exposure Images with CNNs",
    "authors": [
      "Janne Mustaniemi",
      "Juho\t Kannala",
      "Jiri Matas",
      "Simo S\u00e4rkk\u00e4",
      "Janne Heikkila"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0725.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0725.pdf",
    "published": "2020-09",
    "summary": "The paper addresses the problem of acquiring high-quality photographs with handheld smartphone cameras in low-light imaging conditions. We propose an approach based on capturing pairs of short and long exposure images in rapid succession and fusing them into a single high-quality photograph. Unlike existing methods, we take advantage of both images simultaneously and perform a joint denoising and deblurring using a convolutional neural network. A novel approach is introduced to generate realistic short-long exposure image pairs. The method produces good images in extremely challenging conditions and outperforms existing denoising and deblurring methods. It also enables exposure fusion in the presence of motion blur."
  },
  "bmvc2020_main_brinettowardsbridgingtheintra-classandinter-classgapsinone-shotsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "BriNet: Towards Bridging the Intra-class and Inter-class Gaps in One-Shot Segmentation",
    "authors": [
      "Xianghui Yang",
      "Bairun Wang",
      "Xinchi Zhou",
      "Kaige Chen",
      "Shuai Yi",
      "Wanli Ouyang",
      "Luping Zhou"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0139.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0139.pdf",
    "published": "2020-09",
    "summary": "Few-shot segmentation focuses on the generalization of models to segment unseen object instances with limited training samples. Although tremendous improvements have been achieved, existing methods are still constrained by two factors. (1) The information interaction between query and support images is not adequate, leaving intra-class gap. (2) The object categories at the training and inference stages have no overlap, leaving the inter-class gap. Thus, we propose a framework, BriNet, to bridge these gaps. First, more information interactions are encouraged between the extracted features of the query and support images, i.e., using an Information Exchange Module to emphasize the common objects. Furthermore, to precisely localize the query objects, we design a multi-path fine-grained strategy which is able to make better use of the support feature representations. Second, a new online refinement strategy is proposed to help the trained model adapt to unseen classes, achieved by switching the roles of the query and the support images at the inference stage. The effectiveness of our framework is demonstrated by experimental results, which outperforms other competitive methods and leads to a new state-of-the-art on both PASCAL VOC and MSCOCO dataset."
  },
  "bmvc2020_main_synchronousbidirectionallearningformultilinguallipreading": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Synchronous Bidirectional Learning for Multilingual Lip Reading",
    "authors": [
      "Mingshuang Luo",
      "Shuang Yang",
      "Xilin Chen",
      "Zitao Liu",
      "Shiguang Shan"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0796.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0796.pdf",
    "published": "2020-09",
    "summary": "Lip reading has received increasing attention in recent years. This paper focuses on the synergy of multilingual lip reading. There are about as many as 7,000 languages in the world, which implies that it is impractical to train separate lip reading models with large-scale data for each language. Although each language has its own linguistic and pronunciation rules, the lip movements of all languages share similar patterns due to the common structures of human organs. Based on this idea,we try to explore the synergized learning of multilingual lip reading in this paper, and further propose a synchronous bidirectional learning (SBL) framework for effective synergy of multilingual lip reading. We firstly introduce phonemes as our modeling units for the multilingual setting here. Phonemes are more closely related with the lip movements than the alphabet letters. At the same time, similar phonemes always lead to similar visual patterns no matter which type the target language is. Then, a novel SBL block is proposed to learn the rules for each language in a fill-in-the-blank way. Specifically, the model has to learn to infer the target unit given its bidirectional context, which could represent the composition rules of phonemes for each language. To make the learning process more targeted at each particular language, an extra task of predicting the language identity is introduced in the learning process. Finally, a thorough comparison on LRW (English) and LRW-1000 (Mandarin) is performed, which shows the promising benefits from the synergized learning of different languages and also reports a new state-of-the-art result on both datasets."
  },
  "bmvc2020_main_adaptationacrossextremevariationsusingunlabeledbridges": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Adaptation Across Extreme Variations using Unlabeled Bridges",
    "authors": [
      "Shuyang Dai",
      "Kihyuk Sohn",
      "Yi-Hsuan Tsai",
      "Lawrence Carin Duke",
      "Manmohan Chandraker"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0186.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0186.pdf",
    "published": "2020-09",
    "summary": "We tackle an unsupervised domain adaptation problem for which the domain discrepancy between labeled source and unlabeled target domains is large, due to many factors of inter- and intra-domain variation. While deep domain adaptation methods have been realized by reducing the domain discrepancy, these are difficult to apply when domains are significantly different. We propose to decompose domain discrepancy into multiple but smaller, and thus easier to minimize, discrepancies by introducing unlabeled bridging domains that connect the source and target domains. We realize our proposed approach through an extension of the domain adversarial neural network with multiple discriminators, each of which accounts for reducing discrepancies between unlabeled (bridge, target) domains and a mix of all precedent domains including source. We validate the effectiveness of our method on several adaptation tasks including object recognition and semantic segmentation."
  },
  "bmvc2020_main_intrinsicdecompositionofdocumentimagesin-the-wild": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Intrinsic Decomposition of Document Images In-the-Wild",
    "authors": [
      "Sagnik Das",
      "Hassan Sial",
      "Ke Ma",
      "Ram\u00f3n Baldrich",
      "Maria Vanrell",
      "Dimitris Samaras"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0906.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0906.pdf",
    "published": "2020-09",
    "summary": "Automatic document content processing is affected by artifacts caused by the shape of the paper, non-uniform and diverse color of lighting conditions. Fully-supervised methods on real data are impossible due to the large amount of data needed. Hence, the current state of the art deep learning models are trained on fully or partially synthetic images. However, document shadow or shading removal results still suffer because: (a) prior methods rely on uniformity of local color statistics, which limit their application on real-scenarios with complex document shapes and textures and; (b) synthetic or hybrid datasets with non-realistic, simulated lighting conditions are used to train the models.In this paper we tackle these problems with our two main contributions. First, a physically constrained learning-based method that directly estimates document reflectance based on intrinsic image formation which generalizes to challenging illumination conditions. Second, a new dataset that clearlyimproves previous synthetic ones, by adding a large range of realistic shading and diverse multi-illuminant conditions, uniquely customized to deal with documents in-the-wild. The proposed architecture works in two steps. First, a white balancing module neutralizes the color of the illumination on the input image. Based on the proposed multi-illuminant dataset we achieve a good white-balancing in really difficult conditions. Second, the shading separation module accurately disentangles the shading and paper material in a self-supervised manner where only the synthetic texture is used as a weak training signal (obviating the need for very costly ground truth with disentangledversions of shading and reflectance). The proposed approach leads to significant generalization of document reflectance estimation in real scenes with challenging illumination. We extensively evaluate on the real benchmark datasets available for intrinsic image decomposition and document shadow removal tasks. Our reflectance estimation scheme, when used as a pre-processing step of an OCR pipeline, shows a 21 % improvement of character error rate (CER), thus, proving the practical applicability."
  },
  "bmvc2020_main_neighboursmatterimagecaptioningwithsimilarimages": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Neighbours Matter: Image Captioning with Similar Images",
    "authors": [
      "Qingzhong Wang",
      "Jiuniu Wang",
      "Antoni Chan",
      "Siyu Huang",
      "Haoyi Xiong",
      "Xingjian Li",
      "Dejing Dou"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0342.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0342.pdf",
    "published": "2020-09",
    "summary": "Most image captioning models aim to generate captions based solely on the input image.However images that are similar to the given input imagecontain variations of the same or similar concepts as the input image. Thus, aggregating information over similar images could be used to improve image captioning models, by strengthening or inferring concepts that are in the input image. In this paper, we propose an image captioning model based on KNN graphs composed of the input image and its similar images, where each node denotes an image or a caption.An attention-in-attention (AiA)model is developed to refine the node representations. Using the refined features significantly improves the baseline performance, eg, CIDEr score obtained by Updown model increases from 120.1 to 125.6. Compared with the state-of-the-art performance, our proposed method obtains 129.3 of CIDEr and 22.6 of SPICE on Karpathy's test split, which is competitive with the models that employ fine-grained image features such as scene graphs and image parsing trees."
  },
  "bmvc2020_main_inducingpredictiveuncertaintyestimationforfaceverification": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Inducing Predictive Uncertainty Estimation for Face Verification",
    "authors": [
      "Weidi Xie",
      "Jeffrey Byrne (STR",
      "Visym Labs)",
      "Andrew Zisserman"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0149.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0149.pdf",
    "published": "2020-09",
    "summary": "Knowing when an output can be trusted is critical for reliably using face recognition systems. While there has been enormous effort in recent research on improving face verification performance, understanding when a model's predictions should or should not be trusted has received far less attention. Our goal is a method can predict a confidence score for a face image that reflects its quality in terms of recognizable information. To this end, we propose a method for generating image quality training data automatically from `mated-pairs' of face images, and use the generated data to train a lightweight Predictive Confidence Network, termed as PCNet, for estimating the confidence score of a face image. We systematically evaluate the usefulness of PCNet using its error versus reject performance, and demonstrate that it can be universally paired with and improve the robustness of any verification model. We describe three use cases on the public IJB-C face verification benchmark: (i) toimprove 1:1 image-based verification error rates by rejecting low-quality face images; (ii) to improve quality score based fusion performance on the 1:1 set-based verification benchmark; and (iii) its use as a quality measure for selecting high quality (unblurred, good lighting, more frontal) faces from a collection, e.g. for automatic enrolment or display."
  },
  "bmvc2020_main_contrastively-reinforcedattentionconvolutionalneuralnetworkforfine-grainedimagerecognition": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Contrastively-reinforced Attention Convolutional Neural Network for Fine-grained Image Recognition",
    "authors": [
      "Dichao Liu",
      "Yu Wang",
      "Jien Kato",
      "Kenji Mase"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0656.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0656.pdf",
    "published": "2020-09",
    "summary": "Fine-grained visual classification is inherently challenging because of its inter-class similarity and intra-class variance. However, by contrasting the images with same/different labels, a human can instinctively notice that the key clues lie in certain objects while other objects are ignorable. Inspired by this, we propose Contrastively-reinforced Attention Convolutional Neural Network (CRA-CNN), which reinforces the attention awareness of deep activations. CRA-CNN mainly contains two parts: the classification stream and attention regularization stream. The former classifies the input image and simultaneously divides the visual information of the input into attention and redundancy.The latter evaluates the attention/redundancy proposal by classifying the attention and contrasting the attention/redundancy of various inputs. The evaluation information is backpropagated and forces the classification stream to improve its awareness of visual attention, which helps classification. Experimental results on CUB-Birds and Stanford Cars show that CRA-CNN distinctly outperforms the baselines and is comparable with state-of-art studies despite its simplicity."
  },
  "bmvc2020_main_robustunsupervisedcleaningofunderwaterbathymetricpointclouddata": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Robust Unsupervised Cleaning of Underwater Bathymetric Point Cloud Data",
    "authors": [
      "Cong Chen",
      "AbelGawel",
      "Stephen Krauss",
      "Yuliang Zou",
      "Amos Abbott",
      "Daniel Stilwell"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0977.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0977.pdf",
    "published": "2020-09",
    "summary": "This paper presents a novel unified one-stage unsupervised learning framework forpoint cloud cleaning of noisy partial data from underwater side-scan sonars. By combining a swath-based point cloud tensor representation, an adaptive multi-scale feature encoder, and a generative Bayesian framework, the proposed method provides robust sonarpoint cloud denoising, completion, and outlier removal simultaneously. The condensed swath-based tensor representation preserves point cloud of underlying three-dimensionalgeometry of point cloud by reconstructing spatial and temporal correlation of sonar data.The adaptive multi-scale feature encoder distinguishes noisypartialtensordata without handcrafted feature labeling by utilizing CANDECOMP/PARAFAC tensor factorization. Each local embedded outlier feature under various scales is aggregated into aglobal context by a generative Bayesian framework. The model is automatically inferredby a variational Bayesian, without parameter tuning and model pre-training. Extensive experiments on large scale synthetic and real data demonstrates the robustness against environmental perturbation. The proposed algorithm compares favourably with existing methods."
  },
  "bmvc2020_main_uncoveringhiddenchallengesinquery-basedvideomomentretrieval": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Uncovering Hidden Challenges in Query-Based Video Moment Retrieval",
    "authors": [
      "Mayu Otani",
      "Yuta Nakashima",
      "Esa Rahtu",
      "Janne Heikkila"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0306.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0306.pdf",
    "published": "2020-09",
    "summary": "The query-based moment retrieval is a problem of localising a specific clip from an untrimmed video according a query sentence. This is a challenging task that requires interpretation of both the natural language query and the video content. Like in many other areas in computer vision and machine learning, the progress in query-based moment retrieval is heavily driven by the benchmark datasets and, therefore, their quality has significant impact on the field. In this paper, we present a series of experiments assessing how well the benchmark results reflect the true progress in solving the moment retrieval task. Our results indicate substantial biases in the popular datasets and unexpected behaviour of the state-of-the-art models. Moreover, we present new sanity check experiments and approaches for visualising the results. Finally, we suggest possible directions to improve the temporal sentence grounding in the future."
  },
  "bmvc2020_main_albareinforcementlearningforvideoobjectsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "ALBA: Reinforcement Learning for Video Object Segmentation",
    "authors": [
      "Shreyank Gowda",
      "Panagiotis Eustratiadis",
      "Timothy Hospedales",
      "Laura Sevilla-Lara"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0226.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0226.pdf",
    "published": "2020-09",
    "summary": "We consider the challenging problem of zero-shot video object segmentation (VOS). That is, segmenting and tracking multiple moving objects within a video fully automatically, without any manual initialization. We treat this as a grouping problem by exploiting object proposals and making a joint inference about grouping over both space and time. We propose a network architecture for tractably performing proposal selection and joint grouping. Crucially, we then show how to train this network with reinforcement learning so that it learns to perform the optimal non-myopic sequence of grouping decisions to segment the whole video. Unlike standard supervised techniques, this also enables us to directly optimize for the non-differentiable overlap-based metrics used to evaluate VOS. We show state-of-the-art results on DAVIS-2017 and Youtube-VOS benchmarks."
  },
  "bmvc2020_main_unsuperviseddomainadaptationforspatio-temporalactionlocalization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Unsupervised Domain Adaptation for Spatio-Temporal Action Localization",
    "authors": [
      "Nakul Agarwal",
      "Yi-Ting Chen",
      "Behzad Dariush",
      "Ming-Hsuan Yang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0166.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0166.pdf",
    "published": "2020-09",
    "summary": "Spatio-temporal action localization is an important problem in computer vision that involves detecting where and when activities occur, and therefore requires modeling of both spatial and temporal features. This problem is typically formulated in the context of supervised learning, where the learned classifiers operate on the premise that both training and test data are sampled from the same underlying distribution. However, this assumption does not hold when there is a significant domain shift, leading to poor generalization performance on the test data. To address this, we focus on the hard and novel task of generalizing training models to test samples without access to any labels from the latter for spatio-temporal action localization by proposing an end-to-end unsupervised domain adaptation algorithm. We extend the state-of-the-art object detection framework to localize and classify actions. In order to minimize the domain shift, three domain adaptation modules at image level (temporal and spatial) and instance level (temporal) are designed and integrated. We design a new experimental setup and evaluate the proposed method and different adaptation modules on the UCF-Sports, UCF-101 and JHMDB benchmark datasets. We show that significant performance gain can be achieved when spatial and temporal features are adapted separately, or jointly for the most effective results."
  },
  "bmvc2020_main_semi-supervisedactivelearningforinstancesegmentationviascoringpredictions": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Semi-supervised Active Learning for Instance Segmentation via Scoring Predictions",
    "authors": [
      "Jun Wang",
      "Shaoguo Wen",
      "Jianghua Yu",
      "Kaixing Chen",
      "Xin Zhou",
      "Peng Gao",
      "Guotong Xie",
      "Changsheng Li"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0031.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0031.pdf",
    "published": "2020-09",
    "summary": "Active learning generally involves querying the most representative samples for human labeling, which has been widely studied in many fields such as image classification and object detection. However, its potential has not been explored in the more complex instance segmentation task that usually has relatively higher annotation cost. In this paper, we propose a novel and principled semi-supervised active learning framework for instance segmentation. Specifically, we present an uncertainty sampling strategy named Triplet Scoring Predictions (TSP) to explicitly incorporate samples ranking clues from classes, bounding boxes and masks. Moreover, we devise a progressive pseudo labeling regime using the above TSP in semi-supervised manner, it can leverage both the labeled and unlabeled data to minimize labeling effort while maximize performance of instance segmentation. Results on medical images datasets demonstrate that the proposed method results in the embodiment of knowledge from available data in a meaningful way. The extensive quantitatively and qualitatively experiments show that, our method can yield the best-performing model with notable less annotation costs, compared with state-of-the-arts."
  },
  "bmvc2020_main_weaklypairedmulti-domainimagetranslation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Weakly Paired Multi-Domain Image Translation",
    "authors": [
      "Marc Yanlong Zhang",
      "Zhiwu Huang",
      "Danda Pani Paudel",
      "Janine Thoma",
      "Luc Van Gool"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0841.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0841.pdf",
    "published": "2020-09",
    "summary": "In this paper, we aim at studying the new problem of weakly paired multi-domain image translation. To this end, we collect a dataset that contains weakly paired images from multiple domains. Two images are considered to be weakly paired if they are captured from nearby locations and share an overlapping field of view. These images are possibly captured by two asynchronous cameras\u2014often resulting in images from separate domains, e.g. summer and winter. Major motivations for using weakly paired images are: (i) performance improvement towards that of paired data; (ii) cheap labels and abundant data availability. For the first time in this paper, we propose a multi-domain image translation method specifically designed for weakly paired data. The proposed method consists of an attention-based generator and a two-stream discriminator that deals with misalignment between source and target images. Our method generates images in the target domain while preserving source image content, including foreground objects such as cars and pedestrians. Our extensive experiments demonstrate the superiority of the proposed method in comparison to the state-of-the-art. The new dataset and the source code are available at https://github.com/zhangma123/weaklypaired."
  },
  "bmvc2020_main_cascadedchannelpruningusinghierarchicalself-distillation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Cascaded channel pruning using hierarchical self-distillation",
    "authors": [
      "Roy Miles",
      "Krystian Mikolajczyk"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0525.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0525.pdf",
    "published": "2020-09",
    "summary": "In this paper, we propose an approach for filter-level pruning with hierarchical knowledge distillation based on the teacher, teaching-assistant, and student framework.Our method makes use of teaching assistants at intermediate pruning levels that share the same architecture and weights as the target student.We propose to prune each model independently using the gradient information from its corresponding teacher. By considering the relative sizes of each student-teacher pair, this formulation provides a natural trade-off between the capacity gap for knowledge distillation and the bias of the filter saliency updates.Our results show improvements in the attainable accuracy and model compression across the CIFAR10 and ImageNet classification tasks using the VGG16 and ResNet50 architectures.We provide an extensive evaluation that demonstrates the benefits of using a varying number of teaching assistant models at different sizes."
  },
  "bmvc2020_main_visualizingpointcloudclassifiersbycurvaturesmoothing": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Visualizing point cloud classifiers by curvature smoothing",
    "authors": [
      "Chen Ziwen",
      "Wenxuan Wu",
      "Zhongang Qi",
      "Li Fuxin"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0161.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0161.pdf",
    "published": "2020-09",
    "summary": "Recently, several networks that operate directly on point clouds have been proposed. There is significant utility in understanding their mechanisms to classify point clouds, which can potentially help diagnosing them and designing better architectures.In this paper, we propose a novel learning-based approach to visualize features important to the point cloud classifiers. Our approach is based on deleting and inserting curvatures on a point cloud. The resulting point cloud is then evaluated on the original point cloud network to assess the importance of the feature. A technical contribution of the paper is an approximated curvature smoothing algorithm, which can smoothly transition from the original point cloud to one of constant curvature, such as a uniform sphere. We propose PCI-GOS (Point Cloud Integrated-Gradients Optimized Saliency), a visualization technique that can automatically find the minimal saliency map that covers the most important features on a shape. Experiment results revealed insights into those classifiers."
  },
  "bmvc2020_main_towardsahypothesisonvisualtransformationbasedself-supervision": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Towards a Hypothesis on Visual Transformation based Self-Supervision",
    "authors": [
      "Dipan Pal",
      "Sreena Nallamothu",
      "Marios Savvides"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0864.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0864.pdf",
    "published": "2020-09",
    "summary": "We propose the first qualitative hypothesis characterizing the behavior of visual transformation based self-supervision, called the VTSS hypothesis. Given a dataset upon which a self-supervised task is performed while predicting instantiations of a transformation, the hypothesis states that if the predicted instantiations of the transformations are already present in the dataset, then the representation learned will be less useful. The hypothesis was derived by observing a key constraint in the application of self-supervision using a particular transformation. This constraint, which we term the transformation conflict for this paper, forces a network to learn degenerative features thereby reducing the usefulness of the representation. The VTSS hypothesis helps us identify transformations that have the potential to be effective as a self-supervision task. Further, it helps to generally predict whether a particular transformation based self-supervision technique would be effective or not for a particular dataset. We provide extensive evaluations on CIFAR 10, CIFAR 100, SVHN and FMNIST confirming the hypothesis and the trends it predicts. We also proposenovel cost-effective self-supervision techniques based on translation and scale, which when combined with rotation outperform all transformations applied individually. Overall, the aim of this paper is to shed light on the phenomenon of visual transformation based self-supervision."
  },
  "bmvc2020_main_sia-gcnaspatialinformationawaregraphneuralnetworkwith2dconvolutionsforhandposeestimation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D Convolutions for Hand Pose Estimation",
    "authors": [
      "Deying Kong",
      "Haoyu Ma",
      "Xiaohui Xie"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0066.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0066.pdf",
    "published": "2020-09",
    "summary": "Graph Neural Networks (GNNs) generalize neural networks from applications on regular structures to applications on arbitrary graphs, and have shown success in many application domains such as computer vision, social networks and chemistry. In this paper, we extend GNNs along two directions: a) allowing features at each node to be represented by 2D spatial confidence maps instead of 1D vectors; and b) proposing an efficient operation to integrate information from neighboring nodes through 2D convolutions with different learnable kernels at each edge.The proposed SIA-GCN can efficiently extract spatial information from 2D maps at each node and propagate them through graph convolution. By associating each edge with a designated convolution kernel, the SIA-GCN could capture different spatial relationships for different pairs of neighboring nodes. We demonstrate the utility of SIA-GCN on the task of estimating hand keypoints from single-frame images, where the nodes represent the 2D coordinate heatmaps ofkeypoints and the edges denote the kinetic relationships between keypoints.Experiments on multiple datasets show thatSIA-GCN provides a flexible and yet powerful framework to account for structural constraints between keypoints, and can achieve state-of-the-art performance on the task of hand pose estimation."
  },
  "bmvc2020_main_self-supervisedlearningforfacialactionunitrecognitionthroughtemporalconsistency": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Self-Supervised Learning for Facial Action Unit Recognition through Temporal Consistency",
    "authors": [
      "Liupei Lu",
      "Leili Tavabi",
      "Mohammad Soleymani"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0861.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0861.pdf",
    "published": "2020-09",
    "summary": "Facial expressions have inherent temporal dependencies that can be leveraged in automatic facial expression analysis from videos. In this paper, we propose a self-supervised representation learning method for facial Action Unit (AU) recognition through learning temporal consistencies in videos. To this end, we use a triplet-based ranking approach that learns to rank the frames based on their temporal distance from an anchor frame. Instead of manually labeling informative triplets, we randomly select an anchor frame along with two additional frames with predefined distances from the anchor as positive and negative. To develop an effective metric learning approach, we introduce an aggregate ranking loss by taking the sum of multiple triplet losses to allow pairwise comparisons between adjacent frames. A Convolutional Neural Network (CNN) is used as encoder to learn representations by minimizing the objective loss. We demonstrate that our encoder learns meaningful representations for AU recognition with no labels. The encoder is evaluated for AU detection on various detasets including BP4D, EmotioNet and DISFA. Our results are comparable or superior to the state-of-the-art AU recognition through self-supervised learning."
  },
  "bmvc2020_main_sketchhealeragraph-to-sequencenetworkforrecreatingpartialhumansketches": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "SketchHealer: A Graph-to-Sequence Network for Recreating Partial Human Sketches",
    "authors": [
      "Guoyao Su",
      "Yonggang Qi",
      "Kaiyue Pang",
      "Jie Yang",
      "Yi-Zhe Song"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0711.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0711.pdf",
    "published": "2020-09",
    "summary": "To perceive and create a whole from parts is a prime trait of the human visual system. In this paper, we teach machines to perform a similar task by recreating a vectorised human sketch from its incomplete parts. This is fundamentally different to prior work on image completion (i) sketches exhibit a severe lack of visual cue and are of a sequential nature, and more importantly (ii) we ask for an agent that does not just fill in a missing part, but to recreate a novel sketch that closely resembles the partial input from scratch. Central to our contribution is a graph model that encodes both the visual and structural features over multiple categories. A novel sketch graph construction module is proposed that leverages the sequential nature of sketches to associate key parts centred around stroke junctions. The intuition is then that message passing within the said graph will naturally provide the healing power when it comes to missing parts (nodes). Finally, an off-the-shelf LSTM-based decoder is employed to decode sketches in a vectorised fashion. Both qualitative and quantitative results show that the proposed model significantly outperforms state-of-the-art alternatives."
  },
  "bmvc2020_main_bayesiangeodesicregressiononriemannianmanifolds": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Bayesian Geodesic Regression on Riemannian Manifolds",
    "authors": [
      "Youshan Zhang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0901.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0901.pdf",
    "published": "2020-09",
    "summary": "Geodesic regression has been proposed for fitting the geodesic curve. However, it cannot automatically choose the dimensionality of data. In this paper, we develop a Bayesian geodesic regression model on Riemannian manifolds (BGRM) model. To avoid the overfitting problem, we add a regularization term to control the effectiveness of the model. To automatically select the dimensionality, we develop a prior for the geodesic regression model, which can automatically select the number of relevant dimensions by driving unnecessary tangent vectors to zero. To show the validation of our model, we first apply it in the 3D synthetic sphere and 2D pentagon data. We then demonstrate the effectiveness of our model in reducing the dimensionality and analyzing shape variations of human corpus callosum and mandible data."
  },
  "bmvc2020_main_constrainedvideofaceclusteringusing1nnrelations": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Constrained Video Face Clustering using1NN Relations",
    "authors": [
      "Vicky Kalogeiton",
      "Andrew Zisserman"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0899.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0899.pdf",
    "published": "2020-09",
    "summary": "In this work, we introduce the Constrained first nearest neighbour Clustering (C1C) method for video face clustering. Using the premise that the first nearest neighbour (1NN) of an instance is sufficient to discover large chains and groupings, C1C builds upon the hierarchical clustering method FINCH by imposing must-link and cannot-link constraints acquired in a self-supervised manner. We show that adding these constraints leads to performance improvements with a low computational cost. C1C is easily scalable and does not require any training. Additionally, we introduce a new Friends dataset for evaluating the performance of face clustering algorithms. Given that most video datasets for face clustering are saturated or emphasize only the main characters, the Friends dataset is larger, contains identities for several main and secondary characters, and tackles more challenging cases as it labels also the `back of the head\u2019. We evaluate C1C on the Big Bang Theory, Buffy, and Sherlock datasets for video face clustering, and show that it achieves the new state of the art whilst setting the baseline on Friends."
  },
  "bmvc2020_main_improvedtrainablecalibrationmethodforneuralnetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Improved Trainable Calibration Method for Neural Networks",
    "authors": [
      "Gongbo Liang",
      "Yu Zhang",
      "Xiaoqing Wang",
      "Nathan Jacobs"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0059.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0059.pdf",
    "published": "2020-09",
    "summary": "Recent works have shown that modern neural networks can achieve super-human performance in a wide range of image classification tasks. However, these works have primarily focused on classification accuracy, ignoring the important role of uncertainty quantification in medical decision-making. Empirically, neural networks are often miscalibrated and dramatically overconfident in their predictions. This miscalibration could be problematic in any automatic decision-making system, but we focus on the medical field in which neural network miscalibration has the potential to lead to significant treatment errors. We propose a novel approach to neural network calibration that maintains the overall classification accuracy while significantly improving model calibration. The proposed approach is based on ECE, which is a standard metric for quantifying model calibration error. As such, it is a natural and empirical way of assessing model calibration. Our approach can be easily integrated into any classification task as an auxiliary loss term, thus not requiring an explicit training round for calibration. We show that our approach reduces calibration error significantly across various architectures and datasets and that it performs better than temperature scaling, the current state-of-the-art approach."
  },
  "bmvc2020_main_anti-littersurveillancebasedonpersonunderstandingviamulti-tasklearning": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Anti-Litter Surveillance based on Person Understanding via Multi-Task Learning",
    "authors": [
      "Kangmin Bae",
      "Kimin Yun",
      "Hyung-Il Kim",
      "Youngwan Lee",
      "Jongyoul Park"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0279.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0279.pdf",
    "published": "2020-09",
    "summary": "In this paper, we propose a new framework for an anti-litter visual surveillance system to prevent garbage dumping as a real-world application. There have been many efforts to deploy an action recognition based visual surveillance system. However, many conventional methods were overfitted for only specific scenes due to hand-crafted rules and lack of real-world data. To overcome this problem, we propose a novel algorithm that handles the diverse scene properties of the real-world surveillance. In addition to collecting data from the real-world, we train the effective model to understand the person through multiple datasets such as human poses, human coarse action (e.g., upright, bent), and fine action (e.g., pushing a cart) via multi-task learning. As a result, our approach eliminates the need for scene-by-scene tuning and provides robustness to behavior understanding performance in a visual surveillance system. In addition, we propose a new object detection network that is optimized for detecting carryable objects and a person. The proposed detection network reduces the computational cost by specifying potential suspects only to the person who carries an object. Our method outperforms the state-of-the-art methods in detecting the garbage dumping action on real\u2010world surveillance video dataset."
  },
  "bmvc2020_main_lossfunctionsforpersonimagegeneration": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Loss Functions for Person Image Generation",
    "authors": [
      "Haoyue Shi",
      "Le Wang",
      "Wei Tang",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0406.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0406.pdf",
    "published": "2020-09",
    "summary": "Pose-guided person image generation aims to transform a source person image to a target pose. It is an ill-posed problem as we often need to generate pixels that are invisible in the source image. Recent works focus on designing new architectures of deep neural networks and have shown promising results. However, they simply adopt the loss functions commonly used for generic image synthesis and restoration, e.g., L1 loss, adversarial loss, and perceptual loss. This can be suboptimal due to the unique appearance and structure patterns of person images. In this paper, we first have a comprehensive study of the strengths and weaknesses of these prior loss functions for person image generation. We also consider the structural similarity index (SSIM) as a loss function since it is widely used as the evaluation metric and can capture the perceptual quality of generated images. Moreover, motivated by the observation that a person can be divided into part regions with homogeneous pixel values or textures, we extend the SSIM into a novel part-based similarity loss to explicitly account for the articulated body structure. Quantitative and qualitative results indicate that (1) using different loss functions significantly impacts the generated person images and (2) the proposed part-based loss is complementary to the prior losses and helps improve the performance."
  },
  "bmvc2020_main_robustscenetextrecognitionthroughadaptiveimageenhancement": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Robust Scene Text Recognition Through Adaptive Image Enhancement",
    "authors": [
      "Ye Qian",
      "Yuyang Wang",
      "Feng Su"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0257.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0257.pdf",
    "published": "2020-09",
    "summary": "Scene text in natural images often has a complex and varied appearance and a variety of degradations, which pose a great challenge to the reliable recognition of text. In this paper, we propose a novel scene text recognition method that introduces an effective, end-to-end trainable text image enhancement network prior to an attention-based recognition network, which adaptively improves the text image and enhances the performance of the whole recognition model. Specifically, the enhancement network combines a novel hierarchical residual enhancement network, which generates and refines pixel-wise enhancement details that are added to the input text image, and a spatial rectification network regularizing the shape of the text. Through end-to-end training with the recognition network in a weak supervision way with word annotations only, the enhancement network effectively learns to transform the text image to a more favorable form for subsequent recognition. The state-of-the-art results on several standard benchmarks demonstrate the effectiveness of our enhancement-based scene text recognition method."
  },
  "bmvc2020_main_learning-basedregionselectionforend-to-endgazeestimation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning-based Region Selection for End-to-End Gaze Estimation",
    "authors": [
      "Xucong Zhang",
      "Yusuke Sugano",
      "Andreas Bulling",
      "Otmar Hilliges"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0086.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0086.pdf",
    "published": "2020-09",
    "summary": "Traditionally, appearance-based gaze estimation methods use statically defined face regions as input to the gaze estimator, such as eye patches, and therefore suffer from difficult lighting conditions and extreme head poses for which these regions are often not the most informative with respect to the gaze estimation task. We posit that facial regions should be selected dynamically based on the image content and propose a novel gaze estimation method that combines the task of region proposal and gaze estimation into a single end-to-end trainable framework. We introduce a novel loss that allows for unsupervised training of a region proposal network alongside the (supervised) training of the final gaze estimator. We show that our method can learn meaningful region selection strategies and outperforms fixed region approaches. We further show that our method performs particularly well for challenging cases, i.e., those with difficult lighting conditions such as directional lights, extreme head angles, or self-occlusion. Finally, we show that the proposed method achieves better results than the current state-of-the-art method in within and cross-dataset evaluations."
  },
  "bmvc2020_main_localizingnovelattendedobjectsinegocentricviews": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Localizing Novel Attended Objects in Egocentric Views",
    "authors": [
      "Shujon Naha",
      "Md Reza",
      "Chen Yu",
      "David Crandall"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0014.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0014.pdf",
    "published": "2020-09",
    "summary": "People have foveated vision and thus are generally able to attend to just a single object within their field of view at a time. Our goal is to learn a model that can automatically identify which object is being attended, given a person\u2019s field of view captured by a first person camera. This problem is different from traditional salient object detection because our goal is not to identify all of the salient objects in the scene, but to identify the single object to which the camera wearer is attending. We present a model that learns based on very weak supervision, with just annotations of the label of the class that is attended in each frame, without bounding boxes or other spatial location information. We show that by learning disentangled representations for localization and classification, our model can effectively localize novel attended objects that were never seen during training. We propose a multi-stage knowledge distillation strategy to train our generalized localizer model.To the best of our knowledge, our work is the first to explore the problem of learning generalized attended object localization models in egocentric views under weak supervision."
  },
  "bmvc2020_main_imitatingunknownpoliciesviaexploration": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Imitating Unknown Policies via Exploration",
    "authors": [
      "Nathan Gavenski",
      "Juarez Monteiro",
      "Roger Granada",
      "Felipe Meneguzzi",
      "Rodrigo Barros"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0774.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0774.pdf",
    "published": "2020-09",
    "summary": "Behavioral cloning is an imitation learning technique that teaches an agent how to behave through expert demonstrations. Recent approaches use self-supervision of fully-observable unlabeled snapshots of the states to decode state-pairs into actions. However, the iterative learning scheme from these techniques are prone to getting stuck into bad local minima. We address these limitations incorporating a two-phase model into the original framework, which learns from unlabeled observations via exploration, substantially improving traditional behavioral cloning by exploiting (i) a sampling mechanism to prevent bad local minima, (ii) a sampling mechanism to improve exploration, and (iii) self-attention modules to capture global features. The resulting technique outperforms the previous state-of-the-art in four different environments by a large margin."
  },
  "bmvc2020_main_explicitknowledgedistillationfor3dhandposeestimationfrommonocularrgb": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Explicit Knowledge Distillation for 3D Hand Pose Estimation from Monocular RGB",
    "authors": [
      "Yumeng Zhang",
      "Li Chen",
      "Yufeng Liu",
      "Wen Zheng",
      "JunHai Yong"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0242.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0242.pdf",
    "published": "2020-09",
    "summary": "RGB-based 3D hand pose estimation methods frequently produce physiologically invalid gestures due to depth ambiguity and self-occlusion. Existing methods typically adopt complex networks and a large amount of data to avoid invalid gestures by automatically mining the physical constraints of the hand. These networks exhibit high computational complexity and thus are difficult to be deployed into mobile devices. In consideration of this problem, a novel knowledge distillation framework, called Explicit Knowledge Distillation, is proposed to enhance the performance of small pose estimation networks. The proposed teacher network has interpretable knowledge, explicitly passing the physical constraints to the student network. Experimental results on three benchmark datasets with different sized models demonstrate the potential of our approach."
  },
  "bmvc2020_main_payingmoreattentiontosnapshotsofiterativepruningimprovingmodelcompressionviaensembledistillation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Paying more Attention to Snapshots of Iterative Pruning: Improving Model Compression via Ensemble Distillation",
    "authors": [
      "Duong Le",
      "Nhan Vo",
      "Nam Thoai"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0817.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0817.pdf",
    "published": "2020-09",
    "summary": "Network pruning is one of the most dominant methods for reducing the heavy inference cost of deep neural networks. Existing methods often iteratively prune networks to attain high compression ratio without incurring significant loss in performance. However, we argue that conventional methods for retraining pruned networks (i.e., using small, fixed learning rate) are inadequate as they completely ignore the benefits from snapshots of iterative pruning. In this work, we show that strong ensembles can be constructed from snapshots of iterative pruning, which achieve competitive performance and vary in network structure.Furthermore, we present a simple, general and effective pipeline that generates strong ensembles of networks during pruning with textit{large learning rate restarting}, and utilizes knowledge distillation with those ensembles to improve the predictive power of compact models. In standard image classification benchmarks such as CIFAR and Tiny-Imagenet, we advance state-of-the-art pruning ratio of structured pruning by integrating simple $ell_1$-norm filters pruning into our pipeline. Specifically, we reduce 75-80% of total parameters and 65-70% MACs of numerous variants of ResNet architectures while having comparable or better performance than that of original networks."
  },
  "bmvc2020_main_centroidbasedconceptlearningforrgb-dindoorsceneclassification": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Centroid Based Concept Learning for RGB-D Indoor Scene Classification",
    "authors": [
      "Ali Ayub",
      "Alan Wagner"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0063.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0063.pdf",
    "published": "2020-09",
    "summary": "This paper contributes a novel cognitively-inspired method for RGB-D indoor scene classification. High intra-class variance and low inter-class variance makes indoor scene classification an extremely challenging task. To cope with this problem, we propose a clustering approach inspired by the concept learning model of the hippocampus and the neocortex, to generate clusters and centroids for different scene categories. Test images depicting different scenes are classified by using their distance to the closest centroids (concepts). Modeling of RGB-D scenes as centroids not only leads to state-of-the-art classification performance on benchmark datasets (SUN RGB-D and NYU Depth V2), but also offers a method for inspecting and interpreting the space of centroids. Inspection of the centroids generated by our approach on RGB-D datasets leads us to propose a method for merging conceptually similar categories, resulting in improved accuracy for all approaches."
  },
  "bmvc2020_main_robustimagematchingbydynamicfeatureselection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Robust Image Matching By Dynamic Feature Selection",
    "authors": [
      "Hao Huang",
      "Jianchun Chen",
      "Xiang Li",
      "Lingjing Wang",
      "Yi Fang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0803.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0803.pdf",
    "published": "2020-09",
    "summary": "Estimating dense correspondences between images is a long-standing image understanding task. Most recent works introduce convolutional neural networks to extract high-level feature maps and find correspondences through feature matching. However, high-level feature maps are in low spatial resolution and therefore insufficient to provide accurate and fine-grained features to distinguish intra-class variations for correspondence matching.To address this problem, we generate robust features by selecting and combining convolutional features at different levels/scales.To resolve two critical issues in feature selection, i.e., how many and which levels of features to be selected, we frame the feature selection process as a sequential Markov decision-making process (MDP)and introduce an optimal selection strategy using reinforcement learning (RL) to select features.Particularly, we define an RL environment for image matching in which individual actions are either requests for new features or terminate the selection episode by referring a matching score.Deep neural networks are incorporated into our method and trained for decision making.Experimental results show that our method achieves com-parable/superior performance with state-of-the-art methods on three public benchmarks, demonstrating the effectiveness of our proposed feature selection strategy."
  },
  "bmvc2020_main_cornernet-liteefficientkeypointbasedobjectdetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "CornerNet-Lite: Efficient Keypoint based Object Detection",
    "authors": [
      "Hei Law",
      "Yun Teng",
      "Olga Russakovsky",
      "Jia Deng"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0016.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0016.pdf",
    "published": "2020-09",
    "summary": "Keypoint-based methods are a relatively new paradigm in object detection, eliminating the need for anchor boxes and offering a simplified detection framework. Keypoint-based CornerNet achieves state of the art accuracy among single-stage detectors. However, this accuracy comes at high processing cost. In this work, we tackle the problem of efficient keypoint-based object detection and introduce CornerNet-Lite. CornerNet-Lite is a combination of two efficient variants of CornerNet: CornerNet-Saccade, which uses an attention mechanism to eliminate the need for exhaustively processing all pixels of the image, and CornerNet-Squeeze, which introduces a new compact backbone architecture. Together these two variants address the two critical use cases in efficient object detection: improving efficiency without sacrificing accuracy, and improving accuracy at real-time efficiency. CornerNet-Saccade is suitable for offline processing, improving the efficiency of CornerNet by 6.0x and the AP by 1.0% on COCO. CornerNet-Squeeze is suitable for real-time detection, improving both the efficiency and accuracy of the popular real-time detector YOLOv3 (34.4% AP at 30ms for CornerNet-Squeeze compared to 33.0% AP at 39ms for YOLOv3 on COCO). Together these contributions for the first time reveal the potential of keypoint-based detection to be useful for applications requiring processing efficiency."
  },
  "bmvc2020_main_unsuperviseddomainadaptationbyuncertainfeaturealignment": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Unsupervised Domain Adaptation by Uncertain Feature Alignment",
    "authors": [
      "Tobias Ringwald",
      "Rainer Stiefelhagen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0221.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0221.pdf",
    "published": "2020-09",
    "summary": "Unsupervised domain adaptation (UDA) deals with the adaptation of models from a given source domain with labeled data to an unlabeled target domain. In this paper, we utilize the inherent prediction uncertainty of a model to accomplish the domain adaptation task. The uncertainty is measured by Monte-Carlo dropout and used for our proposed Uncertainty-based Filtering and Feature Alignment (UFAL) that combines an Uncertain Feature Loss (UFL) function and an Uncertainty-Based Filtering (UBF) approach for alignment of features in Euclidean space. Our method surpasses recently proposed architectures and achieves state-of-the-art results on multiple challenging datasets. Code is available on the project website."
  },
  "bmvc2020_main_learningtopayattentiontomistakes": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning To Pay Attention To Mistakes",
    "authors": [
      "Moucheng Xu",
      "Neil Oxtoby",
      "Daniel Alexander",
      "Joseph Jacob"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0335.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0335.pdf",
    "published": "2020-09",
    "summary": "In Convolutional Neural Network based medical image segmentation, the periphery of foreground regions representing malignant tissues may be disproportionately assigned as belonging to the background class as healthy tissues. As evidenced in visual results in [18][21][24][12][4], misclassification of foreground pixels as the background class can lead to high False Negative detection rates. In this paper, we propose a novel attention mechanism to directly address such high false negative rates, called Paying Attention to False Positives. Our attention mechanism attempts to steer the models towards false positive identification, thereby addressing the bias towards high false negative rates in segmentation outcomes. The proposed mechanism has two complementary implementations: (a) \u201cexplicit\u201d steering of the model to attend to the \u201cenlarged\u201d Effective Receptive Field on the foreground areas; (b) \u201cimplicit\u201d learning towards false positives, by attending to the \u201cshrunken\u201d Effective Receptive Field on the background areas. We first compare our models with state-of-the-art attention baselines in medical imaging, on a binary dense prediction task between vehicles and the background using CityScapes. We then perform a second task which is to segment Enhanced Tumour Core areas in multi-modal MRI scans from the BRATS2018 datast, under 5-fold cross validation. In the second task, we include more baselines including self-attention, spatial attention and spatial-channel mixed attention. Additionally, we conduct comprehensive ablation studies on our models. Lastly, we evaluate our proposed mechanism against another brain lesion segmentation task, using ultrasound images from the ISLES2018 dataset. Across all of the three different tasks, our models consistently outperform the baseline models in terms of Hausdorff Distance (HD) or/and Intersection Over Union (IoU). For instance, in the second task, the \u201cexplicit\u201d implementation of our mechanism reduces the HD of the best baseline by more than 26%, whilst improving the IoU by more than 3%. We believe our proposed attention mechanism can provide safer computer-aided-detection in a wide range of medical applications. The link to our codes on GitHub is hidden to maintain anonymity during the review period."
  },
  "bmvc2020_main_acnnbasedapproachforthenear-fieldphotometricstereoproblem": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "A CNN Based Approach for the Near-Field Photometric Stereo Problem",
    "authors": [
      "Fotios Logothetis",
      "Ignas Budvytis",
      "Roberto Mecca",
      "Roberto Cipolla"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0277.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0277.pdf",
    "published": "2020-09",
    "summary": "Reconstructing the 3D shape of an object using several images under different light sources is a very challenging task, especially when realistic assumptions such as light propagation and attenuation, perspective viewing geometry and specular light reflection are considered. Many of works tackling Photometric Stereo (PS) problems often relax most of the aforementioned assumptions. Especially they ignore specular reflection and global illumination effects. In this work, we propose the first CNN based approach capable of handling these realistic assumptions in Photometric Stereo. We leverage recent improvements of deep neural networks for far-field Photometric Stereo and adapt them to near field setup. We achieve this by employing an iterative procedure for shape estimation which has two main steps. Firstly we train a per-pixel CNN to predict surface normals from reflectance samples. Secondly, we compute the depth by integrating the normal field in order to iteratively estimate light directions and attenuation which is used to compensate the input images to compute reflectance samples for the next iteration. To the best of our knowledge this is the first near-field framework which is able to accurately predict 3D shape from highly specular objects. Our method outperforms competing state-of-the-art near-field Photometric Stereo approaches on both synthetic and real experiments."
  },
  "bmvc2020_main_aoladaptiveonlinelearningforhumantrajectorypredictionindynamicvideoscenes": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "AOL: Adaptive Online Learning for Human Trajectory Prediction in Dynamic Video Scenes",
    "authors": [
      "Manh Huynh",
      "Gita Alaghband"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0493.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0493.pdf",
    "published": "2020-09",
    "summary": "We present a novel adaptive online learning (AOL) framework to predict human movement trajectories in dynamic video scenes. Our framework learns and adapts to changes in the scene environment and generates best network weights for different scenarios. The framework can be applied to prediction models and improve their performance as it dynamically adjusts when it encounters changes in the scene and can apply the best training weights for predicting the next locations. We demonstrate this by integrating our framework with two existing prediction models: LSTM [3] and Future Person Location (FPL) [1]. Furthermore, we analyze the number of network weights for optimal performance and show that we can achieve real-time with a fixed number of networks using the least recently used (LRU) strategy for maintaining the most recently trained network weights. With extensive experiments, we show that our framework increases prediction accuracies of LSTM and FPL by ~17% and 28% on average, and up to ~50% for FPL on the worst case while achieving real-time (20fps)."
  },
  "bmvc2020_main_videoregionannotationwithsparseboundingboxes": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Video Region Annotation with Sparse Bounding Boxes",
    "authors": [
      "Yuzheng Xu",
      "Yang Wu",
      "Nur Sabrina binti Zuraimi",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0638.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0638.pdf",
    "published": "2020-09",
    "summary": "Video analysis has been moving towards more detailed interpretation (e.g. segmentation) with encouraging progresses. These tasks, however, increasingly rely on densely annotated training data both in space and time. Since such annotation is labour-intensive, few densely annotated video data with detailed region boundaries exist. This work aims to resolve this dilemma by learning to automatically generate region boundaries for all frames of a video from sparsely annotated bounding boxes of target regions. We achieve this with a Volumetric Graph Convolutional Network (VGCN), which learns to iteratively find keypoints on the region boundaries using the spatio-temporal volume of surrounding appearance and motion. The global optimization of VGCN makes it significantly stronger and generalize better than existing solutions. Experimental results using two latest datasets (one real and one synthetic), including ablation studies, demonstrate the effectiveness and superiority of our method."
  },
  "bmvc2020_main_real-timesemanticsegmentationviamultiplyspatialfusionnetwork": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Real-Time Semantic Segmentation via Multiply Spatial Fusion Network",
    "authors": [
      "Haiyang Si",
      "Zhiqiang Zhang",
      "Feng Lu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0678.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0678.pdf",
    "published": "2020-09",
    "summary": "Real-time semantic segmentation plays a significant role in industry applications, such as autonomous driving, robotics and so on. It is a challenging task as both efficiency and performance need to be considered simultaneously. To address such a complex task, this paper proposes an efficient CNN called Multiply Spatial Fusion Network (MSFNet) to achieve fast and accurate perception. The proposed MSFNet uses Class Boundary Supervision to process the relevant boundary information based on our proposed Multi-features Fusion Module which can obtain spatial information and enlarge receptive field. Therefore, the final upsampling of the feature maps of 1/8 original image size can achieve impressive results while maintaining a high speed. Experiments on Cityscapes and Camvid datasets show an obvious advantage of the proposed approach compared with the existing approaches. Specifically, it achieves 77.1% Mean IOU on the Cityscapes test dataset with the speed of 41 FPS for a 1024*2048 input, and 75.4% Mean IOU with the speed of 91 FPS on the Camvid test dataset."
  },
  "bmvc2020_main_superpixelmaskingandinpaintingforself-supervisedanomalydetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Superpixel Masking and Inpainting for Self-Supervised Anomaly Detection",
    "authors": [
      "Zhenyu Li",
      "Ning Li",
      "Kaitao Jiang",
      "Zhiheng Ma",
      "Xing Wei",
      "Xiaopeng Hong",
      "Yihong Gong"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0275.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0275.pdf",
    "published": "2020-09",
    "summary": "Anomaly detection aims at identifying abnormal samples from the normal ones. Existing methods are usually supervised or detect anomalies at the instance level without localization. In this work, we propose an unsupervised method called Superpixel Masking And Inpainting (SMAI) to identify and locate anomalies in images. Specifically, superpixel segmentation is first performed on the images. Then an inpainting module is trained to learn the spatial and texture information of the normal samples through random superpixel masking and restoration. Therefore, the model can reconstruct the superpixel mask with normal content. At the inference stage, we mask the image using superpixels and restore them one by one. By comparing the mask areas of the original image and its reconstruction, we can identify and locate the abnormal regions. We conducted a comprehensive evaluation of SMAI on the latest MVTec anomaly detection dataset, and it shows that SMAI plays favorably against state-of-the-art methods."
  },
  "bmvc2020_main_align-and-attendnetworkforgloballyandlocallycoherentvideoinpainting": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Align-and-Attend Network for Globally and Locally Coherent Video Inpainting",
    "authors": [
      "Sanghyun Woo",
      "Dahun Kim",
      "KwanYong Park",
      "Joon-Young Lee",
      "In So Kweon"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0440.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0440.pdf",
    "published": "2020-09",
    "summary": "Video inpainting is more challenging than image inpainting because of the extra temporal dimension. It requires inpainted contents to be globally coherent in both space and time. A natural solution for this problem is aggregating features from other frames, and thus, existing state-of-the-art methods rely heavily on 3D convolution or optical flow. However, these methods emphasize more on the temporally nearby frames, and long-term temporal information is not sufficiently stressed. In this work, we propose a novel two-stage alignment method. The first stage is an alignment module that uses computed homographies between the target frame and the reference frames. The visible patches are then aggregated based on the frame similarity to fill in the target holes roughly. Despite being able to model only global transformations, we empirically verify that homography-based alignment allows larger temporal window size than the flow-based counterpart. The second stage is an attention module that matches the generated patches with known reference patches in a non-local manner to refine the previous global alignment stage. Both stages consist of large spatial-temporal window size for the reference and thus enable modeling long-range correlations between distant information and the hole regions. Finally, even challenging scenes with large or slowly moving holes can be handled, which have been hardly modeled by existing approaches. Experiments on video object removal demonstrate that our method significantly outperforms previous state-of-the-art learning approaches."
  },
  "bmvc2020_main_adversarialtrainingformulti-channelsignlanguageproduction": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Adversarial Training for Multi-Channel Sign Language Production",
    "authors": [
      "Ben Saunders",
      "Richard Bowden",
      "Necati Cihan Camgoz"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0223.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0223.pdf",
    "published": "2020-09",
    "summary": "Sign Languages are rich multi-channel languages, requiring articulation of both manual (hands) and non-manual (face and body) features in a precise, intricate manner. Sign Language Production (SLP), the automatic translation from spoken to sign languages, must embody this full sign morphology to be truly understandable by the Deaf community. Previous work has mainly focused on manual feature production, with an under-articulated output caused by regression to the mean. In this paper, we propose an Adversarial Multi-Channel approach to SLP. We frame sign production as a minimax game between a transformer-based Generator and a conditional Discriminator. Our adversarial discriminator evaluates the realism of sign production conditioned on the source text, pushing the generator towards a realistic and articulate output. Additionally, we fully encapsulate sign articulators with the inclusion of non-manual features, producing facial features and mouthing patterns. We evaluate on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, and report state-of-the art SLP back-translation performance for manual production. We set new benchmarks for the production of multi-channel sign to underpin future research into realistic SLP."
  },
  "bmvc2020_main_mid-levelfusionforend-to-endtemporalactivitydetectioninuntrimmedvideo": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Mid-level Fusion for End-to-End Temporal Activity Detection in Untrimmed Video",
    "authors": [
      "Md Atiqur Rahman",
      "Robert Laganiere"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0234.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0234.pdf",
    "published": "2020-09",
    "summary": "In this paper, we address the problem of human activity detection in temporally untrimmed long video sequences, where the goal is to classify and temporally localize each activity instance in the input video. Inspired by the recent success of the single-stage object detection methods, we propose an end-to-end trainable framework capable of learning task-specific spatio-temporal features of a video sequence for direct classification and localization of the activities. We, further, systematically investigate how and where to fuse multi-stream feature representations of a video and propose a new fusion strategy for temporal activity detection. Together with the proposed fusion strategy, the novel architecture sets new state-of-the-art on the highly challenging THUMOS'14 benchmark -- up from 44.2% to 53.9% mAP (an absolute 9.7% improvement)."
  },
  "bmvc2020_main_sofa-netsecond-orderandfirst-orderattentionnetworkforcrowdcounting": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "SOFA-Net: Second-Order and First-order Attention Network for Crowd Counting",
    "authors": [
      "Haoran Duan",
      "Shidong Wang",
      "Yu Guan"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0222.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0222.pdf",
    "published": "2020-09",
    "summary": "Automated crowd counting from images/videos has attracted more attention in recent years because of its wide application in smart cities. But modelling the dense crowd heads is challenging and most of the existing works become less reliable. To obtain the appropriate crowd representation, in this work we proposed SOFA-Net(Second-Order and First-order Attention Network): second-order statistics were extracted to retain selectivity of the channel-wise spatial information for dense heads while first-order statistics, which can enhance the feature discrimination for the heads' areas, were used as complementary information. Via a multi-stream architecture, the proposed second/first-order statistics were learned and transformed into attention for robust representation refinement. We evaluated our method on four public datasets and the performance reached state-of-the-art. Extensive experiments were also conducted to study the components in the proposed SOFA-Net, and the results suggested the high-capability of second/first-order statistics on modelling crowd in challenging scenarios. To the best of our knowledge, we are the first work to explore the second/first-order statistics for crowd counting. The source code will be available."
  },
  "bmvc2020_main_epi-basedorientedrelationnetworksforlightfielddepthestimation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "EPI-based Oriented Relation Networks for Light Field Depth Estimation",
    "authors": [
      "Kunyuan Li",
      "Jun Zhang",
      "Rui Sun",
      "Xudong Zhang",
      "Jun Gao"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0096.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0096.pdf",
    "published": "2020-09",
    "summary": "Light field cameras record not only the spatial information of observed scenes but also the directions of all incoming light rays. The spatial and angular information implicitly contain geometrical characteristics such as multi-view or epipolar geometry, which can be exploited to improve the performance of depth estimation. An Epipolar Plane Image (EPI), the unique 2D spatial-angular slice of the light field, contains patterns of oriented lines. The slope of these lines is associated with the disparity. Benefiting from this property of EPIs, some representative methods estimate depth maps by analyzing the disparity of each line in EPIs. However, these methods often extract the optimal slope of the lines from EPIs while ignoring the relationship between neighboring pix- els, which leads to inaccurate depth map predictions. Based on the observation that an oriented line and its neighboring pixels in an EPI share a similar linear structure, we propose an end-to-end fully convolutional network (FCN) to estimate the depth value of the intersection point on the horizontal and vertical EPIs. Specifically, we present a new feature-extraction module, called Oriented Relation Module (ORM), that constructs the relationship between the line orientations. To facilitate training, we also propose a refocusing-based data augmentation method to obtain different slopes from EPIs of the same scene point. Extensive experiments verify the efficacy of learning relations and show that our approach is competitive to other state-of-the-art methods. The code and the trained models are available at https://github.com/lkyahpu/EPI_ORM.git."
  },
  "bmvc2020_main_non-probabilisticcosinesimilaritylossforfew-shotimageclassification": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Non-Probabilistic Cosine Similarity Loss for Few-Shot Image Classification",
    "authors": [
      "Joonhyuk Kim",
      "Inug Yoon",
      "Gyeong-Moon Park",
      "Jong-Hwan Kim"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0282.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0282.pdf",
    "published": "2020-09",
    "summary": "A few-shot image classification problem aims to recognize previously unseen objects with a small amount of data. Many works have been offered to solve the problem, while a simple transfer learning method with the cosine similarity based cross-entropy loss is still powerful compared with other methods. To improve the performance, we propose a novel Non-Probabilistic Cosine similarity (NPC) loss for few-shot classification that can replace the cross-entropy loss with the cosine similarity. A key difference of NPC loss is that it uses values of inputs instead of their probabilities. By simply changing the loss function, our model avoids overfitting on a training set and performs well on few-shot tasks. Experimental results show that the model with NPC loss clearly outperforms those with other loss functions and also achieves excellent performance compared with state-of-the-art algorithms on Mini-Imagenet and CUB-200-2011 datasets."
  },
  "bmvc2020_main_towardsfastandlight-weightrestorationofdarkimages": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Towards Fast and Light-Weight Restoration of Dark Images",
    "authors": [
      "Mohit Lamba",
      "Atul Balaji",
      "Kaushik Mitra"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0145.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0145.pdf",
    "published": "2020-09",
    "summary": "The ability to capture good quality images in the dark and textit{near-zero lux} conditions has been a long-standing pursuit of the computer vision community. The seminal work by Chen etal cite{chen2018learning} has especially caused renewed interest in this area, resulting in methods that build on top of their work in a bid to improve the reconstruction. However, for practical utility and deployment of low-light enhancement algorithms on edge devices such as embedded systems, surveillance cameras, autonomous robots and smartphones, the solution must respect additional constraints such as limited GPU memory and processing power. With this in mind, we propose a deep neural network architecture that aims to strike a balance between the network latency, memory utilization, model parameters, and reconstruction quality. The key idea is to forbid any computation in the High-Resolution (HR) space and instead restrict most of the computations to Low-Resolution (LR) space. However, doing the bulk of computations in the LR space causes a lot of artifacts in the restored image. We propose textit{Pack} and textit{UnPack} operations, which allow us to effectively transit between the HR and LR spaces without incurring much artifacts in the restored image. Most of the state-of-the-art algorithms on dark image enhancement need to pre-amplify the image before processing it. However, they generally use ground truth information to find the amplification factor even during inference, which restricts their applicability for unknown scenes. In contrast, we propose a simple yet effective light-weight mechanism for automatically determining the amplification factor from the input image itself. We show that we can enhance a full resolution, $2848 times 4256$, extremely dark single-image in the ballpark of $3$ seconds even on a CPU. We achieve this with $2-7times$ fewer model parameters, $2-3times$ lower memory utilization, $5-20times$ speed up and yet maintain a competitive image reconstruction quality compared to the current state-of-the-art algorithms."
  },
  "bmvc2020_main_fairfaceganfairness-awarefacialimage-to-imagetranslation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "FairFaceGAN: Fairness-aware Facial Image-to-Image Translation",
    "authors": [
      "Sunhee Hwang",
      "Sungho Park",
      "Dohyung Kim",
      "Mirae Do",
      "Hyeran Byun"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0193.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0193.pdf",
    "published": "2020-09",
    "summary": "In this paper, we introduce FairFaceGAN, a fairness-aware facial Image-to-Image translation model, mitigating the problem of unwanted translation in protected attributes (e.g., gender, age, race) during facial attributes editing. Unlike existing models, FairFaceGAN learns fair representations with two separate latents - one related to the target attributes to translate, and the other unrelated to them. This strategy enables FairFaceGAN to separate the information about protected attributes and that of target attributes. It also prevents unwanted translation in protected attributes while target attributes editing. To evaluate the degree of fairness, we perform two types of experiments on CelebA dataset. First, we compare the fairness-aware classification performances when augmenting data by existing image translation methods and FairFaceGAN respectively. Moreover, we propose a new fairness metric, namely Fr\u00e9chet Protected Attribute Distance (FPAD), which measures how well protected attributes are preserved. Experimental results demonstrate that FairFaceGAN shows consistent improvements in terms of fairness over the existing image translation models. Further, we also evaluate image translation performances, where FairFaceGAN shows competitive results, compared to those of existing methods."
  },
  "bmvc2020_main_seeingwakewordsaudio-visualkeywordspotting": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Seeing wake words: Audio-visual Keyword Spotting",
    "authors": [
      "Liliane Momeni",
      "Triantafyllos Afouras",
      "Themos Stafylakis",
      "Samuel Albanie",
      "Andrew Zisserman"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0043.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0043.pdf",
    "published": "2020-09",
    "summary": "The goal of this work is to automatically determine whether and when a word of interest is spoken by a talking face, with or without the audio. We propose a zero-shot method suitable for \"in the wild\" videos. Our key contributions are: (1) a novel convolutional architecture, KWS-Net, that uses a similarity map intermediate representation to separate the task into (i) sequence matching, and (ii) pattern detection, to decide whether the word is there and when; (2) we demonstrate that if audio is available, visual keyword spotting improves the performance both for a clean and noisy audio signal. Finally, (3) we show that our method generalises to other languages, specifically French and German, and achieves a comparable performance to English with less language specific data, by fine-tuning the network pre-trained on English. The method exceeds the performance of the previous state-of-the-art visual keyword spotting architecture when trained and tested on the same benchmark, and also that of a state-of-the-art lip reading method."
  },
  "bmvc2020_main_cross-modalhierarchicalmodellingforfine-grainedsketchbasedimageretrieval": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval",
    "authors": [
      "Aneeshan Sain",
      "Ayan Kumar Bhunia",
      "Yongxin Yang",
      "Tao Xiang",
      "Yi-Zhe Song"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0102.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0102.pdf",
    "published": "2020-09",
    "summary": "Sketch as an image search query is an ideal alternative to text in capturing the fine-grained visual details. Prior successes on fine-grained sketch-based image retrieval (FG-SBIR) have demonstrated the importance of tackling the unique traits of sketches as opposed to photos, e.g., temporal vs. static, strokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a further trait of sketches that has been overlooked to date, that is, they are hierarchical in terms of the levels of detail -- a person typically sketches up to various extents of detail to depict an object. This hierarchical structure is often visually distinct. In this paper,we design a novel network that is capable of cultivating sketch-specific hierarchies and exploiting them to match sketch with photo at corresponding hierarchical levels. In particular, features from a sketch and a photo are enriched using cross-modal co-attention, coupled with hierarchical node fusion at every level to form a better embedding space to conduct retrieval. Experiments on common benchmarks show our method to outperform state-of-the-arts by a significant margin."
  },
  "bmvc2020_main_physics-informeddetectionandsegmentationoftypeiisolarradiobursts": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Physics-informed detection and segmentation of type II solar radio bursts",
    "authors": [
      "Joseph Jenkins",
      "Adeline Paiement",
      "Jean Aboudarham",
      "Xavier Bonnin"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0445.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0445.pdf",
    "published": "2020-09",
    "summary": "Type II solar radio bursts have proven to be a useful tool for gaining insights into the behaviour of complex solar events and for forecasting and mitigating their damages on Earth. In this work, we detect and segment the occurrence of type II bursts in solar radio spectrograms, thereby facilitating the extraction of parameters needed to gain insight into solar events. We utilise prior knowledge of how type II bursts drift through frequencies over time to assist with these tasks of detection and segmentation. A new adaptive Region of Interest (ROI) is proposed, to constrain the search to regions that follow the burst curvature at a given frequency. It comes with an implicit data normalisation that reduces the variance of burst appearance in the data, hence simplifying the learning process from small datasets. We demonstrate the effectiveness of our methodology using a simple and popular HOG and logistic regression detector and basic segmentation based on voting and background subtraction. On a custom dataset representative of different levels of solar activity, at a wavelength range where no other detection algorithm currently operates, our adaptive ROI significantly improves over traditional sliding windows. In future work, it may be applied to other, state-of-the-art, machine learning algorithms."
  },
  "bmvc2020_main_onmodalitybiasinthetvqadataset": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "On Modality Bias in the TVQA Dataset",
    "authors": [
      "Thomas Winterbottom",
      "Sarah Xiao",
      "Alistair McLean",
      "Noura Al Moubayed"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0476.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0476.pdf",
    "published": "2020-09",
    "summary": "TVQA is a large scale video question answering (video-QA) dataset based on popular TV shows. The questions were specifically designed to require \"both vision and language understanding to answer\". In this work, we demonstrate an inherent bias in the dataset towards the textual subtitle modality. We infer said bias both directly and indirectly, notably finding that models trained with subtitles learn, on-average, to suppress video feature contribution. Our results demonstrate that models trained on only the visual information can answer ~45% of the questions, while using only the subtitles achieves ~68%. We find that a bilinear pooling based joint representation of modalities damages model performance by 9% implying a reliance on modality specific information. We also show that TVQA fails to benefit from the RUBi modality bias reduction technique popularised in VQA. By simply improving text processing using BERT embeddings with the simple model first proposed for TVQA, we achieve state-of-the-art results (72.13%) compared to the highly complex STAGE model (70.50%). We recommend a multimodal evaluation framework that can highlight biases in models and isolate visual and textual reliant subsets of data. Using this framework we propose subsets of TVQA that respond exclusively to either or both modalities in order to facilitate multimodal modelling as TVQA originally intended."
  },
  "bmvc2020_main_sentenceguidedtemporalmodulationfordynamicvideothumbnailgeneration": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Sentence Guided Temporal Modulation for Dynamic Video Thumbnail Generation",
    "authors": [
      "Mrigank Rochan",
      "Mahesh Kumar Krishna Reddy",
      "Yang Wang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0410.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0410.pdf",
    "published": "2020-09",
    "summary": "We consider the problem of sentence specified dynamic video thumbnail generation. Given an input video and a user query sentence, the goal is to generate a video thumbnail that not only provides the preview of the video content, but also semantically corresponds to the sentence. In this paper, we propose a sentence guided temporal modulation (SGTM) mechanism that utilizes the sentence embedding to modulate the normalized temporal activations of the video thumbnail generation network. Unlike the existing state-of-the-art method that uses recurrent architectures, we propose a non-recurrent framework that is simple and allows much more parallelization. Extensive experiments and analysis on a large-scale dataset demonstrate the effectiveness of our framework."
  },
  "bmvc2020_main_visibility-awaremulti-viewstereonetwork": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Visibility-aware Multi-view Stereo Network",
    "authors": [
      "Jingyang Zhang",
      "Yao Yao",
      "Shiwei Li",
      "Zixin Luo",
      "Tian Fang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0421.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0421.pdf",
    "published": "2020-09",
    "summary": "Learning-based multi-view stereo (MVS) methods have demonstrated promising results. However, very few existing networks explicitly take the pixel-wise visibility into consideration, resulting in erroneous cost aggregation from occluded pixels. In this paper, we explicitly infer and integrate the pixel-wise occlusion information in the MVS network via the matching uncertainty estimation. The pair-wise uncertainty map is jointly inferred with the pair-wise depth map, which is further used as weighting guidance during the multi-view cost volume fusion. As such, the adverse influence of occluded pixels is suppressed in the cost fusion. The proposed framework Vis-MVSNet significantly improves depth accuracies in the scenes with severe occlusion. Extensive experiments are performed on DTU, BlendedMVS, and Tanks and Temples datasets to justify the effectiveness of the proposed framework."
  },
  "bmvc2020_main_thoracicdiseaseidentificationandlocalizationusingdistancelearningandregionverification": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Thoracic Disease Identification and Localization using Distance Learning and Region Verification",
    "authors": [
      "Cheng Zhang",
      "Francine Chen",
      "Yan-Ying Chen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0962.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0962.pdf",
    "published": "2020-09",
    "summary": "The identification and localization of diseases in medical images using deep learning models have recently attracted significant interest. Existing methods only consider training the networks with each image independently and most leverage an activation map for disease localization. In this paper, we propose an alternative approach that learns discriminative features among triplets of images and cyclically trains on region features to verify whether attentive regions contain information indicative of a disease. Concretely, we adapt a distance learning framework for multi-label disease classification to differentiate subtle disease features. Additionally, we feed back the features of the predicted class-specific regions to a separate classifier during training to better verify the localized diseases. Our model can achieve state-of-the-art classification performance on the challenging Chest-Xray14 dataset, and our ablation studies indicate that both distance learning and region verification contribute to overall classification performance. Moreover, the distance learning and region verification modules can capture essential information for better localization than baseline models without these modules."
  },
  "bmvc2020_main_determinantalpointprocessasanalternativetonms": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Determinantal Point Process as an alternative to NMS",
    "authors": [
      "Samik Some",
      "Mithun Gupta",
      "Vinay Namboodiri"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0436.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0436.pdf",
    "published": "2020-09",
    "summary": "We present a determinantal point process (DPP) inspired alternative to non-maximum suppression (NMS) which has become an integral step in all state-of-the-art object detection frameworks. DPPs have been shown to encourage diversity in subset selection problems. We pose NMS as a subset selection problem and posit that directly incorporating DPP like framework can improve the overall performance of the object detection system. We propose an optimization problem which takes the same inputs as NMS, but introduces a novel sub-modularity based diverse subset selection functional. Our results strongly indicate that the modifications proposed in this paper can provide consistent improvements to state-of-the-art object detection pipelines."
  },
  "bmvc2020_main_sd-mtcnnself-distilledmulti-taskcnn": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "SD-MTCNN: Self-Distilled Multi-Task CNN",
    "authors": [
      "Ankit Jha",
      "Awanish Kumar",
      "Biplab Banerjee",
      "Vinay Namboodiri"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0448.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0448.pdf",
    "published": "2020-09",
    "summary": "Multi-task learning (MTL) using convolutional neural networks (CNN) deals with training the network for multiple correlated tasks in concert. For accuracy-critical applications, there are endeavors to boost the model performance by resorting to a deeper network, which also increases the model complexity. However, such burdensome models are difficult to be deployed on mobile or edge devices. To ensure a trade-off between performance and complexity of CNNs in the context of MTL, we introduce the novel paradigm of self-distillation within the network. Different from traditional knowledge distillation (KD), which trains the Student in accordance with a cumbersome Teacher, our self-distilled multi-task CNN model: SD-MTCNN aims at distilling knowledge from deeper CNN layers into the shallow layers. Precisely, we follow a hard-sharing based MTL setup where all the tasks share a generic feature-encoder on top of which separate task-specific decoders are enacted. Under this premise, SD-MTCNN distills the more abstract features from the decoders to the encoded feature space, which guarantees improved multi-task performance from different parts of the network. We validate SD-MTCNN on three benchmark datasets: CityScapes, NYUv2, and Mini-Taskonomy, and results confirm the improved generalization capability of self-distilled multi-task CNNs in comparison to the literature and baselines."
  },
  "bmvc2020_main_refinementofboundaryregressionusinguncertaintyintemporalactionlocalization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Refinement of Boundary Regression Using Uncertainty in Temporal Action Localization",
    "authors": [
      "Yunze Chen",
      "Mengjuan Chen",
      "Rui Wu",
      "Jiagang Zhu",
      "Zheng Zhu",
      "Qingyi Gu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0391.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0391.pdf",
    "published": "2020-09",
    "summary": "Boundary localization is a key component of most temporal action localization frameworks for untrimmed video. Deep-learning methods have brought remarkable progress in this field due to large-scale annotated datasets (e.g., THUMOS14 and ActivityNet). However, natural ambiguity exists for labeling accurate action boundary with such datasets. In this paper, we propose a method to model this uncertainty. Specifically, we construct a Gaussian model for predicting the uncertainty variance of boundary. The captured variance is further used to select more reliable proposals, and to refine proposal boundary by variance voting during post-processing. For most existing one- and two-stage frameworks, more accurate boundaries and reliable proposals can be obtained without additional computation. For the one-stage decoupled single-shot temporal action detection (Decouple-SSAD) framework, we first apply adaptive pyramid feature fusion method to fuse its features of different scales and optimize its structure. Then, we introduce the uncertainty based method, and improve state-of-the-art mAP@0.5 value from 37.9% to 41.6% on THUMOS14. Moreover, for the two-stage proposal\u2013proposal interaction through a graph convolutional network (P-GCN), with such uncertainty method, we also gain significant improvements on both THUMOS14 and ActivityNet v1.3 datasets."
  },
  "bmvc2020_main_bias-awarenessforzero-shotlearningtheseenandunseen": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Bias-Awareness for Zero-Shot Learning the Seen and Unseen",
    "authors": [
      "William Thong",
      "Cees Snoek"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0261.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0261.pdf",
    "published": "2020-09",
    "summary": "Generalized zero-shot learning recognizes inputs from both seen and unseen classes. Yet, existing methods tend to be biased towards the classes seen during training. In this paper, we strive to mitigate this bias. We propose a bias-aware learner to map inputs to a semantic embedding space for generalized zero-shot learning. During training, the model learns to regress to real-valued class prototypes in the embedding space with temperature scaling, while a margin-based bidirectional entropy term regularizes seen and unseen probabilities. Relying on a real-valued semantic embedding space provides a versatile approach, as the model can operate on different types of semantic information for both seen and unseen classes. Experiments are carried out on four benchmarks for generalized zero-shot learning and demonstrate the benefits of the proposed bias-aware classifier, both as a stand-alone method or in combination with generated features."
  },
  "bmvc2020_main_learninggaussianmapsfordenseobjectdetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning Gaussian Maps for Dense Object Detection",
    "authors": [
      "Sonaal Kant"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0368.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0368.pdf",
    "published": "2020-09",
    "summary": "Object detection is a popular branch of research in computer vision, many state of the art object detection algorithms have been introduced in the recent past, but how good arethose object detectors when it comes to dense object detection? In this paper we review common and highly accurate object detection methods on the scenes where numerous similar looking objects are placed in close proximity with each other.We also show that, multi-task learning of gaussian maps along with classification and bounding box regression gives us a significant boost in accuracy over the baseline. We introduce Gaussian Layer and Gaussian Decoder in the existing RetinaNet network for better accuracy in dense scenes, with the same computational cost as the RetinaNet. We show the gain of 6% and 5% in mAP with respect to baseline RetinaNet. Our method also achieves the state of the art accuracy on the SKU110Kdataset."
  },
  "bmvc2020_main_ontheexplorationofincrementallearningforfine-grainedimageretrieval": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "On the Exploration of Incremental Learning for Fine-grained Image Retrieval",
    "authors": [
      "Wei Chen",
      "Yu Liu",
      "Weiping Wang",
      "Tinne Tuytelaars",
      "Erwin M. Bakker",
      "Michael Lew"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0079.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0079.pdf",
    "published": "2020-09",
    "summary": "In this paper, we consider the problem of fine-grained image retrieval in an incremental setting, when new categories are added over time. On the one hand, repeatedly training the representation on the extended dataset is time-consuming. On the other hand, fine-tuning the learned representation only with the new classes leads to catastrophic forgetting. To this end, we propose an incremental learning method to mitigate retrieval performance degradation caused by the forgetting issue. Without accessing any samples of the original classes, the classifier of the original network provides soft \u201clabels\u201d to transfer knowledge to train the adaptive network, so as to preserve the previous capability for classification. More importantly, a regularization function based on Maximum Mean Discrepancy is devised to minimize the discrepancy of new classes features from the original network and the adaptive network, respectively. Extensive experiments on two datasets show that our method effectively mitigates the catastrophic forgetting on the original classes while achieving high performance on the new classes."
  },
  "bmvc2020_main_classinterferenceregularization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Class Interference Regularization",
    "authors": [
      "Bharti Munjal",
      "Sikandar Amin",
      "Fabio Galasso"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0451.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0451.pdf",
    "published": "2020-09",
    "summary": "Contrastive losses yield state-of-the-art performance for person re-identification, face verification and few shot learning. They have recently outperformed the cross-entropy loss on classification at the ImageNet scale and outperformed all self-supervision prior results by a large margin (SimCLR). Simple and effective regularization techniques such as label smoothing and self-distillation do not apply anymore, because they act on multinomial label distributions, adopted in cross-entropy losses, and not on tuple comparative terms, which characterize the contrastive losses. Here we propose a novel, simple and effective regularization technique, the Class Interference Regularization (CIR), which applies to cross-entropy losses but is especially effective on contrastive losses. CIR perturbs the output features by randomly moving them towards the average embeddings of the negative classes. To the best of our knowledge, CIR is the first regularization technique to act on the output features. In experimental evaluation, the combination of CIR and a plain Siamese-net with triplet loss yields best few-shot learning performance on the challenging tieredImageNet. CIR also improves the state-of-the-art technique in person re-identification on the Market-1501 dataset, based on triplet loss, and the state-of-the-art technique in person search on the CUHK-SYSU dataset, based on a cross-entropy loss. Finally, on the task of classification CIR performs on par with the popular label smoothing, as demonstrated for CIFAR-10 and -100."
  },
  "bmvc2020_main_domainadaptationregularizationforspectralpruning": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Domain Adaptation Regularization for Spectral Pruning",
    "authors": [
      "Laurent Dillard",
      "Yosuke Shinya",
      "Taiji Suzuki"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0453.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0453.pdf",
    "published": "2020-09",
    "summary": "Deep Neural Networks (DNNs) have recently been achieving state-of-the-art performance on a variety of computer vision related tasks. However, their computational cost limits their ability to be implemented in embedded systems with restricted resources or strict latency constraints. Model compression has therefore been an active field of research to overcome this issue. Additionally, DNNs typically require massive amounts of labeled data to be trained. This represents a second limitation to their deployment. Domain Adaptation (DA) addresses this issue by allowing knowledge learned on one labeled source distribution to be transferred to a target distribution, possibly unlabeled. In this paper, we investigate on possible improvements of compression methods in DA setting. We focus on a compression method that was previously developed in the context of a single data distribution and show that, with a careful choice of data to use during compression and additional regularization terms directly related to DA objectives, it is possible to improve compression results. We also show that our method outperforms an existing compression method studied in the DA setting by a large margin for high compression rates. Although our work is based on one specific compression method, we also outline some general guidelines for improving compression in DA setting."
  },
  "bmvc2020_main_wamdaweightedalignmentofsourcesformulti-sourcedomainadaptation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "WAMDA: Weighted Alignment of Sources for Multi-source Domain Adaptation",
    "authors": [
      "Surbhi Aggarwal",
      "Jogendra Nath Kundu",
      "Venkatesh Babu Radhakrishnan",
      "Anirban Chakraborty"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0423.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0423.pdf",
    "published": "2020-09",
    "summary": "Unsupervised Domain Adaptation aims to learn a model for an unlabelled target domain, given access to a single labelled but differently distributed source domain. However, often multiple labelled sources which share complementary information are present, resulting in the more practical problem of multi-source domain adaptation (MSDA). Recent works in MSDA learn a domain-invariant space from the sources and target. However, they treat each source to be equally relevant to the target and are not sensitive towards the intrinsic statistical similarities amongst domains. In this work, we propose a novel method for MSDA, termed WAMDA, which utilizes the multiple sources based on their relative importance to the target. Our aim is to explore the relevance of each source-target alignment and source-source alignment, and then perform weighted alignment of domains by using the relevance scores. We experimentally validate the performance of our proposed method on multiple datasets, and achieve either state-of-the-art results or competitive performances across all these datasets."
  },
  "bmvc2020_main_novelviewsynthesisfromsingleimagesviapointcloudtransformation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Novel View Synthesis from Single Images via Point Cloud Transformation",
    "authors": [
      "Hoang-An Le",
      "Thomas Mensink",
      "Partha Das",
      "Theo Gevers"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0051.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0051.pdf",
    "published": "2020-09",
    "summary": "In this paper the argument is made that for true novel view synthesis of objects, where the object can be synthesized from any viewpoint, an explicit 3D shape representation is desired. Our method estimates point clouds to capture the geometry of the object, which can be freely rotated into the desired view and then projected into a new image. This image, however, is sparse by nature and hence this coarse view is used as the input of an image completion network to obtain the dense target view. The point cloud is obtained using the predicted pixel-wise depth map, estimated from a single RGB input image, combined with the camera intrinsics. By using forward warping and backward warping between the input view and the target view, the network can be trained end-to-end without supervision on depth. The benefit of using point clouds as an explicit 3D shape for novel view synthesis is experimentally validated on the 3D ShapeNet benchmark."
  },
  "bmvc2020_main_spatialfeedbacklearningtoimprovesemanticsegmentationinhotweather": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Spatial Feedback Learning to Improve Semantic Segmentation in Hot Weather",
    "authors": [
      "Shyam Nandan Rai",
      "Vineeth N Balasubramanian",
      "Anbumani Subramanian",
      "C.V. Jawahar"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0742.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0742.pdf",
    "published": "2020-09",
    "summary": "High-temperature weather conditions induce geometrical distortions in images which can adversely affect the performance of a computer vision model performing downstream tasks such as semantic segmentation. The performance of such models has been shown to improve by adding a restoration network before a semantic segmentation network. The restoration network removes the geometrical distortions from the images and shows improved segmentation results. However, this approach suffers from a major architectural drawback that is the restoration network does not learn directly from the errors of the segmentation network. In other words, the restoration network is not task aware. In this work, we propose a semantic feedback learning approach, which improves the task of semantic segmentation giving a feedback response into the restoration network. This response works as an attend and fix mechanism by focusing on those areas of an image where restoration needs improvement. Also, we proposed loss functions: Iterative Focal Loss (iFL) and Class-Balanced Iterative Focal Loss (CB-iFL), which are specifically designed to improve the performance of the feedback network. These losses focus more on those samples that are continuously miss-classified over successive iterations. Our approach gives a gain of 17.41 mIoU over the standard segmentation model, including the additional gain of 1.9 mIoU with CB-iFL on the Cityscapes dataset."
  },
  "bmvc2020_main_viewsynthlearninglocalfeaturesfromdepthusingviewsynthesis": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "ViewSynth: Learning Local Features from Depth using View Synthesis",
    "authors": [
      "Jisan Mahmud",
      "Rajat Vikram Singh",
      "Peri Akiva",
      "Spondon Kundu",
      "Kuan-Chuan Peng",
      "Jan-MichaelFrahm"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0009.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0009.pdf",
    "published": "2020-09",
    "summary": "The rapid development of inexpensive commodity depth sensors has made keypoint detection and matching in the depth image modality an important problem in computer vision. Despite great improvements in recent RGB local feature learning methods, adapting them directly in the depth modality leads to unsatisfactory performance. Most of these methods do not explicitly reason beyond the visible pixels in the images. To address the limitations of these methods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint invariant keypoint-descriptor from depth images using a proposed Contrastive Matching Loss, and (2) view synthesis of depth images from different viewpoints using the proposed View Synthesis Module and View Synthesis Loss. By learning view synthesis, we explicitly encourage the feature extractor to encode information about not only the visible, but also the occluded parts of the scene. We demonstrate that in the depth modality, ViewSynth outperforms the state-of-the-art depth and RGB local feature extraction techniques in the 3D keypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes, TUM RGBD and CoRBS in most scenarios. We also show the generalizability of ViewSynth in 3D keypoint matching across different datasets."
  },
  "bmvc2020_main_cross-datasetcolorconstancyrevisitedusingsensor-to-sensortransfer": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Cross-dataset Color Constancy Revisited Using Sensor-to-Sensor Transfer",
    "authors": [
      "Samu Koskinen",
      "Dan Yang",
      "Joni-Kristian Kamarainen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0082.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0082.pdf",
    "published": "2020-09",
    "summary": "Color constancy is required for camera captured images and therefore all digital camera imaging pipelines include an Auto White Balance (AWB) algorithm. An intrinsic problem of AWB is that it is sensor specific and therefore developers need to repeatedly collect new in-house datasets to adjust their methods for new sensors. In literature, the best learning-based methods achieve state-of-the-art performance with clear margin on all available datasets, but performance significantly degrades in cross-dataset experiments due to the aforementioned reason. In this work, we introduce a sensor-to-sensor transfer model that can be used to map datasets with known cameras to any other known camera. The only requirement is that spectral characterizations of the camera models are available. In our experiments, we demonstrate improvements in cross-dataset settings using the proposed sensor-to-sensor transfer model. In addition, for the first time we are able to analyze the characteristics of existing datasets in the common standard observer space and our analysis reveals that certain datasets contain images which are not suitable for color constancy. We introduce a unified cross-dataset color constancy benchmark dataset, compare two state-of-the-art learning-based AWB methods and show superior performance of the proposed sensor-to-sensor model."
  },
  "bmvc2020_main_inpaintingnetworkslearntoseparatecellsinmicroscopyimages": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Inpainting Networks Learn to Separate Cells in Microscopy Images",
    "authors": [
      "Steffen Wolf",
      "Fred Hamprecht",
      "Jan Funke"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0528.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0528.pdf",
    "published": "2020-09",
    "summary": "Deep neural networks trained to inpaint partially occluded images show a deep understanding of image composition and have even been shown to remove objects from images convincingly. In this work, we investigate how this implicit knowledge of image composition can be be used to separate cells in densely populated microscopy images. We propose a measure for the independence of two image regions given a fully self-supervised inpainting network and separate objects by maximizing this independence. We evaluate our method on two cell segmentation datasets and show that cells can be separated completely unsupervised. Furthermore, combined with simple foreground detection, our method yields instance segmentation of similar quality to fully supervised methods."
  },
  "bmvc2020_main_marginalizedgraphattentionhashingforzero-shotimageretrieval": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Marginalized Graph Attention Hashing for Zero-Shot Image Retrieval",
    "authors": [
      "Meixue Huang",
      "Dayan Wu",
      "Wanqian Zhang",
      "Zhi Xiong",
      "Bo Li",
      "Weiping Wang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0688.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0688.pdf",
    "published": "2020-09",
    "summary": "Zero-shot image retrieval allows to precisely retrieve candidates relevant to unobserved queries, of which categories have never been seen during training. Recently, research interests arise in exploring hashing methods to solve this problem due to its storage and computational efficiency. However, existing methods only focus on leveraging semantic information, but omit to exploit the similarity structure of visual feature space for knowledge transfer. Besides, the domain shift problem across seen and unseen classes further degrades the performance. To tackle these issues, in this paper, we propose a novel deep zero-shot hashing method, named Marginalized Graph Attention Hashing (MGAH). MGAH introduces the masked attention mechanism to construct a joint-semantics similarity graph, which captures the intrinsic relationship from different metric spaces, making it competent to transfer knowledge from seen classes into unseen classes. Furthermore, we elaborately design an Energy Magnified Softmax (EM-Softmax) loss, which is capable to alleviate the domain shift problem and encourage the generalization ability of hash codes. By using marginalized strategy, EM-Softmax produces the shared decision margin for hard samples, thus can avoid overfitting on seen classes and meanwhile cover more knowledge for the unseen ones. Extensive experiments demonstrate that MGAH delivers superior performance over the state-of-the-art zero-shot hashing methods."
  },
  "bmvc2020_main_learningnon-parametricinvariancesfromdatawithpermanentrandomconnectomes": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning Non-Parametric Invariances from Data with Permanent Random Connectomes",
    "authors": [
      "Dipan Pal",
      "Akshay Chawla",
      "Marios Savvides"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0863.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0863.pdf",
    "published": "2020-09",
    "summary": "Learning non-parametric invariances directly from data remains an important open problem. In this paper, we introduce a new architectural layer for convolutional networks which is capable of learning general invariances from data itself. This layer can learn invariance to non-parametric transformations and interestingly, motivates and incorporates permanent random connectomes, thereby being called Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with random connections (not just weights) which are a small subset of the connections in a fully connected convolution layer. Importantly, these connections in PRC-NPTNs once initialized remain permanent throughout training and testing.Permanent random connectomes make these architectures loosely more biologically plausible than many other mainstream network architectures which require highly ordered structures. We motivate randomly initialized connections as a simple method to learn invariance from data itself while invoking invariance towards multiple nuisance transformations simultaneously. We find that these randomly initialized permanent connections have positive effects on generalization, outperform much larger ConvNet baselines and the recently proposed Non-Parametric Transformation Network (NPTN) on benchmarks such as augmented MNIST, ETH-80 and CIFAR10, that enforce learning invariances from the data itself."
  },
  "bmvc2020_main_isfacerecognitionsexist?no,genderedhairstylesandbiologyare": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Is Face Recognition Sexist? No, Gendered Hairstyles and Biology Are",
    "authors": [
      "V\u00edtor Albiero",
      "Kevin Bowyer"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0905.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0905.pdf",
    "published": "2020-09",
    "summary": "Recent news articles have accused face recognition of being \u201cbiased\u201d, \u201csexist\u201d or \u201cracist\u201d. There is consensus in the research literature that face recognition accuracy is lower for females, who often have both a higher false match rate and a higher false non- match rate. However, there is little published research aimed at identifying the cause of lower accuracy for females. For instance, the 2019 Face Recognition Vendor Test that documents lower female accuracy across a broad range of algorithms and datasets also lists \u201cAnalyze cause and effect\u201d under the heading \u201cWhat we did not do\u201d. We present the first experimental analysis to identify major causes of lower face recognition accuracy for females on datasets where previous research has observed this result. Controlling for equal amount of visible face in the test images reverses the apparent higher false non-match rate for females. Also, principal component analysis indicates that images of two different females are inherently more similar than of two different males, potentially accounting for a difference in false match rates."
  },
  "bmvc2020_main_learning3dglobalhumanmotionestimationfromunpaired,disjointdatasets": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning 3D Global Human Motion Estimation from Unpaired, Disjoint Datasets",
    "authors": [
      "Julian Habekost",
      "Takaaki Shiratori",
      "Yuting Ye",
      "Taku Komura"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0481.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0481.pdf",
    "published": "2020-09",
    "summary": "We propose a novel method to compute both the local and global 3D motion of the human body from a 2D monocular video. Our approach only uses unpaired sets of 2D keypoints from target videos and 3D motion capture data for training. The estimation target video dataset is assumed to lack any ground truth and thus our supervision signal comes from motion datasets that are fully disjoint from the target datasets. For each time step, a temporal convolutional generator configures the human pose in the global space to satisfy both a reprojection loss and an adversarial loss. The translational and rotational global motion is then derived and converted into the egocentric representation in a differentiable manner for adversarial learning. We compare our system to state-of-the-art architectures that use the Human3.6M dataset for paired training, and demonstrate comparable precision even though our system is never trained on the ground truth Human3.6M 3D motion capture data. Due to its unpaired and disjoint nature in the training data, our system can be trained on a large set of videos and 3D motion capture data, which can considerably expand the domain of the applicable motion data types."
  },
  "bmvc2020_main_magnifiernettowardssemanticadversaryandfusionforpersonre-identification": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "MagnifierNet: Towards Semantic Adversary and Fusion for Person Re-identification",
    "authors": [
      "Yushi Lan",
      "Yuan Liu",
      "Xinchi Zhou",
      "Tian Maoqing",
      "Xuesen Zhang",
      "Shuai Yi",
      "Hongsheng Li"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0781.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0781.pdf",
    "published": "2020-09",
    "summary": "Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying the occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this work, we propose MagnifierNet, a triple-branch network which accurately mines details from whole to parts. Firstly, the holistic salient features are encoded by a global branch. Secondly, to enhance detailed representation for each semantic region, the \"Semantic Adversarial Branch\" is designed to learn from dynamically generated semantic-occluded samples during training. Meanwhile, we introduce \"Semantic Fusion Branch\" to filter out irrelevant noises by selectively fusing semantic region information sequentially. To further improve feature diversity, we introduce a novel loss function \"Semantic Diversity Loss\" to remove redundant overlaps across learned semantic representations. State-of-the-art performance has been achieved on three benchmarks by large margins. Specifically, the mAP score is improved by 6% and 5% on the most challenging CUHK03-L and CUHK03-D benchmarks."
  },
  "bmvc2020_main_adversarialcolorenhancementgeneratingunrestrictedadversarialimagesbyoptimizingacolorfilter": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Adversarial Color Enhancement: Generating Unrestricted Adversarial Images by Optimizing a Color Filter",
    "authors": [
      "Zhengyu Zhao",
      "Zhuoran Liu",
      "Martha Larson"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0099.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0099.pdf",
    "published": "2020-09",
    "summary": "We introduce an approach that enhances images using a color filter in order to create adversarial effects, which fool neural networks into misclassification. Our approach, Adversarial Color Enhancement (ACE), generates unrestricted adversarial images by optimizing the color filter via gradient descent. The novelty of ACE is its incorporation of established practice for image enhancement in a transparent manner. Experimental results validate the white-box adversarial strength and black-box transferability of ACE. A range of examples demonstrates the perceptual quality of images that ACE produces. ACE makes an important contribution to recent work that moves beyond $L_p$ imperceptibility and focuses on unrestricted adversarial modifications that yield large perceptible perturbations, but remain non-suspicious, to the human eye. The future potential of filter-based adversaries is also explored in two directions: guiding ACE with common enhancement practices (e.g., Instagram filters) towards specific attractive image styles and adapting ACE to image semantics. Code is available at https://github.com/ZhengyuZhao/ACE."
  },
  "bmvc2020_main_lipo-lcdcombininglinesandpointsforappearance-basedloopclosuredetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "LiPo-LCD: Combining Lines and Points for Appearance-based Loop Closure Detection",
    "authors": [
      "Joan Pep Company-Corcoles",
      "Emilio Garcia-Fidalgo",
      "Alberto Ortiz"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0789.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0789.pdf",
    "published": "2020-09",
    "summary": "Visual SLAM approaches typically depend on loop closure detection to correct the inconsistencies that may arise during the map and camera trajectory calculations, typically making use of point features for detecting and closing the existing loops. In low-textured scenarios, however, it is difficult to find enough point features and, hence, the performance of these solutions drops drastically. An alternative for human-made scenarios, due to their structural regularity, is the use of geometrical cues such as straight segments, frequently present within these environments. Under this context, in this paper we introduce LiPo-LCD, a novel appearance-based loop closure detection method that integrates lines and points. Adopting the idea of incremental Bag-of-Binary-Words schemes, we build separate BoW models for each feature, and use them to retrieve previously seen images using a late fusion strategy. Additionally, a simple but effective mechanism, based on the concept of island, groups similar images close in time to reduce the image candidate search effort. A final step validates geometrically the loop candidates by incorporating the detected lines by means of a process comprising a line feature matching stage, followed by a robust spatial verification stage, now combining both lines and points. As it is reported in the paper, LiPo-LCD compares well with several state-of-the-art solutions for a number of datasets involving different environmental conditions."
  },
  "bmvc2020_main_bipartitegraphreasoninggansforpersonimagegeneration": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Bipartite Graph Reasoning GANs for Person Image Generation",
    "authors": [
      "Hao Tang",
      "Song Bai",
      "Philip Torr",
      "Nicu Sebe"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0689.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0689.pdf",
    "published": "2020-09",
    "summary": "We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the challenging person image generation task. The proposed graph generator mainly consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed Bipartite Graph Reasoning (BGR) block aims to reason the crossing long-range relations between the source pose and the target pose in a bipartite graph, which mitigates some challenges caused by pose deformation. Moreover, we propose a new Interaction-and-Aggregation (IA) block to effectively update and enhance the feature representation capability of both person's shape and appearance in an interactive way. Experiments on two challenging and public datasets,~emph{i.e.},~Market-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN."
  },
  "bmvc2020_main_objectdetectionasapositive-unlabeledproblem": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Object Detection as a Positive-Unlabeled Problem",
    "authors": [
      "Yuewei Yang",
      "Kevin Liang",
      "Lawrence Carin Duke"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0264.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0264.pdf",
    "published": "2020-09",
    "summary": "As with other deep learning methods, label quality is important for learning modern convolutional object detectors. However, the potentially large number and wide diversity of object instances that can be found in complex image scenes makes constituting complete annotations a challenging task. Indeed, objects missing annotations can be observed in a variety of popular object detection datasets. These missing annotations can be problematic, as the standard cross-entropy loss employed to train object detection models treats classification as a positive-negative (PN) problem: unlabeled regions are implicitly assumed to be background. As such, any object missing a bounding box results in a confusing learning signal, the effects of which we observe empirically. To remedy this, we propose treating object detection as a positive-unlabeled (PU) problem, which removes the assumption that unlabeled regions must be negative. We demonstrate that our proposed PU classification loss outperforms the standard PN loss on PASCAL VOC and MS COCO across a range of label missingness, as well as on Visual Genome and DeepLesion with full labels."
  },
  "bmvc2020_main_e2etaganend-to-endtrainablemethodforgeneratinganddetectingfiducialmarkers": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "E2ETag: An End-to-End Trainable Method for Generating and Detecting Fiducial Markers",
    "authors": [
      "John Peace",
      "Eric Psota",
      "Yanfeng Liu",
      "Lance P\u00e9rez"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0890.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0890.pdf",
    "published": "2020-09",
    "summary": "Existing fiducial markers solutions are designed for efficient detection and decoding, however, their ability to stand out in natural environments is difficult to infer from relatively limited analysis.Furthermore, worsening performance in challenging image capturescenarios-suchaspoorexposure,motionblur,andoff-axisviewing-sheds light on their limitations. E2ETag introduces an end-to-end trainable method for designing fiducial markers and a complimentary detector.By introducing back-propagatable marker augmentation and superimposition into training, the method learns to generate markers that can be detected and classified in challenging real-world environments using a fully convolutional detector network.Results demonstrate that E2ETag outperforms existing methods in ideal conditions and performs much better in the presence of motion blur, contrast fluctuations, noise, and off-axis viewing angles."
  },
  "bmvc2020_main_two-in-onerefinementforinteractivesegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Two-in-One Refinement for Interactive Segmentation",
    "authors": [
      "Soumajit Majumder",
      "Abhinav Rai",
      "Ansh Khurana",
      "Angela Yao"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0702.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0702.pdf",
    "published": "2020-09",
    "summary": "Deep convolutional neural networks are now mainstream for click-based interactive image segmentation. In majority of the frameworks, false negatives and false positive regions are refined via a succession of positive and negative clicks placed centrally in these regions.We propose a simple yet intuitive two-in-one refinement strategy by using clicks placed on the boundary of the object of interest.As boundary clicks are a very strong cue for extracting the object of interest and we find that they are much more effective in correcting wrong segmentation masks.In addition, we propose a boundary-aware loss which encourages segmentation masks to respect instance boundaries.We place our new refinement scheme and loss formulation within a task-specialized segmentation framework and achieve state-of-the-art performance on the standard datasets - Berkeley, Pascal VOC 2012, DAVIS and MS COCO. We exceed competing methods by 6.5 %, 9.4 %, 10.5 % and 2.5 % respectively."
  },
  "bmvc2020_main_deepsparselightfieldrefocusing": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Deep Sparse Light Field Refocusing",
    "authors": [
      "Shachar Ben Dayan",
      "David Mendlovic",
      "Raja Giryes"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0160.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0160.pdf",
    "published": "2020-09",
    "summary": "Light field photography enables to record 4D images, containing angular information alongside spatial information of the scene. One of the important applications of light field imaging is post-capture refocusing. Current methods require for this purpose a dense field of angle views; those can be acquired with a micro-lens system or with a compressive system. Both techniques have major drawbacks to consider, including bulky structures and angular-spatial resolution trade-off. We present a novel implementation of digital refocusing based on sparse angular information using neural networks. This allows recording high spatial resolution in favor of the angular resolution, thus, enabling to design compact and simple devices with improved hardware as well as better performance of compressive systems.We use a novel convolutional neural network whose relatively small structure enables fast reconstruction with low memory consumption. Moreover, it allows handling without re-training various refocusing ranges and noise levels. Results show major improvement compared to existing methods."
  },
  "bmvc2020_main_featurebindingwithcategory-dependantmixupforsemanticsegmentationandadversarialrobustness": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness",
    "authors": [
      "Md Amirul Islam",
      "Matthew Kowal",
      "Konstantinos Derpanis",
      "Neil D. B. Bruce"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0614.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0614.pdf",
    "published": "2020-09",
    "summary": "In this paper, we present a strategy for training convolutional neural networks to effectively resolve interference arising from competing hypotheses relating to inter-categorical information throughout the network. The premise is based on the notion of feature binding, which is defined as the process by which activation's spread across space and layers in the network are successfully integrated to arrive at a correct inference decision. In our work, this is accomplished for the task of dense image labelling by blending images based on their class labels, and then training a feature binding network, which simultaneously segments and separates the blended images. Subsequent feature denoising to suppress noisy activations reveals additional desirable properties and high degrees of successful predictions. Through this process, we reveal a general mechanism, distinct from any prior methods, for boosting the performance of the base segmentation network while simultaneously increasing robustness to adversarial attacks."
  },
  "bmvc2020_main_imageharmonizationwithattention-baseddeepfeaturemodulation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Image Harmonization with Attention-based Deep Feature Modulation",
    "authors": [
      "Guoqing Hao",
      "Satoshi Iizuka",
      "Kazuhiro Fukui"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0121.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0121.pdf",
    "published": "2020-09",
    "summary": "We present a learning-based approach for image harmonization, which allows for adjusting the appearance of the foreground to make it compatible with background. We consider improving the realism by adjusting the high-level feature statistics of the foreground according to those of the background, which is motivated by the fact that specific image statistics between the foreground and background typically match in realistic composite images. Based on a fully convolutional network, we propose a novel attention-based module that aligns the standard deviation of the foreground features with that of the background features, capturing global dependencies in the entire image. This module is easily inserted into any convolutional neural networks, and allows improving the harmony of the composites with only a small additional computational cost. Experimental results on the image harmonization dataset and real composite images show that our method outperforms existing methods both quantitatively and qualitatively. Furthermore, in our experiment, our module is able to boost existing harmonization networks by simply inserting it into intermediate layers of those networks."
  },
  "bmvc2020_main_rankposelearninggeneralisedfeaturewithranksupervisionforheadposeestimation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "RankPose: Learning Generalised Feature with Rank Supervision for Head Pose Estimation",
    "authors": [
      "Donggen Dai",
      "Wangkit Wong",
      "Zhuojun Chen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0401.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0401.pdf",
    "published": "2020-09",
    "summary": "We address the challenging problem of RGB image-based head pose estimation. We first reformulate head pose representation learning to constrain it to a bounded space. Head pose represented as vector projection or vector angles shows helpful to improving performance. Further, a ranking loss combined with MSE regression loss is proposed. The ranking loss supervises a neural network with paired samples of the same person and penalises incorrect ordering of pose prediction. Analysis on this new loss function suggests it contributes to a better local feature extractor, where features are generalised to Abstract Landmarks which are pose-related features instead of pose-irrelevant information such as identity, age, and lighting. Extensive experiments show that our method significantly outperforms the current state-of-the-art schemes on public datasets: AFLW2000 and BIWI. Our model achieves significant improvements over previous SOTA MAE on AFLW2000 and BIWI from 4.50 [11] to 3.66 and from 4.0 [24] to 3.71 respectively. Source code is available at: https://github.com/seathiefwang/RankHeadPose."
  },
  "bmvc2020_main_adversarialconcurrenttrainingoptimizingrobustnessandaccuracytrade-offofdeepneuralnetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks",
    "authors": [
      "Elahe Arani",
      "Fahad Sarfraz",
      "Bahram Zonooz"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0859.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0859.pdf",
    "published": "2020-09",
    "summary": "Adversarial training has been proven to be an effective technique for improving the adversarial robustness of models.However, there seems to be an inherent trade-off be-tween optimizing the model for accuracy and robustness. To this end, we propose Adversarial Concurrent Training (ACT), which employs adversarial training in a collaborative learning framework whereby we train a robust model in conjunction with a natural model in a minimax game. ACT encourages the two models to align their feature space by using the task-specific decision boundaries and explore the input space more broadly. Furthermore, the natural model acts as a regularizer, enforcing priors on features that the robust model should learn.Our analyses on the behavior of the models show that ACT leads to a robust model with lower model complexity, higher information compression in the learned representations, and high posterior entropy solutions indicative of convergence to a flatter minima.We demonstrate the effectiveness of the proposed approach across different datasets and network architectures. On ImageNet, ACT achieves 68.20% standard accuracy and 44.29% robustness accuracy under a 100-iteration untargeted attack, improving upon the standard adversarial training method\u2019s 65.70% standard accuracy and 42.36% robustness."
  },
  "bmvc2020_main_attentiondistillationforlearningvideorepresentations": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Attention Distillation for Learning Video Representations",
    "authors": [
      "Miao Liu",
      "Xin Chen",
      "Yun Zhang",
      "Yin Li",
      "James Rehg"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0006.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0006.pdf",
    "published": "2020-09",
    "summary": "We address the challenging problem of learning motion representations using deep models for video recognition. To this end, we make use of attention modules that learn to highlight regions in the video and aggregate features for recognition. Specifically, we propose to leverage output attention maps as a vehicle to transfer the learned representation from a flow network to an RGB network. We systematically study the design of attention modules, and develop a novel method for attention distillation. Our method is evaluated on major action benchmarks. We show that our method not only improves the performance of the baseline RGB network by a significant margin. Moreover, we demonstrate that attention serves a more robust tool for knowledge distillation in video domain. We believe our method provides a step towards learning motion-aware representations in deep models and valuable insights for knowledge distillation. Our project page is available at https://aptx4869lm.github.io/AttentionDistillation/"
  },
  "bmvc2020_main_mixup-camweakly-supervisedsemanticsegmentationviauncertaintyregularization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization",
    "authors": [
      "Yu-Ting Chang",
      "Qiaosong Wang",
      "Wei-Chih Hung",
      "Robinson Piramuthu",
      "Yi-Hsuan Tsai",
      "Ming-Hsuan Yang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0367.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0367.pdf",
    "published": "2020-09",
    "summary": "Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels.However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss.To tackle this issue, we propose a principled and end-to-end train-able framework to allow the network to pay attention to other parts of the object, while producing a more complete and uniform response map.Specifically, we introduce the mixup data augmentation scheme into the classification network and design two uncertainty regularization terms to better interact with the mixup strategy. In experiments, we conduct extensive analysis to demonstrate the proposed method and show favorable performance against state-of-the-art approaches."
  },
  "bmvc2020_main_meta-retinanetforfew-shotobjectdetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Meta-RetinaNet for Few-shot Object Detection",
    "authors": [
      "Shaoqi Li",
      "Wenfeng Song",
      "Shuai Li",
      "Aimin Hao",
      "Hong Qin"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0042.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0042.pdf",
    "published": "2020-09",
    "summary": "Few shot object detection (FSD) is gaining popularity, enhanced by the deep learning methods in recent years. Meanwhile, meta-learning has achieved great success in few-shot image classification benefitting from its adaptive capability corresponding to a suite of tasks. Yet, most object detection models are based on deep neural networks (DNNs), and they are prone to the overfitting problem due to limited samples available during training. To adapt the learned prior knowledge more effectively to new tasks, this paper proposes a novel Meta-RetinaNet for FSD, which avoids a biased meta-learner and improves its generalization ability. It employs a Meta Coefficient Learner (MCL) trained by the Balanced Loss (BL) to augment the DNNs. Specifically, the MCL adapts to tasks by the product of pre-trained convolution weights and coefficient vectors densely for all the convolutional layers, such that it could adequately transfer the learned knowledge to new tasks (while overcoming the overfitting problem) by training fewer parameters. In addition, the BL expedites the training of a Meta-RetinaNet by balancing the performance of a host of tasks, and it also retains stable performance for new tasks. Our experiments showcase the effectiveness of our method, which achieves the state-of-the-art performance on the multiple settings of Pascal VOC and COCO datasets."
  },
  "bmvc2020_main_tacklingtheunannotatedscenegraphgenerationwithbias-reducedmodels": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models",
    "authors": [
      "Tzujui Wang",
      "Selen Pehlivan",
      "Jorma Laaksonen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0541.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0541.pdf",
    "published": "2020-09",
    "summary": "Predicting a scene graph that captures visual entities and their interactions in an image has been considered a crucial step towards full scene comprehension. Recent scene graph generation (SGG) models have shown their capability of capturing the most frequent relations among visual entities. However, the state-of-the-art results are still far from satisfactory, e.g. models can obtain 31% in overall recall R@100, whereas the likewise important mean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The discrepancy between R and mR results urges to shift the focus from pursuing a high R to a high mR with a still competitive R. We suspect that the observed discrepancy stems from both the annotation bias and sparse annotations in VG, in which many visual entity pairs are either not annotated at all or only with a single relation when multiple ones could be valid. To address this particular issue, we propose a novel SGG training scheme that capitalizes on self-learned knowledge. It involves two relation classifiers, one offering a less biased setting for the other to base on. The proposed scheme can be applied to most of the existing SGG models and is straightforward to implement. We observe significant relative improvements in mR (between +6.6% and +20.4%) and competitive or better R (between -2.4% and 0.3%) across all standard SGG tasks."
  },
  "bmvc2020_main_theresistancetolabelnoiseink-nnanddnndependsonitsconcentration": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "The Resistance to Label Noise in K-NN and DNN Depends on its Concentration",
    "authors": [
      "Amnon Drory",
      "Oria Ratzon",
      "Shai Avidan",
      "Raja Giryes"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0176.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0176.pdf",
    "published": "2020-09",
    "summary": "We investigate the classification performance of K-nearest neighbors (K-NN) and deep neural networks (DNNs) in the presence of label noise. We first show empirically that a DNN\u2019s prediction for a given test example depends on the labels of the training examples in its local neighborhood. This motivates us to derive a realizable analytic expression that approximates the multi-class K-NN classification error in the presence of label noise, which is of independent importance. We then suggest that the expression for K-NN may serve as a first-order approximation for the DNN error. Finally, we demonstrate empirically the proximity of the developed expression to the observed performance of K-NN and DNN classifiers. Our result may explain the already observed surprising resistance of DNN to some types of label noise. It also characterizes an important factor of it, showing that the more concentrated the noise the greater is the degradation in performance."
  },
  "bmvc2020_main_abetteruseofaudio-visualcuesdensevideocaptioningwithbi-modaltransformer": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer",
    "authors": [
      "Vladimir Iashin",
      "Esa Rahtu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0111.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0111.pdf",
    "published": "2020-09",
    "summary": "Dense video captioning aims to localize and describe important events in untrimmed videos. Existing methods mainly tackle this task by exploiting only visual features, while completely neglecting the audio track. Only a few prior works have utilized both modalities, yet they show poor results or demonstrate the importance on a dataset with a specific domain. In this paper, we introduce Bi-modal Transformer which generalizes the Transformer architecture for a bi-modal input. We show the effectiveness of the proposed model with audio and visual modalities on the dense video captioning task, yet the module is capable of digesting any two modalities in a sequence-to-sequence task. We also show that the pre-trained bi-modal encoder as a part of the bi-modal transformer can be used as a feature extractor for a simple proposal generation module. The performance is demonstrated on a challenging ActivityNet Captions dataset where our model achieves outstanding performance. The code is available: v-iashin.github.io/bmt"
  },
  "bmvc2020_main_ntganlearningblindimagedenoisingwithoutcleanreference": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "NTGAN: Learning Blind Image Denoising without Clean Reference",
    "authors": [
      "Rui Zhao",
      "Daniel P.K. Lun",
      "Kin-Man Lam"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0046.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0046.pdf",
    "published": "2020-09",
    "summary": "Recent studies on learning-based image denoising have achieved promising performance on various noise reduction tasks. Most of these deep denoisers are trained either under the supervision of clean references, or unsupervised on synthetic noise. The assumption with the synthetic noise leads to poor generalization when facing real photographs. To address this issue, we propose a novel deep unsupervised image-denoising method by regarding the noise reduction task as a special case of the noise transference task. Learning noise transference enables the network to acquire the denoising ability by only observing the corrupted samples. The results on real-world denoising benchmarks demonstrate that our proposed method achieves state-of-the-art performance on removing realistic noises, making it a potential solution to practical noise reduction problems."
  },
  "bmvc2020_main_makingl-bfgsworkwithindustrial-strengthnets": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Making L-BFGS Work with Industrial-Strength Nets",
    "authors": [
      "Abhay Yadav",
      "Tom Goldstein",
      "David Jacobs"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0479.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0479.pdf",
    "published": "2020-09",
    "summary": "L-BFGS has been one of the most popular methods for convex optimization, but good performance by L-BFGS in deep learning has been elusive. Recent work has modified L-BFGS for deep networks for classification tasks and been able to show performance competitive with SGD and Adam (the most popular current algorithms) when batch normalization is not used. However, this work cannot be applied with batch normalization. Since batch normalization is a defacto standard and important to good performance in deep networks, this still limits the use of L-BFGS. In this paper, we address this issue. Our proposed method can be used as a drop-in replacement without changing existing code. The proposed method performs consistently better than Adam and existing L-BFGS approaches, and comparable to carefully tuned SGD. We show results on three datasets, CIFAR-10, CIFAR-100, and STL-10 using three different popular deep networks ResNet, DenseNet and Wide ResNet. This work marks another significant step towards making L-BFGS competitive in the deep learning community."
  },
  "bmvc2020_main_whenhumansmeetmachinestowardsefficientsegmentationnetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "When Humans Meet Machines: Towards Efficient Segmentation Networks",
    "authors": [
      "Peike Li",
      "Xuanyi Dong",
      "Xin Yu",
      "Yi Yang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0209.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0209.pdf",
    "published": "2020-09",
    "summary": "In this paper, we investigate how to achieve a high-performance yet lightweight segmentation network for real-time applications. By analyzing three typical segmentation networks, we observe that the segmentation backbones and heads are often imbalanced which restricts network efficiency. Thus, we develop a lightweight context fusion (LCF) module and a lightweight global enhancement (LGE) module to construct our lightweight segmentation head. Specifically, LCF fuses multi-resolution features to capture image details and LGE is designed to enhance feature representations. In this manner, our lightweight head facilities network efficiency and significantly reduces network parameters. Furthermore, we design a Multi-Resolution Macro Segmentation structure (MRMS) to incorporate human knowledge into our network architecture composition. Given the resource-aware constraint (e.g., latency time), we optimize our network with network architecture search while considering the relationships among atomic operators, network depth and feature resolution in segmentation tasks. Since MRMS embeds the segmentation-specific knowledge, it also provides a better architecture search space. Our Human-Machine collaboratively designed Segmentation network (HMSeg) achieves better performance and faster inference speed. Experiments demonstrate that our network achieves 71.4% mean intersection over union (mIOU) on Cityscapes with only 0.7M parameters at 172.4 FPS on NVIDIA GTX1080Ti."
  },
  "bmvc2020_main_largescalephotometricbundleadjustment": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Large Scale Photometric Bundle Adjustment",
    "authors": [
      "Oliver J. Woodford",
      "Edward Rosten"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0822.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0822.pdf",
    "published": "2020-09",
    "summary": "Direct methods have shown promise on visual odometry and SLAM, leading to greater accuracy and robustness over feature-based methods. However, offline 3-d reconstruction from internet images has not yet benefited from a joint, photometric optimization over dense geometry and camera parameters. Issues such as the lack of brightness constancy, and the sheer volume of data, make this a more challenging task. This work presents a framework for jointly optimizing millions of scene points and hundreds of camera poses and intrinsics, using a photometric cost that is invariant to local lighting changes. The improvement in metric reconstruction accuracy that it confers over feature-based bundle adjustment is demonstrated on the large-scale Tanks & Temples benchmark. We further demonstrate qualitative reconstruction improvements on an internet photo collection, with challenging diversity in lighting and camera intrinsics."
  },
  "bmvc2020_main_graphdensity-awarelossesfornovelcompositionsinscenegraphgeneration": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation",
    "authors": [
      "Boris Knyazev",
      "Harm De Vries",
      "C\u0103t\u0103lina Cangea",
      "Graham Taylor",
      "Aaron Courville",
      "Eugene Belilovsky"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0378.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0378.pdf",
    "published": "2020-09",
    "summary": "Scene graph generation (SGG) aims to predict graph-structured descriptions of input images, in the form of objects and relationships between them. This task is becoming increasingly useful for progress at the interface of vision and language. Here, it is important\u2014yet challenging\u2014to perform well on novel (zero shot) or rare (few shot) compositions of objects and relationships. In this paper, we identify two key issues that limit such generalization. Firstly, we show that the standard loss used in this task is unintentionally a function of scene graph density. This leads to the neglect of individual edges in large sparse graphs during training, even though these contain diverse few shot examples that are important for generalization. Secondly, the frequency of relationships can create a strong bias in this task, such that a ``blind'' model predicting the most frequent relationship achieves good performance. Consequently, some state-of-the-art models exploit this bias to improve results. We show that such models can suffer the most in their ability to generalize to rare compositions, evaluating two different models on the Visual Genome dataset and its more recent, improved version, GQA. To address these issues, we introduce a density-normalized edge loss, which provides more than a two-fold improvement in certain generalization metrics. Compared to other works in this direction, our enhancements require only a few lines of code and no added computational cost. We also highlight the difficulty of accurately evaluating models using existing metrics, especially on zero/few shots, and introduce a novel weighted metric."
  },
  "bmvc2020_main_asap-netattentionandstructureawarepointcloudsequencesegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "ASAP-Net: Attention and Structure Aware Point Cloud Sequence Segmentation",
    "authors": [
      "Hanwen Cao",
      "Yongyi Lu",
      "Bo Pang",
      "Cewu Lu",
      "Alan Yuille",
      "Gongshen Liu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0129.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0129.pdf",
    "published": "2020-09",
    "summary": "Recent works of point clouds show that mulit-frame spatio-temporal modeling outperforms single-frame versions by utilizing cross-frame information. In this paper, we further improve spatio-temporal point cloud feature learning with a flexible module called ASAP considering both attention and structure information across frames, which we find as two important factors for successful segmentation in dynamic point clouds. Firstly, our ASAP module contains a novel attentive temporal embedding layer to fuse the relatively informative local features across frames in a recurrent fashion. Secondly, an efficient spatio-temporal correlation method is proposed to exploit more local structure for embedding, meanwhile enforcing temporal consistency and reducing computation complexity. Finally, we show the generalization ability of the proposed ASAP module with different backbone networks for point cloud sequence segmentation. Our ASAP-Net (backbone plus ASAP module) outperforms baselines and previous methods on both Synthia and SemanticKITTI datasets (+3.4 to +15.2 mIoU points with different backbones). The source codes will be made publicly available."
  },
  "bmvc2020_main_key-netsopticaltransformationconvolutionalnetworksforprivacypreservingvisionsensors": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Key-Nets: Optical Transformation Convolutional Networks for Privacy Preserving Vision Sensors",
    "authors": [
      "Jeffrey Byrne (STR",
      "Visym Labs)",
      "Brian DeCann",
      "Scott Bloom"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0101.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0101.pdf",
    "published": "2020-09",
    "summary": "Modern cameras are not designed with computer vision or machine learning as the target application.There is a need for a new class of vision sensors that are privacy preserving by design, that do not leak private information and collect only the information necessary for a target machine learning task. In this paper, we introduce key-nets, which are convolutional networks paired with a custom vision sensor which applies an optical/analog transform such that the key-net can perform exact encrypted inference on this transformed image, but the image is not interpretable by a human or any other key-net.We provide five sufficient conditions for an optical transformation suitable for a key-net, and show that generalized stochastic matrices (e.g. scale, bias and fractional pixel shuffling) satisfy these conditions.We motivate the key-net by showing that without it there is a utility/privacy tradeoff for a network fine-tuned directly on optically transformed images for face identification and object detection. Finally, we show that a key-net is equivalent to homomorphic encryption using a Hill cipher, with an upper bound on memory and runtime that scales quadratically with a user specified privacy parameter. Therefore, the key-net is the first practical, efficient and privacy preserving vision sensor based on optical homomorphic encryption."
  },
  "bmvc2020_main_high-speedevent-basedcameratracking": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "High-speed event-based camera tracking",
    "authors": [
      "William Chamorro",
      "Juan Andrade-Cetto",
      "Joan Sol\u00e0"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0366.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0366.pdf",
    "published": "2020-09",
    "summary": "Event cameras are bioinspired sensors with reaction times in the order of microseconds. This property makes them appealing for use in highly-dynamic computer vision applications. In this work, we explore the limits of this sensing technology and present an ultra-fast tracking algorithm able to estimate six-degree-of-freedom motion with dynamics over 25.8g, at a throughput of 10kHz, processing over a million events per second. Our method is capable of tracking either camera motion or the motion of an object in front of it, using an error-state Kalman filter formulated in a Lie-theoretic sense. The method includes a robust mechanism for the matching of events with projected line segments with very fast outlier rejection. Meticulous treatment of sparse matrices is applied to achieve real-time performance. Different motion models of varying complexity are considered for the sake of comparison and performance analysis."
  },
  "bmvc2020_main_descdomainadaptationfordepthestimationviasemanticconsistency": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "DESC: Domain Adaptation for Depth Estimation via Semantic Consistency",
    "authors": [
      "Adrian Lopez-Rodriguez",
      "Krystian Mikolajczyk"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0122.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0122.pdf",
    "published": "2020-09",
    "summary": "Accurate real depth annotations are difficult to acquire, needing the use of special devices such as a LiDAR sensor. Self-supervised methods try to overcome this problem by processing video or stereo sequences, which may not always be available. Instead, in this paper, we propose a domain adaptation approach to train a monocular depth estimation model using a fully-annotated source dataset and a non-annotated target dataset. We bridge the domain gap by leveraging semantic predictions and low-level edge features to provide guidance for the target domain. We enforce consistency between the main model and a second model trained with semantic segmentation and edge maps, and introduce priors in the form of instance heights. Our approach is evaluated on standard domain adaptation benchmarks for monocular depth estimation and show consistent improvement upon the state-of-the-art."
  },
  "bmvc2020_main_attribute-guidedimagegenerationfromlayout": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Attribute-Guided Image Generation from Layout",
    "authors": [
      "Ke Ma",
      "Bo Zhao",
      "Leonid Sigal"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0384.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0384.pdf",
    "published": "2020-09",
    "summary": "Recent approaches have achieved great successes in image generation from structured inputs, e.g., semantic segmentation, scene graph or layout. Although these methods allow specification of objects and their locations at image-level, they lack the fidelity and semantic control to specify visual appearance of these objects at an instance-level. To address this limitation, we propose a new image generation method that enables instance-level attribute specification. Specifically, the input to our attribute-guided generative model is a tuple that contains: (1) object bounding boxes, (2) object categories and (3) a (optional) set of attributes for each object. The output is a generated image where the requested objects are in the desired locations and have prescribed attributes. Several losses work collaboratively to encourage accurate, consistent and diverse image generation. Experiments on Visual Genome datasets demonstrate our model's capacity to control object-level attributes in generated images, and validate plausibility of disentangled object-attribute representation in the image generation from layout task. Also, the generated images from our model have higher resolution, object classification accuracy and consistency than the previous state-of-the-art."
  },
  "bmvc2020_main_initialclassifierweightsreplayformemorylessclassincrementallearning": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Initial Classifier Weights Replay for Memoryless Class Incremental Learning",
    "authors": [
      "Eden Belouadah",
      "Adrian Popescu",
      "Ioannis Kanellos"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0743.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0743.pdf",
    "published": "2020-09",
    "summary": "Incremental Learning (IL) is useful when artificial systems need to deal with streams of data and do not have access to all data at all times. The most challenging setting requires a constant complexity of the deep model and an incremental model update without access to a bounded memory of past data. Then, the representations of past classes are strongly affected by catastrophic forgetting. To mitigate its negative effect, an adapted fine tuning which includes knowledge distillation is usually deployed. We propose a different approach based on a vanilla fine tuning backbone. It leverages initial classifier weights which provide a strong representation of past classes because they are trained with all class data. However, the magnitude of classifiers learned in different states varies and normalization is needed for a fair handling of all classes. Normalization is performed by standardizing the initial classifier weights, which are assumed to be normally distributed. In addition, a calibration of prediction scores is done by using state level statistics to further improve classification fairness. We conduct a thorough evaluation with four public datasets in a memoryless incremental learning setting. Results show that our method outperforms existing techniques by a large margin for large-scale datasets."
  },
  "bmvc2020_main_textandstyleconditionedganforthegenerationofoffline-handwritinglines": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Text and Style Conditioned GAN for the Generation of Offline-Handwriting Lines",
    "authors": [
      "Brian Davis",
      "Bryan Morse",
      "Brian Price",
      "Chris Tensmeyer",
      "Curtis Wigington",
      "Rajiv Jain"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0815.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0815.pdf",
    "published": "2020-09",
    "summary": "This paper presents a GAN for generating images of handwritten lines conditioned on arbitrary text and latent style vectors. Unlike prior work, which produce stroke points or single-word images, this model generates entire lines of offline handwriting. The model produces variable-sized images by using style vectors to determine character widths. A generator network is trained with GAN and autoencoder techniques to learn style, and uses a pre-trained handwriting recognition network to induce legibility. A study using human evaluators demonstrates that the model produces images that appear to be written by a human. After training, the encoder network can extract a style vector from an image, allowing images in a similar style to be generated, but with arbitrary text."
  },
  "bmvc2020_main_ladderlatentdatadistributionmodellingwithagenerativeprior": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "LaDDer: Latent Data Distribution Modelling with a Generative Prior",
    "authors": [
      "Shuyu Lin",
      "Ronald Clark"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0816.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0816.pdf",
    "published": "2020-09",
    "summary": "In this paper, we show that the performance of a learnt generative model is closely related to the model's ability to accurately represent the inferred latent data distribution, i.e. its topology and structural properties. We propose LaDDer to achieve accurate modelling of the latent data distribution in a variational autoencoder framework and to facilitate better representation learning. The central idea of LaDDer is a meta-embedding concept, which uses multiple VAE models to learn an embedding of the embeddings, forming a ladder of encodings. We use a non-parametric mixture as the hyper prior for the innermost VAE and learn all the parameters in a unified variational framework. From extensive experiments, we show that our LaDDer model is able to accurately estimate complex latent distribution and results in improvement in the representation quality. We also propose a novel latent space interpolation method that utilises the derived data distribution. The code and demos are available at https://github.com/lin-shuyu/ladder-latent-data-distribution-modelling."
  },
  "bmvc2020_main_first-personviewhandsegmentationofmulti-modalhandactivityvideodataset": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "First-Person View Hand Segmentation of Multi-Modal Hand Activity Video Dataset",
    "authors": [
      "Sangpil Kim",
      "Hyung-gun Chi",
      "Xiao Hu",
      "Anirudh Vegesana",
      "Karthik Ramani"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0570.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0570.pdf",
    "published": "2020-09",
    "summary": "First-person-view videos of hands interacting with tools are widely used in the computer vision industry. However, creating a dataset with pixel-wise segmentation of hands is challenging since most videos are captured with fingertips occluded by the hand dorsum and grasped tools. Current methods often rely on manually segmenting hands to create annotations, which is inefficient and costly. To relieve this challenge, we create a method that utilizes thermal information of hands for efficient pixel-wise hand segmentation to create a multi-modal activity video dataset. Our method is not affected by fingertip and joint occlusions and does not require hand pose ground truth. We show our method to be 24 times faster than the traditional polygon labeling method while maintaining high quality. With the segmentation method, we propose a multi-modal hand activity video dataset with 790 sequences and 401,765 frames of \"hands using tools\" videos captured by thermal and RGB-D cameras with hand segmentation data. We analyze multiple models for hand segmentation performance and benchmark four segmentation networks. We show that our multi-modal dataset with fusing Long-Wave InfraRed~(LWIR) and RGB-D frames achieves 5% better hand IoU performance than using RGB frames."
  },
  "bmvc2020_main_6dofobjectposeestimationviadifferentiableproxyvotingregularizer": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "6DoF Object Pose Estimation via Differentiable Proxy Voting Regularizer",
    "authors": [
      "Xin Yu",
      "Zheyu Zhuang",
      "Piotr Koniusz",
      "HONGDONG LI"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0287.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0287.pdf",
    "published": "2020-09",
    "summary": "Estimating a 6DOF object pose from a single image is very challenging due to occlusions or textureless appearances. Vector-field based keypoint voting has demonstrated its effectiveness and superiority on tackling those issues. However, direct regression of vector-fields neglects that the distances between pixels and keypoints also affect the deviations of hypotheses dramatically. In other words, small errors in direction vectors may generate severely deviated hypotheses when pixels are far away from a keypoint. In this paper, we aim to reduce such errors by incorporating the distances between pixels and keypoints into our objective. To this end, we develop a simple yet effective differentiable proxy voting Regularizer (DPVR) which mimics the hypothesis selection in the voting procedure. By exploiting our voting regularizer, we are able to train our network in an end-to-end manner. Experiments on widely used datasets, ie, LINEMOD and Occlusion LINEMOD, manifest that our DPVR improves pose estimation performance significantly and speeds up the training convergence."
  },
  "bmvc2020_main_notallpointsarecreatedequal-ananisotropiccostfunctionforfaciallandmarklocation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Not all points are created equal - an anisotropic cost function for facial landmark location",
    "authors": [
      "Farshid Rayhan",
      "Aphrodite Galata",
      "Timothy Cootes"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0783.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0783.pdf",
    "published": "2020-09",
    "summary": "An effective approach to locating facial landmarks is to train a CNN to predict their positions directly from an image patch cropped around the face. Earlier work has shown that the choice of cost function comparing predicted with target points is important, but have tended to use the same weighting for each individual point. Since some points, such as those on boundaries, are less clearly defined than those at obvious corners, we propose an alternative cost function which uses anisotropic weights.This penalises movement away from feature boundaries more than that along them. We demonstrate that using this cost function improves location performance and training convergence. We also address the problem of pose imbalance in datasets, suggesting a way of balancing the poses in the training samples. State of the art results on three public datasets (AFLW, WFLW and 300W) demonstrate the effectiveness of these techniques"
  },
  "bmvc2020_main_zero-shotdomaingeneralization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Zero-Shot Domain Generalization",
    "authors": [
      "Udit Maniyar",
      "Joseph K J",
      "Aniket Anand Deshmukh",
      "Urun Dogan",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0673.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0673.pdf",
    "published": "2020-09",
    "summary": "Standard supervised learning setting assumes that training data and test data come from the same distribution (domain). Domain generalization (DG) methods try to learn a model that when trained on data from multiple domains, would generalize to a new unseen domain. We extend DG to an even more challenging setting, where the label space of the unseen domain could also change. We introduce this problem as Zero-Shot Domain Generalization (to the best of our knowledge, the first such effort), where the model generalizes across new domains and also across new classes in those domains. We propose a simple strategy which effectively exploits semantic information of classes, to adapt existing DG methods to meet the demands of Zero-Shot Domain Generalization. We evaluate the proposed methods on CIFAR-10, CIFAR-100, F-MNIST and PACS datasets; establishing a strong baseline to foster interest in this new research direction."
  },
  "bmvc2020_main_attributeadaptivemarginsoftmaxlossusingprivilegedinformation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Attribute Adaptive Margin Softmax Loss using  Privileged Information",
    "authors": [
      "Seyed mehdi Iranmanesh",
      "Ali Dabouei",
      "Nasser Nasrabadi"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0561.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0561.pdf",
    "published": "2020-09",
    "summary": "We present a novel framework to exploit privileged information for recognition which is provided only during the training phase. Here, we focus on recognition task where images are provided as the main view and soft biometric traits (attributes) are provided as the privileged data (only available during training phase). We demonstrate that more discriminative feature space can be learned by enforcing a deep network to adjust adaptive margins between classes utilizing attributes. This tight constraint also effectively reduces the class imbalance inherent in the local data neighborhood, thus carving more balanced class boundaries locally and using feature space more efficiently. Extensive experiments are performed on five different datasets and the results show the superiority of our method compared to the state-of-the-art models in both tasks of face recognition and person re-identification."
  },
  "bmvc2020_main_mda-netmemorabledomainadaptationnetworkformonoculardepthestimation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "MDA-Net: Memorable Domain Adaptation Network for Monocular Depth Estimation",
    "authors": [
      "JingZhu",
      "Yunxiao Shi",
      "Mengwei Ren",
      "Yi Fang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0790.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0790.pdf",
    "published": "2020-09",
    "summary": "Monocular depth estimation is a challenging task that aims to predict a corresponding depth map from a given single RGB image. Recent deep learning models have been proposed to predict the depth from the image by learning the alignment of deep features between the RGB image and the depth domains. In this paper, we present a novel approach, named Memorable Domain Adaptation Network (MDA-Net), to more effectively transfer domain features for monocular depth estimation by taking into account the common structure regularities (e.g., repetitivestructurepatterns,planarsurfaces, symmetries) in domain adaptation. To this end, we introduce a new Structure-Oriented Memory (SOM) module to learn and memorize the structure-specific information between RGB image domain and the depth domain. More specifically, in the SOM module, we develop a Memorable Bank of Filters (MBF) unit to learn a set of filters that memorize the structure-aware image-depth residual pattern, and also an Attention Guided Controller (AGC) unit to control the filter selection in the MBF given image features queries. Given the query image feature, the trained SOM module is able to adaptively select the best customized filters for cross-domain feature transferring with an optimal structural disparity between image and depth. In summary, we focus on addressing this structure-specific domain adaption challenge by proposing a novel end-to-end multi-scale memorable network for monocular depth estimation. The experiments show that our MDA-Net demonstrates the superior performance compared to the existing supervised monocular depth estimation approaches on the challenging KITTI and NYU Depth V2 benchmarks."
  },
  "bmvc2020_main_learningtoadaptmulti-viewstereobyself-supervision": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning to Adapt Multi-View Stereo by Self-Supervision",
    "authors": [
      "Arijit Mallick",
      "Joerg Stueckler",
      "Hendrik Lensch"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0375.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0375.pdf",
    "published": "2020-09",
    "summary": "3D scene reconstruction from multiple views is an important classical problem in computer vision. Deep learning based approaches have recently demonstrated impressive reconstruction results. When training such models, self-supervised methods are favourable since they do not rely on ground truth data which would be needed for supervised training and is often difficult to obtain. Moreover, learned multi-view stereo reconstruction is prone to environment changes and should robustly generalise to different domains. We propose an adaptive learning approach for multi-view stereo which trains a deep neural network for improved adaptability to new target domains. We use model-agnostic meta-learning (MAML) to train base parameters which, in turn, are adapted for multi-view stereo on new domains through self-supervised training. Our evaluations demonstrate that the proposed adaptation method is effective in learning self-supervised multi-view stereo reconstruction in new domains."
  },
  "bmvc2020_main_whatdocnnsgainbyimitatingthevisualdevelopmentofprimateinfants?": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "What do CNNs gain by imitating the visual development of primate infants?",
    "authors": [
      "Shantanu Jaiswal",
      "Dongkyu Choi",
      "Basura Fernando"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0196.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0196.pdf",
    "published": "2020-09",
    "summary": "Deep convolutional neural networks have emerged as strong candidates for a model of human vision, often outperforming competing models on both computer vision benchmarks and computational neuroscience benchmarks of neural response correspondence. The design of these models has undergone several refinements in recent years drawing on both statistical and cognitive insights and, in the process, shown increasing correspondence to primate visual processing representations. However, their training methodology still remains in contrast to the process of primate visual development, and we believe that it can benefit from being more aligned with this natural process. Primate visual development is characterized by low visual acuity and colour sensitivity as well as high plasticity and neuronal growth in the first year of infancy, prior to the development of specific visual-cognitive functions such as visual object recognition. In this work, we investigate the synergy between the gradual variation in the distribution of visual input and the concurrent growth of a statistical model of vision on the task of large-scale object classification, and discuss how it may yield better approaches to training deep convolutional neural networks. The experiments we performed across multiple object classification benchmarks indicate that a growing statistical model trained with a gradually varying visual input distribution converges to a better generalization at a faster rate than traditional, more static training setups."
  },
  "bmvc2020_main_automatedsearchforresource-efficientbranchedmulti-tasknetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Automated Search for Resource-Efficient Branched Multi-Task Networks",
    "authors": [
      "David Br\u00fcggemann",
      "Menelaos Kanakis",
      "Stamatios Georgoulis",
      "Luc Van Gool"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0359.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0359.pdf",
    "published": "2020-09",
    "summary": "The multi-modal nature of many vision problems calls for neural network architectures that can perform multiple tasks concurrently. Typically, such architectures have been handcrafted in the literature. However, given the size and complexity of the problem, this manual architecture exploration likely exceeds human design abilities. In this paper, we propose a principled approach, rooted in differentiable neural architecture search, to automatically define branching (tree-like) structures in the encoding stage of a multi-task neural network. To allow flexibility within resource-constrained environments, we introduce a proxyless, resource-aware loss that dynamically controls the model size. Evaluations across a variety of dense prediction tasks show that our approach consistently finds high-performing branching structures within limited resource budgets."
  },
  "bmvc2020_main_annealinggeneticganforminorityoversampling": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Annealing Genetic GAN for Minority Oversampling",
    "authors": [
      "Jingyu Hao",
      "Chengjia Wang",
      "Heye Zhang",
      "Guang Yang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0243.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0243.pdf",
    "published": "2020-09",
    "summary": "The key to overcome class imbalance problems is to capture the distribution of minority class accurately. Generative Adversarial Networks (GANs) have shown some potentials to tackle class imbalance problems due to their capability of reproducing data distributions given ample training data samples. However, the scarce samples of one or more classes still pose a great challenge for GANs to learn accurate distributions for the minority classes. In this work, we propose an Annealing Genetic GAN (AGGAN) method, which aims to reproduce the distributions closest to the ones of the minority classes using only limited data samples. Our AGGAN renovates the training of GANs as an evolutionary process that incorporates the mechanism of simulated annealing. In particular, the generator uses different training strategies to generate multiple offspring and retain the best. Then, we use the Metropolis criterion in the simulated annealing to decide whether we should update the best offspring for the generator. As the Metropolis criterion allows a certain chance to accept the worse solutions, it enables our AGGAN steering away from the local optimum. According to both theoretical analysis and experimental studies on multiple imbalanced image datasets, we prove that the proposed training strategy can enable our AGGAN to reproduce the distributions of minority classes from scarce samples and provide an effective and robust solution for the class imbalance problem."
  },
  "bmvc2020_main_view-consistent4dlightfielddepthestimation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "View-consistent 4D Light Field Depth Estimation",
    "authors": [
      "Numair Khan",
      "Min H. Kim",
      "James Tompkin"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0395.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0395.pdf",
    "published": "2020-09",
    "summary": "We propose a method to compute depth maps for every sub-aperture image in a light field in a view consistent way. Previous light field depth estimation methods typically estimate a depth map only for the central sub-aperture view, and struggle with view consistent estimation. Our method precisely defines depth edges via EPIs, then we diffuse these edges spatially within the central view. These depth estimates are then propagated to all other views in an occlusion-aware way. Finally, disoccluded regions are completed by diffusion in EPI space. Our method runs efficiently with respect to both other classical and deep learning-based approaches, and achieves competitive quantitative metrics and qualitative performance on both synthetic and real-world light fields."
  },
  "bmvc2020_main_rodeoreplayforonlineobjectdetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "RODEO: Replay for Online Object Detection",
    "authors": [
      "Manoj Acharya",
      "Tyler Hayes",
      "Christopher Kanan"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0526.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0526.pdf",
    "published": "2020-09",
    "summary": "Humans can incrementally learn to do new visual detection tasks, which is a huge challenge for today's computer vision systems. Incrementally trained deep learning models lack backwards transfer to previously seen classes and suffer from a phenomenon known as ``catastrophic forgetting.'' In this paper, we pioneer online streaming learning for object detection, where an agent must learn examples one at a time with severe memory and computational constraints. In object detection, a system must output all bounding boxes for an image with the correct label. Unlike earlier work, the system described in this paper can learn how to do this task in an online manner with new classes being introduced over time. We achieve this capability by usinga novel memory replay mechanism that replays entire scenes in an efficient manner. We achieve state-of-the-art results on both the PASCAL VOC 2007 and MS COCO datasets."
  },
  "bmvc2020_main_hastemulti-hypothesisasynchronousspeeded-uptrackingofevents": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "HASTE: multi-Hypothesis Asynchronous Speeded-up Tracking of Events",
    "authors": [
      "Ignacio Alzugaray",
      "Margarita Chli"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0744.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0744.pdf",
    "published": "2020-09",
    "summary": "Feature tracking using event cameras has experienced significant progress lately, with methods achieving comparable performance to feature trackers using traditional frame-based cameras, even outperforming them on certain challenging scenarios. Most of the event-based trackers, however, still operate on intermediate, frame-like representations generated from accumulated events, on which traditional frame-based techniques can be adopted. Attempting to harness the sparsity and asynchronicity of the event stream, other approaches have emerged to process each event individually, but they lack both in accuracy and efficiency in comparison to the event-based, frame-like alternatives. Aiming to address this shortcoming of asynchronous approaches, in this paper, we propose an asynchronous patch-feature tracker that relies solely on events and processes each event individually as soon as it gets generated. We report significant improvements in tracking quality over the state of the art in publicly available datasets, while performing an order of magnitude more efficiently than similar asynchronous tracking approaches."
  },
  "bmvc2020_main_rnn-basedmotionpredictionincompetitivefencingconsideringinteractionbetweenplayers": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "RNN-based Motion Prediction in Competitive Fencing Considering Interaction between Players",
    "authors": [
      "Yutaro Honda",
      "Rei Kawakami",
      "Takeshi Naemura"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0618.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0618.pdf",
    "published": "2020-09",
    "summary": "The ability to accurately predict the motion of fencing athletes will help to improve the competition techniques of the players and the viewing experience of the audience. Most human-motion prediction methods only consider a single person, but in fencing, the movement of the opponent greatly affects the future movements of the player. In this paper, we propose a motion prediction model that takes into account the interaction between the two players in the game by connecting the recurrent neural networks to each other. In experiments, our model improved the accuracy of predicting movements in response to the opposing player, such as retreating to avoid the opponent's thrusts."
  },
  "bmvc2020_main_neuralnetworkquantizationwithscale-adjustedtraining": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Neural Network Quantization with Scale-Adjusted Training",
    "authors": [
      "Qing Jin",
      "Linjie Yang",
      "Zhenyu Liao",
      "Xiaoning Qian"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0634.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0634.pdf",
    "published": "2020-09",
    "summary": "Quantization has long been studied as a compression and accelerating technique for deep neural networks due to its potential on reducing model size and computational costs, for both general hardware, such as DSP, CPU or GPU, and customized devices with flexible bit-width configurations, including FPGA and ASIC. However, previous works generally achieve network quantization by sacrificing on prediction accuracy with respect to their full-precision counterparts. In this paper, we investigate the underlying mechanism of such performance degeneration based on previous work of parameterized clipping activation (PACT). We find that the key factor is the weight scale in the last layer. Instead of aligning weight distributions of quantized and full-precision models, as generally suggested in the literature, the main issue is that large scale can cause over-fitting problem. We propose a technique called scale-adjusted training (SAT) by directly scaling down weights in the last layer to alleviate such over-fitting. With the proposed technique, quantized networks can demonstrate better performance than their full-precision counter-parts, and we achieve state-of-the-art accuracy with consistent improvement over previous quantization methods for light weight models including MobileNet V1/V2 on ImageNet classification."
  },
  "bmvc2020_main_mishaselfregularizednon-monotonicactivationfunction": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Mish: A Self Regularized Non-Monotonic Activation Function",
    "authors": [
      "Diganta Misra"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0928.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0928.pdf",
    "published": "2020-09",
    "summary": "We propose Mish, a novel self-regularized non-monotonic activation function which can be mathematically defined as: f(x) = xtanh(softplus(x)). As activation functions play a crucial role in the performance and training dynamics in neural networks, we validated experimentally on several well-known benchmarks against the best combinations of architectures and activation functions. We also observe that data augmentation techniques have a favorable effect on benchmarks like ImageNet-1k and MS-COCO across multiple architectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a CSP-DarkNet-53 backbone on average precision (AP-50 val) by 2.1% in MS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy by 1% while keeping all other network parameters and hyperparameters constant. Furthermore, we explore the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks. Code is publicly available at https://github.com/digantamisra98/Mish."
  },
  "bmvc2020_main_relationalgeneralizedfew-shotlearning": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Relational Generalized Few-Shot Learning",
    "authors": [
      "Xiahan Shi",
      "Leonard Salewski",
      "Martin Schiegg",
      "Max Welling"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0220.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0220.pdf",
    "published": "2020-09",
    "summary": "Transferring learned models to novel tasks is a challenging problem, particularly if only very few labeled examples are available. Most proposed methods for this few-shot learning setup focus on discriminating novel classes only. Instead, we consider the extended setup of generalized few-shot learning (GFSL), where the model is required to perform classification on the joint label space consisting of both previously seen and novel classes. We propose a graph-based framework that explicitly models relationships between all seen and novel classes in the joint label space. Our model Graph-convolutional Global Prototypical Networks (GcGPN) incorporates these inter-class relations using graph-convolution in order to embed novel class representations into the existing space of previously seen classes in a globally consistent manner. Our approach ensures both fast adaptation and global discrimination, which is the major challenge in GFSL. We demonstrate the benefits of our model on two challenging benchmark datasets."
  },
  "bmvc2020_main_multi-labelzero-shotclassificationbylearningtotransferfromexternalknowledge": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Multi-label Zero-shot Classification by Learning to Transfer from External Knowledge",
    "authors": [
      "He Huang",
      "Wei Tang",
      "PhilipYu",
      "Yuanwei Chen",
      "Wenhao Zheng",
      "Qing-Guo Chen"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0249.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0249.pdf",
    "published": "2020-09",
    "summary": "Multi-label zero-shot classification aims to predict multiple unseen class labels for an input image. It is more challenging than its single-label counterpart. On one hand, the unconstrained number of labels assigned to each image makes the model more easily overfit to those seen classes. On the other hand, there is a large semantic gap between seen and unseen classes in the existing multi-label classification datasets. To address these difficult issues, this paper introduces a novel multi-label zero-shot classification framework by learning to transfer from external knowledge. We observe that ImageNet is commonly used to pretrain the feature extractor and has a large and fine-grained label space. This motivates us to exploit it as external knowledge to bridge the seen and unseen classes and promote generalization. Specifically, we construct a knowledge graph including not only classes from the target dataset but also those from ImageNet. Since ImageNet labels are not available in the target dataset, we propose a novel PosVAE module to infer their initial states in the extended knowledge graph. Then we design a relational graph convolutional network (RGCN) to propagate information among classes and achieve knowledge transfer. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed approach."
  },
  "bmvc2020_main_asphericalapproachtoplanarsemanticsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "A Spherical Approach to Planar Semantic Segmentation",
    "authors": [
      "Chao Zhang",
      "Sen He",
      "Stephan Liwicki"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0053.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0053.pdf",
    "published": "2020-09",
    "summary": "We investigate a geometrically motivated modification to semantic segmentation. In particular, we reformulate typical planar CNN as a projected spherical CNN where image distortions are reduced, and thus generalisation increased. Since prior formulations of spherical CNNs require computation on full spheres, fair comparison between planar and spherical methods have not been previously presented. In this work, we first extend spherical deep learning to support high-resolution images by exploiting the reduced field of view of classical images. Then, we employ our spherical representation to reduce distortion effects of standard deep learning systems. On typical benchmarks, we apply our spherical representation and consistently outperform the classical representation of multiple existing architectures. Additionally, we introduce direct spherical pretrainingfromplanardatasetstofurtherimproveresults. Finally,wecompare our method on non-planar datasets, where we improve accuracy, and outperform running time of spherical state of the art for non-complete input spheres."
  },
  "bmvc2020_main_neighbourhood-insensitivepointcloudnormalestimationnetwork": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Neighbourhood-Insensitive Point Cloud Normal Estimation Network",
    "authors": [
      "Zirui Wang",
      "Victor Prisacariu"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0028.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0028.pdf",
    "published": "2020-09",
    "summary": "We introduce a novel self-attention-based normal estimation network that is able to focus softly on relevant points and adjust the softness by learning a temperature parameter, making it able to work naturally and effectively within a large neighbourhood range. As a result, our model outperforms all existing normal estimation algorithms by a large margin, achieving 94.1% accuracy in comparison with the previous state of the art of 91.2%, with a 25x smaller model and 12x faster inference time. We also use point-to-plane Iterative Closest Point (ICP) as an application case to show that our normal estimations lead to faster convergence than normal estimations from other methods, without manually fine-tuning neighbourhood range parameters. Code available at https://code.active.vision."
  },
  "bmvc2020_main_whenetreal-timefine-grainedestimationforwiderangeheadpose": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "WHENet: Real-time Fine-Grained Estimation for Wide Range Head Pose",
    "authors": [
      "Yijun Zhou",
      "James Gregson"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0907.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0907.pdf",
    "published": "2020-09",
    "summary": "We present an end-to-end head-pose estimation network designed to predict Euler angles through the full range head yaws from a single RGB image. Existing ethods perform well for frontal views but few target head pose from all viewpoints. This has applications in autonomous driving and retail. Our network builds on multi-loss approaches with changes to loss functions and training strategies adapted to wide range estimation. Additionally, we extract ground truth labelings of anterior views from a current panoptic dataset for the first time. The resulting Wide Headpose Estimation Network (WHENet) is the first fine-grained modern method applicable to the full-range of head yaws (hence wide) yet also meets or beats state-of-the-art methods for frontal head pose estimation. Our network is compact and efficient for mobile devices and applications. We will release the trained model to aid future researchers in this important topic."
  },
  "bmvc2020_main_bipartiteconditionalrandomfieldsforpanopticsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Bipartite Conditional Random Fields for Panoptic Segmentation",
    "authors": [
      "Sadeep Jayasumana",
      "KanchanaRanasinghe",
      "Sahan Liyanaarachchi",
      "Bethmage Mayuka Jayawardhana",
      "Harsha Ranasinghe",
      "Sina Samangooei"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0184.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0184.pdf",
    "published": "2020-09",
    "summary": "We tackle the panoptic segmentation problem with a conditional random field (CRF) model. Panoptic segmentation involves assigning a semantic label and an instance label to each pixel of a given image. At each pixel, the semantic label and the instance label should be compatible. Furthermore, a good panoptic segmentation should have a number of other desirable properties such as the spatial and color consistency of the labeling. To tackle this problem, we propose a CRF model, named Bipartite CRF or BCRF, with two types of random variables for semantic and instance labels. In this formulation, various energies are defined within and across the two types of random variables to encourage a consistent panoptic segmentation. We propose a mean-field-based efficient inference algorithm for solving the CRF and empirically show its convergence properties. This algorithm is fully differentiable, and therefore, BCRF inference can be included as a trainable module in any deep network. In the experimental evaluation, we quantitatively and qualitatively show that the BCRF yields superior panoptic segmentation results in practice."
  },
  "bmvc2020_main_real-timescreenreadingreducingdomainshiftforone-shotlearning": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Real-time screen reading: reducing domain shift for one-shot learning",
    "authors": [
      "James Charles",
      "Stefano Bucciarelli",
      "Roberto Cipolla"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0512.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0512.pdf",
    "published": "2020-09",
    "summary": "Many digital meters such as those used for home health (e.g. blood pressure meters) or meters monitoring industrial equipment do not contain wireless connectivity. Hence, connecting these devices to phone tracking apps or control centres either requires cumbersome manual transcription or is not plausible due to costs. Our motivation is to cheaply retro-fit these types of meters with `smart' data transfer capabilities using a mobile phone app and limited training data. We demonstrate how one can use single training images of meter screens to build efficient custom meter readers targeted to chosen devices. To this end, we build a CNN based system which runs in real-time on mobile device with very high read accuracy (close to 100%). Our contributions include (i) introduction of an exciting new application domain, (ii) a method of training from purely synthetic data by reducing domain shift using a surprisingly simple approach which unlike adversarial training based methods does not even require unlabelled data; (iii) a highly accurate system for parsing digital meter screens and (iv) release of a new screen reading dataset. The system, although trained solely on synthetic data, transfers very well to the real-world. Our method of screen detection and text recognition also improves over the state of the art on our dataset."
  },
  "bmvc2020_main_revisitingtemporalmodelingforvideosuper-resolution": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Revisiting Temporal Modeling for Video Super-resolution",
    "authors": [
      "Takashi Isobe",
      "Fang Zhu",
      "Shengjin Wang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0033.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0033.pdf",
    "published": "2020-09",
    "summary": "Video super-resolution plays an important role in surveillance video analysis and ultra-high-definition video display, which has drawn much attention in both the research and industrial communities. Although many deep learning-based VSR methods have been proposed, it is hard to directly compare these methods since the different loss functions and training datasets have a significant impact on the super-resolution results. In this work, we carefully study and compare three temporal modeling methods (2D CNN with early fusion, 3D CNN with slow fusion and Recurrent Neural Network) for video super-resolution. We also propose a novel Recurrent Residual Network (RRN) for efficient video super-resolution, where residual learning is utilized to stabilize the training of RNN and meanwhile to boost the super-resolution performance. Extensive experiments show that the proposed RRN is highly computational efficiency and produces temporal consistent VSR results with finer details than other temporal modeling methods. Besides, the proposed method achieves state-of-the-art results on several widely used benchmarks."
  },
  "bmvc2020_main_stratifiedautocalibrationofcameraswitheuclideanimageplane": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Stratified Autocalibration of Cameras with Euclidean Image Plane",
    "authors": [
      "Devesh Adlakha",
      "Adlane Habed",
      "Fabio Morbidi",
      "Cedric Demonceaux",
      "Michel de Mathelin"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0383.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0383.pdf",
    "published": "2020-09",
    "summary": "This paper tackles the problem of stratified autocalibration of a moving camera with Euclidean image plane (i.e. zero skew and unit aspect ratio) and constant intrinsic parameters. We show that with these assumptions, in addition to the polynomial derived from the so-called modulus constraint, each image pair provides a new quartic polynomial in the unknown plane at infinity. For three or more images, the plane at infinity estimation is stated as a constrained polynomial optimization problem that can efficiently be solved using Lasserre\u2019s hierarchy of semidefinite relaxations. The calibration parameters and thus a metric reconstruction are subsequently obtained by solving a system of linear equations. Synthetic data and real image experiments show that the new polynomial in our proposed algorithm leads to a more reliable performance than existing methods."
  },
  "bmvc2020_main_towardsconvolutionalneuralnetworkscompressionviaglobal&progressiveproductquantization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Towards Convolutional Neural Networks Compression via Global&Progressive Product Quantization",
    "authors": [
      "Weihan Chen",
      "PeisongWang",
      "Jian Cheng"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0452.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0452.pdf",
    "published": "2020-09",
    "summary": "In recent years, we have witnessed the great success of convolutional neural networks in a wide range of visual applications. However, these networks are typically deficient due to the high cost in storage and computation, which prohibits their further extensions to resource-limited applications. In this paper, we introduce Global&Progressive Product Quantization(G&P PQ), an end-to-end product quantization based network compression method, to merge the separate quantization and finetuning process into a consistent training framework. Compared to existing two-stage methods, we avoid the time-consuming process of choosing layer-wise finetuning hyperparameters and also make the network capable of learning complex dependencies among layers by quantizing globally and progres- sively. To validate the effectiveness, we benchmark G&P PQ by applying it to ResNet-like architectures for image classification and demonstrate state-of- the-art tradeoff in terms of model size vs. accuracy with extensive compression configurations."
  },
  "bmvc2020_main_anetfviewofdropoutregularization": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "An ETF view of Dropout regularization",
    "authors": [
      "Dor Bank",
      "Raja Giryes"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0044.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0044.pdf",
    "published": "2020-09",
    "summary": "Dropout is a popular regularization technique in deep learning. Yet, the reason for its success is still not fully understood. This paper provides a new interpretation of Dropout from a frame theory perspective. By drawing a connection to recent developments in analog channel coding, we suggest that for a certain family of autoencoders with a linear encoder, optimizing the encoder with dropout regularization leads to an equiangular tight frame (ETF). Since this optimization is non-convex, we add another regularization that promotes such structures by minimizing the cross-correlation between filters in the network. We demonstrate its applicability in convolutional and fully connected layers in both feed-forward and recurrent networks. All these results suggest that there is indeed a relationship between dropout and ETF structure of the regularized linear operations."
  },
  "bmvc2020_main_fastconvexrelaxationsusinggraphdiscretizations": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Fast Convex Relaxations using Graph Discretizations",
    "authors": [
      "Jonas Geiping",
      "Fjedor Gaede",
      "Hartmut Bauermeister",
      "Michael Moeller"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0694.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0694.pdf",
    "published": "2020-09",
    "summary": "Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10."
  },
  "bmvc2020_main_reducinglabelnoiseinanchor-freeobjectdetection": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Reducing Label Noise in Anchor-Free Object Detection",
    "authors": [
      "Nermin Samet",
      "Samet Hicsonmez",
      "Emre Akbas"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0737.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0737.pdf",
    "published": "2020-09",
    "summary": "Current anchor-free object detectors label all the features that spatially fall inside a predefined central region of a ground-truth box as positive. This approach causes label noise during training, since some of these positively labeled features may be on the background or an occluder object, or they are simply not discriminative features. In this paper, we propose a new labeling strategy aimed to reduce the label noise in anchor-free detectors. We sum-pool predictions stemming from individual features into a single prediction. This allows the model to reduce the contributions of non-discriminatory features during training. We develop a new one-stage, anchor-free object detector, PPDet, to employ this labeling strategy during training and a similar prediction pooling method during inference. On the COCO dataset, PPDet achieves the best performance among anchor-free top-down detectors and performs on-par with the other state-of-the-art methods. It also outperforms all major one-stage and two-stage methods in small object detection (APs 31.4). Code is available at https://github.com/nerminsamet/ppdet."
  },
  "bmvc2020_main_textattributeaggregationandvisualfeaturedecompositionforpersonsearch": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Text Attribute Aggregation and Visual Feature Decomposition for Person Search",
    "authors": [
      "Sara Iodice",
      "Krystian Mikolajczyk"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0266.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0266.pdf",
    "published": "2020-09",
    "summary": "Person search is the task of retrieving a pedestrian image given a list of text attributes. We investigate a novel mechanism that operates in feature embedding space for matching data across visual and text modalities. We propose a framework (TAVD) with two complementary modules: Text attribute feature aggregation (TA) that aggregates multiple semantic attributes in a bimodal space for globally matching text descriptions with images and Visual feature decomposition (VD) which performs feature embedding for locally matching image regions with text attributes. The results and comparisons to the state of the art on three standard benchmarks demonstrate that our solution is an effective strategy for retrieving person images while retaining the semantic of each query text attribute."
  },
  "bmvc2020_main_branchedmulti-tasknetworksdecidingwhatlayerstoshare": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Branched Multi-Task Networks: Deciding what layers to share",
    "authors": [
      "Simon Vandenhende",
      "Stamatios Georgoulis",
      "Luc Van Gool",
      "Bert De Brabandere"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0213.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0213.pdf",
    "published": "2020-09",
    "summary": "In the context of multi-task learning, neural networks with branched architectures have often been employed to jointly tackle the tasks at hand. Such ramified networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. Understandably, as the number of possible network configurations is combinatorially large, deciding what layers to share and where to branch out becomes cumbersome. Prior works have either relied on ad hoc methods to determine the level of layer sharing, which is suboptimal, or utilized neural architecture search techniques to establish the network design, which is considerably expensive. In this paper, we go beyond these limitations and propose an approach to automatically construct branched multi-task networks, by leveraging the employed tasks' affinities. Given a specific budget, i.e. number of learnable parameters, the proposed approach generates architectures, in which shallow layers are task-agnostic, whereas deeper ones gradually grow more task-specific. Extensive experimental analysis across numerous, diverse multi-tasking datasets shows that, for a given budget, our method consistently yields networks with the highest performance, while for a certain performance threshold it requires the least amount of learnable parameters."
  },
  "bmvc2020_main_liftedregression/reconstructionnetworks": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Lifted Regression/Reconstruction Networks",
    "authors": [
      "Rasmus H\u00f8ier",
      "Christopher Zach"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0662.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0662.pdf",
    "published": "2020-09",
    "summary": "In this work we propose lifted regression/reconstruction networks (LRRNs), which combine lifted neural networks with a guaranteed Lipschitz continuity property for the output layer. Lifted neural networks explicitly optimize an energy model to infer the unit activations and therefore---in contrast to standard feed-forward neural networks---allow bidirectional feedback between layers. So far lifted neural networks have been modelled around standard feed-forward architectures. We propose to take further advantage of the feedback property by letting the layers simultaneously perform regression and reconstruction. The resulting lifted network architecture allows to control the desired amount of Lipschitz continuity, which is an important feature to obtain adversarially robust regression and classification methods. We analyse and numerically demonstrate applications for unsupervised and supervised learning."
  },
  "bmvc2020_main_poseproposalcriticrobustposerefinementbylearningreprojectionerrors": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Pose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors",
    "authors": [
      "Lucas Brynte",
      "FredrikKahl"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0274.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0274.pdf",
    "published": "2020-09",
    "summary": "In recent years, considerable progress has been made for the task of rigid object pose estimation from a single RGB-image, but achieving robustness to partial occlusions remains a challenging problem. Pose refinement via rendering has shown promise in order to achieve improved results, in particular, when data is scarce. In this paper we focus our attention on pose refinement, and show how to push the state-of-the-art further in the case of partial occlusions. The proposed pose refinement method leverages on a simplified learning task, where a CNN is trained to estimate the reprojection error between an observed and a rendered image. We experiment by training on purely synthetic data as well as a mixture of synthetic and real data. Current state-of-the-art results are outperformed for two out of three metrics on the Occlusion LINEMOD benchmark, while performing on-par for the final metric."
  },
  "bmvc2020_main_learningeffectivelyfromnoisysupervisionforweaklysupervisedsemanticsegmentation": {
    "conf_id": "BMVC2020",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2020",
    "title": "Learning Effectively from Noisy Supervision for Weakly Supervised Semantic Segmentation",
    "authors": [
      "Wenbin Xie",
      "Qiaoqiao Wei",
      "Zheng Li",
      "Hui Zhang"
    ],
    "page_url": "https://www.bmvc2020-conference.com/conference/papers/paper_0578.html",
    "pdf_url": "https://www.bmvc2020-conference.com/assets/papers/0578.pdf",
    "published": "2020-09",
    "summary": "Semantic segmentation based on deep learning has undergone tremendous progress in recent years. However, it continues to depend heavily on massive densely annotated data.In this paper, we propose a novel framework for weakly supervised semantic segmentation (WSSS) using bounding boxes to alleviate the need for pixel-wise annotations. We argue that the most important problem of WSSS should be learning effectively from noisy supervision. Therefore, we present a constrained foreground segmentation network (CFS) to generate high-quality dense annotations from noisy proposals. The network converts the segmentation task from multi-class classification to two-class classification and removes most of irrelevant regions, making the task easier to optimize. Besides, we introduce a loss-guided self-attention (LGSA) module to encourage self-correcting among intra-class pixels. Instead of allowing global information exchanges in existing non-local networks, our module imposes loss constraints on the information exchanges between different categories and learns a more reasonable affinity matrix which can be used for further random walk. Experiments indicate that our LGSA module has better performance and interpretability even with noisy supervision. We obtain state-of-the-art results on the Pascal VOC 2012 validation set by combining the two novel components."
  }
}