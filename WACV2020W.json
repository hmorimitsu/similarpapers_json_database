{
  "wacv2020_w1_analysisofgenderinequalityinfacerecognitionaccuracy": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Demographic Variation in the Performance of Biometric Systems",
    "title": "Analysis of Gender Inequality In Face Recognition Accuracy",
    "authors": [
      "Vitor Albiero",
      "Krishnapriya K.S.",
      "Kushal Vangara",
      "Kai Zhang",
      "Michael C. King",
      "Kevin W. Bowyer"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w1/Albiero_Analysis_of_Gender_Inequality_In_Face_Recognition_Accuracy_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w1/Albiero_Analysis_of_Gender_Inequality_In_Face_Recognition_Accuracy_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " We present a comprehensive analysis of how and why face recognition accuracy differs between men and women. We show that accuracy is lower for women due to the combination of (1) the impostor distribution for women having a skew toward higher similarity scores, and (2) the genuine distribution for women having a skew toward lower similarity scores. We show that this phenomenon of the impostor and genuine distributions for women shifting closer towards each other is general across datasets of African-American, Caucasian, and Asian faces. We show that the distribution of facial expressions may differ between male/female, but that the accuracy difference persists for image subsets rated confidently as neutral expression. The accuracy difference also persists for image subsets rated as close to zero pitch angle. Even when removing images with forehead partially occluded by hair/hat, the same impostor/genuine accuracy difference persists. We show that the female genuine distribution improves when only female images without facial cosmetics are used, but that the female impostor distribution also degrades at the same time. Lastly, we show that the accuracy difference persists even if a state-of-the-art deep learning method is trained from scratch using training data explicitly balanced between male and female images and subjects."
  },
  "wacv2020_w1_mitigatingalgorithmicbiasevolvinganaugmentationpolicythatisnon-biasing": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Demographic Variation in the Performance of Biometric Systems",
    "title": "Mitigating Algorithmic Bias: Evolving an Augmentation Policy that is Non-Biasing",
    "authors": [
      "Philip Smith",
      "Karl Ricanek"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w1/Smith_Mitigating_Algorithmic_Bias_Evolving_an_Augmentation_Policy_that_is_Non-Biasing_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w1/Smith_Mitigating_Algorithmic_Bias_Evolving_an_Augmentation_Policy_that_is_Non-Biasing_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Artificial Intelligence promises to make the world a safer place through automation. Automobiles can be steered between traffic lines, spoken words can be translated to textual commands, and wanted persons can be identified by law enforcement. These tasks, once only surmountable by humans, can now be performed by AIs with great speed and precision. If the algorithms are negatively biased against certain groups, what unforeseen harm may come to society? This work focuses on the classification of gender and age, a problem known to have systemic negative bias for certain subgroups, to investigate the role of data augmentation in the mitigation of such bias. A novel approach is proposed for mitigating bias in a deep learning algorithm that estimates age and gender. Settings for numerous data augmentation techniques are learned through an evolutionary process that optimizes data augmentation for specific subgroups. This approach proves to reduce systemic bias while also generalizing models and obtaining results that are state-of-the-art. The tools we use for determining human biometrics must be fair and non-discriminatory. This work examines not only bias, but also the insights gleaned from successful and unsuccessful policies in certain scenarios. "
  },
  "wacv2020_w1_reducinggeographicperformancedifferentialsforfacerecognition": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Demographic Variation in the Performance of Biometric Systems",
    "title": "Reducing Geographic Performance Differentials for Face Recognition",
    "authors": [
      "Martins Bruveris",
      "Jochem Gietema",
      "Pouria Mortazavian",
      "Mohan Mahadevan"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w1/Bruveris_Reducing_Geographic_Performance_Differentials_for_Face_Recognition_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w1/Bruveris_Reducing_Geographic_Performance_Differentials_for_Face_Recognition_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " As face recognition algorithms become more accurate and get deployed more widely, it becomes increasingly important to ensure that the algorithms work equally well for everyone. We study the geographic performance differentials--differences in false acceptance and false rejection rates across different countries--when comparing selfies against photos from ID documents. We show how to mitigate geographic performance differentials using sampling strategies despite large imbalances in the dataset. Using vanilla domain adaptation strategies to fine-tune a face recognition CNN on domain-specific doc-selfie data improves the performance of the model on such data, but, in the presence of imbalanced training data, also significantly increases the demographic bias. We then show how to mitigate this effect by employing sampling strategies to balance the training procedure."
  },
  "wacv2020_w2_re-identificationofzebrafishusingmetriclearning": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w2",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deep Learning Methods and Applications for Animal Re-Identification",
    "title": "Re-Identification of Zebrafish using Metric Learning",
    "authors": [
      "Joakim Bruslund Haurum",
      "Anastasija Karpova",
      "Malte Pedersen",
      "Stefan Hein Bengtson",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w2/Haurum_Re-Identification_of_Zebrafish_using_Metric_Learning_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w2/Haurum_Re-Identification_of_Zebrafish_using_Metric_Learning_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Zebrafish are widely used for drug development and behavioral pattern studies. The currently employed zebrafish re-identification methods rely solely on top-view and grayscale images which require a significant amount of annotated data in order to perform well. In this paper, for the first time, we perform zebrafish re-identification using RGB images recorded from a side-view perspective, while keeping the amount of data annotation to a minimum. Inspired by the person re-identification field, two feature descriptors are tested, each encoding both color and texture information, and five metric and subspace learning methods. The contribution of the color and texture components of the feature descriptors were also investigated. We present and evaluate on a novel publicly available dataset of six zebrafish, recorded in a laboratory setup. The results show that a mean average precision of 99% can be achieved by using just 15 annotated samples per fish. This approach shows a clear potential for incorporating the side-view information in the field of zebrafish tracking, as well as a clear argument for utilizing the color information of the zebrafish."
  },
  "wacv2020_w2_learninglandmarkguidedembeddingsforanimalre-identification": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w2",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deep Learning Methods and Applications for Animal Re-Identification",
    "title": "Learning Landmark Guided Embeddings for Animal Re-identification",
    "authors": [
      "Olga Moskvyak",
      "Frederic Maire",
      "Feras Dayoub",
      "Mahsa Baktashmotlagh"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w2/Moskvyak_Learning_Landmark_Guided_Embeddings_for_Animal_Re-identification_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w2/Moskvyak_Learning_Landmark_Guided_Embeddings_for_Animal_Re-identification_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Re-identification of individual animals in images can be ambiguous due to subtle variations in body markings between different individuals and no constraints on the poses of animals in the wild. Person re-identification is a similar task and it has been approached with a deep convolutional neural network (CNN) that learns discriminative embeddings for images of people. However, learning discriminative features for an individual animal is more challenging than for a person's appearance due to the relatively small size of ecological datasets compared to labelled datasets of person's identities. We propose to improve embedding learning by exploiting body landmarks information explicitly. Body landmarks are provided to the input of a CNN as confidence heatmaps that can be obtained from a separate body landmark predictor. The model is encouraged to use heatmaps by learning an auxiliary task of reconstructing input heatmaps. Body landmarks guide a feature extraction network to learn the representation of a distinctive pattern and its position on the body. We evaluate the proposed method on a large synthetic dataset and a small real dataset. Our method outperforms the same model without body landmarks input by 26% and 18% on the synthetic and the real datasets respectively.The method is robust to noise in input coordinates and can tolerate an error in coordinates up to 10% of the image size."
  },
  "wacv2020_w2_cafma3dmorphablemodelforanimals": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w2",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deep Learning Methods and Applications for Animal Re-Identification",
    "title": "CAFM: A 3D Morphable Model for Animals",
    "authors": [
      "Yifan Sun",
      "Noboru Murata"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w2/Sun_CAFM_A_3D_Morphable_Model_for_Animals_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w2/Sun_CAFM_A_3D_Morphable_Model_for_Animals_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " We present Cat-like Animals Facial Model (CAFM) -- a 3D Morphable Model (3DMM) constructed from 50 samples, including lion, tiger, puma, American Shorthair, Abyssinian cat, etc. To the best of our knowledge, CAFM is the first animal morphable model ever constructed. New animal face images can be registered automatically by fitting pose and shape parameters of CAFM. Moreover, the parametric model regulates the naturalness of the generated animal faces avoiding unreasonable appearance. Computer vision has recently experienced great advances in automatic facial landmark detection. In this paper, to demonstrate CAFM's application to 3D reconstruction of cat face images, and to put effort towards uniform annotation scheme of immense databases and fair experimental comparison of cat-like animals' facial landmark systems, we improve the labeled cat face data set of 10,000 images with 15 landmarks. Besides, we propose an algorithm matching our model to the input cat face images. With the projection parameters and shape parameter of CAFM, we can generate corresponding 3D meshes."
  },
  "wacv2020_w2_siamesenetworkbasedpelagepatternmatchingforringedseal\nre-identification": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w2",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deep Learning Methods and Applications for Animal Re-Identification",
    "title": "Siamese Network Based Pelage Pattern Matching for Ringed Seal\nRe-identification",
    "authors": [
      "Ekaterina Nepovinnykh",
      "Tuomas Eerola",
      "Heikki Kalviainen"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w2/Nepovinnykh_Siamese_Network_Based_Pelage_Pattern_Matching_for_Ringed_SealRe-identification_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w2/Nepovinnykh_Siamese_Network_Based_Pelage_Pattern_Matching_for_Ringed_SealRe-identification_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " In this paper we propose a method to match pelage patterns of the Saimaa ringed seals enabling the re-identification of individuals. First, the pelage pattern is extracted from the seal's fur using a method based on the Sato tubeness filter. After this, the similarities of the pelage pattern patches are computed using a siamese network trained with a triplet loss function and a large dataset of manually selected patches.The similarities are then used to find the best matching patches from the images in the database of known individuals. Furthermore, we employ the proposed pattern matching method to build a full framework for the ringed seal re-identification, consisting of CNN-based animal segmentation, patch correspondence detection, and ranking the images in the database of known seal individuals based on the similarity to the query image. Our experiments on challenging datasets of Saimaa ringed seals show that the proposed method achieves promising identification results, providing a useful tool for the Saimaa ringed seal monitoring."
  },
  "wacv2020_w2_bumblebeere-identificationdataset": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w2",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deep Learning Methods and Applications for Animal Re-Identification",
    "title": "Bumblebee Re-Identification Dataset",
    "authors": [
      "Frederic Tausch",
      "Simon Stock",
      "Julian Fricke",
      "Olaf Klein"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w2/Tausch_Bumblebee_Re-Identification_Dataset_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w2/Tausch_Bumblebee_Re-Identification_Dataset_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Over the past decade, entomologists have observed a worldwide decline in the population of pollinating insects. Even though there is a number of hypothesesabout of potential underlying causes for this decline, thereare concerns that insecticides are at least partially responsible. The design of insecticides requires accuraterisk assessment procedures to avoid damage to beneficial in-sects and like pollinators such as the buff-tailed bumblebee(Bombus terrestris). Therefore, current guideline drafts by the EFSA include homing fight studies to test insect behavioral performance. Since these studies (e.g., homing study) inspect the individual's change in behavior, insect re-ID (reidentification) is crucial. Current bumblebee re-ID techniques are limited to placing markers on individual subjects, which can be analog or digital (RFID). These steps are very time consuming, intrusive and are prone to mechanical failure. To find a solution to demedy these shortcomings, a visual approach is pro-posed. Therefore we propose the following bumblebee re-ID dataset in order to build such systems."
  },
  "wacv2020_w2_fusinganimalbiometricswithautonomousroboticsdrone-basedsearchandindividualidoffriesiancattle(extendedabstract)": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w2",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deep Learning Methods and Applications for Animal Re-Identification",
    "title": "Fusing Animal Biometrics with Autonomous Robotics: Drone-based Search and Individual ID of Friesian Cattle (Extended Abstract)",
    "authors": [
      "William Andrew",
      "Colin Greatwood",
      "Tilo Burghardt"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w2/Andrew_Fusing_Animal_Biometrics_with_Autonomous_Robotics_Drone-based_Search_and_Individual_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w2/Andrew_Fusing_Animal_Biometrics_with_Autonomous_Robotics_Drone-based_Search_and_Individual_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " This work covers the robotic drone integration of a re-identification system for Friesian Cattle. We have built a computationally-enhanced M100 UAV platform with an onboard deep learning inference system for integrated computer vision and navigation able to autonomously find and visually identify by coat pattern individual Holstein Friesian cattle in freely moving herds. For autonomous drone-based identification we describe an approach that utilises three deep convolutional neural network architectures running live onboard the aircraft; that is, a YoloV2-based species detector, a dual-stream Convolutional Neural Network (CNN) delivering exploratory agency and an InceptionV3-based biometric Long-term Recurrent Convolutional Network (LRCN) for individual animal identification. We evaluate the performance of components offline, and also online via real-world field tests of autonomous low-altitude flight in a farm environment. The presented proof-of-concept system is a successful step towards autonomous biometric identification of individual animals fromthe air in open pasture environments and inside farms for tag-less AI support in farming and ecology. The work is published in full in IROS 2019."
  },
  "wacv2020_w2_similaritylearningnetworksforanimalindividualre-identification-beyondthecapabilitiesofahumanobserver": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w2",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deep Learning Methods and Applications for Animal Re-Identification",
    "title": "Similarity Learning Networks for Animal Individual Re-Identification - Beyond the Capabilities of a Human Observer",
    "authors": [
      "Stefan Schneider",
      "Graham W. Taylor",
      "Stefan C. Kremer"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w2/Schneider_Similarity_Learning_Networks_for_Animal_Individual_Re-Identification_-_Beyond_the_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w2/Schneider_Similarity_Learning_Networks_for_Animal_Individual_Re-Identification_-_Beyond_the_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Deep learning has become the standard methodology to approach computer vision tasks when large amounts of labeled data are available. One area where traditional deep learning approaches fail to perform is one-shot learning tasks where a model must correctly classify a new category after seeing only one example. One such domain is animal re-identification, an application of computer vision which can be used globally as a method to automate species population estimates from camera trap images. Our work demonstrates both the application of similarity comparison networks to animal re-identification, as well as the capabilities of deep convolutional neural networks to generalize across domains. Few studies have considered animal re-identification methods across species. Here, we compare two similarity comparison methodologies: Siamese and Triplet-Loss, based on the AlexNet, VGG-19, DenseNet201, MobileNetV2, and InceptionV3 architectures considering mean average precision (mAP)@1 and mAP@5. We consider five data sets corresponding to five different species: humans, chimpanzees, humpback whales, fruit flies, and Siberian tigers, each with their own unique set of challenges. We demonstrate that Triplet Loss outperformed its Siamese counterpart for all species. Without any species-specific modifications, our results demonstrate that similarity comparison networks can reach a performance level beyond that of humans for the task of animal re-identification. The ability for researchers to re-identify an animal individual upon re-encounter is fundamental for addressing a broad range of questions in the study of population dynamics and community/behavioural ecology. Our expectation is that similarity comparison networks are the beginning of a major trend that could stand to revolutionize animal re-identification from camera trap data."
  },
  "wacv2020_w3_impactofimagenetmodelselectionondomainadaptation": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w3",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Neural Architecture Search for Computer Vision in the Wild",
    "title": "Impact of ImageNet Model Selection on Domain Adaptation",
    "authors": [
      "Youshan Zhang",
      "Brian D. Davison"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w3/Zhang_Impact_of_ImageNet_Model_Selection_on_Domain_Adaptation_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w3/Zhang_Impact_of_ImageNet_Model_Selection_on_Domain_Adaptation_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Deep neural networks are widely used in image classification problems. However, little work addresses how features from different deep neural networks affect the domain adaptation problem. Existing methods often extract deep features from one ImageNet model,without exploring other neural networks. In this paper, we investigate how different ImageNet models affect transfer accuracy on domain adaptation problems. We extract features from sixteen distinct pre-trained ImageNet models and examine the performance of twelve benchmarking methods when using the features. Extensive experimental results show that a higher accuracy ImageNet model produces better features, and leads to higher accuracy on domain adaptation problems (with a correlation coefficient of up to 0.95). We also examine the architecture of each neural network to find the best layer for feature extraction. Together, performance from our features exceeds that of the state-of-the-art in three benchmark datasets."
  },
  "wacv2020_w3_memory-efficientmodelsforscenetextrecognitionvianeuralarchitecturesearch": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w3",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Neural Architecture Search for Computer Vision in the Wild",
    "title": "Memory-Efficient Models for Scene Text Recognition via Neural Architecture Search",
    "authors": [
      "SeulGi Hong",
      "DongHyun Kim",
      "Min-Kook Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w3/Hong_Memory-Efficient_Models_for_Scene_Text_Recognition_via_Neural_Architecture_Search_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w3/Hong_Memory-Efficient_Models_for_Scene_Text_Recognition_via_Neural_Architecture_Search_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Meta-learning techniques based on neural architecture search (NAS) show excellent performance in the design of learning models used in deep neural networks. In particular, when NAS is applied to design a convolutional neural network (CNN) for image recognition, the performance of the network when evaluating public benchmark datasets such as CIFAR10 and ImageNet exceeds that of hand-designed models. Nevertheless, there are very few cases wherein NAS has been applied to real-world problems, i.e. recognition problems with a limited dataset. We proposed a method in which the NAS technique does not require a proxy task for the scene text recognition (STR) framework to apply the NAS method to a new image recognition field. Therefore, we proposed an architecture space for CNN-based modules in the STR framework and applied the ProxylessNAS method, enabling end-to-end training while meta learners design a new model that requires only a single commonly used GPU (approximately 100 GPU hours). To evaluate the STR model obtained by the proposed NAS method, seven STR benchmark datasets were used. Finally, the obtained model could achieve a performance similar to that of the ideal model in terms of accuracy and number of parameters. We thus confirm that the model design based on NAS can be effectively applied to STR scenarios."
  },
  "wacv2020_w4_disruptingimage-translation-baseddeepfakealgorithmswithadversarialattacks": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w4",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deepfakes and Presentation Attacks in Biometrics",
    "title": "Disrupting Image-Translation-Based DeepFake Algorithms with Adversarial Attacks",
    "authors": [
      "Chin-Yuan Yeh",
      "Hsi-Wen Chen",
      "Shang-Lun Tsai",
      "Sheng-De Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w4/Yeh_Disrupting_Image-Translation-Based_DeepFake_Algorithms_with_Adversarial_Attacks_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w4/Yeh_Disrupting_Image-Translation-Based_DeepFake_Algorithms_with_Adversarial_Attacks_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " DeepNude, a deep generative software based on image-to-image translation algorithm, excelling in undressing photos of humans and producing realistic nude images. Although the software was later purged from the Internet, image translation algorithms such as CycleGAN, pix2pix, or pix2pixHD can easily be applied by anyone to recreate a new version of DeepNude. This work addresses the issue by introducing a novel aspect of image translating algorithms, namely the possibility of adversarially attacking these algorithms. We modify the input images by the adversarial loss, and thereby the edited images would not be counterfeited easily by these algorithms. The proposed technique can provide a guideline to future research on defending personal images from malicious use of image translation algorithms. "
  },
  "wacv2020_w4_syn2realforgeryclassificationviaunsuperviseddomainadaptation": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w4",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deepfakes and Presentation Attacks in Biometrics",
    "title": "Syn2Real: Forgery Classification via Unsupervised Domain Adaptation",
    "authors": [
      "Akash Kumar",
      "Arnav Bhavsar",
      "Rajesh Verma"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w4/Kumar_Syn2Real_Forgery_Classification_via_Unsupervised_Domain_Adaptation_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w4/Kumar_Syn2Real_Forgery_Classification_via_Unsupervised_Domain_Adaptation_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " In recent years, image manipulation is becoming increasingly more accessible, yielding more natural-looking images, owing to the modern tools in image processing and computer vision techniques. The task of the identification of forged images has become very challenging. Amongst different types of forgeries, the cases of Copy-Move forgery are increasing manifold, due to the difficulties involved to detect this tampering. To tackle such problems, publicly available datasets are insufficient. Addressing this issue, we employed unsupervised domain adaptation to learn the discriminative features from a large dataset and classify the forged images in new domains by feature space mapping. We synthesized a forgery dataset using image inpainting and copy-move forgery algorithm. However, models trained on these synthetic datasets have a significant drop in performance when tested on more realistic data. We improvised the F1 score on CASIA and CoMoFoD dataset to 80.3% and 78.8%, respectively outperforming state-of-the-art copy-move classification algorithms. Our approach can be helpful in those cases where the classification of data is unavailable."
  },
  "wacv2020_w4_imd2020alarge-scaleannotateddatasettailoredfordetectingmanipulatedimages": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w4",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Deepfakes and Presentation Attacks in Biometrics",
    "title": "IMD2020: A Large-Scale Annotated Dataset Tailored for Detecting Manipulated Images",
    "authors": [
      "Adam Novozamsky",
      "Babak Mahdian",
      "Stanislav Saic"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w4/Novozamsky_IMD2020_A_Large-Scale_Annotated_Dataset_Tailored_for_Detecting_Manipulated_Images_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w4/Novozamsky_IMD2020_A_Large-Scale_Annotated_Dataset_Tailored_for_Detecting_Manipulated_Images_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Witnessing impressive results of deep nets in a number of computer vision problems, the image forensic community has begun to utilize them in the challenging domain of detecting manipulated visual content. One of the obstacles to replicate the success of deep nets here is absence of diverse datasets tailored for training and testing of image forensic methods. Such datasets need to be designed to capture wide and complex types of systematic noise and intrinsic artifacts of images in order to avoid overfitting of learning methods to just a narrow set of camera types or types of manipulations. These artifacts are brought into visual content by various components of the image acquisition process as well as the manipulating process. In this paper, we introduce two novel datasets. First, we identified the majority of camera brands and models on the market, which resulted in 2,322 camera models. Then, we collected a dataset of 35,000 real images captured by these camera models. Moreover, we also created the same number of digitally manipulated images by using a large variety of core image manipulation methods as well we advanced ones such as GAN or Inpainting resulting in a dataset of 70,000 images. In addition to this dataset, we also created a dataset of 2,000 \"real-life\" (uncontrolled) manipulated images. They are made by unknown people and downloaded from Internet. The real versions of these images also have been found and are provided. We also manually created binary masks localizing the exact manipulated areas of these images. Both datasets are publicly available for the research community at http://staff.utia.cas.cz/novozada/db."
  },
  "wacv2020_w5_activitydetectioninuntrimmedvideosusingchunk-basedclassifiers": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Activity Detection in Untrimmed Videos Using Chunk-based Classifiers",
    "authors": [
      "Joshua Gleason",
      "Steven Schwarcz",
      "Rajeev Ranjan",
      "Carlos D. Castillo",
      "Jun-Cheng Chen",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Gleason_Activity_Detection_in_Untrimmed_Videos_Using_Chunk-based_Classifiers_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Gleason_Activity_Detection_in_Untrimmed_Videos_Using_Chunk-based_Classifiers_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Activity detection in untrimmed videos - the process of detecting and localizing human activities in potentially long videos - is a challenging problem in computer vision. We propose an algorithm which is based on the proposition that despite the differences between activity classification and detection, a strong classifier can still be used to achieve state-of-the-art performance in detection by breaking the video into multiple overlapping chunks and classifying each individually. We further introduce two new auxiliary tasks which we call chunk inclusion and localization. The outputs of these tasks, when carefully applied, can be used to dramatically improve performance. We call our method Chunk Aggregation. It is straight-forward to implement and use, and is agnostic to the backbone activity classification architecture used. We also demonstrate the effectiveness of chunk association by presenting results and a series of ablation experiments on the THUMOS'14 and ActEV datasets."
  },
  "wacv2020_w5_real-timedetectionofactivitiesinuntrimmedvideos": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Real-time Detection of Activities in Untrimmed Videos",
    "authors": [
      "Joshua Gleason",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Gleason_Real-time_Detection_of_Activities_in_Untrimmed_Videos_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Gleason_Real-time_Detection_of_Activities_in_Untrimmed_Videos_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Real-time detection of spatio-temporal sparse activities in untrimmed videos is a challenging problem. In this work, we present the details of our proposed solution. We begin with a slow baseline implementation of a previously state-of-the-art system and redesign it to achieve real-time performance for detecting 37 activities in the ActEV19 Sequestered Data Leaderboard. This is primarily achieved by introducing speed related hyperparameters into the baseline approach. A tradeoff analysis is performed to assist in hyperparameter selection which results in a real-time, high quality action detection system. Our system achieves an AUDC score of 0.476 on the ActEV19 Sequestered Data Leaderboard."
  },
  "wacv2020_w5_argusefficientactivitydetectionsystemforextendedvideoanalysis": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Argus: Efficient Activity Detection System for Extended Video Analysis",
    "authors": [
      "Wenhe Liu",
      "Guoliang Kang",
      "Po-Yao Huang",
      "Xiaojun Chang",
      "Yijun Qian",
      "Junwei Liang",
      "Liangke Gui",
      "Jing Wen",
      "Peng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Liu_Argus_Efficient_Activity_Detection_System_for_Extended_Video_Analysis_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Liu_Argus_Efficient_Activity_Detection_System_for_Extended_Video_Analysis_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " We propose an Efficient Activity Detection System, Argus, for Extended Video Analysis in the surveillance scenario. For the spatial-temporal event detection in the surveillance video, we first generate video proposals by applying object detection and tracking algorithm which shared the detection features. After that, we extract several different features and apply sequential activity classification with them. Finally, we eliminate inaccurate events and fuse all the predictions from different features. The proposed system wins Trecvid Activities in Extended Video (ActEV) challenge 2019. It achieves the first place with 60.5 mean weighted Pmiss, out-performing the second place system by 14.5 and the baseline R-C3D by 29.0. In TRECVID 2019 Challenge, the proposed system wins the first place with pAUDC@0.2tfa 0.48407"
  },
  "wacv2020_w5_contextsensitivityofspatio-temporalactivitydetectionusinghierarchicaldeepneuralnetworksinextendedvideos": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Context Sensitivity of Spatio-Temporal Activity Detection using Hierarchical Deep Neural Networks in Extended Videos",
    "authors": [
      "Felix Hertlein",
      "David Munch",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Hertlein_Context_Sensitivity_of_Spatio-Temporal_Activity_Detection_using_Hierarchical_Deep_Neural_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Hertlein_Context_Sensitivity_of_Spatio-Temporal_Activity_Detection_using_Hierarchical_Deep_Neural_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " The amount of available surveillance video data is increasing rapidly and therefore makes manual inspection impractical. The goal of activity detection is to automatically localize activities spatially and temporally in a large collection of video data. In this work we will answer the question to what extent context plays a role in spatio-temporal activity detection in extended videos. Towards this end we propose a hierarchical pipeline for activity detection which spatially localizes objects first and subsequently generates spatial-temporal action tubes. Additionally, a suitable metric for performance evaluation is enhanced. Thus, we evaluate our system using the TRECVID 2019 ActEV challenge dataset. We investigated the sensitivity by detecting activities multiple times with various spatial margins around the performing actor. The results showed that our pipeline and metric is suited for detecting activities in extended videos."
  },
  "wacv2020_w5_adaptivefeatureaggregationforvideoobjectdetection": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Adaptive Feature Aggregation for Video Object Detection",
    "authors": [
      "Yijun Qian",
      "Lijun Yu",
      "Wenhe Liu",
      "Guoliang Kang",
      "Alexander G. Hauptmann"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Qian_Adaptive_Feature_Aggregation_for_Video_Object_Detection_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Qian_Adaptive_Feature_Aggregation_for_Video_Object_Detection_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Object detection, as a fundamental research topic of computer vision, is facing the challenges of video-related tasks. Objects in videos tend to be blurred, occluded, or out of focus more frequently. Existing works adopt feature aggregation and enhancement to design video-based object detectors. However, most of them do not consider the diversity of object movements and the quality of aggregated context features. Thus, they can not generate comparable results given blurred or crowded videos. In this paper, we propose an adaptive feature aggregation method for video object detection to deal with these problems. We introduce an adaptive quality-similarity weight, with a sparse and dense temporal aggregation policy, into our model. Compared with both image-based and video-based baselines on ImageNet and VIRAT datasets, our work consistently demonstrates better performance. Especially, our model improves the average precision of person detection in VIRAT from 85.93 to 87.21."
  },
  "wacv2020_w5_summaryofthe2019activitydetectioninextendedvideos\nprizechallenge\n": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Summary of the 2019 Activity Detection in Extended Videos \nPrize Challenge\n",
    "authors": [
      "Yooyoung Lee",
      "Jon Fiscus",
      "Afzal Godil",
      "Andrew Delgado",
      "Jim Golden",
      "Lukas Diduch",
      "Maxime Hubert"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Lee_Summary_of_the_2019_Activity_Detection_in_Extended_Videos_Prize_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Lee_Summary_of_the_2019_Activity_Detection_in_Extended_Videos_Prize_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Despite previous data collection efforts and benchmark studies, progress in activity detection technologies has been slow, especially with applications that meet practical needs for the video analytics domain. In this paper, we discuss the results from the Activity detection in Extended Video Prize Challenge (ActEV-PC) that was sponsored by IARPA. The goal of the ActEV-PC was to promote robust automatic activity detection system development and to reduce the detection error rate. To examine the ability of activity detection systems in different aspects, we opened a competition to the public and ran evaluations (as a task under the ActivityNet workshop at CVPR 2019) with two different phases: an open leaderboard evaluation and a sequestered data evaluation. The Video and Image Retrieval and Analysis Tool (VIRAT) dataset was used for the open leaderboard evaluation while the Multiview Extended Video with Activities (MEVA) dataset was used for the sequestered data evaluation. Eighteen target activities were defined for detection. In this paper, we present results and findings from the two-phase ActEV-PC competition. Eighteen teams from academia and industry participated in the competitions and three top performers received a cash award (funded by IARPA). The winners were presented at the ActivityNet Workshop at CVPR 2019."
  },
  "wacv2020_w5_boostedkernelizedcorrelationfiltersforevent-basedfacedetection": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Boosted Kernelized Correlation Filters for Event-based Face Detection",
    "authors": [
      "Bharath Ramesh",
      "Hong Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Ramesh_Boosted_Kernelized_Correlation_Filters_for_Event-based_Face_Detection_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Ramesh_Boosted_Kernelized_Correlation_Filters_for_Event-based_Face_Detection_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Recently, deep learning has revolutionized the computer vision field and has resulted in steep advances in the performance of vision systems for human detection and classification on large datasets. Nevertheless, these systems rely on static cameras that do not yield practical results, especially for prolonged monitoring periods and when multiple object activities occur simultaneously. We propose that event cameras naturally solve these issues at the hardware level via asynchronous, pixel-level brightness sensing at microsecond time-scale. In particular, event cameras do not output data during no-activity periods and thus data rate is drastically lowered without any additional processing. Secondly, event cameras produce disjoint spatial outputs for multiple objects without requiring segmentation or explicit background modeling. Leveraging these attractive properties, this paper presents an event-based feature learning method using kernelized correlation filters (KCF) within a boosting framework. A key contribution is the reformulation of KCFs to learn the face representation instead of relying on handcrafted feature descriptors as done in previous works. We report a high detection performance on data collected using an event camera and showcase its potential for surveillance applications. For fostering further research, we release the face dataset used in our work to the wider community."
  },
  "wacv2020_w5_real-timeactivitydetectionofhumanmovementinvideosviasmartphonebasedonsynthetictrainingdata": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Real-Time Activity Detection of Human Movement in Videos via Smartphone Based on Synthetic Training Data",
    "authors": [
      "Rico Thomanek",
      "Tony Rolletschke",
      "Benny Platte",
      "Claudia Hosel",
      "Christian Roschke",
      "Robert Manthey",
      "Manuel Heinzig",
      "Richard Vogel",
      "Frank Zimmer",
      "Matthias Vodel",
      "Maximilian Eibl",
      "Marc Ritter"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/Thomanek_Real-Time_Activity_Detection_of_Human_Movement_in_Videos_via_Smartphone_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/Thomanek_Real-Time_Activity_Detection_of_Human_Movement_in_Videos_via_Smartphone_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Current research in the domain of video activity detection focuses on real-time activity detection. This includes multiple approaches in the mobile environment, such as the detection of correct motion sequences in the sports and health area or in safety-relevant environments. Current trends focus on the use of 3D CNNs. This work describes a approach to combine the results of a human skeleton point detector with an LSTM on mobile devices. Frameworks for pose detection are combined with LSTMs for activity detection with sensor data, optimized for the mobile area. The resulting system allows the direct detection of a person pose and the classification of activities in a video recorded with a smartphone. The successful application of the system in several field tests shows that the described approach works in principle and can be transferred to a resource-limited mobile environment by optimization."
  },
  "wacv2020_w5_exploringtechniquestoimproveactivityrecognitionusinghumanposeskeletons": {
    "conf_id": "WACV2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "WACV2020_workshops - Human Activity Detection in multi-camera, Continuous, long-duration Video",
    "title": "Exploring Techniques to Improve Activity Recognition using Human Pose Skeletons",
    "authors": [
      "Bharath Raj N.",
      "Anand Subramanian",
      "Kashyap Ravichandran",
      "Dr. N. Venkateswaran"
    ],
    "page_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/html/w5/N._Exploring_Techniques_to_Improve_Activity_Recognition_using_Human_Pose_Skeletons_WACVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/WACV2020_workshops/../content_WACVW_2020/papers/w5/N._Exploring_Techniques_to_Improve_Activity_Recognition_using_Human_Pose_Skeletons_WACVW_2020_paper.pdf",
    "published": "2020-03",
    "summary": " Human pose skeletons provide an explainable representation of the orientation of a person. Neural network architectures such as OpenPose can estimate the 2D human pose skeletons of people present in an image with good accuracy. Naturally, the human pose is a very attractive choice as a representation for building systems aimed at human activity recognition. However, raw pose keypoint representations suffer from various problems such as variance to translation and scale of the input images. Keypoints are also often missed by the pose estimation framework. These, and other factors lead to poor generalization and learning of networks that may be trained directly on these raw representations. This paper introduces various methods aimed at building a robust representation for training models related to activity recognition tasks, such as the usage of handcrafted features extracted from poses with the intent of introducing scale and translation invariance. Additionally, the usage of train-time techniques such as keypoint dropout are explored to facilitate better learning of models. Finally, we conduct an ablation study comparing the performance of deep learning models trained on raw keypoint representation and handcrafted features whilst incorporating our train-time techniques to quantify the effectiveness of our introduced methods over raw representations."
  }
}