{
  "cvpr2019_cv-cops_privacy-preservingactionrecognitionusingcodedaperturevideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Privacy-Preserving Action Recognition Using Coded Aperture Videos",
    "authors": [
      "Zihao W. Wang",
      "Vibhav Vineet",
      "Francesco Pittaluga",
      "Sudipta N. Sinha",
      "Oliver Cossairt",
      "Sing Bing Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Wang_Privacy-Preserving_Action_Recognition_Using_Coded_Aperture_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Wang_Privacy-Preserving_Action_Recognition_Using_Coded_Aperture_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The risk of unauthorized remote access of streaming video from networked cameras underlines the need for stronger privacy safeguards. We propose a lens-free coded aperture camera system for human action recognition that is privacy-preserving. While coded aperture systems exist, we believe ours is the first system designed for action recognition without the need for image restoration as an intermediate step. Action recognition is done using a deep network that takes in as input, non-invertible motion features between pairs of frames computed using phase correlation and log-polar transformation. Phase correlation encodes translation while the log polar transformation encodes in-plane rotation and scaling. We show that the translation features are independent of the coded aperture design, as long as its spectral response within the bandwidth has no zeros. Stacking motion features computed on frames at multiple different strides in the video can improve accuracy. Preliminary results on simulated data based on a subset of the UCF and NTU datasets are promising. We also describe our prototype lens-free coded aperture camera system, and results for real captured videos are mixed."
  },
  "cvpr2019_cv-cops_evadingfacerecognitionviapartialtamperingoffaces": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Evading Face Recognition via Partial Tampering of Faces",
    "authors": [
      "Puspita Majumdar",
      "Akshay Agarwal",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Majumdar_Evading_Face_Recognition_via_Partial_Tampering_of_Faces_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Majumdar_Evading_Face_Recognition_via_Partial_Tampering_of_Faces_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Advancements in machine learning and deep learning techniques have led to the development of sophisticated and accurate face recognition systems. However, for the past few years, researchers are exploring the vulnerabilities of these systems towards digital attacks. Creation of digitally altered images has become an easy task with the availability of various image editing tools and mobile application such as Snapchat. Morphing based digital attacks are used to elude and gain the identity of legitimate users by fooling the deep networks. In this research, partial face tampering attack is proposed, where facial regions are replaced or morphed to generate tampered samples. Face verification experiments performed using two state-of-the-art face recognition systems, VGG-Face and OpenFace on the CMU- MultiPIE dataset indicates the vulnerability of these systems towards the attack. Further, a Partial Face Tampering Detection (PFTD) network is proposed for the detection of the proposed attack. The network captures the inconsistencies among the original and tampered images by combining the raw and high-frequency information of the input images for the detection of tampered images. The proposed network surpasses the performance of the existing baseline deep neural networks for tampered image detection."
  },
  "cvpr2019_cv-cops_privacy-preservingannotationoffaceimagesthroughattribute-preservingfacesynthesis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Privacy-Preserving Annotation of Face Images Through Attribute-Preserving Face Synthesis",
    "authors": [
      "Sola Shirai",
      "Jacob Whitehill"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Shirai_Privacy-Preserving_Annotation_of_Face_Images_Through_Attribute-Preserving_Face_Synthesis_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Shirai_Privacy-Preserving_Annotation_of_Face_Images_Through_Attribute-Preserving_Face_Synthesis_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We investigate the viability of collecting annotations for face images while preserving privacy by using synthesized images as surrogates. We compare two approaches: a state-of-the-art 3-D face model based on deep neural networks (Extreme3D) to render a detailed 3-D reconstruction of the face from an input image; and a novel generative adversarial network architecture that we propose that extends BEGAN-CS to generate images conditioned on desired low-level facial attributes. Using these two alternative models, we conduct experiments on Mechanical Turk to annotate emotions (\"joy\" and \"anger\") on raw and synthesized versions of face images. Across 60 workers each annotating 3 versions of 60 images in each experiment, we find that: (1) The labeling accuracy when viewing surrogate images can be very similar to the accuracy when viewing raw images, but depends significantly on the labeling task. (2) The proposed extension to BEGAN-CS is effective in generating realistic images that correspond to the input vector of low-level facial attributes. (3) Overall, the GAN-based approach to generating surrogate images gives comparable accuracy as the 3-D face model, but is easier to train."
  },
  "cvpr2019_cv-cops_rru-nettheringedresidualu-netforimagesplicingforgerydetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "RRU-Net: The Ringed Residual U-Net for Image Splicing Forgery Detection",
    "authors": [
      "Xiuli Bi",
      "Yang Wei",
      "Bin Xiao",
      "Weisheng Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Bi_RRU-Net_The_Ringed_Residual_U-Net_for_Image_Splicing_Forgery_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Bi_RRU-Net_The_Ringed_Residual_U-Net_for_Image_Splicing_Forgery_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Detecting a splicing forgery image and then locating the forgery regions is a challenging task. Some traditional feature extraction methods and convolutional neural network (CNN)-based detection methods have been proposed to finish this task by exploring the differences of image attributes between the un-tampered and tampered regions in a image. However, the performance of the existing detection methods is unsatisfactory. In this paper, we propose a ringed residual U-Net (RRU-Net) for image splicing forgery detection. The proposed RRU-Net is an end-to-end image essence attribute segmentation network, which is independent of human visual system, it can accomplish the forgery detection without any preprocessing and post-processing. The core idea of the proposed RRU-Net is to strengthen the learning way of CNN, which is inspired by the recall and the consolidation mechanism of the human brain and implemented by the propagation and the feedback process of the residual in CNN. The residual propagation recalls the input feature information to solve the gradient degradation problem in the deeper network; the residual feedback consolidates the input feature information to make the differences of image attributes between the un-tampered and tampered regions be more obvious. Experimental results show that the proposed detection method can achieve a promising result compared with the state-of-the-art splicing forgery detection methods."
  },
  "cvpr2019_cv-cops_towardsdeepneuralnetworktrainingonencrypteddata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Towards Deep Neural Network Training on Encrypted Data",
    "authors": [
      "Karthik Nandakumar",
      "Nalini Ratha",
      "Sharath Pankanti",
      "Shai Halevi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Nandakumar_Towards_Deep_Neural_Network_Training_on_Encrypted_Data_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Nandakumar_Towards_Deep_Neural_Network_Training_on_Encrypted_Data_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " While deep learning is a valuable tool for solving many tough problems in computer vision, the success of deep learning models is typically determined by: (i) availability of sufficient training data, (ii) access to extensive computational resources, and (iii) expertise in selecting the right model and hyperparameters for the selected task. Often, the availability of data is the hard part due to compliance, legal, and privacy constraints. Cryptographic techniques such as fully homomorphic encryption (FHE) offer a potential solution by enabling processing on encrypted data. While prior work has been done on using FHE for inferencing, training a deep neural network in the encrypted domain is an extremely challenging task due to the computational complexity of the operations involved. In this paper, we evaluate the feasibility of training neural networks on encrypted data in a completely non-interactive way. Our proposed system uses the open-source FHE toolkit HElib to implement a Stochastic Gradient Descent (SGD)-based training of a neural network. We show that encrypted training can be made more computationally efficient by (i) simplifying the network with minimal degradation of accuracy, (ii) choosing appropriate data representation and resolution, and (iii) packing the data elements within the ciphertext in a smart way so as to minimize the number of operations and facilitate parallelization of FHE computations. Based on the above optimizations, we demonstrate that it is possible to achieve more than 50x speed up while training a fully-connected neural network on the MNIST dataset while achieving reasonable accuracy (96%). Though the cost of training a complex deep learning model from scratch on encrypted data is still very high, this work establishes a solid baseline and paves the way for relatively simpler tasks such as fine-tuning of deep learning models based on encrypted data to be implemented in the near future."
  },
  "cvpr2019_cv-cops_foolingautomatedsurveillancecamerasadversarialpatchestoattackpersondetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Fooling Automated Surveillance Cameras: Adversarial Patches to Attack Person Detection",
    "authors": [
      "Simen Thys",
      "Wiebe Van Ranst",
      "Toon Goedeme"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Thys_Fooling_Automated_Surveillance_Cameras_Adversarial_Patches_to_Attack_Person_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Thys_Fooling_Automated_Surveillance_Cameras_Adversarial_Patches_to_Attack_Person_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Adversarial attacks on machine learning models have seen increasing interest in the past years. By making onlysubtle changes to the input of a convolutional neural network, the output of the network can be swayed to output a completely different result.The first attacks did this by changing pixel values of an input image slightly to fool a classifier to output the wrong class. Other approaches have tried to learn \"patches\" that can be applied to an object to fool detectors and classifiers. Some of these approaches have also shown that these attacks are feasible in the real-world, i.e. by modifying an object and filming it with a video camera. However, all of these approaches target classes that contain almost no intra-class variety (e.g. stop signs). The known structure of the object is then used to generate an adversarial patch on top of it.In this paper, we present an approach to generate adversarial patches to targets with lots of intra-class variety, namely persons. The goal is to generate a patch that is able successfullyhide a person from a person detector. An attack that could for instance be used maliciously to circumvent surveillance systems, intruders can sneak around undetected by holding a small cardboard plate in front of their body aimed towards the surveilance camera.From our results we can see that our system is able significantly lower the accuracy of a person detector. Our approach also functions well in real-life scenarios where the patch is filmed by a camera. To the best of our knowledge we are the first to attempt this kind of attack on targets with a high level of intra-class variety like persons."
  },
  "cvpr2019_cv-cops_anonymousnetnaturalfacede-identificationwithmeasurableprivacy": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "AnonymousNet: Natural Face De-Identification With Measurable Privacy",
    "authors": [
      "Tao Li",
      "Lei Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Li_AnonymousNet_Natural_Face_De-Identification_With_Measurable_Privacy_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Li_AnonymousNet_Natural_Face_De-Identification_With_Measurable_Privacy_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With billions of personal images being generated from social media and cameras of all sorts on a daily basis, security and privacy are unprecedentedly challenged. Although extensive attempts have been made, existing face image de-identification techniques are either insufficient in photo-reality or incapable of balancing privacy and usability qualitatively and quantitatively, i.e., they fail to answer counterfactual questions such as \"is it private now?\", \"how private is it?\", and \"can it be more private?\" In this paper, we propose a novel framework called AnonymousNet, with an effort to address these issues systematically, balance usability, and enhance privacy in a natural and measurable manner. The framework encompasses four stages: facial attribute estimation, privacy-metric-oriented face obfuscation, directed natural image synthesis, and adversarial perturbation. Not only do we achieve the state-of-the-arts in terms of image quality and attribute prediction accuracy, we are also the first to show that facial privacy is measurable, can be factorized, and accordingly be manipulated in a photo-realistic fashion to fulfill different requirements and application scenarios. Experiments further demonstrate the effectiveness of the proposed framework."
  },
  "cvpr2019_cv-cops_regularizertomitigategradientmaskingeffectduringsingle-stepadversarialtraining": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Regularizer to Mitigate Gradient Masking Effect During Single-Step Adversarial Training",
    "authors": [
      "Vivek B S",
      "Arya Baburaj",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/S_Regularizer_to_Mitigate_Gradient_Masking_Effect_During_Single-Step_Adversarial_Training_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/S_Regularizer_to_Mitigate_Gradient_Masking_Effect_During_Single-Step_Adversarial_Training_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Neural networks are susceptible to adversarial samples: samples with imperceptible noise, crafted to manipulate network's prediction. In order to learn robust models, a training procedure, called Adversarial Training has been introduced. During adversarial training, models are trained with mini-batch containing adversarial samples. In order to scale adversarial training for large datasets and networks, fast and simple methods (e.g., FGSM:Fast Gradient Sign Method) of generating adversarial samples are used while training. It has been shown that models trained usingsingle-step adversarial training methods (i.e., adversarial samples generated using non-iterative methods such as FGSM) are not robust, instead they learn to generate weaker adversaries by masking the gradients. In this work, we propose a regularization term in the training loss, to mitigate the effect of gradient masking during single-step adversarial training. The proposed regularization term causes training loss to increase when the distance between logits (i.e., pre-softmax output of a classifier) for FGSM and R-FGSM (small random noise is added to the clean sample before computing its FGSM sample) adversaries of a clean sample becomes large. The proposed single-step adversarial training is faster than computationally expensive state-of-the-art PGD adversarial training method, and also achieves on par results. "
  },
  "cvpr2019_cv-cops_privacypreservinggroupmembershipverificationandidentification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Privacy Preserving Group Membership Verification and Identification",
    "authors": [
      "Marzieh Gheisari",
      "Teddy Furon",
      "Laurent Amsaleg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Gheisari_Privacy_Preserving_Group_Membership_Verification_and_Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Gheisari_Privacy_Preserving_Group_Membership_Verification_and_Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " When convoking privacy, group membership verification checks if a biometric trait corresponds to one member of a group without revealing the identity of that member. Similarly, group membership identification states which group the individual belongs to, without knowing his/her identity. A recent contribution provides privacy and security for group membership protocols through the joint use of two mechanisms: quantizing biometric templates into discrete embeddings and aggregating several templates into one group representation. This paper significantly improves that contribution because it jointly learns how to embed and aggregate instead of imposing fixed and hard-coded rules. This is demonstrated by exposing the mathematical underpinnings of the learning stage before showing the improvements through an extensive series of experiments targeting face recognition. Overall, experiments show that learning yields an excellent trade-off between security / privacy and the verification / identification performances."
  },
  "cvpr2019_cv-cops_bag-of-liesamultimodaldatasetfordeceptiondetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Bag-Of-Lies: A Multimodal Dataset for Deception Detection",
    "authors": [
      "Viresh Gupta",
      "Mohit Agarwal",
      "Manik Arora",
      "Tanmoy Chakraborty",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Gupta_Bag-Of-Lies_A_Multimodal_Dataset_for_Deception_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Gupta_Bag-Of-Lies_A_Multimodal_Dataset_for_Deception_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deception detection is a pervasive issue in security. It has been widely studied using traditional modalities, such as video, audio and transcripts; however, there has been a lack of investigation in using modalities such as EEG and Gaze data due to the scarcity of a publicly available dataset. In this paper, a new multimodal dataset is presented, which provides data for deception detection by the aid of various modalities, such as video, audio, EEG and gaze data. The dataset explores the cognitive aspect of deception and combines it with vision. The presented dataset is collected in a realistic scenario and has 35 unique subjects providing 325 annotated data points with an even distribution of truth (163) and lie (162).The benefits provided by incorporating multiple modalities for fusion on the proposed dataset is also investigated. It is our assertion that the availability of this dataset will facilitate the development of better deception detection algorithms which are more relevant to real world scenarios."
  },
  "cvpr2019_cv-cops_droppingpixelsforadversarialrobustness": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Dropping Pixels for Adversarial Robustness",
    "authors": [
      "Hossein Hosseini",
      "Sreeram Kannan",
      "Radha Poovendran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Hosseini_Dropping_Pixels_for_Adversarial_Robustness_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Hosseini_Dropping_Pixels_for_Adversarial_Robustness_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep neural networks are vulnerable against adversarial examples. In this paper, we propose to train and test the networks with randomly subsampled images with high drop rates. We show that this approach significantly improves robustness against adversarial examples in all cases of bounded L0, L2 and L infinityperturbations, while reducing the standard accuracy by a small value. We argue that subsampling pixels can be thought to provide a set of robust features for the input image and, thus, improves robustness without performing adversarial training."
  },
  "cvpr2019_cv-cops_dp-cgandifferentiallyprivatesyntheticdataandlabelgeneration": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "DP-CGAN: Differentially Private Synthetic Data and Label Generation",
    "authors": [
      "Reihaneh Torkzadehmahani",
      "Peter Kairouz",
      "Benedict Paten"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Torkzadehmahani_DP-CGAN_Differentially_Private_Synthetic_Data_and_Label_Generation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Torkzadehmahani_DP-CGAN_Differentially_Private_Synthetic_Data_and_Label_Generation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Generative Adversarial Networks (GANs) are one of the well-known models to generate synthetic data including images, especially for research communities that cannot use original sensitive datasets because they are not publicly accessible. One of the main challenges in this area is to preserve the privacy of individuals who participate in the training of the GAN models. To address this challenge, we introduce a Differentially Private Conditional GAN (DP-CGAN) training framework based on a new clipping and perturbation strategy, which improves the performance of the model while preserving privacy of the training dataset. DP-CGAN generates both synthetic data and corresponding labels and leverages the recently introduced Renyi differential privacy accountant to track the spent privacy budget. The experimental results show that DP-CGAN can generate visually and empirically promising results on the MNIST dataset with a single-digit epsilon parameter in differential privacy."
  },
  "cvpr2019_cv-cops_defendingagainstadversarialattacksusingrandomforest": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CV-COPS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Defending Against Adversarial Attacks Using Random Forest",
    "authors": [
      "Yifan Ding",
      "Liqiang Wang",
      "Huan Zhang",
      "Jinfeng Yi",
      "Deliang Fan",
      "Boqing Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CV-COPS/Ding_Defending_Against_Adversarial_Attacks_Using_Random_Forest_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CV-COPS/Ding_Defending_Against_Adversarial_Attacks_Using_Random_Forest_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " As deep neural networks (DNNs) have become increasingly important and popular, the robustness of DNNs is the key to the safety of both the Internet and physical world. Unfortunately, some recent studies show thatadversarial examples, which are hard to be distinguished from real examples, can easily fool DNNs and manipulate their predictions. Upon observing that adversarial examples are mostly generated by gradient-based methods, in this paper, we first propose to use a simple yet very effective non-differentiable hybrid model that combines DNNs and random forests, rather than hide gradients from attackers, to defend against the attacks. Our experiments show that our model can successfully and completely defend the white-box attacks, has a lower transferability, and is quite resistant to three representative types of black-box attacks; while at the same time, our model achieves similar classification accuracy as the original DNNs. Finally, we investigate and suggest a criterion to define where to grow random forests in DNNs."
  },
  "cvpr2019_vocvalc_cemnetself-supervisedlearningforaccuratecontinuousego-motionestimation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3rd International Workshop on Visual Odometry & Computer Vision Applications Based on Location Clues",
    "title": "CeMNet: Self-Supervised Learning for Accurate Continuous Ego-Motion Estimation",
    "authors": [
      "Minhaeng Lee",
      "Charless C. Fowlkes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/VOCVALC/Lee_CeMNet_Self-Supervised_Learning_for_Accurate_Continuous_Ego-Motion_Estimation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/VOCVALC/Lee_CeMNet_Self-Supervised_Learning_for_Accurate_Continuous_Ego-Motion_Estimation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose a novel self-supervised learning model for estimating continuous ego-motion from video. Our model learns to estimate camera motion by watching RGBD or RGB video streams and determining translational and rotation velocities that correctly predict the appearance of future frames.Our approach differs from other recent work on self-supervised structure-from-motion in its use of a continuous motion formulation and representation of rigid motion fields rather than direct prediction of camera parameters. To make estimation robust in dynamic environments with multiple moving objects, we introduce a simple two-component segmentation process that isolates the rigid background environment from dynamic scene elements. We demonstrate state-of-the-art accuracy of the self-trained model on several benchmark ego-motion datasets and highlight the ability of the model to provide superior rotational accuracy and handling of non-rigid scene motions."
  },
  "cvpr2019_vocvalc_motionanddepthaugmentedsemanticsegmentationforautonomousnavigation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3rd International Workshop on Visual Odometry & Computer Vision Applications Based on Location Clues",
    "title": "Motion and Depth Augmented Semantic Segmentation for Autonomous Navigation",
    "authors": [
      "Hazem Rashed",
      "Ahmad El Sallab",
      "Senthil Yogamani",
      "Mohamed ElHelw"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/VOCVALC/Rashed_Motion_and_Depth_Augmented_Semantic_Segmentation_for_Autonomous_Navigation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/VOCVALC/Rashed_Motion_and_Depth_Augmented_Semantic_Segmentation_for_Autonomous_Navigation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Motion and depth provide critical information in autonomous driving and they are commonly used for generic object detection. In this paper, we leverage them for improving semantic segmentation. Depth cues can be useful for detecting road as it lies below the horizon line. There is also a strong structural similarity for different instances of different objects including buildings and trees. Motion cues are useful as the scene is highly dynamic with moving objects including vehicles and pedestrians. This work utilizes geometric information modelled by depth maps and motion cues represented by optical flow vectors to improve the pixel-wise segmentation task. A CNN architecture is proposed and the variations regarding the stage at which color, depth, and motion information are fused, e.g. early-fusion or mid-fusion, are systematically investigated. Additionally, we implement a multimodal fusion algorithm to maximize the benefit from all the information. The proposed algorithms are evaluated on Virtual-KITTI and Cityscapes datasets where results demonstrate enhanced performance with depth and flow augmentation."
  },
  "cvpr2019_vocvalc_visual-gpsego-downwardandambientvideobasedpersonlocationassociation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3rd International Workshop on Visual Odometry & Computer Vision Applications Based on Location Clues",
    "title": "Visual-GPS: Ego-Downward and Ambient Video Based Person Location Association",
    "authors": [
      "Liang Yang",
      "Hao Jiang",
      "Zhouyuan Huo",
      "Jizhong Xiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/VOCVALC/Yang_Visual-GPS_Ego-Downward_and_Ambient_Video_Based_Person_Location_Association_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/VOCVALC/Yang_Visual-GPS_Ego-Downward_and_Ambient_Video_Based_Person_Location_Association_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In a crowded and cluttered environment, identifying a particular person is a challenging problem. Current identification approaches are not able to handle the dynamic environment. In this paper, we tackle the problem of identifying and tracking a person of interest in the crowded environment using egocentric and third person view videos. We propose a novel method (Visual-GPS) to identify, track, and localize the person, who is capturing the egocentric video, using joint analysis of imagery from both videos. The output of our method is the bounding box of the target person detected in each frame of the third person view and the 3D metric trajectory. At glance, the views of the two cameras are quite different. This paper illustrates an insight into how they are correlated. Our proposed method uses several difference clues. In addition to using RGB images, we take advantage of both the body motion and action features to correlate the two views. We can track and localize the person by finding the most \"correlated\" individual in the third view. Furthermore, the target person's 3D trajectory is recovered based on the mapping of the 2d-3D body joints. Our experiment confirms the effectiveness of ETVIT network and shows 18.32 % improvement in detection accuracy against the baseline methods."
  },
  "cvpr2019_vocvalc_unsupervisedmonoculardepthandego-motionlearningwithstructureandsemantics": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3rd International Workshop on Visual Odometry & Computer Vision Applications Based on Location Clues",
    "title": "Unsupervised Monocular Depth and Ego-Motion Learning With Structure and Semantics",
    "authors": [
      "Vincent Casser",
      "Soeren Pirk",
      "Reza Mahjourian",
      "Anelia Angelova"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/VOCVALC/Casser_Unsupervised_Monocular_Depth_and_Ego-Motion_Learning_With_Structure_and_Semantics_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/VOCVALC/Casser_Unsupervised_Monocular_Depth_and_Ego-Motion_Learning_With_Structure_and_Semantics_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present an approach which takes advantage of both structure and semantics for unsupervised monocular learning of depth and ego-motion. More specifically we model the motions of individual objects and learn their 3D motion vector jointly with depth and ego-motion. We obtain more accurate results, especially for challenging dynamic scenes not addressed by previous approaches. This is an extended version of Casser et al. Code and models have been open sourced at: https://sites.google.com/corp/view/struct2depth. "
  },
  "cvpr2019_precognition_leveragingthepresenttoanticipatethefutureinvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Leveraging the Present to Anticipate the Future in Videos",
    "authors": [
      "Antoine Miech",
      "Ivan Laptev",
      "Josef Sivic",
      "Heng Wang",
      "Lorenzo Torresani",
      "Du Tran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Miech_Leveraging_the_Present_to_Anticipate_the_Future_in_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Miech_Leveraging_the_Present_to_Anticipate_the_Future_in_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Anticipating actions before they are executed is crucial for a wide range of practical applications including autonomous driving and the moderation of live video streaming. While most prior work in this area requires partial observation of executed actions, in the paper we focus on anticipating actions seconds before they start. Our proposed approach is the fusion of a purely anticipatory model with a complementary model constrained to reason about the present. In particular, the latter predicts present action and scene attributes, and reasons about how they evolve over time. By doing so, we aim at modeling action anticipation at a more conceptual level than directly predicting future actions. Our model outperforms previously reported methods on the EPIC-KITCHENS and Breakfast datasets."
  },
  "cvpr2019_precognition_predictingthewhatandhow-aprobabilisticsemi-supervisedapproachtomulti-taskhumanactivitymodeling": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Predicting the What and How - a Probabilistic Semi-Supervised Approach to Multi-Task Human Activity Modeling",
    "authors": [
      "Judith Butepage",
      "Hedvig Kjellstrom",
      "Danica Kragic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Butepage_Predicting_the_What_and_How_-_a_Probabilistic_Semi-Supervised_Approach_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Butepage_Predicting_the_What_and_How_-_a_Probabilistic_Semi-Supervised_Approach_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Video-based prediction of human activity is usually performed on one of two levels: either a model is trained to anticipate high-level action labels or it is trained to predict future trajectories either in skeletal joint space or in image pixel space. This separation of classification and regression tasks implies that models cannot make use of the mutual information between continuous and semantic observations. However, if a model knew that an observed human wants to drink from a nearby glass, the space of possible trajectories would be highly constrained to reaching movements. Likewise, if a model had predicted a reaching trajectory, the inference of future semantic labels would rank \"lifting\" more likely than \"walking\". In this work, we propose a semi-supervised generative latent variable model that addresses both of these levels by modeling continuous observations as well as semantic labels. This fusion of signals allows the model to solve several tasks, such as action detection and anticipation as well as motion prediction and synthesis, simultaneously. We demonstrate this ability on the UTKinect-Action3D dataset, which consists of noisy, partially labeled multi-action sequences. The aim of this work is to encourage research within the field of human activity modeling based on mixed categorical and continuous data. "
  },
  "cvpr2019_precognition_multimodal2dand3dforin-the-wildfacialexpressionrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Multimodal 2D and 3D for In-The-Wild Facial Expression Recognition",
    "authors": [
      "Son Thai Ly",
      "Nhu-Tai Do",
      "Guee-Sang Lee",
      "Soo-Hyung Kim",
      "Hyung-Jeong Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Ly_Multimodal_2D_and_3D_for_In-The-Wild_Facial_Expression_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Ly_Multimodal_2D_and_3D_for_In-The-Wild_Facial_Expression_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, unlike other in-the-wild facial expression recognition (FER) studies which only focused on 2D information, we present a fusion approach for 2D and 3D facial data in FER. In particular, the 3D facial data are first reconstructed from image datasets. The 3D information are then extracted by deep learning technique that could exploit the meaningful facial geometry details for expression. We further demonstrate the potential of using 3D facial data by taking the 2D projected images of 3D face as an additional input for FER. These features are fused with that of 2D features from a typical network. Following the experiment procedure in recent studies, the concatenated features are classified by linear support vector machines (SVMs). Comprehensive experiments are further conducted on integrating facial features for expression prediction. The results show that the proposed method achieves state-of-the-art recognition performances on both RAF database and SFEW 2.0 database. This is the first time such a deep learning combination of 3D and 2D facial modalities is presented in the context of in-the-wild FER."
  },
  "cvpr2019_precognition_futureeventpredictionifandwhen": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Future Event Prediction: If and When",
    "authors": [
      "Lukas Neumann",
      "Andrew Zisserman",
      "Andrea Vedaldi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Neumann_Future_Event_Prediction_If_and_When_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Neumann_Future_Event_Prediction_If_and_When_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We consider the problem of future event prediction in video: ifand when a future event will occur. To this end, we propose a number of representations and loss functions tailored to this problem. These includeseveral probabilistic formulations that also model the uncertainty of the prediction. We train and evaluate theapproach on two entirely different prediction scenarios: if and when a car will stop in theBDD100k car driving dataset; and if and whena player is going to shoot a basketball towards the basket in the NCAA basketball dataset.We show that (i) we are able to predict events far in the future, up to 10 seconds before they occur; and (ii) using attention, we can determine which areas of the image sequence are responsible for these predictions, and find that they are meaningful, e.g. traffic lights are picked out for predicting when a vehicle will stop."
  },
  "cvpr2019_precognition_robustaleatoricmodelingforfuturevehiclelocalization": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Robust Aleatoric Modeling for Future Vehicle Localization",
    "authors": [
      "Max Hudnell",
      "True Price",
      "Jan-Michael Frahm"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Hudnell_Robust_Aleatoric_Modeling_for_Future_Vehicle_Localization_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Hudnell_Robust_Aleatoric_Modeling_for_Future_Vehicle_Localization_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The task of 2D object localization prediction, or the estimation of an object's future location and scale in an image, is a developing area of computer vision research. An accurate prediction of an object's future localization has the potential for drastically improving critical decision making systems. In particular, an autonomous driving system's collision prevention system could make better-informed decisions in the presence of accurate localization predictions for nearby objects (i.e. cars, pedestrians, and hazardous obstacles). Improving the accuracy of such localization systems is crucial to passenger / pedestrian safety. This paper presents a novel technique for determining future bounding boxes, representing the size and location of objects -- and the predictive uncertainty of both aspects -- in a transit setting. We present a simple feed-forward network for robust prediction as a solution of this task, which is able to generate object locality proposals by making use of an object's previous locality information. We evaluate our method against a number of related approaches and demonstrate its benefits for vehicle localization, and different from previous works, we propose to use distribution-based metrics to truly measure the predictive efficiency of the network-regressed uncertainty models."
  },
  "cvpr2019_precognition_learningtoinferrelationsforfuturetrajectoryforecast": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Learning to Infer Relations for Future Trajectory Forecast",
    "authors": [
      "Chiho Choi",
      "Behzad Dariush"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Choi_Learning_to_Infer_Relations_for_Future_Trajectory_Forecast_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Choi_Learning_to_Infer_Relations_for_Future_Trajectory_Forecast_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Inferring relational behavior between road users as well as road users and their surrounding physical space is an important step toward effective modeling and prediction of navigation strategies adopted by participants in road scenes. To this end, we propose a relation-aware framework for future trajectory forecast, which aims to infer relational information from the interactions of road users with each other and with environments. Extensive evaluations on a public benchmark dataset demonstrate the robustness and efficacy of the proposed framework as observed by performances higher than the state-of-the-art methods."
  },
  "cvpr2019_precognition_anticipationofhumanactionswithpose-basedfine-grainedrepresentations": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Anticipation of Human Actions With Pose-Based Fine-Grained Representations",
    "authors": [
      "Sebastian Agethen",
      "Hu-Cheng Lee",
      "Winston H. Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Agethen_Anticipation_of_Human_Actions_With_Pose-Based_Fine-Grained_Representations_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Agethen_Anticipation_of_Human_Actions_With_Pose-Based_Fine-Grained_Representations_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Anticipating an action that is about to happen allows us to be more efficient in interacting with our environment. However, prediction is a challenging task in computer vision, because videos are only partially available when a decision is to be made. Complicating the issue is that it is not always clear which of the visible activities in the scene are relevant to the action, and which ones are not. We suggest that the key to recognizing an action lies with the human actors, and that it is therefore necessary for the prediction process to attend to persons in a scene. In our work, we extract fine-grained features on visible human actors and predict the future via an L2-regression in feature space. This allows the regressed future feature to focus on the actor. Using this, the future action is classified. More specifically, the fine-grained extraction is guided by a pose prediction system that models current and future human poses in the scene. We run qualitative and quantitative experiments on the Charades dataset, and initial results show that our system improves action prediction."
  },
  "cvpr2019_precognition_peekingintothefuturepredictingfuturepersonactivitiesandlocationsinvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Peeking Into the Future: Predicting Future Person Activities and Locations in Videos",
    "authors": [
      "Junwei Liang",
      "Lu Jiang",
      "Juan Carlos Niebles",
      "Alexander Hauptmann",
      "Li Fei-Fei"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Liang_Peeking_Into_the_Future_Predicting_Future_Person_Activities_and_Locations_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Liang_Peeking_Into_the_Future_Predicting_Future_Person_Activities_and_Locations_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in many applications. Motivated by this idea, this paper studies predicting a pedestrian's future path jointly with future activities. We propose an end-to-end, multi-task learning system utilizing rich visual features about human behavioral information and interaction with their surroundings. To facilitate the training, the network is learned with an auxiliary task of predicting future location in which the activity will happen. Experimental results demonstrate our state-of-the-art performance over two public benchmarks on future trajectory prediction. Moreover, our method is able to produce meaningful future activity prediction in addition to the path. The result provides the first empirical evidence that joint modeling of paths and activities benefits future path prediction."
  },
  "cvpr2019_precognition_socialwayslearningmulti-modaldistributionsofpedestriantrajectorieswithgans": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories With GANs",
    "authors": [
      "Javad Amirian",
      "Jean-Bernard Hayet",
      "Julien Pettre"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Amirian_Social_Ways_Learning_Multi-Modal_Distributions_of_Pedestrian_Trajectories_With_GANs_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Amirian_Social_Ways_Learning_Multi-Modal_Distributions_of_Pedestrian_Trajectories_With_GANs_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper proposes a novel approach for predicting the motion of pedestrians interacting with others. It uses a Generative Adversarial Network (GAN) to sample plausible predictions for any agent in the scene. As GANs are very susceptible to mode collapsing and dropping, we show that the recently proposed Info-GAN allows dramatic improvements in multi-modal pedestrian trajectory prediction to avoid these issues. We also left out L2-loss in training the generator, unlike some previous works, because it causes serious mode collapsing though faster convergence. We show through experiments on real and synthetic data that the proposed method leads to generate more diverse samples and to preserve the modes of the predictive distribution. In particular, to prove this claim, we have designed a toy example dataset of trajectories that can be used to assess the performance of different methods in preserving the predictive distribution modes."
  },
  "cvpr2019_precognition_supertmltwo-dimensionalwordembeddingfortheprecognitiononstructuredtabulardata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Precognition: Seeing through the Future",
    "title": "SuperTML: Two-Dimensional Word Embedding for the Precognition on Structured Tabular Data",
    "authors": [
      "Baohua Sun",
      "Lin Yang",
      "Wenhan Zhang",
      "Michael Lin",
      "Patrick Dong",
      "Charles Young",
      "Jason Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Precognition/Sun_SuperTML_Two-Dimensional_Word_Embedding_for_the_Precognition_on_Structured_Tabular_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Precognition/Sun_SuperTML_Two-Dimensional_Word_Embedding_for_the_Precognition_on_Structured_Tabular_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Tabular data is the most commonly used form of data in industry according to a Kaggle ML and DS Survey. Gradient Boosting Trees, Support Vector Machine, Random Forest, and Logistic Regression are typically used for classification tasks on tabular data. DNN models using categorical embeddings are also applied in this task, but all attempts thus far have used one-dimensional embeddings. The recent work of Super Characters method using two-dimensional word embeddings achieved state-of-the-art results in text classification tasks, showcasing the promise of this new approach. In this paper, we propose the SuperTML method, which borrows the idea of Super Characters method and two-dimensional embeddings to address the problem of classification on tabular data. For each input of tabular data, the features are first projected into two-dimensional embeddings like an image, and then this image is fed into fine-tuned two-dimensional CNN models for classification. The proposed SuperTML method handles the categorical data and missing values in tabular data automatically, without any need to pre-process into numerical values. Comparisons of model performance are conducted on one of the largest and most active competitions on the Kaggle platform, as well as on the top three most popular data sets in the UCI Machine Learning Repository. Experimental results have shown that the proposed SuperTML method have achieved state-of-the-art results on both large and small datasets. "
  },
  "cvpr2019_mula_cutqualityestimationinindustriallasercuttingmachinesamachinelearningapproach": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "Cut Quality Estimation in Industrial Laser Cutting Machines: A Machine Learning Approach",
    "authors": [
      "Giorgio Santolini",
      "Paolo Rota",
      "Davide Gandolfi",
      "Paolo Bosetti"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Santolini_Cut_Quality_Estimation_in_Industrial_Laser_Cutting_Machines_A_Machine_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Santolini_Cut_Quality_Estimation_in_Industrial_Laser_Cutting_Machines_A_Machine_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The use of machine learning models to improve industrial production quality is becoming more popular year after year. The main reason is the huge data availability and the impressive boost of performance of such methods achieved in the last decade. In this work we propose an adaptation of three well known machine learning algorithms to estimate the quality of cut in industrial laser cutting machines. The challenge here is to use a pool of multimodal parameters coming from different sensors and fuse them in order to detect the cutting status of the machine in a near-online modality. We analyze then generative and discriminative approaches based on Gaussian Mixture Models, Recurrent Neural Networks, and Convolutional Neural Networks in a supervised setting. Results are computed on a brand-new dataset that is freely available for reference."
  },
  "cvpr2019_mula_alarge-scaleattributedatasetforzero-shotlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "A Large-Scale Attribute Dataset for Zero-Shot Learning",
    "authors": [
      "Bo Zhao",
      "Yanwei Fu",
      "Rui Liang",
      "Jiahong Wu",
      "Yonggang Wang",
      "Yizhou Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Zhao_A_Large-Scale_Attribute_Dataset_for_Zero-Shot_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Zhao_A_Large-Scale_Attribute_Dataset_for_Zero-Shot_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Zero-Shot Learning (ZSL) has attracted huge research attention over the past few years; it aims to learn the new concepts that have never been seen before. Each concept (class) is embedded in two or more modalities, e.g., the image features and semantic embeddings. Attributes are introduced as the intermediate semantic representation to realize the knowledge transfer from seen to unseen classes. Previous ZSL algorithms are tested on several benchmark datasets, which are defective in terms of the image distribution and attribute diversity. In addition, we argue that the \"co-occurrence bias problem\" of existing datasets, which is caused by the biased co-occurrence of objects, significantly hinders models from correctly learning the concept. To overcome these problems, we propose a Large-scale Attribute Dataset (LAD) with 78,017 images of 230 classes. 359 attributes of visual, semantic and subjective properties are defined and annotated in instance-level. Seven state-of-the-art ZSL algorithms are tested on this new dataset. The experimental results reveal the challenge of implementing ZSL on our dataset. Based on the proposed dataset, Zero-shot Learning Competition of AI Challenger (>110 teams attended) has been organized for promoting ZSL research."
  },
  "cvpr2019_mula_learningcommonrepresentationfromrgbanddepthimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "Learning Common Representation From RGB and Depth Images",
    "authors": [
      "Giorgio Giannone",
      "Boris Chidlovskii"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Giannone_Learning_Common_Representation_From_RGB_and_Depth_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Giannone_Learning_Common_Representation_From_RGB_and_Depth_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a new deep learning architecture for the tasks of semantic segmentation and depth prediction from RGB-D images. We revise the state of art based on the RGB and depth feature fusion, where both modalities are assumed to be available at train and test time. We propose a new architecture where the feature fusion is replaced with a common deep representation. Combined with an encoder-decoder type of the network, the architecture can jointly learn models for semantic segmentation and depth estimation based on their common representation.This representation, inspired by multi-view learning, offers several important advantages, such as using one modality available at test timeto reconstruct the missing modality. In the RGB-D case, this enables the cross-modality scenarios, such as using depth data for semantically segmentation and the RGB images for depth estimation. We demonstrate the effectiveness of the proposed network on two publicly available RGB-D datasets. The experimental results show that the proposed method works well in both semantic segmentation and depth estimation tasks."
  },
  "cvpr2019_mula_twostream3dsemanticscenecompletion": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "Two Stream 3D Semantic Scene Completion",
    "authors": [
      "Martin Garbade",
      "Yueh-Tung Chen",
      "Johann Sawatzky",
      "Juergen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Garbade_Two_Stream_3D_Semantic_Scene_Completion_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Garbade_Two_Stream_3D_Semantic_Scene_Completion_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Inferring the 3D geometry and the semantic meaning of surfaces, which are occluded, is a very challenging task. Recently, a first end-to-end learning approach has been proposed that completes a scene from a single depth image. The approach voxelizes the scene and predicts for each voxel if it is occupied and, if it is occupied, the semantic class label. In this work, we propose a two stream approach that leverages depth information and semantic information, which is inferred from the RGB image, for this task. The approach constructs an incomplete 3D semantic tensor, which uses a compact three-channel encoding for the inferred semantic information, and uses a 3D CNN to infer the complete 3D semantic tensor. In our experimental evaluation, we show that the proposed two stream approach substantially outperforms the state-of-the-art for semantic scene completion."
  },
  "cvpr2019_mula_wifiandvisionmultimodallearningforaccurateandrobustdevice-freehumanactivityrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "WiFi and Vision Multimodal Learning for Accurate and Robust Device-Free Human Activity Recognition",
    "authors": [
      "Han Zou",
      "Jianfei Yang",
      "Hari Prasanna Das",
      "Huihan Liu",
      "Yuxun Zhou",
      "Costas J. Spanos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Zou_WiFi_and_Vision_Multimodal_Learning_for_Accurate_and_Robust_Device-Free_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Zou_WiFi_and_Vision_Multimodal_Learning_for_Accurate_and_Robust_Device-Free_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Human activity recognition plays an indispensable role in a myriad of emerging applications in context-aware services. Accurate activity recognition systems usually require the user to carry mobile or wearable devices, which is inconvenient for long term usage. In this paper, we design WiVi, a novel human activity recognition scheme that is able to identify common human activities in an accurate and device-free manner via multimodal machine learning using only commercial WiFi-enabled IoT devices and camera. For sensing using WiFi, a new platform is developed to extract fine-grained WiFi channel information and transform them into WiFi frames. A tailored convolutional neural network model is designed to extract high-level representative features among the WiFi frames in order to provide human activity estimation. We utilized a variant of C3D model for activity sensing using vision. Following this, WiVi performs multimodal fusion at the decision level to combine the strength of WiFi and vision by constructing an ensembled DNN model. Extensive experiments are conducted in an indoor environment, demonstrating that WiVi achieves 97.5% activity recognition accuracy and is robust under unfavorable situations, as each modality provides the complementary sensing when the other faces its limiting conditions."
  },
  "cvpr2019_mula_unsuperviseddomainadaptationformultispectralpedestriandetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "Unsupervised Domain Adaptation for Multispectral Pedestrian Detection",
    "authors": [
      "Dayan Guan",
      "Xing Luo",
      "Yanpeng Cao",
      "Jiangxin Yang",
      "Yanlong Cao",
      "George Vosselman",
      "Michael Ying Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Guan_Unsupervised_Domain_Adaptation_for_Multispectral_Pedestrian_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Guan_Unsupervised_Domain_Adaptation_for_Multispectral_Pedestrian_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Multimodal information (e.g., visible and thermal) can generate robust pedestrian detections to facilitate around-the-clock computer vision applications, such as autonomous driving and video surveillance. However, it still remains a crucial challenge to train a reliable detector working well in different multispectral pedestrian datasets without manual annotations. In this paper, we propose a novel unsupervised multimodal domain adaptation framework for multispectral pedestrian detection, by iteratively generating pseudo annotations and updating the parameters of our designed multispectral pedestrian detector on target domain. Pseudo annotations are generated using the detector trained on source domain, and then updated by fixing the parameters of detector and minimizing the cross entropy loss without back-propagation. Training labels are generated using the pseudo annotations by considering the characteristics of similarity and complementarity between well-aligned visible and infrared image pairs. The parameters of detector are updated using the generated training labels by minimizing our defined multi-detection loss function with back-propagation. The optimal parameters of detector can be obtained after iteratively updating the pseudo annotations and parameters. Experimental results show that our proposed unsupervised multimodal domain adaptation method achieves significantly higher detection performance than the approach without domain adaptation, and is competitive with the supervised multispectral pedestrian detectors."
  },
  "cvpr2019_mula_naturallanguageguidedvisualrelationshipdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "Natural Language Guided Visual Relationship Detection",
    "authors": [
      "Wentong Liao",
      "Bodo Rosenhahn",
      "Ling Shuai",
      "Michael Ying Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Liao_Natural_Language_Guided_Visual_Relationship_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Liao_Natural_Language_Guided_Visual_Relationship_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Reasoning about the relationships between object pairs in images is a crucial task for holistic scene understanding. Most of the existing works treat this task as a pure visual classification task: each type of relationship or phrase is classified as a relation category based on the extracted visual features. However, each kind of relationships has a wide variety of object combination and each pair of objects has diverse interactions. Obtaining sufficient training samples for all possible relationship categories is difficult and expensive. In this work, we propose a natural language guided framework to tackle this problem. We propose to use a generic bi-directional recurrent neural network to predict the semantic connection between the participating objects in the relationship from the aspect of natural language. The proposed simple method achieves the state-of-the-art on the Visual Relationship Detection (VRD) and Visual Genome datasets, especially when predicting unseen relationships (e.g., recall improved from 76.42% to 89.79% on VRD zeroshot testing set)."
  },
  "cvpr2019_mula_cross-streamselectivenetworksforactionrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "Cross-Stream Selective Networks for Action Recognition",
    "authors": [
      "Bowen Pan",
      "Jiankai Sun",
      "Wuwei Lin",
      "Limin Wang",
      "Weiyao Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Pan_Cross-Stream_Selective_Networks_for_Action_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Pan_Cross-Stream_Selective_Networks_for_Action_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Combining multiple information streams has shown obvi- ous improvements in video action recognition. Most exist- ing works handle each stream independently or perform a simple combination on temporally simultaneous samples in multi-streams, which fails to make full use of the streamwise complementary property due to the negligence of the temporal pattern gaps among streams. In this paper, we propose a cross-stream selective network (CSN) to properly integrate and evaluate information in multi-streams. The proposed CSN first introduces a local selective-sampling module (LSM), which can find asynchronous correspondences among streams and construct high-correlated sample groups across multiple information streams. This LSM can effectively deal with the temporal dis-alignment among different streams, leading to a better integration of cross-stream information. We further introduce a global adaptive- weighting module (GAM). It adaptively evaluates the importance weights for each cross-stream sample group and selects temporally more important ones in action recognition. With the integration of cross-stream information, our GAM can obtain more reasonable importance than the existing single- stream weighting schemes. Extensive experiments on benchmark datasets of UCF101 and HMDB51 demonstrate the effectiveness of our approach over previous state-of-the-art methods."
  },
  "cvpr2019_mula_co-compressingandunifyingdeepcnnmodelsforefficienthumanfaceandspeakerrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multimodal Learning and Applications",
    "title": "Co-Compressing and Unifying Deep CNN Models for Efficient Human Face and Speaker Recognition",
    "authors": [
      "Timmy S. T. Wan",
      "Jia-Hong Lee",
      "Yi-Ming Chan",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MULA/Wan_Co-Compressing_and_Unifying_Deep_CNN_Models_for_Efficient_Human_Face_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MULA/Wan_Co-Compressing_and_Unifying_Deep_CNN_Models_for_Efficient_Human_Face_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep CNN models have become state-of-the-art techniques in many application, e.g., face recognition, speaker recognition, and image classification. Although many studies address on speedup or compression of individual models, very few studies focus on co-compressing and unifying models from different modalities. In this work, to joint and compress face and speaker recognition models, a shared-codebook approach is adopted to reduce the redundancy of the combined model. Despite the modality of the inputs of these two CNN models are quite different, the shared codebook can support two CNN models of sound and image for speaker and face recognition. Experiments show the promising results of unified and co-compressing heterogeneous models for efficient inference."
  },
  "cvpr2019_pcv_measuringtheeffectsoftemporalcoherenceindepthestimationfordynamicscenes": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Photogrammetric Computer Vision",
    "title": "Measuring the Effects of Temporal Coherence in Depth Estimation for Dynamic Scenes",
    "authors": [
      "Iraklis Tsekourakis",
      "Philippos Mordohai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PCV/Tsekourakis_Measuring_the_Effects_of_Temporal_Coherence_in_Depth_Estimation_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PCV/Tsekourakis_Measuring_the_Effects_of_Temporal_Coherence_in_Depth_Estimation_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper presents a new algorithm for enforcing temporal coherence on depth estimation from multi-view videos of dynamic scenes as well as the first substantial quantitative evaluation of the improvement in depth estimation accuracy due to temporal coherence. The proposed algorithm is generally applicable and practical since it bypasses explicit scene flow estimation, which has a very large state space, and relies only on optical flow which is used to impose soft constraints on depth estimation for the next frame. As a result, our algorithm is applicable to scenes with large depth and motion ranges. The output is a sequence of depth maps that can be used for novel view synthesis among other applications. While it is intuitive that enforcing temporal coherence should improve the accuracy of depth estimation, this improvement has never been assessed quantitatively due to the lack of data with ground truth. To overcome this limitation we use the image prediction error as the criterion and show that the benefits of temporal coherence are significant on a diverse set of multi-view video sequences."
  },
  "cvpr2019_pcv_ismo-ganadversariallearningformonocularnon-rigid3dreconstruction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Photogrammetric Computer Vision",
    "title": "IsMo-GAN: Adversarial Learning for Monocular Non-Rigid 3D Reconstruction",
    "authors": [
      "Soshi Shimada",
      "Vladislav Golyanik",
      "Christian Theobalt",
      "Didier Stricker"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PCV/Shimada_IsMo-GAN_Adversarial_Learning_for_Monocular_Non-Rigid_3D_Reconstruction_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PCV/Shimada_IsMo-GAN_Adversarial_Learning_for_Monocular_Non-Rigid_3D_Reconstruction_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The majority of the existing methods for non-rigid 3D surface regression from a single 2D image require an object template or point tracks over multiple frames as an input, and are still far from real-time processing rates. In this work, we present the Isometry-Aware Monocular Generative Adversarial Network (IsMo-GAN) -- an approach for direct 3D reconstruction from a single image, trained for the deformation model in an adversarial manner on a light-weight synthetic dataset. IsMo-GAN reconstructs surfaces from real images under varying illumination, camera poses, textures and shading at over 250 Hz. In multiple experiments, it consistently outperforms multiple approaches in the reconstruction accuracy, runtime, generalisation to unknown surfaces and robustness to occlusions. In comparison to the state-of-the-art, we reduce the reconstruction error by 10-30% including the textureless case and our surfaces evince fewer artefacts qualitatively. "
  },
  "cvpr2019_pcv_learnstereo,infermonosiamesenetworksforself-supervised,monocular,depthestimation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Photogrammetric Computer Vision",
    "title": "Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised, Monocular, Depth Estimation",
    "authors": [
      "Matan Goldman",
      "Tal Hassner",
      "Shai Avidan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PCV/Goldman_Learn_Stereo_Infer_Mono_Siamese_Networks_for_Self-Supervised_Monocular_Depth_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PCV/Goldman_Learn_Stereo_Infer_Mono_Siamese_Networks_for_Self-Supervised_Monocular_Depth_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The field of self-supervised monocular depth estimation has seen huge advancements in recent years. Most methods assume stereo data is available during training but usually under-utilize it and only treat it as a reference signal. We propose a novel self-supervised approach which uses both left and right images equally during training, but can still be used with a single input image at test time, for monocular depth estimation. Our Siamese network architecture consists of two, twin networks, each learns to predict a disparity map from a single image. At test time, however, only one of these networks is used in order to infer depth. We show state-of-the-art results on the standard KITTI Eigen split benchmark as well as being the highest scoring self-supervised method on the new KITTI single view benchmark. To demonstrate the ability of our method to generalize to new data sets, we further provide results on the Make3D benchmark, which was not used during training."
  },
  "cvpr2019_pcv_robustifyingrelativeorientationswithrespecttorepetitivestructuresandveryshortbaselinesforglobalsfm": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Photogrammetric Computer Vision",
    "title": "Robustifying Relative Orientations With Respect to Repetitive Structures and Very Short Baselines for Global SfM",
    "authors": [
      "Xin Wang",
      "Teng Xiao",
      "Michael Gruber",
      "Christian Heipke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PCV/Wang_Robustifying_Relative_Orientations_With_Respect_to_Repetitive_Structures_and_Very_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PCV/Wang_Robustifying_Relative_Orientations_With_Respect_to_Repetitive_Structures_and_Very_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, global SfM has been attracting many researchers, mainly because of its time efficiency. Most of these methods are based on averaging relative orientations (ROs). Therefore, eliminating incorrect ROs is of great significance for improving the robustness of global SfM. In this paper, we propose a method to eliminate wrong ROs which have resulted from repetitive structure (RS) and very short baselines (VSB). We suggest two corresponding criteria that indicate the quality of ROs. These criteria are functions of potentially conjugate points resulting from local image matching of image pairs, followed by a geometry check using the 5-point algorithm combined with RANSAC. RS is detected based on counts of corresponding conjugate points of the various pairs, while VSB is found by inspecting the intersection angles of corresponding image rays. Based on these two criteria, incorrect ROs are eliminated. We demonstrate the proposed method on various datasets by inserting our refined ROs into a global SfM pipeline. The experiments show that compared to other methods we can generate the better results in this way."
  },
  "cvpr2019_pcv_adigitalimageprocessingpipelineformodellingofrealisticnoiseinsyntheticimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Photogrammetric Computer Vision",
    "title": "A Digital Image Processing Pipeline for Modelling of Realistic Noise in Synthetic Images",
    "authors": [
      "Oleksandra Bielova",
      "Ronny Hansch",
      "Andreas Ley",
      "Olaf Hellwich"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PCV/Bielova_A_Digital_Image_Processing_Pipeline_for_Modelling_of_Realistic_Noise_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PCV/Bielova_A_Digital_Image_Processing_Pipeline_for_Modelling_of_Realistic_Noise_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The evaluation of computer vision methods on synthetic images offers control over scene, object, and camera properties. The disadvantage is that synthetic data usually lack many of the effects of real cameras that pose the actual challenge to the methods under investigation. Among those, noise is one of the effects more difficult to simulate as it changes the signal at an early stage and is strongly influenced by the camera's internal processing chain. The resulting noise is highly complex, intensity dependent, as well as spatially and spectrally correlated. We propose to transform synthetic images into the raw format of digital cameras, alter them with a physically motivated noise model, and then apply a processing chain that resembles a digital camera. Experiments show that the resulting noise exhibits a strong similarity to noise in real digital images, which further decreases the gap between synthesized images and real photographs."
  },
  "cvpr2019_bmtt_multiplepeopletrackingusingbodyandjointdetections": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BMTT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 4th BMTT MOTChallenge Workshop - How crowded can it get?",
    "title": "Multiple People Tracking Using Body and Joint Detections",
    "authors": [
      "Roberto Henschel",
      "Yunzhe Zou",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BMTT/Henschel_Multiple_People_Tracking_Using_Body_and_Joint_Detections_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BMTT/Henschel_Multiple_People_Tracking_Using_Body_and_Joint_Detections_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Most multiple people tracking systems compute trajectories based on the tracking-by-detection paradigm.Consequently, the performance depends to a large extent on the quality of the employed input detections. However, despite an enormous progress in recent years, partially occluded people are still often not recognized. Also, many correct detections are mistakenly discarded when the non-maximum suppression is performed. Improving the tracking performance thus requires to augment the coarse input. Well-suited for this task are fine-graded body joint detections, as they allow to locate even strongly occluded persons. Thus in this work, we analyze the suitability of including joint detections for multiple people tracking. We introduce different affinities between the two detection types and evaluate their performances. Tracking is then performed within a near-online framework based on a min cost graph labeling formulation. As a result, our framework can recover heavily occluded persons and solve the data association efficiently. We evaluate our framework on the MOT16/17 benchmark. Experimental results demonstrate that our framework achieves state-of-the-art results. "
  },
  "cvpr2019_bmtt_simultaneousidentificationandtrackingofmultiplepeopleusingvideoandimus": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BMTT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 4th BMTT MOTChallenge Workshop - How crowded can it get?",
    "title": "Simultaneous Identification and Tracking of Multiple People Using Video and IMUs",
    "authors": [
      "Roberto Henschel",
      "Timo von Marcard",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BMTT/Henschel_Simultaneous_Identification_and_Tracking_of_Multiple_People_Using_Video_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BMTT/Henschel_Simultaneous_Identification_and_Tracking_of_Multiple_People_Using_Video_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Most modern approaches for multiple people tracking rely on human appearance to exploit similarity between person detections.In this work we propose an alternative tracking method that does not depend on visual appearance and is still capable to deal with very dynamic motions and long-term occlusions.We make this feasible by: (i) incorporating additional information from body-worn inertial sensors, (ii) designing a neural network to relate person detections to orientation measurements and (iii) formulating a graph labeling problem to obtain a tracking solution that is globally consistent with the video and inertial recordings.We evaluate our approach on several challenging tracking sequences and achieve a very high IDF1 score of 91.2%. We outperform appearance-based baselines in scenarios where appearance is less informative and are on-par in situations with discriminative people appearance. "
  },
  "cvpr2019_wicv_wicv2019thesixthwomenincomputervisionworkshop": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "WiCV 2019: The Sixth Women In Computer Vision Workshop",
    "authors": [
      "Irene Amerini",
      "Elena Balashova",
      "Sayna Ebrahimi",
      "Kathryn Leonard",
      "Arsha Nagrani",
      "Amaia Salvador"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper we present the Women in Computer Vision Workshop - WiCV 2019, organized in conjunction with CVPR 2019. This event is meant for increasing the visibility and inclusion of women researchers in computer vision field. Computer vision and machine learning have made incredible progress over the past years, but the number of female researchers is still low both in the academia and in the industry. WiCV is organized especially for this reason: to raise visibility of female researchers, to increase collaborations between them, and to provide mentorship to female junior researchers in the field. In this paper, we present a report of trends over the past years, along with a summary of statistics regarding presenters, attendees, and sponsorship for the current workshop."
  },
  "cvpr2019_wicv_postdisastermappingwithsemanticchangedetectioninsatelliteimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Post Disaster Mapping With Semantic Change Detection in Satellite Imagery",
    "authors": [
      "Ananya Gupta",
      "Elisabeth Welburn",
      "Simon Watson",
      "Hujun Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Gupta_Post_Disaster_Mapping_With_Semantic_Change_Detection_in_Satellite_Imagery_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Gupta_Post_Disaster_Mapping_With_Semantic_Change_Detection_in_Satellite_Imagery_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate road maps are important for timely disaster relief efforts and risk management. Current disaster mapping is done manually by volunteers following a disaster and the process is slow and error prone. We propose a framework for identifying accessible roads in post-disaster satellite imagery by detecting changes from pre-disaster imagery, in conjunction with OpenStreetMap data. We validate our results with data from Indonesia 2018 tsunami, obtained from DigitalGlobe."
  },
  "cvpr2019_wicv_sidodasyntheticimagedatasetfor3dobjectposerecognitionwithdistractors": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition With Distractors",
    "authors": [
      "Mona Jalal",
      "Josef Spjut",
      "Ben Boudaoud",
      "Margrit Betke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Jalal_SIDOD_A_Synthetic_Image_Dataset_for_3D_Object_Pose_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Jalal_SIDOD_A_Synthetic_Image_Dataset_for_3D_Object_Pose_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a new, publicly-available image dataset generated by the NVIDIA Deep Learning Data Synthesizer intended for use in object detection, pose estimation, and tracking applications. This dataset contains 144k stereo image pairs that synthetically combine 18 camera viewpoints of three photorealistic virtual environments with up to 10 objects (chosen randomly from the 21 object models of the YCB dataset ) and flying distractors. Object and camera pose, scene lighting, and quantity of objects and distractors were randomized. Each provided view includes RGB, depth, segmentation, and surface normal images, all pixel level. We describe our approach for domain randomization and provide insight into the decisions that produced the dataset."
  },
  "cvpr2019_wicv_residualattention-basedfusionforvideoclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Residual Attention-Based Fusion for Video Classification",
    "authors": [
      "Samira Pouyanfar",
      "Tianyi Wang",
      "Shu-Ching Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Pouyanfar_Residual_Attention-Based_Fusion_for_Video_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Pouyanfar_Residual_Attention-Based_Fusion_for_Video_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Video data is inherently multimodal and sequential. Therefore, deep learning models need to aggregate all data modalities while capturing the most relevant spatio-temporal information from a given video. This paper presents a multimodal deep learning framework for video classification using a Residual Attention-based Fusion (RAF) method. Specifically, this framework extracts spatio-temporal features from each modality using residual attention-based bidirectional Long Short-Term Memory and fuses the information using a weighted Support Vector Machine to handle the imbalanced data. Experimental results on a natural disaster video dataset show that our approach improves upon the state-of-the-art by 5% and 8% regarding F1 and MAP metrics, respectively. Most remarkably, our proposed residual attention model reaches a 0.95 F1-score and 0.92 MAP for this dataset. "
  },
  "cvpr2019_wicv_singleimagemulti-spectralphotometricstereousingasplitu-shapedcnn": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Single Image Multi-Spectral Photometric Stereo Using a Split U-Shaped CNN",
    "authors": [
      "Doris Antensteiner",
      "Svorad Stolc",
      "Daniel Soukup"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Antensteiner_Single_Image_Multi-Spectral_Photometric_Stereo_Using_a_Split_U-Shaped_CNN_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Antensteiner_Single_Image_Multi-Spectral_Photometric_Stereo_Using_a_Split_U-Shaped_CNN_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a system to extract surface orientation and albedos from a single shot image using three differently colored illumination sources. Photometric stereo allows one to extract local surface information such as normals or gradients. Traditionally, the local orientations and albedos are computed using serveral acquisitions of the same viewing angle and under varying illumination directions. In applications with moving objects, where the acquisition- as well as processing speed are essential, such setups are poorly suited. We propose a single shot decomposition using three differently colored light sources under defined illumination directions. To allow for a fast and regularized inference, we built a split U-shaped convolutional neural network, which takes a single shot input and estimates both the surface orientation and albedo simultaneously. "
  },
  "cvpr2019_wicv_segmentationcertaintythroughuncertaintyuncertainty-refinedbinaryvolumetricsegmentationundermultifactordomainshift": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Segmentation Certainty Through Uncertainty: Uncertainty-Refined Binary Volumetric Segmentation Under Multifactor Domain Shift",
    "authors": [
      "Carianne Martinez",
      "Kevin M. Potter",
      "Matthew D. Smith",
      "Emily A. Donahue",
      "Lincoln Collins",
      "John P. Korbin",
      "Scott A. Roberts"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Martinez_Segmentation_Certainty_Through_Uncertainty_Uncertainty-Refined_Binary_Volumetric_Segmentation_Under_Multifactor_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Martinez_Segmentation_Certainty_Through_Uncertainty_Uncertainty-Refined_Binary_Volumetric_Segmentation_Under_Multifactor_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep learning segmentation models are known to be sensitive to the scale, contrast, and distribution of pixel values when applied to Computed Tomography (CT) images. For material samples, scans are often obtained from a variety of scanning equipment and resolutions resulting in domain shift. The ability of segmentation models to generalize to examples from these shifted domains relies on how well the distribution of the training data represents the overall distribution of the target data. We present a method to overcome the challenges presented by domain shifts. Our results indicate that we can leverage a deep learning model trained on one domain to accurately segment similar materials at different resolutions by refining binary predictions using uncertainty quantification (UQ). We apply this technique to a set of unlabeled CT scans of woven composite materials with clear qualitative improvement of binary segmentations over the original deep learning predictions. In contrast to prior work, our technique enables refined segmentations without the expense of the additional training time and parameters associated with deep learning models used to address domain shift."
  },
  "cvpr2019_wicv_usingaprioriknowledgetoimprovesceneunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Using a Priori Knowledge to Improve Scene Understanding",
    "authors": [
      "Brigit Schroeder",
      "Alexandre Alahi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Schroeder_Using_a_Priori_Knowledge_to_Improve_Scene_Understanding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Schroeder_Using_a_Priori_Knowledge_to_Improve_Scene_Understanding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Semantic segmentation algorithms that can robustly segment objects across multiple camera viewpoints is crucialfor assuring navigation and safety in emerging applicationssuch as autonomous driving. Existing algorithms treat eachimage in isolation, but autonomous vehicles often revisit thesame locations. We propose leveraging this a priori knowledge to improve semantic segmentation of images from se-quential driving datasets. We examine several methods tofuse these temporal scene priors, and introduce a prior fusion network that is able to learn how to transfer this information. Our model improves the accuracy of dynamic object classes from 69.1% to 73.3%, and static classes from 88.2% to 89.1%."
  },
  "cvpr2019_wicv_towardscomputervisionpoweredcolor-nutrientassessmentofpureedfood": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Towards Computer Vision Powered Color-Nutrient Assessment of Pureed Food",
    "authors": [
      "Kaylen J. Pfisterer",
      "Robert Amelard",
      "Braeden Syrnyk",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Pfisterer_Towards_Computer_Vision_Powered_Color-Nutrient_Assessment_of_Pureed_Food_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Pfisterer_Towards_Computer_Vision_Powered_Color-Nutrient_Assessment_of_Pureed_Food_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With one in four individuals afflicted with malnutrition, computer vision may provide a way of introducing a new level of automation in the nutrition field to reliably monitor food and nutrient intake. We present results on the link between color and vitamin A content using transmittance imaging of a pureed foods dilution series in a computer-vision powered intelligent nutrient sensing system prototype and use a fine-tuned deep autoencoder network to predict the relative concentration of sweet potato purees. Results indicate an network accuracy of 80% across beginner (6 month) and intermediate (8 month) commercially prepared pureed sweet potato samples. Network errors may be explained by fundamental differences in optical properties which are further discussed."
  },
  "cvpr2019_wicv_superpixel-based3dbuildingmodelrefinementandchangedetection,usingvhrstereosatelliteimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Superpixel-Based 3D Building Model Refinement and Change Detection, Using VHR Stereo Satellite Imagery",
    "authors": [
      "Zeinab Gharibbafghi",
      "Jiaojiao Tian",
      "Peter Reinartz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Gharibbafghi_Superpixel-Based_3D_Building_Model_Refinement_and_Change_Detection_Using_VHR_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Gharibbafghi_Superpixel-Based_3D_Building_Model_Refinement_and_Change_Detection_Using_VHR_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Buildings are one of the main objects in urban remote sensing and photogrammetric computer vision applications using satellite data. In this paper a superpixel-based approach is presented to refine 3D building models from stereo satellite imagery. First, for each epoch in time, a multispectral very high resolution (VHR) satellite image is segmented using an efficient superpixel, called edge-based simple linear iterative clustering (ESLIC). The ESLIC algorithm segments the image utilizing the spectral and spatial information, as well as the statistical measures from the gray-level co-occurrence matrix (GLCM), simultaneously. Then the resulting superpixels are imposed on the corresponding 3D model of the scenes taken from each epoch. Since ESLIC has high capability of preserving edges in the image, normalized digital surface models (nDSMs) can be modified by averaging height values inside superpixels. These new normalized models for epoch 1 and epoch 2, are then used to detect the 3D change of each building in the scene."
  },
  "cvpr2019_wicv_assessingarchitecturalsimilarityinpopulationsofdeepneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Assessing Architectural Similarity in Populations of Deep Neural Networks",
    "authors": [
      "Audrey G. Chung",
      "Paul Fieguth",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Chung_Assessing_Architectural_Similarity_in_Populations_of_Deep_Neural_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Chung_Assessing_Architectural_Similarity_in_Populations_of_Deep_Neural_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Evolutionary deep intelligence has recently shown great promise for producing small, powerful deep neural network models via the synthesis of increasingly efficient architectures over successive generations. Despite recent research showing the efficacy of multi-parent evolutionary synthesis, little has been done to directly assess architectural similarity between networks during the synthesis process for improved parent network selection. In this work, we present a preliminary study into quantifying architectural similarity via the percentage overlap of architectural clusters. Results show that networks synthesized using architectural alignment (via gene tagging) maintain higher architectural similarities within each generation, potentially restricting the search space of highly efficient network architectures."
  },
  "cvpr2019_wicv_visualtransferbetweenatarigamesusingcompetitivereinforcementlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Visual Transfer Between Atari Games Using Competitive Reinforcement Learning",
    "authors": [
      "Akshita Mittel",
      "Purna Sowmya Munukutla"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Mittel_Visual_Transfer_Between_Atari_Games_Using_Competitive_Reinforcement_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Mittel_Visual_Transfer_Between_Atari_Games_Using_Competitive_Reinforcement_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Modern deep Reinforcement Learning (RL) methods are highly effective at selecting optimal policies to maximize rewards. The combination of these methods with Deep Learning approaches shows promise for challenging tasks by leveraging rich visual information for policy selection. In this paper, we explore the use of visual representations to transfer the knowledge of an RL agent from one domain to another. More specifically, we propose a method that can generalize for a target game using an RL agent trained for a source game in Atari 2600 environment. Instead of fine-tuning a pre-trained model for the target game, we propose a learning approach to update the model using multiple RL agents trained in parallel with different representations of the target game. The visual representations of the target game are generated by learning a visual mapping between the source game and the target game in an unsupervised manner. The visual mapping between sequences of transfer pairs has been shown to derive new representations of the target game; training on which improves the RL agent updates in terms of performance, data efficiency and stability. In order to demonstrate the effectiveness of this approach, the transfer learning procedure is evaluated on two pairs of Atari games taken in contrasting settings."
  },
  "cvpr2019_wicv_riemannianlossforimagerestoration": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Riemannian Loss for Image Restoration",
    "authors": [
      "Jing Mu",
      "Xinfeng Zhang",
      "Shuyuan Zhu",
      "Ruiqin Xiong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Mu_Riemannian_Loss_for_Image_Restoration_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Mu_Riemannian_Loss_for_Image_Restoration_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep neural networks are widely used for image restoration, however the loss criteration is usually set as l2. l2 penalizes larger errors, which is unstable for outliers. To avoid the disadvantages, l1 is utilized as a more robust and well behaved loss. This paper proposes a novel loss function for restoration networks, which measures geodesic distance in Riemannianmanifold and exploits the outstanding properties of l1. Different from l1 and l2 loss which reflects pixel distance, our loss in Riemannian reflects the structure distance of image. The proposed loss not only preserves the robutness of l1 loss, but also reflects the image contrasts. Experimental results on image super resolution and compressed sensing show that our proposed loss function achieves more accurate reconstructions, according to both the objective and perceptual qualities."
  },
  "cvpr2019_wicv_runetarobustunetarchitectureforimagesuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "RUNet: A Robust UNet Architecture for Image Super-Resolution",
    "authors": [
      "Xiaodan Hu",
      "Mohamed A. Naiel",
      "Alexander Wong",
      "Mark Lamm",
      "Paul Fieguth"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Hu_RUNet_A_Robust_UNet_Architecture_for_Image_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Hu_RUNet_A_Robust_UNet_Architecture_for_Image_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Single image super-resolution (SISR) is a challenging ill-posed problem which aims to restore or infer a high-resolution image from a low-resolution one. Powerful deep learning-based techniques have achieved state-of-the-art performance in SISR; however, they can underperform when handling images with non-stationary degradations, such as for the application of projector resolution enhancement. In this paper, a new UNet architecture that is able to learn the relationship between a set of degraded low-resolution images and their corresponding original high-resolution images is proposed. We propose employing a degradation model on training images in a non-stationary way, allowing the construction of a robust UNet (RUNet) for image super-resolution (SR). Experimental results show that the proposed RUNet improves the visual quality of the obtained super-resolution images while maintaining a low reconstruction error."
  },
  "cvpr2019_wicv_transferlearningforclassifyingsinglehandgesturesoncomprehensivebharatanatyammudradataset": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Transfer Learning for Classifying Single Hand Gestures on Comprehensive Bharatanatyam Mudra Dataset",
    "authors": [
      "Anuja P. Parameshwaran",
      "Heta P. Desai",
      "Rajshekhar Sunderraman",
      "Michael Weeks"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Parameshwaran_Transfer_Learning_for_Classifying_Single_Hand_Gestures_on_Comprehensive_Bharatanatyam_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Parameshwaran_Transfer_Learning_for_Classifying_Single_Hand_Gestures_on_Comprehensive_Bharatanatyam_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " For any dance form, either classical or folk, visual expressions - facial expressions and hand gestures play a key role in conveying the storyline of the accompanied music to the audience. Bharatanatyam - a classical dance form which has origins from the southern states of India, is on the verge of being completely automated partly due to an acute dearth of qualified and dedicated teachers/gurus. In an honest effort to speed up this automation process and at the same time preserve the cultural heritage, we have chosen to identify and classify the single hand gestures/mudras/hastas against their true labels by using two variations of the convolutional neural networks (CNNs) that demonstrates the exceeding effectiveness of transfer learning irrespective of the domain difference between the pre-training and the training dataset. This work is primarily aimed at 1) building a novel dataset of 2D single hand gestures belonging to 27 classes that were collected from Google search engine (Google images), YouTube videos (dynamic and with background considered) and professional artists under staged environment constraints (plain backgrounds), 2) exploring the effectiveness of Convolutional Neural Networks in identifying and classifying the single hand gestures by optimizing the hyperparameters, and 3) evaluating the impacts of transfer learning and double transfer learning, which is a novel concept explored in this paper for achieving higher classification accuracy."
  },
  "cvpr2019_wicv_improvedautomatingseismicfaciesanalysisusingdeepdilatedattentionautoencoders": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Improved Automating Seismic Facies Analysis Using Deep Dilated Attention Autoencoders",
    "authors": [
      "Zengyan Wang",
      "Fangyu Li",
      "Thiab R. Taha",
      "Hamid R. Arabnia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Wang_Improved_Automating_Seismic_Facies_Analysis_Using_Deep_Dilated_Attention_Autoencoders_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Wang_Improved_Automating_Seismic_Facies_Analysis_Using_Deep_Dilated_Attention_Autoencoders_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With the dramatic growth and complexity of seismic data, manual annotation of seismic facies has become a significant challenge. The encoder-decoder neural network architecture has been widely used in image segmentation.In recent years, the same architecture has also been used in seismic surveys for facies classification applications.In this paper, a modified U-Net architecture with trainable soft attention mechanism and dilated convolution is proposed to improve the automatic seismic facies analysis. This proposed framework generates more accurate results in a more efficient way. The dilated convolution achieves more accurate results with less computation than the CNN with pooling in U-Net. With the attention mechanism, the dilated U-Net model further improves classification accuracy. Our experiments show that the dilated attention autoencoder model is less prone to overfitting and at the same time, it achieves a smoother increasing validation accuracy."
  },
  "cvpr2019_wicv_autonomousneurosurgicalinstrumentsegmentationusingend-to-endlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - WOMEN IN COMPUTER VISION WORKSHOP",
    "title": "Autonomous Neurosurgical Instrument Segmentation Using End-To-End Learning",
    "authors": [
      "Niveditha Kalavakonda",
      "Blake Hannaford",
      "Zeeshan Qazi",
      "Laligam Sekhar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WiCV/Kalavakonda_Autonomous_Neurosurgical_Instrument_Segmentation_Using_End-To-End_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WiCV/Kalavakonda_Autonomous_Neurosurgical_Instrument_Segmentation_Using_End-To-End_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Monitoring surgical instruments is an essential task in computer-assisted interventions and surgical robotics. It is also important for navigation, data analysis, skill assessment and surgical workflow analysis in conventional surgery. However, there are no standard datasets and benchmarks for tool identification in neurosurgery. To this end, we are releasing a novel neurosurgical instrument segmentation dataset called NeuroID for advancing research in the field. Delineating surgical tools from the background requires accurate pixel-wise instrument segmentation. In this paper, we present a comparison between three encoder-decoder approaches to binary segmentation of neurosurgical instruments, where we classify each pixel in the image to be either tool or background. A baseline performance was obtained by using heuristics to combine extracted features. We also extend the analysis to a publicly available robotic instrument segmentation dataset and include its results. The source code for our methods and the neurosurgical instrument dataset will be made publicly available (http://brl.ee.washington.edu/robotics/surgical-robotics/neurosurgical-instrument-segmentation) to facilitate reproducibility."
  },
  "cvpr2019_ffss-usad_fashionaiahierarchicaldatasetforfashionunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "FashionAI: A Hierarchical Dataset for Fashion Understanding",
    "authors": [
      "Xingxing Zou",
      "Xiangheng Kong",
      "Waikeung Wong",
      "Congde Wang",
      "Yuguang Liu",
      "Yang Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Zou_FashionAI_A_Hierarchical_Dataset_for_Fashion_Understanding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Zou_FashionAI_A_Hierarchical_Dataset_for_Fashion_Understanding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Fine-grained attribute recognition is critical for fashion understanding, yet is missing in existing professional and comprehensive fashion datasets. In this paper, we present a large scale attribute dataset with manual annotation in high quality. To this end, complex fashion knowledge is disassembled into mutually exclusive concepts and form a hierarchical structure to describe the cognitive process. Such well-structured knowledge is reflected by dataset in terms of its clear definition and precise annotation. The problems which are common in the process of annotation, including structured noise, occlusion, uncertain problems, and attribute inconsistency, are well addressed instead of merely discarding those bad data. Further, we propose an iterative process of building a dataset with practical usefulness. With 24 key points, 245 labels that cover 6 categories of women's clothing, and a total of 41 subcategories, the cre- ation of our dataset drew upon a large amount of crowd staff engagement. Extensive experiments quantitatively and qualitatively demonstrate its effectiveness."
  },
  "cvpr2019_ffss-usad_ten-million-orderhumandatabaseforworld-widefashioncultureanalysis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "Ten-Million-Order Human Database for World-Wide Fashion Culture Analysis",
    "authors": [
      "Hirokatsu Kataoka",
      "Yutaka Satoh",
      "Kaori Abe",
      "Munetaka Minoguchi",
      "Akio Nakamura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Kataoka_Ten-Million-Order_Human_Database_for_World-Wide_Fashion_Culture_Analysis_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Kataoka_Ten-Million-Order_Human_Database_for_World-Wide_Fashion_Culture_Analysis_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The paper proposes a huge geo-tagged image database, referred to as the Fashion Culture DataBase (FCDB), which is considered to be an automatic image collection in cloud-based services such as social networks. In the stage of database construction, we introduce the large-scale data collection, refinement, and representation for SNS-based analysis. The proposed FCDB consists of 25,707,690 images for use in (i) inter-city siimlarity analysis and (ii) fashion trends visualization.We also present a simple but effective representation for the spatial and temporal analysis. Finally, we visualize an inter-city graph and the yearly trend change with the refined FCDB."
  },
  "cvpr2019_ffss-usad_learningpersonaltastesinchoosingfashionoutfits": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "Learning Personal Tastes in Choosing Fashion Outfits",
    "authors": [
      "Yusan Lin",
      "Maryam Moosaei",
      "Hao Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Lin_Learning_Personal_Tastes_in_Choosing_Fashion_Outfits_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Lin_Learning_Personal_Tastes_in_Choosing_Fashion_Outfits_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With the emergence of fashion recommendation, many researchers have attempted to recommend fashion items that fit consumers' tastes. However, few have looked into fashion outfits as a whole when making recommendations. In this paper, we propose a neural network that learns one's fashion taste and predicts whether an individual likes a fashion outfit. To improve learning, we also develop a fashion outfit negative sampling scheme to sample fashion outfits that are different enough. With experiments on the collected Polyvore dataset, we find that using complete images of fashion outfits performs well when learning individuals' tastes toward fashion outfits. Our proposed negative sampling scheme also improves the model's performance significantly, compared to random negative sampling."
  },
  "cvpr2019_ffss-usad_studyonfashionimageretrievalmethodsforefficientfashionvisualsearch": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "Study on Fashion Image Retrieval Methods for Efficient Fashion Visual Search",
    "authors": [
      "Sanghyuk Park",
      "Minchul Shin",
      "Sungho Ham",
      "Seungkwon Choe",
      "Yoohoon Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Park_Study_on_Fashion_Image_Retrieval_Methods_for_Efficient_Fashion_Visual_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Park_Study_on_Fashion_Image_Retrieval_Methods_for_Efficient_Fashion_Visual_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Fashion image retrieval (FIR) is a challenging task, which requires searching for exact items accurately from massive collections of fashion products based on a query image. Despite recent advances, FIR still has limitations for application to real-world visual searches. The main reason for this is not only the trade-off between the model complexity and performance, but also the common nature of fashion images captured under uncontrolled circumstances (e.g. varying viewpoints and lighting conditions). In particular, fashion images are vulnerable to shape deformations and suffer from inconsistency between the user's query images and refined product images. Moreover, multiple fashion objects can be present simultaneously within a single image. In this paper, we considered an FIR method that is optimized for the fashion domain. We investigated training strategies and deep models to improve the retrieval performance. The experimental results on three benchmarks from DeepFashion dataset show that considered methods could achieve the significant improvements compared to the previous FIR methods."
  },
  "cvpr2019_ffss-usad_modelingimagecompositionforvisualaestheticassessment": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "Modeling Image Composition for Visual Aesthetic Assessment",
    "authors": [
      "Dong Liu",
      "Rohit Puri",
      "Nagendra Kamath",
      "Subhabrata Bhattacharya"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Liu_Modeling_Image_Composition_for_Visual_Aesthetic_Assessment_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Liu_Modeling_Image_Composition_for_Visual_Aesthetic_Assessment_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Composition information is an important cue to characterize the aesthetic property of an image. We propose to model the image composition information as the mutual dependencies of its local regions, and design an architecture to leverage such information to boost aesthetics assessment. We adopt a Fully Convolutional Network (FCN) as the feature encoder of the input image and use the encoded feature map to represent the individual local regions and their spatial layout in the image. Then we build a region composition graph in which each node denotes one region and any two nodes are connected by an edge weighted by the similarity of the region features. We perform reasoning on this graph via graph convolution, in which the activation of each node is determined by its highly correlated neighbors. Our method achieves the state-of-the-art performance on the benchmark visual aesthetic06 dataset."
  },
  "cvpr2019_ffss-usad_fashion-attganattribute-awarefashioneditingwithmulti-objectivegan": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "Fashion-AttGAN: Attribute-Aware Fashion Editing With Multi-Objective GAN",
    "authors": [
      "Qing Ping",
      "Bing Wu",
      "Wanying Ding",
      "Jiangbo Yuan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Ping_Fashion-AttGAN_Attribute-Aware_Fashion_Editing_With_Multi-Objective_GAN_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Ping_Fashion-AttGAN_Attribute-Aware_Fashion_Editing_With_Multi-Objective_GAN_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we introduce a novel task, namely attribute-aware fashion-editing, to the fashion domain. A first dataset is constructed for this task with 14,221 images and 22 attributes, which has been made publically available. We re-define the overall objectives in AttGAN and propose the Fashion-AttGAN model to tackle this new task. Experimental results show effectiveness of our Fashion-AttGAN on fashion editing over the original AttGAN."
  },
  "cvpr2019_ffss-usad_poweringrobustfashionretrievalwithinformationrichfeatureembeddings": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "Powering Robust Fashion Retrieval With Information Rich Feature Embeddings",
    "authors": [
      "Ayush Chopra",
      "Abhishek Sinha",
      "Hiresh Gupta",
      "Mausoom Sarkar",
      "Kumar Ayush",
      "Balaji Krishnamurthy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Chopra_Powering_Robust_Fashion_Retrieval_With_Information_Rich_Feature_Embeddings_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Chopra_Powering_Robust_Fashion_Retrieval_With_Information_Rich_Feature_Embeddings_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Visual content based product retrieval has become increasingly important for e-commerce. Fashion retrieval, in particular, is a challenging problem owing to a wide range of deformations of clothing items along with visual distortions in their product images. In this paper, we propose a Grid Search Network (GSN) for learning feature embeddings for fashion retrieval. The proposed approach posits the training procedure as a search problem, focused on locating matches for a reference query image in a grid containing both positive and negative images w.r.t the query. The proposed framework significantly outperforms existing state-of-art methods on benchmark fashion datasets. We also utilize a reinforcement learning based strategy to learn a specialized transformation function which further improves retrieval performance when applied over the feature embeddings. We also extend the reinforcement learning based strategy to learn custom kernel functions for SVM based classification over FashionMNIST and MNIST datasets, showing improved performance. We highlight the generalization capabilities of this search strategy by showing performance improvement in search and attribution tasks in domains beyond fashion."
  },
  "cvpr2019_ffss-usad_sizenetweaklysupervisedlearningofvisualsizeandfitinfashionimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "SizeNet: Weakly Supervised Learning of Visual Size and Fit in Fashion Images",
    "authors": [
      "Nour Karessli",
      "Romain Guigoures",
      "Reza Shirvany"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Karessli_SizeNet_Weakly_Supervised_Learning_of_Visual_Size_and_Fit_in_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Karessli_SizeNet_Weakly_Supervised_Learning_of_Visual_Size_and_Fit_in_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Finding clothes that fit is a hot topic in the e-commerce fashion industry. Most approaches addressing this problem are based on statistical methods relying on historical data of articles purchased and returned to the store. Such approaches suffer from the cold start problem for the thousands of articles appearing on the shopping platforms everyday, for which no prior purchase history is available. We propose to employ visual data to infer size and fit characteristics of fashion articles. We introduce SizeNet, a weakly supervised teacher-student training framework that leverages the power of statistical models combined with the rich visual information from article images to learn visual cues for size and fit characteristics, capable of tackling the challenging cold start problem. Detailed experiments are performed on thousands of textile garments, including dresses, trousers, knitwear, tops, etc. from hundreds of different brands."
  },
  "cvpr2019_ffss-usad_adetect-then-retrievemodelformulti-domainfashionitemretrieval": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "FFSS-USAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - FFSS-USAD",
    "title": "A Detect-Then-Retrieve Model for Multi-Domain Fashion Item Retrieval",
    "authors": [
      "Michal Kucer",
      "Naila Murray"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/FFSS-USAD/Kucer_A_Detect-Then-Retrieve_Model_for_Multi-Domain_Fashion_Item_Retrieval_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/FFSS-USAD/Kucer_A_Detect-Then-Retrieve_Model_for_Multi-Domain_Fashion_Item_Retrieval_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Street-to-Shop fashion item retrieval is an instance-level image retrieval task in which a photo from a user is used to query a fashion image database in order to retrieve either the same or similar fashion items. This task is particularly challenging due to the domain shift between database photos, which tend to be stages, professional shots, and consumer photos that have a much greater variety in terms of quality, pose, etc. To reduce the problem difficulty, state-of-the-art approaches train one retrieval model per domain or fashion item category. In this work we propose a single detect-then-retrieve model that can be applied to any (query or database) image and which outperforms methods using domain or category-specific retrieval models by significant margins on the Exact Street2Shop benchmark dataset."
  },
  "cvpr2019_pbvs_segmentationoflow-leveltemporalplumepatternsfromirvideo": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Segmentation of Low-Level Temporal Plume Patterns From IR Video",
    "authors": [
      "Rajeev Bhatt",
      "M. Gokhan Uzunbas",
      "Thai Hoang",
      "Ozge C. Whiting"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Bhatt_Segmentation_of_Low-Level_Temporal_Plume_Patterns_From_IR_Video_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Bhatt_Segmentation_of_Low-Level_Temporal_Plume_Patterns_From_IR_Video_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, a method to segment out gas or steam plumes in IR videos collected from fixed cameras is presented. We propose a spatio-temporal U-Net architecture that captures deforming blobs of gas/steam plumes that have a unique temporal signature. In this task, the blob shapes are not semantically meaningful and change from frame to frame with no consistency across different exemplar plumes; however, there is spatial and temporal continuity in the way blobs deform suggesting a need for a low-level spatio-temporal segmentation network. The proposed method is compared to an LSTM-based segmentation network on a challenging IR video dataset collected in a controlled environment. In the controlled dataset there is motion due to steam plumes with deforming blob patterns as well as due to walking people with more structured high-level patterns. The experiments show that plume patterns are successfully segmented out with no confusion to moving people and the proposed spatiotemporal U-Net outperforms LSTM-based network in terms of pixelwise accuracy of output masks."
  },
  "cvpr2019_pbvs_comparingtheeffectsofannotationtypeonmachinelearningdetectionperformance": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Comparing the Effects of Annotation Type on Machine Learning Detection Performance",
    "authors": [
      "James F. Mullen Jr.",
      "Franklin R. Tanner",
      "Phil A. Sallee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Mullen_Comparing_the_Effects_of_Annotation_Type_on_Machine_Learning_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Mullen_Comparing_the_Effects_of_Annotation_Type_on_Machine_Learning_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The most prominent machine learning (ML) methods in use today are supervised, meaning they require groundtruth labeling of the data on which they are trained. Annotating data is arduous and expensive. Additionally, data sets for image object detection may be annotated by drawing polygons, drawing bounding boxes, or providing single points on targets. Selection of annotation technique is a tradeoff between time to annotate and accuracy of the annotation. When annotating a dataset for machine object recognition algorithms, researchers may not know the most advantageous method of annotation for their experiments. This paper evaluates the performance tradeoffs of three alternative methods of annotating imagery for use in ML. A neural network was trained using the different types of annotations and compares the detection accuracy of and differences between the resultant models. In addition to the accuracy, cost is analyzed for each of the models and respective datasets."
  },
  "cvpr2019_pbvs_siamesecnnsforrgb-lwirdisparityestimation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Siamese CNNs for RGB-LWIR Disparity Estimation",
    "authors": [
      "David-Alexandre Beaupre",
      "Guillaume-Alexandre Bilodeau"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Beaupre_Siamese_CNNs_for_RGB-LWIR_Disparity_Estimation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Beaupre_Siamese_CNNs_for_RGB-LWIR_Disparity_Estimation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Currently, for the task of color (RGB) and thermal in- frared (LWIR) disparity estimation, handcrafted feature descriptors such as mutual information are the methods achieving best performance. In this work, we aim to as- sess if convolutional neural networks (CNNs) can achieve competitive performance in this task. We developed an ar- chitecture made of two subnetworks, each consisting of the same siamese network, but taking different image patches as input. Each siamese network, in the feature space, searches for the disparity between the left and right patch. The out- put of the two subnetworks are summed together so that we can be more confident in the predicted disparity by enforc- ing left-right consistency. We show that having two subnet- works working together in parallel to get the final prediction helps achieve better performance when compared to a sin- gle subnetwork by itself. We tested our method on the LITIV dataset and found the results competitive when compared to handcrafted feature descriptors. The source code of our method will be available online upon publication."
  },
  "cvpr2019_pbvs_imagerecoveryintheinfrareddomainviapath-augmentedcompressivesamplingmatchingpursuit": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Image Recovery in the Infrared Domain via Path-Augmented Compressive Sampling Matching Pursuit",
    "authors": [
      "Tegan H. Emerson",
      "Colin C. Olson",
      "Anthony Lutz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Emerson_Image_Recovery_in_the_Infrared_Domain_via_Path-Augmented_Compressive_Sampling_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Emerson_Image_Recovery_in_the_Infrared_Domain_via_Path-Augmented_Compressive_Sampling_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We consider compressive sensing as a means of acquiring high-resolution images from low-cost, low-resolution sensors in the infrared domain. In particular, we reduce errors arising from basis mismatch between the observed image and the signal model by modifying a baseline matching pursuit recovery algorithm. Specifically, we introduce a modification to the analysis step which seeks to find more representative image atoms by searching over a 2-Wasserstein geodesic formed between the two most-correlated atoms at that step. We test our extension by quantifying recovery performance on an ensemble of representative infrared maritime scenes and find improvement over baseline when measured using PSNR, SSIM, and a metric that quantifies global edge recovery performance. We find that the most notable gains occur for very low sparsity levels which favors reduced computational load for the recovery."
  },
  "cvpr2019_pbvs_channelattentionnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Channel Attention Networks",
    "authors": [
      "Alexei A. Bastidas",
      "Hanlin Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Bastidas_Channel_Attention_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Bastidas_Channel_Attention_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Multi-band images beyond RGB are becoming popular in both commercial applications and research datasets, yet existing deep learning models were designed for academic RGB datasets. In this talk, we propose Channel Attention Networks (CAN), a deep learning model that uses soft attention on individual channels. We jointly train this model end-to-end on Spacenet, a challenging multi-spectral semantic segmentation dataset. In a comparative study, CAN outperforms previous models. We also demonstrate that CAN is significantly more robust to noise in individual bands than the other models, because the attention network allocates attention away from the noisy channels. Our proposed method marks the first step in designing deep learning algorithms specifically for multi-spectral imagery."
  },
  "cvpr2019_pbvs_sur-realfrechetmeananddistancetransformforcomplex-valueddeeplearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Sur-Real: Frechet Mean and Distance Transform for Complex-Valued Deep Learning",
    "authors": [
      "Rudrasis Chakraborty",
      "Jiayun Wang",
      "Stella X. Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Chakraborty_Sur-Real_Frechet_Mean_and_Distance_Transform_for_Complex-Valued_Deep_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Chakraborty_Sur-Real_Frechet_Mean_and_Distance_Transform_for_Complex-Valued_Deep_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We develop a novel deep learning architecture for naturally complex valued data, which are often subject to complex scaling ambiguity. We treat each sample as a field in the space of complex numbers.With the polar form of a complex number, the general group that acts on this space is the product of planar rotation and non-zero scaling. This perspective allows us to develop not only a novel convoluation operator using weighted Frechet mean (wFM) on a Riemannian manifold, but also to a novel fully connected layer operator using the distance to the wFM, with natural equivariant properties to non-zero scaling and planar rotations for the former and invariance properites for the latter.We demonstrate our method on widely used SAR dataset MSTAR and RadioML dataset.On MSTAR data, without any preprocessing, our network can achieve 98% classification accuracy on this highly imbalanced dataset using only44,000 parameters, as opposed to 94% accuracy with more than 500,000 parameters with a baseline real-valued network on the two-channel real representation of the complex valued data. On RadioML data, we got comparable classification accuracy with the baseline with only using 10% of the parameters as the baseline model."
  },
  "cvpr2019_pbvs_variationallearningofbeta-liouvillehiddenmarkovmodelsforinfraredactionrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Variational Learning of Beta-Liouville Hidden Markov Models for Infrared Action Recognition",
    "authors": [
      "Samr Ali",
      "Nizar Bouguila"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Ali_Variational_Learning_of_Beta-Liouville_Hidden_Markov_Models_for_Infrared_Action_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Ali_Variational_Learning_of_Beta-Liouville_Hidden_Markov_Models_for_Infrared_Action_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Infrared (IR) images are characterized by a lower sensitivity to lighting conditions than the visible spectrum. This opens the door to relatively untapped research potential of automatic recognition systems that are robust to shadows and variability in illumination levels or appearance. IR action recognition (AR) is one such application. It remains a fairly unexplored domain in IR. As such, in this paper, we propose the use of hidden Markov models (HMM) for IR AR. We also derive the mathematical model for the variational learning of Beta-Liouville (BL) HMMs. Next, we present the results of the proposed model on the Infrared Action Recognition (InfAR) dataset. To the best of our knowledge, this is the first application of HMMs to AR in the IR domain, and the first application of the BL HMMs to AR. Experimental results demonstrate promising results using different features extracted from the InfAR dataset."
  },
  "cvpr2019_pbvs_sarimageclassificationusingfew-shotcross-domaintransferlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "SAR Image Classification Using Few-Shot Cross-Domain Transfer Learning",
    "authors": [
      "Mohammad Rostami",
      "Soheil Kolouri",
      "Eric Eaton",
      "Kyungnam Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Rostami_SAR_Image_Classification_Using_Few-Shot_Cross-Domain_Transfer_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Rostami_SAR_Image_Classification_Using_Few-Shot_Cross-Domain_Transfer_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Data-driven classification algorithms based on deep convolutional neural networks (CNNs) have reached human-level performance for many tasks withinElectro-Optical (EO)computer vision.Despite being the prevailing visual sensory data, EO imaging is not effective in applications such as environmental monitoring at extended periods,where data collection at occluded weather is necessary.Synthetic Aperture Radar (SAR) is an effective imagingtool to circumvent these limitations andcollect visual sensory information continually. However, replicating the success of deep learning on SAR domains is not straightforward. This is mainly because training deep networks requires huge labeled datasets anddata labeling is a lot more challenging in SAR domains. We develop an algorithm totransfer knowledge from EO domains to SAR domains to eliminate the need for huge labeled data points in the SAR domains. Our idea is to learn a shared domain-invariant embedding for cross-domain knowledge transfer such that the embedding is discriminative for two related EO and SAR tasks, while the latent data distributions for both domains remain similar. As a result, a classifier learned using mostly EOdata can generalize well on the related task for the EO domain."
  },
  "cvpr2019_pbvs_abenchmarkfordeeplearningbasedobjectdetectioninmaritimeenvironments": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "A Benchmark for Deep Learning Based Object Detection in Maritime Environments",
    "authors": [
      "Sebastian Moosbauer",
      "Daniel Konig",
      "Jens Jakel",
      "Michael Teutsch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Moosbauer_A_Benchmark_for_Deep_Learning_Based_Object_Detection_in_Maritime_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Moosbauer_A_Benchmark_for_Deep_Learning_Based_Object_Detection_in_Maritime_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Object detection in maritime environments is a rather unpopular topic in the field of computer vision. In contrast to object detection for automotive applications, no sufficiently comprehensive public benchmark exists. In this paper, we propose a benchmark that is based on the Singapore Maritime Dataset (SMD). This dataset provides Visual-Optical (VIS) and Near Infrared (NIR) videos along with annotations for object detection and tracking. We analyze the utilization of deep learning techniques and therefore evaluate two state-of-the-art object detection approaches for their applicability in the maritime domain: Faster R-CNN and Mask R-CNN. To train the Mask R-CNN including the instance segmentation branch, a novel algorithm for automated generation of instance segmentation labels is introduced. The obtained results show that the SMD is sufficient to be used for domain adaptation. The highest f-score is achieved with a fine-tuned Mask R-CNN. This is a benchmark that encourages reproducibility and comparability for object detection in maritime environments."
  },
  "cvpr2019_pbvs_generativeadversarialnetworksforspectralsuper-resolutionandbidirectionalrgb-to-multispectralmapping": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Generative Adversarial Networks for Spectral Super-Resolution and Bidirectional RGB-To-Multispectral Mapping",
    "authors": [
      "Kin Gwn Lore",
      "Kishore K. Reddy",
      "Michael Giering",
      "Edgar A. Bernal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Lore_Generative_Adversarial_Networks_for_Spectral_Super-Resolution_and_Bidirectional_RGB-To-Multispectral_Mapping_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Lore_Generative_Adversarial_Networks_for_Spectral_Super-Resolution_and_Bidirectional_RGB-To-Multispectral_Mapping_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Acquisition of multi- and hyperspectral imagery imposes significant requirements on the hardware capabilities of the sensors involved.In order to keep costs manageable, and due to limitations in the sensing technology, tradeoffs between the spectral and the spatial resolution of hyperspectral images are usually made.Such tradeoffs are usually not necessary when considering acquisition of traditional RGB imagery.We investigate the use of statistical learning, and in particular, of conditional generative adversarial networks (cGANs) to estimate mappings from three-channel RGB to 31-band multispectral imagery.We demonstrate the application of the proposed approach to (i) RGB-to-multispectral image mapping, (ii) spectral super-resolution of image data, and (iii) recovery of RGB imagery from multispectral data."
  },
  "cvpr2019_pbvs_three-streamconvolutionalneuralnetworkwithmulti-taskandensemblelearningfor3dactionrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Three-Stream Convolutional Neural Network With Multi-Task and Ensemble Learning for 3D Action Recognition",
    "authors": [
      "Duohan Liang",
      "Guoliang Fan",
      "Guangfeng Lin",
      "Wanjun Chen",
      "Xiaorong Pan",
      "Hong Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Liang_Three-Stream_Convolutional_Neural_Network_With_Multi-Task_and_Ensemble_Learning_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Liang_Three-Stream_Convolutional_Neural_Network_With_Multi-Task_and_Ensemble_Learning_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose a three-stream convolutional neural network (3SCNN) for action recognition from skeleton sequences, which aims to thoroughly and fully exploit the skeleton data by extracting, learning, fusing and inferring multiple motion-related features, including 3D joint positions and joint displacements across adjacent frames as well as oriented bone segments. The proposed 3SCNN involves three sequential stages. The first stage enriches three independently extracted features by co-occurrence feature learning. The second stage involves multi-channel pairwise fusion to take advantage of the complementary and diverse nature among three features. The third stage is a multi-task and ensemble learning network to further improve the generalization ability of 3SCNN. Experimental results on the standard dataset show the effectiveness of our proposed multi-stream feature learning, fusion and inference method for skeleton-based 3D action recognition."
  },
  "cvpr2019_pbvs_in-vehicleoccupancydetectionwithconvolutionalnetworksonthermalimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "In-Vehicle Occupancy Detection With Convolutional Networks on Thermal Images",
    "authors": [
      "Farzan Erlik Nowruzi",
      "Wassim A. El Ahmar",
      "Robert Laganiere",
      "Amir H. Ghods"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Nowruzi_In-Vehicle_Occupancy_Detection_With_Convolutional_Networks_on_Thermal_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Nowruzi_In-Vehicle_Occupancy_Detection_With_Convolutional_Networks_on_Thermal_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Counting people is a growing field of interest for researchers in recent years. In-vehicle passenger counting is an interesting problem in this domain that has several applications including High Occupancy Vehicle (HOV) lanes. In this paper, present a new in-vehicle thermal image dataset. We propose a tiny convolutional model to count on-board passengers and compare it to well known methods. We show that our model surpasses state-of-the-art methods in classification and has comparable performance in detection. Moreover, our model outperforms the state-of-the-art architectures in terms of speed, making it suitable for deployment on embedded platforms. We present the results of multiple deep learning models and thoroughly analyze them."
  },
  "cvpr2019_pbvs_filterguidedmanifoldoptimizationintheautoencoderlatentspace": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Filter Guided Manifold Optimization in the Autoencoder Latent Space",
    "authors": [
      "Nate Lannan",
      "Guoliang Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Lannan_Filter_Guided_Manifold_Optimization_in_the_Autoencoder_Latent_Space_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Lannan_Filter_Guided_Manifold_Optimization_in_the_Autoencoder_Latent_Space_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " An autoencoder is a class of neural network that is trained to output an accurate reproduction of the input while learning key lower dimensional features, otherwise known as a manifold.A lower dimensional representation of the original input, referred to as the latent space, encodes the intrinsic data structure over the manifold. This paper proposes filter-guided manifold optimization in the latent space of a convolutional autoencoder torecover noisy motion data collected by a depth sensor. Autoencoder output is smoothed using four traditional filters and employed as target motion data in an objective function. The difference between the actual output and target is minimized through stochastic gradient descent over the latent space, using manifold optimization to produce the expected smooth output. The advantage of this filter-guided approach over traditional filtering is that the resultant motion data still adheresto the manifold in the latent space learned by the autoencorder from training on motion data."
  },
  "cvpr2019_pbvs_hyperspectraldatatorelativelidardepthaninverseproblemforremotesensing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Hyperspectral Data to Relative Lidar Depth: An Inverse Problem for Remote Sensing",
    "authors": [
      "Savas Ozkan",
      "Gozde Bozdagi Akar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Ozkan_Hyperspectral_Data_to_Relative_Lidar_Depth_An_Inverse_Problem_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Ozkan_Hyperspectral_Data_to_Relative_Lidar_Depth_An_Inverse_Problem_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Hyperspectral data provides rich information about a scene in terms of spectral details since it encapsulates measurements/observations from a wide large range of spectrum. To this end, it has been used in different problems mostly related to identification and detection processes.However, the main limitation arises for the accessibility of data. More precisely, there is no sufficient amount of hyperspectral data available compared to visible range data for trainable models. In this paper, we tackle an inverse problem to estimate the relative lidar depth from hyperspectral data. To solve its limitation, we integrate semantic information existed in data with supervised labels to decrease the possibility of parameter overfitting. Moreover, details of the output responses are enhanced with Laplacian pyramids and attention layers in which the model makes predictions from each subsequent scale instead of a single shot prediction from the top of the model. In our experiments, we use the 2018 IEEE GRSS Data Fusion Challenge dataset. From the experimental results, we prove that use of hyperspectral data instead of visible range data improves the performance. Moreover, we show that results are significantly improved if a sparse set of depth measurements is used along with hyperspectral data. Lastly, the integration of semantic information to the solution yields more stable and better results compared to the baselines. "
  },
  "cvpr2019_pbvs_onlinereconstructionofindoorsceneswithlocalmanhattanframegrowing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Online Reconstruction of Indoor Scenes With Local Manhattan Frame Growing",
    "authors": [
      "Mahdi Yazdanpour",
      "Guoliang Fan",
      "Weihua Sheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Yazdanpour_Online_Reconstruction_of_Indoor_Scenes_With_Local_Manhattan_Frame_Growing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Yazdanpour_Online_Reconstruction_of_Indoor_Scenes_With_Local_Manhattan_Frame_Growing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose an efficient approach for robust reconstruction of indoor scenes by taking advantage of the geometric relation between consecutive Manhattan keyframes and local pose refinement to improve the accuracy and fidelity of the reconstructed models. At the core of our framework, we have a Local Manhattan Frame Growing system, which finds the principal directions of the scene and aligns point clouds with the dominant plane, and a Local Pose Optimization, which refines the pose estimation for a specific range of frames. During the reconstruction process, we use Manhattan keyframes for a planar pre-alignment to provide a robust initialization for the final surface registration. All Manhattan keyframes are integrated using a frame-to-model scheme to create local models based on the refined camera poses. The final dense model is reconstructed by adopting a geometric registration between local segments and integrating them into a global frame. The experimental results show the effectiveness of our approach to reduce the cumulative registration error and overall geometric drift."
  },
  "cvpr2019_pbvs_colorizingnearinfraredimagesthroughacyclicadversarialapproachofunpairedsamples": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Colorizing Near Infrared Images Through a Cyclic Adversarial Approach of Unpaired Samples",
    "authors": [
      "Armin Mehri",
      "Angel D. Sappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Mehri_Colorizing_Near_Infrared_Images_Through_a_Cyclic_Adversarial_Approach_of_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Mehri_Colorizing_Near_Infrared_Images_Through_a_Cyclic_Adversarial_Approach_of_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper presents a novel approach for colorizing near infrared (NIR) images. The approach is based on image-to-image translation using a Cycle-Consistent adversarial network for learning the color channels on unpaired dataset. This architecture is able to handle unpaired datasets. The approach uses as generators tailored networks that require less computation times, converge faster, less sensitive to hyper-parameters' selection and generate high quality samples. The obtained results have been quantitatively---using standard evaluation metrics---and qualitatively evaluated showing considerable improvements with respect to the state of the art."
  },
  "cvpr2019_pbvs_anexaminationofdeep-learningbasedlandmarkdetectionmethodsonthermalfaceimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "An Examination of Deep-Learning Based Landmark Detection Methods on Thermal Face Imagery",
    "authors": [
      "Domenick Poster",
      "Shuowen Hu",
      "Nasser Nasrabadi",
      "Benjamin Riggan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Poster_An_Examination_of_Deep-Learning_Based_Landmark_Detection_Methods_on_Thermal_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Poster_An_Examination_of_Deep-Learning_Based_Landmark_Detection_Methods_on_Thermal_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Thermal-to-visible face verification algorithms commonly require pre-aligned images.However, thermal images with their low contrast, low resolution, and lack of textural information have proven a challenging obstacle for the detection of the fiducial landmarks used for image alignment. This paper studies the ability of modern landmark detection algorithms to cope with the adversarial conditions present in the thermal domain by exploring the strengths and weaknesses of three deep-learning based landmark detection architectures originally developed for visible images: the Deep Alignment Network (DAN), Multi-task Convolutional Neural Network (MTCNN), and a Multi-class Patch-based fully-convolutional neural network (PBC). Our experiments yield a normalized mean squared error of 0.04 at an offset distance of 2.5 meters using the DAN architecture, indicating an ability for cascaded shape regression neural networks to adapt to thermal images. However, we find that even small alignment errors disproportionately reduce correct recognition rates.With images aligned using the best performing model, an 8.2% drop in EER is observed as compared with ground truth alignments, leaving further room for improvement in this area. "
  },
  "cvpr2019_pbvs_pedestriandetectioninthermalimagesusingsaliencymaps": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Pedestrian Detection in Thermal Images Using Saliency Maps",
    "authors": [
      "Debasmita Ghose",
      "Shasvat M. Desai",
      "Sneha Bhattacharya",
      "Deep Chakraborty",
      "Madalina Fiterau",
      "Tauhidur Rahman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Ghose_Pedestrian_Detection_in_Thermal_Images_Using_Saliency_Maps_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Ghose_Pedestrian_Detection_in_Thermal_Images_Using_Saliency_Maps_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Thermal images are mainly used to detect the presence of people at night or in bad lighting conditions, but perform poorly at daytime. To solve this problem, most state-of-the-art techniques employ a fusion network that uses features from paired thermal and color images. Instead, we propose to augment thermal images with their saliency maps, to serve as an attention mechanism for the pedestrian detector especially during daytime. We investigate how such an approach results in improved performance for pedestrian detection using only thermal images, eliminating the need for paired color images. For our experiments, we train the Faster R-CNN for pedestrian detection and report the added effect of saliency maps generated using static and deep methods (PiCA-Net and R3-Net). Our best performing model results in an absolute reduction of miss rate by 13.4% and 19.4% over the baseline in day and night images respectively. We also annotate and release pixel level masks of pedestrians on a subset of the KAIST Multispectral Pedestrian Detection dataset, which is a first publicly available dataset for salient pedestrian detection."
  },
  "cvpr2019_pbvs_surrogatecontrastivenetworkforsupervisedbandselectioninmultispectralcomputervisiontasks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Surrogate Contrastive Network for Supervised Band Selection in Multispectral Computer Vision Tasks",
    "authors": [
      "Edgar A. Bernal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Bernal_Surrogate_Contrastive_Network_for_Supervised_Band_Selection_in_Multispectral_Computer_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Bernal_Surrogate_Contrastive_Network_for_Supervised_Band_Selection_in_Multispectral_Computer_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Computer vision techniques that operate on hyper- and multispectral imagery benefit from the additional amount of spectral information relative to those that exploit traditional RGB or monochromatic visual data.However, the increased volume of data to be processed brings about additional memory, storage and computational requirements.In order to address such limitations, a wide range of techniques for dimensionality reduction have been introduced by previous work.In this paper, we propose a framework for spectral band selection that is highly data- and computationally efficient.The method leverages a convolutional siamese network learned by optimizing a contrastive loss, and performs band selection based on the low-dimensional data embeddings produced by the network.We empirically demonstrate the efficacy of the method on an object detection task from aerial multispectral imagery.The results show that, in spite of the method's frugality, it produces very competitive band selection results against the evaluated competing techniques."
  },
  "cvpr2019_pbvs_dualgraphicalmodelsforrelationalmodelingofindoorobjectcategories": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Dual Graphical Models for Relational Modeling of Indoor Object Categories",
    "authors": [
      "Lin Guo",
      "Guoliang Fan",
      "Weihua Sheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Guo_Dual_Graphical_Models_for_Relational_Modeling_of_Indoor_Object_Categories_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Guo_Dual_Graphical_Models_for_Relational_Modeling_of_Indoor_Object_Categories_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " There are three levels for indoor scene understanding, pixel level labeling, object level recognition and scene level holistic understanding. The three levels provide complementary bottom-up scene representation. Traditional research often addresses these three tasks separately where the three levels of semantic data are seldom jointly considered. We propose a new method to bridge the three semantic levels by using dual graphical models for relational modeling of object categories in indoor scenes. The vertical placement model captures top-down object configuration by which the visible pixels of some accessory objects could be used to infer the presence of a supportive object underneath. The horizontal placement model reveals how multiple object categories are related to each other on the ground in different indoor scenes. The experimental results show improvements on the bounding box accuracy using both vertical and horizontal placement models from pixel level labeling. "
  },
  "cvpr2019_pbvs_imagevegetationindexthroughacyclegenerativeadversarialnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Image Vegetation Index Through a Cycle Generative Adversarial Network",
    "authors": [
      "Patricia L. Suarez",
      "Angel D. Sappa",
      "Boris X. Vintimilla",
      "Riad I. Hammoud"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Suarez_Image_Vegetation_Index_Through_a_Cycle_Generative_Adversarial_Network_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Suarez_Image_Vegetation_Index_Through_a_Cycle_Generative_Adversarial_Network_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper proposes a novel approach to estimate the Normalized Difference Vegetation Index (NDVI) just from an RGB image. The NDVI values are obtained by using images fromthevisible spectral band together with a synthetic near infrared image obtained by a cycled GAN. The cycled GAN network is able to obtain a NIR image from a given gray scale image. It is trained by using unpaired set of gray scale and NIR images by using a U-net architecture and a multiple loss function (gray scale images are obtained from the provided RGB images). Then, the NIR image estimated with the proposed cycle generative adversarial network is used to compute the NDVI index. Experimental results are provided showing the validity of the proposed approach. Additionally, comparisons with previous approaches are also provided."
  },
  "cvpr2019_pbvs_mu-netdeeplearning-basedthermalirimageestimationfromrgbimage": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "MU-Net: Deep Learning-Based Thermal IR Image Estimation From RGB Image",
    "authors": [
      "Yumi Iwashita",
      "Kazuto Nakashima",
      "Sir Rafol",
      "Adrian Stoica",
      "Ryo Kurazume"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Iwashita_MU-Net_Deep_Learning-Based_Thermal_IR_Image_Estimation_From_RGB_Image_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Iwashita_MU-Net_Deep_Learning-Based_Thermal_IR_Image_Estimation_From_RGB_Image_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Terrain imagery collected by satellite remote sensing or by rover on-board sensors is the primary source for terrain classification used in determining terrain traversibility and mission plans for planetary rovers. Mapping models between RGB and IR for terrain classes are learned from real RGB and IR data examples in the same or similar terrain.This paper adds a new class of deep learning architectures called MU-Net (Multiple U-Net) and shows its efficiency in deriving better RGB-to-IR mapping models, improving over past work the estimation of thermal IR images from incoming RGB images and learned RGB-IR mappings."
  },
  "cvpr2019_pbvs_borrowfromanywherepseudomulti-modalobjectdetectioninthermalimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Perception Beyond the Visible Spectrum workshop",
    "title": "Borrow From Anywhere: Pseudo Multi-Modal Object Detection in Thermal Imagery",
    "authors": [
      "Chaitanya Devaguptapu",
      "Ninad Akolekar",
      "Manuj M Sharma",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/PBVS/Devaguptapu_Borrow_From_Anywhere_Pseudo_Multi-Modal_Object_Detection_in_Thermal_Imagery_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/PBVS/Devaguptapu_Borrow_From_Anywhere_Pseudo_Multi-Modal_Object_Detection_in_Thermal_Imagery_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Can we improve detection in the thermal domain by borrowing features from rich domains like visual RGB? In this paper, we propose a pseudo-multimodal object detector trained on natural image domain data to help improve the performance of object detection in thermal images. We assume access to a large-scale dataset in the visual RGB domain and relatively smaller dataset (in terms of instances) in the thermal domain, as is common today. We propose the use of well-known image-to-image translation frameworks to generate pseudo-RGB equivalents of a given thermal image and then use a multi-modal architecture for object detection in the thermal image. We show that our framework outperforms existing benchmarks without the explicit need for paired training examples from the two domains. We also show that our framework has the ability to learn with less data from thermal domain when using our approach. "
  },
  "cvpr2019_evw_condensation-netmemory-efficientnetworkarchitecturewithcross-channelpoolinglayersandvirtualfeaturemaps": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Embedded Vision Workshop",
    "title": "Condensation-Net: Memory-Efficient Network Architecture With Cross-Channel Pooling Layers and Virtual Feature Maps",
    "authors": [
      "Tse-Wei Chen",
      "Motoki Yoshinaga",
      "Hongxing Gao",
      "Wei Tao",
      "Dongchao Wen",
      "Junjie Liu",
      "Kinya Osa",
      "Masami Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EVW/Chen_Condensation-Net_Memory-Efficient_Network_Architecture_With_Cross-Channel_Pooling_Layers_and_Virtual_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EVW/Chen_Condensation-Net_Memory-Efficient_Network_Architecture_With_Cross-Channel_Pooling_Layers_and_Virtual_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " \"Lightweight convolutional neural networks\" is an important research topic in the field of embedded vision. To implement image recognition tasks on a resource-limited hardware platform, it is necessary to reduce the memory size and the computational cost. The contribution of this paper is stated as follows. First, we propose an algorithm to process a specific network architecture (Condensation-Net) without increasing the maximum memory storage for feature maps. The architecture for virtual feature maps saves 26.5% of memory bandwidth by calculating the results of cross-channel pooling before storing the feature map into the memory. Second, we show that cross-channel pooling can improve the accuracy of object detection tasks, such as face detection, because it increases the number of filter weights. Compared with Tiny-YOLOv2, the improvement of accuracy is 2.0% for quantized networks and 1.5% for full-precision networks when the false-positive rate is 0.1. Last but not the least, the analysis results show that the overhead to support the cross-channel pooling with the proposed hardware architecture is negligible small. The extra memory cost to support Condensation-Net is 0.2% of the total size, and the extra gate count is only 2.1% of the total size."
  },
  "cvpr2019_evw_leveragingconfidentpointsforaccuratedepthrefinementonembeddedsystems": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Embedded Vision Workshop",
    "title": "Leveraging Confident Points for Accurate Depth Refinement on Embedded Systems",
    "authors": [
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EVW/Tosi_Leveraging_Confident_Points_for_Accurate_Depth_Refinement_on_Embedded_Systems_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EVW/Tosi_Leveraging_Confident_Points_for_Accurate_Depth_Refinement_on_Embedded_Systems_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Despite the notable progress in stereo disparity estimation, algorithms are still prone to errors in challenging conditions. Thus, heuristic disparity refinement techniques are usually deployed to improve accuracy. Moreover, state-of-the-art methods rely on complex CNNs requiring power hungry GPUs not suited for many practical applications constrained by limited computing resources.In this paper, we propose a novel technique for disparity refinement leveraging on confidence measures and a novel, automatic learning-based selection method to discard outliers. Then, a non-local strategy infers missing disparities by analyzing the closest reliable points. This framework is very fast and does not require any hand-tuned thresholding. We assess the performance of our Non-Local Anchoring (NLA), standalone refinement techniques and methods leveraging on confidence measures inside the stereo algorithm. Our evaluation with two popular stereo algorithms shows that our proposal significantly ameliorates their accuracy on Middlebury v3 and KITTI 2015 datasets. Moreover, since our method relies only on cues computed in the disparity domain, it is suited even for COTS stereo cameras coupled with embedded systems, e.g. nVidia Jetson TX2. "
  },
  "cvpr2019_evw_dupnettowardsverytinyquantizedcnnwithimprovedaccuracyforfacedetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Embedded Vision Workshop",
    "title": "DupNet: Towards Very Tiny Quantized CNN With Improved Accuracy for Face Detection",
    "authors": [
      "Hongxing Gao",
      "Wei Tao",
      "Dongchao Wen",
      "Junjie Liu",
      "Tse-Wei Chen",
      "Kinya Osa",
      "Masami Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EVW/Gao_DupNet_Towards_Very_Tiny_Quantized_CNN_With_Improved_Accuracy_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EVW/Gao_DupNet_Towards_Very_Tiny_Quantized_CNN_With_Improved_Accuracy_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deploying deep learning based face detectors on edge devices is a challenging task due to the limited computation resources. Even though binarizing the weights of a very tiny network gives impressive compactness on model size (e.g. 240.9 KB for IFQ-Tinier-YOLO), it is not tiny enough to fit in the embedded devices with strict memory constraints. In this paper, we propose DupNet which consists of two parts. Firstly, we employ weights with duplicated channels for the weight-intensive layers to reduce the model size. Secondly, for the quantization-sensitive layers whose quantization causes notable accuracy drop, we duplicate its input feature maps. It allows us to use more weights channels for convolving more representative outputs. Based on that, we propose a very tiny face detector, DupNet-Tinier-YOLO, which is 6.5x times smaller on model size and 42.0% less complex on computation and meanwhile 2.4% higher detection than IFQ-Tinier-YOLO. Comparing with the full precision Tiny-YOLO, our DupNet-Tinier-YOLO gives 1,694.2x and 389.9x times savings on model size and computation complexity respectively with only 4.0% drop on detection rate (0.880 vs. 0.920). Moreover, our DupNet-Tinier-YOLO is only 36.9 KB, which is the tiniest deep face detector to our best knowledge."
  },
  "cvpr2019_befa_facegenderidexploitinggenderinformationindcnnsfacerecognitionsystems": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BEFA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bias Estimation in Face Analytics (BEFA)",
    "title": "FaceGenderID: Exploiting Gender Information in DCNNs Face Recognition Systems",
    "authors": [
      "Ruben Vera-Rodriguez",
      "Marta Blazquez",
      "Aythami Morales",
      "Ester Gonzalez-Sosa",
      "Joao C. Neves",
      "Hugo Proenca"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BEFA/Vera-Rodriguez_FaceGenderID_Exploiting_Gender_Information_in_DCNNs_Face_Recognition_Systems_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BEFA/Vera-Rodriguez_FaceGenderID_Exploiting_Gender_Information_in_DCNNs_Face_Recognition_Systems_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper addresses the effect of gender as a covariate in face verification systems. Even though pre-trained models based on Deep Convolutional Neural Networks (DCNNs), such as VGG-Face or ResNet-50, achieve very high performance, they are trained on very large datasets comprising millions of images, which have biases regarding demographic aspects like the gender and the ethnicity among others. In this work, we first analyse the separate performance of these state-of-the-art models for males and females. We observe a gap between face verification performances obtained by both gender classes. These results suggest that features obtained by biased models are affected by the gender covariate. We propose a gender-dependent training approach to improve the feature representation for both genders, and develop both: i) gender specific DCNNs models, and ii) a gender balanced DCNNs model. Our results show significant and consistent improvements in face verification performance for both genders, individually and in general with our proposed approach. Finally, we announce the availability (at GitHub) of the FaceGenderID DCNNs models proposed in this work, which can support further experiments on this topic."
  },
  "cvpr2019_befa_analyzingandreducingthedamageofdatasetbiastofacerecognitionwithsyntheticdata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BEFA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bias Estimation in Face Analytics (BEFA)",
    "title": "Analyzing and Reducing the Damage of Dataset Bias to Face Recognition With Synthetic Data",
    "authors": [
      "Adam Kortylewski",
      "Bernhard Egger",
      "Andreas Schneider",
      "Thomas Gerig",
      "Andreas Morel-Forster",
      "Thomas Vetter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BEFA/Kortylewski_Analyzing_and_Reducing_the_Damage_of_Dataset_Bias_to_Face_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BEFA/Kortylewski_Analyzing_and_Reducing_the_Damage_of_Dataset_Bias_to_Face_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " It is well known that deep learning approaches to face recognition suffer from various biases in the available training data.In this work, we demonstrate the large potential of synthetic data for analyzing and reducing the negative effects of dataset bias on deep face recognition systems.In particular we explore two complementary application areas for synthetic face images:1) Using fully annotated synthetic face images we can study the face recognition rate as a function of interpretable parameters such as face pose.This enables us to systematically analyze the effect of different types of dataset biases on the generalization ability of neural network architectures. Our analysis reveals that deeper neural network architectures can generalize better to unseen face poses. Furthermore, our study shows that current neural network architectures cannot disentangle face pose and facial identity, which limits their generalization ability. 2) We pre-train neural networks with large-scale synthetic data that is highly variable in face pose and the number of facial identities. After a subsequent fine-tuning with real-world data, we observe that the damage of dataset bias in the real-world data is largely reduced. Furthermore, we demonstrate that the size of real-world datasets can be reduced by 75% while maintaining competitive face recognition performance. The data and software used in this work are publicly available."
  },
  "cvpr2019_befa_facerecognitionalgorithmbiasperformancedifferencesonimagesofchildrenandadults": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BEFA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bias Estimation in Face Analytics (BEFA)",
    "title": "Face Recognition Algorithm Bias: Performance Differences on Images of Children and Adults",
    "authors": [
      "Nisha Srinivas",
      "Karl Ricanek",
      "Dana Michalski",
      "David S. Bolme",
      "Michael King"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BEFA/Srinivas_Face_Recognition_Algorithm_Bias_Performance_Differences_on_Images_of_Children_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BEFA/Srinivas_Face_Recognition_Algorithm_Bias_Performance_Differences_on_Images_of_Children_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work, we examine if current state-of-the-art deep learning face recognition systems exhibit a negative bias (i.e., poorer performance) for children when compared to the performance obtained on adults. The systems selected for this work are five top performing commercial-off-the-shelf face recognition systems, two government-off-the-shelf face recognition systems and one open-source face recognition solution. The datasets used to evaluate the performance of the systems are both unconstrained in age, pose, illumination, and expression and are publicly available. These datasets are indicative of photo journalistic face datasets published and evaluated on over the last few years. Our findings show a negative bias for each algorithm on children. Genuine and imposter distributions highlight the performance bias between the datasets further supporting the need for a deeper investigation into algorithm bias as a function of age cohorts. To combat the performance decline on the child demographic, several score-level fusion strategies were evaluated. This work identifies the best score-level fusion technique for the child demographic."
  },
  "cvpr2019_befa_characterizingthevariabilityinfacerecognitionaccuracyrelativetorace": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BEFA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bias Estimation in Face Analytics (BEFA)",
    "title": "Characterizing the Variability in Face Recognition Accuracy Relative to Race",
    "authors": [
      "Krishnapriya K. S",
      "Kushal Vangara",
      "Michael C. King",
      "Vitor Albiero",
      "Kevin Bowyer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BEFA/S_Characterizing_the_Variability_in_Face_Recognition_Accuracy_Relative_to_Race_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BEFA/S_Characterizing_the_Variability_in_Face_Recognition_Accuracy_Relative_to_Race_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Many recent news headlines have labeled face recognition technology as \"biased\" or \"racist\". We report on a methodical investigation into differences in face recognition accuracy between African-American and Caucasian image cohorts of the MORPH dataset. We find that, for all four matchers considered, the impostor and the genuine distributions are statistically significantly different between cohorts. For a fixed decision threshold, the African-American image cohort has a higher false match rate and a lower false non-match rate. ROC curves compare verification rates at the same false match rate, but the different cohorts achieve the same false match rate at different thresholds. This means that ROC comparisons are not relevant to operational scenarios that use a fixed decision threshold. We show that, for the ResNet matcher, the two cohorts have approximately equal separation of impostor and genuine distributions. Using ICAO compliance as a standard of image quality, we find that the initial image cohorts have unequal rates of good quality images. The ICAO-compliant subsets of the original image cohorts show improved accuracy, with the main effect being to reducing the low-similarity tail of the genuine distributions."
  },
  "cvpr2019_befa_color-theoreticexperimentstounderstandunequalgenderclassificationaccuracyfromfaceimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BEFA",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bias Estimation in Face Analytics (BEFA)",
    "title": "Color-Theoretic Experiments to Understand Unequal Gender Classification Accuracy From Face Images",
    "authors": [
      "Vidya Muthukumar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BEFA/Muthukumar_Color-Theoretic_Experiments_to_Understand_Unequal_Gender_Classification_Accuracy_From_Face_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BEFA/Muthukumar_Color-Theoretic_Experiments_to_Understand_Unequal_Gender_Classification_Accuracy_From_Face_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent work shows unequal performance of commercial face classification services in the gender classification task across intersectional groups defined by skin type and gender. Accuracy on dark-skinned females is significantly worse than on any other group. We provide initial evidence that skin type alone is not the driver for this disparity by conducting novel stability experiments that vary an image's skin type via color-theoretic methods, namely luminance mode-shift and optimal transport. We evaluate the effect of skin type change on the gender classification decision of a pair of state-of-the-art commercial and open-source gender classifiers. The results raise the possibility that broader differences in ethnicity, as opposed to the skin type alone, are what contribute to unequal gender classification accuracy in face images."
  },
  "cvpr2019_earthvision_whenafewclicksmakeallthedifferenceimprovingweakly-supervisedwildlifedetectioninuavimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "When a Few Clicks Make All the Difference: Improving Weakly-Supervised Wildlife Detection in UAV Images",
    "authors": [
      "Benjamin Kellenberger",
      "Diego Marcos",
      "Devis Tuia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Kellenberger_When_a_Few_Clicks_Make_All_the_Difference_Improving_Weakly-Supervised_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Kellenberger_When_a_Few_Clicks_Make_All_the_Difference_Improving_Weakly-Supervised_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Automated object detectors on Unmanned Aerial Vehicles (UAVs) are increasingly employed for a wide range of tasks. However, to be accurate in their specific task they need expensive ground truth in the form of bounding boxes or positional information. Weakly-Supervised Object Detection (WSOD) overcomes this hindrance by localizing objects with only image-level labels that are faster and cheaper to obtain, but is not on par with fully-supervised models in terms of performance. In this study we propose to combine both approaches in a model that is principally apt for WSOD, but receives full position ground truth for a small number of images. Experiments show that with just 1% of densely annotated images, but simple image-level counts as remaining ground truth, we effectively match the performance of fully-supervised models on a challenging dataset with scarcely occurring wildlife on UAV images from the African savanna. As a result, with a very limited amount of precise annotations our model can be trained with ground truth that is orders of magnitude cheaper and faster to obtain while still providing the same detection performance."
  },
  "cvpr2019_earthvision_intrinsicscenepropertiesfromhyperspectralimagesandlidar": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "Intrinsic Scene Properties From Hyperspectral Images and LiDAR",
    "authors": [
      "Xudong Jin",
      "Yanfeng Gu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Jin_Intrinsic_Scene_Properties_From_Hyperspectral_Images_and_LiDAR_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Jin_Intrinsic_Scene_Properties_From_Hyperspectral_Images_and_LiDAR_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, a novel reflectance model is proposed to recover intrinsic images from remote sensing hyperspectral images (HSIs). Intrinsic image recovery is a well-known challenging and underconstrained problem in computer vision and it becomes even more severely ill posed for HSIs. To reduce the uncertainties and improve the recovery accuracy, two kinds of priors are introduced: 1) shading prior which describes the geometric relation between illuminate and object surface; 2) reflectance prior based on L1-graph coding, which describes the relation between pigment density with reflectance. These priors can effectively eliminate the reflectance inhomogeneity caused by surface normal changes or pigment density variations other than material changes. Then, a non-iterative optimization method is proposed to combine the shading prior and reflectance prior, with which closed-form solutions can be derived and thus avoided falling into local optimums. The experimental results demonstrate that the proposed method can efficiently improve the spectral reflectance homogeneity within a class while preserving the image boundaries; it also produces competitive performance with the state-of-art when utilizing the extracted intrinsic hyperspectral reflectance feature in the task of HSI classification."
  },
  "cvpr2019_earthvision_theeffectsofsuper-resolutiononobjectdetectionperformanceinsatelliteimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "The Effects of Super-Resolution on Object Detection Performance in Satellite Imagery",
    "authors": [
      "Jacob Shermeyer",
      "Adam Van Etten"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Shermeyer_The_Effects_of_Super-Resolution_on_Object_Detection_Performance_in_Satellite_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Shermeyer_The_Effects_of_Super-Resolution_on_Object_Detection_Performance_in_Satellite_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We explore the application of super-resolution techniques to satellite imagery, and the effects of these techniques on object detection algorithm performance. Specifically, we enhance satellite imagery beyond its native resolution, and test if we can identify various types of vehicles, planes, and boats with greater accuracy than native resolution. Using the Very Deep Super-Resolution (VDSR) framework and a custom Random Forest Super-Resolution (RFSR) framework we generate enhancement levels of 2x, 4x, and 8x over five distinct resolutions ranging from 30 cm to 4.8 meters. Using both native and super-resolved data, we then train several custom detection models using the SIMRDWN object detection framework. SIMRDWN combines a number of popular object detection algorithms (e.g. SSD, YOLO) into a unified framework that is designed to rapidly detect objects in large satellite images. This approach allows us to quantify the effects of super-resolution techniques on object detection performance across multiple classes and resolutions. We also quantify the performance of object detection as a function of native resolution and object pixel size. For our test set we note that performance degrades from mean average precision (mAP) = 0.53 at 30 cm resolution, down to mAP = 0.11 at 4.8 m resolution. Super-resolving native 30 cm imagery to 15 cm yields the greatest benefit; a 13-36% improvement in mAP. Super-resolution is less beneficial at coarser resolutions, though still provides a small improvement in performance."
  },
  "cvpr2019_earthvision_large-scaledtmgenerationfromsatellitedata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "Large-Scale DTM Generation From Satellite Data",
    "authors": [
      "Liuyun Duan",
      "Mathieu Desbrun",
      "Anne Giraud",
      "Frederic Trastour",
      "Lionel Laurore"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Duan_Large-Scale_DTM_Generation_From_Satellite_Data_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Duan_Large-Scale_DTM_Generation_From_Satellite_Data_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In remote sensing, Digital Terrain Models (DTM) generation is a long-standing problem involving bare-terrain extraction and surface reconstruction to estimate a DTM from a Digital Surface Model (DSM). Most existing methods (including commercial software packages) have difficulty handling large-scale satellite data of inhomogeneous quality and resolution, and often need an expert-driven manual parameter-tuning process for each geographical type of DSM. In this paper we propose an automated and versatile DTM generation method from satellite data that is perfectly suited to large-scale applications.A novel set of feature descriptors based on multiscale morphological analysis are first computed to extract reliable bare-terrain elevations from DSMs. This terrain extraction algorithm is robust to noise and adapts well to local reliefs in both flat and highly mountainous areas. Then, we reconstruct the final DTM mesh using relative coordinates with respect to the sparse elevations previously detected, and induce preservation of geometric details by adapting these coordinates based on local relief attributes. Experiments on worldwide DSMs show the potential of our approach for large-scale DTM generation without parameter tuning. Our system is flexible as well, as it allows for a straightforward integration of multiple external masks (e.g., forest, road line, buildings, lake, etc) to better handle complex cases, resulting in further improvements of the quality of the output DTM."
  },
  "cvpr2019_earthvision_urbansemantic3dreconstructionfrommultiviewsatelliteimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "Urban Semantic 3D Reconstruction From Multiview Satellite Imagery",
    "authors": [
      "Matthew J. Leotta",
      "Chengjiang Long",
      "Bastien Jacquet",
      "Matthieu Zins",
      "Dan Lipsa",
      "Jie Shan",
      "Bo Xu",
      "Zhixin Li",
      "Xu Zhang",
      "Shih-Fu Chang",
      "Matthew Purri",
      "Jia Xue",
      "Kristin Dana"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Leotta_Urban_Semantic_3D_Reconstruction_From_Multiview_Satellite_Imagery_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Leotta_Urban_Semantic_3D_Reconstruction_From_Multiview_Satellite_Imagery_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Methods for automated 3D urban modeling typically result in very dense point clouds or surface meshes derived from either overhead lidar or imagery (multiview stereo). Such models are very large and have no semantic separation of individual structures (i.e. buildings, bridges) from the terrain. Furthermore, such dense models often appear \"melted\" and do not capture sharp edges. This paper demonstrates an end-to-end system for segmenting buildings and bridges from terrain and estimating simple, low polygon, textured mesh models of these structures. The approach uses multiview-stereo satellite imagery as a starting point, but this work focuses on segmentation methods and regularized 3D surface extraction. Our work is evaluated on the IARPA CORE3D public data set using the associated ground truth and metrics. A web-based application deployed on AWS runs the algorithms and provides visualization of the results. Both the algorithms and web application are provided as open source software as a resource for further research or product development."
  },
  "cvpr2019_earthvision_guidedanisotropicdiffusionanditerativelearningforweaklysupervisedchangedetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "Guided Anisotropic Diffusion and Iterative Learning for Weakly Supervised Change Detection",
    "authors": [
      "Rodrigo Caye Daudt",
      "Bertrand Le Saux",
      "Alexandre Boulch",
      "Yann Gousseau"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Daudt_Guided_Anisotropic_Diffusion_and_Iterative_Learning_for_Weakly_Supervised_Change_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Daudt_Guided_Anisotropic_Diffusion_and_Iterative_Learning_for_Weakly_Supervised_Change_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Large scale datasets created from user labels or openly available data have become crucial to provide training data for large scale learning algorithms. While these datasets are easier to acquire, the data are frequently noisy and unreliable, which is motivating research on weakly supervised learning techniques. In this paper we propose an iterative learning method that extracts the useful information from a large scale change detection dataset generated from open vector data to train a fully convolutional network which surpasses the performance obtained by naive supervised learning. We also propose the guided anisotropic diffusion algorithm, which improves semantic segmentation results using the input images as guides to perform edge preserving filtering, and is used in conjunction with the iterative training method to improve results."
  },
  "cvpr2019_earthvision_lateorearlierinformationfusionfromdepthandspectraldata?large-scaledigitalsurfacemodelrefinementbyhybrid-cgan": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "Late or Earlier Information Fusion From Depth and Spectral Data? Large-Scale Digital Surface Model Refinement by Hybrid-CGAN",
    "authors": [
      "Ksenia Bittner",
      "Peter Reinartz",
      "Marco Korner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Bittner_Late_or_Earlier_Information_Fusion_From_Depth_and_Spectral_Data_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Bittner_Late_or_Earlier_Information_Fusion_From_Depth_and_Spectral_Data_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present the workflow of a digital surface model (DSM) refinement methodology using a Hybrid-cGAN where the generative part consists of two encoders and a common decoder which blends the spectral and height information within one network. The inputs to the Hybrid-cGAN are single-channel photogrammetric DSMs with continuous values and single-channel pan-chromatic (PAN) half-meter resolution satellite images. Experimental results demonstrate that the earlier information fusion from data with different physical meanings helps to propagate fine details and complete an inaccurate or missing 3D information about building forms. Moreover, it improves the building boundaries making them more rectilinear."
  },
  "cvpr2019_earthvision_weaklysupervisedfusionofmultipleoverheadimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EarthVision",
    "title": "Weakly Supervised Fusion of Multiple Overhead Images",
    "authors": [
      "Muhammad Usman Rafique",
      "Hunter Blanton",
      "Nathan Jacobs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EarthVision/Rafique_Weakly_Supervised_Fusion_of_Multiple_Overhead_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EarthVision/Rafique_Weakly_Supervised_Fusion_of_Multiple_Overhead_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This work addresses the problem of combining noisy overhead images to make a single high-quality image of a region. Existing fusion methods rely on supervised learning, which requires image quality annotations, or ad hoc criteria, which do not generalize well. We formulate a weakly supervised method, which learns to predict image quality at the pixel-level by optimizing for semantic segmentation.This means our method only requires semantic segmentation labels, not explicit artifact annotations in the input images.We evaluate our method under varying levels of occlusions and clouds.Experimental results show that our method is significantly better than a baseline fusion approach and nearly as good as the ideal case, a single noise-free image."
  },
  "cvpr2019_bic_m2u-neteffectiveandefficientretinalvesselsegmentationforreal-worldapplications": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bioimage Computing",
    "title": "M2U-Net: Effective and Efficient Retinal Vessel Segmentation for Real-World Applications",
    "authors": [
      "Tim Laibacher",
      "Tillman Weyde",
      "Sepehr Jalali"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BIC/Laibacher_M2U-Net_Effective_and_Efficient_Retinal_Vessel_Segmentation_for_Real-World_Applications_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BIC/Laibacher_M2U-Net_Effective_and_Efficient_Retinal_Vessel_Segmentation_for_Real-World_Applications_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present a novel neural network architecture for retinal vessel segmentation that improves over the state of the art on two benchmark datasets, is the first to run in real time on high resolution images, and its small memory and processing requirements make it deployable in mobile and embedded systems. The M2U-Net has a new encoder-decoder architecture that is inspired by the U-Net. It adds pretrained components of MobileNetV2 in the encoder part and novel contractive bottleneck blocks in the decoder part that, combined with bilinear upsampling, drastically reduce the parameter count to 0.55M compared to 31.03M in the original U-Net. We have evaluated its performance against a wide body of previously published results on three public datasets.On two of them, the M2U-Net achieves new state-of-the-art performance by a considerable margin. When implemented on a GPU, our method is the first to achieve real-time inference speeds on high-resolution fundus images. We also implemented our proposed network on an ARM-based embedded system where it segments images in between 0.6 and 15 sec, depending on the resolution.Thus, the M2U-Net enables a number of applications of retinal vessel structure extraction, such as early diagnosis of eye diseases, retinal biometric authentication systems, and robot assisted microsurgery."
  },
  "cvpr2019_bic_intersectiontooverpassinstancesegmentationonfilamentousstructureswithanorientation-awareneuralnetworkandterminuspairingalgorithm": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bioimage Computing",
    "title": "Intersection to Overpass: Instance Segmentation on Filamentous Structures With an Orientation-Aware Neural Network and Terminus Pairing Algorithm",
    "authors": [
      "Yi Liu",
      "Abhishek Kolagunda",
      "Wayne Treible",
      "Alex Nedo",
      "Jeffrey Caplan",
      "Chandra Kambhamettu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BIC/Liu_Intersection_to_Overpass_Instance_Segmentation_on_Filamentous_Structures_With_an_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BIC/Liu_Intersection_to_Overpass_Instance_Segmentation_on_Filamentous_Structures_With_an_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Filamentous structures play an important role in biological systems. Extracting individual filaments is fundamental for analyzing and quantifying related biological processes. However, segmenting filamentous structures at an instance level is hampered by their complex architecture, uniform appearance, and image quality. In this paper, we introduce an orientation-aware neural network, which contains six orientation-associated outputs layer. Each layer detects filaments with specific range of orientations, thus separating them at junctions, and turning intersections to overpasses. A terminus pairing algorithm is also proposed to regroup filaments from different layers, and achieve individual filaments extraction. We create a synthetic dataset to train our network, and annotate real full resolution microscopy images of microtubules to test our approach. Our experiments have shown that our proposed method outperforms most existing approaches for filaments extraction. We also show that our approach works on other similar structures with a road network dataset."
  },
  "cvpr2019_bic_surfaceparameterizationandregistrationforstatisticalmultiscaleatlasingoforgandevelopment": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bioimage Computing",
    "title": "Surface Parameterization and Registration for Statistical Multiscale Atlasing of Organ Development",
    "authors": [
      "Faical Selka",
      "Jasmine Burguet",
      "Eric Biot",
      "Thomas Blein",
      "Patrick Laufs",
      "Philippe Andrey"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BIC/Selka_Surface_Parameterization_and_Registration_for_Statistical_Multiscale_Atlasing_of_Organ_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BIC/Selka_Surface_Parameterization_and_Registration_for_Statistical_Multiscale_Atlasing_of_Organ_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " During organ development, morphological and topological changes jointly occur at the cellular and tissue levels. Hence, the systematic and integrative quantification of cellular parameters during growth is essential to better understand organogenesis. We developed an atlasing strategy to quantitatively map cellular parameters during organ growth. Our approach is based on the computation of prototypical shapes, which are average shapes of individual organs at successive developmental stages, whereupon statistical descriptors of cellular parameters measured from individual organs are projected.We describe here the algorithmic pipeline we developed for 3D organ shape registration, based on the establishment of an organ-centered coordinate system and on the automatic parameterization of organ surface. Using our framework, dynamic developmental trajectories can be readily reconstructed using point-to-point interpolation between parameterized organ surfaces at different time points. We illustrate and validate our pipeline using 3D confocal images of developing plant leaves."
  },
  "cvpr2019_bic_automatedsegmentationofthevocalfoldsinlaryngealendoscopyvideosusingdeepconvolutionalregressionnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Bioimage Computing",
    "title": "Automated Segmentation of the Vocal Folds in Laryngeal Endoscopy Videos Using Deep Convolutional Regression Networks",
    "authors": [
      "Ali Hamad",
      "Megan Haney",
      "Teresa E. Lever",
      "Filiz Bunyak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BIC/Hamad_Automated_Segmentation_of_the_Vocal_Folds_in_Laryngeal_Endoscopy_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BIC/Hamad_Automated_Segmentation_of_the_Vocal_Folds_in_Laryngeal_Endoscopy_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Swallowing and breathing are vital, life-sustaining upper airway functions that require precise, reciprocal coordination of the vocal folds (VFs). During swallowing, the VFs must fully close to prevent aspiration of food/liquid into the lungs, whereas during breathing, the VFs must remain open to prevent obstruction of airflow into and out of the lungs. This coordination may become impaired by a variety of neurological conditions and diseases. Clinical evaluation relies on transnasal endoscopy to visualize the VFs within the larynx, and subjective interpretation of VF function by clinicians. However, objective, quantitative, and high-throughput analysis of VF function is important for early diagnosis, monitoring disease progression, treatment monitoring, and treatment discovery. In this paper we propose a fully automated, deep learning based VF segmentation system for the analysis of VF motion behavior captured using flexible endoscopes with low-speed capability. Experimental results on human laryngeal videos showed promising results that were robust to many challenges caused by imaging, anatomical, and behavioral variations. The proposed segmentation and tracking system will be used to compute quantitative outcome measures describing VF motion behavior in order to help clinical practice and scientific discovery."
  },
  "cvpr2019_cvsports_earlydetectionofinjuriesinmlbpitchersfromvideo": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Early Detection of Injuries in MLB Pitchers From Video",
    "authors": [
      "AJ Piergiovanni",
      "Michael S. Ryoo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Piergiovanni_Early_Detection_of_Injuries_in_MLB_Pitchers_From_Video_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Piergiovanni_Early_Detection_of_Injuries_in_MLB_Pitchers_From_Video_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Injuries are a major cost in sports. Teams spend millions of dollars every year on players who are hurt and unable to play, resulting in lost games, decreased fan interest and additional wages for replacement players. Modern convolutional neural networks have been successfully applied to many video recognition tasks. In this paper, we introduce the problem of injury detection/prediction in MLB pitchers and experimentally evaluate the ability of such convolutional models to detect and predict injuries in pitches only from video data. We conduct experiments on a large dataset of TV broadcast MLB videos of 20 different pitchers who were injured during the 2017 season. We experimentally evaluate the model's performance on each individual pitcher, how well it generalizes to new pitchers, how it performs for various injuries, and how early it can predict or detect an injury. "
  },
  "cvpr2019_cvsports_fine-grainedvisualdribblingstyleanalysisforsoccervideoswithaugmenteddribbleenergyimage": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Fine-Grained Visual Dribbling Style Analysis for Soccer Videos With Augmented Dribble Energy Image",
    "authors": [
      "Runze Li",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Li_Fine-Grained_Visual_Dribbling_Style_Analysis_for_Soccer_Videos_With_Augmented_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Li_Fine-Grained_Visual_Dribbling_Style_Analysis_for_Soccer_Videos_With_Augmented_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent advances in interpretations of soccer are predominantly made through analyzing high-level contents of soccer videos. This work targets on these highlight actions and movements in soccer games and it focuses on dribbling skills performed by the top players. Our work leverages understanding of complex dribbling video clips by representing a video sequence with a single Dribble Energy Image(DEI) that is informative for dribbling styles recognition. To overcome the shortage of labelled data, this paper introduces a dataset of soccer video clips from Youtube, employs Mask-RCNN to segment out dribbling players and OpenPose to obtain joints information of dribbling players. Besides, to solve issues caused by camera motions in highlight soccer videos, our work proposes to register a video sequence to generate a single image representation DEI and dribbling styles classification. Our approach can achieve an accuracy of 87.65% on dribbling styles classification and it is observed that data augmentation using joints-reasoned GAN can improve the classification performance."
  },
  "cvpr2019_cvsports_investigationoncombining3dconvolutionofimagedataandopticalflowtogeneratetemporalactionproposals": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Investigation on Combining 3D Convolution of Image Data and Optical Flow to Generate Temporal Action Proposals",
    "authors": [
      "Patrick Schlosser",
      "David Munch",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Schlosser_Investigation_on_Combining_3D_Convolution_of_Image_Data_and_Optical_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Schlosser_Investigation_on_Combining_3D_Convolution_of_Image_Data_and_Optical_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, several variants of two-stream architectures for temporal action proposal generation in long, untrimmed videos are presented. Inspired by the recent advances in the field of human action recognition utilizing 3D convolutions in combination with two-stream networks and based on the Single-Stream Temporal Action Proposals (SST) architecture, four different two-stream architectures utilizing sequences of images on one stream and sequences of images of optical flow on the other stream are subsequently investigated. The four architectures fuse the two separate streams at different depths in the model; for each of them, a broad range of parameters is investigated systematically as well as an optimal parametrization is empirically determined. The experiments on the THUMOS'14 dataset - containing untrimmed videos of 20 different sporting activities for temporal action proposals - show that all four two-stream architectures are able to outperform the original single-stream SST and achieve state of the art results. Additional experiments revealed that the improvements are not restricted to one method of calculating optical flow by exchanging the method of Brox with FlowNet2 and still achieving improvements."
  },
  "cvpr2019_cvsports_pose-guidedr-cnnforjerseynumberrecognitioninsports": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Pose-Guided R-CNN for Jersey Number Recognition in Sports",
    "authors": [
      "Hengyue Liu",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Liu_Pose-Guided_R-CNN_for_Jersey_Number_Recognition_in_Sports_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Liu_Pose-Guided_R-CNN_for_Jersey_Number_Recognition_in_Sports_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recognizing player jersey number in sports match video streams is a challenging computer vision task. The human pose and view-point variations displayed in frames lead to many difficulties in recognizing the digits on jerseys. These challenges are addressed here using an approach that exploits human body part cues with a Region-based Convolutional Neural Network (R-CNN) variant for digit level localization and classification. The paper first adopts the Region Proposal Network (RPN) to perform anchor classification and bounding-box regression over three classes: background, person and digit. The person and digit proposals are geometrically related and fed to a network classifier. Subsequently, it introduces a human body key-point prediction branch and a pose-guided regressor to get better bounding-box offsets for generating digit proposals. A novel dataset of soccer-match video frames with corresponding multi-digit class labels, player and jersey number bounding boxes, and single digit segmentation masks is collected. Our framework outperforms all existing models on jersey number recognition task. This work will be essential to the automation of player identification across multiple sports, and releasing the dataset will ease future research on sports video analysis."
  },
  "cvpr2019_cvsports_attentivespatio-temporalrepresentationlearningfordivingclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Attentive Spatio-Temporal Representation Learning for Diving Classification",
    "authors": [
      "Gagan Kanojia",
      "Sudhakar Kumawat",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Kanojia_Attentive_Spatio-Temporal_Representation_Learning_for_Diving_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Kanojia_Attentive_Spatio-Temporal_Representation_Learning_for_Diving_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Competitive diving is a well recognized aquatic sport in which a person dives froma platform or a springboard into the water. Based on the acrobatics performed during the dive, diving is classified into a finite set of action classes which are standardized by FINA. In this work, we propose an attention guided LSTM-based neural network architecture for the task of diving classification. The network takes the frames of a diving video as input and determines its class. We evaluate the performance of the proposed model on a recently introduced competitive diving dataset, Diving48. It contains over 18000 video clips which covers 48 classes of diving. The proposed model outperforms the classification accuracy of the state-of-the-art models in both 2D and 3D frameworks by 11.54% and 4.24%, respectively. We show that the network is able to localize the diver in the video frames during the dive without being trained with such a supervision."
  },
  "cvpr2019_cvsports_associativeembeddingforteamdiscrimination": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Associative Embedding for Team Discrimination",
    "authors": [
      "Maxime Istasse",
      "Julien Moreau",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Istasse_Associative_Embedding_for_Team_Discrimination_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Istasse_Associative_Embedding_for_Team_Discrimination_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Assigning team labels to players in a sport game is not a trivial task when no prior is known about the visual appearance of each team. Our work builds on a Convolutional Neural Network (CNN) to learn a descriptor, namely a pixel-wise embedding vector, that is similar for pixels depicting players from the same team, and dissimilar when pixels correspond to distinct teams. The advantage of this idea is that no per-game learning is needed, allowing efficient team discrimination as soon as the game starts. In principle, the approach follows the associative embedding framework to differentiate instances of objects. Our work is however different in that it derives the embeddings from a lightweight segmentation network and, more fundamentally, because it considers the assignment of the same embedding to unconnected pixels, as required by pixels of distinct players from the same team. Excellent results, both in terms of team labelling accuracy and generalization to new games/arenas, have been achieved on panoramic views of a large variety of basketball games involving players interactions and occlusions. This makes our method a good candidate to integrate team separation in many CNN-based sport analytics pipelines."
  },
  "cvpr2019_cvsports_multi-person3dposeestimationandtrackinginsports": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Multi-Person 3D Pose Estimation and Tracking in Sports",
    "authors": [
      "Lewis Bridgeman",
      "Marco Volino",
      "Jean-Yves Guillemaut",
      "Adrian Hilton"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Bridgeman_Multi-Person_3D_Pose_Estimation_and_Tracking_in_Sports_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Bridgeman_Multi-Person_3D_Pose_Estimation_and_Tracking_in_Sports_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present an approach to multi-person 3D pose estimation and tracking from multi-view video. Following independent 2D pose detection in each view, we: (1) correct errors in the output of the pose detector; (2) apply a fast greedy algorithm for associating 2D pose detections between camera views; and (3) use the associated poses to generate and track 3D skeletons. Previous methods for estimating skeletons of multiple people suffer long processing times or rely on appearance cues, reducing their applicability to sports. Our approach to associating poses between views works by seeking the best correspondences first in a greedy fashion, while reasoning about the cyclic nature of correspondences to constrain the search. The associated poses can be used to generate 3D skeletons, which we produce via robust triangulation. Our method can track 3D skeletons in the presence of missing detections, substantial occlusions, and large calibration error. We believe ours is the first method for full-body 3D pose estimation and tracking of multiple players in highly dynamic sports scenes. The proposed method achieves a significant improvement in speed over state-of-the-art methods."
  },
  "cvpr2019_cvsports_sportscameracalibrationviasyntheticdata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Sports Camera Calibration via Synthetic Data",
    "authors": [
      "Jianhui Chen",
      "James J. Little"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Chen_Sports_Camera_Calibration_via_Synthetic_Data_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Chen_Sports_Camera_Calibration_via_Synthetic_Data_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Calibrating sports cameras is important for autonomous broadcasting and sports analysis. Here we propose a highly automatic method for calibrating sports cameras from a single image using synthetic data. First, we develop a novel camera pose engine that generates camera poses by randomly sampling camera parameters. The camera pose engine has only three significant free parameters so that it can effectively generate diverse camera poses and corresponding edge (i.e. field marking) images. Then, we learn compact feature descriptors via a siamese network from paired edge images and build a feature-pose database. After that, we use a novel GAN (generative adversarial network) model to detect field markings in real images. Finally, we query an initial camera pose from the feature-pose database and refine camera poses using truncated distance images. We evaluate our method on both synthetic and real data. Our method not only demonstrates the robustness on the synthetic data but also achieves state-of-the-art accuracy on a standard soccer dataset and very high performance on a volleyball dataset. "
  },
  "cvpr2019_cvsports_arthusadaptivereal-timehumansegmentationinsportsthroughonlinedistillation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "ARTHuS: Adaptive Real-Time Human Segmentation in Sports Through Online Distillation",
    "authors": [
      "Anthony Cioppa",
      "Adrien Deliege",
      "Maxime Istasse",
      "Christophe De Vleeschouwer",
      "Marc Van Droogenbroeck"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Cioppa_ARTHuS_Adaptive_Real-Time_Human_Segmentation_in_Sports_Through_Online_Distillation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Cioppa_ARTHuS_Adaptive_Real-Time_Human_Segmentation_in_Sports_Through_Online_Distillation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Semantic segmentation can be regarded as a useful tool for global scene understanding in many areas, including sports, but has inherent difficulties, such as the need for pixel-wise annotated training data and the absence of well-performing real-time universal algorithms. To alleviate these issues, we sacrifice universality by developing a general method, named ARTHuS, that produces adaptive real-time match-specific networks for human segmentation in sports videos, without requiring any manual annotation. This is done by an online knowledge distillation process, in which a fast student network is trained to mimic the output of an existing slow but effective universal teacher network, while being periodically updated to adjust to the latest play conditions. As a result, ARTHuS allows to build highly effective real-time human segmentation networks that evolve through the match and that sometimes outperform their teacher. The usefulness of producing adaptive match-specific networks and their excellent performances are demonstrated quantitatively and qualitatively for soccer and basketball matches."
  },
  "cvpr2019_cvsports_generationofballpossessionstatisticsinsoccerusingminimum-costflownetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Generation of Ball Possession Statistics in Soccer Using Minimum-Cost Flow Network",
    "authors": [
      "Saikat Sarkar",
      "Amlan Chakrabarti",
      "Dipti Prasad Mukherjee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Sarkar_Generation_of_Ball_Possession_Statistics_in_Soccer_Using_Minimum-Cost_Flow_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Sarkar_Generation_of_Ball_Possession_Statistics_in_Soccer_Using_Minimum-Cost_Flow_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present an automatic technique for calculating ball possession statistics from the video of a soccer match. The possession statistics is generated based on the number of valid passes made by an individual team. A valid pass is detected as a split or merge event of the ball with a player. A pass starts when the ball splits from a player. A pass ends when the ball merges with a player. We use a minimum-cost flow network to model number of valid passes in the soccer match. The ball and the players represent the nodes of the network. Each edge of the network is associated with a cost derived from the between-frame correspondences of the ball and the players. The total flow through the network is optimized to track the number of valid passes. Experimental results show that the accuracy of the proposed method is at least 4% better than that of a similar approach."
  },
  "cvpr2019_cvsports_refiningjointlocationsforhumanposetrackinginsportsvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Refining Joint Locations for Human Pose Tracking in Sports Videos",
    "authors": [
      "Dan Zecha",
      "Moritz Einfalt",
      "Rainer Lienhart"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Zecha_Refining_Joint_Locations_for_Human_Pose_Tracking_in_Sports_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Zecha_Refining_Joint_Locations_for_Human_Pose_Tracking_in_Sports_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The estimation of an athlete's pose in video footage enables the automation of athletic performance assessment, the prediction of motion kinematics and dynamics in sports videos and the possibility of technology-assisted, direct training feedback. Despite remarkable progress in the field of deep learning assisted human pose estimation, the performance of such systems decreases while noise and errors increase with the complexity of the scene. In this paper, we focus on aquatic training scenarios, where even novel pose estimators produce several types of orthogonal errors, including joint swaps and prediction outliers. In order to improve the estimation of an athlete's pose in swimming, we propose a graph partitioning problem that connects pose estimates over time and explicitly allows for joints to switch labels if their location better fits each other's trajectory. We optimize the problem using integer linear programming, which partitions the graph into the most probable joint trajectories. We show experimentally that our method of joint rectification improves the joint detection precision of swimmers in a swimming channel by 0.8%-4.8% PCK for anti-symmetrical motion and up to 1.8% PCK for symmetrical styles."
  },
  "cvpr2019_cvsports_temporaldistancematricesforsquatclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Temporal Distance Matrices for Squat Classification",
    "authors": [
      "Ryoji Ogata",
      "Edgar Simo-Serra",
      "Satoshi Iizuka",
      "Hiroshi Ishikawa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Ogata_Temporal_Distance_Matrices_for_Squat_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Ogata_Temporal_Distance_Matrices_for_Squat_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " When working out, it is necessary to perform the same action many times for it to have effect. If the action, such as squats or bench pressing, is performed with poor form, it can lead to serious injuries in the long term. For this purpose, we present an action dataset of squats where different types of poor form have been annotated with a diversity of users and backgrounds, and propose a model, based on temporal distance matrices, for the classification task. We first run a 3D pose detector, then we normalize the pose and compute the distance matrix, in which each element represents the distance between two joints. This representation is invariant to differences in individuals, global translation, and global rotation, allowing for high generalization to real world data. Our classification model consists of a CNN with 1D convolutions. Results show that our method significantly outperforms existing approaches for the task."
  },
  "cvpr2019_cvsports_temporalhockeyactionrecognitionviaposeandopticalflows": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "Temporal Hockey Action Recognition via Pose and Optical Flows",
    "authors": [
      "Zixi Cai",
      "Helmut Neher",
      "Kanav Vats",
      "David A. Clausi",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/Cai_Temporal_Hockey_Action_Recognition_via_Pose_and_Optical_Flows_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/Cai_Temporal_Hockey_Action_Recognition_via_Pose_and_Optical_Flows_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, a novel two-stream architecture has been designed to improve action recognition accuracy for hockey using three main components.First, pose is estimated via the Part Affinity Fields model to extract meaningful cues from the player.Second, optical flow (using LiteFlownet) is used to extract temporal features.Third, pose and optical flow streams are fused and passed to fully-connected layers to estimate the hockey player's action. A novel publicly available dataset named HARPET (Hockey Action Recognition Pose Estimation, Temporal) was created, composed of sequences of annotated actions and pose of hockey players including their hockey sticks as an extension of human body pose.Three contributions are recognized.(1) The novel two-stream architecture achieves 85% action recognition accuracy, with the inclusion of optical flows increasing accuracy by about 10%. Thus, demonstrating the complementary nature of pose estimation and optical flow. (2) The unique localization of hand-held objects (e.g., hockey sticks) as part of pose increases accuracy by about 13%.(3) For pose estimation, a bigger and more general dataset, MSCOCO, is successfully used for transfer learning to a smaller and more specific dataset, HARPET, achieving a PCKh of 87%."
  },
  "cvpr2019_cvsports_golfdbavideodatabaseforgolfswingsequencing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 5th International Workshop on Computer Vision in Sports (CVsports)",
    "title": "GolfDB: A Video Database for Golf Swing Sequencing",
    "authors": [
      "William McNally",
      "Kanav Vats",
      "Tyler Pinto",
      "Chris Dulhanty",
      "John McPhee",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVSports/McNally_GolfDB_A_Video_Database_for_Golf_Swing_Sequencing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVSports/McNally_GolfDB_A_Video_Database_for_Golf_Swing_Sequencing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The golf swing is a complex movement requiring considerable full-body coordination to execute proficiently. As such, it is the subject of frequent scrutiny and extensive biomechanical analyses. In this paper, we introduce the notion of golf swing sequencing for detecting key events in the golf swing and facilitating golf swing analysis. To enable consistent evaluation of golf swing sequencing performance, we also introduce the benchmark database GolfDB, consisting of 1400 high-quality golf swing videos, each labeled with event frames, bounding box, player name and sex, club type, and view type. Furthermore, to act as a reference baseline for evaluating golf swing sequencing performance on GolfDB, we propose a lightweight deep neural network called SwingNet, which possesses a hybrid deep convolutional and recurrent neural network architecture. SwingNet correctly detects eight golf swing events at an average rate of 76.1%, and six out of eight events at a rate of 91.8%.In line with the proposed baseline SwingNet, we advocate the use of computationally efficient models in future research to promote in-the-field analysis via deployment on readily-available mobile devices."
  },
  "cvpr2019_cvppp_dataaugmentationfromrgbtochlorophyllfluorescenceimagingapplicationtoleafsegmentationofarabidopsisthalianafromtopviewimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Data Augmentation From RGB to Chlorophyll Fluorescence Imaging Application to Leaf Segmentation of Arabidopsis thaliana From Top View Images",
    "authors": [
      "Natalia Sapoukhina",
      "Salma Samiei",
      "Pejman Rasti",
      "David Rousseau"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Sapoukhina_Data_Augmentation_From_RGB_to_Chlorophyll_Fluorescence_Imaging_Application_to_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Sapoukhina_Data_Augmentation_From_RGB_to_Chlorophyll_Fluorescence_Imaging_Application_to_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this report we investigate various strategies to boost the performance for leaf segmentation of Arabidopsis thaliana in chlorophyll fluorescent imaging without any manual annotation. Direct conversion of RGB images to gray levels picked from CVPPP challenge or from a virtual Arabidopsis thaliana simulator are tested together with synthetic noisy versions of these. Segmentation performed with a state of the art U-Net convolutional neural network is shown to benefit from these approaches with a Dice coefficient between 0.95 and 0.97 on the segmentation of the border of the leaves. A new annotated dataset of fluorescent images is made available."
  },
  "cvpr2019_cvppp_detectionofsinglegrapevineberriesinimagesusingfullyconvolutionalneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Detection of Single Grapevine Berries in Images Using Fully Convolutional Neural Networks",
    "authors": [
      "Laura Zabawa",
      "Anna Kicherer",
      "Lasse Klingbeil",
      "Andres Milioto",
      "Reinhard Topfer",
      "Heiner Kuhlmann",
      "Ribana Roscher"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Zabawa_Detection_of_Single_Grapevine_Berries_in_Images_Using_Fully_Convolutional_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Zabawa_Detection_of_Single_Grapevine_Berries_in_Images_Using_Fully_Convolutional_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Yield estimation and forecasting are of special interest in the field of grapevine breeding and viticulture. The number of harvested berries per plant is strongly correlated with the resulting quality. Therefore, early yield forecasting can enable a focused thinning of berries to ensure a high quality end product. Traditionally yield estimation is done by extrapolating from a small sample size and by utilizing historic data. Moreover, it needs to be carried out by skilled experts with much experience in this field. Berry detection in images offers a cheap, fast and non-invasive alternative to the otherwise time-consuming and subjective on-site analysis by experts. We apply fully convolutional neural networks on images acquired with the Phenoliner, a field phenotyping platform. We count single berries in images to avoid the error-prone detection of grapevine clusters. Clusters are often overlapping and can vary a lot in the size which makes the reliable detection of them difficult. We address especially the detection of white grapes directly in the vineyard. The detection of single berries is formulated as a classification task with three classes, namely 'berry', 'edge' and 'background'. A connected component algorithm is applied to determine the number of berries in one image. We compare the automatically counted number of berries with the manually detected berries in 60 images showing Riesling plants in vertical shoot positioned trellis (VSP) and semi minimal pruned hedges (SMPH). We are able to detect berries correctly within the VSP system with an accuracy of 94.0 % and for the SMPH system with 85.6 %. "
  },
  "cvpr2019_cvppp_dataaugmentationforleafsegmentationandcountingtasksinrosetteplants": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Data Augmentation for Leaf Segmentation and Counting Tasks in Rosette Plants",
    "authors": [
      "Dmitry Kuznichov",
      "Alon Zvirin",
      "Yaron Honen",
      "Ron Kimmel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Kuznichov_Data_Augmentation_for_Leaf_Segmentation_and_Counting_Tasks_in_Rosette_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Kuznichov_Data_Augmentation_for_Leaf_Segmentation_and_Counting_Tasks_in_Rosette_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep learningtechniques involving image processing and data analysis are constantly evolving. Many domains adapt these techniques for object segmentation, instantiation and classification. Recently, agricultural industries adopted those techniques in order to bring automation to farmers around the globe. One analysis procedure required for automatic visual inspection in this domain is leaf count and segmentation. Collecting labeled data from field crops and greenhouses is a complicated task due to the large variety of crops, growth seasons, climate changes, phenotype diversity, and more, especially, when specific learning tasks require a large amount of labeled data for training. Data augmentation for training deep neural networks is well established, examples include data synthesis, using generative semi-synthetic models, and applying various kinds of transformations. In this paper we propose a data augmentation method that preserves the geometric structure of the data objects, thus keeping the physical appearance of the data-set as close as possible to imaged plants in real agricultural scenes. The proposed method provides state of the art results when applied to the standard benchmark in the field, namely, the ongoing Leaf Segmentation Challenge hosted by Computer Vision Problems in Plant Phenotyping."
  },
  "cvpr2019_cvppp_leafcountingwithoutannotationsusingadversarialunsuperviseddomainadaptation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Leaf Counting Without Annotations Using Adversarial Unsupervised Domain Adaptation",
    "authors": [
      "Mario Valerio Giuffrida",
      "Andrei Dobrescu",
      "Peter Doerner",
      "Sotirios A. Tsaftaris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Giuffrida_Leaf_Counting_Without_Annotations_Using_Adversarial_Unsupervised_Domain_Adaptation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Giuffrida_Leaf_Counting_Without_Annotations_Using_Adversarial_Unsupervised_Domain_Adaptation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep learning is making strides in plant phenotyping and agriculture. But pretrained models require significant adaptation to work on new target datasets originating from a different experiment even on the same species. The current solution is to retrain the model on the new target data implying the need for annotated and labelled images. This paper addresses the problem of adapting a previously trained model on new target but unlabelled images.Our method falls in the broad machine learning problem of domain adaptation, where our aim is to reduce the difference between the source and target dataset (domains). Most classical approaches necessitate that both source and target data are simultaneously available to solve the problem. In agriculture it is possible that source data cannot be shared. Hence, we propose to update the model without necessarily sharing the data of the training source to preserve confidentiality. Our major contribution is a model that reduces the domain shift using an unsupervised adversarial adaptation mechanism on statistics of the training (source) data. In addition, we propose a multi-output training process that (i) allows (quasi-)integer leaf counting predictions; and (ii) improves the accuracy on the target domain, by minimising the distance between the counting distributions on the source and target domain. In our experiments we used a reduced version of the CVPPP dataset as source domain. We performed two sets of experiments, showing domain adaptation in the intra- and inter-species case. Using an Arabidopsis dataset as target domain, the prediction results exhibit a mean squared error (MSE) of 2.3. When a different plant species was used (Komatsuna), the MSE was 1.8."
  },
  "cvpr2019_cvppp_understandingdeepneuralnetworksforregressioninleafcounting": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Understanding Deep Neural Networks for Regression in Leaf Counting",
    "authors": [
      "Andrei Dobrescu",
      "Mario Valerio Giuffrida",
      "Sotirios A. Tsaftaris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Dobrescu_Understanding_Deep_Neural_Networks_for_Regression_in_Leaf_Counting_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Dobrescu_Understanding_Deep_Neural_Networks_for_Regression_in_Leaf_Counting_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep learning methods are constantly increasing in popularity and success across a wide range of computer vision applications. However, they are perceived as `black boxes', due to the lack of an intuitive interpretation of their decision processes. We present a study aimed at understanding how Deep Neural Networks (DNN) reach a decision in regression tasks. This study focuses on deep learning approaches in the common plant phenotyping task of leaf counting. We employ Layerwise Relevance Propagation (LRP) and Guided Back Propagation to provide insight into which parts of the input contribute to intermediate layers and the output. We observe that the network largely disregards the background and focuses on the plant during training. More importantly, we found that the leaf blade edges are the most relevant part of the plant for the network model in the counting task. Results are evaluated using a VGG-16 deep neural network on the CVPPP 2017 Leaf Counting Challenge dataset."
  },
  "cvpr2019_cvppp_lengthphenotypingwithinterestpointdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Length Phenotyping With Interest Point Detection",
    "authors": [
      "Adar Vit",
      "Guy Shani",
      "Aharon Bar-Hillel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Vit_Length_Phenotyping_With_Interest_Point_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Vit_Length_Phenotyping_With_Interest_Point_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Plant phenotyping is the task of measuring plant attributes. We term `length phenotyping' the task of measuring the length of a part of interest of a plant. The recent rise of low cost RGB-D sensors, and accurate deep networks, provides new opportunities for length phenotyping. In this paper we present a general technique for measuring length, based on three stages: object detection, point of interest identification, and a 3D measurement phase. We address object detection and interest point identification by training network models for each task, and use robust de-projection for the 3D measurement stage. We apply our method to two real world tasks: measuring the height of a banana tree, and measuring the length, width, and aspect ratio of banana leaves in potted plants. Our results indicate satisfactory measurement accuracy, with less than 10% deviation in all measurements. The two tasks were solved using the same pipeline with minor adaptations, indicating the general potential of the method."
  },
  "cvpr2019_cvppp_adversariallarge-scalerootgapinpainting": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Adversarial Large-Scale Root Gap Inpainting",
    "authors": [
      "Hao Chen",
      "Mario Valerio Giuffrida",
      "Peter Doerner",
      "Sotirios A. Tsaftaris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Chen_Adversarial_Large-Scale_Root_Gap_Inpainting_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Chen_Adversarial_Large-Scale_Root_Gap_Inpainting_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Root imaging of a growing plant in a non-invasive, affordable, and effective way remains challenging. One approach is to image roots by growing them in a rhizobox, a soil-filled transparent container,imaging them with digital cameras, and segmenting root from soil background.However, due to soil occlusion and the fact that digital imaging is a 2D projection of a 3D object, gaps are present on the segmentation masks, which may hinder the extraction of finely grained root system architecture (RSA) traits. Herein, we develop an image inpainting technique to recover gaps from disconnected root segments. We train a patch-based deep fully convolutional network using a supervised loss but also use adversarial mechanisms at patch and whole root level. We use Policy Gradient method, to endow the model with large-scale whole root view during training. We train our model using synthetic root data.In our experiments, we show that using adversarial mechanisms at local and whole-root level we obtain a 72% improvement in performanceon recovering gaps of real chickpea data when using only patch-level supervision."
  },
  "cvpr2019_cvppp_protractoralightweightgroundimagingandanalysissystemforearly-seasonfieldphenotyping": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "ProTractor: A Lightweight Ground Imaging and Analysis System for Early-Season Field Phenotyping",
    "authors": [
      "Nico Higgs",
      "Blanche Leyeza",
      "Jordan Ubbens",
      "Josh Kocur",
      "William van der Kamp",
      "Theron Cory",
      "Christina Eynck",
      "Sally Vail",
      "Mark Eramian",
      "Ian Stavness"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Higgs_ProTractor_A_Lightweight_Ground_Imaging_and_Analysis_System_for_Early-Season_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Higgs_ProTractor_A_Lightweight_Ground_Imaging_and_Analysis_System_for_Early-Season_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Acquiring high-resolution images in the field for image-based crop phenotyping is typically performed by complicated, custom built \"pheno-mobiles.\" In this paper, we demonstrate that large datasets of crop row images can be easily acquired with consumer cameras attached to a regular tractor. Localization and labeling of individual rows of plants are performed by a computer vision approach, rather than sophisticated real-time geo-location hardware on the tractor. We evaluate our approach for cropping rows of early-season plants from a Brassica carinata field trial where we achieve 100% recall and 99% precision. We also demonstrate a proof-of-concept plant counting method for our ProTractor system using an object detection network that achieves a mean average precision of 0.82 when detecting plants, and an R2 of 0.89 when counting plants. The ProTractor design and software are open source to advance the collection of large outdoor plant phenotyping datasets with inexpensive and easy to use acquisition systems."
  },
  "cvpr2019_cvppp_aguidedmulti-scalecategorizationofplantspeciesinnaturalimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "A Guided Multi-Scale Categorization of Plant Species in Natural Images",
    "authors": [
      "Jonas Krause",
      "Kyungim Baek",
      "Lipyeow Lim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Krause_A_Guided_Multi-Scale_Categorization_of_Plant_Species_in_Natural_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Krause_A_Guided_Multi-Scale_Categorization_of_Plant_Species_in_Natural_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Automatic categorization of plant species in natural images is an important computer vision problem with numerous applications in agriculture and botany. The problem is particularly challenging due to the large number of plant species, the inter-species similarity, the large scale variations in natural images, and the lack of annotated data. In this paper, we present a guided multi-scale approach that segments the regions of interest (containing a plant) from a complex background of the natural image and systematically extracts scale-representative patches based on those regions. These multi-scale patches are used to train state-of-the-art Convolutional Neural Network (CNN) models that analyze a given plant image and determine its species. Focusing specifically on the identification of plant species in natural images, we show that the proposed approach is a very effective way of making deep learning models more robust to scale variations. We perform a comprehensive experimental evaluation of our proposed method over several CNN models. Our best result on the Inception-ResNet-v2 model achieves a top-1 classification accuracy of 89.21% for 100 plant species which represents a 5.4% increase over using random cropping to generate training data."
  },
  "cvpr2019_cvppp_rolsrobustobject-levelslamforgrapecounting": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "ROLS : Robust Object-Level SLAM for Grape Counting",
    "authors": [
      "Anjana K. Nellithimaru",
      "George A. Kantor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Nellithimaru_ROLS__Robust_Object-Level_SLAM_for_Grape_Counting_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Nellithimaru_ROLS__Robust_Object-Level_SLAM_for_Grape_Counting_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Camera based Simultaneous Localization and Mapping (SLAM) in an agricultural field can be used by crop growers to count fruits and estimate yield. It is challenging due to dynamics, illumination conditions and limited texture inherent in an outdoor environment. We propose a pipeline that combines the recent advances in deep learning with traditional 3D processing techniques to achieve fast and accurate SLAM in vineyards. We use images captured by a stereo camera and their 3D reconstruction to detect objects of interest and divide them into classes: grapes, leaves and branches. The accuracy of these detections is improved by leveraging information about objects' local neighborhood in 3D. We achieve a F1 score of 0.977 with ground truth grape counts from images. Our method builds a dense 3D model of the scene with a localization accuracy in centimeters without any assumption of constant illumination conditions or scene dynamics. This method can be easily generalized to other crops such as oranges and apples with minor modifications in the pipeline."
  },
  "cvpr2019_cvppp_croplodgingpredictionfromuav-acquiredimagesofwheatandcanolausingadcnnaugmentedwithhandcraftedtexturefeatures": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Crop Lodging Prediction From UAV-Acquired Images of Wheat and Canola Using a DCNN Augmented With Handcrafted Texture Features",
    "authors": [
      "Sara Mardanisamani",
      "Farhad Maleki",
      "Sara Hosseinzadeh Kassani",
      "Sajith Rajapaksa",
      "Hema Duddu",
      "Menglu Wang",
      "Steve Shirtliffe",
      "Seungbum Ryu",
      "Anique Josuttes",
      "Ti Zhang",
      "Sally Vail",
      "Curtis Pozniak",
      "Isobel Parkin",
      "Ian Stavness",
      "Mark Eramian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Mardanisamani_Crop_Lodging_Prediction_From_UAV-Acquired_Images_of_Wheat_and_Canola_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Mardanisamani_Crop_Lodging_Prediction_From_UAV-Acquired_Images_of_Wheat_and_Canola_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Lodging, the permanent bending over of food crops, leads to poor plant growth and development. Consequently, lodging results in reduced crop quality, lowers crop yield, and makes harvesting difficult. Plant breeders routinely evaluate several thousand breeding lines, and therefore, automatic lodging detection and prediction is of great value aid in selection. In this paper, we propose a deep convolutional neural network (DCNN) architecture for lodging classification using five spectral channel orthomosaic images from canola and wheat breeding trials. Also, using transfer learning, we trained 10 lodging detection models using well-established deep convolutional neural network architectures. Our proposed model outperforms the state-of-the-art lodging detection methods in the literature that use only handcrafted features. In comparison to 10 DCNN lodging detection models, our proposed model achieves comparable results while having a substantially lower number of parameters. This makes the proposed model suitable for applications such as real-time classification using inexpensive hardware for high-throughput phenotyping pipelines. The GitHub repository at https://github. com/FarhadMaleki/LodgedNet contains code and models. "
  },
  "cvpr2019_cvppp_beansplitratiofordrybeancanningqualityandvarietyanalysis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Bean Split Ratio for Dry Bean Canning Quality and Variety Analysis",
    "authors": [
      "Yunfei Long",
      "Amber Bassett",
      "Karen Cichy",
      "Addie Thompson",
      "Daniel Morris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Long_Bean_Split_Ratio_for_Dry_Bean_Canning_Quality_and_Variety_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Long_Bean_Split_Ratio_for_Dry_Bean_Canning_Quality_and_Variety_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Splits on canned beans appear in the process of preparation and canning. Researchers are studying how they are influenced by cooking environment and genotype. However, there is no existing method to automatically quantify or to characterize the severity of splits. To solve this, we propose two measures: the Bean Split Ratio (BSR) that quantifies the overall severity of splits, and the Bean Split Histogram (BSH) that characterizes the size distribution of splits.We create a pixel-wise segmentation method to automatically estimate these measures from images.We also present a bean dataset of recombinant inbred lines of two genotypes, use the BSR and BSH to assess canning quality, and explore heritability of these properties."
  },
  "cvpr2019_cvppp_visndavisualizationtoolformultidimensionalmodelofcanopy": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "VisND: A Visualization Tool for Multidimensional Model of Canopy",
    "authors": [
      "Ali Shafiekhani",
      "Felix B. Fritschi",
      "Guilherme N. DeSouza"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Shafiekhani_VisND_A_Visualization_Tool_for_Multidimensional_Model_of_Canopy_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Shafiekhani_VisND_A_Visualization_Tool_for_Multidimensional_Model_of_Canopy_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Plant phenotyping is a data-driven research where interpretation of large and often multidimensional data is required. Therefore, effective visualization of plant phenotypes plays a major role in data analytics as it can provide scientists with the required tool to extract and infer important information. In that sense, unifying the large and multidimensional phenotypical data into one single model of the canopy can help plant biologists to correlate information from different dimensions and derive new observations and understandings in plant sciences. In this paper, we proposed a spatio-temporal tool for high-dimensional modeling and visualization of canopy for plant phenotyping. The goal is to offer an open-source visualization tool, named VisND (for N-Dimensional), that will provide a Graphical User Interface (GUI) where plant scientists can easily extract and analyze multidimensional models, registered over time, from different sensors and viewpoints. For this paper, we created 5D models (3D-RGB and Temperature over Time) of a crop by fusing and registering data captured using our field-based phenotyping platform: Vinoculer, a trinocular, multi-spectrum, observation tower. The platform is part of a study on the behavior of plants in response to different biotic and/or abiotic stresses. The data was captured using Infrared Thermography (IRT) along with multiview, visible imaging technology over an entire planting season and on a 24/7 basis. While currently VisND is being demonstrated for 5D models, it can be easily extended to incorporate other modality of sensors (more dimensions) and from other sources, such as other robotic platforms also operating in the field e.g. our mobile robot, Vinobot."
  },
  "cvpr2019_cvppp_thegrasscloverimagedatasetforsemanticandhierarchicalspeciesunderstandinginagriculture": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "The GrassClover Image Dataset for Semantic and Hierarchical Species Understanding in Agriculture",
    "authors": [
      "Soren Skovsen",
      "Mads Dyrmann",
      "Anders K. Mortensen",
      "Morten S. Laursen",
      "Rene Gislum",
      "Jorgen Eriksen",
      "Sadaf Farkhani",
      "Henrik Karstoft",
      "Rasmus N. Jorgensen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Skovsen_The_GrassClover_Image_Dataset_for_Semantic_and_Hierarchical_Species_Understanding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Skovsen_The_GrassClover_Image_Dataset_for_Semantic_and_Hierarchical_Species_Understanding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " GrassClover is a diverse image and biomass dataset collected in an outdoor agricultural setting. The images contain dense populations of grass and clover mixtures with heavy occlusions and occurrences of weeds. Fertilization and treatment of mixed crops depend on the local species composition. Therefore, the overall challenge is related to predicting the species composition in the canopy image and in the biomass. The dataset is collected with three different acquisition systems with ground sampling distances of 4--8 px/mm. The observed mixed crops vary both in setting (field vs plot trial), seed compositions, yield, years since establishment and time of the season. Synthetic training images with pixel-wise hierarchical and instance labels are provided for supervised training. 31 600 unlabeled images are additionally provided for pre-training, semi-supervised training or unsupervised training. Furthermore, this paper provides challenges of semantic segmentation and prediction of the biomass compositions and a baseline model for this dataset."
  },
  "cvpr2019_cvppp_leafsegmentationbyfunctionalmodeling": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Leaf Segmentation by Functional Modeling",
    "authors": [
      "Yuhao Chen",
      "Sriram Baireddy",
      "Enyu Cai",
      "Changye Yang",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Chen_Leaf_Segmentation_by_Functional_Modeling_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Chen_Leaf_Segmentation_by_Functional_Modeling_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The use of Unmanned Aerial Vehicles (UAVs) is a recent trend in field based plant phenotyping data collection. However, UAVs often provide low spatial resolution images when flying at high altitudes. This can be an issue when extracting individual leaves from these images. Leaf segmentation is even more challenging because of densely overlapping leaves. Segmentation of leaf instances in the UAV images can be used to measure various phenotypic traits such as leaf length, maximum leaf width, and leaf area index. Successful leaf segmentation accurately detects leaf edges. Popular deep neural network approaches have loss functions that do not consider the spatial accuracy of the segmentation near an object's edge. This paper proposes a shape-based leaf segmentation method that segments leaves using continuous functions and produces precise contours for the leaf edges. Experimental results prove the feasibility of the method and demonstrate better performance than the Mask R-CNN."
  },
  "cvpr2019_cvppp_predictionofsorghumbiomassusinguavtimeseriesdataandrecurrentneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "Prediction of Sorghum Biomass Using Uav Time Series Data and Recurrent Neural Networks",
    "authors": [
      "Ali Masjedi",
      "Neal R. Carpenter",
      "Melba M. Crawford",
      "Mitch R. Tuinstra"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Masjedi_Prediction_of_Sorghum_Biomass_Using_Uav_Time_Series_Data_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Masjedi_Prediction_of_Sorghum_Biomass_Using_Uav_Time_Series_Data_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Phenotyping via Unmanned Aerial Vehicles (UAVs) is of increasing interest for many applications because of their capability to carry advanced sensors and achieve accurate positioning required to collect both high temporal and high spatial resolution data required over relatively limited areas. This paper focuses development of a data analytics based predictive modeling strategy that incorporates multi-sensor data acquisition systems and accommodates environmental inputs. Unsupervised feature learning based on fully connected and convolutional neural networks is investigated. Predictive models based on Recurrent Neural Networks (RNNs) are designed and implemented to accommodate high dimensional, multi-modal, multi-temporal data. Remote sensing data, including Light Detection and Ranging (LiDAR) and hyperspectral inputs, as well as weather data, are incorporated in RNN models. Results from multiple experiments focused on high throughput phenotyping of sorghum for biomass predictions are provided and evaluated for agricultural test fields at the Agronomy Center for Research and Education (ACRE) at Purdue University."
  },
  "cvpr2019_cvppp_theoilradishgrowthdatasetforsemanticsegmentationandyieldestimation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVPPP",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION PROBLEMS IN PLANT PHENOTYPING",
    "title": "The Oil Radish Growth Dataset for Semantic Segmentation and Yield Estimation",
    "authors": [
      "Anders Krogh Mortensen",
      "Soren Skovsen",
      "Henrik Karstoft",
      "Rene Gislum"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVPPP/Mortensen_The_Oil_Radish_Growth_Dataset_for_Semantic_Segmentation_and_Yield_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVPPP/Mortensen_The_Oil_Radish_Growth_Dataset_for_Semantic_Segmentation_and_Yield_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Data sharing in research is important in order to reproduce results, develop global models, and benchmark methods. This paper presents a dataset containing image and field data from a field plot experiment with oil radish (Raphanus sativus L. var oleiformis) as catch crop after spring barley. The field data consists of fresh weight, dry weight, Carbon content and Nitrogen content from multiple weekly plant samples collected from the plots. The image data consists of images collected weekly prior to the plant samples. A subset of the images corresponding to the plant sampling areas have been annotated pixelwise. In addition to the image and field data, weather data from the growing period is also included in the dataset. The dataset is accompanied by two challenges: 1) semantic segmentation of crops and 2) oil radish yield estimation. The former challenge focuses on data image, while the latter focuses on the field data. Baseline methods and results are provided for both challenges."
  },
  "cvpr2019_isic_acustomizedcameraimagingpipelinefordermatologicalimaging": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "A Customized Camera Imaging Pipeline for Dermatological Imaging",
    "authors": [
      "Hakki Can Karaimer",
      "Iman Khodadad",
      "Farnoud Kazemzadeh",
      "Michael S. Brown"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Karaimer_A_Customized_Camera_Imaging_Pipeline_for_Dermatological_Imaging_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Karaimer_A_Customized_Camera_Imaging_Pipeline_for_Dermatological_Imaging_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper describes the customization of the camera processing pipeline of a machine vision camera that has been integrated into a hand-held dermatological imaging device. The device uses a combination of visible and non-visible spectral LEDs to allow capture of visible RGB imagery as well as selected non-visible wavelengths. Our customization involves two components. The first component is a color calibration procedure that ensures the captured images are colorimetrically more accurate than those obtained through the machine vision camera's native API.The need for color calibration is a critical component that is often overlooked or poorly understood by computer vision engineers.Our second component is a fast method to integrate the narrow band spectral images (some of which are outside the visible range) into the visible RGB image for enhanced visualization.This component of our pipeline involves evaluating several algorithms capable of multiple image fusion to determine the most suitable one for our application.Quantitative and subject results, including feedback from clinicians, demonstrate the effectiveness of our customization procedure. "
  },
  "cvpr2019_isic_towardsautomatedmelanomadetectionwithdeeplearningdatapurificationandaugmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "Towards Automated Melanoma Detection With Deep Learning: Data Purification and Augmentation",
    "authors": [
      "Devansh Bisla",
      "Anna Choromanska",
      "Russell S. Berman",
      "Jennifer A. Stein",
      "David Polsky"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Bisla_Towards_Automated_Melanoma_Detection_With_Deep_Learning_Data_Purification_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Bisla_Towards_Automated_Melanoma_Detection_With_Deep_Learning_Data_Purification_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Melanoma is one of ten most common cancers in the US. Early detection is crucial for survival, but often the cancer is diagnosed in the fatal stage. Deep learning has the potential to improve cancer detection rates, but its applicability to melanoma detection is compromised by the limitations of the available skin lesion data bases, which are small, heavily imbalanced, and contain images with occlusions. We build deep-learning-based tools for data purification and augmentation to counter-act these limitations. The developed tools can be utilized in a deep learning system for lesion classification and we show how to build such system. The system heavily relies on the processing unit for removing image occlusions and the data generation unit, based on generative adversarial networks, for populating scarce lesion classes, or equivalently creating virtual patients with pre-defined types of lesions. We empirically verify our approach and show that incorporating these two units into melanoma detection system results in the superior performance over common baselines."
  },
  "cvpr2019_isic_interpretingfine-graineddermatologicalclassificationbydeeplearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "Interpreting Fine-Grained Dermatological Classification by Deep Learning",
    "authors": [
      "Sourav Mishra",
      "Hideaki Imaizumi",
      "Toshihiko Yamasaki"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Mishra_Interpreting_Fine-Grained_Dermatological_Classification_by_Deep_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Mishra_Interpreting_Fine-Grained_Dermatological_Classification_by_Deep_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper analyzes a deep learning based classification process for common East Asian dermatological conditions. We have chosen ten common categories based on prevalence. With more than 85% accuracy in our experiments, we have tried to investigate why current models are yet to reach accuracy benchmarks seen in object identification tasks. Our current attempt sheds light on how deep learning based dermoscopic identification and dataset creation could be improved."
  },
  "cvpr2019_isic_segmentationofprognostictissuestructuresincutaneousmelanomausingwholeslideimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "Segmentation of Prognostic Tissue Structures in Cutaneous Melanoma Using Whole Slide Images",
    "authors": [
      "Adon Phillips",
      "Iris Teo",
      "Jochen Lang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Phillips_Segmentation_of_Prognostic_Tissue_Structures_in_Cutaneous_Melanoma_Using_Whole_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Phillips_Segmentation_of_Prognostic_Tissue_Structures_in_Cutaneous_Melanoma_Using_Whole_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Our work applies modern machine learning techniques to melanoma diagnostics. First, we curated a new dataset of 50 patient cases of cutaneous melanoma in whole slide images (WSIs). We applied gold standard annotations for three tissue types (tumour, epidermis, and dermis) which are important for the prognostic measurements known as Breslow thickness and Clark level. Then, we devised a novel multi-stride fully convolutional network (FCN) architecture that outperformed other networks trained and tested using the same data and evaluated on standard metrics. Three pathologists measured the Breslow thickness on the network's output. Their responses were diagnostically equivalent to the ground truth measurements, showing that it is possible to overcome the discriminative challenges of the skin and tumour anatomy for segmentation. Though more work is required to improve the network's performance on dermis segmentation, we have shown it is possible to achieve a level of accuracy required to manually perform the Breslow thickness measurement."
  },
  "cvpr2019_isic_melanomathicknesspredictionbasedonconvolutionalneuralnetworkwithvgg-19modeltransferlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "Melanoma Thickness Prediction Based on Convolutional Neural Network With VGG-19 Model Transfer Learning",
    "authors": [
      "Joanna Jaworek-Korjakowska",
      "Pawel Kleczek",
      "Marek Gorgon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Jaworek-Korjakowska_Melanoma_Thickness_Prediction_Based_on_Convolutional_Neural_Network_With_VGG-19_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Jaworek-Korjakowska_Melanoma_Thickness_Prediction_Based_on_Convolutional_Neural_Network_With_VGG-19_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Over the past two decades, malignant melanoma incidence rate has dramatically risen but melanoma mortality has only recently stabilized. Due to its propensity to metastasize and lack of effective therapies for most patients with advanced disease, early detection of melanoma is a clinical imperative. Thickness is one of the most important factor in melanoma prognosis and it is used to establish the size of the surgical margin, as well as to select patients for sentinel lymph node biopsy. However, little work has concentrated on the evaluation of melanoma thickness both from the clinical as well as computer-aided diagnostic side. To address this problem, we propose an effective computer-vision based machine learning tool that can perform the preoperative evaluation of melanoma thickness. The novelty of our approach is that we directly predict the thickness of the skin lesion into one of three classes: less than 0.75 mm, 0.76-1.5 mm, and greater that 1.5 mm. In this study, we use transfer learning of the pre-trained, adapted to our application VGG-19 convolutional neural network (CNN) with an adjusted densely-connected classifier. Due to the limited data we investigate the transfer learning method where we apply knowledge from model trained on a different task. Our database contains 244 dermoscopy images. Experiments confirm the developed algorithm's ability to classify skin lesion thickness with 87.2% overall accuracy what is a state-of-the-art result in melanoma thickness prediction."
  },
  "cvpr2019_isic_deepattentionmodelforthehierarchicaldiagnosisofskinlesions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "Deep Attention Model for the Hierarchical Diagnosis of Skin Lesions",
    "authors": [
      "Catarina Barata",
      "Jorge S. Marques",
      "M. Emre Celebi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Barata_Deep_Attention_Model_for_the_Hierarchical_Diagnosis_of_Skin_Lesions_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Barata_Deep_Attention_Model_for_the_Hierarchical_Diagnosis_of_Skin_Lesions_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deeplearninghasplayedamajorroleintherecent advances in the dermoscopy image analysis field. However, such advances came at the cost of reducing the interpretability of the developed diagnostic systems, which do not comply with the requirements of the medical community nor with the most recent laws on machine learning explainability. Recent advances in the deep learning field, namely attention maps, improved the interpretability of these methods.Incorporating medical knowledge in the systems has also proved useful to increase their performance.In this work we propose to combine these two approaches in a formulation that: i) makes use of the hierarchical organization of skin lesions, as identified by dermatologists, to develop a classification model; and ii) uses an attention module to identify relevant regions in the skin lesions and guide the classification decisions. We demonstrate the potential of the proposed approach in two state-of-the-art dermoscopy sets (ISIC 2017 and ISIC 2018)."
  },
  "cvpr2019_isic_(de)constructingbiasonskinlesiondatasets": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "(De)Constructing Bias on Skin Lesion Datasets",
    "authors": [
      "Alceu Bissoto",
      "Michel Fornaciali",
      "Eduardo Valle",
      "Sandra Avila"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Bissoto_DeConstructing_Bias_on_Skin_Lesion_Datasets_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Bissoto_DeConstructing_Bias_on_Skin_Lesion_Datasets_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Melanoma is the deadliest form of skin cancer. Automated skin lesion analysis plays an important role for early detection. Nowadays, the ISIC Archive and the Atlas of Dermoscopy dataset are the most employed skin lesion sources to benchmark deep-learning based tools. However, all datasets contain biases, often unintentional, due to how they were acquired and annotated. Those biases distort the performance of machine-learning models, creating spurious correlations that the models can unfairly exploit, or, contrarily destroying cogent correlations that the models could learn. In this paper, we propose a set of experiments that reveal both types of biases, positive and negative, in existing skin lesion datasets. Our results show that models can correctly classify skin lesion images without clinically-meaningful information: disturbingly, the machine-learning model learned over images where no information about the lesion remains, presents an accuracy above the AI benchmark curated with dermatologists' performances. That strongly suggests spurious correlations guiding the models. We fed models with additional clinically meaningful information, which failed to improve the results even slightly, suggesting the destruction of cogent correlations. Our main findings raise awareness of the limitations of models trained and evaluated in small datasets such as the ones we evaluated, and may suggest future guidelines for models intended for real-world deployment."
  },
  "cvpr2019_isic_soloorensemble?choosingacnnarchitectureformelanomaclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - ISIC Skin Image Analysis Workshop",
    "title": "Solo or Ensemble? Choosing a CNN Architecture for Melanoma Classification",
    "authors": [
      "Fabio Perez",
      "Sandra Avila",
      "Eduardo Valle"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/ISIC/Perez_Solo_or_Ensemble_Choosing_a_CNN_Architecture_for_Melanoma_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/ISIC/Perez_Solo_or_Ensemble_Choosing_a_CNN_Architecture_for_Melanoma_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional neural networks (CNNs) deliver exceptional results for computer vision, including medical image analysis. With the growing number of available architectures, picking one over another is far from obvious. Existing art suggests that, when performing transfer learning, the performance of CNN architectures on ImageNet correlates strongly with their performance on target tasks. We evaluate that claim for melanoma classification, over 9 CNNs architectures, in 5 sets of splits created on the ISIC Challenge 2017 dataset, and 3 repeated measures, resulting in 135 models. The correlations we found were, to begin with, much smaller than those reported by existing art, and disappeared altogether when we considered only the top-performing networks: uncontrolled nuisances (i.e., splits and randomness) overcome any of the analyzed factors. Whenever possible, the best approach for melanoma classification is still to create ensembles of multiple models. We compared two choices for selecting which models to ensemble: picking them at random (among a pool of high-quality ones) vs. using the validation set to determine which ones to pick first. For small ensembles, we found a slight advantage on the second approach but found that random choice was also competitive. Although our aim in this paper was not to maximize performance, we easily reached AUCs comparable to the first place on the ISIC Challenge 2017."
  },
  "cvpr2019_skelneton_skelneton2019datasetandchallengeondeeplearningforgeometricshapeunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "SkelNetOn 2019: Dataset and Challenge on Deep Learning for Geometric Shape Understanding",
    "authors": [
      "Ilke Demir",
      "Camilla Hahn",
      "Kathryn Leonard",
      "Geraldine Morin",
      "Dana Rahbani",
      "Athina Panotopoulou",
      "Amelie Fondevilla",
      "Elena Balashova",
      "Bastien Durix",
      "Adam Kortylewski"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Demir_SkelNetOn_2019_Dataset_and_Challenge_on_Deep_Learning_for_Geometric_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Demir_SkelNetOn_2019_Dataset_and_Challenge_on_Deep_Learning_for_Geometric_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present SkelNetOn 2019 Challenge and Deep Learning for Geometric Shape Understanding workshop to utilize existing and develop novel deep learning architectures for shape understanding. We observed that unlike traditional segmentation and detection tasks, geometry understanding is still a new area for deep learning techniques. SkelNetOn aims to bring together researchers from different domains to foster learning methods on global shape understanding tasks. We aim to improve and evaluate the state-of-the-art shape understanding approaches, and to serve as reference benchmarks for future research. Similar to other challenges in computer vision, SkelNetOn proposes three datasets and corresponding evaluation methodologies; all coherently bundled in three competitions with a dedicated workshop co-located with CVPR 2019 conference. In this paper, we describe and analyze characteristics of datasets, define the evaluation criteria of the public competitions, and provide baselines for each task."
  },
  "cvpr2019_skelneton_parametricshapemodelingandskeletonextractionwithradialbasisfunctionsusingsimilaritydomainsnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "Parametric Shape Modeling and Skeleton Extraction With Radial Basis Functions Using Similarity Domains Network",
    "authors": [
      "Sedat Ozer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Ozer_Parametric_Shape_Modeling_and_Skeleton_Extraction_With_Radial_Basis_Functions_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Ozer_Parametric_Shape_Modeling_and_Skeleton_Extraction_With_Radial_Basis_Functions_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We demonstrate the use of similarity domains (SDs) for shape modeling and skeleton extraction. SDs are recently proposed and they can be utilized in a neural network framework to help us analyze shapes. SDs are modeled with radial basis functions with varying shape parameters in Similarity Domains Networks (SDNs). In this paper, we demonstrate how using SDN can first help us model a pixel-based image in terms of SDs and then demonstrate how those learned SDs can be used to extract the skeleton of a shape."
  },
  "cvpr2019_skelneton_multi-level3dcnnforlearningmulti-scalespatialfeatures": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "Multi-Level 3D CNN for Learning Multi-Scale Spatial Features",
    "authors": [
      "Sambit Ghadai",
      "Xian Yeow Lee",
      "Aditya Balu",
      "Soumik Sarkar",
      "Adarsh Krishnamurthy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Ghadai_Multi-Level_3D_CNN_for_Learning_Multi-Scale_Spatial_Features_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Ghadai_Multi-Level_3D_CNN_for_Learning_Multi-Scale_Spatial_Features_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " 3D object recognition accuracy can be improved by learning the multi-scale spatial features from 3D spatial geometric representations of objects such as point clouds, 3D models, surfaces, and RGB-D data. Current deep learning approaches learn such features either using structured data representations (voxel grids and octrees) or from unstructured representations (graphs and point clouds). Learning features from such structured representations is limited by the restriction on resolution and tree depth while unstructured representations creates a challenge due to non-uniformity among data samples. In this paper, we propose an end-to-end multi-level learning approach on a multi-level voxel grid to overcome these drawbacks. To demonstrate the utility of the proposed multi-level learning, we use a multi-level voxel representation of 3D objects to perform object recognition. The multi-level voxel representation consists of a coarse voxel grid that contains volumetric information of the 3D object. In addition, each voxel in the coarse grid that contains a portion of the object boundary is subdivided into multiple fine-level voxel grids. The performance of our multi-level learning algorithm for object recognition is comparable to dense voxel representations while using significantly lower memory."
  },
  "cvpr2019_skelneton_invariancetoaffine-permutationdistortions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "Invariance to Affine-Permutation Distortions",
    "authors": [
      "Liang-Yan Gui",
      "David A. Sepiashvili",
      "Jose M. F. Moura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Gui_Invariance_to_Affine-Permutation_Distortions_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Gui_Invariance_to_Affine-Permutation_Distortions_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " An object imaged from various viewpoints appears very different. Hence, effective shape representation of objects becomes central in many applications of computer vision. We consider affine and permutation distortions. We derive the affine-permutation shape space that extends, to include permutation distortions, the affine only shape space (the Grassmannian). We compute the affine-permutation shape space metric, the sample mean of multiple shapes, the geodesic defined by two shapes, and a canonical representative for a shape equivalence class. We illustrate our approach in several applications including clustering and morphing of shapes of different objects along a geodesic path. The experimental results on key benchmark datasets demonstrate the effectiveness of our framework."
  },
  "cvpr2019_skelneton_anovelalgorithmforskeletonextractionfromimagesusingtopologicalgraphanalysis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "A Novel Algorithm for Skeleton Extraction From Images Using Topological Graph Analysis",
    "authors": [
      "Liping Yang",
      "Diane Oyen",
      "Brendt Wohlberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Yang_A_Novel_Algorithm_for_Skeleton_Extraction_From_Images_Using_Topological_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Yang_A_Novel_Algorithm_for_Skeleton_Extraction_From_Images_Using_Topological_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Skeletonization, also called thinning, is an important pre-processing step in computer vision and image processing tasks such as shape analysis and vectorization. It is a morphological process that generates a skeleton from an input image. Many thinning algorithms have been proposed, but accurate and fast algorithms are still in demand. In this paper, we propose a novel algorithm using embedded topological graphs and computational geometry that can extract skeletons from input binary images. We compare three well-known thinning algorithms with our method, with the experimental results showing effectiveness of the proposed method and algorithms."
  },
  "cvpr2019_skelneton_parametricskeletongenerationviagaussianmixturemodels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "Parametric Skeleton Generation via Gaussian Mixture Models",
    "authors": [
      "Chang Liu",
      "Dezhao Luo",
      "Yifei Zhang",
      "Wei Ke",
      "Fang Wan",
      "Qixiang Ye"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Liu_Parametric_Skeleton_Generation_via_Gaussian_Mixture_Models_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Liu_Parametric_Skeleton_Generation_via_Gaussian_Mixture_Models_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose an efficient and effective control point extraction algorithm for parametric skeleton generation. The object skeleton pixels are predicted via an hourglass network and partitioned into skeleton branches using Gaussian Mixture Models. For each skeleton branch, a Bezier curve is utilized to generate the control points. The radius of the skeleton is computed by the distance between the border of the object and the Bezier curve. The branches are sorted by the area so that the parametric skeleton representation is unique. For the Parametric SkelNetOn competition, the proposed approach achieves the prediction score of 11793.89, which is in the first place on the performance leader-board."
  },
  "cvpr2019_skelneton_featurehourglassnetworkforskeletondetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "Feature Hourglass Network for Skeleton Detection",
    "authors": [
      "Nan Jiang",
      "Yifei Zhang",
      "Dezhao Luo",
      "Chang Liu",
      "Yu Zhou",
      "Zhenjun Han"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Jiang_Feature_Hourglass_Network_for_Skeleton_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Jiang_Feature_Hourglass_Network_for_Skeleton_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Geometric shape understanding provides an intuitive representation of object shapes. Skeleton is typical geometrical information. Lots of traditional approaches are developed for skeleton extraction and pruning, while it is still a new area to investigate deep learning for geometric shape understanding. In this paper, we build a fully convolutional network named Feature Hourglass Network (FHN) for skeleton detection. FHN uses rich features of a fully convolutional network by hierarchically integrating side-outputs with a deep-to-shallow manner to decrease the residual between the prediction result and the ground-truth. Experiment data shows that FHN achieves better performance compared with baseline on both Pixel SkelNetOn and Point SkelNetOn datasets."
  },
  "cvpr2019_skelneton_pyramidu-networkforskeletonextractionfromshapepoints": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "Pyramid U-Network for Skeleton Extraction From Shape Points",
    "authors": [
      "Rowel Atienza"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Atienza_Pyramid_U-Network_for_Skeleton_Extraction_From_Shape_Points_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Atienza_Pyramid_U-Network_for_Skeleton_Extraction_From_Shape_Points_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The knowledge about the skeleton of a given geometric shape has many practical applications such as shape animation, shape comparison, shape recognition, and estimating structural strength. Skeleton extraction becomes a more challenging problem when the topology is represented in point cloud domain. In this paper, we present the network architecture, PSPU-SkelNet, for TeamPH which ranked 3rd in Point SkelNetOn 2019 challenge. PSPU-SkelNet is a pyramid of three U-Nets that predicts the skeleton from a given shape point cloud. PSPU-SkelNet achieves a Chamfer Distance (CD) of 2.9105 on the final test dataset. The code of PSPU SkelNet is available at https://github.com/roatienza/skelnet."
  },
  "cvpr2019_skelneton_skeletonnetshapepixeltoskeletonpixel": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "SkeletonNet: Shape Pixel to Skeleton Pixel",
    "authors": [
      "Sabari Nathan",
      "Priya Kansal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Nathan_SkeletonNet_Shape_Pixel_to_Skeleton_Pixel_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Nathan_SkeletonNet_Shape_Pixel_to_Skeleton_Pixel_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep Learning for Geometric Shape Understating has organized a challenge for extracting different kinds of skeletons from the images of different objects. This competition is organized in association with CVPR 2019. There are three different tracks of this competition. The present manuscript describes the method used to train the model for the dataset provided in the first track. The first track aims to extract skeleton pixels from the shape pixels of 89 different objects. For the purpose of extracting the skeleton, a U-net model which is comprised of an encoder-decoder structure has been used. In our proposed architecture, unlike the plain decoder in the traditional U net, we have designed the decoder in the format of HED architecture, wherein we have introduced 4 side layers and fused them to one dilation convolutional layer to connect the broken links of the skeleton. Our proposed architecture achieved the F1 score of 0.77 on test data."
  },
  "cvpr2019_skelneton_u-netbasedconvolutionalneuralnetworkforskeletonextraction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SkelNetOn",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Learning for Geometric Shape Understanding Workshop",
    "title": "U-Net Based Convolutional Neural Network for Skeleton Extraction",
    "authors": [
      "Oleg Panichev",
      "Alona Voloshyna"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SkelNetOn/Panichev_U-Net_Based_Convolutional_Neural_Network_for_Skeleton_Extraction_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SkelNetOn/Panichev_U-Net_Based_Convolutional_Neural_Network_for_Skeleton_Extraction_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Skeletonization is a process aimed to extract a line-like object shape representation, skeleton, which is of great interest for optical character recognition, shape-based object matching, recognition, biomedical image analysis, etc.. Existing methods for skeleton extraction are typically based on topological, morphological or distance transform and are known to be sensitive to the noise on the boundary and require post-processing procedure for redundant branches pruning. In this work, we introduce U-net based approach for direct skeleton extraction of the object within Pixel SkelNetOn - CVPR 2019 challenge, inspired by CNNs success in skeleton extraction from real images task. The main idea of our approach is to consistently edit a skeleton mask by feature propagation through different scale layers. It opposes final skeleton generation from different scale object shape representations as occurs in approaches with deep supervision for skeleton extraction from the real image.Our U-net based model showed 0.75 F1-score on the validation set and the ensemble of eight identical models, trained on different data subsets, got 0.7846 F1-score on the test data."
  },
  "cvpr2019_mbccv_changingtheimagememorabilityfrombasicphotoeditingtogans": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MBCCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Mutual benefits of cognitive and computer vision",
    "title": "Changing the Image Memorability: From Basic Photo Editing to GANs",
    "authors": [
      "Oleksii Sidorov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MBCCV/Sidorov_Changing_the_Image_Memorability_From_Basic_Photo_Editing_to_GANs_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MBCCV/Sidorov_Changing_the_Image_Memorability_From_Basic_Photo_Editing_to_GANs_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Memorability is considered to be an important characteristic of visual content, whereas for advertisement and educational purposes it is often crucial. Despite numerous studies on understanding and predicting image memorability, there are almost no achievements in memorability modification. In this work, we study two approaches to image editing - GAN and classical image processing - and show their impact on memorability.The visual features which influence memorability directly stay unknown till now, hence it is impossible to control it manually. As a solution, we let GAN learn it deeply using labeled data, and then use it for conditional generation of new images. By analogy with algorithms which edit facial attributes, we consider memorability as yet another attribute and operate with it in the same way. Obtained data is also interesting for analysis, simply because there are no real-world examples of successful change of image memorability while preserving its other attributes. We believe this may give many new answers to the question \"what makes an image memorable?\" Apart from that we also study the influence of conventional photo-editing tools (Photoshop, Instagram, etc.) used daily by a wide audience on memorability. In this case, we start from real practical methods and study it using statistics and recent advances in memorability prediction. Photographers, designers, and advertisers will benefit from the results of this study directly."
  },
  "cvpr2019_mbccv_isimagememorabilitypredictionsolved?": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MBCCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Mutual benefits of cognitive and computer vision",
    "title": "Is Image Memorability Prediction Solved?",
    "authors": [
      "Shay Perera",
      "Ayellet Tal",
      "Lihi Zelnik-Manor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MBCCV/Perera_Is_Image_Memorability_Prediction_Solved_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MBCCV/Perera_Is_Image_Memorability_Prediction_Solved_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper deals with the prediction of the memorability of a given image. We start by proposing an algorithm that reaches human-level performance on the LaMem dataset--the only large scale benchmark for memorability prediction. The suggested algorithm is based on three observations we make regarding convolutional neural networks (CNNs) that affect memorability prediction. Having reached human-level performance we were humbled, and asked ourselves whether indeed we have resolved memorability prediction--and answered this question in the negative. We studied a few factors and made some recommendations that should be taken into account when designing the next benchmark."
  },
  "cvpr2019_mbccv_susinetsee,understandandsummarizeit": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MBCCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Mutual benefits of cognitive and computer vision",
    "title": "SUSiNet: See, Understand and Summarize It",
    "authors": [
      "Petros Koutras",
      "Petros Maragos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MBCCV/Koutras_SUSiNet_See_Understand_and_Summarize_It_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MBCCV/Koutras_SUSiNet_See_Understand_and_Summarize_It_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work we propose a multi-task spatio-temporal network, called SUSiNet, that can jointly tackle the spatio-temporal problems of saliency estimation, action recognition and video summarization. Our approach employs a single network that is jointly end-to-end trained for all tasks with multiple and diverse datasets related to the exploring tasks. The proposed network uses a unified architecture that includes global and task specific layer and produces multiple output types, i.e., saliency maps or classification labels, by employing the same video input. Moreover, one additional contribution is that the proposed network can be deeply supervised through an attention module that is related to human attention as it is expressed by eye-tracking data. From the extensive evaluation, on seven different datasets, we have observed that the multi-task network performs as well as the state-of-the-art single-task methods (or in some cases better), while it requires less computational budget than having one independent network per each task."
  },
  "cvpr2019_mbccv_visualattentioninmulti-labelimageclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MBCCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Mutual benefits of cognitive and computer vision",
    "title": "Visual Attention in Multi-Label Image Classification",
    "authors": [
      "Yan Luo",
      "Ming Jiang",
      "Qi Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MBCCV/Luo_Visual_Attention_in_Multi-Label_Image_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MBCCV/Luo_Visual_Attention_in_Multi-Label_Image_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " One of the most significant challenges in multi-label image classification is the learning of representative features that capture the rich semantic information in a cluttered scene. As an information bottleneck, the visual attention mechanism allows humans to selectively process the most important visual input, enabling rapid and accurate scene understanding. In this work, we study the correlation between visual attention and multi-label image classification, and exploit an extra attention pathway for improving multi-label image classification performance. Specifically, we propose a dual-stream neural network that consists of two sub-networks: one is a conventional classification model and the other is a saliency prediction model trained with human fixations. Features computed with the two sub-networks are trained separately and then fine-tuned jointly using a multiple cross entropy loss. Experimental results show that the additional saliency sub-network improves multi-label image classification performance on the MS COCO dataset. The improvement is consistent across various levels of scene clutterness."
  },
  "cvpr2019_mbccv_benchmarkinggazepredictionforcategoricalvisualsearch": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MBCCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Mutual benefits of cognitive and computer vision",
    "title": "Benchmarking Gaze Prediction for Categorical Visual Search",
    "authors": [
      "Gregory Zelinsky",
      "Zhibo Yang",
      "Lihan Huang",
      "Yupei Chen",
      "Seoyoung Ahn",
      "Zijun Wei",
      "Hossein Adeli",
      "Dimitris Samaras",
      "Minh Hoai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MBCCV/Zelinsky_Benchmarking_Gaze_Prediction_for_Categorical_Visual_Search_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MBCCV/Zelinsky_Benchmarking_Gaze_Prediction_for_Categorical_Visual_Search_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Movements of human attention during free viewing have received wide interest in the computer vision community. However, search behavior, where the fixation scanpaths are highly dependent on the viewer's goals, has received much less attention, even though visual search constitutes much of human everyday behavior. One reason is the absence of real-world image datasets on which models of search can be trained. In this paper we present a carefully created dataset for two target categories, microwaves and clocks, curated from the COCO2014 dataset. A total of 2183 images were presented to multiple participants, who were tasked to search for one of the two categories. This yields a total of 16184 validated fixations used for training, making our microwave-clock dataset currently one of the largest datasets of eye fixations in categorical search. Another contribution is our collection of a 40-image testing dataset, where images contained both a microwave and a clock target. Distinct fixation patterns emerged depending on whether participants searched for a microwave (n=30) or a clock (n=30) in the same images. Models therefore had to predict different search scanpaths from the same pixel inputs. This dataset will provide a useful testbed for methods of generating category-specific priority maps for the modeling of visual search behavior. We have implemented a number of state-of-the-art models that will be made available with the dataset, together with a protocol for quantitative and qualitative evaluations. "
  },
  "cvpr2019_mbccv_ferattfacialexpressionrecognitionwithattentionnet": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MBCCV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Mutual benefits of cognitive and computer vision",
    "title": "FERAtt: Facial Expression Recognition With Attention Net",
    "authors": [
      "Pedro D. Marrero Fernandez",
      "Fidel A. Guerrero Pena",
      "Tsang Ing Ren",
      "Alexandre Cunha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MBCCV/Fernandez_FERAtt_Facial_Expression_Recognition_With_Attention_Net_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MBCCV/Fernandez_FERAtt_Facial_Expression_Recognition_With_Attention_Net_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a new end-to-end network architecture for facial expression recognition with an attention model. It focuses attention in the human face and uses a Gaussian space representation for expression recognition. We devise this architecture based on two fundamental complementary components: (1) facial image correction and attention and (2) facial expression representation and classification. The first component uses an encoder-decoder style network and a convolutional feature extractor that are pixel-wise multiplied to obtain a feature attention map. The second component is responsible for obtaining an embedded representation and classification of the facial expression. We propose a loss function that creates a Gaussian structure on the representation space. To demonstrate the proposed method, we create two larger and more comprehensive synthetic datasets using the traditional BU3DFE and CK+ facial datasets. We compared results with the PreActResNet18 baseline. Our experiments on these datasets have shown the superiority of our approach in recognizing facial expressions."
  },
  "cvpr2019_amfg_arealisticdatasetandbaselinetemporalmodelforearlydrowsinessdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection",
    "authors": [
      "Reza Ghoddoosian",
      "Marnim Galib",
      "Vassilis Athitsos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Ghoddoosian_A_Realistic_Dataset_and_Baseline_Temporal_Model_for_Early_Drowsiness_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Ghoddoosian_A_Realistic_Dataset_and_Baseline_Temporal_Model_for_Early_Drowsiness_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Drowsiness can put lives of many drivers and workers in danger. It is important to design practical and easy-to-deploy real-world systems to detect the onset of drowsiness. In this paper, we address early drowsiness detection, which can provide early alerts and offer subjects ample time to react. We present a large and public real-life dataset of 60 subjects, with video segments labeled as alert, low vigilant, or drowsy. This dataset consists of around 30 hours of video, with contents ranging from subtle signs of drowsiness to more obvious ones. We also benchmark a temporal model for our dataset, which has low computational and storage demands. The core of our proposed method is a Hierarchical Multiscale Long Short-Term Memory (HM-LSTM) network, that is fed by detected blink features in sequence. Our experiments demonstrate the relationship between the sequential blink features and drowsiness. In the experimental results, our baseline method produces higher accuracy than human judgment."
  },
  "cvpr2019_amfg_stackedmulti-targetnetworkforrobustfaciallandmarklocalisation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Stacked Multi-Target Network for Robust Facial Landmark Localisation",
    "authors": [
      "Yun Yang",
      "Bing Yu",
      "Xiaodong Li",
      "Bailan Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Yang_Stacked_Multi-Target_Network_for_Robust_Facial_Landmark_Localisation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Yang_Stacked_Multi-Target_Network_for_Robust_Facial_Landmark_Localisation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We thoroughly analyse regression-based face alignment methods and introduce a novel stacked multi-target network for robust facial landmark localisation. The primary heatmap regression-based network concentrates on locating the coarse position of pre-defined landmarks while the secondary coordinate regression-based network is responsible for modelling fine sub-pixel features. Specifically, we elaborate the differences among widely-used Cross Entropy related loss functions and propose a new Bilateral Inhibition Cross Entropy loss function, which enlarges the margin between elements in the output heatmaps.Besides, in order to deal with the discrepancy between optimization and evaluation, we propose to dynamically adjust the radius of kernel function during the training process. We demonstrate that training with decreasing radius in temporal order performs much better than assigning it spatially, i.e. decreasing radius along the stages of stacked hourglass networks. Finally, we innovatively limit the output of the secondary coordinate regression network to a reasonable range by importing the hinge loss to refine the coarse coordinate locations for sub-pixel accuracy. Extensive experiments on public datasets such as 300-W, COFW, and AFLW demonstrate that our proposed method performs superiorly to the state-of-the-art approaches."
  },
  "cvpr2019_amfg_analysisofdeepfusionstrategiesformulti-modalgesturerecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Analysis of Deep Fusion Strategies for Multi-Modal Gesture Recognition",
    "authors": [
      "Alina Roitberg",
      "Tim Pollert",
      "Monica Haurilet",
      "Manuel Martin",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Roitberg_Analysis_of_Deep_Fusion_Strategies_for_Multi-Modal_Gesture_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Roitberg_Analysis_of_Deep_Fusion_Strategies_for_Multi-Modal_Gesture_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Video-based gesture recognition has a wide spectrum of applications, ranging from sign language understanding to driver monitoring in autonomous cars. As different sensors suffer from their individual limitations, combining multiple sources has strong potential to improve the results. A number of deep architectures have been proposed to recognize gestures from e.g. both color and depth data. However, these models conventionally comprise separate networks for each modality, which are then combined in the final layer (e.g. via simple score averaging). In this work, we take a closer look at different fusion strategies for gesture recognition especially focusing on the information exchange in the intermediate layers. We compare three fusion strategies on the widely used C3D architecture: 1) late fusion, combining the streams in the final layer; 2) information exchange in an intermediate layer using an additional convolution layer; and 3) linking information at multiple layers simultaneously using the cross-stitch units, originally designed for multi-task learning. Our proposed C3D-Stitch model achieves the best recognition rate, demonstrating the effectiveness of sharing information at earlier stages."
  },
  "cvpr2019_amfg_lbvcnnlocalbinaryvolumeconvolutionalneuralnetworkforfacialexpressionrecognitionfromimagesequences": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "LBVCNN: Local Binary Volume Convolutional Neural Network for Facial Expression Recognition From Image Sequences",
    "authors": [
      "Sudhakar Kumawat",
      "Manisha Verma",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Kumawat_LBVCNN_Local_Binary_Volume_Convolutional_Neural_Network_for_Facial_Expression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Kumawat_LBVCNN_Local_Binary_Volume_Convolutional_Neural_Network_for_Facial_Expression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recognizing facial expressions is one of the central problems in computer vision. Temporal image sequences have useful spatio-temporal features for recognizing expressions.In this paper, we propose a new 3D Convolution Neural Network (CNN) that can be trained end-to-end for facial expression recognition on temporal image sequences without using facial landmarks. More specifically, a novel 3D convolutional layer that we call Local Binary Volume (LBV) layer is proposed. The LBV layer, when used with our newly proposed LBVCNN network, achieve comparable results compared to state-of-the-art landmark-based or without landmark-based models on image sequences from CK+, Oulu-CASIA, and UNBC McMaster shoulder pain datasets. Furthermore,our LBV layer reduces the number of trainable parameters by a significant amount when compared to a conventional 3D convolutional layer. As a matter of fact, when compared to a 3x3x3 conventional 3D convolutional layer, the LBV layer uses 27 times less trainable parameters."
  },
  "cvpr2019_amfg_personalizedestimationofengagementfromvideosusingactivelearningwithdeepreinforcementlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Personalized Estimation of Engagement From Videos Using Active Learning With Deep Reinforcement Learning",
    "authors": [
      "Ognjen (Oggi) Rudovic",
      "Hae Won Park",
      "John Busche",
      "Bjorn Schuller",
      "Cynthia Breazeal",
      "Rosalind W. Picard"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Rudovic_Personalized_Estimation_of_Engagement_From_Videos_Using_Active_Learning_With_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Rudovic_Personalized_Estimation_of_Engagement_From_Videos_Using_Active_Learning_With_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Perceiving users' engagement accurately is important for technologies that need to respond to learners in a natural and intelligent way. In this paper, we address the problem of automated estimation of engagement from videos of child-robot interactions recorded in unconstrained environments (kindergartens). This is challenging due to diverse and person-specific styles of engagement expressions through facial and body gestures, as well as because of illumination changes, partial occlusion, and a changing background in the classroom as each child is active. To tackle these difficult challenges, we propose a novel deep reinforcement learning architecture for active learning and estimation of engagement from video data. The key to our approach is the learning of a personalized policy that enables the model to decide whether to estimate the child's engagement level (low, medium, high) or, when uncertain, to query a human for a video label. Queried videos are labeled by a human expert in an offline manner, and used to personalize the policy and engagement classifier to a target child over time. We show on a database of 43 children involved in robot-assisted learning activities (8 sessions over 3 months), that this combined human-AI approach can easily adapt its interpretations of engagement to the target child using only a handful of labeled videos, while being robust to the many complex influences on the data. The results show large improvements over a non-personalized approach and over traditional active learning methods."
  },
  "cvpr2019_amfg_apaadaptiveposealignmentforrobustfacerecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "APA: Adaptive Pose Alignment for Robust Face Recognition",
    "authors": [
      "Zhanfu An",
      "Weihong Deng",
      "Yaoyao Zhong",
      "Yaohai Huang",
      "Xunqiang Tao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/An_APA_Adaptive_Pose_Alignment_for_Robust_Face_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/An_APA_Adaptive_Pose_Alignment_for_Robust_Face_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose a new face alignment method, called adaptive pose alignment (APA) which can greatly reduce the intra-class difference and correct the noise caused by the traditional method in the alignment process, especially in unconstrained settings. Instead of aligning all faces to the pre-defined, uniform frontal shape, we adaptively learn the alignment templates according to the facial poses and then align each face of training or testing sets to its related template. To further improve the face recognition performance, we propose a simple, yet effective feature normalization method which can generate more discriminative feature representation of a face or template combined with the APA method. Furthermore, we introduce a poseinvariant face recognition pipeline that sequentially applies APA based alignment, deep representation by Softmax or Arcface, and the effective feature normalization procedure. We empirically show that APA based images can accelerate the training of deep face recognition model by aligning all the images to the optimal templates. Moreover, experiments show that the proposed method achieves the state-of-theart performance on challenging IJB-A, IJB-C and CPLFW datasets."
  },
  "cvpr2019_amfg_expressionclassificationinchildrenusingmeansuperviseddeepboltzmannmachine": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Expression Classification in Children Using Mean Supervised Deep Boltzmann Machine",
    "authors": [
      "Shruti Nagpal",
      "Maneet Singh",
      "Mayank Vatsa",
      "Richa Singh",
      "Afzel Noore"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Nagpal_Expression_Classification_in_Children_Using_Mean_Supervised_Deep_Boltzmann_Machine_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Nagpal_Expression_Classification_in_Children_Using_Mean_Supervised_Deep_Boltzmann_Machine_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Automated facial expression classification has widespread application in multiple domains such as human computer interaction, health and entertainment, biometrics, and security. There are six basic facial expressions: Anger, Disgust, Fear, Happiness, Sadness, and Surprise, apart from a neutral state. Most of the research in expression classification has focused on adult face images, with no dedicated research on automating expression classification for children. To the best of our knowledge, this is the first research which presents a deep learning based expression classification approach for children. A novel supervised deep learning formulation, termed as Mean Supervised Deep Boltzmann Machine (msDBM) is proposed which classifies an input face image into one of the seven expression classes. The proposed approach has been evaluated on two child face datasets - Radboud Faces and CAFE, along with experiments on the adult face images of the Radboud Faces dataset. Experimental results and analysis reinforces the challenging nature of the task at hand, and the effectiveness of the proposed msDBM model."
  },
  "cvpr2019_amfg_understandingbeautyviadeepfacialfeatures": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Understanding Beauty via Deep Facial Features",
    "authors": [
      "Xudong Liu",
      "Tao Li",
      "Hao Peng",
      "Iris Chuoying Ouyang",
      "Taehwan Kim",
      "Ruizhe Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Liu_Understanding_Beauty_via_Deep_Facial_Features_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Liu_Understanding_Beauty_via_Deep_Facial_Features_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The concept of beauty has been debated by philosophers and psychologists for centuries, but most definitions are subjective and metaphysical, and deficit in accuracy, generality, and scalability. In this paper, we present a novel study on mining beauty semantics of facial attributes based on big data, with an attempt to objectively construct descriptions of beauty in a quantitative manner. We first deploy a deep Convolutional Neural Network (CNN) to extract facial attributes, and then investigate correlations between these features and attractiveness on two large-scale datasets labelled with beauty scores. Not only do we discover the secrets of beauty verified by statistical significance tests, our findings also align perfectly with existing psychological studies that, e.g., small nose, high cheekbones, and femininity contribute to attractiveness. We further leverage these high-level representations to original images by a generative adversarial network (GAN). Beauty enhancements after synthesis are visually compelling and statistically convincing verified by a user survey of 10,000 data points."
  },
  "cvpr2019_amfg_modellingmulti-channelemotionsusingfacialexpressionandtrajectorycuesforimprovingsocially-awarerobotnavigation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Modelling Multi-Channel Emotions Using Facial Expression and Trajectory Cues for Improving Socially-Aware Robot Navigation",
    "authors": [
      "Aniket Bera",
      "Tanmay Randhavane",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Bera_Modelling_Multi-Channel_Emotions_Using_Facial_Expression_and_Trajectory_Cues_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Bera_Modelling_Multi-Channel_Emotions_Using_Facial_Expression_and_Trajectory_Cues_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Using facial expressions and trajectory signals, we present an emotion-aware navigation algorithm for social robots. Our approach uses a combination of Bayesian-inference, CNN-based learning and the Pleasure-Arousal-Dominance model from psychology to estimate time-varying emotional behaviors of pedestrians from their faces and trajectories. For each pedestrian, these PAD characteristics are used to generate proxemic constraints. We use a multi-channel model to classify pedestrian features into four categories of emotions (happy, sad, angry, neutral). We observe an emotional detection accuracy of 85.33% in our validation results. In low-to medium-density environments, we formulate emotion-based proxemic constraints to perform socially conscious robot navigation. With Pepper, a social humanoid robot, we demonstrate the benefits of our algorithm in simulated environments with tens of pedestrians as well as in a real world setting. "
  },
  "cvpr2019_amfg_efficientandaccuratefacealignmentbyglobalregressionandcascadedlocalrefinement": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Efficient and Accurate Face Alignment by Global Regression and Cascaded Local Refinement",
    "authors": [
      "Jinzhan Su",
      "Zhe Wang",
      "Chunyuan Liao",
      "Haibin Ling"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Su_Efficient_and_Accurate_Face_Alignment_by_Global_Regression_and_Cascaded_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Su_Efficient_and_Accurate_Face_Alignment_by_Global_Regression_and_Cascaded_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Despite great advances witnessed on facial image alignment in recent years, high accuracy high speed face alignment algorithms still have rooms to improve especially for applications where computation resources are limited. Addressing this issue, we propose a new face landmark localization algorithm by combining global regression and local refinement. In particular, for a given image, our algorithm first estimates its global facial shape through a global regression network (GRegNet) and then using cascaded local refinement networks (LRefNet) to sequentially improve the alignment result. Compared with previous face alignment algorithms, our key innovation is the sharing of low level features in GRegNet with LRefNet. Such feature sharing not only significantly improves the algorithm efficiency, but also allows full exploration of rich locality-sensitive details carried with shallow network layers and consequently boosts the localization accuracy. The advantages of our algorithm is clearly validated in our thorough experiments on four popular face alignment benchmarks, 300-W, AFLW, COFW and WFLW. On all datasets, our algorithm produces state-of-the-art alignment accuracy, while enjoys the smallest computational complexity."
  },
  "cvpr2019_amfg_2d-3dheterogeneousfacerecognitionbasedondeepcoupledspectralregression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "2D-3D Heterogeneous Face Recognition Based on Deep Coupled Spectral Regression",
    "authors": [
      "Yangtao Zheng",
      "Di Huang",
      "Weixin Li",
      "Shupeng Wang",
      "Yunhong Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Zheng_2D-3D_Heterogeneous_Face_Recognition_Based_on_Deep_Coupled_Spectral_Regression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Zheng_2D-3D_Heterogeneous_Face_Recognition_Based_on_Deep_Coupled_Spectral_Regression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " As one of the major branches in Face Recognition (FR), 2D-3D Heterogeneous FR (HFR), where face comparison is achieved across the texture and shape modalities, has become more important. This paper proposes a novel deep learning based end-to-end approach, namely Deep Coupled Spectral Regression (DCSR), for such an issue. It jointly makes use of both the advantages of CNN based deep features and CSR based common subspace. Specifically, from 2D texture and 3D depth face maps, DCSR extracts more powerful features by a deep network with the cross-modality triplet loss, which show much better uniqueness and robustness than the hand-crafted ones. Further, DCSR learns the shared space between different modalities with the constraints of sample labels, and is thereby more discriminative than the widely used unsupervised methods. More importantly, the two steps above are integrated through a couple layer to explicitly optimize the weights of deep features and projection directions rather than a simple combination. Experiments are carried out on the FRGC v2.0 database, and the results reported clearly demonstrate the competency of our proposed method. Its generalization ability is also validated by additional experiments conducted on the CASIA NIR-VIS 2.0 database."
  },
  "cvpr2019_amfg_accurate3dfacereconstructionwithweakly-supervisedlearningfromsingleimagetoimageset": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Accurate 3D Face Reconstruction With Weakly-Supervised Learning: From Single Image to Image Set",
    "authors": [
      "Yu Deng",
      "Jiaolong Yang",
      "Sicheng Xu",
      "Dong Chen",
      "Yunde Jia",
      "Xin Tong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AMFG/Deng_Accurate_3D_Face_Reconstruction_With_Weakly-Supervised_Learning_From_Single_Image_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AMFG/Deng_Accurate_3D_Face_Reconstruction_With_Weakly-Supervised_Learning_From_Single_Image_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, deep learning based 3D face reconstruction methods have shown promising results in both quality and efficiency. However, training deep neural networks typically requires a large volume of data, whereas face images with ground-truth 3D face shapes are scarce. In this paper, we propose a novel deep 3D face reconstruction approach that 1) leverages a robust, hybrid loss function for weakly-supervised learning which takes into account both low-level and perception-level information for supervision, and 2) performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation. Our method is fast, accurate, and robust to occlusion and large pose. We provide comprehensive experiments on MICC Florence and Facewarehouse datasets, systematically comparing our method with fifteen recent methods and demonstrating its state-of-the-art performance. Code available at https://github.com/Microsoft/Deep3DFaceReconstruction"
  },
  "cvpr2019_wad_complexer-yoloreal-time3dobjectdetectionandtrackingonsemanticpointclouds": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds",
    "authors": [
      "Martin Simon",
      "Karl Amende",
      "Andrea Kraus",
      "Jens Honer",
      "Timo Samann",
      "Hauke Kaulbersch",
      "Stefan Milz",
      "Horst Michael Gross"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Simon_Complexer-YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_Point_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Simon_Complexer-YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_Point_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20% and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection."
  },
  "cvpr2019_wad_multinet++multi-streamfeatureaggregationandgeometriclossstrategyformulti-tasklearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "MultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning",
    "authors": [
      "Sumanth Chennupati",
      "Ganesh Sistu",
      "Senthil Yogamani",
      "Samir A Rawashdeh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Chennupati_MultiNet_Multi-Stream_Feature_Aggregation_and_Geometric_Loss_Strategy_for_Multi-Task_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Chennupati_MultiNet_Multi-Stream_Feature_Aggregation_and_Geometric_Loss_Strategy_for_Multi-Task_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": "Multi-task learning is commonly used in autonomous driving for solving various visual perception tasks. It offers significant benefits in terms of both performance and computational complexity. Current work on multi-task learning networks focus on processing a single input image and there is no known implementation of multi-task learning handling a sequence of images. In this work, we propose a multi-stream multi-task network to take advantage of using feature representations from preceding frames in a video sequence for joint learning of segmentation, depth, and motion. The weights of the current and previous encoder are shared so that features computed in the previous frame can be leveraged without additional computation. In addition, we propose to use the geometric mean of task losses as a better alternative to the weighted average of task losses. The proposed loss function facilitates better handling of the difference in convergence rates of different tasks. Experimental results on KITTI, Cityscapes and SYNTHIA datasets demonstrate that the proposed strategies outperform various existing multi-task learning solutions. "
  },
  "cvpr2019_wad_unsuperviseddomainadaptationforsemanticsegmentationofurbanscenes": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Unsupervised Domain Adaptation for Semantic Segmentation of Urban Scenes",
    "authors": [
      "Matteo Biasetton",
      "Umberto Michieli",
      "Gianluca Agresti",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Biasetton_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Biasetton_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The semantic understanding of urban scenes is one of the key components for an autonomous driving system. Complex deep neural networks for this task require to be trained with a huge amount of labeled data, which is difficult and expensive to acquire. A recently proposed workaround is the usage of synthetic data, however the differences between real world and synthetic scenes limit the performance. We propose an unsupervised domain adaptation strategy to adapt a synthetic supervised training to real world data. The proposed learning strategy exploits three components: a standard supervised learning on synthetic data, an adversarial learning strategy able to exploit both labeled synthetic data and unlabeled real data and finally a self-teaching strategy working on unlabeled data only. The last component is guided by the segmentation confidence, estimated by the fully convolutional discriminator of the adversarial learning module, helping to further reduce the domain shift between synthetic and real data. Furthermore we weighted this loss on the basis of the class frequencies to enhance the performance on less common classes. Experimental results prove the effectiveness of the proposed strategy in adapting a segmentation network trained on synthetic datasets, like GTA5 and SYNTHIA, to a real dataset as Cityscapes."
  },
  "cvpr2019_wad_railsem19adatasetforsemanticrailsceneunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "RailSem19: A Dataset for Semantic Rail Scene Understanding",
    "authors": [
      "Oliver Zendel",
      "Markus Murschitz",
      "Marcel Zeilinger",
      "Daniel Steininger",
      "Sara Abbasi",
      "Csaba Beleznai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Zendel_RailSem19_A_Dataset_for_Semantic_Rail_Scene_Understanding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Zendel_RailSem19_A_Dataset_for_Semantic_Rail_Scene_Understanding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Solving tasks for autonomous road vehicles using computer vision is a dynamic and active research field.However, one aspect of autonomous transportation has received little contributions: the rail domain. In this paper, we introduce the first public dataset for semantic scene understanding for trains and trams: RailSem19. This dataset consists of 8500 annotated short sequences from the ego-perspective of trains, including over 1000 examples with railway crossings and 1200 tram scenes.Since it is the first image dataset targeting the rail domain, a novel label policy has been designed from scratch. It focuses on rail-specific labels not covered by any other datasets. In addition to manual annotations in the form of geometric shapes, we also supply dense pixel-wise semantic labeling. The dense labeling is a semantic-aware combination of (a) the geometric shapes and (b) weakly supervised annotations generated by existing semantic segmentation networks from the road domain. Finally, multiple experiments give a first impression on how the new dataset can be used to improve semantic scene understanding in the rail environment. We present prototypes for the image-based classification of trains, switches, switch states, platforms, buffer stops, rail traffic signs and rail traffic lights.Applying transfer learning, we present an early prototype for pixel-wise semantic segmentation on rail scenes. The resulting predictions show that this new data also significantly improves scene understanding in situations where cars and trains interact."
  },
  "cvpr2019_wad_sensorfusionforjoint3dobjectdetectionandsemanticsegmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation",
    "authors": [
      "Gregory P. Meyer",
      "Jake Charland",
      "Darshan Hegde",
      "Ankit Laddha",
      "Carlos Vallespi-Gonzalez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Meyer_Sensor_Fusion_for_Joint_3D_Object_Detection_and_Semantic_Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Meyer_Sensor_Fusion_for_Joint_3D_Object_Detection_and_Semantic_Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present an extension to LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector. We propose a method for fusing image data with the LiDAR data and show that this sensor fusion method improves the detection performance of the model especially at long ranges. The addition of image data is straightforward and does not require image labels. Furthermore, we expand the capabilities of the model to perform 3D semantic segmentation in addition to 3D object detection. On a large benchmark dataset, we demonstrate our approach achieves state-of-the-art performance on both object detection and semantic segmentation while maintaining a low runtime."
  },
  "cvpr2019_wad_6d-vnetend-to-end6-dofvehicleposeestimationfrommonocularrgbimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "6D-VNet: End-To-End 6-DoF Vehicle Pose Estimation From Monocular RGB Images",
    "authors": [
      "Di Wu",
      "Zhaoyong Zhuang",
      "Canqun Xiang",
      "Wenbin Zou",
      "Xia Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Wu_6D-VNet_End-To-End_6-DoF_Vehicle_Pose_Estimation_From_Monocular_RGB_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Wu_6D-VNet_End-To-End_6-DoF_Vehicle_Pose_Estimation_From_Monocular_RGB_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a conceptually simple framework for 6DoF object pose estimation, especially for autonomous driving scenario. Our approach efficiently detects traffic participants in a monocular RGB image while simultaneously regressing their 3D translation and rotation vectors. The method, called 6D-VNet, extends Mask R-CNN by adding customised heads for predicting vehicle's finer class, rotation and translation. The proposed 6D-VNet is trained end-to-end compared to previous methods. Furthermore, we show that the inclusion of translational regression in the joint losses is crucial for the 6DoF pose estimation task, where object translation distance along longitudinal axis varies significantly, e.g., in autonomous driving scenarios. Additionally, we incorporate the mutual information between traffic participants via a modified non-local block. As opposed to the original non-local block implementation, the proposed weighting modification takes the spatial neighbouring information into consideration whilst counteracting the effect of extreme gradient values. Our 6D-VNet reaches the 1 st place in ApolloScape challenge 3D Car Instance task. Code has been made available at: https://github.com/stevenwudi/6DVNET ."
  },
  "cvpr2019_wad_rgb-dindoormappingusingdeepfeatures": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "RGB-D Indoor Mapping Using Deep Features",
    "authors": [
      "Oguzhan Guclu",
      "Ali Caglayan",
      "Ahmet Burak Can"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Guclu_RGB-D_Indoor_Mapping_Using_Deep_Features_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Guclu_RGB-D_Indoor_Mapping_Using_Deep_Features_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " RGB-D indoor mapping has been an active research topic in the last decade with the advance of depth sensors. However, despite the great success of deep learning techniques on various problems, similar approaches for SLAM have not been much addressed yet. In this work, an RGB-D SLAM system using a deep learning approach for mapping indoor environments is proposed. A pre-trained CNN model with multiple random recursive structures is utilized to acquire deep features in an efficient way with no need for training. Deep features present strong representations from color frames and enable better data association. To increase computational efficiency, deep feature vectors are considered as points in a high dimensional space and indexed in a priority search k-means tree. The search precision is improved by employing an adaptive mechanism. For motion estimation, a sparse feature based approach is adopted by employing a robust keypoint detector and descriptor combination. The system is assessed on TUM RGB-D benchmark using the sequences recorded in medium and large sized environments. The experimental results demonstrate the accuracy and robustness of the proposed system over the state-of-the-art, especially in large sequences."
  },
  "cvpr2019_wad_distancenetestimatingtraveleddistancefrommonocularimagesusingarecurrentconvolutionalneuralnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "DistanceNet: Estimating Traveled Distance From Monocular Images Using a Recurrent Convolutional Neural Network",
    "authors": [
      "Robin Kreuzig",
      "Matthias Ochs",
      "Rudolf Mester"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Kreuzig_DistanceNet_Estimating_Traveled_Distance_From_Monocular_Images_Using_a_Recurrent_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Kreuzig_DistanceNet_Estimating_Traveled_Distance_From_Monocular_Images_Using_a_Recurrent_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Classical monocular vSLAM/VO methods suffer from the scale ambiguity problem. Hybrid approaches solve this problem by adding deep learning methods, for example by using depth maps which are predicted by a CNN. We suggest that it is better to base scale estimation on estimating the traveled distance for a set of subsequent images. In this paper, we propose a novel end-to-end many-to-one traveled distance estimator. By using a deep recurrent convolutional neural network (RCNN), the traveled distance between the first and last image of a set of consecutive frames is estimated by our DistanceNet. Geometric features are learned in the CNN part of our model, which are subsequently used by the RNN to learn dynamics and temporal information. Moreover, we exploit the natural order of distances by using ordinal regression to predict the distance. The evaluation on the KITTI dataset shows that our approach outperforms current state-of-the-art deep learning pose estimators and classical mono vSLAM/VO methods in terms of distance prediction. Thus, our DistanceNet can be used as a component to solve the scale problem and help improve current and future classical mono vSLAM/VO methods."
  },
  "cvpr2019_wad_roadsrandomizationforobstacleavoidanceanddrivinginsimulation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "ROADS: Randomization for Obstacle Avoidance and Driving in Simulation",
    "authors": [
      "Samira Pouyanfar",
      "Muneeb Saleem",
      "Nikhil George",
      "Shu-Ching Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Pouyanfar_ROADS_Randomization_for_Obstacle_Avoidance_and_Driving_in_Simulation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Pouyanfar_ROADS_Randomization_for_Obstacle_Avoidance_and_Driving_in_Simulation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " End-to-end deep learning has emerged as a simple and promising approach for autonomous driving recently. However, collecting large-scale real-world data representing the full spectrum of road scenarios and rare events remains the main hurdle in this area. For this purpose, this paper addresses the problem of end-to-end collision-free deep driving using only simulation data. It extends the idea of domain randomization to bridge the reality gap between simulation and the real world. Using a range of domain randomization flavors in a primitive simulation, it is shown that a model can learn to drive in realistic environments without seeing any real or photo-realistic images. The proposed work dramatically reduces the need for collecting large real-world or high-fidelity simulated datasets, along with allowing for the creation of rare events in the simulation. Finally, this is the first time domain randomization is used for the application of \"deep driving\" which can avoid obstacles. The effectiveness of the proposed method is demonstrated with extensive experiments on both simulation and real-world datasets. "
  },
  "cvpr2019_wad_real-timephysics-basedremovalofshadowsandshadingfromroadsurfaces": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Real-Time Physics-Based Removal of Shadows and Shading From Road Surfaces",
    "authors": [
      "Bruce A. Maxwell",
      "Casey A. Smith",
      "Maan Qraitem",
      "Ross Messing",
      "Spencer Whitt",
      "Nicolas Thien",
      "Richard M. Friedhoff"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Maxwell_Real-Time_Physics-Based_Removal_of_Shadows_and_Shading_From_Road_Surfaces_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Maxwell_Real-Time_Physics-Based_Removal_of_Shadows_and_Shading_From_Road_Surfaces_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a real-time physics-based system for generating an illumination free representation of road surfaces that maintains the distinction between asphalt and painted road markings.Cast shadows on road surfaces can create false features and modify the color of road markings, potentially masking important information for vehicle vision systems. We demonstrate a novel method for identifying the relative spectral properties of the direct and ambient illumination conditions and for using that to create an illumination-free 2D chromaticity space in log RGB. We then show how that representation can be used to generate an illumination-free greyscale representation that distinguishes road, white paint, and yellow paint, making it suitable for further analysis and classification. The entire process runs faster than 30Hz on current automotive-grade embedded processing systems.We evaluate the system on a paint detection task, comparing two types of learned classifiers, random forests and convolutional neural networks. For each type, one classifier is trained on the original images, and the other is trained on the illumination-free greyscale output. The classifiers are of identical complexity and trained on the same size data set. For both types, the classifier trained on the illumination-free outputs performs better, even on images with no cast shadows. The gap in performance is indicative of the cost of forcing a classifier to learn a task in the presence of the confounding illumination signal."
  },
  "cvpr2019_wad_spatialsamplingnetworkforfastsceneunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Spatial Sampling Network for Fast Scene Understanding",
    "authors": [
      "Davide Mazzini",
      "Raimondo Schettini"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Mazzini_Spatial_Sampling_Network_for_Fast_Scene_Understanding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Mazzini_Spatial_Sampling_Network_for_Fast_Scene_Understanding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a network architecture to perform efficient scene understanding. This work presents three main novelties: the first is a module named Improved Guided Upsampling Module that can replace in toto the decoder part in common semantic segmentation networks. Our second contribution is the introduction of a new module based on spatial sampling to perform Instance Segmentation. It provides a very fast instance segmentation needing only a simple post-processing step at inference time. Finally, we propose a novel efficient network design that includes the new modules and test it against different datasets for outdoor scene understanding. To our knowledge,our network is one of the most efficient architectures for scene understanding published to date, furthermore being 8.6% more accurate than the fastest competitor on semantic segmentation and almost five times faster than the most efficient network for instance segmentation."
  },
  "cvpr2019_wad_attentionalpointnetfor3d-objectdetectioninpointclouds": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Attentional PointNet for 3D-Object Detection in Point Clouds",
    "authors": [
      "Anshul Paigwar",
      "Ozgur Erkent",
      "Christian Wolf",
      "Christian Laugier"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate detection of objects in 3D point clouds is a central problem for autonomous navigation. Most existing methods use techniques of hand-crafted features representation or multi-sensor approaches prone to sensor failure. Approaches like PointNet that directly operate on sparse point data have shown good accuracy in the classification of single 3D objects. However, LiDAR sensors on Autonomous Vehicles generate a large scale point cloud. Real-time object detection in such a cluttered environment still remains a challenge. In this study, we propose Attentional PointNet, which is a novel end-to-end trainable deep architecture for object detection in point clouds. We extend the theory of visual attention mechanisms to 3D point clouds and introduce a new recurrent 3D Localization Network module. Rather than processing the whole point cloud, the network learns where to look (finding regions of interest), which significantly reduces the number of points to be processed and inference time. Evaluation on KITTI car detection benchmark shows that our Attentional PointNet achieves comparable results with the state-of-the-art LiDAR-based 3D detection methods in detection and speed."
  },
  "cvpr2019_wad_accuratevisuallocalizationforautomotiveapplications": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Accurate Visual Localization for Automotive Applications",
    "authors": [
      "Eli Brosh",
      "Matan Friedmann",
      "Ilan Kadar",
      "Lev Yitzhak Lavy",
      "Elad Levi",
      "Shmuel Rippa",
      "Yair Lempert",
      "Bruno Fernandez-Ruiz",
      "Roei Herzig",
      "Trevor Darrell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Brosh_Accurate_Visual_Localization_for_Automotive_Applications_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Brosh_Accurate_Visual_Localization_for_Automotive_Applications_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate vehicle localization is a crucial step towards building effective Vehicle-to-Vehicle networks and automotive applications. Yet standard grade GPS data, such as that provided by mobile phones, is often noisy and exhibits significant localization errors in many urban areas. Approaches for accurate localization from imagery often rely on structure-based techniques, and thus are limited in scale and are expensive to compute. In this paper, we present a scalable visual localization approach geared for real-time performance. We propose a hybrid coarse-to-fine approach that leverages visual and GPS location cues. Our solution uses a self-supervised approach to learn a compact road image representation. This representation enables efficient visual retrieval and provides coarse localization cues, which are fused with vehicle ego-motion to obtain high accuracy location estimates.As a benchmark to evaluate the performance of our visual localization approach, we introduce a new large-scale driving dataset based on video and GPS data obtained from a large-scale network of connected dash-cams. Our experiments confirm that our approach is highly effective in challenging urban environments, reducing localization error by an order of magnitude."
  },
  "cvpr2019_wad_dscnetreplicatinglidarpointcloudswithdeepsensorcloning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "DSCnet: Replicating Lidar Point Clouds With Deep Sensor Cloning",
    "authors": [
      "Paden Tomasello",
      "Sammy Sidhu",
      "Anting Shen",
      "Matthew W. Moskewicz",
      "Nobie Redmon",
      "Gayatri Joshi",
      "Romi Phadte",
      "Paras Jain",
      "Forrest Iandola"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Tomasello_DSCnet_Replicating_Lidar_Point_Clouds_With_Deep_Sensor_Cloning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Tomasello_DSCnet_Replicating_Lidar_Point_Clouds_With_Deep_Sensor_Cloning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional neural networks (CNNs) have become increasingly popular for solving a variety of computer vision tasks, ranging from image classification to image segmentation. Recently, autonomous vehicles have created a demand for depth information, which is often obtained using hardware sensors such as Light detection and ranging (LIDAR). Although it can provide precise distance measurements, most LIDARs are still far too expensive to sell in mass-produced consumer vehicles, which has motivated methods to generate depth information from commodity automotive sensors like cameras.In this paper, we propose an approach called Deep Sensor Cloning (DSC). The idea is to use Convolutional Neural Networks in conjunction with inexpensive sensors to replicate the 3D point-clouds that are created by expensive LIDARs. To accomplish this, we develop a new dataset (DSCdata) and a new family of CNN architectures (DSCnets). While previous tasks such as KITTI depth prediction use interpolated RGB-D images as ground-truth for training, we instead use DSCnets to directly predict LIDAR point-clouds.When we compare the output of our models to a 75,000 LIDAR, we find that our most accurate DSCnet achieves a relative error of 5.77% using a single camera and 4.69% using stereo cameras. "
  },
  "cvpr2019_wad_attention-basedhierarchicaldeepreinforcementlearningforlanechangebehaviorsinautonomousdriving": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Attention-Based Hierarchical Deep Reinforcement Learning for Lane Change Behaviors in Autonomous Driving",
    "authors": [
      "Yilun Chen",
      "Chiyu Dong",
      "Praveen Palanisamy",
      "Priyantha Mudalige",
      "Katharina Muelling",
      "John M. Dolan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Chen_Attention-Based_Hierarchical_Deep_Reinforcement_Learning_for_Lane_Change_Behaviors_in_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Chen_Attention-Based_Hierarchical_Deep_Reinforcement_Learning_for_Lane_Change_Behaviors_in_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Performing safe and efficient lane changes is a crucial feature for creating fully autonomous vehicles. Recent advances have demonstrated successful lane following behavior using deep reinforcement learning, yet the interactions with other vehicles on road for lane changes are rarely considered. In this paper, we design a hierarchical Deep Reinforcement Learning (DRL) algorithm to learn lane change behaviors in dense traffic. By breaking down overall behavior to sub-policies, faster and safer lane change actions can be learned. We also apply temporal and spatial attention to the DRL architecture, which helps the vehicle focus more on surrounding vehicles and leads to smoother lane change behavior. We conduct our experiments in the TORCS simulator and the results outperform the state-of-art deep reinforcement learning algorithm in various lane change scenarios."
  },
  "cvpr2019_wad_arguingmachineshumansupervisionofblackboxaisystemsthatmakelife-criticaldecisions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop on Autonomous Driving",
    "title": "Arguing Machines: Human Supervision of Black Box AI Systems That Make Life-Critical Decisions",
    "authors": [
      "Lex Fridman",
      "Li Ding",
      "Benedikt Jenik",
      "Bryan Reimer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/WAD/Fridman_Arguing_Machines_Human_Supervision_of_Black_Box_AI_Systems_That_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/WAD/Fridman_Arguing_Machines_Human_Supervision_of_Black_Box_AI_Systems_That_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We consider the paradigm of a black box AI system that makes life-critical decisions. We propose an \"arguing machines\" framework that pairs the primary AI system with a secondary one that is independently trained to perform the same task. We show that disagreement between the two systems, without any knowledge of underlying system design or operation, is sufficient to arbitrarily improve the accuracy of the overall decision pipeline given human supervision over disagreements. We demonstrate this system in two applications: (1) an illustrative example of image classification and (2) on large-scale real-world semi-autonomous driving data.For the first application, we apply this framework to image classification achieving a reduction from 8.0% to 2.8% top-5 error on ImageNet. For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4% of system disengagements that were labeled by human annotators as challenging and needing human supervision."
  },
  "cvpr2019_bcmcvai_aprobabilisticmodelofthebitcoinblockchain": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "A Probabilistic Model of the Bitcoin Blockchain",
    "authors": [
      "Marc Jourdan",
      "Sebastien Blandin",
      "Laura Wynter",
      "Pralhad Deshpande"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Jourdan_A_Probabilistic_Model_of_the_Bitcoin_Blockchain_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Jourdan_A_Probabilistic_Model_of_the_Bitcoin_Blockchain_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The Bitcoin transaction graph is a public data structure organized as transactions between addresses, each associated with a logical entity. In this work, we introduce a complete probabilistic model of the Bitcoin Blockchain, setting the basis for follow-up AI applications on Bitcoin transactions. We first formulate a set of conditional dependencies induced by the Bitcoin protocol at the block level and derive a corresponding fully observed graphical model of a Bitcoin block. We then extend the model to include hidden entity attributes such as the functional category of the associated logical agent and derive asymptotic bounds on the privacy properties implied by this model. At the network level, we show evidence of complex transaction-to-transaction behavior and present a relevant discriminative model of the agent categories. Performance of both the block-based graphical model and the network-level discriminative model are evaluated on a subset of the public Bitcoin Blockchain."
  },
  "cvpr2019_bcmcvai_archangeltamper-proofingvideoarchivesusingtemporalcontenthashesontheblockchain": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "ARCHANGEL: Tamper-Proofing Video Archives Using Temporal Content Hashes on the Blockchain",
    "authors": [
      "Tu Bui",
      "Daniel Cooper",
      "John Collomosse",
      "Mark Bell",
      "Alex Green",
      "John Sheridan",
      "Jez Higgins",
      "Arindra Das",
      "Jared Keller",
      "Olivier Thereaux",
      "Alan Brown"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Bui_ARCHANGEL_Tamper-Proofing_Video_Archives_Using_Temporal_Content_Hashes_on_the_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Bui_ARCHANGEL_Tamper-Proofing_Video_Archives_Using_Temporal_Content_Hashes_on_the_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present ARCHANGEL; a novel distributed ledger based system for assuring the long-term integrity of digital video archives.First, we describe a novel deep network architecture for computing compact temporal content hashes (TCHs) fromaudio-visual streams with durations of minutes or hours.Our TCHs are sensitive to accidental or malicious content modification (tampering) but invariant to the codec used to encode the video.This is necessary due to the curatorial requirement for archives to format shift video over time to ensure futureaccessibility.Second, we describe how the TCHs (and themodels used to derive them) are secured via a proof-of-authority blockchain distributed across multiple independent archives. We report on the efficacy of ARCHANGEL within the context of a trial deployment in which the national government archives of the United Kingdom, Estonia and Norway participated. "
  },
  "cvpr2019_bcmcvai_exploitingcomputationpowerofblockchainforbiomedicalimagesegmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "Exploiting Computation Power of Blockchain for Biomedical Image Segmentation",
    "authors": [
      "Boyang Li",
      "Changhao Chenli",
      "Xiaowei Xu",
      "Taeho Jung",
      "Yiyu Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Li_Exploiting_Computation_Power_of_Blockchain_for_Biomedical_Image_Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Li_Exploiting_Computation_Power_of_Blockchain_for_Biomedical_Image_Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Biomedical image segmentation based on Deep neural network (DNN) is a promising approach that assists clinical diagnosis. This approach demands enormous computation power because these DNN models are complicated, and the size of the training data is usually very huge. As blockchain technology based on Proof-of-Work (PoW) has been widely used, an immense amount of computation power is consumed to maintain the PoW consensus. In this paper, we propose a design to exploit the computation power of blockchain miners for biomedical image segmentation, which lets miners perform image segmentation as the Proof-of-Useful-Work (PoUW) instead of calculating useless hash values. This work distinguishes itself from other PoUW by addressing various limitations of related others. As the overhead evaluation shown in Section 5 indicates, for U-net and FCN, the average overhead of digital signature is 1.25 seconds and 0.98 seconds, respectively, and the average overhead of network is 3.77 seconds and 3.01 seconds, respectively. These quantitative experiment results prove that the overhead of the digital signature and network is small and comparable to other existing PoUW designs."
  },
  "cvpr2019_bcmcvai_robotworkspacemonitoringusingablockchain-based3dvisionapproach": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "Robot Workspace Monitoring Using a Blockchain-Based 3D Vision Approach",
    "authors": [
      "Vasco Lopes",
      "Nuno Pereira",
      "Luis A. Alexandre"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Lopes_Robot_Workspace_Monitoring_Using_a_Blockchain-Based_3D_Vision_Approach_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Lopes_Robot_Workspace_Monitoring_Using_a_Blockchain-Based_3D_Vision_Approach_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Blockchain has been used extensively for financial purposes, but this technology can also be beneficial in other contexts where multi-party cooperation, security and decentralization of the data is essential. Properties such as immutability, accessibility and non-repudiation and the existence of smart-contracts make blockchain technology very interesting in robotic contexts that require event registration or integration with Artificial Intelligence. In this paper, we propose a system that leverages blockchain as a ledger to register events and information to be processed by Oracles and uses smart-contracts to control robots by adjusting their velocity, or stopping them, if a person enters the robot working space without permission. We show how blockchain can be used in computer vision problems by interacting with multiple external parties, Oracles, that perform image analysis and how it is possible to use multiple smart-contracts for different tasks. The method proposed is shown in a scenario representing a factory environment, but since it is modular, it can be easily adapted and extended for other contexts, allowing for simple integration and maintenance."
  },
  "cvpr2019_bcmcvai_deepringprotectingdeepneuralnetworkwithblockchain": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "DeepRing: Protecting Deep Neural Network With Blockchain",
    "authors": [
      "Akhil Goel",
      "Akshay Agarwal",
      "Mayank Vatsa",
      "Richa Singh",
      "Nalini Ratha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Goel_DeepRing_Protecting_Deep_Neural_Network_With_Blockchain_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Goel_DeepRing_Protecting_Deep_Neural_Network_With_Blockchain_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Several computer vision applications such as object detection and face recognition have started to completely rely on deep learning based architectures. These architectures, when paired with appropriate loss functions and optimizers, produce state-of-the-art results in a myriad of problems. On the other hand, with the advent of \"blockchain\", the cybersecurity industry has developed a new sense of trust which was earlier missing from both the technical and commercial perspectives. Employment of cryptographic hash as well as symmetric/asymmetric encryption and decryption algorithms ensure security without any human intervention (i.e., centralized authority). In this research, we present the synergy between the best of both these worlds. We first propose a model which uses the learned parameters of a typical deep neural network and is secured from external adversaries by cryptography and blockchain technology. As the second contribution of the proposed research, a new parameter tampering attack is proposed to properly justify the role of blockchain in machine learning."
  },
  "cvpr2019_bcmcvai_biometrictemplatestoragewithblockchainafirstlookintocostandperformancetradeoffs": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "Biometric Template Storage With Blockchain: A First Look Into Cost and Performance Tradeoffs",
    "authors": [
      "Oscar Delgado-Mohatar",
      "Julian Fierrez",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Delgado-Mohatar_Biometric_Template_Storage_With_Blockchain_A_First_Look_Into_Cost_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Delgado-Mohatar_Biometric_Template_Storage_With_Blockchain_A_First_Look_Into_Cost_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We explore practical tradeoffs in blockchain-based biometric template storage. We first discuss opportunities and challenges in the integration of blockchain and biometrics, with emphasis in biometric template storage and protection, a key problem in biometrics still largely unsolved. Blockchain technologies provide excellent architectures and practical tools for securing and managing the sensitive and private data stored in biometric templates, but at a cost. We explore experimentally the key tradeoffs involved in that integration, namely: latency, processing time, economic cost, and biometric performance. We experimentally study those factors by implementing a smart contract on Ethereum for biometric template storage, whose cost-performance is evaluated by varying the complexity of state-of-the-art schemes for face and handwriting signature biometrics. We report our experiments using popular benchmarks in biometrics research, including deep learning approaches and databases captured in the wild. As a result, we experimentally show that straightforward schemes for data storage in blockchain (i.e., direct and hash-based) may be prohibitive for biometric template storage using state-of-the-art biometric methods. A good cost-performance tradeoff is shown by using a blockchain approach based on Merkle trees."
  },
  "cvpr2019_bcmcvai_incentive-basedledgerprotocolsforsolvingmachinelearningtasksandoptimizationproblemsviacompetitions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "Incentive-Based Ledger Protocols for Solving Machine Learning Tasks and Optimization Problems via Competitions",
    "authors": [
      "David Amar",
      "Lior Zilpa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Amar_Incentive-Based_Ledger_Protocols_for_Solving_Machine_Learning_Tasks_and_Optimization_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Amar_Incentive-Based_Ledger_Protocols_for_Solving_Machine_Learning_Tasks_and_Optimization_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose incentive-based protocols that use competitions and public ledgers to solve optimization problems. We introduce Proof-of-Accumulated-Work (PoAW): miners compete in costumer-submitted jobs and accumulate recorded work on which they are later remunerated. These new competitions replace the standard hash puzzle-based competitions. A competition is managed by a dynamically-created small masternode network (dTMN) of invested miners, which improves scalability as we do not need the entire network to manage the competition. Using a careful design of incentives, our system preserves security, avoids attacks, and offers new markets to the miners. Finally, we illustrate how the new protocols can be used for implementing machine learning competitions."
  },
  "cvpr2019_bcmcvai_selfisself-sovereignbiometricids": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "SelfIs: Self-Sovereign Biometric IDs",
    "authors": [
      "Luis Bathen",
      "German H. Flores",
      "Gabor Madl",
      "Divyesh Jadav",
      "Andreas Arvanitis",
      "Krishna Santhanam",
      "Connie Zeng",
      "Alan Gordon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Bathen_SelfIs_Self-Sovereign_Biometric_IDs_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Bathen_SelfIs_Self-Sovereign_Biometric_IDs_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We live in a connected world that requires us to identify ourselves everytime we want to access our emails, work stations, bank accounts, health care records, etc. Every system we interact with requires us to remember a username/passwordcombination, have access to some private/public key pair, a hardware token,or some third party authentication software. Our digital identity is ownedby the services we are trying to access, no longer under our control. Self-Sovereign Identity promises to give back control of his or her identity to the user. It is in this context that we explorethe use of biometrics in order to empower users to be their own passwords, their ownkeys, their own means to authenticate themselves. We propose Self-Sovereign Biometric IDs (SelfIs), a novel approach that marries the concepts ofdecentralization, cancelable biometrics, bloom filters, and machine learning to develop a privacy-firstsolution capable of allowing users to control how their biometrics are usedwithout risking their raw biometric templates."
  },
  "cvpr2019_bcmcvai_blockchainenabledaimarketplacethepriceyoupayfortrust": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "BCMCVAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - BLOCKCHAIN MEETS COMPUTER VISION & ARTIFICIAL INTELLIGENCE",
    "title": "Blockchain Enabled AI Marketplace: The Price You Pay for Trust",
    "authors": [
      "Kanthi Sarpatwar",
      "Venkata Sitaramagiridharganesh Ganapavarapu",
      "Karthikeyan Shanmugam",
      "Akond Rahman",
      "Roman Vaculin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/BCMCVAI/Sarpatwar_Blockchain_Enabled_AI_Marketplace_The_Price_You_Pay_for_Trust_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/BCMCVAI/Sarpatwar_Blockchain_Enabled_AI_Marketplace_The_Price_You_Pay_for_Trust_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " There has been a considerable amount of interest in exploring blockchain technologies for enabling marketplaces of different kinds. In this work, we provide a blockchain implementation that enables an \"AI marketplace\": a platform where consumers and data providers can transact data and/or models and derive value. Preserving privacy and trust during these transactions is a paramount concern. As an enabling use case, we consider a transfer learning setting. In this setting, a consumer entity wants to acquire a large training set, from different private data providers, that matches a small validation dataset provided by the consumer. Data providers expect fair value for their contribution and the consumer also wants to maximize its benefit. We implement a distributed protocol on a blockchain that provides guarantees on privacy and consumer's benefit. We also demonstrate that our blockchain implementation plays a crucial role in addressing the issue of fair value attribution and privacy in a trustable way. We consider three different designs for a blockchain implementation that trades off trust requirements on different entities and the overhead in terms of time taken for completion of the task. The first design provides no trust guarantees. The second one guarantees trust with respect to other participants if the platform is trustworthy. The third one guarantees complete trust with no requirements. Our experiments show that the performance in the second and third cases, with partial/complete trust guarantees, degrade by roughly 2x and 5x respectively, compared to the baseline with no trust guarantees."
  },
  "cvpr2019_cefrl_robustvisualtrackingviacollaborativeandreinforcedconvolutionalfeaturelearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Robust Visual Tracking via Collaborative and Reinforced Convolutional Feature Learning",
    "authors": [
      "Dongdong Li",
      "Yangliu Kuai",
      "Gongjian Wen",
      "Li Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Li_Robust_Visual_Tracking_via_Collaborative_and_Reinforced_Convolutional_Feature_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Li_Robust_Visual_Tracking_via_Collaborative_and_Reinforced_Convolutional_Feature_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional neural networks are potent models that yield hierarchies of features and have drawn increasing interest in the visual tracking field. In the paper, we design an end-to-end trainable tracking framework based on Siamese network, which proposes to learn the low-level fine-grained and high-level semantic representations simultaneously with the aim of mutual benefit. Due to the distinct and complementary characteristics of the feature hierarchies, different tracking mechanisms are adopted for different feature layers. The low-level features are exploited and updated with a correlation filter layer for adaptive tracking and the high-level features are compared through cross-correlation directly for robust tracking. The two-level features are jointly trained with a multi-task loss function end-to-end. The proposed tracker takes full advantage of the adaptability of the low-level features and the generalization ability of the high-level features. Extensive experimental tracking results on the widely used OTB and TC128 benchmarks demonstrate the superiority of our tracker. Meanwhile, our proposed tracker can achieve a real-time tracking speed."
  },
  "cvpr2019_cefrl_weaklysupervisedobjectdiscoverybygenerativeadversarial&rankingnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Weakly Supervised Object Discovery by Generative Adversarial & Ranking Networks",
    "authors": [
      "Ali Diba",
      "Vivek Sharma",
      "Rainer Stiefelhagen",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Diba_Weakly_Supervised_Object_Discovery_by_Generative_Adversarial__Ranking_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Diba_Weakly_Supervised_Object_Discovery_by_Generative_Adversarial__Ranking_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The deep generative adversarial networks (GAN) recently have been shown to be promising for different computer vision applications, like image editing, synthesizing high resolution images, generating videos, etc. These networks and the corresponding learning scheme can handle various visual space mappings. We approach GANs with a novel training method and learning objective, to discover multiple object instances for three cases: 1) synthesizing a picture of a specific object within a cluttered scene; 2)localizing different categories in images for weakly supervised object detection; and 3) improving object discovery in object detection pipelines. A crucial advantage of our method is that it learns a new deep similarity metric, to distinguish multiple objects in one image. We demonstrate that the network can act as an encoder-decoder generating parts of an image which contain an object, or as a modified deep CNN to represent images for object detection in supervised and weakly supervised scheme. Our ranking GAN offers a novel way to search through images for object specificpatterns. We have conducted experiments for different scenarios and demonstrate the method performance for object synthesizing and weakly supervised object detection and classification using the MS-COCO and PASCAL VOC datasets."
  },
  "cvpr2019_cefrl_video-basedactionrecognitionusingdimensionreductionofdeepcovariancetrajectories": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Video-Based Action Recognition Using Dimension Reduction of Deep Covariance Trajectories",
    "authors": [
      "Mengyu Dai",
      "Anuj Srivastava"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Dai_Video-Based_Action_Recognition_Using_Dimension_Reduction_of_Deep_Covariance_Trajectories_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Dai_Video-Based_Action_Recognition_Using_Dimension_Reduction_of_Deep_Covariance_Trajectories_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional Neural Networks (CNNs) have been very successful in extracting discriminative features from video data. These deep features can be summarized using covariance descriptors for further analysis. However, due to large number of potential features, the covariance descriptors are often very high dimensional.To facilitate large scale data analysis, we propose a novel, metric-baseddimension-reduction technique that reduces large covariances to small ones. Then, we represent videos as trajectories on the space of covariance matrices, or symmetric-positive definite matrices (SPDMs), and use a Riemannian metricon this space to quantify differences across these trajectories. These distance features can then be used for classification of video sequences. We illustrate this comprehensive framework using data from the UCF11 dataset for action recognition, with classification rates that match or outperform state-of-the-art techniques."
  },
  "cvpr2019_cefrl_adaptivelabelingfordeeplearningtohash": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Adaptive Labeling for Deep Learning to Hash",
    "authors": [
      "Huei-Fang Yang",
      "Cheng-Hao Tu",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Yang_Adaptive_Labeling_for_Deep_Learning_to_Hash_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Yang_Adaptive_Labeling_for_Deep_Learning_to_Hash_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Hash function learning has been widely used for large-scale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary hash function learning approach via deep neural networks in this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward network training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class label representations together with the network weights are updated in the learning process. As the label representations (or referred to as codewords in this work), are learned from data, semantically similar classes will be assigned with the codewords that are close to each other in terms of Hamming distance in the label space. The codewords then serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which can jointly learn label representations and infer compact binary codes from data. It is applicable to both supervised and semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance of AdaLabelHash."
  },
  "cvpr2019_cefrl_deepanchoredconvolutionalneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Deep Anchored Convolutional Neural Networks",
    "authors": [
      "Jiahui Huang",
      "Kshitij Dwivedi",
      "Gemma Roig"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Huang_Deep_Anchored_Convolutional_Neural_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Huang_Deep_Anchored_Convolutional_Neural_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional Neural Networks (CNNs) have been proven to be extremely successful at solving computer vision tasks. State-of-the-art methods favor such deep network architectures for its accuracy performance, with the cost of having massive number of parameters and high weights redundancy. Previous works have studied how to prune such CNNs weights.In this paper, we go to another extreme and analyze the performance of a network stacked with a single convolution kernel across layers, as well as other weights sharing techniques. We name it Deep Anchored Convolutional Neural Network (DACNN). Sharing the same kernel weights across layers allows to reduce the model size tremendously, more precisely, the network is compressed in memory by a factor of L, where L is the desired depth of the network, disregarding the fully connected layer for prediction. The number of parameters in DACNN barely increases as the network grows deeper, which allows us to build deep DACNNs without any concern about memory costs. We also introduce a partial shared weights network (DACNN-mix) as well as an easy-plug-in module, coined regulators, to boost the performance of our architecture.We validated our idea on 3 datasets: CIFAR-10, CIFAR-100 and SVHN. Our results show that we can save massive amounts of memory with our model, while maintaining a high accuracy performance. "
  },
  "cvpr2019_cefrl_knowledgerepresentingefficient,sparserepresentationofpriorknowledgeforknowledgedistillation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation",
    "authors": [
      "Junjie Liu",
      "Dongchao Wen",
      "Hongxing Gao",
      "Wei Tao",
      "Tse-Wei Chen",
      "Kinya Osa",
      "Masami Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Liu_Knowledge_Representing_Efficient_Sparse_Representation_of_Prior_Knowledge_for_Knowledge_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Liu_Knowledge_Representing_Efficient_Sparse_Representation_of_Prior_Knowledge_for_Knowledge_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Despite the recent works on knowledge distillation (KD) have achieved a further improvement through elaborately modeling the decision boundary as the posterior knowledge, their performance is still dependent on the hypothesis that the target network has a powerful capacity (representation ability). In this paper, we propose a knowledge representing (KR) framework mainly focusing on modeling the parameters distribution as prior knowledge. Firstly, we suggest a knowledge aggregation scheme in order to answer how to represent the prior knowledge from teacher network. Through aggregating the parameters distribution from teacher network into more abstract level, the scheme is able to alleviate the phenomenon of residual accumulation in the deeper layers. Secondly, as the critical issue of what the most important prior knowledge is for better distilling, we design a sparse recoding penalty for constraining the student network to learn with the penalized gradients. With the proposed penalty, the student network can effectively avoid the over-regularization during knowledge distilling and converge faster. The quantitative experiments exhibit that the proposed framework achieves the state-ofthe-arts performance, even though the target network does not have the expected capacity. Moreover, the framework is flexible enough for combining with other KD methods based on the posterior knowledge."
  },
  "cvpr2019_cefrl_singleimagebasedmetriclearningviaoverlappingblocksmodelforpersonre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Single Image Based Metric Learning via Overlapping Blocks Model for Person Re-Identification",
    "authors": [
      "Yipeng Chen",
      "Cairong Zhao",
      "Tianli Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Chen_Single_Image_Based_Metric_Learning_via_Overlapping_Blocks_Model_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Chen_Single_Image_Based_Metric_Learning_via_Overlapping_Blocks_Model_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Considering the pedestrian structure characteristics, the first step of many person re-identification algorithms is to divide the pedestrian images or feature map into several blocks, and then the blocks in the same location are used to calculate the special loss functions that metric the differences between different images, to reduce the distance between intra-samples and to increase the distance between inter-samples. However, most of those blocks based deep metric learning methods only measure the difference between different images, but ignored the metrics between different blocks in a single image. In this paper, we propose a novel blocks based method for person re-identification called Overlapping Blocks Model (OBM), in which an innovative strategy of overlapping partition on convolutional features is used to construct multiple overlapping blocks structure and a novel overlapping blocks loss function is utilized to measure the difference between different blocks in a single image, to ensure more blocks can bring more discriminate information and higher performance. We conduct thorough validation experiments on the Market-1501, CUHK03, and DukeMTMC-reID datasets, which demonstrate that our proposed Overlapping Blocks Model can effectively improve the recognition performance of networks by adding the multiple overlapping blocks structure and the overlapping blocks loss."
  },
  "cvpr2019_cefrl_classconsistencydrivenunsuperviseddeepadversarialdomainadaptation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Class Consistency Driven Unsupervised Deep Adversarial Domain Adaptation",
    "authors": [
      "Sayan Rakshit",
      "Ushasi Chaudhuri",
      "Biplab Banerjee",
      "Subhasis Chaudhuri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Rakshit_Class_Consistency_Driven_Unsupervised_Deep_Adversarial_Domain_Adaptation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Rakshit_Class_Consistency_Driven_Unsupervised_Deep_Adversarial_Domain_Adaptation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In unsupervised deep domain adaptation (DA), the use of adversarial domain classifiers is popular in learning a shared feature space which reduces the distributions gap for a pair of source (with training data) and target (with only test data) domains. In the new space, a classifier trained on source training data is expected to generalize well for the target domain samples. We hypothesize that such a feature space obtained by aligning the domains globally ignores the category level feature distributions. This, in turn, leads to erroneous mapping for fine-grained classes. Besides, the discriminativeness of the shared space is not explicitly addressed. In order to resolve both the issues, we propose a novel adversarial approach which judiciously refines the space learned by the domain classifier by incorporating class level information. We follow an ensemble classifiers based approach to model the source domain and introduce a novel consistency constrain on the classifier's outcomes when evaluated on a held-out set of target domain samples. We further leverage the ensemble learning strategy during the inference, as opposed to the existing single classifier based methods. We find that our deep DA model is capable of producing a compact and better domain aligned feature space. Experimental results obtained on the Office-Home, Office- CalTech, MNIST-USPS, and a remote sensing dataset confirm the superiority of the proposed approach."
  },
  "cvpr2019_cefrl_dynamicrepresentationstowardefficientinferenceondeepneuralnetworksbydecisiongates": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Dynamic Representations Toward Efficient Inference on Deep Neural Networks by Decision Gates",
    "authors": [
      "Mohammad Saeed Shafiee",
      "Mohammad Javad Shafiee",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Shafiee_Dynamic_Representations_Toward_Efficient_Inference_on_Deep_Neural_Networks_by_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Shafiee_Dynamic_Representations_Toward_Efficient_Inference_on_Deep_Neural_Networks_by_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " While deep neural networks extract rich features from the input data, the current trade-off between depth and computational cost makes it difficult to adopt deep neural networks for many industrial applications, especially when computing power is limited. Here, we are inspired by the idea that, while deeper embeddings are needed to discriminate difficult samples (i.e., fine-grained discrimination), a large number of samples can be well discriminated via much shallower embeddings (i.e., coarse-grained discrimination). In this study, we introduce the simple yet effective concept of decision gates (d-gate), modules trained to decide whether a sample needs to be projected into a deeper embedding or if an early prediction can be made at the d-gate, thus enabling the computation of dynamic representations at different depths.The proposed d-gate modules can be integrated with any deep neural network and reduces the average computational cost of the deep neural networks while maintaining modeling accuracy. The proposed d-gate framework is examined via different network architectures and datasets, withexperimental results showing that leveraging the proposed d-gate modules led to a43% speed-up and 44% FLOPs reduction on ResNet-101 and 55% speed-up and 39% FLOPs reduction on DenseNet-201 trained on the CIFAR10 dataset with only2% drop in accuracy. Furthermore, experiments where d-gate modules are integrated into ResNet-101 trained on the ImageNet dataset demonstrate that it is possible to reduce the computational cost of the network by 1.5 GFLOPs without any drop in the modeling accuracy."
  },
  "cvpr2019_cefrl_compactscenegraphsforlayoutcompositionandpatchretrieval": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Compact Scene Graphs for Layout Composition and Patch Retrieval",
    "authors": [
      "Subarna Tripathi",
      "Sharath Nittur Sridhar",
      "Sairam Sundaresan",
      "Hanlin Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Tripathi_Compact_Scene_Graphs_for_Layout_Composition_and_Patch_Retrieval_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Tripathi_Compact_Scene_Graphs_for_Layout_Composition_and_Patch_Retrieval_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Structured representations such as scene graphs serve as an efficient and compact representation that can be used for downstream rendering or retrieval tasks. However, existing efforts to generate realistic images from scene graphs perform poorly on scene composition for cluttered or complex scenes. We propose two contributions to improve the scene composition.First, we enhance the scene graph representation with heuristic-based relations, which add minimal storage overhead. Second, we use extreme points representation to supervise the learning of the scene composition network. These methods achieve significantly higher performance over existing work (69.0% vs 51.2% in relation score metric).We additionally demonstrate how scene graphs can be used to retrieve pose-constrained image patches that are semantically similar to the source query. Improving structured scene graph representations for rendering or retrieval are an important step towards realistic image generation. "
  },
  "cvpr2019_cefrl_attonetscompactandefficientdeepneuralnetworksfortheedgeviahuman-machinecollaborativedesign": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "AttoNets: Compact and Efficient Deep Neural Networks for the Edge via Human-Machine Collaborative Design",
    "authors": [
      "Alexander Wong",
      "Zhong Qiu Lin",
      "Brendan Chwyl"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Wong_AttoNets_Compact_and_Efficient_Deep_Neural_Networks_for_the_Edge_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Wong_AttoNets_Compact_and_Efficient_Deep_Neural_Networks_for_the_Edge_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " While deep neural networks have achieved state-of-the-art performance across a large number of complex tasks, it remains a big challenge to deploy such networks for practical, on-device edge scenarios such as on mobile devices, consumer devices, drones, and vehicles. There has been significant recent effort in designing small, low-footprint deep neural networks catered for low-power edge devices, with much of the focus on two extremes: hand-crafting via design principles or fully automated network architecture search.In this study, we take a deeper exploration into a human-machine collaborative design approach for creating highly efficient deep neural networks through a synergy between principled network design prototyping and machine-driven design exploration.The efficacy of human-machine collaborative design is demonstrated through the creation of AttoNets, a family of highly efficient deep neural networks for on-device edge deep learning. Each AttoNet possesses a human-specified network-level macro-architecture comprising of custom modules with unique machine-designed module-level macro-architecture and micro-architecture designs, all driven by human-specified design requirements.Experimental results for the task of object recognition showed that the AttoNets created via human-machine collaborative design has significantly fewer parameters and computational costs than state-of-the-art networks designed for efficiency while achieving noticeably higher accuracy (with the smallest AttoNet achieving1.8% higher accuracy while requiring10x fewer multiply-add operations and parameters than MobileNet-V1).Furthermore, the efficacy of the AttoNets is demonstrated for the task of instance segmentation and object detection, where an AttoNet-based Mask R-CNN network was constructed with significantly fewer parameters and computational costs ( 5x fewer multiply-add operations and2x fewer parameters) than a ResNet-50 based Mask R-CNN network."
  },
  "cvpr2019_cefrl_efficientsuperresolutionusingbinarizedneuralnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Efficient Super Resolution Using Binarized Neural Network",
    "authors": [
      "Yinglan Ma",
      "Hongyu Xiong",
      "Zhe Hu",
      "Lizhuang Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Ma_Efficient_Super_Resolution_Using_Binarized_Neural_Network_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Ma_Efficient_Super_Resolution_Using_Binarized_Neural_Network_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep convolutional neural networks (DCNNs) have recently demonstrated high-quality results in single-image super-resolution (SR). DCNNs often suffer from over-parametrization and large amounts of redundancy, which results in inefficient inference and high memory usage, preventing massive applications on mobile devices. As a way to significantly reduce model size and computation time, binarized neural network has only been shown to excel on semantic-level tasks such as image classification and recognition. However, little effort of network quantization has been spent on image enhancement tasks like SR, as network quantization is usually assumed to sacrifice pixel-level accuracy. In this work, we explore an network-binarization approach for SR tasks without sacrificing much reconstruction accuracy. To achieve this, we binarize the convolutional filters in only residual blocks, and adopt a learnable weight for each binary filter. We evaluate this idea on several state-of-the-art DCNN-based architectures, and show that binarized SR networks achieve comparable qualitative and quantitative results as their real-weight counterparts. Moreover, the proposed binarized strategy could help reduce model size by 80% when applying on SRResNet, and could potentially speed up inference by 5 times."
  },
  "cvpr2019_cefrl_generativemodelforzero-shotsketch-basedimageretrieval": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Generative Model for Zero-Shot Sketch-Based Image Retrieval",
    "authors": [
      "Vinay Kumar Verma",
      "Aakansha Mishra",
      "Ashish Mishra",
      "Piyush Rai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Verma_Generative_Model_for_Zero-Shot_Sketch-Based_Image_Retrieval_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Verma_Generative_Model_for_Zero-Shot_Sketch-Based_Image_Retrieval_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a probabilistic model for Sketch-Based Image Retrieval (SBIR) where, at retrieval time, we are given sketches from novel classes, that were not present at training time. Existing SBIR methods, most of which rely on learning class-wise correspondences between sketches and images, typically work well only for previously seen sketch classes, and result in poor retrieval performance on novel classes. To address this, we propose a generative model that learns to generate images, conditioned on a given novel class sketch. This enables us to reduce the SBIR problem to a standard image-to-image search problem. Our model is based on an inverse auto-regressive flow based variational autoencoder, with a feedback mechanism to ensure robust image generation. We evaluate our model on two very challenging datasets, Sketchy, and TU Berlin, with novel train-test split. The proposed approach significantly outperforms various baselines on both the datasets."
  },
  "cvpr2019_cefrl_efficientdeeppalmprintrecognitionviadistilledhashingcoding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Efficient Deep Palmprint Recognition via Distilled Hashing Coding",
    "authors": [
      "Huikai Shao",
      "Dexing Zhong",
      "Xuefeng Du"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Shao_Efficient_Deep_Palmprint_Recognition_via_Distilled_Hashing_Coding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Shao_Efficient_Deep_Palmprint_Recognition_via_Distilled_Hashing_Coding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Efficient deep palmprint recognition has become an urgent issue for the demand of personal identification on mobile/wearable devices. Compared to other biometrics, palmprint recognition has many unique advantages, e.g. richness of features, high user-friendliness, suitability for private security, etc. Existing deep learning based methods are computationally exhaustive in feature representation and learning, which are not suitable for large-scale deployment in portable authentication systems. In this paper, we combine hash coding and knowledge distillation to explore efficient deep palmprint recognition. Based on deep hashing network, palmprint images were converted to binary codes to save storage space and speed up matching. Combining hashing coding with knowledge distillation can further compress deep model to achieve an efficient recognition by light networks. Unlike previous palmprint recognition on datasets collected by dedicated devices in a controlled environment, we establish a novel database for unconstrained palmprint recognition, which consists of more than 30,000 images collected by 5 different mobile phones. Moreover, we manually labeled 14 key points on each image for region of interest (ROI) extraction. Comprehensive experiments were conducted on this palmprint database. The results indicate the feasibility of our database and the potential of palmprint recognition to be used as an efficient biometrics for deployment on consumer devices."
  },
  "cvpr2019_cefrl_imagedenoisingusingdeepcganwithbi-skipconnections": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Image Denoising Using Deep CGAN With Bi-Skip Connections",
    "authors": [
      "Peng Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Wang_Image_Denoising_Using_Deep_CGAN_With_Bi-Skip_Connections_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Wang_Image_Denoising_Using_Deep_CGAN_With_Bi-Skip_Connections_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With the rapid development of neural networks, many deep learning-based image processing tasks have shown outstanding performance. In this paper, we describe a unified deep learning-based approach for image image denoising. The proposed method is composed of deep convolutional neural and conditional generative adversarial networks. For the discriminator network, we present a new network architecture with bi-skip connections to address hard training and details losing issues. In the generative network, a objective optimization is derived to solve the problem of common conditions being non-identical. Through extensive experiments on image denoising task on both qualitative and quantitative criteria, we demonstrate that our proposed method performs favorably against current state-of-the-art approaches."
  },
  "cvpr2019_cefrl_pairwiseteacher-studentnetworkforsemi-supervisedhashing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Pairwise Teacher-Student Network for Semi-Supervised Hashing",
    "authors": [
      "Shifeng Zhang",
      "Jianmin Li",
      "Bo Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Zhang_Pairwise_Teacher-Student_Network_for_Semi-Supervised_Hashing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Zhang_Pairwise_Teacher-Student_Network_for_Semi-Supervised_Hashing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Hashing method maps similar high-dimensional data to binary hashcodes with smaller hamming distance, and it has received broad attention due to its low storage cost and fast retrieval speed. Pairwise similarity is easily obtained and widely used for retrieval, and most supervised hashing algorithms are carefully designed for the pairwise supervisions. As labeling all data pairs is difficult, semi-supervised hashing is proposed which aims at learning efficient codes with limited labeled pairs and abundant unlabeled ones. Existing methods build graphs to capture the structure of dataset, but they are not working well for complex data as the graph is built based on the data representations and determining the representations of complex data is difficult. In this paper, we propose a novel teacher-student semi-supervised hashing framework in which the student is trained with the pairwise information produced by the teacher network. The network follows the smoothness assumption, which achieves consistent distances for similar data pairs so that the retrieval results are similar for neighborhood queries. Experiments on large-scale datasets show that the proposed method reaches impressive gain over the supervised baselines and is superior to state-of-the-art semi-supervised hashing methods. "
  },
  "cvpr2019_cefrl_asitemodelbasedchangedetectionmethodforsarimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "A Site Model Based Change Detection Method for SAR Images",
    "authors": [
      "Wei Wang",
      "Jianhua Shi",
      "Lingjun Zhao",
      "Xingwei Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Wang_A_Site_Model_Based_Change_Detection_Method_for_SAR_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Wang_A_Site_Model_Based_Change_Detection_Method_for_SAR_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, a new method of site model based change detection is presented for multitemporal synthetic aperture radar (SAR) images. It first constructs a site model offline making use of a high resolution image of the fixed site, and then accurate registration is carried out between the model and the images. With the location information contained in the site model, the region of interest (ROI) can be extracted easily and robustly. Finally, the significant changes are obtained by comparing the invariant features extracted separately from the shapes of targets in ROI of two images. The experiment of change detection for two SAR images ofairport demonstratedthe validity of the proposed method."
  },
  "cvpr2019_cefrl_salientobjectdetectioninlowcontrastimagesviaglobalconvolutionandboundaryrefinement": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Salient Object Detection in Low Contrast Images via Global Convolution and Boundary Refinement",
    "authors": [
      "Nan Mu",
      "Xin Xu",
      "Xiaolong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Mu_Salient_Object_Detection_in_Low_Contrast_Images_via_Global_Convolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Mu_Salient_Object_Detection_in_Low_Contrast_Images_via_Global_Convolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Benefit from the powerful features created by using deep learning technology, salient object detection has recently witnessed remarkable progresses. However, it is difficult for a deep network to achieve satisfactory results in low contrast images, due to the low signal to noise ratio property, thus previous deep learning based saliency methods may output maps with ambiguous salient objects and blurred boundaries. To address this issue, we propose a deep fully convolutional framework with a global convolutional module (GCM) and a boundary refinement module (BRM) for saliency detection. Our model drives the network to learn the local and global information to discriminate pixels belonging to salient objects or not, thus can produce more uniform saliency map. To refine the localization and classification performance of the network, five GCMs are integrated to preserve more spatial knowledge of feature maps and enable the densely connections with classifiers. Besides, to propagate saliency information with rich boundary content, a BRM is embed behind each convolutional layer. Experiments on six challenging datasets show that the proposed saliency model achieves state-of-the-art performance compared to nine existing approaches in terms of nine evaluation metrics."
  },
  "cvpr2019_cefrl_anenergyandgpu-computationefficientbackbonenetworkforreal-timeobjectdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection",
    "authors": [
      "Youngwan Lee",
      "Joong-won Hwang",
      "Sangrok Lee",
      "Yuseok Bae",
      "Jongyoul Park"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/Lee_An_Energy_and_GPU-Computation_Efficient_Backbone_Network_for_Real-Time_Object_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/Lee_An_Energy_and_GPU-Computation_Efficient_Backbone_Network_for_Real-Time_Object_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " As DenseNet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. Although feature reuse enables DenseNet to produce strong features with a small number of model parameters and FLOPs, the detector with DenseNet backbone shows rather slow speed and low energy efficiency. We find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. To solve the inefficiency of DenseNet, we propose an energy and computation efficient architecture called VoVNet comprised of One-Shot Aggregation (OSA). The OSA not only adopts the strength of DenseNet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. To validate the effectiveness of VoVNet as a backbone network, we design both lightweight and large-scale VoVNet and apply them to one-stage and two-stage object detectors. Our VoVNet based detectors outperform DenseNet based ones with 2x faster speed and the energy consumptions are reduced by 1.6x - 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet backbone with faster speed and better energy efficiency. In particular, the small object detection performance has been significantly improved over DenseNet and ResNet."
  },
  "cvpr2019_cefrl_scan-floodfill(scaff)anefficientautomaticpreciseregionfillingalgorithmforcomplicatedregions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Scan-Flood Fill(SCAFF): An Efficient Automatic Precise Region Filling Algorithm for Complicated Regions",
    "authors": [
      "Yixuan He",
      "Tianyi Hu",
      "Delu Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CEFRL/He_Scan-Flood_FillSCAFF_An_Efficient_Automatic_Precise_Region_Filling_Algorithm_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CEFRL/He_Scan-Flood_FillSCAFF_An_Efficient_Automatic_Precise_Region_Filling_Algorithm_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, instant level labeling for supervised machine learning requires a considerable number of filled masks. In this paper, we propose an efficient automatic region filling algorithm for complicated regions. Distinguishing between adjacent connected regions, the Main Filling Process scans through all pixels and fills all the pixels except boundary ones with either exterior or interior label color. In this way, we succeed in classifying all the pixels inside the region except boundary ones in the given image to form two groups: a background group and a mask group. We then set all exterior label pixels to background color, and interior label pixels to mask color. With this algorithm, we are able to generate output masks precisely and efficiently even for complicated regions as long asboundary pixels are given. Experimental results show that the proposed algorithm can generate precise masks that allow for various machine learning tasks such as supervised training. This algorithm can effectively handle multiple regions, complicated `holes' and regions whose boundaries touch the image border. By testing the algorithm on both toy and practical images, we show that the performance of Scan-flood Fill(SCAFF) has achieved favorable results."
  },
  "cvpr2019_ntire_realphotographsdenoisingwithnoisedomainadaptationandattentivegenerativeadversarialnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Real Photographs Denoising With Noise Domain Adaptation and Attentive Generative Adversarial Network",
    "authors": [
      "Kai Lin",
      "Thomas H. Li",
      "Shan Liu",
      "Ge Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Lin_Real_Photographs_Denoising_With_Noise_Domain_Adaptation_and_Attentive_Generative_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Lin_Real_Photographs_Denoising_With_Noise_Domain_Adaptation_and_Attentive_Generative_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Nowadays, deep convolutional neural networks(CNNs) based methods have achieved favorable performance in synthetic noisy image denoising, but they are very limited in real photographs denoising since it's hard to obtain ground truth clean image to generate paired training data. Besides, the existing training datasets for real photographs denoising are too small. To solve this problem, we construct a new dataset and obtain corresponding ground truth by averaging, and then extend them through noise domain adaptation. Furthermore, we propose a attentive generative network by injecting visual attention into the generative network. During the training, visual attention map learn noise regions. The generative network will pay more attention to noise regions, which contributes to balancing between noise removal and texture preservation. Extensive experiments show that our method outperforms several state-of-the-art methods quantitatively and qualitatively."
  },
  "cvpr2019_ntire_multi-levelencoder-decoderarchitecturesforimagerestoration": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Level Encoder-Decoder Architectures for Image Restoration",
    "authors": [
      "Indra Deep Mastan",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Mastan_Multi-Level_Encoder-Decoder_Architectures_for_Image_Restoration_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Mastan_Multi-Level_Encoder-Decoder_Architectures_for_Image_Restoration_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Many real-world solutions for image restoration are learning-free and based on handcrafted image priors such as self-similarity. Recently, deep-learning methods that use training data have achieved state-of-the-art results in various image restoration tasks (e.g., super-resolution and inpainting). Ulyanov et al. bridge the gap between these two families of methods (CVPR 18). They have shown that learning-free methods perform close to the state-of-the-art learning-based methods (approximately 1 PSNR). Their approach benefits from the encoder-decoder network. In this paper, we propose a framework based on the multi-level extensions of the encoder-decoder network, to investigate interesting aspects of the relationship between image restoration and network construction independent of learning. Our framework allows various network structures by modifying the following network components: skip links, cascading of the network input into intermediate layers, a composition of the encoder-decoder subnetworks, and network depth. These handcrafted network structures illustrate how the construction of untrained networks influence the following image restoration tasks: denoising, super-resolution, and inpainting. We also demonstrate image reconstruction using flash and no-flash image pairs. We provide performance comparisons with the state-of-the-art methods for all the restoration tasks above. "
  },
  "cvpr2019_ntire_learningdeepimagepriorsforblindimagedenoising": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Learning Deep Image Priors for Blind Image Denoising",
    "authors": [
      "Xianxu Hou",
      "Hongming Luo",
      "Jingxin Liu",
      "Bolei Xu",
      "Ke Sun",
      "Yuanhao Gong",
      "Bozhi Liu",
      "Guoping Qiu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Hou_Learning_Deep_Image_Priors_for_Blind_Image_Denoising_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Hou_Learning_Deep_Image_Priors_for_Blind_Image_Denoising_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Image denoising is the process of removing noise from noisy images, which is an image domain transferring task, i.e., from a single or several noise level domains to a photo-realistic domain. In this paper, we propose an effective image denoising method by learning two image priors from the perspective of domain alignment. We tackle the domain alignment on two levels. 1) the feature-level prior is to learn domain-invariant features for corrupted images with different level noise; 2) the pixel-level prior is used to push the denoised images to the natural image manifold. The two image priors are based on H-divergence theory and implemented by learning classifiers in adversarial training manners. We evaluate our approach on multiple datasets. The results demonstrate the effectiveness of our approach for robust image denoising on both synthetic and real-world noisy images. Furthermore, we show that the feature-level prior is capable of alleviating the discrepancy between different level noise. It can be used to improve the blind denoising performance in terms of distortion measures (PSNR and SSIM), while pixel-level prior can effectively improve the perceptual quality to ensure the realistic outputs, which is further validated by subjective evaluation. "
  },
  "cvpr2019_ntire_conditionalgansformulti-illuminantcolorconstancyrevolutionoryetanotherapproach?": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Conditional GANs for Multi-Illuminant Color Constancy: Revolution or yet Another Approach?",
    "authors": [
      "Oleksii Sidorov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Sidorov_Conditional_GANs_for_Multi-Illuminant_Color_Constancy_Revolution_or_yet_Another_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Sidorov_Conditional_GANs_for_Multi-Illuminant_Color_Constancy_Revolution_or_yet_Another_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Non-uniform and multi-illuminant color constancy are important tasks, the solution of which will allow to discard information about lighting conditions in the image. Non-uniform illumination and shadows distort colors of real-world objects and mostly do not contain valuable information. Thus, many computer vision and image processing techniques would benefit from automatic discarding of this information at the pre-processing step. In this work we propose novel view on this classical problem via generative end-to-end algorithm based on image conditioned Generative Adversarial Network. We also demonstrate the potential of the given approach for joint shadow detection and removal. Forced by the lack of training data, we render the largest existing shadow removal dataset and make it publicly available. It consists of approximately 6,000 pairs of wide field of view synthetic images with and without shadows."
  },
  "cvpr2019_ntire_deepgraphlaplacianregularizationforrobustdenoisingofrealimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep Graph Laplacian Regularization for Robust Denoising of Real Images",
    "authors": [
      "Jin Zeng",
      "Jiahao Pang",
      "Wenxiu Sun",
      "Gene Cheung"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Zeng_Deep_Graph_Laplacian_Regularization_for_Robust_Denoising_of_Real_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Zeng_Deep_Graph_Laplacian_Regularization_for_Robust_Denoising_of_Real_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent developments in deep learning have revolutionized the paradigm of image restoration. However, its applications on real image denoising are still limited, due to its sensitivity to training data and the complex nature of real image noise. In this work, we combine the robustness merit of model-based approaches and the learning power of data-driven approaches for real image denoising. Specifically, by integrating graph Laplacian regularization as a trainable module into a deep learning framework, we are less susceptible to overfitting than pure CNN-based approaches, achieving higher robustness to small datasets and cross-domain denoising. First, a sparse neighborhood graph is built from the output of a convolutional neural network (CNN). Then the image is restored by solving an unconstrained quadratic programming problem, using a corresponding graph Laplacian regularizer as a prior term. The proposed restoration pipeline is fully differentiable and hence can be end-to-end trained. Experimental results demonstrate that our work is less prone to overfitting given small training data. It is also endowed with strong cross-domain generalization power, outperforming the state-of-the-art approaches by a remarkable margin."
  },
  "cvpr2019_ntire_multi-stageoptimizationforphotorealisticneuralstyletransfer": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Stage Optimization for Photorealistic Neural Style Transfer",
    "authors": [
      "Richard R. Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Yang_Multi-Stage_Optimization_for_Photorealistic_Neural_Style_Transfer_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Yang_Multi-Stage_Optimization_for_Photorealistic_Neural_Style_Transfer_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This work introduces a new approach toward photorealistic style transfer. When applying current style transfer techniques on real world photographs, the generated results often contain distortions and artifacts that diminish the real-world quality of the photograph. To address these issues, we propose a two-stage optimization process that transfers style globally and regionally and applies a sharpening filter after each step. As evaluated by a user study, our method is qualitatively comparable to existing state-of-the-art methods, but successfully handles previous failure cases. Our method also quantitatively outperform previous methods as evaluated by natural scene statistic metrics."
  },
  "cvpr2019_ntire_naturalimagenoisedataset": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Natural Image Noise Dataset",
    "authors": [
      "Benoit Brummer",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Brummer_Natural_Image_Noise_Dataset_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Brummer_Natural_Image_Noise_Dataset_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional neural networks have been the focus of research aiming to solve image denoising problems, but their performance remains unsatisfactory for most applications. These networks are trained with synthetic noise distributions that do not accurately reflect the noise captured by image sensors. Some datasets of clean-noisy image pairs have been introduced but they are usually meant for benchmarking or specific applications. We introduce a dataset [NIND] of DSLR-like images with varying levels of ISO noise which is large enough to train models for blind denoising over a wide range of noise. We demonstrate a denoising model trained with the NIND and show that it significantly outperforms BM3D on ISO noise from unseen images, even when generalizing to images from a different type of camera. The Natural Image Noise Dataset is published on Wikimedia Commons such that it remains open for curation and contributions. We expect that this dataset will prove useful for future image denoising applications."
  },
  "cvpr2019_ntire_vornetspatio-temporallyconsistentvideoinpaintingforobjectremoval": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "VORNet: Spatio-Temporally Consistent Video Inpainting for Object Removal",
    "authors": [
      "Ya-Liang Chang",
      "Zhe Yu Liu",
      "Winston Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Chang_VORNet_Spatio-Temporally_Consistent_Video_Inpainting_for_Object_Removal_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Chang_VORNet_Spatio-Temporally_Consistent_Video_Inpainting_for_Object_Removal_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Video object removal is a challenging task in video processing that often requires massive human efforts. Given the mask of the foreground object in each frame, the goal is to complete (inpaint) the object region and generate a video without the target object. While recently deep learning based methods have achieved great success on the image inpainting task, they often lead to inconsistent results between frames when applied to videos. In this work, we propose a novel learning-based Video Object Removal Network (VORNet) to solve the video object removal task in a spatio-temporally consistent manner, by combining the optical flow warping and image-based inpainting model. Experiments are done on our Synthesized Video Object Removal (SVOR) dataset based on the YouTube-VOS video segmentation dataset, and both the objective and subjective evaluation demonstrate that our VORNet generates more spatially and temporally consistent videos compared with existing methods."
  },
  "cvpr2019_ntire_densenetwithdeepresidualchannel-attentionblocksforsingleimagesuperresolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DenseNet With Deep Residual Channel-Attention Blocks for Single Image Super Resolution",
    "authors": [
      "Dong-Won Jang",
      "Rae-Hong Park"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Jang_DenseNet_With_Deep_Residual_Channel-Attention_Blocks_for_Single_Image_Super_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Jang_DenseNet_With_Deep_Residual_Channel-Attention_Blocks_for_Single_Image_Super_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper proposes a DenseNet with deep Residual Channel Attention (DRCA) for single image super resolution. Recent works have shown that skip connections between layers improve the performance of the convolutional neural network such as ResNet and DenseNet. We have interpreted the role of ResNet (feature value refinement by addition) and DenseNet (feature value memory by concatenation). The contribution of the proposed network is dense connections between residual groups rather than convolution layers. In terms of feature value refinement and memory, the proposed method refines the feature values sufficiently (by residual group) and memorizes the refined feature values intermittently (by dense connections between residual groups). Experimental results show that the proposed DRCA (14.2M) achieved better performance than the state-of-the-art methods with fewer parameters."
  },
  "cvpr2019_ntire_lightfieldsuper-resolutionabenchmark": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Light Field Super-Resolution: A Benchmark",
    "authors": [
      "Zhen Cheng",
      "Zhiwei Xiong",
      "Chang Chen",
      "Dong Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Cheng_Light_Field_Super-Resolution_A_Benchmark_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Cheng_Light_Field_Super-Resolution_A_Benchmark_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Lenslet-based light field imaging generally suffers from a fundamental trade-off between spatial and angular resolutions, which limits its promotion to practical applications. To this end, a substantial amount of efforts have been dedicated to light field super-resolution (SR) in recent years. Despite the demonstrated success, existing light field SR methods are often evaluated based on different degradation assumptions using different datasets, and even contradictory results are reported in literature. In this paper, we conduct the first systematic benchmark evaluation for representative light field SR methods on both synthetic and real-world datasets with various downsampling kernels and scaling factors. We then analyze and discuss the advantages and limitations of each kind of method from different perspectives. Especially, we find that CNN-based single image SR without using any angular information outperforms most light field SR methods even including learning-based ones. This benchmark evaluation, along with the comprehensive analysis and discussion, sheds light on the future researches in light field SR."
  },
  "cvpr2019_ntire_exemplarguidedfaceimagesuper-resolutionwithoutfaciallandmarks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Exemplar Guided Face Image Super-Resolution Without Facial Landmarks",
    "authors": [
      "Berk Dogan",
      "Shuhang Gu",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Dogan_Exemplar_Guided_Face_Image_Super-Resolution_Without_Facial_Landmarks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Dogan_Exemplar_Guided_Face_Image_Super-Resolution_Without_Facial_Landmarks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Nowadays, due to the ubiquitous visual media there are vast amounts of already available high-resolution (HR) face images. Therefore, for super-resolving a given very low-resolution (LR) face image of a person it is very likely to find another HR face image of the same person which can be used to guide the process. In this paper, we propose a convolutional neural network (CNN)-based solution, namely GWAInet, which applies super-resolution (SR) by a factor 8x on face images guided by another unconstrained HR face image of the same person with possible differences in age, expression, pose or size. GWAInet is trained in an adversarial generative manner to produce the desired high quality perceptual image results. The utilization of the HR guiding image is realized via the use of a warper subnetwork that aligns its contents to the input image and the use of a feature fusion chain for the extracted features from the warped guiding image and the input image. In training, the identity loss further helps in preserving the identity related features by minimizing the distance between the embedding vectors of SR and HR ground truth images. Contrary to the current state-of-the-art in face super-resolution, our method does not require facial landmark points for its training, which helps its robustness and allows it to produce fine details also for the surrounding face region in a uniform manner. Our method GWAInet produces photo-realistic images in upscaling factor 8x and outperforms state-of-the-art in quantitative terms and perceptual quality."
  },
  "cvpr2019_ntire_recursiveimagedehazingviaperceptuallyoptimizedgenerativeadversarialnetwork(pogan)": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Recursive Image Dehazing via Perceptually Optimized Generative Adversarial Network (POGAN)",
    "authors": [
      "Yixin Du",
      "Xin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Du_Recursive_Image_Dehazing_via_Perceptually_Optimized_Generative_Adversarial_Network_POGAN_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Du_Recursive_Image_Dehazing_via_Perceptually_Optimized_Generative_Adversarial_Network_POGAN_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Existing approaches towards single image dehazing including both model-based and learning-based heavily rely on the estimation of so-called transmission maps. Despite its conceptual simplicity, using transmission maps as an intermediate step often makes it more difficult to optimize the perceptual quality of reconstructed images. To overcome this weakness, we propose a direct deep learning approach toward image dehazing bypassing the step of transmission map estimation and facilitating end-to-end perceptual optimization. Our technical contributions are mainly three-fold. First, based on the analogy between dehazing and denoising, we propose to directly learn a nonlinear mapping from the space of degraded images to that of haze-free ones via recursive deep residual learning; Second, inspired by the success of generative adversarial networks (GAN), we propose to optimize the perceptual quality of dehazed images by introducing a discriminator and a loss function adaptive to hazy conditions; Third, we propose to remove notorious halo-like artifacts at large scene depth discontinuities by a novel application of guided filtering. Extensive experimental results have shown that the subjective qualities of dehazed images by the proposed perceptually optimized GAN (POGAN) are often more favorable than those by existing state-of-the-art approaches especially when hazy condition varies."
  },
  "cvpr2019_ntire_aspect-ratio-preservingmulti-patchimageaestheticsscoreprediction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Aspect-Ratio-Preserving Multi-Patch Image Aesthetics Score Prediction",
    "authors": [
      "Lijie Wang",
      "Xueting Wang",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Wang_Aspect-Ratio-Preserving_Multi-Patch_Image_Aesthetics_Score_Prediction_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Wang_Aspect-Ratio-Preserving_Multi-Patch_Image_Aesthetics_Score_Prediction_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Owing to the spread of social networking services (SNS), there is an increasing demand for automatically selecting, editing or generating impressive images, which raises the importance of evaluating image aesthetics. We propose the first multi-patch method for image aesthetic score prediction with the original image aspect ratios being preserved. Our method just uses images for training and does not require external information both in training as well as prediction. In an experiment using the large-scale AVA dataset containing 250,000 images, our approach outperforms other existing methods in image aesthetic score prediction, especially reducing mean squared error (MSE) of predicted aesthetic scores by 0.061 (18%) and improving the linear correlation coefficient (LCC) by 0.056 (8.9%). Noticeably, the decrease in mean absolute error (MAE) by our method for images with an unbalanced aspect ratio is at most 7.9 times larger than the decrease in MAE for images with a typical digital camera aspect ratio. This result indicates that our multi-patch method expands the range of aspect ratios with which aesthetics scores of images can be predicted accurately."
  },
  "cvpr2019_ntire_videnndeepblindvideodenoising": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "ViDeNN: Deep Blind Video Denoising",
    "authors": [
      "Michele Claus",
      "Jan van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Claus_ViDeNN_Deep_Blind_Video_Denoising_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Claus_ViDeNN_Deep_Blind_Video_Denoising_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose ViDeNN: a CNN for Video Denoising without prior knowledge on the noise distribution (blind denoising). The CNN architecture uses a combination of spatial and temporal filtering, learning to spatially denoise the frames first and at the same time how to combine their temporal information, handling objects motion, brightness changes, low-light conditions and temporal inconsistencies. We demonstrate the importance of the data used for CNNs training, creating for this purpose a specific dataset for low-light conditions. We test ViDeNN on common benchmarks and on self-collected data, achieving good results comparable with the state-of-the-art."
  },
  "cvpr2019_ntire_anepipolarvolumeautoencoderwithadversariallossfordeeplightfieldsuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "An Epipolar Volume Autoencoder With Adversarial Loss for Deep Light Field Super-Resolution",
    "authors": [
      "Minchen Zhu",
      "Anna Alperovich",
      "Ole Johannsen",
      "Antonin Sulc",
      "Bastian Goldluecke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Zhu_An_Epipolar_Volume_Autoencoder_With_Adversarial_Loss_for_Deep_Light_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Zhu_An_Epipolar_Volume_Autoencoder_With_Adversarial_Loss_for_Deep_Light_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " When capturing a light field of a scene, one typically faces a trade-off between more spatial or more angular resolution. Fortunately, light fields are also a rich source of information for solving the problem of super-resolution. Contrary to single image approaches, where high-frequency content has to be hallucinated to be the most likely source of the downscaled version, sub-aperture views from the light field can help with an actual reconstruction of those details that have been removed by downsampling. In this paper, we propose a three-dimensional generative adversarial autoencoder network to recover the high-resolution light field from a low-resolution light field with a sparse set of viewpoints. We require only three views along both horizontal and vertical axis to increase angular resolution by a factor of three while at the same time increasing spatial resolution by a factor of either two or four in each direction, respectively."
  },
  "cvpr2019_ntire_evaluatingparameterizationmethodsforconvolutionalneuralnetwork(cnn)-basedimageoperators": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Evaluating Parameterization Methods for Convolutional Neural Network (CNN)-Based Image Operators",
    "authors": [
      "Seung-Wook Kim",
      "Sung-Jin Cho",
      "Kwang-Hyun Uhm",
      "Seo-Won Ji",
      "Sang-Won Lee",
      "Sung-Jea Ko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Kim_Evaluating_Parameterization_Methods_for_Convolutional_Neural_Network_CNN-Based_Image_Operators_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Kim_Evaluating_Parameterization_Methods_for_Convolutional_Neural_Network_CNN-Based_Image_Operators_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, deep neural networks have been widely used to approximate or improve image operators. In general, an image operator has some hyper-parameters that change its operating configurations, e.g., the strength of smoothing or up-scale factors in super-resolution. To address the varying parameter setting, an image operator taking such parameters as its input, namely a parameterized image operator, is an essential cue in image processing. Since many types of parameterization techniques exist, comparative analysis is required in the context of image processing. In this paper, we therefore analytically explore the operation principles of these parameterization techniques and study their differences. In addition, performance comparisons between image operators parameterized by using these methods are assessed experimentally on common image processing tasks including image smoothing, denoising, deblocking, and super-resolution."
  },
  "cvpr2019_ntire_edgedetectiontechniquesforquantifyingspatialimagingsystemperformanceandimagequality": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Edge Detection Techniques for Quantifying Spatial Imaging System Performance and Image Quality",
    "authors": [
      "Oliver van Zwanenberg",
      "Sophie Triantaphillidou",
      "Robin Jenkin",
      "Alexandra Psarrou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/van_Zwanenberg_Edge_Detection_Techniques_for_Quantifying_Spatial_Imaging_System_Performance_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/van_Zwanenberg_Edge_Detection_Techniques_for_Quantifying_Spatial_Imaging_System_Performance_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Measuring camera system performance and associating it directly to image quality is very relevant, whether images are aimed for viewing, or as input to machine learning and automated recognition algorithms. The Modulation Transfer Function (MTF) is a well-established measure for evaluating this performance. This study proposes a novel methodology for measuring system MTFs directly from natural scenes, by adapting the standardized Slanted Edge Method (ISO 12233). The method involves edge detection techniques, to select and extract suitable step edges from pictorial images. The scene MTF aims to account for camera non-linear scene dependent processes.This measure is more relevant to image quality modelling than the traditionally measured MTFs. Preliminary research results indicate that the proposed method can provide reliable MTFs, following the trends of the ISO 12233. Further development and validation are required before it is proposed as a universal camera measuring technique."
  },
  "cvpr2019_ntire_histogramlearninginimagecontrastenhancement": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Histogram Learning in Image Contrast Enhancement",
    "authors": [
      "Bin Xiao",
      "Yunqiu Xu",
      "Han Tang",
      "Xiuli Bi",
      "Weisheng Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Xiao_Histogram_Learning_in_Image_Contrast_Enhancement_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Xiao_Histogram_Learning_in_Image_Contrast_Enhancement_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose a novel contrast enhancement method utilizing a fully convolutional network (FCN) to learn the weighted histograms from input images. In this method, the enhanced image references are not required. The training images are synthesized by randomly adding illumination on different regions in the source images to simulate the input images with poor contrast in different regions, and to enlarge the scale of training image set. And with this data-driven strategy for learning the underlying ill-posed illumination information of each pixel, a novel weighted image histogram is developed. It not only describes the distribution of pixel intensity, but also contains the illumination information of input images. Consequently, the proposed method can fast and efficiently enhance the regions with poor contrast and have the regions with acceptable contrast preserved, which keeps vivid color and rich details of the enhanced images. Experimental results demonstrate the effectiveness of our proposed method in comparison with some state-of-the-art methods."
  },
  "cvpr2019_ntire_optimization-baseddatagenerationforphotoenhancement": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Optimization-Based Data Generation for Photo Enhancement",
    "authors": [
      "Mayu Omiya",
      "Yusuke Horiuchi",
      "Edgar Simo-Serra",
      "Satoshi Iizuka",
      "Hiroshi Ishikawa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Omiya_Optimization-Based_Data_Generation_for_Photo_Enhancement_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Omiya_Optimization-Based_Data_Generation_for_Photo_Enhancement_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The preparation of large amounts of high-quality training data has always been the bottleneck for the performance of supervised learning methods. It is especially time-consuming for complicated tasks such as photo enhancement. A recent approach to ease data annotation creates realistic training data automatically with optimization. In this paper, we improve upon this approach by learning image-similarity which, in combination with a Covariance Matrix Adaptation optimization method, allows us to create higher quality training data for enhancing photos. We evaluate our approach on challenging real world photo-enhancement images by conducting a perceptual user study, which shows that its performance compares favorably with existing approaches."
  },
  "cvpr2019_ntire_frescofastradiometricegocentricscreencompensation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "FRESCO: Fast Radiometric Egocentric Screen Compensation",
    "authors": [
      "Matthew Post",
      "Paul Fieguth",
      "Mohamed A. Naiel",
      "Zohreh Azimifar",
      "Mark Lamm"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Post_FRESCO_Fast_Radiometric_Egocentric_Screen_Compensation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Post_FRESCO_Fast_Radiometric_Egocentric_Screen_Compensation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Existingradiometriccompensationmethodsfor projector-camera systems have been shown to produce compensated colours which are inconsistent to a human viewer. In this paper, a novel radiometric compensation method for projector-camera systems and textured surfaces is introduced based on the human visual system (HVS) colour response. The proposed method can extend established compensation methods to produce colours which are human-perceived to be correct (egocentric modelling). As a result, this method performs radiometric compensation which is not only consistent and precise, but also produces images which are visually accurate to an external colour reference. This method is shown to produce generally the lowest average radiometric compensation error when compared to compensation performed using only the response of a camera, demonstrated through quantitative analysis of compensated colours, and supported by qualitative results."
  },
  "cvpr2019_ntire_kalmanfilteringofpatchesforframe-recursivevideodenoising": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Kalman Filtering of Patches for Frame-Recursive Video Denoising",
    "authors": [
      "Pablo Arias",
      "Jean-Michel Morel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Arias_Kalman_Filtering_of_Patches_for_Frame-Recursive_Video_Denoising_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Arias_Kalman_Filtering_of_Patches_for_Frame-Recursive_Video_Denoising_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " A frame recursive video denoising method computes each output frame as a function of only the current noisy frame and the previous denoised output. Frame recursive methods were among the earliest approaches for video denoising. However in the last fifteen years they have been used almost exclusively for real-time applications with denoising performance far from being state-of-the-art. In this work we propose a simple frame recursive method which is fast, has a low memory complexity and achieves results competitive with more complex state-of-the-art methods that require processing several input frames for producing each output frame. Furthermore, in terms of visual quality, the proposed approach is able to recover many details that are missed by most non-recursive methods. As an additional contribution we also propose an off-line post-processing of the denoised video that boosts denoising quality and temporal consistency. "
  },
  "cvpr2019_ntire_high-resolutionsingleimagedehazingusingencoder-decoderarchitecture": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "High-Resolution Single Image Dehazing Using Encoder-Decoder Architecture",
    "authors": [
      "Simone Bianco",
      "Luigi Celona",
      "Flavio Piccoli",
      "Raimondo Schettini"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Bianco_High-Resolution_Single_Image_Dehazing_Using_Encoder-Decoder_Architecture_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Bianco_High-Resolution_Single_Image_Dehazing_Using_Encoder-Decoder_Architecture_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work we propose HR-Dehazer, a novel and accurate method for image dehazing. An encoder-decoder neural network is trained to learn a direct mapping between a hazy image and its respective clear version. We designed a special loss that forces the network to keep into account the semantics of the input image and to promote consistency among local structures. In addition, this loss makes the system more invariant to scale changes. Quantitative results on the recently released DenseHaze dataset introduced for the NTIRE2019-Dehazing challenge demonstrates the effectiveness of the proposed method. Furthermore, qualitative results on real data show that the described solution generalizes well to different never-seen scenarios."
  },
  "cvpr2019_ntire_content-preservingtoneadjustmentforimageenhancement": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Content-Preserving Tone Adjustment for Image Enhancement",
    "authors": [
      "Simone Bianco",
      "Claudio Cusano",
      "Flavio Piccoli",
      "Raimondo Schettini"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Bianco_Content-Preserving_Tone_Adjustment_for_Image_Enhancement_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Bianco_Content-Preserving_Tone_Adjustment_for_Image_Enhancement_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a novel method based on Convolutional Neural Networks for content-preserving tone adjustment. The method is at the same time fast and accurate since we decouple the inference of the parameters and the color transform: the parameters are inferred from a downsampled version of the input image and the transformation is applied to the full resolution input. The method includes two steps of image enhancement: the first one is a global color transformation, while the second one is a local transformation.Experiments conducted on the DPED - DSLR Photo Enhancement Dataset, that has been used for the NTIRE19 Image Enhancement Challenge, and on the MIT-Adobe FiveK dataset, that is widely used for image enhancement, demonstrate the effectiveness of the proposed method."
  },
  "cvpr2019_ntire_orientation-awaredeepneuralnetworkforrealimagesuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Orientation-Aware Deep Neural Network for Real Image Super-Resolution",
    "authors": [
      "Chen Du",
      "He Zewei",
      "Sun Anshun",
      "Yang Jiangxin",
      "Cao Yanlong",
      "Cao Yanpeng",
      "Tang Siliang",
      "Michael Ying Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Du_Orientation-Aware_Deep_Neural_Network_for_Real_Image_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Du_Orientation-Aware_Deep_Neural_Network_for_Real_Image_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, Convolutional Neural Network (CNN) based approaches have achieved impressive single image super-resolution (SISR) performance in terms of accuracy and visual effects. It is noted that most SISR methods assume that the low-resolution (LR) images are obtained through bicubic interpolation down-sampling, thus their performance on real-world LR images is limited. In this paper, we proposed a novel orientation-aware deep neural network (OA-DNN) model, which incorporate a number of orientation feature extraction and channel attention modules (OAMs), to achieve good SR performance on real-world LR images captured by a single-lens reflex (DSLR) camera. Orientation-aware features extracted in different directions are adaptively combined through a channel-wise attention mechanism to generate more distinctive features for high-fidelity recovery of image details. Moreover, we reshape the input image into smaller spatial size but deeper depth via an inverse pixel-shuffle operation to accelerate the training/testing speed without sacrificing restoration accuracy. Extensive experimental results indicate that our OA-DNN model achieves a good balance between accuracy and speed. The extended OA-DNN*+ model further increases PSNR index by 0.18 dB compared with our previously submitted version. Codes will be made public after publication."
  },
  "cvpr2019_ntire_edvrvideorestorationwithenhanceddeformableconvolutionalnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "EDVR: Video Restoration With Enhanced Deformable Convolutional Networks",
    "authors": [
      "Xintao Wang",
      "Kelvin C.K. Chan",
      "Ke Yu",
      "Chao Dong",
      "Chen Change Loy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Wang_EDVR_Video_Restoration_With_Enhanced_Deformable_Convolutional_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Wang_EDVR_Video_Restoration_With_Enhanced_Deformable_Convolutional_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable convolutions, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at https://github.com/xinntao/EDVR."
  },
  "cvpr2019_ntire_suppressingmodeloverfittingforimagesuper-resolutionnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Suppressing Model Overfitting for Image Super-Resolution Networks",
    "authors": [
      "Ruicheng Feng",
      "Jinjin Gu",
      "Yu Qiao",
      "Chao Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Feng_Suppressing_Model_Overfitting_for_Image_Super-Resolution_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Feng_Suppressing_Model_Overfitting_for_Image_Super-Resolution_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Large deep networks have demonstrated competitive performance in single image super-resolution (SISR), with a huge volume of data involved. However, in real-world scenarios, due to the limited accessible training pairs, large models exhibit undesirable behaviors such as overfitting and memorization. To suppress model overfitting and further enjoy the merits of large model capacity, we thoroughly investigate generic approaches for supplying additional training data pairs. In particular, we introduce a simple learning principle MixUp to train networks on interpolations of sample pairs, which encourages networks to support linear behavior in-between training samples. In addition, we propose a data synthesis method with learned degradation, enabling models to use extra high-quality images with higher content diversity. This strategy proves to be successful in reducing the biases of data. By combining these components -- MixUp and synthetic training data, large models can be trained without overfitting under very limited data samples and achieve satisfactory generalization performance. Our method won the second place in NTIRE2019 Real SR Challenge. "
  },
  "cvpr2019_ntire_ntire2019challengeonvideodeblurringmethodsandresults": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Challenge on Video Deblurring: Methods and Results",
    "authors": [
      "Seungjun Nah",
      "Radu Timofte",
      "Sungyong Baik",
      "Seokil Hong",
      "Gyeongsik Moon",
      "Sanghyun Son",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_Methods_and_Results_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_Methods_and_Results_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper reviews the first NTIRE challenge on video deblurring (restoration of rich details and high frequency components from blurred video frames) with focus on the proposed solutions and results. A new REalistic and Diverse Scenes dataset (REDS) was employed. The challenge was divided into 2 tracks. Track 1 employed dynamic motion blurs while Track 2 had additional MPEG video compression artifacts. Each competition had 109 and 93 registered participants. Total 13 teams competed in the final testing phase. They gauge the state-of-the-art in video deblurring problem."
  },
  "cvpr2019_ntire_ntire2019challengeonvideosuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Challenge on Video Super-Resolution: Methods and Results",
    "authors": [
      "Seungjun Nah",
      "Radu Timofte",
      "Shuhang Gu",
      "Sungyong Baik",
      "Seokil Hong",
      "Gyeongsik Moon",
      "Sanghyun Son",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Super-Resolution_Methods_and_Results_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Super-Resolution_Methods_and_Results_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper reviews the first NTIRE challenge on video super-resolution (restoration of rich details in low-resolution video frames) with focus on proposed solutions and results. A new REalistic and Diverse Scenes dataset (REDS) was employed. The challenge was divided into 2 tracks. Track 1 employed standard bicubic downscaling setup while Track 2 had realistic dynamic motion blurs. Each competition had 124 and 104 registered participants. There were total 14 teams in the final testing phase. They gauge the state-of-the-art in video super-resolution."
  },
  "cvpr2019_ntire_ntire2019challengeonvideodeblurringandsuper-resolutiondatasetandstudy": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Challenge on Video Deblurring and Super-Resolution: Dataset and Study",
    "authors": [
      "Seungjun Nah",
      "Sungyong Baik",
      "Seokil Hong",
      "Gyeongsik Moon",
      "Sanghyun Son",
      "Radu Timofte",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_and_Super-Resolution_Dataset_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_and_Super-Resolution_Dataset_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper introduces a novel large dataset for video deblurring, video super-resolution and studies the state-of-the-art as emerged from the NTIRE 2019 video restoration challenges. The video deblurring and video super-resolution challenges are each the first challenge of its kind, with 4 competitions, hundreds of participants and tens of proposed solutions. Our newly collected REalistic and Diverse Scenes dataset (REDS) was employed by the challenges. In our study, we compare the solutions from the challenges to a set of representative methods from the literature and evaluate them on our proposed REDS dataset. We find that the NTIRE 2019 challenges push the state-of-the-art in video deblurring and super-resolution, reaching compelling performance on our newly proposed REDS dataset."
  },
  "cvpr2019_ntire_multi-scaledeepneuralnetworksforrealimagesuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Scale Deep Neural Networks for Real Image Super-Resolution",
    "authors": [
      "Shangqi Gao",
      "Xiahai Zhuang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Gao_Multi-Scale_Deep_Neural_Networks_for_Real_Image_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Gao_Multi-Scale_Deep_Neural_Networks_for_Real_Image_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Single image super-resolution (SR) is extremely difficult if the upscaling factors of image pairs are unknown and different from each other, which is common in real image SR. To tackle the difficulty, we develop two multi-scale deep neural networks (MsDNN) in this work. Firstly, due to the high computation complexity in high-resolution spaces, we process an input image mainly in two different downscaling spaces, which could greatly lower the usage of GPU memory. Then, to reconstruct the details of an image, we design a multi-scale residual network (MsRN) in the downscaling spaces based on the residual blocks. Besides, we propose a multi-scale dense network based on the dense blocks to compare with MsRN. Finally, our empirical experiments show the robustness of MsDNN for image SR when the upscaling factor is unknown. According to the preliminary results of NTIRE 2019 image SR challenge, our team (ZXHresearch@fudan) ranks 21-st among all participants. The implementation of MsDNN is released at: https://github.com/shangqigao/gsq-image-SR"
  },
  "cvpr2019_ntire_ri-gananend-to-endnetworkforsingleimagehazeremoval": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "RI-GAN: An End-To-End Network for Single Image Haze Removal",
    "authors": [
      "Akshay Dudhane",
      "Harshjeet Singh Aulakh",
      "Subrahmanyam Murala"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Dudhane_RI-GAN_An_End-To-End_Network_for_Single_Image_Haze_Removal_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Dudhane_RI-GAN_An_End-To-End_Network_for_Single_Image_Haze_Removal_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The presence of the haze or fog particles in the atmosphere causes visibility degradation in the captured scene. Most of the initial approaches anticipate the transmission map of the hazy scene, airlight component and make use of an atmospheric scattering model to reduce the effect of haze and to recover the haze-free scene. In spite of the remarkable progress of these approaches, they propagate cascaded error upstretched due to the employed priors. We embrace this observation and designed an end-to-end generative adversarial network (GAN) for single image haze removal. Proposed network bypasses the intermediate stages and directly recovers the haze-free scene. Generator architecture of the proposed network is designed using a novel residual inception (RI) module. Proposed RI module comprises of dense connections within the multi-scale convolution layers which allows it to learn the integrated flavors of the haze-related features. Discriminator of the proposed network is built using the dense residual module. Further, to preserve the edge and the structural details in the recovered haze-free scene, structural similarity index and edge loss along with the L1 loss are incorporated in the GAN loss. Experimental analysis has been carried out on NTIRE2019 dehazing challenge dataset, D-Hazy [1] and indoor SOTS [22] databases. Experiments on the publically available datasets show that the proposed method outperforms the existing methods for image de-hazing."
  },
  "cvpr2019_ntire_scanspatialcolorattentionnetworksforrealsingleimagesuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "SCAN: Spatial Color Attention Networks for Real Single Image Super-Resolution",
    "authors": [
      "Xuan Xu",
      "Xin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Xu_SCAN_Spatial_Color_Attention_Networks_for_Real_Single_Image_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Xu_SCAN_Spatial_Color_Attention_Networks_for_Real_Single_Image_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Conceptually similar to adaptation in model-based approaches, attention has received increasing more attention in deep learning recently. As a tool to reallocate limited computational resources based on the importance of informative components, attention mechanism has found successful applications in both high-level and low-level vision tasks which includes channel attention, spatial attention, non-local attention and etc. However, to the best of our knowledge, attention mechanism has not been studied for the R,G,B channels of color images in the open literature. In this paper, we propose a spatial color attention networks (SCAN) designed to jointly exploit the spatial and spectral dependency within color images. More specifically, we present a spatial color attention module that calibrates important color information for individual color components from output feature maps of residual groups. When compared against previous state-of-the-art method Residual Channel Attention Networks (RCAN), SCAN has achieved superior performance in terms of both subjective and objective qualities on the dataset provided by NTIRE2019 real single image super-resolution challenge."
  },
  "cvpr2019_ntire_adaptingimagesuper-resolutionstate-of-the-artsandlearningmulti-modelensembleforvideosuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Adapting Image Super-Resolution State-Of-The-Arts and Learning Multi-Model Ensemble for Video Super-Resolution",
    "authors": [
      "Chao Li",
      "Dongliang He",
      "Xiao Liu",
      "Yukang Ding",
      "Shilei Wen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Li_Adapting_Image_Super-Resolution_State-Of-The-Arts_and_Learning_Multi-Model_Ensemble_for_Video_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Li_Adapting_Image_Super-Resolution_State-Of-The-Arts_and_Learning_Multi-Model_Ensemble_for_Video_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, image super-resolution has been widely studied and achieved significant progress by leveraging the power of deep convolutional neural networks. However, there has been limited advancement in video super-resolution (VSR) due to the complex temporal patterns in videos. In this paper, we investigate how to adapt state-of-the-art methods of image super-resolution for video super-resolution. The proposed adapting method is straightforward. The informationamong successive frames is well exploited, while the overhead on the original image super-resolution method is negligible. Furthermore, we propose an learning-based method to ensemble the outputs from multiple super-resolution models. Our methods show superior performance and rank second in the NTIRE2019 Video Super-Resolution Challenge Track 1."
  },
  "cvpr2019_ntire_hierarchicalbackprojectionnetworkforimagesuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Hierarchical Back Projection Network for Image Super-Resolution",
    "authors": [
      "Zhi-Song Liu",
      "Li-Wen Wang",
      "Chu-Tak Li",
      "Wan-Chi Siu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Liu_Hierarchical_Back_Projection_Network_for_Image_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Liu_Hierarchical_Back_Projection_Network_for_Image_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep learning based single image super-resolution methods use a large number of training datasets and have recently achieved great quality progress both quantitatively and qualitatively. Most deep networks focus on nonlinear mapping from low-resolution inputs to high-resolution outputs via residual learning without exploring the feature abstraction and analysis. We propose a Hierarchical Back Projection Network (HBPN), that cascades multiple HourGlass (HG) modules to bottom-up and top-down process features across all scales to capture various spatial correlations and then consolidates the best representation for reconstruction. We adopt the back projection blocks in our proposed network to provide the error correlated up- and down-sampling process to replace simple deconvolution and pooling process for better estimation. A new Softmax based Weighted Reconstruction (WR) process is used to combine the outputs of HG modules to further improve super-resolution. Experimental results on various datasets (including the validation dataset, NTIRE2019, of the Real Image Super-resolution Challenge) show that our proposed approach can achieve and improve the performance of the state-of-the-art methods for different scaling factors."
  },
  "cvpr2019_ntire_multi-scaleadaptivedehazingnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Scale Adaptive Dehazing Network",
    "authors": [
      "Shuxin Chen",
      "Yizi Chen",
      "Yanyun Qu",
      "Jingying Huang",
      "Ming Hong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Chen_Multi-Scale_Adaptive_Dehazing_Network_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Chen_Multi-Scale_Adaptive_Dehazing_Network_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Since haze degrades an image including contrast decreasing and color lost, which has a negative effect on the subsequent object detection and recognition. single image dehazing is a challenging visual task. Most existing dehazing methods are not robust to uneven haze. In this paper, we developed an adaptive distillation network to solve the dehaze problem with non-uniform haze, which does not rely on the physical scattering model. The proposed model consists of two parts: an adaptive distillation module and a multi-scale enhancing module. The adaptive distillation block reassigns the channel feature response via adaptively weighting the input maps. And then the important feature maps are separated from the trivial for further focused learning. After that, a multi-scale enhancing module containing two pyramid downsampling layers is employed to fuse the context features for haze-free images restoration in a coarse-to-fine way. Extensive experimental results on synthetic and real datasets demonstrates that the proposed approach outperforms the state-of-the-arts in both quantitative and qualitative evaluations."
  },
  "cvpr2019_ntire_multibootvsrmulti-stagemulti-referencebootstrappingforvideosuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "MultiBoot Vsr: Multi-Stage Multi-Reference Bootstrapping for Video Super-Resolution",
    "authors": [
      "Ratheesh Kalarot",
      "Fatih Porikli"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Kalarot_MultiBoot_Vsr_Multi-Stage_Multi-Reference_Bootstrapping_for_Video_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Kalarot_MultiBoot_Vsr_Multi-Stage_Multi-Reference_Bootstrapping_for_Video_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " To make the best use of the previous estimations and shared redundancy across the consecutive video frames, here we propose a scene and class agnostic, fully convolutional neural network model for 4xvideo super-resolution. One stage of our network is composed of a motion compensation based input subnetwork, a blending backbone, and a spatial upsampling subnetwork. We recurrently apply this network to reconstruct high-resolution frames and then reuse them as additional reference frames after reshuffling them into multiple low-resolution images. This allows us to bootstrap and enhance image quality progressively. Our experiments show that our method generates temporally consistent and high-quality results without artifacts. Our method is ranked as the second best based on the SSIM scores on the NTIRE2019 VSR Challenge, Clean Track."
  },
  "cvpr2019_ntire_learningrawimagedenoisingwithbayerpatternunificationandbayerpreservingaugmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Learning Raw Image Denoising With Bayer Pattern Unification and Bayer Preserving Augmentation",
    "authors": [
      "Jiaming Liu",
      "Chi-Hao Wu",
      "Yuzhi Wang",
      "Qin Xu",
      "Yuqian Zhou",
      "Haibin Huang",
      "Chuan Wang",
      "Shaofan Cai",
      "Yifan Ding",
      "Haoqiang Fan",
      "Jue Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Liu_Learning_Raw_Image_Denoising_With_Bayer_Pattern_Unification_and_Bayer_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Liu_Learning_Raw_Image_Denoising_With_Bayer_Pattern_Unification_and_Bayer_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present new data pre-processing and augmentation techniques for DNN-based raw image denoising. Compared with traditional RGB image denoising, performing this task on direct camera sensor readings presents new challenges such as how to effectively handle various Bayer patterns from different data sources, and subsequently how to perform valid data augmentation with raw images. To address the first problem, we propose a Bayer pattern unification (BayerUnify) method to unify different Bayer patterns. This allows us to fully utilize a heterogeneous dataset to train a single denoising model instead of training one model for each pattern. Furthermore, while it is essential to augment the dataset to improve model generalization and performance, we discovered that it is error-prone to modify raw images by adapting augmentation methods designed for RGB images. Towards this end, we present a Bayer preserving augmentation (BayerAug) method as an effective approach for raw image augmentation. Combining these data processing technqiues with a modified U-Net, our method achieves a PSNR of 52.11 and a SSIM of 0.9969 in NTIRE 2019 Real Image Denoising Challenge, demonstrating the state-of-the-art performance."
  },
  "cvpr2019_ntire_featureforwardingforefficientsingleimagedehazing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Feature Forwarding for Efficient Single Image Dehazing",
    "authors": [
      "Peter Morales",
      "Tzofi Klinghoffer",
      "Seung Jae Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Morales_Feature_Forwarding_for_Efficient_Single_Image_Dehazing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Morales_Feature_Forwarding_for_Efficient_Single_Image_Dehazing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Haze degrades content and obscures information of images, which can negatively impact vision-based decision-making in real-time systems. In this paper, we propose an efficient fully convolutional neural network (CNN) image dehazing method designed to run on edge graphical processing units (GPUs). We utilize three variants of our architecture to explore the dependency of dehazed image quality on parameter count and model design. The first two variants presented, a small and big version, make use of a single efficient encoder-decoder convolutional feature extractor. The final variant utilizes a pair of encoder-decoders for atmospheric light and transmission map estimation. Each variant ends with an image refinement pyramid pooling network to form the final dehazed image. For the big variant of the single-encoder network, we demonstrate state-of-the-art performance on the NYU Depth dataset. For the small variant, we maintain competitive performance on the super-resolution O/I-HAZE datasets without the need for image cropping. Finally, we examine some challenges presented by the Dense-Haze dataset when leveraging CNN architectures for dehazing of dense haze imagery and examine the impact of loss function selection on image quality. Benchmarks are included to show the feasibility of introducing this approach into real-time systems."
  },
  "cvpr2019_ntire_grdngroupedresidualdensenetworkforrealimagedenoisingandgan-basedreal-worldnoisemodeling": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "GRDN:Grouped Residual Dense Network for Real Image Denoising and GAN-Based Real-World Noise Modeling",
    "authors": [
      "Dong-Wook Kim",
      "Jae Ryun Chung",
      "Seung-Won Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Kim_GRDNGrouped_Residual_Dense_Network_for_Real_Image_Denoising_and_GAN-Based_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Kim_GRDNGrouped_Residual_Dense_Network_for_Real_Image_Denoising_and_GAN-Based_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent research on image denoising has progressed with the development of deep learning architectures, especially convolutional neural networks. However, real-world image denoising is still very challenging because it is not possible to obtain ideal pairs of ground-truth images and real-world noisy images. Owing to the recent release of benchmark datasets, the interest of the image denoising community is now moving toward the real-world denoising problem. In this paper, we propose a grouped residual dense network (GRDN), which is an extended and generalized architecture of the state-of-the-art residual dense network (RDN). The core part of RDN is defined as grouped residual dense block (GRDB) and used as a building module of GRDN. We experimentally show that the image denoising performance can be significantly improved by cascading GRDBs. In addition to the network architecture design, we also develop a new generative adversarial network-based real-world noise modeling method. We demonstrate the superiority of the proposed methods by achieving the highest score in terms of both the peak signal-to-noise ratio and the structural similarity in the NTIRE2019 Real Image Denoising Challenge - Track 2:sRGB."
  },
  "cvpr2019_ntire_deepiterativedown-upcnnforimagedenoising": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep Iterative Down-Up CNN for Image Denoising",
    "authors": [
      "Songhyun Yu",
      "Bumjun Park",
      "Jechang Jeong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Yu_Deep_Iterative_Down-Up_CNN_for_Image_Denoising_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Yu_Deep_Iterative_Down-Up_CNN_for_Image_Denoising_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Networks using down-scaling and up-scaling of feature maps have been studied extensively in low-level vision research owing to efficient GPU memory usage and their capacity to yield large receptive fields. In this paper, we propose a deep iterative down-up convolutional neural network (DIDN) for image denoising, which repeatedly decreases and increases the resolution of the feature maps. The basic structure of the network is inspired by U-Net which was originally developed for semantic segmentation. We modify the down-scaling and up-scaling layers for image denoising task. Conventional denoising networks are trained to work with a single-level noise, or alternatively use noise information as inputs to address multi-level noise with a single model. Conversely, because the efficient memory usage of our network enables it to handle multiple parameters, it is capable of processing a wide range of noise levels with a single model without requiring noise-information inputs as a work-around. Consequently, our DIDN exhibits state-of-the-art performance using the benchmark dataset and also demonstrates its superiority in the NTIRE 2019 real image denoising challenge."
  },
  "cvpr2019_ntire_denselyconnectedhierarchicalnetworkforimagedenoising": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Densely Connected Hierarchical Network for Image Denoising",
    "authors": [
      "Bumjun Park",
      "Songhyun Yu",
      "Jechang Jeong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Park_Densely_Connected_Hierarchical_Network_for_Image_Denoising_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Park_Densely_Connected_Hierarchical_Network_for_Image_Denoising_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, deep convolutional neural networks have been applied in numerous image processing researches and have exhibited drastically improved performances. In this study, we introduce a densely connected hierarchical image denoising network (DHDN), which exceeds the performances of state-of-the-art image denoising solutions. Our proposed network improves the image denoising performance by applying the hierarchical architecture of the modified U-Net; this makes our network to use a larger number of parameters than other methods. In addition, we induce feature reuse and solve the vanishing-gradient problem by applying dense connectivity and residual learning to our convolution blocks and network. Finally, we successfully apply the model ensemble and self-ensemble methods; this enable us to improve the performance of the proposed network. The performance of the proposed network is validated by winning the second place in the NTRIE 2019 real image denoising challenge sRGB track and the third place in the raw-RGB track. Additional experimental results on additive white Gaussian noise removal also establishes that the proposed network outperforms conventional methods; this is notwithstanding the fact that the proposed network handles a wide range of noise levels with a single set of trained parameters."
  },
  "cvpr2019_ntire_fractalresidualnetworkandsolutionsforrealsuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Fractal Residual Network and Solutions for Real Super-Resolution",
    "authors": [
      "Junhyung Kwak",
      "Donghee Son"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Kwak_Fractal_Residual_Network_and_Solutions_for_Real_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Kwak_Fractal_Residual_Network_and_Solutions_for_Real_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The degradation function in single image super-resolution (SISR) is usually bicubic with an integer scale factor. However, bicubic is not realistic and a scale factor is not always an integer number in the real world. We introduce some solutions that are appropriate for realistic SR. First, we propose down-upsampling module which allows general SR network to use GPU memory efficiently. With the module, we can stack more convolutional layers, resulting in a higher performance. We also adopt a new regularization loss, auto-encoder loss. That loss generalizes down-upsampling module. Furthermore, we propose fractal residual network (FRN) for SISR. We extend residual in residual structure by adding new residual shells and name that structure FRN because of the self-similarity like the fractal. We show that our proposed model outperforms state-of-the-art methods and demonstrate the effectiveness of our solutions by several experiments on NTIRE 2019 dataset. "
  },
  "cvpr2019_ntire_densesceneinformationestimationnetworkfordehazing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Dense Scene Information Estimation Network for Dehazing",
    "authors": [
      "Tiantong Guo",
      "Xuelu Li",
      "Venkateswararao Cherukuri",
      "Vishal Monga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Guo_Dense_Scene_Information_Estimation_Network_for_Dehazing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Guo_Dense_Scene_Information_Estimation_Network_for_Dehazing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Image dehazing continues to be one of the most challenging inverse problems. Deep learning methods have emerged to complement traditional model-based methods and have helped define a new state of the art in achievable dehazed image quality. Yet, practical challenges remain in dehazing of real-world images where the scene is heavily covered with dense haze, even to the extent that no scene information can be observed visually. Many recent dehazing methods have addressed this challenge by designing deep networks that estimate physical parameters in the haze model, i.e. ambient light (A) and transmission map (t). The inverse of the haze model may then be used to estimate the dehazed image. In this work, we develop two novel network architectures to further this line of investigation. Our first model, denoted as At-DH, designs a shared DenseNet based encoder and two distinct DensetNet based decoders to jointly estimate the scene information viz. A and t respectively. This in contrast to most recent efforts (include those published in CVPR'18) that estimate these physical parameters separately.As a natural extension of At-DH, we develop the AtJ-DH network, which adds one more DenseNet based decoder to jointly recreate the haze-free image along with A and t. The knowledge of (ground truth) training dehazed/clean images can be exploited by a custom regularization term that further enhances the estimates of model parameters A and t in AtJ-DH. Experiments performed on challenging benchmark image datasets of NTIRE'19 and NTIRE'18 demonstrate that At-DH and AtJ-DH can outperform state-of-the-art alternatives, especially when recovering images corrupted by dense haze."
  },
  "cvpr2019_ntire_dense123colorenhancementdehazingnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Dense `123' Color Enhancement Dehazing Network",
    "authors": [
      "Tiantong Guo",
      "Venkateswararao Cherukuri",
      "Vishal Monga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Guo_Dense_123_Color_Enhancement_Dehazing_Network_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Guo_Dense_123_Color_Enhancement_Dehazing_Network_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Single image dehazing has gained much attention recently. A typical learning based approach uses example hazy and clean image pairs to train a mapping between the two. Of the learning based methods, those based on deep neural networks have shown to deliver state of the art performance. An important aspect of recovered image quality is the color information, which is severely compromised when the image is corrupted by very dense haze. While many different network architectures have been developed for recovering dehazed images, an explicit attention to recovering individual color channels with a design that ensures their quality has been missing. Our proposed work, focuses on this issue by developing a novel network structure that comprises of: a common DenseNet based feature encoder whose output branches into three distinct DensetNet based decoders to yield estimates of the R, G and B color channels of the image. A subsequent refinement block further enhances the final synthesized RGB/color image by joint processing of these color channels. Inspired by its structure, we call our approach the One-To-Three Color Enhancement Dehazing (123-CEDH) network. To ensure the recovery of physically meaningful and high quality color channels, the main network loss function is further regularized by a multi-scale structural similarity index term as well as a term that enhances color contrast. Experiments reveal that 123-CEDH has the ability to recover color information at early training stages (i.e. in the first few epochs) vs. other highly competitive methods. Validation on the benchmark datasets of the NTIRE'19 and NTIRE'18 dehazing challenges reveals the 123-CEDH to be one of the Top-3 methods based on results released in the NTIRE'19 competition."
  },
  "cvpr2019_ntire_adeepmotiondeblurringnetworkbasedonper-pixeladaptivekernelswithresidualdown-upandup-downmodules": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Deep Motion Deblurring Network Based on Per-Pixel Adaptive Kernels With Residual Down-Up and Up-Down Modules",
    "authors": [
      "Hyeonjun Sim",
      "Munchurl Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Sim_A_Deep_Motion_Deblurring_Network_Based_on_Per-Pixel_Adaptive_Kernels_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Sim_A_Deep_Motion_Deblurring_Network_Based_on_Per-Pixel_Adaptive_Kernels_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Due to the object motion during the camera exposure time, latent pixel information appears scattered in a blurred image. A large dataset of dynamic motion blur and blur-free frame pairs enables deep neural networks to learn deblurring operations directly in end-to-end manners. In this paper, we propose a novel motion deblurring kernel learning network that predicts the per-pixel deblur kernel and a residual image. The learned deblur kernel filters and linearly combines neighboring pixels to restore the clean pixels in its corresponding location. The per-pixel adaptive convolution with the learned deblur kernel can effectively handle non-uniform blur. At the same time, the generated residual image is added to the adaptive convolution result to compensate for the limited receptive field of the learned deblur kernel. That is, the adaptive convolution and the residual image play different but complementary roles each other to reconstruct the latent clean images in a collaborative manner. We also propose residual down-up (RDU) and residual up-down (RUD) blocks that help improve the motion deblurring performance. The RDU and RUD blocks are designed to adjust the spatial size and the number of channels of the intermediate feature within the blocks. We demonstrate the effectiveness of our motion deblurring kernel learning network by showing intensive experimental results compared to those of the state-of-the-art methods."
  },
  "cvpr2019_ntire_imagecolorizationbycapsulenetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Image Colorization by Capsule Networks",
    "authors": [
      "Gokhan Ozbulak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Ozbulak_Image_Colorization_by_Capsule_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Ozbulak_Image_Colorization_by_Capsule_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, a simple topology of Capsule Network (CapsNet) is investigated for the problem of image colorization. The generative and segmentation capabilities of the original CapsNet topology, which is proposed for image classification problem, is leveraged for the colorization of the images by modifying the network as follows: 1) The original CapsNet model is adapted to map the grayscale input to the output in the CIE Lab colorspace, 2) The feature detector part of the model is updated by using deeper feature layers inherited from VGG-19 pre-trained model with weights in order to transfer low-level image representation capability to this model, 3) The margin loss function is modified as Mean Squared Error (MSE) loss to minimize the image-to-image mapping. The resulting CapsNet model is named as Colorizer Capsule Network (ColorCapsNet). The performance of the ColorCapsNet is evaluated on the DIV2K dataset and promising results are obtained to investigate Capsule Networks further for image colorization problem."
  },
  "cvpr2019_ntire_anempiricalinvestigationofefficientspatio-temporalmodelinginvideorestoration": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "An Empirical Investigation of Efficient Spatio-Temporal Modeling in Video Restoration",
    "authors": [
      "Yuchen Fan",
      "Jiahui Yu",
      "Ding Liu",
      "Thomas S. Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Fan_An_Empirical_Investigation_of_Efficient_Spatio-Temporal_Modeling_in_Video_Restoration_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Fan_An_Empirical_Investigation_of_Efficient_Spatio-Temporal_Modeling_in_Video_Restoration_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present an empirical investigation of efficient spatio-temporal modeling in video restoration tasks. To achieve a better speed-accuracy trade-off, our investigation covers the intersection of three dimensions in deep video restoration networks: spatial-wise, channel-wise and temporal-wise. We enumerate various network architectures ranging from 2D convolutional models to 3D convolutional models, and discuss their gain and loss in terms of training time, model size, boundary effects, prediction accuracy and the visual quality of restored videos. Under a strictly controlled computational budget, we also specifically explore the design inside each residual building block in a video restoration network, which consists a mixture of 2D and 3D convolutional layers.Our findings are summarized as follows: (1) In 3D convolutional models, setting more computation/channels for spatial convolution leads to better performance than on temporal convolution. (2) The best variant of 3D convolutional models is better than 2D convolutional models, but the performance gap is close. (3) In a very limited range, the performance can be improved by the increase of window size (5 frames for 2D model) or padding size (6 frames for 3D model). Based on these findings, we introduce the WDVR, wide-activated 3D convolutional network for video restoration, which achieves a better accuracy under similar computational budgets and runtime latency. Our solution based on WDVR also won 2nd places in three out of four tracks of NTIRE 2019 Challenge for Video Super-Resolution and Deblurring. "
  },
  "cvpr2019_ntire_encoder-decoderresidualnetworkforrealsuper-resolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Encoder-Decoder Residual Network for Real Super-Resolution",
    "authors": [
      "Guoan Cheng",
      "Ai Matsune",
      "Qiuyu Li",
      "Leilei Zhu",
      "Huaijuan Zang",
      "Shu Zhan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Cheng_Encoder-Decoder_Residual_Network_for_Real_Super-Resolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Cheng_Encoder-Decoder_Residual_Network_for_Real_Super-Resolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Real single image super-resolution is a challenging task to restore lost information and attenuate noise from images mixed unknown degradations complicatedly. Classic single image super-resolution, aims to enhance the resolution of bicubically degraded images, has recently obtained great success via deep learning. However, these existing methods do not perform well for real single image super-resolution. In this paper, we propose an Encoder-Decoder Residual Network (EDRN) for real single image super-resolution. We adopt an encoder-decoder structure to encode highly effective features and embed the coarse-to-fine method. The coarse-to-fine structure can gradually restore lost information and reduce noise effects. We empirically rethink and discuss the usage of batch normalization. Compared with state-of-the-art methods in classic single image super-resolution, our EDRN can efficiently restore the corresponding high-resolution image from a degraded input image. Our EDRN achieved the 9th place for PSNR and top 5 for SSIM in the final result of NTIRE 2019 Real Super-resolution Challenge. The source code and the trained model are available at https://github.com/yyknight/NTIRE2019_EDRN."
  },
  "cvpr2019_ntire_ganmerareproducingaestheticallypleasingphotographsusingdeepadversarialnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "GANmera: Reproducing Aesthetically Pleasing Photographs Using Deep Adversarial Networks",
    "authors": [
      "Nelson Chong",
      "Lai-Kuan Wong",
      "John See"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Chong_GANmera_Reproducing_Aesthetically_Pleasing_Photographs_Using_Deep_Adversarial_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Chong_GANmera_Reproducing_Aesthetically_Pleasing_Photographs_Using_Deep_Adversarial_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Generative adversarial networks (GANs) have become increasingly popular in recent years owing to its ability to synthesize and transfer. The image enhancement task can also be modeled as an image-to-image translation problem. In this paper, we propose GANmera, a deep adversarial network which is capable of performing aesthetically-driven enhancement of photographs. The network adopts a 2-way GAN architecture and is semi-supervised with aesthetic-based binary labels (good and bad). The network is trained with unpaired image sets, hence eliminating the need for strongly supervised before-after pairs. Using CycleGAN as the base architecture, several fine-grained modifications are made to the loss functions, activation functions and resizing schemes, to achieve improved stability in the generator. Two training strategies are devised to produce results with varying aesthetic output. Quantitative evaluation on the recent benchmark MIT-Adobe-5K dataset demonstrate the capability of our method in achieving state-of-the-art PSNR results. We also show qualitatively that the proposed approach produces aesthetically-pleasing images. This work is a shortlisted submission to the CVPR 2019 NTIRE Image Enhancement Challenge."
  },
  "cvpr2019_ntire_ntire2019challengeonrealimagedenoisingmethodsandresults": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Challenge on Real Image Denoising: Methods and Results",
    "authors": [
      "Abdelrahman Abdelhamed",
      "Radu Timofte",
      "Michael S. Brown"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Abdelhamed_NTIRE_2019_Challenge_on_Real_Image_Denoising_Methods_and_Results_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Abdelhamed_NTIRE_2019_Challenge_on_Real_Image_Denoising_Methods_and_Results_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper reviews the NTIRE 2019 challenge on real image denoising with focus on the proposed methods and their results. The challenge has two tracks for quantitatively evaluating image denoising performance in (1) the Bayer-pattern raw-RGB and (2) the standard RGB (sRGB) color spaces. The tracks had 216 and 220 registered participants, respectively. A total of 15 teams, proposing 17 methods, competed in the final phase of the challenge. The proposed methods by the 15 teams represent the current state-of-the-art performance in image denoising targeting real noisy images."
  },
  "cvpr2019_ntire_ntire2019challengeonrealimagesuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Challenge on Real Image Super-Resolution: Methods and Results",
    "authors": [
      "Jianrui Cai",
      "Shuhang Gu",
      "Radu Timofte",
      "Lei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Cai_NTIRE_2019_Challenge_on_Real_Image_Super-Resolution_Methods_and_Results_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Cai_NTIRE_2019_Challenge_on_Real_Image_Super-Resolution_Methods_and_Results_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper reviewed the 3rd NTIRE challenge on single-image super-resolution (restoration of rich details in a low-resolution image) with a focus on proposed solutions and results. The challenge had 1 track, which was aimed at the real-world single image super-resolution problem with an unknown scaling factor. Participants were mapping low-resolution images captured by a DSLR camera with a shorter focal length to their high-resolution images captured at a longer focal length. With this challenge, we introduced a novel real-world super-resolution dataset (RealSR). The track had 403 registered participants, and 36 teams competed in the final testing phase. They gauge the state-of-the-art in real-world single image super-resolution."
  },
  "cvpr2019_ntire_ntire2019challengeonimageenhancementmethodsandresults": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Challenge on Image Enhancement: Methods and Results",
    "authors": [
      "Andrey Ignatov",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Ignatov_NTIRE_2019_Challenge_on_Image_Enhancement_Methods_and_Results_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Ignatov_NTIRE_2019_Challenge_on_Image_Enhancement_Methods_and_Results_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper reviews the first NTIRE challenge on perceptual image enhancement with the focus on proposed solutions and results. The participating teams were solving a real-world photo enhancement problem, where the goal was to map low-quality photos from the iPhone 3GS device to the same photos captured with Canon 70D DSLR camera. The considered problem embraced a number of computer vision subtasks, such as image denoising, image resolution and sharpness enhancement, image color/contrast/exposure adjustment, etc. The target metric used in this challenge combined PSNR and SSIM scores with solutions' perceptual results measured in the user study. The proposed solutions significantly improved baseline results, defining the state-of-the-art for practical image enhancement."
  },
  "cvpr2019_ntire_ntire2019challengeonimagecolorizationreport": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Challenge on Image Colorization: Report",
    "authors": [
      "Shuhang Gu",
      "Radu Timofte",
      "Richard Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Gu_NTIRE_2019_Challenge_on_Image_Colorization_Report_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Gu_NTIRE_2019_Challenge_on_Image_Colorization_Report_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper reviews the NTIRE challenge on image colorization (estimating color information from the corresponding gray image) with focus on proposed solutions and results. It is the first challenge of its kind. The challenge had 2 tracks. Track 1 takes a single gray image as input. In Track 2, in addition to the gray input image, some color seeds (randomly samples from the latent color image) are also provided for guiding the colorization process. The operators were learnable through provided pairs of gray and color training images. The tracks had 188 registered participants, and 8 teams competed in the final testing phase."
  },
  "cvpr2019_ntire_ntire2019imagedehazingchallengereport": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2019 Image Dehazing Challenge Report",
    "authors": [
      "Codruta O. Ancuti",
      "Cosmin Ancuti",
      "Radu Timofte",
      "Luc Van Gool",
      "Lei Zhang",
      "Ming-Hsuan Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/NTIRE/Ancuti_NTIRE_2019_Image_Dehazing_Challenge_Report_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/NTIRE/Ancuti_NTIRE_2019_Image_Dehazing_Challenge_Report_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper reviews the second challenge on image dehazing (restoration of rich details in hazy image) with focus on proposed solutions and results. The training data consists from 55 hazy images (with dense haze generated in an indoor or outdoor environment) and their corresponding ground truth (haze-free) images of the same scene. The dense haze has been produced using a professional haze/fog generator that imitates the real conditions of haze scenes. The evaluation consists from the comparison of the dehazed images with the ground truth images. The dehazing process was learnable through provided pairs of haze-free and hazy train images. There were270 registered participants and 23 teams competed in the final testing phase. They gauge the state-of-the-art in image dehazing."
  },
  "cvpr2019_biometrics_segmentation-lessandnon-holisticdeep-learningframeworksforirisrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Segmentation-Less and Non-Holistic Deep-Learning Frameworks for Iris Recognition",
    "authors": [
      "Hugo Proenca",
      "Joao C. Neves"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Proenca_Segmentation-Less_and_Non-Holistic_Deep-Learning_Frameworks_for_Iris_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Proenca_Segmentation-Less_and_Non-Holistic_Deep-Learning_Frameworks_for_Iris_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Driven by the pioneer iris biometrics approach, the most relevant recognition methods published over the years are \"phase-based\", and segment/normalize the iris to obtain dimensionless representations of the data that attenuate the differences in scale, translation, rotation and pupillary dilation. In this paper we present a recognition method that dispenses the iris segmentation, noise detection and normalization phases, and is agnostic to the levels of pupillary dilation, while maintaining state-of-the-artperformance. Based on deep-learning classification models, we analyze the displacements between biologically corresponding patches in pairs of iris images, to discriminate between genuine and impostor comparisons. Such corresponding patches are firstly learned in the normalized representations of the irises - the domain where they are optimally distinguishable -but are remapped into a segmentation-less polar coordinate system that uniquely requires iris detection.In recognition time, samples are only converted into this segmentation-less coordinate system, where matching is performed. In the experiments, we considered the challenging open-world setting, and used three well known data sets (CASIA-4-Lamp, CASIA-4-Thousand and WVU), concluding positively about the effectiveness of the proposed algorithm, particularly in cases where accurately segmenting the iris is a challenge."
  },
  "cvpr2019_biometrics_mobiletouchdbmobiletouchcharacterdatabaseinthewildandbiometricbenchmark": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "MobileTouchDB: Mobile Touch Character Database in the Wild and Biometric Benchmark",
    "authors": [
      "Ruben Tolosana",
      "Javier Gismero-Trujillo",
      "Ruben Vera-Rodriguez",
      "Julian Fierrez",
      "Javier Ortega-Garcia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Tolosana_MobileTouchDB_Mobile_Touch_Character_Database_in_the_Wild_and_Biometric_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Tolosana_MobileTouchDB_Mobile_Touch_Character_Database_in_the_Wild_and_Biometric_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we introduce a new database of mobile touch on-line data named MobileTouchDB. The database contains more than 64K on-line character samples performed by 217 users, using 94 different smartphone models, with an average of 314 samples per user. In each acquisition session, users had to draw all numbers (from 0 to 9), upper- and lower-case letters (54), different symbols (8), and passwords composed of 4 numbers (6). Regarding the acquisition protocol, MobileTouchDB comprises a maximum of 6 captured sessions per subject with a time gap between them of at least 2 days. This database studies an unsupervised mobile scenario with no restrictions in terms of position, posture, and devices. Users downloaded and used the acquisition app on their own devices freely. In addition, we also report a benchmark evaluation of biometric authentication on MobileTouchDB, providing an easily reproducible framework for two different scenarios of biometric user authentication: i) based on one character, and ii) based on character combinations.The database was collected with three main goals in mind: i) analyse the discriminative power of novel human touch interaction dynamics, ii) enhance traditional password authentication systems through the incorporation of touch biometric information as a second level of user authentication, and iii) analyse the way we interact with mobile devices on a daily basis in order to enhance continuous authentication systems. MobileTouchDB is publicly available in GitHub."
  },
  "cvpr2019_biometrics_facialsoftbiometricsdetectiononlowpowerdevices": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Facial Soft Biometrics Detection on Low Power Devices",
    "authors": [
      "Manolis Vasileiadis",
      "Georgios Stavropoulos",
      "Dimitrios Tzovaras"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Vasileiadis_Facial_Soft_Biometrics_Detection_on_Low_Power_Devices_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Vasileiadis_Facial_Soft_Biometrics_Detection_on_Low_Power_Devices_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Soft biometric traits have been proven to enhance person identification accuracy, when used complementary to primary biometric traits. They present a series of advantages such as compliance to the human language, robustness to low quality data, non-intrusive and consent free acquisition, and privacy preservation, increasing their applicability in realistic conditions. They can be extracted from a variety of individual modalities, with the human face being considered as the most informative source of attributes, as it provides rich geometrical and texture features. Recent advances in computer vision have allowed the accurate detection of such features under varying, non-ideal capturing conditions, with this increase in detection capacity, however, coming at the cost of high computational complexity. Meanwhile, the research and market interest has shifted towards the implementation of such methods on low power devices (i.e mobile phones), with data security concerns favoring on-device offline computation instead of cloud-based services. Towards this end, and taking into consideration recent advances in computationally efficient CNN design and multitask learning, we propose a novel CNN architecture, suitable for real time implementation on low power devices, which simultaneously performs gender, age, race, eyes state, eyewear, smile, beard and moustache estimation from unconstrained face images. The architecture employs the Mobilenet architecture and exploits the correlation between the individual biometric features, performing comparably to three state-of-the-art face analysis systems, while requiring significantly lower computational resources."
  },
  "cvpr2019_biometrics_hierarchicalfeature-pairrelationnetworksforfacerecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Hierarchical Feature-Pair Relation Networks for Face Recognition",
    "authors": [
      "Bong-Nam Kang",
      "Yonghyun Kim",
      "Bongjin Jun",
      "Daijin Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Kang_Hierarchical_Feature-Pair_Relation_Networks_for_Face_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Kang_Hierarchical_Feature-Pair_Relation_Networks_for_Face_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a novel face recognition method using a Hierarchical Feature Relational Network (HFRN) which extracts facial part representations around facial landmark points, and predicts hierarchical latent relations between facial part representations. These hierarchical latent relations should be unique relations within the same identity and discriminative relations among different identities for face recognition task. To do this, the HFRN extracts appearance features as facial parts representations around facial landmark points on the feature maps, globally pool these extracted appearance features onto single feature vectors, and captures the relations for the pairs of appearance features. The HFRN captures the locally detailed relations in the low-level layers and the locally abstracted global relations in the high-level layers for the pairs of appearance features extracted around facial landmark points projected on each layer, respectively. These relations from low-level layers to high-level layers are concatenated into a single hierarchical relation feature. To further improve the accuracy of face recognition, we combine the global appearance feature with the hierarchical relation feature. In experiments, the proposed method achieves the comparable performance in the 1:1 face verification and 1:N face identification tasks compared to existing state-of-the-art methods on the challenging IARPA Janus Benchmark A (IJB-A) and IARPA Janus Benchmark B (IJB-B) datasets."
  },
  "cvpr2019_biometrics_detectingtexturedcontactlensinuncontrolledenvironmentusingdensepad": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Detecting Textured Contact Lens in Uncontrolled Environment Using DensePAD",
    "authors": [
      "Daksha Yadav",
      "Naman Kohli",
      "Mayank Vatsa",
      "Richa Singh",
      "Afzel Noore"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Yadav_Detecting_Textured_Contact_Lens_in_Uncontrolled_Environment_Using_DensePAD_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Yadav_Detecting_Textured_Contact_Lens_in_Uncontrolled_Environment_Using_DensePAD_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The ubiquitous use of smartphones has spurred the research in mobile iris devices. Due to their convenience, these mobile devices are also utilized in unconstrained outdoor conditions. This scenario has necessitated the development of reliable iris recognition algorithms for such an uncontrolled environment. Additionally, iris presentation attacks such as textured contact lens pose a major challenge to current iris recognition systems. Motivated by these, this paper presents two key contributions. First, a new Unconstrained Multi-sensor Iris Presentation Attack (UnMIPA) database is created. It consists of more than 18,000 iris images of subjects wearing textured contact lens and without wearing contact lenses captured in both indoor and outdoor environment using multiple iris sensors. The second contribution of this paper is a novel algorithm, DensePAD, which utilizes DenseNet based convolutional neural network architecture for iris presentation attack detection. In-depth experimental evaluation of this algorithm reveals its superior performance in detecting iris presentation attack images on multiple databases. The performance of the proposed DensePAD algorithm is also evaluated in real-world scenarios of open-set iris presentation attacks which highlights the challenging nature of detecting iris presentation attack images from unseen distributions."
  },
  "cvpr2019_biometrics_personre-identificationfromgaitusinganautocorrelationnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Person Re-Identification From Gait Using an Autocorrelation Network",
    "authors": [
      "Cassandra Carley",
      "Ergys Ristani",
      "Carlo Tomasi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Carley_Person_Re-Identification_From_Gait_Using_an_Autocorrelation_Network_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Carley_Person_Re-Identification_From_Gait_Using_an_Autocorrelation_Network_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a new biometric feature based on autocorrelation using an end-to-end trained network to capture human gait from different viewpoints. Our method condenses an unbounded image stream into a fixed size descriptor, and capitalizes on the periodic nature of walking to leverage sequence self-similarity. Autocorrelation is invariant to start or end of the gait cycle, can be efficiently computed online, and is well suited for capturing pose frequencies. We demonstrate empirically that under equal settings an autocorrelation network provides a more complete representation for gait than existing work, resulting in improved person re-identification performance."
  },
  "cvpr2019_biometrics_revisitingdepth-basedfacerecognitionfromaqualityperspective": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Revisiting Depth-Based Face Recognition From a Quality Perspective",
    "authors": [
      "Zhenguo Hu",
      "Qijun Zhao",
      "Feng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Hu_Revisiting_Depth-Based_Face_Recognition_From_a_Quality_Perspective_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Hu_Revisiting_Depth-Based_Face_Recognition_From_a_Quality_Perspective_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Face recognition using depth data has attracted increasing attention from both academia and industry in the past five years. Despite the large number of depth-based face recognition methods in the literature, high quality data are usually required for high recognition accuracy. In this paper, we measure the quality of 3D face data in terms of resolution and precision, and evaluate how the accuracy of three deep face recognition models varies on several benchmark databases as the facial depth data resolution changes from dense to sparse and as the precision changes from high to low. From the experimental results, several observations are made. (i) Given a high precision, a low resolution of 3K is sufficient to represent a 3D face; when the precision decreases, using higher resolutions can benefit face recognition, but the recognition accuracy becomes saturated as the resolution reaches 10K. (ii) Depth precision is more critical than resolution in depth-based face recognition, and a precision of 1mm is generally preferred as a good balance between accuracy and cost. (iii) The deep models trained with low-quality data perform more stable across data of different quality levels. We believe that these observations are beneficial for both depth sensor manufacturers and depth-based face recognition system developers."
  },
  "cvpr2019_biometrics_exploringfactorsforimprovinglowresolutionfacerecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Exploring Factors for Improving Low Resolution Face Recognition",
    "authors": [
      "Omid Abdollahi Aghdam",
      "Behzad Bozorgtabar",
      "Hazim Kemal Ekenel",
      "Jean-Philippe Thiran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Aghdam_Exploring_Factors_for_Improving_Low_Resolution_Face_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Aghdam_Exploring_Factors_for_Improving_Low_Resolution_Face_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " State-of-the-art deep face recognition approaches report near perfect performance on popular benchmarks, e.g., Labeled Faces in the Wild. However, their performance deteriorates significantly when they are applied on low quality images, such as those acquired by surveillance cameras. A further challenge for low resolution face recognition for surveillance applications is the matching of recorded low resolution probe face images with high resolution reference images, which could be the case in watchlist scenarios. In this paper, we have addressed these problems and investigated the factors that would contribute to the identification performance of the state-of-the-art deep face recognition models when they are applied to low resolution face recognition under mismatched conditions. We have observed that the following factors affect performance in a positive way: appearance variety and resolution distribution of the training dataset, resolution matching between the gallery and probe images, and the amount of information included in the probe images. By leveraging this information, we have utilized deep face models trained on MS-Celeb-1M and fine-tuned on VGGFace2 dataset and achieved state-of-the-art accuracies on the SCFace and ICB-RW benchmarks, even without using any training data from the datasets of these benchmarks."
  },
  "cvpr2019_biometrics_facesynthesisandrecognitionusingdisentangledrepresentation-learningwassersteingan": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Face Synthesis and Recognition Using Disentangled Representation-Learning Wasserstein GAN",
    "authors": [
      "Gee-Sern Jison Hsu",
      "Chia-Hao Tang",
      "Moi Hoon Yap"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Hsu_Face_Synthesis_and_Recognition_Using_Disentangled_Representation-Learning_Wasserstein_GAN_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Hsu_Face_Synthesis_and_Recognition_Using_Disentangled_Representation-Learning_Wasserstein_GAN_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose the Disentangled Representation-learning Wasserstein GAN (DR-WGAN) trained on augmented data for face recognition and face synthesis across pose. We improve the state-of-the-art DR-GAN with the Wasserstein loss considered in the discriminator so that the generative and adversarial framework can be better trained. The improved training leads to better face disentanglement and synthesis. We also highlight the influences of imbalanced training data on the disentangled facial representation learning, and point out the difficulty of generating faces of extreme poses. We explore the recently proposed nonlinear 3D Morphable Model (3DMM) to augment the training data, and verify the contributions made by the learning on augmented data. Additionally, we also compare different data normalization schemes and reveal the benefit of using the group normalization. The proposed framework is verified through the experiments on benchmark databases, and compared with contemporary approaches for performance evaluation. "
  },
  "cvpr2019_biometrics_fastcontinuoususerauthenticationusingdistancemetricfusionoffree-textkeystrokedata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Fast Continuous User Authentication Using Distance Metric Fusion of Free-Text Keystroke Data",
    "authors": [
      "Blaine Ayotte",
      "Jiaju Huang",
      "Mahesh K. Banavar",
      "Daqing Hou",
      "Stephanie Schuckers"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Ayotte_Fast_Continuous_User_Authentication_Using_Distance_Metric_Fusion_of_Free-Text_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Ayotte_Fast_Continuous_User_Authentication_Using_Distance_Metric_Fusion_of_Free-Text_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Keystroke dynamics are a powerful behavioral biometric capable of determining user identity and for continuous authentication. It is an unobtrusive method that can complement an existing security system such as a password scheme and provides continuous user authentication. Existing methods record all keystrokes and use n-graphs that measure the timing between consecutive keystrokes to distinguish between users. Current state-of-the-art algorithms report EER's of 7.5% or higher with 1000 characters. With 1000 characters it takes a longer time to detect an imposter and significant damage could be done.In this paper, we investigate how quickly a user is authenticated or how many digraphs are required to accurately detect an imposter in an uncontrolled free-text environment. We present and evaluate the effectiveness of three distance metrics individually and fused with each other. We show that with just 100 digraphs, about the length of a single sentence, we achieve an EER of 35.3%. At 200 digraphs the EER drops to 15.3%. With more digraphs, the performance continues to steadily improve. With 1000 digraphs the EER drops to 3.6% which is an improvement over the state-of-the-art. "
  },
  "cvpr2019_biometrics_significantfeaturebasedrepresentationfortemplateprotection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Significant Feature Based Representation for Template Protection",
    "authors": [
      "Deen Dayal Mohan",
      "Nishant Sankaran",
      "Sergey Tulyakov",
      "Srirangaraj Setlur",
      "Venu Govindaraju"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Mohan_Significant_Feature_Based_Representation_for_Template_Protection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Mohan_Significant_Feature_Based_Representation_for_Template_Protection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The security of biometric templates is of paramount importance. Leakage of biometric information may result in loss of private data and can lead to the compromise of the biometric system. Yet, the security of templates is often overlooked in favour of performance. In this paper, we present a plug-and-play framework for creating secure face templates with negligible degradation in the performance of the system. We propose a significant bit based representation whichguarantees security in addition to other biometric aspects such as cancelability and reproducibility. In addition to being scalable, the proposed method does not make unrealistic assumptions regarding the pose or illumination of the face images. We provide experimental results on two unconstrained datasets - IJB-A and IJB-C."
  },
  "cvpr2019_biometrics_subsurfaceandlayerintertwinedtemplateprotectionusinginherentpropertiesoffull-fieldopticalcoherencetomographyfingerprintimaging": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Subsurface and Layer Intertwined Template Protection Using Inherent Properties of Full-Field Optical Coherence Tomography Fingerprint Imaging",
    "authors": [
      "Kiran B. Raja",
      "Ramachandra Raghavendra",
      "Egidijus Auksorius",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Raja_Subsurface_and_Layer_Intertwined_Template_Protection_Using_Inherent_Properties_of_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Raja_Subsurface_and_Layer_Intertwined_Template_Protection_Using_Inherent_Properties_of_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The emergence of Full Field-Optical Coherence Tomography (FF-OCT) for fingerprint imaging has shown it's ability in addressing and solving the drawbacks of traditional fingerprinting solutions such as spoofing attacks, low accuracy for abraded fingerprint. With the availability of multiple internal fingerprints (from subsurface captured at different depths), it is also essential to consider the aspects of ideal biometrics where the privacy of the fingerprint data is preserved. In this work, we propose a new framework for fingerprint template protection, highly customized to FF-OCT by exploring the interplay between subsurface. As a first of it's kind work attempting template protection for FF-OCT fingerprints, we explore deeply learnt features to derive first level of template for subsurface fingerprint image. We further propose to intertwine subsurface level templates to provide better and robust templates. With the set of extensive experiments on a FF-OCT fingerprint database of 200 unique fingerprints with a total of 2400 images, we demonstrate reliable biometric performance resulting in EER of 5.69% for unprotected template at first layer (subsurface) of fingerprint in FF-OCT, an EER of 5.86% for the protected templates at same layer and EER of 5.08% with the final protected templates with proposed intertwining of subsurface fingerprint. Further, through the security analysis, we also validate the strength of the proposed approach with near ideal unlinkability."
  },
  "cvpr2019_biometrics_facehallucinationrevisitedanexploratorystudyondatasetbias": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Face Hallucination Revisited: An Exploratory Study on Dataset Bias",
    "authors": [
      "Klemen Grm",
      "Martin Pernus",
      "Leo Cluzel",
      "Walter J. Scheirer",
      "Simon Dobrisek",
      "Vitomir Struc"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Grm_Face_Hallucination_Revisited_An_Exploratory_Study_on_Dataset_Bias_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Grm_Face_Hallucination_Revisited_An_Exploratory_Study_on_Dataset_Bias_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Contemporary face hallucination (FH) models exhibit considerable ability to reconstruct high-resolution (HR) details from low-resolution (LR) face images.This ability is commonly learned from examples of corresponding HR-LR image pairs, created by artificially down-sampling the HR ground truth data. This down-sampling (or degradation) procedure not only defines the characteristics of the LR training data, but also determines the type of image degradations the learned FH models are eventually able to handle. If the image characteristics encountered with real-world LR images differ from the ones seen during training, FH models are still expected to perform well, but in practice may not produce the desired results. In this paper we study this problem and explore the bias introduced into FH models by the characteristics of the training data. We systematically analyze the generalization capabilities of several FH models in various scenarios where the degradation function does not match the training setup and conduct experiments with synthetically downgraded as well as real-life low-quality images. We make several interesting findings that provide insight into existing problems with FH models and point to future research directions."
  },
  "cvpr2019_biometrics_multimodalageandgenderclassificationusingearandprofilefaceimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Multimodal Age and Gender Classification Using Ear and Profile Face Images",
    "authors": [
      "Dogucan Yaman",
      "Fevziye Irem Eyiokur",
      "Hazim Kemal Ekenel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Yaman_Multimodal_Age_and_Gender_Classification_Using_Ear_and_Profile_Face_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Yaman_Multimodal_Age_and_Gender_Classification_Using_Ear_and_Profile_Face_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present multimodal deep neural network frameworks for age and gender classification, which take input a profile face image as well as an ear image. Our main objective is to enhance the accuracy of soft biometric trait extraction from profile face images by additionally utilizing a promising biometric modality: ear appearance. For this purpose, we provided end-to-end multimodal deep learning frameworks. We explored different multimodal strategies by employing data, feature, and score level fusion. To increase representation and discrimination capability of the deep neural networks, we benefited from domain adaptation and employed center loss besides softmax loss. We conducted extensive experiments on the UND-F, UND-J2, and FERET datasets. Experimental results indicated that profile face images contain a rich source of information for age and gender classification. We found that the presented multimodal system achieves very high age and gender classification accuracies. Moreover, we attained superior results compared to the state-of-the-art profile face image or ear image-based age and gender classification methods."
  },
  "cvpr2019_biometrics_synthesizingirisimagesusingrasganwithapplicationinpresentationattackdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Biometrics",
    "title": "Synthesizing Iris Images Using RaSGAN With Application in Presentation Attack Detection",
    "authors": [
      "Shivangi Yadav",
      "Cunjian Chen",
      "Arun Ross"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Biometrics/Yadav_Synthesizing_Iris_Images_Using_RaSGAN_With_Application_in_Presentation_Attack_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Biometrics/Yadav_Synthesizing_Iris_Images_Using_RaSGAN_With_Application_in_Presentation_Attack_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work we design a new technique for generating synthetic iris images and demonstrate its potential for presentation attack detection (PAD). The proposed technique utilizes the generative capability of a Relativistic Average Standard Generative Adversarial Network (RaSGAN) to synthesize high quality images of the iris. Unlike traditional GANs, RaSGAN enhances the generative power of the network by introducing a \"relativistic\" discriminator (and generator), which aims to maximize the probability that the real input data is more realistic than the synthetic data (and vice-versa, respectively). The resultant generated images are observed to be very similar to real iris images. Furthermore, we demonstrate the viability of using these synthetic images to train a PAD system that can generalize well to \"unseen\" attacks, i.e., the PAD system is able to detect attacks that were not used during the training phase."
  },
  "cvpr2019_saiad_anempiricalevaluationstudyonthetrainingofsdcfeaturesfordensepixelmatching": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "An Empirical Evaluation Study on the Training of SDC Features for Dense Pixel Matching",
    "authors": [
      "Rene Schuster",
      "Oliver Wasenmuller",
      "Christian Unger",
      "Didier Stricker"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Schuster_An_Empirical_Evaluation_Study_on_the_Training_of_SDC_Features_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Schuster_An_Empirical_Evaluation_Study_on_the_Training_of_SDC_Features_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Training a deep neural network is a non-trivial task. Not only the tuning of hyperparameters, but also the gathering and selection of training data, the design of the loss function, and the construction of training schedules is important to get the most out of a model. In this study, we perform a set of experiments all related to these issues. The model for which different training strategies are investigated is the recently presented SDC descriptor network (stacked dilated convolution). It is used to describe images on pixel-level for dense matching tasks. Our work analyzes SDC in more detail, validates some best practices for training deep neural networks, and provides insights into training with multiple domain data."
  },
  "cvpr2019_saiad_dscdense-sparseconvolutionforvectorizedinferenceofconvolutionalneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "DSC: Dense-Sparse Convolution for Vectorized Inference of Convolutional Neural Networks",
    "authors": [
      "Alexander Frickenstein",
      "Manoj Rohit Vemparala",
      "Christian Unger",
      "Fatih Ayar",
      "Walter Stechele"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Frickenstein_DSC_Dense-Sparse_Convolution_for_Vectorized_Inference_of_Convolutional_Neural_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Frickenstein_DSC_Dense-Sparse_Convolution_for_Vectorized_Inference_of_Convolutional_Neural_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The efficient applications of Convolutional Neural Networks (CNNs) in automotive-rated and safety critical hardware-accelerators require an interplay of DNN design optimization, programming techniques and hardware resources. Ad-hoc pruning would result in irregular sparsity and compression leading in very inefficient real world applications. Therefore, the proposed methodology, called Dense-Sparse Convolution, makes use of the right balance between pruning regularity, quantization and the underlying vectorized hardware. Different word length compute units, e.g. CPU, are used for low latency inference of the spares CNNs. The proposed open source CPU-kernel scales along with the vector word length and the number of cores."
  },
  "cvpr2019_saiad_uncertaintymeasuresandpredictionqualityratingforthesemanticsegmentationofnestedmultiresolutionstreetsceneimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Uncertainty Measures and Prediction Quality Rating for the Semantic Segmentation of Nested Multi Resolution Street Scene Images",
    "authors": [
      "Matthias Rottmann",
      "Marius Schubert"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Rottmann_Uncertainty_Measures_and_Prediction_Quality_Rating_for_the_Semantic_Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Rottmann_Uncertainty_Measures_and_Prediction_Quality_Rating_for_the_Semantic_Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In the semantic segmentation of street scenes the reliability of the prediction and therefore uncertainty measures are of highest interest. We present a method that generates for each input image a hierarchy of nested crops around the image center and presents these, all re-scaled to the same size, to a neural network for semantic segmentation. The resulting softmax outputs are then post processed such that we can investigate mean and variance over all image crops as well as mean and variance of uncertainty heat maps obtained from pixel-wise uncertainty measures, like the entropy, applied to each crop's softmax output. In our tests, we use the publicly available DeepLabv3+ MobilenetV2 network (trained on the Cityscapes dataset) and demonstrate that the incorporation of crops improves the quality of the prediction and that we obtain more reliable uncertainty measures. These are then aggregated over predicted segments for either classifying between IoU=0 and IoU>0 (meta classification) or predicting the IoU via linear regression (meta regression). The latter yields reliable performance estimates for segmentation networks, in particular useful in the absence of ground truth. For the task of meta classification we obtain a classification accuracy of 81.93% and an AUROC of 89.89%. For meta regression we obtain an R2 value of 84.77%. These results yield significant improvements compared to other approaches."
  },
  "cvpr2019_saiad_theattackgeneratorasystematicapproachtowardsconstructingadversarialattacks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks",
    "authors": [
      "Felix Assion",
      "Peter Schlicht",
      "Florens Gressner",
      "Wiebke Gunther",
      "Fabian Huger",
      "Nico Schmidt",
      "Umair Rasheed"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Assion_The_Attack_Generator_A_Systematic_Approach_Towards_Constructing_Adversarial_Attacks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Assion_The_Attack_Generator_A_Systematic_Approach_Towards_Constructing_Adversarial_Attacks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Most state-of-the-art machine learning (ML) classification systems are vulnerable to adversarial perturbations. As a consequence, adversarial robustness poses a significant challenge for the deployment of ML-based systems in safety- and security-critical environments like autonomous driving, disease detection or unmanned aerial vehicles. In the past years we have seen an impressive amount of publications presenting more and more new adversarial attacks. However, the attack research seems to be rather unstructured and new attacks often appear to be random selections from the unlimited set of possible adversarial attacks. With this publication, we present a structured analysis of the adversarial attack creation process. By detecting different building blocks of adversarial attacks, we outline the road to new sets of adversarial attacks. We call this the \"attack generator\". In the pursuit of this objective, we summarize and extend existing adversarial perturbation taxonomies. The resulting taxonomy is then linked to the application context of computer vision systems for autonomous vehicles, i.e. semantic segmentation and object detection. Finally, in order to prove the usefulness of the attack generator, we investigate existing semantic segmentation attacks with respect to the detected defining components of adversarial attacks."
  },
  "cvpr2019_saiad_ontherobustnessofredundantteacher-studentframeworksforsemanticsegmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "On the Robustness of Redundant Teacher-Student Frameworks for Semantic Segmentation",
    "authors": [
      "Andreas Bar",
      "Fabian Huger",
      "Peter Schlicht",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Bar_On_the_Robustness_of_Redundant_Teacher-Student_Frameworks_for_Semantic_Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Bar_On_the_Robustness_of_Redundant_Teacher-Student_Frameworks_for_Semantic_Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The trend towards autonomous systems in today's technology comes with the need for environment perception. Deep neural networks (DNNs) constantly showed state-of-the-art performance over the last few years in visual machine perception, e.g., semantic segmentation. While DNNs work fine on uncorrupted data, recently introduced adversarial examples (AEs) led to misclassification with high confidence. This lack of robustness against such adversarial attacks questions the use of DNNs in safety-critical autonomous systems, e.g., autonomous driving vehicles. In this work, we address the mentioned problem with the use of a redundant teacher-student framework, consisting of a static teacher network (T), a static student network (S), and a constantly adapting student network (A). By using this triplet in combination with a novel inverse feature matching (IFM) loss, we show that a significant robustness increase of student DNNs against adversarial attacks is achieveable, while maintaining semantic segmentation quality at a reasonably high level. With our approach, we manage to increase the mean intersection over union (mean IoU) ratio between static student adversarial examples and clean images from about 35 % to about 80 % on the Cityscapes dataset. Moreover, our proposed method can be integrated into any DNN-based perception mechanism to increase the (online) robustness in an adversarial environment, created from static model knowledge."
  },
  "cvpr2019_saiad_leveragingsemanticembeddingsforsafety-criticalapplications": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Leveraging Semantic Embeddings for Safety-Critical Applications",
    "authors": [
      "Thomas Brunner",
      "Frederik Diehl",
      "Michael Truong Le",
      "Alois Knoll"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Brunner_Leveraging_Semantic_Embeddings_for_Safety-Critical_Applications_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Brunner_Leveraging_Semantic_Embeddings_for_Safety-Critical_Applications_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Semantic Embeddings are a popular way to represent knowledge in the field of zero-shot learning. We observe their interpretability and discuss their potential utility in a safety-critical context. Concretely, we propose to use them to add introspection and error detection capabilities to neural network classifiers. First, we show how to create embeddings from symbolic domain knowledge. We discuss how to use them for interpreting mispredictions and propose a simple error detection scheme. We then introduce the concept of semantic distance: a real-valued score that measures confidence in the semantic space. We evaluate this score on a traffic sign classifier and find that it achieves near state-of-the-art performance, while being significantly faster to compute than other confidence scores. Our approach requires no changes to the original network and is thus applicable to any task for which domain knowledge is available."
  },
  "cvpr2019_saiad_theethicaldilemmawhen(not)settingupcost-baseddecisionrulesinsemanticsegmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "The Ethical Dilemma When (Not) Setting up Cost-Based Decision Rules in Semantic Segmentation",
    "authors": [
      "Robin Chan",
      "Matthias Rottmann",
      "Radin Dardashti",
      "Fabian Huger",
      "Peter Schlicht",
      "Hanno Gottschalk"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Chan_The_Ethical_Dilemma_When_Not_Setting_up_Cost-Based_Decision_Rules_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Chan_The_Ethical_Dilemma_When_Not_Setting_up_Cost-Based_Decision_Rules_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Neural networks for semantic segmentation can be seen as statistical models that provide for each pixel of one image a probability distribution on predefined classes. The predicted class is then usually obtained by the maximum a-posteriori probability (MAP) which is known as Bayes rule in decision theory. From decision theory we also know that the Bayes rule is optimal regarding the simple symmetric cost function. Therefore, it weights each type of confusion between two different classes equally, e.g., given images of urban street scenes there is no distinction in the cost function if the network confuses a person with a street or a building with a tree. Intuitively, there might be confusions of classes that are more important to avoid than others. In this work, we want to raise awareness of the possibility of explicitly defining confusion costs and the associated ethical difficulties if it comes down to providing numbers. We define two cost functions from different extreme perspectives, an egoistic and an altruistic one, and show how safety relevant quantities like precision / recall and (segment-wise) false positive / negative rate change when interpolating between MAP, egoistic and altruistic decision rules."
  },
  "cvpr2019_saiad_unsuperviseddomainadaptationtoimproveimagesegmentationqualitybothinthesourceandtargetdomain": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Unsupervised Domain Adaptation to Improve Image Segmentation Quality Both in the Source and Target Domain",
    "authors": [
      "Jan-Aike Bolte",
      "Markus Kamp",
      "Antonia Breuer",
      "Silviu Homoceanu",
      "Peter Schlicht",
      "Fabian Huger",
      "Daniel Lipinski",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SAIAD/Bolte_Unsupervised_Domain_Adaptation_to_Improve_Image_Segmentation_Quality_Both_in_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SAIAD/Bolte_Unsupervised_Domain_Adaptation_to_Improve_Image_Segmentation_Quality_Both_in_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Domain adaptation is becoming more and more important with the advancing development of machine learning and the ever-increasing diversity of available data. The advancement of autonomous driving depends very much on progress in machine learning, which relies heavily on vast amounts of training data. It is well known that the performance of such models drops, as soon as the data used during inference stems from a different domain as the training data. To avoid the need to label a separate dataset for each new domain, e.g., each new camera sensor, methods for domain adaptation are necessary. Most interesting are unsupervised domain adaptation approaches since they do not require costly labels for the target domain. In this paper we adapt a known domain adaptation approach to work in an unsupervised fashion for semantic segmentation on high resolution data and provide some analysis of the learned representations. With our domain-adapted semantic segmentation we were able to achieve a significant 15 % absolute increase in mean intersection over union (mIoU), securing a surprisingly good 5th rank on the target domain KITTI test set without having used any KITTI labels during training. In addition to that, we even improved quality on the source domain data."
  },
  "cvpr2019_trmtmct_bagoftricksandastrongbaselinefordeeppersonre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "Bag of Tricks and a Strong Baseline for Deep Person Re-Identification",
    "authors": [
      "Hao Luo",
      "Youzhi Gu",
      "Xingyu Liao",
      "Shenqi Lai",
      "Wei Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper explores a simple and efficient baseline for person re-identification (ReID). Person re-identification (ReID) with deep neural networks has made progress and achieved high performance in recent years. However, many state-of-the-arts methods design complex network structure and concatenate multi-branch features. In the literature, some effective training tricks are briefly appeared in several papers or source codes. This paper will collect and evaluate these effective training tricks in person ReID. By combining these tricks together, the model achieves 94.5% rank-1 and 85.9% mAP on Market1501 with only using global features. Our codes and models are available at https://github.com/michuanhaohao/reid-strong-baseline."
  },
  "cvpr2019_trmtmct_maskedgraphattentionnetworkforpersonre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "Masked Graph Attention Network for Person Re-Identification",
    "authors": [
      "Liqiang Bao",
      "Bingpeng Ma",
      "Hong Chang",
      "Xilin Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Bao_Masked_Graph_Attention_Network_for_Person_Re-Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Bao_Masked_Graph_Attention_Network_for_Person_Re-Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The mainstream methods for person re-identification (ReID) mainly focus on the correspondence between individual sample images and labels, while ignoring rich global mutual information resides in the whole sample set. We propose a method called Masked Graph Attention Network (MGAT) to address this problem. MGAT operates on the complete graph constructed with the extracted features, where nodes are able to directionally attend over other nodes' features under the guidance of label information in the form of mask matrix. By using MGAT module, the previously neglected global mutual information is exploited to generate an optimized feature space with more discriminant power. Meanwhile, we propose to feedback the optimization information learned by MGAT module to the feature-embedding network to enhance the mapping capability, thus avoiding the difficulty to handle large-scale graphs in testing phase. To evaluate our method, we conduct experiments on three commonly used ReID datasets. The results show that our method outperforms most mainstream methods, and is highly comparable to the state-of-the-art method."
  },
  "cvpr2019_trmtmct_state-awarere-identificationfeatureformulti-targetmulti-cameratracking": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "State-Aware Re-Identification Feature for Multi-Target Multi-Camera Tracking",
    "authors": [
      "Peng Li",
      "Jiabin Zhang",
      "Zheng Zhu",
      "Yanwei Li",
      "Lu Jiang",
      "Guan Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Li_State-Aware_Re-Identification_Feature_for_Multi-Target_Multi-Camera_Tracking_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Li_State-Aware_Re-Identification_Feature_for_Multi-Target_Multi-Camera_Tracking_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Multi-target Multi-camera Tracking (MTMCT) aims to extract the trajectories from videos captured by a set of cameras. Recently, the tracking performance of MTMCT is significantly enhanced with the employment of re-identification (Re-ID) model. However, the appearance feature usually becomes unreliable due to the occlusion and orientation variance of the targets. Directly applying Re-ID model in MTMCT will encounter the problem of identity switches (IDS) and tracklet fragment caused by occlusion. To solve these problems, we propose a novel tracking framework in this paper. In this framework, the occlusion status and orientation information are utilized in Re-ID model with human pose information considered. In addition, the tracklet association using the proposed fused tracking feature is adopted to handle the fragment problem. The proposed tracker achieves 81.3% IDF1 on the multiple-camera hard sequence, which outperforms all other reference methods by a large margin. "
  },
  "cvpr2019_trmtmct_camera-awareimage-to-imagetranslationusingsimilaritypreservingstarganforpersonre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "Camera-Aware Image-To-Image Translation Using Similarity Preserving StarGAN for Person Re-Identification",
    "authors": [
      "Dahjung Chung",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Chung_Camera-Aware_Image-To-Image_Translation_Using_Similarity_Preserving_StarGAN_for_Person_Re-Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Chung_Camera-Aware_Image-To-Image_Translation_Using_Similarity_Preserving_StarGAN_for_Person_Re-Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Person re-identification is a crucial task in intelligent video surveillance systems.It can be defined as recognizing the same person from images of a person taken from different cameras at different times. In this paper, we present a camera-aware image-to-image translation using similarity preserving StarGAN (SP-StarGAN) as the data augmentation for person re-identification.We propose the addition of an identity mapping term and a multi-scale structural similarity term asadditional losses for the generator. SP-StarGAN can learn the relationship among the multiple cameras with a single model and generate the camera-aware extra training samples for person re-identification. We evaluate our proposed method onpublic datasets (Market-1501 and DukeMTMC-reID) and demonstrate the efficacy of ourmethod. We also reportcompetitive performance with the state-of-the-art methods."
  },
  "cvpr2019_trmtmct_indefenseoftheclassificationlossforpersonre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "In Defense of the Classification Loss for Person Re-Identification",
    "authors": [
      "Yao Zhai",
      "Xun Guo",
      "Yan Lu",
      "Houqiang Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Zhai_In_Defense_of_the_Classification_Loss_for_Person_Re-Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Zhai_In_Defense_of_the_Classification_Loss_for_Person_Re-Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The recent research for person re-identification has been focused on two trends. One is learning the part-based local features to form more informative feature descriptors. The other is designing effective metric learning loss functions such as the triplet loss family. We argue that learning global features with classification loss could achieve the same goal, even with some simple and cost-effective architecture design. In this paper, we first explain why the person re-id framework with standard classification loss usually has inferior performance compared to metric learning. Based on that, we further propose a person re-id framework featured by channel grouping and multi-branch strategy, which divides global features into multiple channel groups and learns the discriminative channel group features by multi-branch classification layers. The extensive experiments show that our framework outperforms prior state-of-the-arts in terms of both accuracy and inference speed."
  },
  "cvpr2019_trmtmct_unsupervisedpersonre-identificationwithiterativeself-superviseddomainadaptation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "Unsupervised Person Re-Identification With Iterative Self-Supervised Domain Adaptation",
    "authors": [
      "Haotian Tang",
      "Yiru Zhao",
      "Hongtao Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Tang_Unsupervised_Person_Re-Identification_With_Iterative_Self-Supervised_Domain_Adaptation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Tang_Unsupervised_Person_Re-Identification_With_Iterative_Self-Supervised_Domain_Adaptation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In real applications, person re-identification (re-id) is an inherently domain adaptive computer vision task which often requires the model trained on a group of people to perform well on an unlabeled dataset consisting of another group of pedestrians without supervised fine-tuning. Furthermore, there are typically a large number of classes (people) with small number of samples belonging to each class.Based on the characteristics of person re-id and general assumptions related to domain adaptation, we put forward a novel algorithm for cross-dataset person re-id. Our idea is simple yet effective: first, we preprocess the source dataset with style transfer GAN and train a baseline on it in a supervised learning manner, then we assign pseudo labels to unlabeled samples in target dataset based on the model trained on labeled source dataset; finally, we train on the target dataset with pseudo labels in traditional supervised learning manner. We adopt the idea of co-training in the training process to make the pseudo labels more reliable. We show the superiority of our model over all state-of-the-art methods through extensive experiments. "
  },
  "cvpr2019_trmtmct_aggregatingdeeppyramidalrepresentationsforpersonre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "Aggregating Deep Pyramidal Representations for Person Re-Identification",
    "authors": [
      "Niki Martinel",
      "Gian Luca Foresti",
      "Christian Micheloni"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Martinel_Aggregating_Deep_Pyramidal_Representations_for_Person_Re-Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Martinel_Aggregating_Deep_Pyramidal_Representations_for_Person_Re-Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Learning discriminative, view-invariant and multi-scale representations of person appearance with different semantic levels is of paramount importance for person Re-Identification (Re-ID). A surge of effort has been spent by the community to learn deep Re-ID models capturing a holistic single semantic level feature representation. To improve the achieved results, additional visual attributes and body part-driven models have been considered. However, these require extensive human annotation labor or demand additional computational efforts. We argue that a pyramid-inspired method capturing multi-scale information may overcome such requirements. Precisely, multi-scale stripes that represent visual information of a person can be used by a novel architecture factorizing them into latent discriminative factors at multiple semantic levels. A multi-task loss is combined with a curriculum learning strategy to learn a discriminative and invariant person representation which is exploited for triplet-similarity learning. Results on three benchmark Re-ID datasets demonstrate that better performance than existing methods are achieved (e.g., more than 90% accuracy on the Duke-MTMC dataset)."
  },
  "cvpr2019_trmtmct_multi-scalebody-partmaskguidedattentionforpersonre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "TRMTMCT",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Target Re-identification and Multi-Target Multi-Camera Tracking",
    "title": "Multi-Scale Body-Part Mask Guided Attention for Person Re-Identification",
    "authors": [
      "Honglong Cai",
      "Zhiguan Wang",
      "Jinxing Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/TRMTMCT/Cai_Multi-Scale_Body-Part_Mask_Guided_Attention_for_Person_Re-Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/TRMTMCT/Cai_Multi-Scale_Body-Part_Mask_Guided_Attention_for_Person_Re-Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Person re-identification becomes a more and more important task due to its wide applications. In practice, person re-identification still remains challenging due to the variation of person pose, different lighting, occlusion, misalignment, background clutter, etc. In this paper, we propose a multi-scale body-part mask guided attention network (MMGA), which jointly learns whole-body and part-body attention to help extract global and local features simultaneously. In MMGA, body-part masks are used to guide the training of corresponding attention. Experiments show that our proposed method can reduce the negative influence of variation of person pose, misalignment and background clutter. Our method achieves rank-1/mAP of 95.0%/87.2% on the Market1501 dataset, 89.5%/78.1% on the DukeMTMC-reID dataset, outperforming current state-of-the-art methods."
  },
  "cvpr2019_cvmi_redbloodcellimagegenerationfordataaugmentationusingconditionalgenerativeadversarialnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Red Blood Cell Image Generation for Data Augmentation Using Conditional Generative Adversarial Networks",
    "authors": [
      "Oleksandr Bailo",
      "DongShik Ham",
      "Young Min Shin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Bailo_Red_Blood_Cell_Image_Generation_for_Data_Augmentation_Using_Conditional_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Bailo_Red_Blood_Cell_Image_Generation_for_Data_Augmentation_Using_Conditional_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we describe how to apply image-to-image translation techniques to medical blood smear data to generate new data samples and meaningfully increase small datasets. Specifically, given the segmentation mask of the microscopy image, we are able to generate photorealistic images of blood cells which are further used alongside real data during the network training for segmentation and object detection tasks. This image data generation approach is based on conditional generative adversarial networks which have proven capabilities to high-quality image synthesis. In addition to synthesizing blood images, we synthesize segmentation mask as well which leads to a diverse variety of generated samples. The effectiveness of the technique is thoroughly analyzed and quantified through a number of experiments on a manually collected and annotated dataset of blood smear taken under a microscope."
  },
  "cvpr2019_cvmi_automatedfocusdistanceestimationfordigitalmicroscopyusingdeepconvolutionalneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Automated Focus Distance Estimation for Digital Microscopy Using Deep Convolutional Neural Networks",
    "authors": [
      "Tathagato Rai Dastidar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Dastidar_Automated_Focus_Distance_Estimation_for_Digital_Microscopy_Using_Deep_Convolutional_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Dastidar_Automated_Focus_Distance_Estimation_for_Digital_Microscopy_Using_Deep_Convolutional_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " An essential component of an automated digital microscopy system is auto focusing, which involves moving the microscope stage along the vertical axis to find the position where the underlying image is the sharpest. Auto focusing algorithms deployed in current commercially available digital microscopes cannot match the efficiency of a trained human operator. Traditionally, auto focusing has been achieved by acquiring multiple images in the vertical direction and maximising a measure of image sharpness. This paper presents a method for auto focusing based on deep convolutional neural networks (CNN). Given two images in the vertical focus stack, the CNN predicts the optimal distance the stage needs to be moved to achieve best focus, relative to the current position. The method was trained and results are demonstrated on a publicly available data set. It is shown to outperform previously published work on this data set. The compute and memory requirements of the model are shown to be ideal for deployment in an edge device with limited computing resources."
  },
  "cvpr2019_cvmi_onlineneuralcelltrackingusingblob-seedsegmentationandopticalflow": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Online Neural Cell Tracking Using Blob-Seed Segmentation and Optical Flow",
    "authors": [
      "Jingru Yi",
      "Pengxiang Wu",
      "Qiaoying Huang",
      "Hui Qu",
      "Daniel J. Hoeppner",
      "Dimitris N. Metaxas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Yi_Online_Neural_Cell_Tracking_Using_Blob-Seed_Segmentation_and_Optical_Flow_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Yi_Online_Neural_Cell_Tracking_Using_Blob-Seed_Segmentation_and_Optical_Flow_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Existing neural cell tracking methods generally use the morphology cell features for data association. However, these features are limited to the quality of cell segmentation and are prone to errors for mitosis determination. To overcome these issues, in this work we propose an online multi-object tracking method that leverages both cell appearance and motion features for data association. In particular, we propose a supervised blob-seed network (BSNet) to predict the cell appearance features and an unsupervised optical flow network (UnFlowNet) for capturing the cell motions. The data association is then solved using the Hungarian algorithm. Experimental evaluation shows that our approach achieves better performance than existing neural cell tracking methods. "
  },
  "cvpr2019_cvmi_cellimagesegmentationbyintegratingpix2pixsforeachclass": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Cell Image Segmentation by Integrating Pix2pixs for Each Class",
    "authors": [
      "Hiroki Tsuda",
      "Kazuhiro Hotta"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Tsuda_Cell_Image_Segmentation_by_Integrating_Pix2pixs_for_Each_Class_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Tsuda_Cell_Image_Segmentation_by_Integrating_Pix2pixs_for_Each_Class_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper presents a cell image segmentation method using Generative Adversarial Network (GAN) with multiple different roles. Pix2pix is a kind of GAN can be used for image segmentation. However, the accuracy is not sufficient because generator predicts multiple classes simultaneously. Thus, we propose to use multiple GANs with different roles. Each generator and discriminator has a specific role such as segmentation of cell membrane or nucleus. Since we assign each generator and discriminator to a different role, they can learn it efficiently. We evaluate the proposed method on the segmentation problem of cell images. The proposed method improved the segmentation accuracy in comparison to conventional pix2pix."
  },
  "cvpr2019_cvmi_automaticclassificationofwholeslidepapsmearimagesusingcnnwithpcabasedfeatureinterpretation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Automatic Classification of Whole Slide Pap Smear Images Using CNN With PCA Based Feature Interpretation",
    "authors": [
      "Kranthi Kiran GV",
      "G Meghana Reddy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/GV_Automatic_Classification_of_Whole_Slide_Pap_Smear_Images_Using_CNN_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/GV_Automatic_Classification_of_Whole_Slide_Pap_Smear_Images_Using_CNN_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Classification of whole slide image (WSI) cervical cell clusters traditionally involved two stages including segmentation to crop single cell patches followed by the classification of single cell patches. Hence the performance of classification pipeline depends on segmentation accuracy. We propose a first-time-right method which is a segmentation-free direct classification of WSI cervical cell clusters (without the extraction of single cell patches). The proposed method is evaluated on SIPaKMeD and Herlev datasets. Our method significantly outperformed previous methods and baselines with an accuracy of 96.37% on WSI patches (cell clusters) and 99.63% on single cell images.We also propose a PCA based feature interpretation method to visualize and understand the model to make its decisions more transparent. Our solution is promising in the development of automatic whole slide pap smear image classification system."
  },
  "cvpr2019_cvmi_deepmetriclearningforidentificationofmitoticpatternsofhep-2cellimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Deep Metric Learning for Identification of Mitotic Patterns of HEp-2 Cell Images",
    "authors": [
      "Krati Gupta",
      "Daksh Thapar",
      "Arnav Bhavsar",
      "Anil K. Sao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Gupta_Deep_Metric_Learning_for_Identification_of_Mitotic_Patterns_of_HEp-2_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Gupta_Deep_Metric_Learning_for_Identification_of_Mitotic_Patterns_of_HEp-2_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": "Automatic identification of mitotic type staining patterns in microscopy images is an important and challenging task, in computer-aided diagnosis (CAD) of autoimmune diseases. Such patterns are manifested on a HEp-2 based cell substrate and captured via Indirect immunoflourescence (IIF) based microscopy imaging technique. The present study proposes a deep metric learning methodology, in order to identify the mitotic staining patterns which are rather rare, among several other interphase patterns present in majority. Hence, the problem is framed as a mitotic v/s non-mitotic/interphase pattern classification problem. Here, the implemented network maps the input images into a latent space, in order to compare the distances between the samples, for class declaration, via a triplet-loss based framework. The framework yields good classification performance as max. 0.85 Matthews correlation coefficient in one case, with less false positive cases, when validated over a public dataset. "
  },
  "cvpr2019_cvmi_multi-objectportiontrackingin4dfluorescencemicroscopyimagerywithdeepfeaturemaps": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Multi-Object Portion Tracking in 4D Fluorescence Microscopy Imagery With Deep Feature Maps",
    "authors": [
      "Yang Jiao",
      "Mo Weng",
      "Mei Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Jiao_Multi-Object_Portion_Tracking_in_4D_Fluorescence_Microscopy_Imagery_With_Deep_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Jiao_Multi-Object_Portion_Tracking_in_4D_Fluorescence_Microscopy_Imagery_With_Deep_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " 3D fluorescence microscopy of living organisms has increasingly become an essential and powerful tool in biomedical research and diagnosis. Exploding amount of imaging data has been collected whereas efficient and effective computational tools to extract information from them are still lagged behind. This largely is due to the challenges in analyzing biological data. Interesting biological structures are not only small but often are morphologically irregular and highly dynamic. Tracking cells in live organisms has been studied for years as a sophisticated mission in bioinformatics. However, existing tracking methods for cells are not effective in tracking subcellular structures, such as protein complexes, which feature in continuous morphological changes, such as split and merge, in addition to fast migration and complex motion. In this paper, we first define the problem of multi-object portion tracking to model protein object tracking process. A multi-object tracking method with portion matching is proposed based on 3D segmentation results. The proposed method distills deep feature maps from deep networks, then recognizes and matches objects' portions using extended search. Experimental results confirm that the proposed method achieves 2.98% higher on consistent tracking accuracy and 35.48% higher on event identification accuracy."
  },
  "cvpr2019_cvmi_enhancedrotation-equivariantu-netfornuclearsegmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Enhanced Rotation-Equivariant U-Net for Nuclear Segmentation",
    "authors": [
      "Benjamin Chidester",
      "That-Vinh Ton",
      "Minh-Triet Tran",
      "Jian Ma",
      "Minh N. Do"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Chidester_Enhanced_Rotation-Equivariant_U-Net_for_Nuclear_Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Chidester_Enhanced_Rotation-Equivariant_U-Net_for_Nuclear_Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Despite recent advances in deep learning, the crucial task of nuclear segmentation for computational pathology remains challenging. Recently, deep learning, and specifically U-Nets, have shown significant improvements for this task, but there is still room for improvement by further enhancing the design and training of U-Nets for nuclear segmentation. Specifically, we consider enforcing rotation equivariance in the network, the placement of residual blocks, and applying novel data augmentation designed specifically for histopathology images, and show the relative improvement and merit of each. Incorporating all of these enhancements in the design and training of a U-Net yields significantly improved segmentation results while still maintaining a speed of inference that is sufficient for real-world applications, in particular, analyzing whole-slide images (WSIs). Code for our enhanced U-Net is available at https://github.com/thatvinhton/G-U-Net."
  },
  "cvpr2019_cvmi_multiscalekernelsforenhancedu-shapednetworktoimprove3dneurontracing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Multiscale Kernels for Enhanced U-Shaped Network to Improve 3D Neuron Tracing",
    "authors": [
      "Heng Wang",
      "Donghao Zhang",
      "Yang Song",
      "Siqi Liu",
      "Heng Huang",
      "Mei Chen",
      "Hanchuan Peng",
      "Weidong Cai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Wang_Multiscale_Kernels_for_Enhanced_U-Shaped_Network_to_Improve_3D_Neuron_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Wang_Multiscale_Kernels_for_Enhanced_U-Shaped_Network_to_Improve_3D_Neuron_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Digital neuron morphology reconstruction from three-dimensional (3D) volumetric optical microscope images is an important procedure to rebuild the connections and structures of neural circuits. Even though many approaches have been proposed to achieve precise tracing, it is still a challenging task especially when images are polluted by noise or have discontinuity in their neuron structures. In this paper, we propose a new framework to overcome these issues by performing neuron segmentation prior to tracing. Our proposed framework adopts a novel 3D U-shaped convolutional neural network (CNN) with multiscale kernel fusion and spatial fusion to perform the image segmentation. We then perform the iterative back-tracking tracing algorithm on the output of the network. Evaluated on the Janelia dataset from the BigNeuron project, our proposed framework achieves competitive tracing performance."
  },
  "cvpr2019_cvmi_cellimagesegmentationusinggenerativeadversarialnetworks,transferlearning,andaugmentations": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Cell Image Segmentation Using Generative Adversarial Networks, Transfer Learning, and Augmentations",
    "authors": [
      "Michael Majurski",
      "Petru Manescu",
      "Sarala Padi",
      "Nicholas Schaub",
      "Nathan Hotaling",
      "Carl Simon Jr",
      "Peter Bajcsy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Majurski_Cell_Image_Segmentation_Using_Generative_Adversarial_Networks_Transfer_Learning_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Majurski_Cell_Image_Segmentation_Using_Generative_Adversarial_Networks_Transfer_Learning_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We address the problem of segmenting cell contours from microscopy images of human induced pluripotent Retinal Pigment Epithelial stem cells (iRPE) using Convolutional Neural Networks (CNN). Our goal is to compare the accuracy gains of CNN-based segmentation by using (1) un-annotated images via Generative Adversarial Networks (GAN), (2) annotated out-of-bio-domain images via transfer learning, and (3) a priori knowledge about microscope imaging mapped into geometric augmentations of a small collection of annotated images. First, the GAN learns an abstract representation of cell objects. Next, this unsupervised learned representation is transferred to the CNN segmentation models which are further fine-tuned on a small number of manually segmented iRPE cell images. Second, transfer learning is applied by pre-training a part of the CNN segmentation model with the COCO dataset containing semantic segmentation labels. The CNN model is then adapted to the iRPE cell domain using a small set of annotated iRPE cell images. Third, augmentations based on geometrical transformations are applied to a small collection of annotated images. All these approaches to training CNN-based segmentation model are compared to a baseline CNN model trained on a small collection of annotated images. For very small annotation counts, the results show accuracy improvements up to 20 % by the best approach in comparison to the accuracy achieved using a baseline U-Net model. For larger annotation counts these approaches asymptotically approach the same accuracy. "
  },
  "cvpr2019_cvmi_partially-independentframeworkforbreastcancerhistopathologicalimageclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Partially-Independent Framework for Breast Cancer Histopathological Image Classification",
    "authors": [
      "Vibha Gupta",
      "Arnav Bhavsar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/Gupta_Partially-Independent_Framework_for_Breast_Cancer_Histopathological_Image_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/Gupta_Partially-Independent_Framework_for_Breast_Cancer_Histopathological_Image_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The automated classification of histopathology images relives pathologists workloadand, hence utilizing the resources to focus more on the most suspicious cases.More recently, inspired by the success of deep learning methods in computer vision application, such frameworks have also been applied in various medical image analysis applications. However, existing approaches showed less interest in exploring multi-layer features for improving the classification. We propose the integration of multi-layer features from a ResNet model for breast cancer histopathology image classification. Specifically, this work focuses on making a framework which considers both independent nature of layers as well as some partial dependency among them.Knowing that, not all the layers learn discriminative features, consideration of layers which learn to negative features will deteriorate the accuracy. Hence, we select the optimal subset of the layers based on an information-theoretic measure (ITS). Various experiments are performed on publicly available BreaKHis dataset, and demonstrate that the proposed multi-layer feature fusion yields better performance than the traditional way of using the highest layer features. This indicates that mid- and low-level features also carry useful discriminative information when explicitly considered. We also demonstrate improved performance, in most cases, over various state-of-the-art methods."
  },
  "cvpr2019_cvmi_identificationoftuberculosisbacilliinzn-stainedsputumsmearimagesadeeplearningapproach": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Identification of Tuberculosis Bacilli in ZN-Stained Sputum Smear Images: A Deep Learning Approach",
    "authors": [
      "Moumen El-Melegy",
      "Doaa Mohamed",
      "Tarek ElMelegy",
      "Mostafa Abdelrahman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CVMI/El-Melegy_Identification_of_Tuberculosis_Bacilli_in_ZN-Stained_Sputum_Smear_Images_A_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CVMI/El-Melegy_Identification_of_Tuberculosis_Bacilli_in_ZN-Stained_Sputum_Smear_Images_A_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Tuberculosis (TB) is a serious infectious disease that remains a global health problem with an enormous burden of disease. TB spreads widely in low and middle income countries, which depend primarily on ZN-stained sputum smear test using conventional light microscopy in disease diagnosis. In this paper we propose a new deep-learning approach for bacilli localization and classification in conventional ZN-stained microscopic images. The new approach is based on the state of the art Faster Region-based Convolutional Neural Network (RCNN) framework, followed by a CNN to reduce false positive rate. This is the first time to apply this framework to this problem. Our experimental results show significant improvement by the proposed approach compared to existing methods, which will help in accurate disease diagnosis. "
  },
  "cvpr2019_uavision_deep-learning-basedaerialimageclassificationforemergencyresponseapplicationsusingunmannedaerialvehicles": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "Deep-Learning-Based Aerial Image Classification for Emergency Response Applications Using Unmanned Aerial Vehicles",
    "authors": [
      "Christos Kyrkou",
      "Theocharis Theocharides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Kyrkou_Deep-Learning-Based_Aerial_Image_Classification_for_Emergency_Response_Applications_Using_Unmanned_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Kyrkou_Deep-Learning-Based_Aerial_Image_Classification_for_Emergency_Response_Applications_Using_Unmanned_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Unmanned Aerial Vehicles (UAVs), equipped with camera sensors can facilitate enhanced situational awareness for many emergency response and disaster management applications since they are capable of operating in remote and difficult to access areas. In addition, by utilizing an embedded platform and deep learning UAVs can autonomously monitor a disaster stricken area, analyze the image in real-time and alert in the presence of various calamities such as collapsed buildings, flood, or fire in order to faster mitigate their effects on the environment and on human population. To this end, this paper focuses on the automated aerial scene classification of disaster events from on-board a UAV. Specifically, a dedicated Aerial Image Database for Emergency Response (AIDER) applications is introduced and a comparative analysis of existing approaches is performed. Through this analysis a lightweight convolutional neural network (CNN) architecture is developed, capable of running efficiently on an embedded platform achieving3x higher performance compared to existing models with minimal memory requirements with less than 2% accuracy drop compared to the state-of-the-art. These preliminary results provide a solid basis for further experimentation towards real-time aerial image classification for emergency response applications using UAVs."
  },
  "cvpr2019_uavision_hadanetsflexiblequantizationstrategiesforneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "HadaNets: Flexible Quantization Strategies for Neural Networks",
    "authors": [
      "Yash Akhauri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Akhauri_HadaNets_Flexible_Quantization_Strategies_for_Neural_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Akhauri_HadaNets_Flexible_Quantization_Strategies_for_Neural_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " On-board processing elements on UAVs are currently inadequate for training and inference of Deep Neural Networks. This is largely due to the energy consumption of memory accesses in such a network. HadaNets introduce a flexible train-from-scratch tensor quantization scheme by pairing a full precision tensor to a binary tensor in the form of a Hadamard product. Unlike wider reduced precision neural network models, we preserve the train-time parameter count, thus out-performing XNOR-Nets without a train-time memory penalty. Such training routines could see great utility in semi-supervised online learning tasks. Our method also offers advantages in model compression, as we reduce the model size of ResNet-18 by 7.43 times with respect to a full precision model without utilizing any other compression techniques. We also demonstrate a 'Hadamard Binary Matrix Multiply' kernel, which delivers a 10-fold increase in performance over full precision matrix multiplication with a similarly optimized kernel."
  },
  "cvpr2019_uavision_real-timedensestereoembeddedinauavforroadinspection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "Real-Time Dense Stereo Embedded in a UAV for Road Inspection",
    "authors": [
      "Rui Fan",
      "Jianhao Jiao",
      "Jie Pan",
      "Huaiyang Huang",
      "Shaojie Shen",
      "Ming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Fan_Real-Time_Dense_Stereo_Embedded_in_a_UAV_for_Road_Inspection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Fan_Real-Time_Dense_Stereo_Embedded_in_a_UAV_for_Road_Inspection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The condition assessment of road surfaces is essential to ensure their serviceability while still providing maximum road traffic safety. This paper presents a robust stereo vision system embedded in an unmanned aerial vehicle (UAV). The perspective view of the target image is first transformed into the reference view, and this not only improves the disparity accuracy, but also reduces the algorithm's computational complexity. The cost volumes generated from stereo matching are then filtered using a bilateral filter. The latter has been proved to be a feasible solution for the functional minimisation problem in a fully connected Markov random field model. Finally, the disparity maps are transformed by minimising an energy function with respect to the roll angle and disparity projection model. This makes the damaged road areas more distinguishable from the road surface. The proposed system is implemented on an NVIDIA Jetson TX2 GPU with CUDA for real-time purposes. It is demonstrated through experiments that the damaged road areas can be easily distinguished from the transformed disparity maps. "
  },
  "cvpr2019_uavision_uav-netafastaerialvehicledetectorformobileplatforms": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "UAV-Net: A Fast Aerial Vehicle Detector for Mobile Platforms",
    "authors": [
      "Tobias Ringwald",
      "Lars Sommer",
      "Arne Schumann",
      "Jurgen Beyerer",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Ringwald_UAV-Net_A_Fast_Aerial_Vehicle_Detector_for_Mobile_Platforms_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Ringwald_UAV-Net_A_Fast_Aerial_Vehicle_Detector_for_Mobile_Platforms_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle detection in aerial imagery is a challenging task due to small object sizes, high object density and partial occlusions. While past research mostly focused on improving detection accuracy, inference speed is another important factor when using CNN object detectors in a real life scenario - especially when targeting mobile platforms like unmanned aerial vehicles (UAVs). In this work, we compare several established detection frameworks in terms of their accuracy-speed trade-off and show that the Single Shot MultiBox Detector (SSD) offers the best compromise. We subsequently undertake a thorough evaluation of several design choices to further increase detection speed while sacrificing little to no accuracy. This includes the choice of base network architecture, improved prediction layers and an automatic model pruning approach. Given our evaluation results, we finally construct UAV-Net - a novel aerial vehicle detector that has a model size of less than 0.4 MiB and is more than 16 times faster than current top performing approaches. UAV-Net is well suited for on-board processing and operates in real time on a Jetson TX2 platform. Nevertheless, its accuracy is on par with state-of-the-art approaches on the DLR 3K, VEDAI and UAVDT datasets. Code and models are available on the project website."
  },
  "cvpr2019_uavision_mid-airamulti-modaldatasetforextremelylowaltitudedroneflights": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "Mid-Air: A Multi-Modal Dataset for Extremely Low Altitude Drone Flights",
    "authors": [
      "Michael Fonder",
      "Marc Van Droogenbroeck"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Fonder_Mid-Air_A_Multi-Modal_Dataset_for_Extremely_Low_Altitude_Drone_Flights_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Fonder_Mid-Air_A_Multi-Modal_Dataset_for_Extremely_Low_Altitude_Drone_Flights_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Flying a drone in unstructured environments with varying conditions is challenging. To help producing better algorithms, we present Mid-Air, a multi-purpose synthetic dataset for low altitude drone flights in unstructured environments. It contains synchronized data of multiple sensors for a total of 54 trajectories and more than 420k video frames simulated in various climate conditions. In this work, we motivate design choices, explain how the data was simulated, and present the content of the dataset. Finally, a benchmark for positioning and a benchmark for image generation tasks show how Mid-Air can be used to set up a standard evaluation method for assessing computer vision algorithms in terms of robustness and generalization. We illustrate this by providing a baseline for depth estimation and by comparing it with results obtained on an existing dataset. The Mid-Air dataset is publicly downloadable, with additional details on the data format and organization, at http://midair.ulg.ac.be."
  },
  "cvpr2019_uavision_ahybridmethodfortrackingofobjectsbyuavs": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "A Hybrid Method for Tracking of Objects by UAVs",
    "authors": [
      "Hasan Saribas",
      "Bedirhan Uzun",
      "Burak Benligiray",
      "Onur Eker",
      "Hakan Cevikalp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Saribas_A_Hybrid_Method_for_Tracking_of_Objects_by_UAVs_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Saribas_A_Hybrid_Method_for_Tracking_of_Objects_by_UAVs_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Object tracking remains one of the fundamental problems of computer vision since it becomes difficult under some realistic conditions such as fast camera movement, occlusion and similar of objects to the tracked targets. As a real-world application, tracking objects using cameras mounted on unmanned aerial vehicles (UAVs) has become very popular. With the increasing availability of small single board computers with high parallel processing power capabilities, tracking of objects by using onboard computers within UAVs in real-time has become feasible. Although these onboard computers allow a wide variety of computer vision methods to be executed on a UAV, there is still a need to optimize these methods for running time and power consumption. In this paper, we propose a hybrid method for a UAV to detect and track other UAVs efficiently. To detect the target UAV at the beginning of the video and in the case where the tracked UAV has been lost, we use the deep learning-based YOLOv3 and YOLOv3-Tiny models, which provide one of the best trade-offs between speed and accuracy in the literature. To track the detected UAVs in real-time, a kernelized correlation filter is used. Combining these two methods provides high accuracy and speed even on onboard computers. To train the neural nets and test our method, we have collected a new dataset composed of videos of various UAVs in flight, captured from another UAV. The performance of the proposed method has been compared with other state-of-the-art methods in the literature on this dataset. Additionally, we also tested the proposed trackers on aerial videos captured from UAVs.Experimental results show that the proposed hybrid trackers achieve the state-of-the-art performance on all tested datasets. The code is available at https://github.com/bdrhn9/hybrid-tracker."
  },
  "cvpr2019_uavision_learningacontrollerfusionnetworkbyonlinetrajectoryfilteringforvision-baseduavracing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "Learning a Controller Fusion Network by Online Trajectory Filtering for Vision-Based UAV Racing",
    "authors": [
      "Matthias Muller",
      "Guohao Li",
      "Vincent Casser",
      "Neil Smith",
      "Dominik L. Michels",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Muller_Learning_a_Controller_Fusion_Network_by_Online_Trajectory_Filtering_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Muller_Learning_a_Controller_Fusion_Network_by_Online_Trajectory_Filtering_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Autonomous UAV racing has recently emerged as an interesting research problem. The dream is to beat humans in this new fast-paced sport. A common approach is to learn an end-to-end policy that directly predicts controls from raw images by imitating an expert. However, such a policy is limited by the expert it imitates and scaling to other environments and vehicle dynamics is difficult. One approach to overcome the drawbacks of an end-to-end policy is to train a network only on the perception task and handle control with a PID or MPC controller. However, a single controller must be extensively tuned and cannot usually cover the whole state space. In this paper, we propose learning an optimized controller using a DNN that fuses multiple controllers. The network learns a robust controller with online trajectory filtering, which suppresses noisy trajectories and imperfections of individual controllers. The result is a network that is able to learn a good fusion of filtered trajectories from different controllers leading to significant improvements in overall performance. We compare our trained network to controllers it has learned from, end-to-end baselines and human pilots in a realistic simulation; our network beats all baselines in extensive experiments and approaches the performance of a professional human pilot."
  },
  "cvpr2019_uavision_thepoweroftilingforsmallobjectdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UAVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UAVision",
    "title": "The Power of Tiling for Small Object Detection",
    "authors": [
      "F. Ozge Unel",
      "Burak O. Ozkalayci",
      "Cevahir Cigla"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UAVision/Unel_The_Power_of_Tiling_for_Small_Object_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UAVision/Unel_The_Power_of_Tiling_for_Small_Object_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep neural network based techniques are state-of-the-art for object detection and classification with the help of the development in computational power and memory efficiency. Although these networks are adapted for mobile platforms with sacrifice in accuracy; the resolution increase in visual sources makes the problem even harder by raising the expectations to leverage all the details in images. Real-time small object detection in low power mobile devices has been one of the fundamental problems of surveillance applications. In this study, we address the detection of pedestrians and vehicles onboard a micro aerial vehicle (MAV) with high-resolution imagery. For this purpose, we exploit PeleeNet, to our best knowledge the most efficient network model on mobile GPUs, as the backbone of an SSD network as well as 38x38 feature map in the earlier layer. After illustrating the low accuracy of state-of-the-art object detectors under the MAV scenario, we introduce a tiling based approach that is applied in both training and inference phases. The proposed technique limits the detail loss in object detection while feeding the network with a fixed size input. The improvements provided by the proposed approach are shown by in-depth experiments performed along Nvidia Jetson TX1 and TX2 using the VisDrone2018 dataset."
  },
  "cvpr2019_eventvision_ev-segnetsemanticsegmentationforevent-basedcameras": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "EV-SegNet: Semantic Segmentation for Event-Based Cameras",
    "authors": [
      "Inigo Alonso",
      "Ana C. Murillo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Alonso_EV-SegNet_Semantic_Segmentation_for_Event-Based_Cameras_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Alonso_EV-SegNet_Semantic_Segmentation_for_Event-Based_Cameras_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Event cameras, as Dynamic Vision Sensor (DVS), are very promising sensors which have shown several advantages over frame-based cameras. However, most recent works on real applications of these cameras are focused on 3D reconstruction and 6-DOF camera tracking. Deep learning based approaches, which are leading the state-of-the-art in visual recognition tasks, could potentially take advantage of the benefits of DVS, but some adaptations are needed still needed in order to effectively work on these cameras.This work introduces the first baseline for semantic segmentation with this kind of data. We build a semantic segmentation CNN based on state-of-the-art techniques which takes event information as the only input. Besides, we propose a novel representation for DVS data that outperforms previously used event representations for related tasks. Since there is no existing labeled dataset for this task, we propose how to automatically generate approximated semantic segmentation labels for some sequences of the DDD17 dataset, which we publish together with the model, and demonstrate they are valid to train a model for DVS data only. We compare our results on semantic segmentation from DVS data with results using corresponding grayscale images, demonstrating how they are complementary and worth combining."
  },
  "cvpr2019_eventvision_learningevent-basedheightfromplaneandparallax": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Learning Event-Based Height From Plane and Parallax",
    "authors": [
      "Kenneth Chaney",
      "Alex Zihao Zhu",
      "Kostas Daniilidis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Chaney_Learning_Event-Based_Height_From_Plane_and_Parallax_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Chaney_Learning_Event-Based_Height_From_Plane_and_Parallax_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Event-based cameras are a novel asynchronous sensing modality that provides exciting benefits, such as the ability to track fast moving objects with no motion blur and low latency, high dynamic range, and low power consumption. Given the low latency of the cameras, as well as their ability to work in challenging lighting conditions, these cameras are a natural fit for reactive problems such as fast local structure estimation. In this work, we propose a fast method to perform structure estimation for vehicles traveling in a roughly 2D environment (e.g. in an environment with a ground plane). Our method transfers the method of plane and parallax to events, which, given the homography to a ground plane and the pose of the camera, generates a warping of the events which removes the optical flow for events on the ground plane, while inducing flow for events above the ground plane. We then estimate dense flow in this warped space using a self-supervised neural network, which provides the height of all points in the scene. We evaluate our method on the Multi Vehicle Stereo Event Camera dataset, and show its ability to rapidly estimate the scene structure both at high speeds and in low lighting conditions."
  },
  "cvpr2019_eventvision_real-time6dofposerelocalizationforeventcameraswithstackedspatiallstmnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Real-Time 6DOF Pose Relocalization for Event Cameras With Stacked Spatial LSTM Networks",
    "authors": [
      "Anh Nguyen",
      "Thanh-Toan Do",
      "Darwin G. Caldwell",
      "Nikos G. Tsagarakis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Nguyen_Real-Time_6DOF_Pose_Relocalization_for_Event_Cameras_With_Stacked_Spatial_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Nguyen_Real-Time_6DOF_Pose_Relocalization_for_Event_Cameras_With_Stacked_Spatial_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a new method to relocalize the 6DOF pose of an event camera solely based on the event stream. Our method first creates the event image from a list of events that occurs in a very short time interval, then a Stacked Spatial LSTM Network (SP-LSTM) is used to learn the camera pose. Our SP-LSTM is composed of a CNN to learn deep features from the event images and a stack of LSTM to learn spatial dependencies in the image feature space. We show that the spatial dependency plays an important role in the relocalization task with event images and the SP-LSTM can effectively learn this information. The extensively experimental results on a publicly available dataset show that our approach outperforms recent state-of-the-art methods by a substantial margin, as well as generalizes well in challenging training/testing splits. The source code and trained models are available at https://github.com/nqanh/pose_relocalization."
  },
  "cvpr2019_eventvision_startrackingusinganeventcamera": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Star Tracking Using an Event Camera",
    "authors": [
      "Tat-Jun Chin",
      "Samya Bagchi",
      "Anders Eriksson",
      "Andre van Schaik"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Chin_Star_Tracking_Using_an_Event_Camera_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Chin_Star_Tracking_Using_an_Event_Camera_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Star trackers are primarily optical devices that are used to estimate the attitude of a spacecraft by recognising and tracking star patterns. Currently, most star trackers use conventional optical sensors. In this application paper, we propose the usage of event sensors for star tracking. There are potentially two benefits of using event sensors for star tracking: lower power consumption and higher operating speeds. Our main contribution is to formulate an algorithmic pipeline for star tracking from event data that includes novel formulations of rotation averaging and bundle adjustment. In addition, we also release with this paper a dataset for star tracking using event cameras. With this work, we introduce the problem of star tracking using event cameras to the computer vision community, whose expertise in SLAM and geometric optimisation can be brought to bear on this commercially important application."
  },
  "cvpr2019_eventvision_asynchronousconvolutionalnetworksforobjectdetectioninneuromorphiccameras": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Asynchronous Convolutional Networks for Object Detection in Neuromorphic Cameras",
    "authors": [
      "Marco Cannici",
      "Marco Ciccone",
      "Andrea Romanoni",
      "Matteo Matteucci"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Cannici_Asynchronous_Convolutional_Networks_for_Object_Detection_in_Neuromorphic_Cameras_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Cannici_Asynchronous_Convolutional_Networks_for_Object_Detection_in_Neuromorphic_Cameras_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Event-based cameras, also known as neuromorphic cameras, are bioinspired sensors able to perceive changes in the scene at high frequency with low power consumption. Becoming available only very recently, a limited amount of work addresses object detection on these devices. In this paper we propose two neural networks architectures for object detection: YOLE, which integrates the events into surfaces and uses a frame-based model to process them, and fcYOLE, an asynchronous event-based fully convolutional network which uses a novel and general formalization of the convolutional and max pooling layers to exploit the sparsity of camera events. We evaluate the algorithm with different extensions of publicly available datasets, and on a novel synthetic dataset."
  },
  "cvpr2019_eventvision_detahigh-resolutiondvsdatasetforlaneextraction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "DET: A High-Resolution DVS Dataset for Lane Extraction",
    "authors": [
      "Wensheng Cheng",
      "Hao Luo",
      "Wen Yang",
      "Lei Yu",
      "Shoushun Chen",
      "Wei Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Cheng_DET_A_High-Resolution_DVS_Dataset_for_Lane_Extraction_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Cheng_DET_A_High-Resolution_DVS_Dataset_for_Lane_Extraction_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Lane extraction is a basic yet necessary task for autonomous driving. Although past years have witnessed major advances in lane extraction with deep learning models, they all aim at ordinary RGB images generated by frame-based cameras, which limits their performance in nature. To tackle this problem, we introduce Dynamic Vision Sensor (DVS), a type of event-based sensor to lane extraction task and build a high-resolution DVS dataset for lane extraction (DET). We collect the raw event data and generate 5,424 event-based sensor images with a resolution of 1280x800, the highest one among all DVS datasets available now. These images include complex traffic scenes and various lane types. All images of DET are annotated with multi-class segmentation format. The fully annotated DET images contains 17,103 lane instances, each of which is labeled pixel by pixel manually. We evaluate state-of-the-art lane extraction models on DET to build a benchmark for lane extraction task with event-based sensor images. Experimental results demonstrate that DET is quite challenging for even state-of-the-art lane extraction methods. DET is made publicly available, including the raw event data, accumulated images and labels."
  },
  "cvpr2019_eventvision_livedemonstrationjointestimationofopticalflowandintensityimagefromeventsensors": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Live Demonstration: Joint Estimation of Optical Flow and Intensity Image From Event Sensors",
    "authors": [
      "Prasan Shedligeri",
      "Kaushik Mitra"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Shedligeri_Live_Demonstration_Joint_Estimation_of_Optical_Flow_and_Intensity_Image_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Shedligeri_Live_Demonstration_Joint_Estimation_of_Optical_Flow_and_Intensity_Image_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Event sensors provide asynchronous, high-temporal rate information about the pixel-level brightness changes in the scene. Temporal information is lost when these event sensor data is converted into an event frame. This temporal information can be recovered if these event frames are processed as a sequence. We propose a deep learning based method to reconstruct high-quality, high-dynamic range, high frame rate and temporally consistent pseudo-images which can run in real-time. Our proposed method can reconstruct pseudo-images at high temporal resolution, even though it is supervised using intensity images from a low-frame rate sensor. We propose convolutional-LSTM based seq2seq deep learning model which takes in as input a sequence of event frames and reconstructs a sequence of pseudo-images. To further enhance the quality of our reconstructed pseudo-images, we propose a model to jointly learn to reconstruct pseudo-images and optical flow. We propose a novel brightness agnostic loss function to supervise the training of pseudo-images. The model learns to estimate optical flow using a self-supervised learning method. We show that our model can produce updates at upto 150 Hz on a GPU while out-performing previous state-of-the-art methods in reconstruction quality. We quantitatively show that our joint learning model to estimate optical flow performs comparably with previous state-of-the-art methods which are tuned only to estimate optical flow. We show that joint estimation of optical flow and pseudo-images leads to better reconstruction quality of the pseudo-images."
  },
  "cvpr2019_eventvision_livedemonstrationareal-timeevent-basedfastcornerdetectiondemobasedonfpga": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Live Demonstration: A Real-Time Event-Based Fast Corner Detection Demo Based on FPGA",
    "authors": [
      "Min Liu",
      "Wei-Tse Kao",
      "Tobi Delbruck"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Liu_Live_Demonstration_A_Real-Time_Event-Based_Fast_Corner_Detection_Demo_Based_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Liu_Live_Demonstration_A_Real-Time_Event-Based_Fast_Corner_Detection_Demo_Based_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Corner detection is widely used as a pre-processing step for many computer vision (CV) problems. It is well studied in the conventional CV community and many popular methods are still used nowadays such as Harris, FAST and SIFT. For event cameras like Dynamic Vision Sensors (DVS), similar approaches also have been proposed in recent years. Two of them are event-based harris(eHARRIS) and event-based FAST (eFAST).This demo presents our recent work in which we implement eFAST on MiniZed FPGA. The power consumption of the whole system is less than 4W and the hardware eFAST consumes about 0.9W. This demo processes at least 5M events per second, and achieves a power-speed improvement factor product of more than 30X compared with CPU implementation of eFAST. This embedded component could be suitable for integration to applications such as drones and autonomous cars that produce high event rates."
  },
  "cvpr2019_eventvision_livedemonstrationfacerecognitiononanultra-lowpowerevent-drivenconvolutionalneuralnetworkasic": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Live Demonstration: Face Recognition on an Ultra-Low Power Event-Driven Convolutional Neural Network ASIC",
    "authors": [
      "Qian Liu",
      "Ole Richter",
      "Carsten Nielsen",
      "Sadique Sheik",
      "Giacomo Indiveri",
      "Ning Qiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Liu_Live_Demonstration_Face_Recognition_on_an_Ultra-Low_Power_Event-Driven_Convolutional_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Liu_Live_Demonstration_Face_Recognition_on_an_Ultra-Low_Power_Event-Driven_Convolutional_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The paper demonstrates an event-driven deep learning (DL) hardware software ecosystem. The user-friendly software tools port models from Keras (popular machine learning libraries), automaticly convert of DL models to Spiking equivalents, i.e. Spiking Neural Networks (SNNs) run spiking simulations of the converted models on the hardware emulator for testing and prototyping. More importantly, the software ports the converted models onto a novel, ultra-low power, real-time, event-driven ASIC SCNN Chip: DynapCNN. An interactive demonstration of a real-time face recognition system built using the above pipeline is shown as an example."
  },
  "cvpr2019_eventvision_livedemonstrationcelex-va1mpixelmulti-modeevent-basedsensor": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Live Demonstration: CeleX-V: A 1M Pixel Multi-Mode Event-Based Sensor",
    "authors": [
      "Shoushun Chen",
      "Menghan Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Chen_Live_Demonstration_CeleX-V_A_1M_Pixel_Multi-Mode_Event-Based_Sensor_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Chen_Live_Demonstration_CeleX-V_A_1M_Pixel_Multi-Mode_Event-Based_Sensor_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We demonstrate a new generation smart image sensor, CeleX-V. With 1280x800 pixels, 9.8um pitch, the sensor integrates several vision functions into one chip, such as full-array-parallel motion detection and on-chip optical flow extraction. CeleX-V is also capable of producing high-quality full-frame pictures and thus is compatible with traditional picture-based algorithms. The sensor supports both MIPI and parallel interface, with typical 400mW power consumption."
  },
  "cvpr2019_eventvision_cedcoloreventcameradataset": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "CED: Color Event Camera Dataset",
    "authors": [
      "Cedric Scheerlinck",
      "Henri Rebecq",
      "Timo Stoffregen",
      "Nick Barnes",
      "Robert Mahony",
      "Davide Scaramuzza"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Scheerlinck_CED_Color_Event_Camera_Dataset_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Scheerlinck_CED_Color_Event_Camera_Dataset_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Event cameras are novel, bio-inspired visual sensors, whose pixels output asynchronous and independent timestamped spikes at local intensity changes, called 'events'. Event cameras offer advantages over conventional frame-based cameras in terms of latency, high dynamic range (HDR) and temporal resolution. Until recently, event cameras have been limited to outputting events in the intensity channel, however, recent advances have resulted in the development of color event cameras, such as the Color-DAVIS346. In this work, we present and release the first Color Event Camera Dataset (CED), containing 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes, which we hope will help drive forward event-based vision research. We also present an extension of the event camera simulator ESIM that enables simulation of color events. Finally, we present an evaluation of three state-of-the-art image reconstruction methods that can be used to convert the Color-DAVIS346 into a continuous-time, HDR, color video camera to visualise the event stream, and for use in downstream vision applications."
  },
  "cvpr2019_eventvision_livedemonstrationunsupervisedevent-basedlearningofopticalflow,depthandegomotion": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Live Demonstration: Unsupervised Event-Based Learning of Optical Flow, Depth and Egomotion",
    "authors": [
      "Alex Zihao Zhu",
      "Liangzhe Yuan",
      "Kenneth Chaney",
      "Kostas Daniilidis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Zhu_Live_Demonstration_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Zhu_Live_Demonstration_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a demo of our work, Unsupervised Event-based Learning of Optical Flow, Depth and Egomotion, which will also appear at CVPR 2019. Our demo consists of a CNN which takes as input events from a DAVIS-346b event camera, represented as a discretized event volume, and predicts optical flow for each pixel in the image. Due to the generalization abilities of our network, we are able to predict accurate optical flow for a very wide range of scenes, including for very fast motions and challenging lighting. "
  },
  "cvpr2019_eventvision_dhp19dynamicvisionsensor3dhumanposedataset": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "DHP19: Dynamic Vision Sensor 3D Human Pose Dataset",
    "authors": [
      "Enrico Calabrese",
      "Gemma Taverni",
      "Christopher Awai Easthope",
      "Sophie Skriabine",
      "Federico Corradi",
      "Luca Longinotti",
      "Kynan Eng",
      "Tobi Delbruck"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Calabrese_DHP19_Dynamic_Vision_Sensor_3D_Human_Pose_Dataset_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Calabrese_DHP19_Dynamic_Vision_Sensor_3D_Human_Pose_Dataset_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Human pose estimation has dramatically improved thanks to the continuous developments in deep learning. However, marker-free human pose estimation based on standard frame-based cameras is still slow and power hungry for real-time feedback interaction because of the huge number of operations necessary for large Convolutional Neural Network (CNN) inference. Event-based cameras such as the Dynamic Vision Sensor (DVS) quickly output sparse moving-edge information. Their sparse and rapid output is ideal for driving low-latency CNNs, thus potentially allowing real-time interaction for human pose estimators. Although the application of CNNs to standard frame-based cameras for human pose estimation is well established, their application to event-based cameras is still under study.This paper proposes a novel benchmark dataset of human body movements, the Dynamic Vision Sensor Human Pose dataset (DHP19). It consists of recordings from 4 synchronized 346x260 pixel DVS cameras, for a set of 33 movements with 17 subjects. DHP19 also includes a 3D pose estimation model that achieves an average 3D pose estimation error of about 8 cm, despite the sparse and reduced input data from the DVS."
  },
  "cvpr2019_eventvision_livedemonstrationdigitrecognitiononpixelprocessorarrays": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Live Demonstration: Digit Recognition on Pixel Processor Arrays",
    "authors": [
      "Laurie Bose",
      "Jianing Chen",
      "Stephen J. Carey",
      "Piotr Dudek",
      "Walterio Mayol-Cuevas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Bose_Live_Demonstration_Digit_Recognition_on_Pixel_Processor_Arrays_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Bose_Live_Demonstration_Digit_Recognition_on_Pixel_Processor_Arrays_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this demo, we will showcase recent work on implementing convolutional neural networks directly on pixel processor arrays (PPA). As CNNs demonstrate enhanced performance across tasks from classification to image synthesis, it becomes essential to find the most adequate ways to realize them especially for embedded, real-time and reactive tasks in areas across Computer Vision and Robotics. The PPA concept is one architecture that pairs sensing and massively parallel processing at the focal plane level and allow mid to high level tasks to be run wholly embedded within them. They allow operation at high framerates and low energy consumption (<= 2W), and without the need for external signal interpretation or processing. In this demo we will showcase our recent work on the implementation of CNNs on the SCAMP5 architecture as a step towards true end-to-end operation on flexibly programmable PPA hardware. In particular, we will showcase live how our modifications to CNNs allow them to run tasks such as handwritten number classification from image capture to classification wholly embedded on the PPA."
  },
  "cvpr2019_eventvision_livedemonstrationreal-timevi-slamwithhigh-resolutioneventcamera": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Live Demonstration: Real-Time Vi-SLAM With High-Resolution Event Camera",
    "authors": [
      "Gongyu Yang",
      "Qilin Ye",
      "Wanjun He",
      "Lifeng Zhou",
      "Xinyu Chen",
      "Lei Yu",
      "Wen Yang",
      "Shoushun Chen",
      "Wei Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Yang_Live_Demonstration_Real-Time_Vi-SLAM_With_High-Resolution_Event_Camera_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Yang_Live_Demonstration_Real-Time_Vi-SLAM_With_High-Resolution_Event_Camera_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": "Event camera is bio-inspired vision sensors that output pixel-level brightness changes asynchronously. Compared to the conventional frame-based camera, it is with high dynamic range, low latency and high sensitivity, and thus can be exploited in SLAM to tackle the problem of occasions with high-speed camera moving and low-light scenes. In this demo, we implement the visual-inerial SLAM in real time with the recently released event camera, namely, CeleX-V. With the feature of high spatial resolution (1280x800) and low latency <0.5us, the proposed method can provide frames with abundant textures and high time response, which leads to more stable tracking ability and better performance in SLAM system."
  },
  "cvpr2019_eventvision_event-basedattentionandtrackingonneuromorphichardware": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - EventVision",
    "title": "Event-Based Attention and Tracking on Neuromorphic Hardware",
    "authors": [
      "Alpha Renner",
      "Matthew Evanusa",
      "Yulia Sandamirskaya"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/EventVision/Renner_Event-Based_Attention_and_Tracking_on_Neuromorphic_Hardware_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/EventVision/Renner_Event-Based_Attention_and_Tracking_on_Neuromorphic_Hardware_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a fully event-driven vision and processing system for selective attention and tracking, realized on a neuromorphic processor Loihiinterfaced to an event-based Dynamic Vision Sensor DAVIS. The attention mechanism is realized as a recurrent spiking neural network that implements attractor-dynamics of dynamic neural fields. We demonstrate capability of the system to create sustained activation that supports object tracking when distractors are present or when the object slows down or stops, reducing the number of generated events."
  },
  "cvpr2019_cfs_high-levelfeaturesformultimodaldeceptiondetectioninvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CFS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - CFS",
    "title": "High-Level Features for Multimodal Deception Detection in Videos",
    "authors": [
      "Rodrigo Rill-Garcia",
      "Hugo Jair Escalante",
      "Luis Villasenor-Pineda",
      "Veronica Reyes-Meza"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CFS/Rill-Garcia_High-Level_Features_for_Multimodal_Deception_Detection_in_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CFS/Rill-Garcia_High-Level_Features_for_Multimodal_Deception_Detection_in_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deception (the action of deliberately cause someone to believe something that is not true) can have many different repercussions in daily life. However, deception detection is an inherently complex task for humans. Due to this, not only there is uncertainty on which features should be used as cues for automatic deception detection, but labeled data is scarce. In this paper, we explore typical features that can be extracted from videos for affective computing and study their performance for deception detection in videos. Additionally, we perform a study of different multimodal fusion methods meant to improve the results obtained by using the different sets of extracted features separately, including a novel set of methods based on boosting. For this study, high level features are extracted with open automatic tools for the visual, acoustical and textual modalities, respectively. Experiments are conducted using a real-life trial dataset for deception detection, as well as a novel Mexican deception detection dataset using Spanish as the spoken language."
  },
  "cvpr2019_cfs_feathernetsconvolutionalneuralnetworksaslightasfeatherforfaceanti-spoofing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CFS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - CFS",
    "title": "FeatherNets: Convolutional Neural Networks as Light as Feather for Face Anti-Spoofing",
    "authors": [
      "Peng Zhang",
      "Fuhao Zou",
      "Zhiwen Wu",
      "Nengli Dai",
      "Skarpness Mark",
      "Michael Fu",
      "Juan Zhao",
      "Kai Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CFS/Zhang_FeatherNets_Convolutional_Neural_Networks_as_Light_as_Feather_for_Face_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CFS/Zhang_FeatherNets_Convolutional_Neural_Networks_as_Light_as_Feather_for_Face_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Face Anti-spoofing gains increased attentions recently in both academic and industrial fields. With the emergence of various CNN based solutions, the multi-modal(RGB, depth and IR) methods based CNN showed better performance than single modal classifiers. However, there is a need for improving the performance and reducing the complexity. Therefore, an extreme light network architecture(FeatherNet A/B) is proposed with a streaming module which fixes the weakness of Global Average Pooling and uses less parameters. Our single FeatherNet trained by depth image only, provides a higher baseline with 0.00168 ACER, 0.35M parameters and 83M FLOPS. Furthermore, a novel fusion procedure with \"ensemble + cascade\" structure is presented to satisfy the performance preferred use cases. Meanwhile, the MMFD dataset is collected to provide more attacks and diversity to gain better generalization. We use the fusion method in the Face Anti-spoofing Attack Detection Challenge@CVPR2019 and got the result of 0.0013(ACER), 0.999(TPR@FPR=10e-2), 0.998(TPR@FPR=10e-3) and 0.9814(TPR@FPR=10e-4)."
  },
  "cvpr2019_cfs_multi-modalfacepresentationattackdetectionviaspatialandchannelattentions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CFS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - CFS",
    "title": "Multi-Modal Face Presentation Attack Detection via Spatial and Channel Attentions",
    "authors": [
      "Guoqing Wang",
      "Chuanxin Lan",
      "Hu Han",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CFS/Wang_Multi-Modal_Face_Presentation_Attack_Detection_via_Spatial_and_Channel_Attentions_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CFS/Wang_Multi-Modal_Face_Presentation_Attack_Detection_via_Spatial_and_Channel_Attentions_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Face presentation attack detection (PAD) has drawn increasing attentions to secure face recognition (FR) systems which are being widely used in many applications from access control to smartphone unlock. Traditional approaches for PADmay lack good generalization capability into new application scenarios due to the limited number of subjects and data modality.In this work, we propose an end-to-end multi-modal fusion approach via spatial and channel attention to improve PAD performance on CASIA-SURF. Specifically, We first build four branches integrated with spatial and channel attention module to obtain the uniform features of different modalities, i.e., RGB, Depth, IR and the fused modality with 9 channels which concatenating three modalities. Subsequently, the features extracted from the four branches are concatenated and fed into the shared layers to learn more discriminative features from the fusion perspective.Finally, we get the classification confidence scores w.r.t. PAD or not. The entire network is optimized with the joint of the center loss and softmax loss and SGRD solver to update the parameters. The proposed approach shows promising results on the CASIA-SURF dataset."
  },
  "cvpr2019_cfs_deepanomalydetectionforgeneralizedfaceanti-spoofing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CFS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - CFS",
    "title": "Deep Anomaly Detection for Generalized Face Anti-Spoofing",
    "authors": [
      "Daniel Perez-Cabo",
      "David Jimenez-Cabello",
      "Artur Costa-Pazo",
      "Roberto J. Lopez-Sastre"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CFS/Perez-Cabo_Deep_Anomaly_Detection_for_Generalized_Face_Anti-Spoofing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CFS/Perez-Cabo_Deep_Anomaly_Detection_for_Generalized_Face_Anti-Spoofing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Face recognition has achieved unprecedented results, surpassing human capabilities in certain scenarios. However, these automatic solutions are not ready for production because they can be easily fooled by simple identity impersonation attacks. And although much effort has been devoted to develop face anti-spoofing models, their generalization capacity still remains a challenge in real scenarios. In this paper, we introduce a novel approach that reformulates the Generalized Presentation Attack Detection (GPAD) problem from an anomaly detection perspective. Technically, a deep metric learning model is proposed, where a triplet focal loss is used as a regularization for a novel loss coined \"metric-softmax\", which is in charge of guiding the learning process towards more discriminative feature representations in an embedding space. Finally, we demonstrate the benefits of our deep anomaly detection architecture, by introducing a few-shot a posteriori probability estimation that does not need any classifier to be trained on the learned features.We conduct extensive experiments using the GRAD-GPAD framework that provides the largest aggregated dataset for face GPAD. Results confirm that our approach is able to outperform all the state-of-the-art methods by a considerable margin."
  },
  "cvpr2019_cfs_multi-modalfaceanti-spoofingattackdetectionchallengeatcvpr2019": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CFS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - CFS",
    "title": "Multi-Modal Face Anti-Spoofing Attack Detection Challenge at CVPR2019",
    "authors": [
      "Ajian Liu",
      "Jun Wan",
      "Sergio Escalera",
      "Hugo Jair Escalante",
      "Zichang Tan",
      "Qi Yuan",
      "Kai Wang",
      "Chi Lin",
      "Guodong Guo",
      "Isabelle Guyon",
      "Stan Z. Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CFS/Liu_Multi-Modal_Face_Anti-Spoofing_Attack_Detection_Challenge_at_CVPR2019_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CFS/Liu_Multi-Modal_Face_Anti-Spoofing_Attack_Detection_Challenge_at_CVPR2019_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Anti-spoofing attack detection is critical to guarantee the security of face-based authentication and facial analysis systems. Recently, a multi-modal face anti-spoofing dataset, CASIA-SURF, has been released with the goal of boosting research in this important topic. CASIA-SURF is the largest public data set for facial anti-spoofing attack detection in terms of both, diversity and modalities: it comprises 1,000 subjects and 21,000 video samples. We organized a challenge around this novel resource to boost research in the subject. The Chalearn LAP multi-modal face anti-spoofing attack detection challenge attracted more than 300 teams for the development phase with a total of 13 teams qualifying for the final round. This paper presents an overview of the challenge, including its design, evaluation protocol and a summary of results. We analyze the top ranked solutions and draw conclusions derived from the competition. In addition we outline future work directions."
  },
  "cvpr2019_cfs_facebagnetbag-of-local-featuresmodelformulti-modalfaceanti-spoofing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CFS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - CFS",
    "title": "FaceBagNet: Bag-Of-Local-Features Model for Multi-Modal Face Anti-Spoofing",
    "authors": [
      "Tao Shen",
      "Yuyu Huang",
      "Zhijun Tong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CFS/Shen_FaceBagNet_Bag-Of-Local-Features_Model_for_Multi-Modal_Face_Anti-Spoofing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CFS/Shen_FaceBagNet_Bag-Of-Local-Features_Model_for_Multi-Modal_Face_Anti-Spoofing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Face anti-spoofing detection is a crucial procedure in biometric face recognition systems. State-of-the-art approaches, based on Convolutional Neural Networks (CNNs), present good results in this field. However, previous works focus on one single modal data with limited number of subjects. The recently published CASIA-SURF dataset is the largest dataset that consists of 1000 subjects and 21000 video clips with 3 modalities (RGB, Depth and IR). In this paper, we propose a multi-stream CNN architecture called FaceBagNet to make full use of this data. The input of FaceBagNet is patch-level images which contributes to extract spoof-specific discriminative information. In addition, in order to prevent overfitting and for better learning the fusion features, we design a Modal Feature Erasing (MFE) operation on the multi-modal features which erases features from one randomly selected modality during training. As the result, our approach wins the second place in CVPR 2019 ChaLearn Face Anti-spoofing attack detection challenge. Our final submission gets the score of 99.8052% (TPR@FPR = 10e-4) on the test set."
  },
  "cvpr2019_cfs_recognizingmulti-modalfacespoofingwithfacerecognitionnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CFS",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - CFS",
    "title": "Recognizing Multi-Modal Face Spoofing With Face Recognition Networks",
    "authors": [
      "Aleksandr Parkin",
      "Oleg Grinchuk"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CFS/Parkin_Recognizing_Multi-Modal_Face_Spoofing_With_Face_Recognition_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CFS/Parkin_Recognizing_Multi-Modal_Face_Spoofing_With_Face_Recognition_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Detecting spoofing attacks plays a vital role for deploying automatic face recognition for biometric authentication in applications such as access control, face payment, device unlock, etc. In this paper we propose a new anti-spoofing network architecture that takes advantage of multi-modal image data and aggregates intra-channel features at multiple network layers. We also transfer strong facial features learned for face recognition and show their benefits for detecting spoofing attacks. Finally, to increase the generalization ability of our method to unseen attacks, we use an ensemble of models trained separately for distinct types of spoofing attacks. The proposed method achieves state-of-the-art result on the largest multi-modal anti-spoofing dataset CASIA-SURF."
  },
  "cvpr2019_deep_vision_workshop_convolutionalneuralnetworksonrandomizeddata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Convolutional Neural Networks on Randomized Data",
    "authors": [
      "Cristian Ivan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Ivan_Convolutional_Neural_Networks_on_Randomized_Data_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Ivan_Convolutional_Neural_Networks_on_Randomized_Data_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional Neural Networks (CNNs) are build specifically for computer vision tasks for which it is known that the input data is a hierarchical structure based on locally correlated elements. The question that naturally arises is what happens with the performance of CNNs if one of the basic properties of the data is removed, e.g. what hap- pens if the image pixels are randomly permuted? Intuitively one expects that the convolutional network performs poorly in these circumstances in contrast to a multilayer perceptron (MLPs) whose classification accuracy should not be affected by the pixel randomization. This work shows that by randomizing image pixels the hierarchical structure of the data is destroyed and long range correlations are introduced which standard CNNs are not able to capture. We show that their classification accuracy is heavily dependent on the class similarities as well as the pixel randomization process. We also indicate that dilated convolutions are able to recover some of the pixel correlations and improve the performance."
  },
  "cvpr2019_deep_vision_workshop_semi-supervisedrobustdeepneuralnetworksformulti-labelclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Semi-Supervised Robust Deep Neural Networks for Multi-Label Classification",
    "authors": [
      "Hakan Cevikalp",
      "Burak Benligiray",
      "Omer Nezih Gerek",
      "Hasan Saribas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Cevikalp_Semi-Supervised_Robust_Deep_Neural_Networks_for_Multi-Label_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Cevikalp_Semi-Supervised_Robust_Deep_Neural_Networks_for_Multi-Label_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose a robust method for semi-supervised training of deep neural networks for multi-label image classification. To this end, we use ramp loss, which is more robust against noisy and incomplete image labels compared to the classical hinge loss. The proposed method allows for learning from both labeled and unlabeled data in a semi-supervised learning setting. This is achieved by propagating labels from the labeled images to their unlabeled neighbors. Using a robust loss function be- comes crucial here, as the initial label propagations may include many errors, which degrades the performance of non-robust loss functions. In contrast, the proposed robust ramp loss restricts extreme penalties for the samples with incorrect labels, and the label assignment improves in each iteration and contributes to the learning process. The proposed method achieves state-of-the-art results in semi-supervised learning experiments on the CIFAR-10 and STL-10 datasets, and comparable results to the state-of the-art in supervised learning experiments on the NUS-WIDE and MS-COCO datasets."
  },
  "cvpr2019_deep_vision_workshop_jointlearningofneuralnetworksviaiterativereweightedleastsquares": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Joint Learning of Neural Networks via Iterative Reweighted Least Squares",
    "authors": [
      "Zaiwei Zhang",
      "Xiangru Huang",
      "Qixing Huang",
      "Xiao Zhang",
      "Yuan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Zhang_Joint_Learning_of_Neural_Networks_via_Iterative_Reweighted_Least_Squares_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Zhang_Joint_Learning_of_Neural_Networks_via_Iterative_Reweighted_Least_Squares_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we introduce the problem of jointly learning feed-forward neural networks across a set of relevant but diverse datasets. Compared to learning a separate network from each dataset in isolation, joint learning enables us to extract correlated information across multiple datasets to significantly improve the quality of learned networks. We formulate this problem as joint learning of multiple copies of the same network architecture and enforce the network weights to be shared across these networks. Instead of hand-encoding the shared network layers, we solve an optimization problem to automatically determine how layers should be shared between each pair of datasets. Experimental results show that our approach outperforms baselines without joint learning and those using pretraining-and-fine-tuning. We show the effectiveness of our approach on three tasks: image classification, learning auto-encoders, and image generation."
  },
  "cvpr2019_deep_vision_workshop_enhancingsalientobjectsegmentationthroughattention": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Enhancing Salient Object Segmentation Through Attention",
    "authors": [
      "Anuj Pahuja",
      "Avishek Majumder",
      "Anirban Chakraborty",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Pahuja_Enhancing_Salient_Object_Segmentation_Through_Attention_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Pahuja_Enhancing_Salient_Object_Segmentation_Through_Attention_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Segmenting salient objects in an image is an important vision task with ubiquitous applications. The problem becomes more challenging in the presence of a cluttered and textured background, low resolution and/or low contrast images. Even though existing algorithms perform well in segmenting most of the object(s) of interest, they often end up segmenting false positives due to resembling salient objects in the background. In this work, we tackle this problem by iteratively attending to image patches in a recurrent fashion and subsequently enhancing the predicted segmentation mask. Saliency features are estimated independently for every image patch which are further combined using an aggregation strategy based on a Convolutional Gated Recurrent Unit (ConvGRU) network. The proposed approach works in an end-to-end manner, removing background noise and false positives incrementally. Through extensive evaluation on various benchmark datasets, we show superior performance to the existing approaches without any post- processing."
  },
  "cvpr2019_deep_vision_workshop_deeprobustsingleimagedepthestimationneuralnetworkusingsceneunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Deep Robust Single Image Depth Estimation Neural Network Using Scene Understanding",
    "authors": [
      "Haoyu Ren",
      "Mostafa El-khamy",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Ren_Deep_Robust_Single_Image_Depth_Estimation_Neural_Network_Using_Scene_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Ren_Deep_Robust_Single_Image_Depth_Estimation_Neural_Network_Using_Scene_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Single image depth estimation (SIDE) plays a crucial role in 3D computer vision. In this paper, we propose a two-stage robust SIDE framework that can perform blind SIDE for both indoor and outdoor scenes. At the first stage, the scene understanding module will categorize the RGB image into different depth ranges. We introduce two different scene understanding modules based on scene classification and coarse depth estimation respectively. At the second stage, SIDE networks trained by the images of specific depth range are applied to obtain an accurate depth map. In order to improve the accuracy, we further design a multi-task encoding-decoding SIDE network DS-SIDENet based on depth-wise separable convolutions. DS-SIDENet is optimized to minimize both depth classification and depth regression losses. This improves the accuracy compared to a single-task SIDE network. Experimental results demonstrate that training DS-SIDENet on an individual dataset such as NYU achieves competitive performance to the state- of-art methods with much better efficiency. Ours proposed robust SIDE framework also shows good performance for the ScanNet indoor images and KITTI outdoor images simultaneously. It achieves the top performance compared to the Robust Vision Challenge (ROB) 2018 submissions."
  },
  "cvpr2019_deep_vision_workshop_maximallycompactandseparatedfeatureswithregularpolytopenetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Maximally Compact and Separated Features with Regular Polytope Networks",
    "authors": [
      "Federico Pernici",
      "Matteo Bruni",
      "Claudio Baecchi",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Pernici_Maximally_Compact_and_Separated_Features_with_Regular_Polytope_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Pernici_Maximally_Compact_and_Separated_Features_with_Regular_Polytope_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional Neural Networks (CNNs) trained with the Softmax loss are widely used classification models for several vision tasks. Typically, a learnable transformation (i.e. the classifier) is placed at the end of such models returning class scores that are further normalized into probabilities by Softmax. This learnable transformation has a fundamental role in determining the network internal feature representation. In this work we show how to extract from CNNs features with the properties of maximum inter-class separability and maximum intra-class compactness by setting the parameters of the classifier transformation as not train- able (i.e. fixed). We obtain features similar to what can be obtained with the well-known OCenter LossO [1] and other similar approaches but with several practical advantages including maximal exploitation of the available feature space representation, reduction in the number of net- work parameters, no need to use other auxiliary losses besides the Softmax. Our approach unifies and generalizes into a common approach two apparently different classes of methods regarding: discriminative features, pioneered by the Center Loss [1] and fixed classifiers, firstly evaluated in [2]. Preliminary qualitative experimental results provide some insight on the potentialities of our combined strategy."
  },
  "cvpr2019_deep_vision_workshop_normalestimationforaccurate3dmeshreconstructionwithpointcloudmodelincorporatingspatialstructure": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Normal Estimation for Accurate 3D Mesh Reconstruction with Point Cloud Model Incorporating Spatial Structure",
    "authors": [
      "Taisuke Hashimoto",
      "Masaki Saito"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Hashimoto_Normal_Estimation_for_Accurate_3D_Mesh_Reconstruction_with_Point_Cloud_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Hashimoto_Normal_Estimation_for_Accurate_3D_Mesh_Reconstruction_with_Point_Cloud_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose a network that can accurately infer normal vectors from a point cloud without sacrificing inference speed. The key idea of our model is to introduce a voxel structure to extract spatial features from a given point cloud. Specifically, unlike the other existing methods directly exploiting point clouds, our model leverages two subnetworks called a Opoint networkO and a Ovoxel networkO. The point network extracts local features of a surface from a point cloud, whereas the voxel network transforms the point cloud into voxels and encodes the spatial features from them. The experimental results demonstrate the effectiveness of our method."
  },
  "cvpr2019_deep_vision_workshop_anonlinear,noise-aware,quasi-clusteringapproachtolearningdeepcnnsfromnoisylabels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "A Nonlinear, Noise-aware, Quasi-clustering Approach to Learning Deep CNNs from Noisy Labels",
    "authors": [
      "Ishan Jindal",
      "Matthew Nokleby",
      "Daniel Pressel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Jindal_A_Nonlinear_Noise-aware_Quasi-clustering_Approach_to_Learning_Deep_CNNs_from_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Jindal_A_Nonlinear_Noise-aware_Quasi-clustering_Approach_to_Learning_Deep_CNNs_from_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The success of deep convolutional networks on image classification and recognition tasks depends on the avail- ability of large, labeled datasets, but it can be difficult to obtain a large number of accurate labels. To deal with this problem, we present Nonlinear, Noise-aware, Quasi- clustering (NNAQC), a method for learning deep convolutional networks from datasets corrupted by unknown label noise. We append a nonlinear noise model to a standard convolutional network, which is learned in tandem with the parameters of the network. Further, we train the network using a loss function that encourages the clustering of training images. We argue that the non-linear noise model, while not rigorous as a probabilistic model, results in a more effective denoising operator during backpropagation. We evaluate the performance of NNAQC on artificially injected label noise to MNIST, CIFAR-10, CIFAR-100 and ImageNet datasets and on a large-scale Clothing1M dataset with inherent label noise. On all these datasets, NNAQC provides significantly improved classification performance over the state of the art and is robust to the amount of label noise and the training samples."
  },
  "cvpr2019_deep_vision_workshop_globalsumpoolingageneralizationtrickforobjectcountingwithsmalldatasetsoflargeimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Global Sum Pooling: A Generalization Trick for Object Counting with Small Datasets of Large Images",
    "authors": [
      "Shubhra Aich",
      "Ian Stavness"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Aich_Global_Sum_Pooling_A_Generalization_Trick_for_Object_Counting_with_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Aich_Global_Sum_Pooling_A_Generalization_Trick_for_Object_Counting_with_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we explore the problem of training one- look regression models for counting objects in datasets comprising a small number of high-resolution, variable-shaped images. We illustrate that conventional global average pooling (GAP) based models are unreliable due to the patchwise cancellation of true overestimates and under-estimates for patchwise inference. To overcome this limitation and reduce overfitting caused by the training on full- resolution images, we propose to employ global sum pooling (GSP) instead of GAP or fully connected (FC) layers at the backend of a convolutional network. Although computationally equivalent to GAP, we show through comprehensive experimentation that GSP allows convolutional networks to learn the counting task as a simple linear mapping problem generalized over the input shape and the number of objects present. This generalization capability allows GSP to avoid both patchwise cancellation and overfitting by training on small patches and inference on full-resolution images as a whole. We evaluate our approach on four different aerial image datasets D two car counting datasets (CARPK and COWC), one crowd counting dataset (ShanghaiTech; parts A and B) and one new challenging dataset for wheat spike counting. Our GSP models improve upon the state-of-the- art approaches on all four datasets with a simple architecture. Also, GSP architectures trained with smaller-sized image patches exhibit better localization property due to their focus on learning from smaller regions while training."
  },
  "cvpr2019_deep_vision_workshop_styleaugmentationdataaugmentationviastylerandomization": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Style Augmentation: Data Augmentation via Style Randomization",
    "authors": [
      "Philip T. Jackson",
      "Amir Atapour-Abarghouei",
      "Stephen Bonner",
      "Toby P. Breckon",
      "Boguslaw Obara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Jackson_Style_Augmentation_Data_Augmentation_via_Style_Randomization_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Jackson_Style_Augmentation_Data_Augmentation_via_Style_Randomization_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We introduce style augmentation, a new form of data augmentation based on random style transfer, for improving the robustness of Convolutional Neural Networks (CNN) over both classification and regression based tasks. During training, style augmentation randomizes texture, contrast and color, while preserving shape and semantic content. This is accomplished by adapting an arbitrary style transfer network to perform style randomization, by sampling target style embeddings from a multivariate normal distribution instead of computing them from a style image. In addition to standard classification experiments, we investigate the effect of style augmentation (and data augmentation generally) on domain transfer tasks. We find that data augmentation significantly improves robustness to domain shift, and can be used as a simple, domain agnostic alternative to domain adaptation. Comparing style augmentation against a mix of seven traditional augmentation techniques, we find that it can be readily combined with them to improve network performance. We validate the efficacy of our technique with domain transfer experiments in classification and monocular depth estimation illustrating superior performance over benchmark tasks."
  },
  "cvpr2019_deep_vision_workshop_budget-awaresemi-supervisedsemanticandinstancesegmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Budget-aware Semi-Supervised Semantic and Instance Segmentation",
    "authors": [
      "Miriam Bellver",
      "Amaia Salvador",
      "Jordi Torrres",
      "Xavier Giro-i-Nieto"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Bellver_Budget-aware_Semi-Supervised_Semantic_and_Instance_Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Bellver_Budget-aware_Semi-Supervised_Semantic_and_Instance_Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Methods that move towards less supervised scenarios are key for image segmentation, as dense labels demand significant human intervention. Generally, the annotation burden is mitigated by labeling datasets with weaker forms of supervision, e.g. image-level labels or bounding boxes. Another option are semi-supervised settings, that commonly leverage a few strong annotations and a huge number of unlabeled/weakly-labeled data. In this paper, we revisit semi-supervised segmentation schemes and narrow down significantly the annotation budget (in terms of total labeling time of the training set) compared to previous approaches. With a very simple pipeline, we demonstrate that at low annotation budgets, semi-supervised methods outperform by a wide margin weakly-supervised ones for both semantic and instance segmentation. Our approach also outperforms previous semi-supervised works at a much reduced labeling cost. We present results for the Pascal VOC benchmark and unify weakly and semi-supervised ap- proaches by considering the total annotation budget, thus allowing a fairer comparison between methods."
  },
  "cvpr2019_deep_vision_workshop_sanetowardsimprovedpredictionrobustnessviastochasticallyactivatednetworkensembles": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "SANE: Towards Improved Prediction Robustness via Stochastically Activated Network Ensembles",
    "authors": [
      "Ibrahim Ben Daya",
      "Mohammad Javad Shafiee",
      "Michelle Karg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Daya_SANE_Towards_Improved_Prediction_Robustness_via_Stochastically_Activated_Network_Ensembles_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Daya_SANE_Towards_Improved_Prediction_Robustness_via_Stochastically_Activated_Network_Ensembles_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " A major challenge to the widespread adoption and deployment of deep neural networks in real-world operational scenarios relates to issues related to robustness and ability to deal with uncertainty when making predictions. One of the most effective strategies for improving robustness and handling uncertainty used in machine learning is the use of probabilistic modelling; however, there has been limited exploration into their use in improving the robustness of deep neural networks. In this study, we propose a new framework for improving the prediction robustness of deep neural network models via the notion of stochastically activated network ensembles (SANE), where an ensemble of deep neural networks with heterogeneous architectures are stochastically activated such that a subset of networks in the ensemble that are found to be more reliable for a given input will be responsible for a prediction. The proposed SANE framework takes advantage of a probabilistic graphical model to estimate the reliability of each network in the ensemble in predicting the correct class label for an input image given the beliefs of other networks. In other words, the graphical model enables the detection of networks in the ensemble that are likely to produce reliable predictions and include them in the final prediction process. The pro- posed SANE framework is evaluated on both non-targeted perturbations (e.g., random perturbations) as well as targeted perturbations (e.g., adversarial perturbations). Experimental results show that the proposed SANE framework can noticeably improve prediction robustness compared to a general ensemble approach, as well as providing further improvements in robustness against targeted perturbations when combined with additional stochastic mechanisms."
  },
  "cvpr2019_deep_vision_workshop_unsupervisedlearningofpairedstylestatisticsforunpairedimagetranslation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Unsupervised Learning of Paired Style Statistics for Unpaired Image Translation",
    "authors": [
      "Saeid Motiian",
      "Quinn Jones",
      "Stanislav Pidhorskyi",
      "Gianfranco Doretto"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Motiian_Unsupervised_Learning_of_Paired_Style_Statistics_for_Unpaired_Image_Translation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Motiian_Unsupervised_Learning_of_Paired_Style_Statistics_for_Unpaired_Image_Translation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Image-to-image translation has the goal of learning how to transform an input image from one domain as if it was from another domain, while preserving semantic and global information from the input. We present an image-to-image translation method that can be trained with unpaired images from source and target domains. However, we introduce a regularization that allows the model to specifically translate the local spatial statistic from one domain to another in an effort to leave unchanged gross structures and discourage translations of the semantic content. We do so by learning to generate paired images mapping the local statistic from one domain to the other. In turn, such images are used to improve the training of the translation networks, which become more focused on translating only the OstyleO of images while preserving the semantic content. Experiments on domain translation as well as domain adaptation highlight the effectiveness of our approach in comparison with the state-of-the-art."
  },
  "cvpr2019_deep_vision_workshop_automaticlabelingofdatafortransferlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Deep_Vision_Workshop",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Deep Vision Workshop",
    "title": "Automatic Labeling of Data for Transfer Learning",
    "authors": [
      "Parijat Dube",
      "Bishwaranjan Bhattacharjee",
      "Siyu Huo",
      "Patrick Watson",
      "Brian Belgodere"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Deep_Vision_Workshop/Dube_Automatic_Labeling_of_Data_for_Transfer_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Deep Vision Workshop/Dube_Automatic_Labeling_of_Data_for_Transfer_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Transfer learning uses trained weights from a source model as the initial weights for the training of a target dataset. A well chosen source with a large number of labeled data leads to significant improvement in accuracy. We demonstrate a technique that automatically labels large unlabeled datasets so that they can train source models for transfer learning. We experimentally evaluate this method, using a baseline dataset of human-annotated ImageNet1K labels, against five variations of this technique. We show that the performance of these automatically trained models come within 6% of baseline."
  },
  "cvpr2019_explainable_ai_visualizingdeepnetworksbyoptimizingwithintegratedgradients": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Visualizing Deep Networks by Optimizing with Integrated Gradients",
    "authors": [
      "Zhongang Qi",
      "Saeed Khorram",
      "Fuxin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Qi_Visualizing_Deep_Networks_by_Optimizing_with_Integrated_Gradients_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Qi_Visualizing_Deep_Networks_by_Optimizing_with_Integrated_Gradients_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Understanding and interpreting the decisions made by deep learning models is valuable in many domains. In computer vision, computing heatmaps from a deep network is a popular approach for visualizing and understanding deep networks. However, heatmaps that do not correlate with the network may mislead human, hence the performance of heatmaps in providing a faithful explanation to the underlying deep network is crucial. In this paper, we propose I-GOS, which optimizes for a heatmap so that the classification scores on the masked image would maximally decrease. The main novelty of the approach is to compute descent directions based on the integrated gradients instead of the normal gradient, which avoids local optima and speeds up convergence. Extensive experiments show that the heatmaps produced by our approach are more correlated with the decision of the underlying deep network, in comparison with other state-of-the-art approaches. "
  },
  "cvpr2019_explainable_ai_deepcouplingofrandomferns": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Deep Coupling of Random Ferns",
    "authors": [
      "Sangwon Kim",
      "Mira Jeong",
      "Deokwoo Lee",
      "Byoung Chul Ko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Kim_Deep_Coupling_of_Random_Ferns_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Kim_Deep_Coupling_of_Random_Ferns_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The purpose of this study is to design a new lightweight explainable deep model instead of deep neural networks (DNN) because of its high memory and processing resource requirement as well as black-box training although DNN is a powerful algorithm for classification and regression problems. This study propose a non-neural network style deep model based on combination of deep coupling random ferns (DCRF). In proposed DCRF, each neuron of a layer is replaced with the Fern and each layer consists of several type of Ferns. The proposed method showed a higher uniform performance in terms of the number of parameters and operations without a loss of accuracy compared to a few related studies including a DNN based model compression algorithm."
  },
  "cvpr2019_explainable_ai_unsupervisedclusteringbasedunderstandingofcnn": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Unsupervised clustering based understanding of CNN",
    "authors": [
      "Deeptha Girish",
      "Vineeta Singh",
      "Anca Ralescu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Girish_Unsupervised_clustering_based_understanding_of_CNN_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Girish_Unsupervised_clustering_based_understanding_of_CNN_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional Neural networks have been very successful for most computer vision tasks such as image recognition, classification, object detection and segmentation. Even though CNNs are very successful and give superior results as compared to traditional image processing algorithms, interpretability of their results remains an important issue to be solved. Indeed, lack of interpretability and explainability of how CNN work at their various levels, caused a certain skepticism among their potential users, as for example those working in medical diagnosis or autonomous driving cars. The current study aims to answer some of the issues related to interpretability by the use un- supervised methods to discern the features learned by the CNN in different layers."
  },
  "cvpr2019_explainable_ai_beyondexplainabilityleveraginginterpretabilityforimprovedadversariallearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Beyond Explainability: Leveraging Interpretability for Improved Adversarial Learning",
    "authors": [
      "Devinder Kumarl Ibrahim Ben Daya",
      "Kanav Vats",
      "Jeffery Feng",
      "Graham Taylor",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Daya_Beyond_Explainability_Leveraging_Interpretability_for_Improved_Adversarial_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Daya_Beyond_Explainability_Leveraging_Interpretability_for_Improved_Adversarial_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this study, we propose the leveraging of interpretability for tasks beyond purely the purpose of explainability. In particular, this study puts forward a novel strategy for leveraging gradient-based interpretability in the realm of adversarial examples, where we use insights gained to aid adversarial learning. More specifically, we introduce the concept of spatially constrained one-pixel adversarial perturbations, where we guide the learning of such adversarial perturbations towards more susceptible areas identified via gradient-based interpretability. Experimental results using different benchmark datasets show that such a spatially constrained one-pixel adversarial perturbation strategy can noticeably improve the speed of convergence as well as produce successful attacks that were also visually difficult to perceive, thus illustrating an effective use of interpretability methods for tasks outside of the purpose of purely explainability."
  },
  "cvpr2019_explainable_ai_buildingexplainableaievaluationforautonomousperception": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Building Explainable AI Evaluation for Autonomous Perception",
    "authors": [
      "Chi Zhang",
      "Biyao Shang",
      "Ping Wei",
      "Li Li",
      "Yuehu Liu",
      "Nanning Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Zhang_Building_Explainable_AI_Evaluation_for_Autonomous_Perception_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Zhang_Building_Explainable_AI_Evaluation_for_Autonomous_Perception_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The development of the robust visual intelligence is one of the long-term challenging problems. From the perspective of artificial intelligence evaluation, the need to discover and explain the potential shortness of the evaluated intelligent algorithms/systems as well as the need to evaluate the intelligence level of such testees are of equal importance. In this paper, we propose a possible solution to these challenges: Explainable Evaluation for visual intelligence. Compared to the existing work on Explainable AI, we focus on the problem setting where the internal mechanisms of AI algorithms are sophisticated, heterogeneous or unreachable. In this case, the interpretability of test output is formulated as an semantic embedding to the existing correlation between factors of data variances and test outputs. Dictionary learning is introduced to jointly estimate the semantic mapping and the semantic representations for explanation. The optimal solution of proposed method could be reached via an alternating optimization process. The application of the \"Explainable AI Evaluation\" will strengthen the influence of objective assessment for visual intelligence."
  },
  "cvpr2019_explainable_ai_unitimpulseresponseasanexplainerofredundancyinadeepconvolutionalneuralnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Unit Impulse Response as an Explainer of Redundancy in a Deep Convolutional Neural Network",
    "authors": [
      "Rachana Sathish",
      "Debdoot Sheet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Sathish_Unit_Impulse_Response_as_an_Explainer_of_Redundancy_in_a_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Sathish_Unit_Impulse_Response_as_an_Explainer_of_Redundancy_in_a_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional neural networks (CNN) are generally designed with a heuristic initialization of network architecture and trained for a certain task. This often leads to over-parametrization after learning and induces redundancy in the information flow paths within the network. This robustness and reliability is at the increased cost of redundant computations. Several methods have been proposed which leverage metrics that quantify the redundancy in each layer. However, layer-wise evaluation in these methods disregards the long-range redundancy which exists across depth on account of the distributed nature of the features learned by the model. In this paper, we propose (i) a mechanism to empirically demonstrate the robustness in performance of a CNN on account of redundancy across its depth, (ii) a method to identify the systemic redundancy in response of a CNN across depth using the understanding of unit impulse response, we subsequently demonstrate use of these methods to interpret redundancy in few networks as example. These techniques provide better insights into the internal dynamics of a CNN."
  },
  "cvpr2019_explainable_ai_localizingcommonobjectsusingcommoncomponentactivationmap": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Localizing Common Objects Using Common Component Activation Map",
    "authors": [
      "Weihao Li",
      "Omid Hosseini Jafari",
      "Carsten Rother"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Li_Localizing_Common_Objects_Using_Common_Component_Activation_Map_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Li_Localizing_Common_Objects_Using_Common_Component_Activation_Map_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work, we propose an approach to localize common objects from novel object categories in a set of images. We solve this problem using a new common component activation map (CCAM) in which we treat the class-specific activation maps (CAM) as components to discover the com- mon components in the image set. We show that our approach can generalize on novel object categories in our experiments."
  },
  "cvpr2019_explainable_ai_interpretationofdeepcnnrecognitionwithfilterspaceclusteringinfeatureextractionandreconstruction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Interpretation of Deep CNN Recognition with Filter Space Clustering in Feature Extraction and Reconstruction",
    "authors": [
      "Sukhan Lee",
      "Naeem Ul Islam"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Lee_Interpretation_of_Deep_CNN_Recognition_with_Filter_Space_Clustering_in_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Lee_Interpretation_of_Deep_CNN_Recognition_with_Filter_Space_Clustering_in_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Interpreting a deep Convolutional Neural Network (CNN) involves identifying the features in a hierarchy of layers that contribute to recognition. Although the current approaches serve as methods to interpret a deep CNN, further advancement is required for a more accurate and efficient way of understanding how a hierarchy of features formed by a deep CNN contributes to recognition. In this paper, we propose attaching a feedback CNN to a pretrained feedforward CNN as a means of learning how recognition is performed by the feedforward CNN. In other words, the features reconstructed in a hierarchy of the feedback CNN represent those learned by the feedforward CNN. By analyzing how clusters are formed in the layers of feature spaces in the feedback CNN, we can interpret which features critically contribute to recognition. It also helps to evaluate whether or not recognition is done successfully. In order to show this, we experimentally verify the capabilities of the proposed approach in terms of identifying incorrectly recognized input data by pinpointing the source of the error in feature spaces. Experiments conducted on the ModelNet datasets indicate that the proposed approach offers an extended capability of interpreting a deep CNN as described above with higher accuracy than conventional approaches."
  },
  "cvpr2019_explainable_ai_interpretationoffeaturespaceusingmulti-channelattentionalsub-networks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Interpretation of Feature Space using Multi-Channel Attentional Sub-Networks",
    "authors": [
      "Masanari Kimura",
      "Masayuki Tanaka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Kimura_Interpretation_of_Feature_Space_using_Multi-Channel_Attentional_Sub-Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Kimura_Interpretation_of_Feature_Space_using_Multi-Channel_Attentional_Sub-Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional Neural Networks have achieved impressive results in various tasks, but interpreting the internal mechanism is a challenging problem. To tackle this problem, we exploit a multi-channel attention mechanism in feature space. Our network architecture allows us to obtain an attention mask for each feature while existing CNN visualization methods provide only a common attention mask for all features. We apply the proposed multi-channel attention mechanism to multi-attribute recognition task. We can obtain different attention mask for each feature and for each attribute. Those analyses give us deeper insight into the feature space of CNNs. The experimental results for the benchmark dataset show that the proposed method gives high interpretability to humans while accurately grasping the attributes of the data."
  },
  "cvpr2019_explainable_ai_relevanceregularizationofconvolutionalneuralnetworkforinterpretableclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Relevance Regularization of Convolutional Neural Network for Interpretable Classification",
    "authors": [
      "Chae Hwa Yoo",
      "Nayoung Kim",
      "Je-Won Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Yoo_Relevance_Regularization_of_Convolutional_Neural_Network_for_Interpretable_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Yoo_Relevance_Regularization_of_Convolutional_Neural_Network_for_Interpretable_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Conventional end-to-end learning algorithm considers only the final prediction output and ignores layer-wise relational reasoning during the training. In this paper, we propose to use a forward and backward interacted-activation (FBI) loss function that regularizes training a CNN so that the prediction model can provide interpretable results for classification. From our best knowledge, the proposed algorithm is the first work to use a regularization function without any prior knowledge or pre-defined terms to allow for a CNN to be more explainable. It is demonstrated with quantitative and qualitative analysis that the proposed technique can be used for efficiently train a CNN with more interpretability, applied to a well-known classification problem."
  },
  "cvpr2019_explainable_ai_interpretablemachinelearningforgeneratingsemanticallymeaningfulformativefeedback": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Interpretable Machine Learning for Generating Semantically Meaningful Formative Feedback",
    "authors": [
      "Nese Alyuz",
      "Tevfik Metin Sezgin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Alyuz_Interpretable_Machine_Learning_for_Generating_Semantically_Meaningful_Formative_Feedback_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Alyuz_Interpretable_Machine_Learning_for_Generating_Semantically_Meaningful_Formative_Feedback_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We express our emotional state through a range of ex- pressive modalities such as facial expressions, vocal cues, or body gestures. However, children on the Autism Spectrum experience difficulties in expressing and recognizing emotions with the accuracy of their neurotypical peers. Research shows that children on the Autism Spectrum can be trained to recognize and express emotions if they are given supportive and constructive feedback. In particular, providing formative feedback, (e.g., feedback given by an expert describing how they need to modify their behavior to improve their expressiveness), has been found valuable in rehabilitation. Unfortunately, generating such formative feedback requires constant supervision of an expert. In this work, we describe a system for automatic formative assessment integrated into an automatic emotion recognition setup. Our system is built on an interpretable machine learning framework that answers the question of what needs to be modified in human behavior to achieve a desired expressive display. It propagates the desired changes to human-understandable attributes through explanation vectors operating on a shared low level feature space. We report experiments conducted on a childrens voice data set with expression variations, showing that the proposed mechanism generates formative feedback aligned with the expectations reported from a clinical perspective."
  },
  "cvpr2019_explainable_ai_tobelieveornottobelievevalidatingexplanationfidelityfordynamicmalwareanalysis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "To believe or not to believe: Validating explanation fidelity for dynamic malware analysis",
    "authors": [
      "Li Chen",
      "Carter Yagemann",
      "Evan Downing"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Chen_To_believe_or_not_to_believe_Validating_explanation_fidelity_for_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Chen_To_believe_or_not_to_believe_Validating_explanation_fidelity_for_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Converting malware into images followed by vision-based deep learning algorithms has shown superior threat detection efficacy compared with classical machine learning algorithms. When malware are visualized as images, visual-based interpretation schemes can also be applied to extract insights of why individual samples are classified as malicious. In this work, via two case studies of dynamic malware classification, we extend the local interpretable model-agnostic explanation algorithm to explain image-based dynamic malware classification and examine its interpretation fidelity. For both case studies, we first train deep learning models via transfer learning on malware images, demonstrate high classification effectiveness, apply an explanation method on the images, and correlate the results back to the samples to validate whether the algorithmic insights are consistent with security domain expertise. In our first case study, the interpretation framework identifies indirect calls that uniquely characterize the underlying exploit behavior of a malware family. In our second case study, the interpretation framework extracts insightful information such as cryptography-related APIs when applied on images created from API existence, but generate ambiguous interpretation on images created from API sequences and frequencies. Our findings indicate that current image-based interpretation techniques are promising for explaining vision-based malware classification. We continue to develop image-based interpretation schemes specifically for security applications."
  },
  "cvpr2019_explainable_ai_learningsemanticallymeaningfulembeddingsusinglinearconstraints": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Learning Semantically Meaningful Embeddings Using Linear Constraints",
    "authors": [
      "Shuyu Lin",
      "Bo Yang",
      "Robert Birke",
      "Ronald Clark"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Learning an interpretable representation is an essential task in machine learning, as many fields, such as legislation and healthcare, require explainability in the decision-making process where costly consequences can be easily incurred. In this paper, we propose a simple embedding learning method that jointly optimises for an auto-encoding reconstruction task and for estimating the corresponding attribute labels associated with the raw data. We restrict the attribute estimation model to be linear, constraining the learnt embedding space to be close to the interpretable attribute space. As a result, we are able to interpret the learnt embedding as a mixture of different attributes, i.e. semantic information has been embedded in the latent representation. Furthermore, as the linear mapping is fully invertible, we are able to generate any data samples from a list of specified attributes."
  },
  "cvpr2019_explainable_ai_deepvisualcityrecognitionvisualization": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Deep Visual City Recognition Visualization",
    "authors": [
      "Xiangwei Shi",
      "Seyran Khademi",
      "Jan van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Shi_Deep_Visual_City_Recognition_Visualization_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Shi_Deep_Visual_City_Recognition_Visualization_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Understanding how cities visually differ from each others is interesting for planners, residents, and historians. We investigate the interpretation of deep features learned by convolutional neural networks (CNNs) for city recognition. Given a trained city recognition network, we first generate weighted masks using the known Grad-CAM technique and to select the most discriminate regions in the image. Since the image classification label is the city name, it contains no information of objects that are class-discriminate, we investigate the interpretability of deep representations with two methods. (i) Unsupervised method is used to cluster the objects appearing in the visual explanations. (ii) A pretrained semantic segmentation model is used to label objects in pixel level, and then we introduce statistical measures to quantitatively evaluate the interpretability of discriminate objects. The influence of network architectures and random initializations in training, is studied on the interpretability of CNN features for city recognition. The results suggest that network architectures would affect the interpretability of learned visual representations greater than different initializations."
  },
  "cvpr2019_explainable_ai_explainablehierarchicalsemanticconvolutionalneuralnetworkforlungcancerdiagnosis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Explainable Hierarchical Semantic Convolutional Neural Network for Lung Cancer Diagnosis",
    "authors": [
      "Shiwen Shen",
      "Simon X Han",
      "Denise R Aberle",
      "Alex A Bui",
      "William Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Shen_Explainable_Hierarchical_Semantic_Convolutional_Neural_Network_for_Lung_Cancer_Diagnosis_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Shen_Explainable_Hierarchical_Semantic_Convolutional_Neural_Network_for_Lung_Cancer_Diagnosis_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " While deep learning methods have demonstrated classification performance comparable to human readers in tasks such as computer-aided diagnosis, these models are difficult to interpret, do not incorporate prior domain knowledge, and are often considered as a \"black box.\" We present a novel interpretable deep hierarchical semantic convolutional neural network (HSCNN) to predict whether a given pulmonary nodule observed on a computed tomography (CT) scan is malignant. Our network provides two levels of output: 1) low-level semantic features; and 2) a high-level prediction of nodule malignancy. The low-level outputs reflect diagnostic features often reported by radiologists and serve to explain how the model interprets the images in an expert-interpretable manner. The information from these low-level outputs, along with the representations learned by the convolutional layers, are then combined and used to infer the high-level output. Our experimental results using the Lung Image Database Consortium (LIDC) show that the proposed method not only produces interpretable lung cancer predictions but also achieves comparable results with the state-of-the-art methods."
  },
  "cvpr2019_explainable_ai_arecnnpredictionsbasedonreasonableevidence?": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Are CNN Predictions based on Reasonable Evidence?",
    "authors": [
      "Sarah Adel Bargal",
      "Andrea Zunino",
      "Vitali Petsiuk",
      "Jianming Zhang",
      "Kate Saenko",
      "Vittorio Murino",
      "Stan Sclaroff"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Bargal_Are_CNN_Predictions_based_on_Reasonable_Evidence_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Bargal_Are_CNN_Predictions_based_on_Reasonable_Evidence_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose Guided Zoom, an approach that utilizes spatial grounding to make more informed predictions. It does so by making sure the model has \"the right reasons\" for a prediction, being defined as reasons that are coherent with those used to make similar correct decisions at training time. The reason/evidence upon which a deep neural network makes a prediction is defined to be the spatial grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom question show reasonable the evidence used to make a prediction is. We show that Guided Zoom results in the refinement of a model's classification accuracy on two fine-grained classification datasets."
  },
  "cvpr2019_explainable_ai_explainingthepointnetwhathasbeenlearnedinsidethepointnet?": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Explaining the PointNet: What Has Been Learned Inside the PointNet?",
    "authors": [
      "Binbin Zhang",
      "Shikun Huang",
      "Wen Shen",
      "Zhihua Wei"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Zhang_Explaining_the_PointNet_What_Has_Been_Learned_Inside_the_PointNet_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Zhang_Explaining_the_PointNet_What_Has_Been_Learned_Inside_the_PointNet_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work, we focus on explaining the PointNet [4], the first deep learning framework to directly handle 3D point clouds. We raise two issues based on the nature of PointNet and give solutions. First, we visualize the activation of point functions to examine the issue how global features represent different classes? Then, we propose a derivative of PointNet, named C-PointNet, to generate the class-attentive responce maps to explore that based on what information in the point cloud is the PointNet making a decision? The experiments on ModelNet40 demonstrate the efficacy of our work for getting better understanding of PointNet"
  },
  "cvpr2019_explainable_ai_towardsanunderstandingofneuralnetworksinnatural-imagespaces": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Towards an Understanding of Neural Networks in Natural-Image Spaces",
    "authors": [
      "Yifei Fan",
      "Anthony Yezzi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Fan_Towards_an_Understanding_of_Neural_Networks_in_Natural-Image_Spaces_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Fan_Towards_an_Understanding_of_Neural_Networks_in_Natural-Image_Spaces_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Two major uncertainties, dataset bias and adversarial examples, prevail in state-of-the-art AI algorithms with deep neural networks. In this paper, we present an intuitive explanation of these issues as well as an interpretation of the performance of deep networks in a natural- image space. The explanation consists of two parts: the variational-calculus view of machine learning and a hypothetical model of natural-image spaces. Following the explanation, we (1) demonstrate that the values of training samples differ, (2) provide incremental boosts to the accuracy of a CIFAR-10 classifier by introducing an additional \"random-noise\" category during training, and (3) alleviate over-fitting thereby enhancing the robustness of a classifier against adversarial examples by detecting and excluding illusive training samples that are consistently misclassified. Our overall contribution is therefore twofold. First, while most existing algorithms treat data equally and have a strong appetite for more data, we demonstrate in contrast that an individual datum can sometimes have disproportionate and counterproductive influence, and that it is not always better to train neural networks with more data. Next, we consider more thoughtful strategies by taking into account the geometric and topological properties of natural-image spaces to which deep networks are applied."
  },
  "cvpr2019_explainable_ai_geometricinterpretationofacnnslastlayer": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Geometric interpretation of a CNN's last layer",
    "authors": [
      "Alejandro de la Calle",
      "Aitor Aller",
      "Javier Tovar",
      "Emilio J. Almazan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/de_la_Calle_Geometric_interpretation_of_a_CNNs_last_layer_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/de_la_Calle_Geometric_interpretation_of_a_CNNs_last_layer_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Training Convolutional Neural Networks (CNNs) remains a non-trivial task that in many cases relies on the skills and experience of the person conducting the training. Choosing hyperparameters, knowing when the training should be interrupted, or even when to stop trying training strategies are some difficult decisions that have to be made. These decisions are difficult partly because we still know little about the internal behaviour of CNNs, especially during training. In this work we conduct a methodical experimentation on MNIST public database of handwritten digits to better understand the evolution of the last layer from a geometric perspective: namely the classification vectors and the image embedding vectors. Within this context we present the problem of the variability across equal set-up trainings due to the random component of the initialisation method. We propose a novel approach that guides the initialisation of the parameters in the classification layer. This method reduces 12% the variability across repetitions and leads to accuracies 18% higher on average."
  },
  "cvpr2019_explainable_ai_medicaltimeseriesclassificationwithhierarchicalattention-basedtemporalconvolutionalnetworksacasestudyofmyotonicdystrophydiagnosis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Medical Time Series Classification with Hierarchical Attention-based Temporal Convolutional Networks: A Case Study of Myotonic Dystrophy Diagnosis",
    "authors": [
      "Lei Lin",
      "Beilei Xu",
      "Wencheng Wu",
      "Trevor W. Richardson",
      "Edgar A. Bernal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Lin_Medical_Time_Series_Classification_with_Hierarchical_Attention-based_Temporal_Convolutional_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Lin_Medical_Time_Series_Classification_with_Hierarchical_Attention-based_Temporal_Convolutional_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Myotonia, which refers to delayed muscle relaxation after contraction, is the main symptom of myotonic dystrophy patients. We propose a hierarchical attention-based temporal convolutional network (HA-TCN) architecture for myotonic dystrohpy diagnosis from handgrip force time series data, and introduce mechanisms that enable model explainability. We compare the performance of the HA-TCN model against that of benchmark TCN models, LSTM models with and without attention mechanisms, and SVM approaches with handcrafted features. In terms of classification accuracy and F1 score, we found deep learning models have similar levels of performance, and they all outperform SVM. Further, the HA-TCN model outperforms its TCN counterpart with regards to computational efficiency regardless of network depth, and in terms of performance particularly when the number of hidden layers is small. Lastly, HA-TCN models can consistently identify relevant time series segments in the relaxation phase of the handgrip force time series, and exhibit increased robustness to noise when compared to attention-based LSTM models."
  },
  "cvpr2019_explainable_ai_naturallanguageinteractionwithexplainableaimodels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Natural Language Interaction with Explainable AI Models",
    "authors": [
      "Arjun R Akula",
      "Sinisa Todorovic",
      "Joyce Y Chai",
      "Song-Chun Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Akula_Natural_Language_Interaction_with_Explainable_AI_Models_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Akula_Natural_Language_Interaction_with_Explainable_AI_Models_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper presents an explainable AI (XAI) system that provides explanations for its predictions. The system consists of two key components - namely, the prediction And-Or graph (AOG) model for recognizing and localizing concepts of interest in input data, and the XAI model for providing explanations to the user about the AOG's predictions. In this work, we focus on the XAI model specified to interact with the user in natural language, whereas the AOG's predictions are considered given and represented by the corresponding parse graphs (pg's) of the AOG. Our XAI model takes pg's as input and provides answers to the user's questions using the following types of reasoning: direct evidence (e.g., detection scores), part-based inference (e.g., detected parts provide evidence for the concept asked), and other evidences from spatiotemporal context (e.g., constraints from the spatiotemporal surround). We identify several correlations between user's questions and the XAI answers using Youtube Action dataset."
  },
  "cvpr2019_explainable_ai_explainableaiascollaborativetasksolving": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Explainable AI as Collaborative Task Solving",
    "authors": [
      "Arjun Akula",
      "Changsong Liu",
      "Sinisa Todorovic",
      "Joyce Chai",
      "Song-Chun Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Akula_Explainable_AI_as_Collaborative_Task_Solving_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Akula_Explainable_AI_as_Collaborative_Task_Solving_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a new framework for explainable AI systems (XAI) aimed at increasing human trust in the system's performance through explanations. Based on the Theory of Mind, our framework X-ToM explicitly models machine's mind, human's mind as inferred by the machine, as well as machine's mind as inferred by the human. These mental representations are incorporated to (1) learn an optimal explanation policy that takes into account human's perception and beliefs; and (2) quantitatively evaluate human's trust of machine behaviors. We have applied X-ToM in the context of visual recognition. Compared to the most popularly used attribution based explanations (saliency maps), our X-ToM significantly improves human trust in the underlying vision system."
  },
  "cvpr2019_explainable_ai_explainabilityforcontent-basedimageretrieval": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Explainability for Content-Based Image Retrieval",
    "authors": [
      "Bo Dong",
      "Roddy Collins",
      "Anthony Hoogs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Dong_Explainability_for_Content-Based_Image_Retrieval_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Dong_Explainability_for_Content-Based_Image_Retrieval_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We discuss how the concept of \"explainability\" may be applied to Content-Based Image Retrieval (CBIR) systems. CBIR typically transforms an image into a feature representation for which a similarity distance metric may be computed; recent systems have improved performance by using features from deep learning networks [11, 6, 3]. However, as these representations have no direct semantic interpretability, the behavior of the system can be difficult for the user to understand in terms of semantically significant objects in the scene which may have no significant presence in the feature representation. Conversely, the similarity metric for two images may be dominated by pixel content which is not the semantic focus of the images, such as the background. We propose Similarity Based Saliency Maps (SBSM) to illustrate which areas in an image the CBIR system uses when retrieving and ranking results; the SBSM thus serves to \"explain\" the CBIR's decisions to the user. We have implemented SBSMs in our opensource Social Media Query Toolkit (SMQTK) [4], and have conducted preliminary user studies to demonstrate that SBSMs allow the user to more efficiently retrieve images."
  },
  "cvpr2019_explainable_ai_analysisofthecontributionandtemporaldependencyoflstmlayersforreinforcementlearningtasks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Analysis of the contribution and temporal dependency of LSTM layers for reinforcement learning tasks",
    "authors": [
      "Teng-Yok Lee",
      "Jeroen van Baar",
      "Kent Wittenburg",
      "Alan Sullivan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Lee_Analysis_of_the_contribution_and_temporal_dependency_of_LSTM_layers_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Lee_Analysis_of_the_contribution_and_temporal_dependency_of_LSTM_layers_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Long short-term memory (LSTM) architectures are widely used in deep neural networks (DNN) when the input data is time-varying, because of their ability to capture (often unknown) long-term dependencies of sequential data. In this paper, we present an approach to analyze the temporal dependencies needed by an LSTM layer. Our approach first locates so-called salient LSTM cells that contribute most to the neural network output, by combining both forward and backward propagation. For these salient cells, we compare their output contributions and the internal gates of LSTM to see whether the activation of gates precedes the increasing of contribution, and how far beforehand the precedence occurs. We apply our analysis in the context of reinforcement learning (RL) for robot control to understand how the LSTM layer reacts under different circumstances."
  },
  "cvpr2019_explainable_ai_directingdnnsattentionforfacialattributionclassificationusinggradient-weightedclassactivationmapping": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Directing DNNs Attention for Facial Attribution Classification using Gradient-weighted Class Activation Mapping",
    "authors": [
      "Xi Yang",
      "Bojian Wu",
      "Issei Sato",
      "Takeo Igarashi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Yang_Directing_DNNs_Attention_for_Facial_Attribution_Classification_using_Gradient-weighted_Class_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Yang_Directing_DNNs_Attention_for_Facial_Attribution_Classification_using_Gradient-weighted_Class_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep neural networks (DNNs) have a high accuracy on image classification tasks. However, DNNs trained by such dataset with co-occurrence bias may rely on wrong features while making decisions for classification. It will greatly affect the transferability of pre-trained DNNs. In this paper, we propose an interactive method to direct classifiers paying attentions to the regions that are manually specified by the users, in order to mitigate the influence of co-occurrence bias. We test on CelebA dataset, the pre-trained AlexNet is fine-tuned to focus on the specific facial attributes based on the results of Grad-CAM."
  },
  "cvpr2019_explainable_ai_visualizingtheresilienceofdeepconvolutionalnetworkinterpretations": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Visualizing the Resilience of Deep Convolutional Network Interpretations",
    "authors": [
      "Bhavan Vasu",
      "Andreas Savakis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Vasu_Visualizing_the_Resilience_of_Deep_Convolutional_Network_Interpretations_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Vasu_Visualizing_the_Resilience_of_Deep_Convolutional_Network_Interpretations_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper aims at visualizing the resiliency of deep net- work interpretations across datasets. We further explore how these interpretations change when network weights are damaged. We utilize Class Activation Maps to obtain heatmaps of deep network interpretations and identify salient local regions. We apply our methods on two remote sensing datasets and demonstrate that representations are resilient across similar datasets. We also demonstrate the benefits of transfer learning for different datasets. We further analyze these interpretations when the network weights are damaged and illustrate that retraining a damaged network is useful in recovering its performance. Our visualization results, based on ResNet50, offer insights in the resiliency of convolutional network architectures."
  },
  "cvpr2019_explainable_ai_beautylearningandcounterfactualinference": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Beauty Learning and Counterfactual Inference",
    "authors": [
      "Tao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Li_Beauty_Learning_and_Counterfactual_Inference_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Li_Beauty_Learning_and_Counterfactual_Inference_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This work showcases a new approach for causal discovery by leveraging user experiments and recent advances in photo-realistic image editing, demonstrating a potential of identifying causal factors and understanding complex systems counterfactually. We introduce the beauty learning problem as an example, which has been discussed metaphysically for centuries and recently been proved exists, is quantifiable, and can be learned by deep models in our paper [1], where we utilize a natural image generator coupled with user studies to infer causal effects from facial semantics to beauty outcomes, the results of which also align with existing empirical studies. We expect the proposed framework for a broader application in causal inference."
  },
  "cvpr2019_explainable_ai_visualizingthedecision-makingprocessindeepneuraldecisionforest": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "Visualizing the Decision-making Process in Deep Neural Decision Forest",
    "authors": [
      "Shichao Li",
      "Kwang-Ting Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Li_Visualizing_the_Decision-making_Process_in_Deep_Neural_Decision_Forest_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Li_Visualizing_the_Decision-making_Process_in_Deep_Neural_Decision_Forest_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep neural decision forest (NDF) achieved remarkable performance on various vision tasks via combining decision tree and deep representation learning. In this work, we first trace the decision-making process of this model and visualize saliency maps to understand which portion of the input influence it more for both classification and regression problems. We then apply NDF on a multi-task coordinate regression problem and demonstrate the distribution of routing probabilities, which is vital for interpreting NDF yet not shown for regression problems. The pre-trained model and code for visualization will be available at https://github.com/Nicholasli1995/ VisualizingNDF "
  },
  "cvpr2019_explainable_ai_l1-normgradientpenaltyfornoisereductionofattributionmaps": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Explainable_AI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Explainable AI",
    "title": "L1-Norm Gradient Penalty for Noise Reduction of Attribution Maps",
    "authors": [
      "Keisuke Kiritoshi",
      "Ryosuke Tanno",
      "Tomonori Izumitani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Explainable_AI/Kiritoshi_L1-Norm_Gradient_Penalty_for_Noise_Reduction_of_Attribution_Maps_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Explainable AI/Kiritoshi_L1-Norm_Gradient_Penalty_for_Noise_Reduction_of_Attribution_Maps_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Determining the attribution of the input elements to the output values is very important for interpretability when we use deep neural network (DNN) models in real-world tasks. Gradient-based methods are widely used because they can represent the relationship between each input and output pair in the shape of a partial derivative. Attribution values determined from DNN models that use batch normalization include high levels of noise. This is problematic because it significantly reduces the interpretability of the model. To obtain sparse and interpretable attribution maps, we developed a new regularization method that includes a penalty term, based on the L1-norm of gradient values calculated through back-propagation procedures, in the loss function. We evaluated the effectiveness of the method using CIFAR-10 image datasets. "
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_activeadversarialdomainadaptation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Active Adversarial Domain Adaptation",
    "authors": [
      "Jong-Chyi Su",
      "Yi-Hsuan Tsai",
      "Kihyuk Sohn",
      "Buyu Liu",
      "Subhransu Maji",
      "Manmohan Chandraker"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Su_Active_Adversarial_Domain_Adaptation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Su_Active_Adversarial_Domain_Adaptation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The covariate shift problem is common in many practical computer vision applications, where the training and test data are drawn from different distribution, e.g., the seasonal distribution of natural species may change in a camera trap dataset. Many domain adaptation (DA) methods have been proposed to address this issue [3, 10, 19, 17, 11, 5, 18] by matching the marginal distributions of source and target domain. While domain adaptation provides a good starting point, the performance of unsupervised DA methods often fall far behind their supervised counterparts [16, 1]. In such cases, some labeled data from the target domain can bring in performance benefits. However, obtaining ground-truth annotations can be laborious and naively collecting annotated data could be inefficient. In this work, we aim to answer the following questions: 1) how to select data to label from the target domain effectively, and 2) how to perform adaptation given these labeled data from the target domain."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_learntobeuncertainleveraginguncertainlabelsinchestx-rayswithbayesianneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Learn To Be Uncertain: Leveraging Uncertain Labels In Chest X-rays With Bayesian Neural Networks",
    "authors": [
      "Hao-Yu Yang",
      "Junling Yang",
      "Yue Pan",
      "Kunlin Cao",
      "Qi Song",
      "Feng Gao",
      "Youbing Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Yang_Learn_To_Be_Uncertain_Leveraging_Uncertain_Labels_In_Chest_X-rays_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Yang_Learn_To_Be_Uncertain_Leveraging_Uncertain_Labels_In_Chest_X-rays_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Communication of uncertainty is important for both radiology reports and deep neural networks (DNNs). For radiologists, conveying diagnostic uncertainty in the written report is a challenging and yet inevitable task. On the other hand, while deep learning models have shown compelling potentials in disease classification and lesion detection, applications of DNNs in the medical domain should provide a quantitative measurement of prediction confidence for risk management purposes. In this paper, we investigate the relationship between uncertainty in diagnostic chest x-ray radiology reports and uncertainty estimation of corresponding DNN models using Bayesian approaches. Two sampling methods, Bernoulli and Gaussian dropout have been tested. Our results show that the incorporation of uncertainty labels during model training results in higher predictive variance for uncertain cases at test time. The uncertain cases are inherently difficult to diagnose for human readers, which often needs a further psychical examination to confirm. Returning uncertain predictions on these cases will prevent the DNN model from making over-confident mistakes."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_modelingassumptionsandevaluationschemesontheassessmentofdeeplatentvariablemodels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Modeling assumptions and evaluation schemes: On the assessment of deep latent variable models",
    "authors": [
      "Judith Butepage",
      "Petra Poklukar",
      "Danica Kragic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Butepage_Modeling_assumptions_and_evaluation_schemes_On_the_assessment_of_deep_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Butepage_Modeling_assumptions_and_evaluation_schemes_On_the_assessment_of_deep_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent findings indicate that deep generative models can assign unreasonably high likelihoods to out-of-distribution data points. Especially in applications such as autonomous driving, medicine and robotics, these overconfident ratings can have detrimental effects. In this work, we argue that two points contribute to these findings: 1) modeling assumptions such as the choice of the likelihood, and 2) the evaluation under local posterior distributions vs global prior distributions. We demonstrate experimentally how these mechanisms can bias the likelihood estimates of variational autoencoders."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_onthesensitivityofadversarialrobustnesstoinputdatadistributions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions",
    "authors": [
      "Gavin Weiguang Ding",
      "Kry Yik Chau Lui",
      "Xiaomeng Jin",
      "Luyu Wang",
      "Ruitong Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Neural networks are vulnerable to small adversarial perturbations. While existing literature largely focused on the vulnerability of learned models, we demonstrate an intriguing phenomenon that adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarially trained model that is both trained and evaluated on the new distribution. We show this by constructing semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve similar clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. The full paper (ICLR 2019) can be found at https://openreview.net/forum?id= S1xNEhR9KX."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_empiricalstudyofmc-dropoutinvariousastronomicalobservingconditions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Empirical Study of MC-Dropout in Various Astronomical Observing Conditions",
    "authors": [
      "Noble Kennamer",
      "Alex Ihler",
      "David Kirkby"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Kennamer_Empirical_Study_of_MC-Dropout_in_Various_Astronomical_Observing_Conditions_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Kennamer_Empirical_Study_of_MC-Dropout_in_Various_Astronomical_Observing_Conditions_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The analysis of large astronomical surveys increasingly incorporates machine learning models to handle a diverse set of tasks. It is important for the scientific analysis of these surveys that the uncertainty of the models be well understood and the predictions properly calibrated. Here we present an empirical study of MC-Dropout for a core prediction problem in astronomy emphasizing how the modeled uncertainty is influenced by changes in observing conditions. We will show that while MC-Dropout results in improved accuracy and better calibrated predictions there is still an underestimation of uncertainty that needs to be addressed."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_multi-tasklearningbasedonseparableformulationofdepthestimationanditsuncertainty": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Multi-Task Learning based on Separable Formulation of Depth Estimation and its Uncertainty",
    "authors": [
      "Akari Asai",
      "Daiki Ikami",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Asai_Multi-Task_Learning_based_on_Separable_Formulation_of_Depth_Estimation_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Asai_Multi-Task_Learning_based_on_Separable_Formulation_of_Depth_Estimation_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present an optimization framework for uncertainty estimation in a regression problem. To obtain predictive uncertainty inherent in the observation, we formulate regression with uncertainty estimation as a multi-task learning problem and a new uncertainty loss function, inspired by variational representations of robust estimation. Contrary to existing approaches, our approach allows balancing between the predictive task loss and uncertainty estimation loss. We evaluate the efficacy of our approach on NYU Depth Dataset V2 and show that our proposed method consistently yields better performance than the previous approaches, for both depth and uncertainty estimation."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_automatedlabelnoiseidentificationforfacialattributerecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Automated Label Noise Identification for Facial Attribute Recognition",
    "authors": [
      "Jeremy Speth",
      "Emily M. Hand"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Speth_Automated_Label_Noise_Identification_for_Facial_Attribute_Recognition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Speth_Automated_Label_Noise_Identification_for_Facial_Attribute_Recognition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Current state-of-the-art facial attribute recognition techniques use exceedingly deep convolutional neural networks (CNNs), which require large human-annotated datasets that are costly and time-consuming to collect. In most domains, there are several large-scale datasets for researchers to work with. In facial attribute recognition, there is only one large-scale dataset available - CelebA - causing researchers to rely too heavily on this one set of data. While CelebA provides the scale necessary for training deep networks, there are several types of noise present in the dataset. We address the problem of label noise by introducing a novel multi-label verification framework to identify mislabeled samples. Our work is applicable to data collection, cleaning, and multi-label verification. Our method is used to analyze label noise in CelebA and perform extensive experiments with additive noise to show the efficacy of the proposed approach."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_incrementallearningwithunlabeleddatainthewild": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Incremental Learning with Unlabeled Data in the Wild",
    "authors": [
      "Kibok Lee",
      "Kimin Lee",
      "Jinwoo Shin",
      "Honglak Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Lee_Incremental_Learning_with_Unlabeled_Data_in_the_Wild_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Lee_Incremental_Learning_with_Unlabeled_Data_in_the_Wild_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose to leverage a continuous and large stream of unlabeled data in the wild to alleviate catastrophic forget- ting in class-incremental learning. Our experimental results on CIFAR and ImageNet datasets demonstrate the superiority of the proposed methods over prior methods: compared to the state-of-the-art method, our proposed method shows up to 14.9% higher accuracy and 45.9% less forgetting."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_uncertaintybaseddetectionandrelabelingofnoisyimagelabels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Uncertainty Based Detection and Relabeling of Noisy Image Labels",
    "authors": [
      "Jan M. Kohler",
      "Maximilian Autenrieth",
      "William H. Beluch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Kohler_Uncertainty_Based_Detection_and_Relabeling_of_Noisy_Image_Labels_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Kohler_Uncertainty_Based_Detection_and_Relabeling_of_Noisy_Image_Labels_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep neural networks (DNNs) are powerful tools in computer vision tasks. However, in many realistic scenarios label noise is prevalent in the training images, and overfitting to these noisy labels can significantly harm the generalization performance of DNNs. We propose a novel technique to identify data with noisy labels based on the different distributions of the predictive uncertainties from a DNN over the clean and noisy data. Additionally, the behavior of the uncertainty over the course of training helps to identify the network weights which best can be used to re- label the noisy labels. Data with noisy labels can therefore be cleaned in an iterative process. Our proposed method can be easily implemented, and shows promising performance on the task of noisy label detection on CIFAR-10 and CIFAR-100."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_measuringcalibrationindeeplearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Measuring Calibration in Deep Learning",
    "authors": [
      "Jeremy Nixon",
      "Michael W. Dusenberry",
      "Linchuan Zhang",
      "Ghassen Jerfel",
      "Dustin Tran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Nixon_Measuring_Calibration_in_Deep_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Nixon_Measuring_Calibration_in_Deep_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The reliability of a machine learning model's confidence in its predictions is critical for high-risk applications. Calibration--the idea that a model's predicted probabilities of outcomes reflect true probabilities of those outcomes--formalizes this notion. Current calibration metrics fail to consider all of the predictions made by machine learning models, and are in- efficient in their estimation of the calibration error. We design the Adaptive Calibration Error (ACE) metric to resolve these pathologies and show that it outperforms other metrics, especially in settings where predictions beyond the maximum prediction that is chosen as the output class matter."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_benchmarkingsampling-basedprobabilisticobjectdetectors": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Benchmarking Sampling-based Probabilistic Object Detectors",
    "authors": [
      "Dimity Miller",
      "Niko Sunderhauf",
      "Haoyang Zhang",
      "David Hall",
      "Feras Dayoub"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper provides the first benchmark for sampling- based probabilistic object detectors. A probabilistic object detector expresses uncertainty for all detections that reliably indicates object localisation and classification performance. We compare performance for two sampling-based uncertainty techniques, namely Monte Carlo Dropout and Deep Ensembles, when implemented into one-stage and two-stage object detectors, Single Shot MultiBox Detector and Faster R-CNN. Our results show that Deep Ensembles outperform MC Dropout for both types of detectors. We also introduce a new merging strategy for sampling-based techniques and one-stage object detectors. We show this novel merging strategy has competitive performance with previously established strategies, while only having one free parameter."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_f-vaegan-d2afeaturegeneratingframeworkforany-shotlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning",
    "authors": [
      "Yongqin Xian",
      "Saurabh Sharma",
      "Bernt Schiele",
      "Zeynep Akata"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Xian_f-VAEGAN-D2_A_Feature_Generating_Framework_for_Any-Shot_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Xian_f-VAEGAN-D2_A_Feature_Generating_Framework_for_Any-Shot_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " When labeled training data is scarce, a promising data augmentation approach is to generate visual features of un- known classes using their attributes. To learn the class conditional distribution of CNN features, these models rely on pairs of image features and class attributes. Hence, they can not make use of the abundance of unlabeled data samples. In this paper, we tackle any-shot learning problems i.e. zero-shot and few-shot, in a unified feature generating framework that operates in both inductive and transductive learning settings. We develop a conditional generative model that combines the strength of VAE and GANs and in addition, via an unconditional discriminator, learns the marginal feature distribution of unlabeled images. We empirically show that our model learns highly discriminative CNN features for CUB and FLO datasets, and establish a new state-of-the-art in any-shot learning, i.e. inductive and transductive generalized zero- and few-shot learning settings."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_structuredaleatoricuncertaintyinhumanposeestimation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Structured Aleatoric Uncertainty in Human Pose Estimation",
    "authors": [
      "Nitesh B. Gundavarapu",
      "Divyansh Srivastava",
      "Rahul Mitra",
      "Abhishek Sharma",
      "Arjun Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Gundavarapu_Structured_Aleatoric_Uncertainty_in_Human_Pose_Estimation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Gundavarapu_Structured_Aleatoric_Uncertainty_in_Human_Pose_Estimation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Human pose estimation from monocular images exhibits an inherent uncertainty through self-occlusions and inter-person occlusions, aside from typical sources of uncertainty. Recently, there has been an increased focus in modelling uncertainty in supervised machine learning tasks. In line with this trend, we propose a novel formulation to capture aleatoric uncertainty in human pose using a multivariate Gaussian distribution over all the joints of human body and show that this improves generalization in 2D hu- man pose estimation by implicitly suppressing the gradients from uncertain joints. Further, we develop a novel method to triangulate 3D human pose from predicted 2D poses, under the predicted uncertainty, that out-performs the baselines by over 10.8% and provide a multi-view inference benchmark for 3D human pose estimation on Human 3.6M dataset."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_generalizedzero-shotlearningviaalignedvariationalautoencoders": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Generalized Zero-Shot Learning via Aligned Variational Autoencoders",
    "authors": [
      "Edgar Schonfeld",
      "Sayna Ebrahimi",
      "Samarth Sinha",
      "Trevor Darrell",
      "Zeynep Akata"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Schonfeld_Generalized_Zero-Shot_Learning_via_Aligned_Variational_Autoencoders_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Schonfeld_Generalized_Zero-Shot_Learning_via_Aligned_Variational_Autoencoders_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Most approaches in generalized zero-shot learning rely on cross-modal mapping between an image feature space and a class embedding space or on generating artificial image features. However, learning a shared cross-modal embedding by aligning the latent spaces of modality-specific autoencoders is shown to be promising in (generalized) zero-shot learning. While following the same direction, we also take artificial feature generation one step further and propose a model where a shared latent space of image features and class embeddings is learned by aligned variational autoencoders, for the purpose of generating latent features to train a softmax classifier. We evaluate our learned latent features on conventional benchmark datasets and establish a new state of the art on generalized zero-shot learning. Moreover, our results on ImageNet with various zero-shot splits show that our latent features generalize well in large-scale settings. The extended version of this work is accepted for publication at CVPR 2019[16]."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_whyrelunetworksyieldhigh-confidencepredictionsfarawayfromthetrainingdataandhowtomitigatetheproblem": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem",
    "authors": [
      "Matthias Hein",
      "Maksym Andriushchenko",
      "Julian Bitterwolf"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Hein_Why_ReLU_networks_yield_high-confidence_predictions_far_away_from_the_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Hein_Why_ReLU_networks_yield_high-confidence_predictions_far_away_from_the_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Classifiers used in the wild, in particular for safety-critical systems, should know when they don't know, in particular make low confidence predictions far away from the training data. We show that ReLU type neural networks fail in this regard as they produce almost always high confidence predictions far away from the training data. For bounded domains we propose a new robust optimization technique similar to adversarial training which enforces low confidence predictions far away from the training data. We show that this technique is surprisingly effective in reducing the confidence of predictions far away from the training data while maintaining high confidence predictions and test error on the original classification task compared to standard training. This is a short version of the corresponding CVPR paper."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_uncertainty-guidedcontinuallearninginbayesianneuralnetworks-extendedabstract": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Uncertainty-Guided Continual Learning in Bayesian Neural Networks - Extended Abstract",
    "authors": [
      "Sayna Ebrahimi",
      "Mohamed Elhoseiny",
      "Trevor Darrell",
      "Marcus Rohrbach"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Ebrahimi_Uncertainty-Guided_Continual_Learning_in_Bayesian_Neural_Networks_-_Extended_Abstract_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Ebrahimi_Uncertainty-Guided_Continual_Learning_in_Bayesian_Neural_Networks_-_Extended_Abstract_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' importance. In contrast, we propose Bayesian Continual Learning (BCL), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. We evaluate our BCL approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally we show that our model can be task-independent at test time, i.e. it does not presume knowledge of which task a sample belongs to."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_iterativeself-learningsemi-supervisedimprovementtodatasetvolumesandmodelaccuracy": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Iterative Self-Learning: Semi-Supervised Improvement to Dataset Volumes and Model Accuracy",
    "authors": [
      "Robert Dupre",
      "Jiri Fajtl",
      "Vasileios Argyriou",
      "Paolo Remagnino"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Dupre_Iterative_Self-Learning_Semi-Supervised_Improvement_to_Dataset_Volumes_and_Model_Accuracy_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Dupre_Iterative_Self-Learning_Semi-Supervised_Improvement_to_Dataset_Volumes_and_Model_Accuracy_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " A novel semi-supervised learning technique is introduced based on a simple iterative learning cycle together with learned thresholding techniques and an ensemble decision support system. State-of-the-art model performance and increased training data volume are demonstrated, through the use of unlabelled data when training deeply learned classification models. Evaluation of the proposed approach is performed on commonly used datasets when evaluating semi-supervised learning techniques as well as a number of more challenging image classification datasets (CIFAR-100 and a 200 class subset of ImageNet)."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_deepprobabilisticregressionofelementsofso(3)usingquaternionaveraginganduncertaintyinjection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Deep Probabilistic Regression of Elements of SO(3) using Quaternion Averaging and Uncertainty Injection",
    "authors": [
      "Valentin Peretroukhin",
      "Brandon Wagstaff",
      "and Jonathan Kelly"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Peretroukhin_Deep_Probabilistic_Regression_of_Elements_of_SO3_using_Quaternion_Averaging_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Peretroukhin_Deep_Probabilistic_Regression_of_Elements_of_SO3_using_Quaternion_Averaging_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Consistent estimates of rotation are crucial to vision-based motion estimation in augmented reality and robotics. In this work, we present a method to extract probabilistic estimates of rotation from deep regression models. First, we build on prior work and develop a multi-headed network structure we name HydraNet that can account for both aleatoric and epistemic uncertainty. Second, we extend HydraNet to targets that belong to the rotation group, SO(3), by regressing unit quaternions and using the tools of rotation averaging and uncertainty injection onto the manifold to produce three-dimensional covariances. Finally, we present results and analysis on a synthetic dataset, learn consistent orientation estimates on the 7-Scenes dataset, and show how we can use our learned covariances to fuse deep estimates of relative orientation with classical stereo visual odometry to improve localization on the KITTI dataset."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_improvingdeepnetworkrobustnesstounknowninputswithobjectosphere": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Improving Deep Network Robustness to Unknown Inputs with Objectosphere",
    "authors": [
      "Akshay Raj Dhamija",
      "Manuel Gunther",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Dhamija_Improving_Deep_Network_Robustness_to_Unknown_Inputs_with_Objectosphere_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Dhamija_Improving_Deep_Network_Robustness_to_Unknown_Inputs_with_Objectosphere_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep Neural Networks trained on academic datasets often fail when applied to the real world. These failures generally arise from unknown inputs that are not of interest to the system. The mis-classification of these unknown inputs as one of the known classes highlights the need for more robust deep networks. The problem of identifying samples that are not of interest to the system has previously been tackled by either thresholding softmax, which by construction cannot return none of the known classes itself, or by learning new features for the unknown inputs using an additional back- ground or garbage class. As demonstrated, both of these approaches help but are generally insufficient when previously unseen classes are encountered. This paper overviews our recent publication Reducing Network Agnostophobia, NeurIPS 2018. The paper presented two novel loss functions that effectively handle unseen classes while providing a new measure for uncertainty. The ability to identify unknown samples plays a crucial role in developing robust networks that may be used in open-world problems. The paper also introduced an evaluation metric that focused on comparing performance of multiple approaches in an open-set setting."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_learningconditionalerrormodelforsimulatedtime-seriesdata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Learning Conditional Error Model for Simulated Time-Series Data",
    "authors": [
      "Ashish Shrivastava",
      "Oncel Tuzel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Shrivastava_Learning_Conditional_Error_Model_for_Simulated_Time-Series_Data_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Shrivastava_Learning_Conditional_Error_Model_for_Simulated_Time-Series_Data_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Applications such as autonomous navigation [1], human- robot interaction [2], game-playing robots [8], etc., use simulation to minimize the cost of testing in real world. Furthermore, some machine learning algorithms, like reinforcement learning, use simulation for training a model. To test reliably in simulation or deploy a model in the real world that is trained with simulated data, the simulator should be representative of the real environment. Usually, the simulator is based on manually designed rules and ignores the stochastic behavior of measurements. In particular, we would like to learn a model that captures uncertainties of the sensing algorithms (e.g. neural networks used to detect objects) in real world and add them in simulation. We model the distribution of residuals between the ground truth states of the objects and their perceived states by the sensing algorithm. This error distribution depends both on the current state of the object (e.g. distance from the sensor) and its past residuals. We assume the error distribution is conditionally Gaussian, and we use a deep neural neural network (DNN) to map the object states and past residuals to the distribution parameters (mean and variance). Our conditional model perturbs the dynamic objects' states (position, velocities, orientations, and shape) and produces smoother trajectories which look similar to the real data."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_saneexploringadversarialrobustnesswithstochasticallyactivatednetworkensembles": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "SANE: Exploring Adversarial Robustness With Stochastically Activated Network Ensembles",
    "authors": [
      "Ibrahim Ben Daya",
      "Mohammad Javad Shafiee",
      "Michelle Karg",
      "Christian Scharfenberger",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Daya_SANE_Exploring_Adversarial_Robustness_With_Stochastically_Activated_Network_Ensembles_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Daya_SANE_Exploring_Adversarial_Robustness_With_Stochastically_Activated_Network_Ensembles_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " A major challenge to the adoption of deep neural net- works in real-world applications is their robustness in different scenarios. Deep neural networks have been shown to be particularly susceptible to adversarial attacks: malicious perturbations to the input that fool networks into predicting the wrong label. In this study, we propose a new framework to improve adversarial robustness using stochastically activated network ensembles (SANE), where an ensemble of deep neural networks with heterogeneous architectures is stochastically activated such that a subset of the more robust networks in the ensemble are responsible for a prediction. The proposed framework treats networks as nodes in a probabilistic graphical model to detect networks in the ensemble that are likely to be robust against an adversarial attack and activate them to be part of the decision making process. Experimental results under different adversarial attacks show that the proposed SANE cannot only noticeably improve robustness to adversarial attacks compared to a general ensemble approach, but provide further improvements against adversarial attacks when combined with additional stochastic defense mechanisms."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_unsuperviseddomainadaptationviacalibratinguncertainties": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Unsupervised Domain Adaptation via Calibrating Uncertainties",
    "authors": [
      "Ligong Han",
      "Yang Zou",
      "Ruijiang Gao",
      "Lezi Wang",
      "Dimitris Metaxas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Han_Unsupervised_Domain_Adaptation_via_Calibrating_Uncertainties_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Han_Unsupervised_Domain_Adaptation_via_Calibrating_Uncertainties_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Unsupervised domain adaptation (UDA) aims at inferring class labels for unlabeled target domain given a related labeled source dataset. Intuitively, the model trained on labeled data will produce high uncertainty estimation for unseen data. Under this assumption, models trained in the source domain would produce high uncertainties when tested on the target domain. In this work, we build on this assumption and propose to adapt from source and target domain via calibrating their predictive uncertainties. We employ variational Bayes learning for uncertainty estimation which is quantified as the predicted Renyi entropy on the target domain. We discuss the theoretical properties of our proposed framework and demonstrate its effectiveness on three domain-adaptation tasks."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_uncertaintyawareaudiovisualactivityrecognitionusingdeepbayesianvariationalinference": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "Uncertainty aware audiovisual activity recognition using deep Bayesian variational inference",
    "authors": [
      "Mahesh Subedar",
      "Ranganath Krishnan",
      "Paulo Lopez Meyer",
      "Omesh Tickoo",
      "Jonathan Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Subedar_Uncertainty_aware_audiovisual_activity_recognition_using_deep_Bayesian_variational_inference_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Subedar_Uncertainty_aware_audiovisual_activity_recognition_using_deep_Bayesian_variational_inference_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Deep neural networks (DNNs) provide state-of-the-art results for a multitude of applications, but the approaches using DNNs for multimodal audiovisual applications do not consider predictive uncertainty associated with individual modalities. Bayesian deep learning methods provide principled confidence and quantify predictive uncertainty. Our contribution in this work is to propose an uncertainty aware multimodal Bayesian fusion framework for activity recognition. We demonstrate a novel approach that combines deterministic and variational layers to scale Bayesian DNNs to deeper architectures. Our experiments using in- and out-of- distribution samples selected from a subset of Moments-in-Time (MiT) dataset show a more reliable confidence measure as compared to the non-Bayesian baseline and the Monte Carlo dropout (MC dropout) approximate Bayesian inference. We also demonstrate the uncertainty estimates obtained from the proposed framework can identify out-of-distribution data on the UCF101 and MiT datasets. In the multimodal setting, the proposed framework improved precision-recall AUC by 10.2% on the subset of MiT dataset as compared to non-Bayesian baseline."
  },
  "cvpr2019_uncertainty_and_robustness_in_deep_visual_learning_gandataaugmentationthroughactivelearninginspiredsampleacquisition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Uncertainty_and_Robustness_in_Deep_Visual_Learning",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Uncertainty and Robustness in Deep Visual Learning",
    "title": "GAN Data Augmentation Through Active Learning Inspired Sample Acquisition",
    "authors": [
      "Christopher Nielsen",
      "Michal Okoniewski"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Uncertainty_and_Robustness_in_Deep_Visual_Learning/Nielsen_GAN_Data_Augmentation_Through_Active_Learning_Inspired_Sample_Acquisition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Uncertainty and Robustness in Deep Visual Learning/Nielsen_GAN_Data_Augmentation_Through_Active_Learning_Inspired_Sample_Acquisition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Data augmentation is frequently used to increase the effective training set size when training deep neural networks for supervised learning tasks. This technique is particularly beneficial when the size of the training set is small. Recently, data augmentation using GAN generated samples has been shown to provide performance improvement for supervised learning tasks. In this paper we propose a method of GAN data augmentation for image classification that uses the prediction uncertainty of the classifier network to determine the optimal GAN samples to augment the training set. We apply the acquisition function framework originally developed for active learning to evaluate the sample uncertainty. Preliminary experimental results are provided to demonstrate the benefit of this technique."
  },
  "cvpr2019_aamvem_deeplearningforsemanticsegmentationofcoralreefimagesusingmulti-viewinformation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AAMVEM",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Automated Analysis of Marine Video for Environmental Monitoring",
    "title": "Deep Learning for Semantic Segmentation of Coral Reef Images Using Multi-View Information",
    "authors": [
      "Andrew King",
      "Suchendra M.Bhandarkar",
      "Brian M. Hopkinson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AAMVEM/King_Deep_Learning_for_Semantic_Segmentation_of_Coral_Reef_Images_Using_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AAMVEM/King_Deep_Learning_for_Semantic_Segmentation_of_Coral_Reef_Images_Using_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Two major deep learning architectures, i.e., patch-based convolutional neural networks (CNNs) and fully convolutional neural networks (FCNNs), are studied in the context of semantic segmentation of underwater images of coral reef ecosystems. Patch-based CNNs are typically used to enable single-entity classification whereas FCNNs are used to generate a semantically segmented output from an input image. In coral reef mapping tasks, one typically obtains multiple images of a coral reef from varying viewpoints either using stereoscopic image acquisition or while conducting underwater video surveys. We propose and compare patch-based CNN and FCNN architectures capable of exploiting multi-view image information to improve the accuracy of classification and semantic segmentation of the input images. We investigate extensions of the conventional FCNN architecture to incorporate stereoscopic input image data and extensions of patch-based CNN architectures to incorporate multi-view input image data. Experimental results show the proposed TwinNet architecture to be the best performing FCNN architecture, performing comparably with its baseline Dilation8 architecture when using just a left-perspective input image, but markedly improving over Dilation8 when using a stereo pair of input images. Likewise, the proposed nViewNet-8 architecture is shown to be the best performing patch-based CNN architecture, outperforming its single-image ResNet152 baseline architecture in terms of classification accuracy."
  },
  "cvpr2019_aamvem_exemplar-basedunderwaterimageenhancementaugmentedbywaveletcorrectedtransforms": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AAMVEM",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Automated Analysis of Marine Video for Environmental Monitoring",
    "title": "Exemplar-based Underwater Image Enhancement Augmented by Wavelet Corrected Transforms",
    "authors": [
      "Adarsh Jamadandi",
      "Uma Mudenagudi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AAMVEM/Jamadandi_Exemplar-based_Underwater_Image_Enhancement_Augmented_by_Wavelet_Corrected_Transforms_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AAMVEM/Jamadandi_Exemplar-based_Underwater_Image_Enhancement_Augmented_by_Wavelet_Corrected_Transforms_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper we propose a novel deep learning framework to enhance underwater images by augmenting our network with wavelet corrected transformations. Wavelet transforms have recently made way into deep learning frameworks and their ability to reconstruct arbitrary signals accurately makes them favourable for many applications. Underwater images are subjected to unique distortions, this is mainly attributed to the fact that red wavelength light gets absorbed dominantly giving a greenish, blue hue. This wavelength dependent selective absorption of light and also scattering by the suspended particles introduce non-linear distortions that affect the quality of the images. We propose an encoder-decoder module with wavelet pooling and unpooling as one of the network components to perform progressive whitening and coloring transforms to enhance underwater images via realistic style transfer. We give a sound theoretical proof as to why wavelet transforms are better for signal reconstruction. We demonstrate our proposed framework on popular underwater images dataset and evaluate it using metrics like SSIM, PSNR and UCIQE and show that we achieve state-of-the-art results compared to those mentioned in the literature."
  },
  "cvpr2019_aamvem_detectionofmarineanimalsinanewunderwaterdatasetwithvaryingvisibility": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AAMVEM",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Automated Analysis of Marine Video for Environmental Monitoring",
    "title": "Detection of Marine Animals in a New Underwater Dataset with Varying Visibility",
    "authors": [
      "Malte Pedersen",
      "Joakim Bruslund Haurum",
      "Rikke Gade",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AAMVEM/Pedersen_Detection_of_Marine_Animals_in_a_New_Underwater_Dataset_with_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AAMVEM/Pedersen_Detection_of_Marine_Animals_in_a_New_Underwater_Dataset_with_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The increasing demand for marine monitoring calls for robust automated systems to support researchers in gathering information from marine ecosystems. This includes computer vision based marine organism detection and species classification systems. Current state-of-the-art marine vision systems are based on CNNs, which in nature require a relatively large amount of varied training data. In this paper we present a new publicly available underwater dataset with annotated image sequences of fish, crabs, and starfish captured in brackish water with varying visibility. The dataset is called the Brackish Dataset and it is the first part of a planned long term monitoring of the marine species visiting the strait where the cameras are permanently mounted. To the best of our knowledge, this is the first annotated underwater image dataset captured in temperate brackish waters. In order to obtain a baseline performance for future reference, the YOLOv2 and YOLOv3 CNNs were fine-tuned and tested on the Brackish Dataset."
  },
  "cvpr2019_lowpower_image_recognition_challenge_areyoupayingattention?classifyingattentioninpivotalresponsetreatmentvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "LowPower_Image_Recognition_Challenge",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Low-Power Image Recognition Challenge",
    "title": "Are You Paying Attention? Classifying Attention in Pivotal Response Treatment Videos",
    "authors": [
      "Corey D C Heath",
      "Hemanth Venkateswara",
      "Sethuraman Panchanathan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/LowPower_Image_Recognition_Challenge/Heath_Are_You_Paying_Attention_Classifying_Attention_in_Pivotal_Response_Treatment_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Low-Power Image Recognition Challenge/Heath_Are_You_Paying_Attention_Classifying_Attention_in_Pivotal_Response_Treatment_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Pivotal response treatment (PRT) has been empirically shown to aid children with autism spectrum disorder ASD improve their communication skills. The child's primary caregivers can effectively implement PRT when provided with training and support, leading to greater opportunities for the child to improve. Utilization of computer vision technology is a critical component of creating more opportunities to support individuals implementing PRT. Automatically extracting data from videos of caregivers' interactions with their child during PRT sessions would alleviate the human effort required to provide assessment and feedback, which would allow experts to provide greater support to more individuals. Additionally, this data could be used to provide immediate automated feedback. The process of extracting data from PRT videos is complicated and provides excellent context for a computer vision challenge. PRT videos consist of 'in-the-wild' conditions of dyadic interactions recorded on ubiquitously available devices, and vary in filming quality. The challenge presented tasks researchers with inferring the child's attention state in relation to the caregiver in the video based on body pose information and visual cues. Approaches will be evaluated based on accuracy metrics, how- ever, the algorithm's speed is also important. Having fast algorithms will reduce the time between performance and assessment, allowing for greater opportunities to situate feedback in the context of the learning activity. Low-power solutions are also necessary to accommodate delivering results on mobile devices."
  },
  "cvpr2019_lowpower_image_recognition_challenge_winningsolutiononlpirc-llcompetition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "LowPower_Image_Recognition_Challenge",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Low-Power Image Recognition Challenge",
    "title": "Winning Solution on LPIRC-ll Competition",
    "authors": [
      "Alexander Goncharenko",
      "Sergey Alyamkin",
      "Andrey Denisov",
      "Evgeny Terentev"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/LowPower_Image_Recognition_Challenge/Goncharenko_Winning_Solution_on_LPIRC-ll_Competition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Low-Power Image Recognition Challenge/Goncharenko_Winning_Solution_on_LPIRC-ll_Competition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The neural network quantization is highly desired procedure to perform before running neural networks on mobile devices. Quantization without fine-tuning leads to accuracy drop of the model, whereas commonly used training with quantization is done on the full set of the labeled data and therefore is both time- and resource-consuming. Real life applications require simplification and acceleration of quantization procedure that will maintain the accuracy of full-precision neural network, especially for modern mobile neural network architectures like Mobilenet-v1, MobileNet-v2 and MNAS. Here we present two methods to significantly optimize the training with quantization procedure. The first one is introducing the trained scale factors for discretization thresholds that are separate for each filter. The second one is based on mutual rescaling of consequent depth-wise separable convolution and convolution layers. Using the proposed techniques, we quantize the modern mobile architectures of neural networks with the set of train data of only 10% of the total ImageNet 2012 sample. Such reduction of train dataset size and small number of trainable parameters allow to fine-tune the network for several hours while maintaining the high accuracy of quantized model (accuracy drop was less than 0.5%). Ready-for-use models and code are available at: https://github.com/agoncharenko1992/FAT-fast-adjustable-threshold."
  },
  "cvpr2019_face_and_gesture_analysis_for_health_informatics_vision-basedactionunderstandingforassistivehealthcareashortreview": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Face_and_Gesture_Analysis_for_Health_Informatics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Face and Gesture Analysis for Health Informatics",
    "title": "Vision-based Action Understanding for Assistive Healthcare: A Short Review",
    "authors": [
      "Md Atiqur Rahman Ahad",
      "Anindya Das Antar",
      "Omar Shahid"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Face_and_Gesture_Analysis_for_Health_Informatics/Ahad_Vision-based_Action_Understanding_for_Assistive_Healthcare_A_Short_Review_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Face and Gesture Analysis for Health Informatics/Ahad_Vision-based_Action_Understanding_for_Assistive_Healthcare_A_Short_Review_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The scarcity of trained therapist, economic imbalance, and an increasing amount of elderly people are the reasons for poor rehabilitation treatment and inadequate healthcare facilities in many countries. Vision-based rehabilitation treatment, monitoring daily living, and advanced healthcare can improve technology that allows people with an injury to practice intense movement training without taking help from a therapist daily. This technology has remarkable basic notable benefits as vision-based systems are non-contact, precise, immune to electromagnetic interference, nondestructive, and they can be used for long range and multiple target monitoring. The objective of this survey paper is devoted to exhibiting a summary of the challenges and difficulties in this domain along with some solutions. Besides, in order to guide the researchers in this field, we have discussed available sensing devices in the field of computer vision that can be used for taking data in hospitals and rehabilitation centers. We have also analyzed some benchmark datasets regarding gestures, medical activities, sports and exercise actions, 3D actions, and so on with relevant information. Moreover, this article also provides a comparison among existing research works on some benchmark datasets related to this field of research."
  },
  "cvpr2019_face_and_gesture_analysis_for_health_informatics_classificationoffacialmicro-expressionsusingmotionmagnifiedemotionavatarimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Face_and_Gesture_Analysis_for_Health_Informatics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Face and Gesture Analysis for Health Informatics",
    "title": "CLASSIFICATION OF FACIAL MICRO-EXPRESSIONS USING MOTION MAGNIFIED EMOTION AVATAR IMAGES",
    "authors": [
      "Ankith Jain Rakesh Kumar",
      "Rajkumar Theagarajan",
      "Omar Peraza",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Face_and_Gesture_Analysis_for_Health_Informatics/Kumar_CLASSIFICATION_OF_FACIAL_MICRO-EXPRESSIONS_USING_MOTION_MAGNIFIED_EMOTION_AVATAR_IMAGES_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Face and Gesture Analysis for Health Informatics/Kumar_CLASSIFICATION_OF_FACIAL_MICRO-EXPRESSIONS_USING_MOTION_MAGNIFIED_EMOTION_AVATAR_IMAGES_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Facial micro-expressions are subtle involuntary movements of the facial muscles, characterized by a rapid, short duration and genuine emotions. The detection and classification of these micro-expressions by humans and machines is challenging due to their short duration and subtlety. These micro-expressions have many important applications, especially in therapy, monitoring and depression analysis. It has been shown that during therapy, the facial micro-expressions of patients diagnosed with depression are very difficult to identify and in most cases are very subtle. In this paper, the primary focus is on recognition of facial micro-expressions and to overcome the class imbalance of the datasets. Firstly, a novel approach that uses multiple magnified ratios of Eulerian motion magnification is applied to the videos to extract the suppressed micro-expressions. Secondly, we remove the micro-expression frames with low textural variance and obtain the Emotion Avatar Image (EAI). Finally, Deep Convolutional Neural Network (CNN) is used to extract robust facial features from the motion magnified EAI images. These features are classified into three different classes: positive, negative and surprise. The approach is evaluated on three spontaneous micro-expression datasets SMIC, SAMM, and CASME II, and the results are compared with the current approaches that show the effectiveness and significance of the approach."
  },
  "cvpr2019_face_and_gesture_analysis_for_health_informatics_improvingsocially-awaremulti-channelhumanemotionpredictionforrobotnavigation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Face_and_Gesture_Analysis_for_Health_Informatics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Face and Gesture Analysis for Health Informatics",
    "title": "Improving Socially-aware Multi-channel Human Emotion Prediction for Robot Navigation",
    "authors": [
      "Aniket Bera",
      "Tanmay Randhavane",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Face_and_Gesture_Analysis_for_Health_Informatics/Bera_Improving_Socially-aware_Multi-channel_Human_Emotion_Prediction_for_Robot_Navigation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Face and Gesture Analysis for Health Informatics/Bera_Improving_Socially-aware_Multi-channel_Human_Emotion_Prediction_for_Robot_Navigation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a real-time algorithm for emotion-aware navigation of a robot among pedestrians. Our approach estimates time-varying emotional behaviors of pedestrians from their faces and trajectories using a combination of Bayesian- inference, CNN-based learning, and the PAD (Pleasure-Arousal- Dominance) model from psychology. These PAD characteristics are used for long-term path prediction and generating proxemic constraints for each pedestrian. We use a multi-channel model to classify pedestrian characteristics into four emotion categories (happy, sad, angry, neutral). In our validation results, we observe an emotion detection accuracy of 85.33%. We formulate emotion-based proxemic constraints to perform socially-aware robot navigation in low- to medium-density environments. We demonstrate the benefits of our algorithm in simulated environments with tens of pedestrians as well as in a real-world setting with Pepper, a social humanoid robot."
  },
  "cvpr2019_face_and_gesture_analysis_for_health_informatics_limitationsandbiasesinfaciallandmarkdetectiondanempiricalstudyonolderadultswithdementia": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Face_and_Gesture_Analysis_for_Health_Informatics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Face and Gesture Analysis for Health Informatics",
    "title": "Limitations and Biases in Facial Landmark Detection D An Empirical Study on Older Adults with Dementia",
    "authors": [
      "Azin Asgarian",
      "Shun Zhao",
      "Ahmed B. Ashraf",
      "M. Erin Browne",
      "Kenneth M. Prkachin",
      "Alex Mihailidis",
      "Thomas Hadjistavropoulos",
      "Babak Taati"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Face_and_Gesture_Analysis_for_Health_Informatics/Asgarian_Limitations_and_Biases_in_Facial_Landmark_Detection_D_An_Empirical_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Face and Gesture Analysis for Health Informatics/Asgarian_Limitations_and_Biases_in_Facial_Landmark_Detection_D_An_Empirical_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate facial expression analysis is an essential step in various clinical applications that involve physical and mental health assessments of older adults (e.g. diagnosis of pain or depression). Although remarkable progress has been achieved toward developing robust facial landmark detection methods, state-of-the-art methods still face many challenges when encountering uncontrolled environments, different ranges of facial expressions, and different demographics of population. A recent study has revealed that the health status of individuals can also affect the performance of facial landmark detection methods on front views of faces. In this work, we investigate this matter in a much greater context using seven facial landmark detection methods. We perform our evaluation not only on frontal faces but also on profile faces and in various regions of the face. Our results shed light on limitations of the existing methods and challenges of applying these methods in clinical settings by indicating: 1) a significant difference between the performance of state-of-the-art when tested on the profile or frontal faces of individuals with vs. without dementia; 2) insights on the existing bias for all regions of the face; and 3) the presence of this bias despite re-training/fine-tuning with various configurations of six datasets."
  },
  "cvpr2019_face_and_gesture_analysis_for_health_informatics_beyonddeepfeatureaveragingsamplingvideostowardspracticalfacialpainrecognition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Face_and_Gesture_Analysis_for_Health_Informatics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Face and Gesture Analysis for Health Informatics",
    "title": "Beyond Deep Feature Averaging: Sampling Videos Towards Practical Facial Pain Recognition",
    "authors": [
      "Xiang Xiang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Face_and_Gesture_Analysis_for_Health_Informatics/Xiang_Beyond_Deep_Feature_Averaging_Sampling_Videos_Towards_Practical_Facial_Pain_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Face and Gesture Analysis for Health Informatics/Xiang_Beyond_Deep_Feature_Averaging_Sampling_Videos_Towards_Practical_Facial_Pain_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In hospitals, automatic identification of patients with cameras can greatly generalize the applicability of intelligent patient monitoring. However, patients unaware of being monitored do not adjust their behaviors, making pose variation a challenge. We argue that the frame-wise feature mean is unable to characterize the variation among frames. We propose to preserve the overall pose diversity if we want the video feature to represent the subject identity. Then identity will be the only source of variation across videos since pose varies even within a single video. Following that variation disentanglement idea, we present a pose-robust face verification algorithm with each video represented as an ensemble of frame-wise CNN features. Another challenge is that patients may move anytime, which makes real-time processing of a video stream a necessity. Instead of simply using all the frames, the algorithm is highlighted at the key frame selection by pose quantization using pose distances to K-means centroids, which reduces the number of feature vectors from hundreds to K while still preserving the overall diversity. We analyze how such a video sampling strategy is better than random sampling. An end-to-end face recognition algorithm is developed for real-time patient identification with a rank-list of one-to-one similarities using the proposed video representation. It works well in practice and generates a private patient dataset on the fly. On the official 5000 video-pairs of public YouTube Face dataset, our algorithm achieves a comparable performance with state-of-the-art that averages over deep features of all frames. In summary, the main contribution of this paper is a video-versus-video consensus with discriminative metric learning on the fly, which is verified in a working system for the patient monitoring system."
  },
  "cvpr2019_face_and_gesture_analysis_for_health_informatics_categoricaltimelineallocationandalignmentfordiagnosticheadmovementtrackingfeatureanalysis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Face_and_Gesture_Analysis_for_Health_Informatics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Face and Gesture Analysis for Health Informatics",
    "title": "Categorical Timeline Allocation and Alignment for Diagnostic Head Movement Tracking Feature Analysis",
    "authors": [
      "Mitsunori Ogihara",
      "Zakia Hammal",
      "Katherine B. Martin",
      "Jeffrey F. Cohn",
      "Justine Cassell",
      "Gang Ren",
      "Daniel S. Messinger"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Face_and_Gesture_Analysis_for_Health_Informatics/Ogihara_Categorical_Timeline_Allocation_and_Alignment_for_Diagnostic_Head_Movement_Tracking_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Face and Gesture Analysis for Health Informatics/Ogihara_Categorical_Timeline_Allocation_and_Alignment_for_Diagnostic_Head_Movement_Tracking_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Atypical head movement pattern characterization is a potentially important cue for identifying children with autism spectrum disorder. In this paper, we implemented a computational framework for extracting the temporal patterns of head movement and utilizing the imbalance of temporal pattern distribution between diagnostic categories (e.g., children with or without autism spectrum disorder) as potential diagnostic cues. The timeline analysis results show a large number of temporal patterns with significant imbalances between diagnostic categories. The temporal patterns show strong classification power on discriminative and predictive analysis metrics. The long time-span temporal patterns (e.g., patterns spanning 15-30 sec.) exhibit stronger discriminative capabilities compared with the temporal patterns with relatively shorter time spans. Temporal patterns with high coverage ratios (existing in a large portion of the video durations) also show high discriminative capacity."
  },
  "cvpr2019_sumo_convolutionsonsphericalimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SUMO",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The 2019 Scene Understanding and Modeling (SUMO) Workshop",
    "title": "Convolutions on Spherical Images",
    "authors": [
      "Marc Eder",
      "Jan-Michael Frahm"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SUMO/Eder_Convolutions_on_Spherical_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SUMO/Eder_Convolutions_on_Spherical_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Applying convolutional neural networks to spherical images requires particular considerations. We look to the millennia of work on cartographic map projections to provide the tools to define an optimal representation of spherical images for the convolution operation. We propose a representation for deep spherical image inference based on the icosahedral Snyder equal-area (ISEA) projection, a projection onto a geodesic grid, and show that it vastly exceeds the state-of-the-art for convolution on spherical images, improving semantic segmentation results by 12.6%."
  },
  "cvpr2019_sumo_kerneltransformernetworksforcompactsphericalconvolution": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SUMO",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The 2019 Scene Understanding and Modeling (SUMO) Workshop",
    "title": "Kernel Transformer Networks for Compact Spherical Convolution",
    "authors": [
      "Yu-Chuan Su",
      "Kristen Grauman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SUMO/Su_Kernel_Transformer_Networks_for_Compact_Spherical_Convolution_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SUMO/Su_Kernel_Transformer_Networks_for_Compact_Spherical_Convolution_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Ideally, 360deg imagery could inherit the convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. We present the Kernel Transformer Network (KTN) to efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360deg images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360deg image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. KTNs successfully preserve the source CNN's accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint."
  },
  "cvpr2019_sumo_learningsingle-view3dreconstructionwithlimitedposesupervision": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SUMO",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The 2019 Scene Understanding and Modeling (SUMO) Workshop",
    "title": "Learning Single-View 3D Reconstruction with Limited Pose Supervision",
    "authors": [
      "Guandao Yang",
      "Yin Cui",
      "Serge Belongie",
      "Bharath Hariharan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SUMO/Yang_Learning_Single-View_3D_Reconstruction_with_Limited_Pose_Supervision_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SUMO/Yang_Learning_Single-View_3D_Reconstruction_with_Limited_Pose_Supervision_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " It is expensive to label images with 3D structure or precise camera pose. Yet, this is precisely the kind of annotation required to train single-view 3D reconstruction models. In contrast, unlabeled images or images with just category labels are easy to acquire, but few current models can use this weak supervision. We present a unified framework that can combine both types of supervision: a small amount of camera pose annotations are used to enforce pose-invariance and view-point consistency, and unlabeled images combined with an adversarial loss are used to enforce the realism of rendered, generated models. We use this unified framework to measure the impact of each form of supervision in three paradigms: semi-supervised, multi-task, and transfer learning. We show that with a combination of these ideas, we can train single-view reconstruction models that improve up to 7 points in performance (AP) when using only 1% pose annotated training data."
  },
  "cvpr2019_sumo_multi-planarmonocularreconstructionofmanhattanindoorscenes": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SUMO",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The 2019 Scene Understanding and Modeling (SUMO) Workshop",
    "title": "Multi-planar Monocular Reconstruction of Manhattan Indoor Scenes",
    "authors": [
      "Seongdo Kim",
      "Roberto Manduchi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SUMO/Kim_Multi-planar_Monocular_Reconstruction_of_Manhattan_Indoor_Scenes_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SUMO/Kim_Multi-planar_Monocular_Reconstruction_of_Manhattan_Indoor_Scenes_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a novel algorithm for geometry and camera pose reconstruction from image sequences that is specialized for indoor Manhattan scenes. Unlike general-purpose SfM/SLAM, our system represents geometric primitives in terms of canonically oriented planes. The algorithm starts by computing multi-planar segmentation and motion estimation from image pairs using constrained homographies. It then proceeds to recover the relative scale at each frame and to determine chains of match clusters, where each cluster is associated with a plane in the scene. Motion and scene geometry (expressed in terms of planar models) are then optimized using a novel formulation of Bundle Adjustment. Compared with other state-of-the-art SfM/SLAM algorithms, our technique is shown to produce superior and realistic surface reconstruction for a monocular indoor scene."
  },
  "cvpr2019_sumo_multi-layerdepthandepipolarfeaturetransformersfor3dscenereconstruction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SUMO",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The 2019 Scene Understanding and Modeling (SUMO) Workshop",
    "title": "Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction",
    "authors": [
      "Daeyun Shin",
      "Zhile Ren",
      "Erik B. Sudderth",
      "Charless C. Fowlkes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SUMO/Shin_Multi-layer_Depth_and_Epipolar_Feature_Transformers_for_3D_Scene_Reconstruction_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SUMO/Shin_Multi-layer_Depth_and_Epipolar_Feature_Transformers_for_3D_Scene_Reconstruction_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel \"Epipolar Feature Transformer\" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes. "
  },
  "cvpr2019_sumo_efficientplane-basedoptimizationofgeometryandtextureforindoorrgb-dreconstruction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "SUMO",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - The 2019 Scene Understanding and Modeling (SUMO) Workshop",
    "title": "Efficient Plane-Based Optimization of Geometry and Texture for Indoor RGB-D Reconstruction",
    "authors": [
      "Chao Wang",
      "Xiaohu Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/SUMO/Wang_Efficient_Plane-Based_Optimization_of_Geometry_and_Texture_for_Indoor_RGB-D_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/SUMO/Wang_Efficient_Plane-Based_Optimization_of_Geometry_and_Texture_for_Indoor_RGB-D_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a novel approach to reconstruct RGB-D indoor scene based on plane primitives. Our approach takes as input a RGB-D sequence and a dense coarse mesh reconstructed from it, and generates a lightweight, low-polygonal mesh with clear face textures and sharp features without losing geometry details from the original scene. Compared to existing methods which only cover large planar regions in the scene, our method builds the entire scene by adaptive planes without losing geometry details and also preserves sharp features in the mesh. Experiments show that our method is more efficient to generate textured mesh from RGB-D data than state-of-the-arts."
  },
  "cvpr2019_media_forensics_acoarse-to-finedeepconvolutionalneuralnetworkframeworkforframeduplicationdetectionandlocalizationinforgedvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "A Coarse-to-fine Deep Convolutional Neural Network Framework for Frame Duplication Detection and Localization in Forged Videos",
    "authors": [
      "Chengjiang Long",
      "Arslan Basharat",
      "Anthony Hoogs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Long_A_Coarse-to-fine_Deep_Convolutional_Neural_Network_Framework_for_Frame_Duplication_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Long_A_Coarse-to-fine_Deep_Convolutional_Neural_Network_Framework_for_Frame_Duplication_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Videos can be manipulated by duplicating a sequence of consecutive frames with the goal of concealing or imitating a specific content in the same video. In this paper, we propose a novel coarse-to-fine framework based on deep Convolutional Neural Networks to automatically detect and localize such frame duplication. First an I3D network finds coarse-level matches between candidate duplicated frame sequences and the corresponding selected original frame sequences. Then a Siamese network based on ResNet architecture identifies fine-level correspondences between an individual duplicated frame and the corresponding selected frame. We also propose a robust statistical approach to compute a video-level score indicating the likelihood of manipulation or forgery. Additionally, for providing manipulation localization information we develop an inconsistency detector based on the I3D network to distinguish the duplicated frames from the selected original frames. Quantified evaluation on two challenging video forgery datasets clearly demonstrates that this approach performs significantly better than four recent state-of-the-art methods."
  },
  "cvpr2019_media_forensics_robusthomomorphicimagehashing": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Robust Homomorphic Image Hashing",
    "authors": [
      "Priyanka Singh",
      "Hany Farid"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Singh_Robust_Homomorphic_Image_Hashing_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Singh_Robust_Homomorphic_Image_Hashing_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Most image forensic techniques are concerned with authenticating the contents of an image, linking an image to a device or class of devices, or extracting forensically useful information from an image. Another aspect of image forensics is the identification of previously identified content, particularly in the face of simple image modifications. Such so-called robust image hashing techniques can be highly effective at finding child sexual abuse material, revenge porn, terrorism-related material, and dangerous or hateful conspiracy material. The growing use of end-to-end encryption on commercial platforms makes identification of such material significantly more challenging. We describe a robust image hashing algorithm that is both robust to simple image manipulations and that can operate on an encrypted image, without the need or even ability to decipher the underlying encrypted image."
  },
  "cvpr2019_media_forensics_applicationofdensenetincameramodelidentificationandpost-processingdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Application of DenseNet in Camera Model Identification and Post-processing Detection",
    "authors": [
      "Abdul Muntakim Rafi",
      "Uday Kamal",
      "Rakibul Hoque",
      "Abid Abrar",
      "Sowmitra Das",
      "Robert Laganiere",
      "Md. Kamrul Hasan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Rafi_Application_of_DenseNet_in_Camera_Model_Identification_and_Post-processing_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Rafi_Application_of_DenseNet_in_Camera_Model_Identification_and_Post-processing_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Camera model identification has earned paramount importance in the field of image forensics with an upsurge of digitally altered images which are constantly being shared through websites, media, and social applications. But, the task of identification becomes quite challenging if metadata are absent from the image and/or if the image has been post-processed. In this paper, we present a DenseNet pipeline to solve the problem of identifying the source camera-model of an image. Our approach is to extract patches of 256 x 256 from a labeled image dataset and apply augmentations, i.e., Empirical Mode Decomposition (EMD). We use this extended dataset to train a Neural Network with the DenseNet-201 architecture. We concatenate the output features for 3 different sizes (64x64, 128x128, 256x256) and pass them to a secondary network to make the final prediction. This strategy proves to be very robust for identifying the source camera model, even when the original image is post-processed. Our model has been trained and tested on the Forensic Camera-Model Identification Dataset provided for the IEEE Signal Processing (SP) Cup 2018. During testing we achieved an overall accuracy of 98.37%, which is the current state-of-the-art on this dataset using a single model. We used transfer learning and tested our model on the Dresden Database for Camera Model Identification, with an overall test accuracy of over 99% for 19 models. In addition, we demonstrate that the proposed pipeline is suit- able for other image-forensic classification tasks, such as, detecting the type of post-processing applied to an image with an accuracy of 96.66% - which indicates the generality of our approach."
  },
  "cvpr2019_media_forensics_manipulationdatacollectionandannotationtoolformediaforensics": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Manipulation Data Collection and Annotation Tool for Media Forensics",
    "authors": [
      "Eric Robertson",
      "Haiying Guan",
      "Mark Kozak",
      "Yooyoung Lee",
      "Amy N. Yates",
      "Andrew Delgado",
      "Daniel Zhou",
      "Timothee Kheyrkhah",
      "Jeff Smith",
      "Jonathan Fiscus"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Robertson_Manipulation_Data_Collection_and_Annotation_Tool_for_Media_Forensics_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Robertson_Manipulation_Data_Collection_and_Annotation_Tool_for_Media_Forensics_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With the increasing diversity and complexity of media forensics techniques, the evaluation of state-of-the-art detectors are impeded by lacking the metadata and manipulation history ground-truth. This paper presents a novel image/video manipulation Journaling Tool (JT) that automatically or semi-automatically helps a media manipulator record, or journal, the steps, methods, and tools used to manipulate media into a modified form. JT is a unified framework using a directed acyclic graph representation to support: recording the manipulation history (journal); automating the collection of operation- specific localization masks identifying the set of manipulated pixels; integrating annotations and metadata collection; and execution of automated manipulation tools to extend existing journals or automatically build new journals. Using JT to support the 2017 and 2018 Media Forensics Challenge (MFC) evaluations, a large collection of image manipulations was assembled that included a variety of different manipulation operations across image, video, and audio. To date, the MFC's media manipulation team has collected more than 4500 human-manipulated image journals containing over 100,000 images, more than 400 manipulated video journals containing over 4,000 videos, and generated thousands of extended journals and hundreds of auto-manipulated journals. This paper discusses the JT's design philosophy and requirements, localization mask production, automated journal construction tools, and evaluation data derivation from journals for performance evaluation of media forensics applications. JT enriches the metadata collection, provides consistent and detailed annotations, and builds scalable automation tools to produce manipulated media, which enables the research community to better understand the problem domain and the algorithm models."
  },
  "cvpr2019_media_forensics_protectingworldleadersagainstdeepfakes": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Protecting World Leaders Against Deep Fakes",
    "authors": [
      "Shruti Agarwal",
      "Hany Farid",
      "Yuming Gu",
      "Mingming He",
      "Koki Nagano",
      "Hao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Agarwal_Protecting_World_Leaders_Against_Deep_Fakes_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Agarwal_Protecting_World_Leaders_Against_Deep_Fakes_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The creation of sophisticated fake videos has been largely relegated to Hollywood studios or state actors. Recent advances in deep learning, however, have made it significantly easier to create sophisticated and compelling fake videos. With relatively modest amounts of data and computing power, the average person can, for example, create a video of a world leader confessing to illegal activity leading to a constitutional crisis, a military leader saying something racially insensitive leading to civil unrest in an area of military activity, or a corporate titan claiming that their profits are weak leading to global stock manipulation. These so called deep fakes pose a significant threat to our democracy, national security, and society. To contend with this growing threat, we describe a forensic technique that models facial expressions and movements that typify an individual's speaking pattern. Although not visually apparent, these correlations are often violated by the nature of how deep-fake videos are created and can, therefore, be used for authentication. "
  },
  "cvpr2019_media_forensics_exposingdeepfakevideosbydetectingfacewarpingartifacts": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
    "authors": [
      "Yuezun Li",
      "Siwei Lyu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Li_Exposing_DeepFake_Videos_By_Detecting_Face_Warping_Artifacts_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Li_Exposing_DeepFake_Videos_By_Detecting_Face_Warping_Artifacts_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as DeepFake videos hereafter) from real videos. Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video. Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs). Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classifier, our method does not need DeepFake generated images as negative training examples since we target the artifacts in affine face warping as the distinctive feature to distinguish real and fake images. The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example. Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others. Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice. "
  },
  "cvpr2019_media_forensics_cnn-basedsystemforspeakerindependentcell-phoneidentificationfromrecordedaudio": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "CNN-based System for Speaker Independent Cell-Phone Identification from Recorded Audio",
    "authors": [
      "Vinay Verma",
      "Nitin Khanna"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Verma_CNN-based_System_for_Speaker_Independent_Cell-Phone_Identification_from_Recorded_Audio_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Verma_CNN-based_System_for_Speaker_Independent_Cell-Phone_Identification_from_Recorded_Audio_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper proposes a cell-phone identification system independent of speech content as well as the speaker. Audio recorded from a cell-phone contains specific signatures corresponding to that cell-phone. These unique signatures of the cell-phone implicitly captured in the recorded audio can be utilized to identify the cell-phone. These signatures of a cell-phone obtained from the recorded audio are visually more distinct in the frequency domain than in the time domain signal. Thus, by utilizing the distinctiveness of the signatures in the frequency domain and learning capability of the Convolutional Neural Network (CNN), we propose a system which learns unique signatures of the cell-phones from the frequency domain representation of the audio. In particular, we have used the magnitude of the Discrete Fourier Transform (DFT) as the frequency representation of an audio signal. An extensive set of experiments performed on a large duration dataset shows that the proposed system outperforms the existing state-of-the- art systems, notably in the cases where recordings used for training and testing the systems contain mutually exclusive audio content as well as speakers."
  },
  "cvpr2019_media_forensics_anomaly-basedmanipulationdetectioninsatelliteimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Anomaly-Based Manipulation Detection in Satellite Images",
    "authors": [
      "Janos Horvath",
      "David Guera",
      "Sri Kalyan Yarlagadda",
      "Paolo Bestagini",
      "Fengqing Maggie Zhu",
      "Stefano Tubaro",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Horvath_Anomaly-Based_Manipulation_Detection_in_Satellite_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Horvath_Anomaly-Based_Manipulation_Detection_in_Satellite_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Satellite overhead imagery can be easily acquired and shared. The integrity of these type of images cannot longer be assumed, due to availability of sophisticated classical and machine learning based image manipulation tools. In this paper we proposed a deep learning based method for detecting and localizing splicing manipulations in overhead images. Our method uses recent advances in anomaly detection and does not require any prior knowledge of the type of manipulations that an adversary could insert in the satellite imagery. We compare our method against robust satellite-based manipulation detection approaches. We show that our proposed technique outperforms all previous methods, especially in detecting small-sized manipulations."
  },
  "cvpr2019_media_forensics_spliceradaralearnedmethodforblindimageforensics": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "SpliceRadar: A Learned Method For Blind Image Forensics",
    "authors": [
      "Aurobrata Ghosh",
      "Zheng Zhong",
      "Terrance E Boult",
      "Maneesh Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Ghosh_SpliceRadar_A_Learned_Method_For_Blind_Image_Forensics_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Ghosh_SpliceRadar_A_Learned_Method_For_Blind_Image_Forensics_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Detection and localization of image manipulations like splices are gaining in importance with the easy accessibility to image editing softwares. While detection generates a verdict for an image it provides no insight into the manipulation. Localization helps explain a positive detection by identifying the pixels of the image which have been tampered. We propose a deep learning based method for splice localization without prior knowledge of a test image's camera-model. It comprises a novel approach for learning rich filters and for suppressing image-edges. Additionally, we train our model on a surrogate task of camera model identification, which allows us to leverage large and widely available, unmanipulated, camera-tagged image databases. During inference, we assume that the spliced and host regions come from different camera-models and we segment these regions using a Gaussian-mixture model. Experiments on three test databases demonstrate results on par with and above the state-of-the-art and a good generalization ability to unknown datasets."
  },
  "cvpr2019_media_forensics_recurrentconvolutionalstrategiesforfacemanipulationdetectioninvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Recurrent Convolutional Strategies for Face Manipulation Detection in Videos",
    "authors": [
      "Ekraam Sabir",
      "Jiaxin Cheng",
      "Ayush Jaiswal",
      "Wael AbdAlmageed",
      "Iacopo Masi",
      "Prem Natarajan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Sabir_Recurrent_Convolutional_Strategies_for_Face_Manipulation_Detection_in_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Sabir_Recurrent_Convolutional_Strategies_for_Face_Manipulation_Detection_in_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The spread of misinformation through synthetically generated yet realistic images and videos has become a significant problem, calling for robust manipulation detection methods. Despite the predominant effort of detecting face manipulation in still images, less attention has been paid to the identification of tampered faces in videos by taking advantage of the temporal information present in the stream. Recurrent convolutional models are a class of deep learning models which have proven effective at exploiting the temporal information from image streams across domains. We thereby distill the best strategy for combining variations in these models along with domain specific face preprocessing techniques through extensive experimentation to obtain state-of-the-art performance on publicly available video- based facial manipulation benchmarks. Specifically, we attempt to detect Deepfake, Face2Face and FaceSwap tampered faces in video streams. Evaluation is performed on the recently introduced FaceForensics++ dataset, improving the previous state-of-the-art by up to 4.55% in accuracy."
  },
  "cvpr2019_media_forensics_onlinesignatureverificationbasedonwriterspecificfeatureselectionandfuzzysimilaritymeasure": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Online Signature Verification Based on Writer Specific Feature Selection and Fuzzy Similarity Measure",
    "authors": [
      "Chandra Sekhar",
      "Prerna M",
      "Guru D",
      "Viswanath P"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Sekhar_Online_Signature_Verification_Based_on_Writer_Specific_Feature_Selection_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Sekhar_Online_Signature_Verification_Based_on_Writer_Specific_Feature_Selection_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Online Signature Verification (OSV) is a widely used biometric attribute for user behavioral characteristic verification in digital forensics. In this manuscript, owing to large intra-individual variability, a novel method for OSV based on an interval symbolic representation and a fuzzy similarity measure grounded on writer specific parameter selection is proposed. The two parameters, namely, writer specific acceptance threshold and optimal feature set to be used for authenticating the writer are selected based on minimum equal error rate (EER) attained during parameter fixation phase using the training signature samples. This is in variation to current techniques for OSV, which are primarily writer independent, in which a common set of features and acceptance threshold are chosen. To prove the robustness of our system, we have exhaustively assessed our system with four standard datasets i.e. MCYT-100 (DB1), MCYT-330 (DB2), SUSIG-Visual corpus and SVC-2004- Task2. Experimental outcome confirms the effectiveness of fuzzy similarity metric-based writer dependent parameter selection for OSV by achieving a lower error rate as compared to many recent and state-of-the art OSV models."
  },
  "cvpr2019_media_forensics_sourcegeneratorattributionviainversion": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Source Generator Attribution via Inversion",
    "authors": [
      "Michael Albright",
      "Scott McCloskey"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Albright_Source_Generator_Attribution_via_Inversion_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Albright_Source_Generator_Attribution_via_Inversion_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With advances in Generative Adversarial Networks (GANs) leading to dramatically-improved synthetic images and video, there is an increased need for algorithms which extend traditional forensics to this new category of imagery. While GANs have been shown to be helpful in a number of computer vision applications, there are other problematic uses such as 'deep fakes' which necessitate such forensics. Source camera attribution algorithms using various cues have addressed this need for imagery captured by a camera, but there are fewer options for synthetic imagery. We address the problem of attributing a synthetic image to a specific generator in a white box setting, by inverting the process of generation. This enables us to simultaneously determine whether the generator produced the image and recover an input which produces a close match to the synthetic image."
  },
  "cvpr2019_media_forensics_detectingai-synthesizedspeechusingbispectralanalysis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Detecting AI-Synthesized Speech Using Bispectral Analysis",
    "authors": [
      "Ehab A. AlBadawy",
      "Siwei Lyu",
      "Hany Farid"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/AlBadawy_Detecting_AI-Synthesized_Speech_Using_Bispectral_Analysis_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/AlBadawy_Detecting_AI-Synthesized_Speech_Using_Bispectral_Analysis_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " From speech to images, and videos, advances in machine learning have led to dramatic improvements in the quality and realism of so-called AI-synthesized content. While there are many exciting and interesting applications, this type of content can also be used to create convincing and dangerous fakes. We seek to develop forensic techniques that can distinguish a real human voice from synthesized voice. We observe that deep neural networks used to synthesize speech introduce specific and unusual spectral correlations not typically found in human speech. Although not necessarily audible, these correlations can be measured using tools from bispectral analysis and used to distinguish human from synthesized speech."
  },
  "cvpr2019_media_forensics_jpeggriddetectionbasedonthenumberofdctzerosanditsapplicationtoautomaticandlocalizedforgerydetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "JPEG Grid Detection based on the Number of DCT Zeros and its Application to Automatic and Localized Forgery Detection",
    "authors": [
      "T. Nikoukhah",
      "J. Anger",
      "T. Ehret",
      "M. Colom",
      "J.M. Morel",
      "R. Grompone von Gioi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Nikoukhah_JPEG_Grid_Detection_based_on_the_Number_of_DCT_Zeros_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Nikoukhah_JPEG_Grid_Detection_based_on_the_Number_of_DCT_Zeros_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This work proposes a novel method for detecting JPEG compression, as well as its grid origin, based on counting the number of zeros in the DCT of 8 x 8 blocks. When applied locally, the same method can be used to detect grid alignment abnormalities. It therefore detects local image forgeries such as copy-move. The algorithm includes a statistical validation step which gives theoretical guarantees on the number of false alarms and provides secure guarantees for tampering detection. The performance of the proposed method is illustrated with both quantitative and visual results from well-known image databases and comparisons with state of the art methods."
  },
  "cvpr2019_media_forensics_askipconnectionarchitectureforlocalizationofimagemanipulations": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "A Skip Connection Architecture for Localization of Image Manipulations",
    "authors": [
      "Ghazal Mazaheri",
      "Niluthpol Chowdhury Mithun",
      "Jawadul H. Bappy",
      "Amit K. Roy-Chowdhury"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Mazaheri_A_Skip_Connection_Architecture_for_Localization_of_Image_Manipulations_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Mazaheri_A_Skip_Connection_Architecture_for_Localization_of_Image_Manipulations_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Detection and localization of image manipulations are becoming of increasing interest to researchers in recent years due to the significant rise of malicious content- changing image tampering on the web. One of the major challenges for an image manipulation detection method is to discriminate between the tampered regions and other regions in an image. We observe that most of the manipulated images leave some traces near boundaries of manipulated regions including blurred edges. In order to exploit these traces in localizing the tampered regions, we propose an encoder-decoder based network where we fuse representations from early layers in the encoder (which are richer in low-level spatial cues, like edges) by skip pooling with representations of the last layer of the decoder and use for manipulation detection. In addition, we utilize resampling features extracted from patches of images by feeding them to LSTM cells to capture the transition between manipulated and non-manipulated blocks in the frequency domain and combine the output of the LSTM with our encoder. The overall framework is capable of detecting different types of image manipulations simultaneously including copy-move, removal, and splicing. Experimental results on two standard benchmark datasets (CASIA 1.0 and NIST'16) demonstrate that the proposed method can achieve a significantly better performance than the state-of-the-art methods and baselines."
  },
  "cvpr2019_media_forensics_extractingcamera-basedfingerprintsforvideoforensics": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Extracting camera-based fingerprints for video forensics",
    "authors": [
      "Davide Cozzolino Giovanni Poggi Luisa Verdoliva"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Verdoliva_Extracting_camera-based_fingerprints_for_video_forensics_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Verdoliva_Extracting_camera-based_fingerprints_for_video_forensics_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Video source attribution is an important operation in forensics applications. Identifying which specific device or camera model took a video can help in authorship verification, but can be also a precious source of information for detecting a possible manipulation. The key observation is that any physical device leaves peculiar traces in the acquired content, a sort of fingerprint that can be exploited to establish data provenance. Moreover, absence or modification of such traces may reveal a possible manipulation. In this paper, inspired by recent work on images, we train a neural network that enhances the model-related traces hidden in a video, extracting a sort of camera fingerprint, called video noiseprint. The net is trained on pristine videos with a Siamese strategy, minimizing distances between same-model patches, and maximizing distances between unrelated patches. Experiments show that methods based on video noiseprints perform well in major forensic tasks, such as camera model identification and video forgery localization, with no need of prior knowledge on the specific manipulation or any form of fine-tuning."
  },
  "cvpr2019_media_forensics_detectingimageforgerybasedoncolorphenomenology": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Detecting Image Forgery Based On Color Phenomenology",
    "authors": [
      "Jamie Stanton",
      "Keigo Hirakawa",
      "Scott McCloskey"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Stanton_Detecting_Image_Forgery_Based_On_Color_Phenomenology_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Stanton_Detecting_Image_Forgery_Based_On_Color_Phenomenology_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose White Point-Illuminant Consistency (WPIC) algorithm that detects manipulations in images based on the phenomenology of color. Segmented regions of the image are converted to chromaticity coordinates and compared to the white point reported in the camera's EXIF file. In manipulated images, the chromaticity coordinates will have a shifted illuminant color relative to the EXIF-reported white point. Absent manipulation, chromaticity coordinates will be in agreement with the specified white point. We detect image manipulations using a convolutional neural net- work operating on a histogram of relevant statistics that indicate the white point shift. We verify this using a real world data set to demonstrate its effectiveness."
  },
  "cvpr2019_media_forensics_classificationofcomputergeneratedandnaturalimagesbasedonefficientdeepconvolutionalrecurrentattentionmodel": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Media_Forensics",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Media Forensics",
    "title": "Classification of Computer Generated and Natural Images based on Efficient Deep Convolutional Recurrent Attention Model",
    "authors": [
      "Diangarti Bhalang Tarianga",
      "Prithviraj Senguptab",
      "Aniket Roy",
      "Rajat Subhra Chakraborty",
      "Ruchira Naskar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Media_Forensics/Tarianga_Classification_of_Computer_Generated_and_Natural_Images_based_on_Efficient_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Media Forensics/Tarianga_Classification_of_Computer_Generated_and_Natural_Images_based_on_Efficient_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Most state-of-the-art techniques of distinguishing natural images and computer generated images based on hand-crafted feature and Convolutional Neural Network require processing of the entire input image pixels uniformly. As a result, such techniques usually require extensive computation time and memory, that scale linearly with the size of the input image in terms of number of pixels. In this paper, we deploy an efficient Deep Convolutional Recurrent Attention model with relatively less number of parameters, to distinguish between natural and computer generated images. The proposed model uses a glimpse network to locally process a sequence of selected image regions; hence, the number of parameters and computation time can be controlled effectively. We also adopt a local-to-global strategy by training image patches and classifying full-sized images using the simple majority voting rule. The proposed approach achieves superior classification accuracy compared to recently proposed approaches based on deep learning."
  },
  "cvpr2019_ai_city_vehiclenetlearningrobustfeaturerepresentationforvehiclere-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "VehicleNet: Learning Robust Feature Representation for Vehicle Re-identification",
    "authors": [
      "Zhedong Zheng",
      "Tao Ruan",
      "Yunchao Wei",
      "Yi Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Zheng_VehicleNet_Learning_Robust_Feature_Representation_for_Vehicle_Re-identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Zheng_VehicleNet_Learning_Robust_Feature_Representation_for_Vehicle_Re-identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. In this paper, we present our solution to AICity Vehicle Re-id Challenge 2019. The limited training data motivates us to leverage the free data from the web and deploy the two-stage learning strategy. The success of large-scale datasets, i.e., ImageNet, inspires us to build a large-scale vehicle dataset called VehicleNet upon the public web data. Specifically, we combine the provided training set with other public vehicle datasets, i.e., VeRi-776, CompCar and VehicleID as VehicleNet. In the first stage, the training set is scaled up about 16 times, from 26,803 to 434,453 images. Despite the bias between different datasets, e.g., illumination and scene, VehicleNet generally provides the common knowledge of the vehicle, benefiting the deeply-learned model in learning the invariant representation towards different viewpoints. In the second stage, we further fine-tune the trained model only on the original training set. The second stage intends to minor the gap between VehicleNet and the original training set. Albeit simple, we achieve mAP 75.60% on the private testing set without extra information, e.g., temporal or spatial annotation of test data. "
  },
  "cvpr2019_ai_city_challengesintime-stampawareanomalydetectionintrafficvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Challenges in Time-Stamp Aware Anomaly Detection in Traffic Videos",
    "authors": [
      "Kuldeep Marotirao Biradar",
      "Ayushi Gupta",
      "Murari Mandal",
      "Santosh Kumar Vipparthi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Biradar_Challenges_in_Time-Stamp_Aware_Anomaly_Detection_in_Traffic_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Biradar_Challenges_in_Time-Stamp_Aware_Anomaly_Detection_in_Traffic_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Time-stamp aware anomaly detection in traffic videos is an essential task for the advancement of intelligent transportation system. Anomaly detection in videos is a challenging problem due to sparse occurrence of anomalous events, inconsistent behavior of different type of anomalies and imbalanced available data for normal and abnormal scenarios. In this paper we present a three-stage pipeline to learn the motion patterns in videos to detect visual anomaly. First, the background is estimated from recent history frames to identify the motionless objects. This background image is used to localize the normal/abnormal behavior within the frame. Further, we detect object of interest in the estimated background and categorize it into anomaly based on a time-stamp aware anomaly detection algorithm. We also discuss the challenges faced in improving performance over the unseen test data for traffic anomaly detection. Experiments are conducted over Track 3 of NVIDIA AI city challenge 2019. The results show the effectiveness of the proposed method in detecting time-stamp aware anomalies in traffic/road videos. "
  },
  "cvpr2019_ai_city_multiviewvehicletrackingbygraphmatchingmodel": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Multiview Vehicle Tracking by Graph Matching Model",
    "authors": [
      "Minye Wu",
      "Guli Zhang",
      "Ning Bi",
      "Ling Xie",
      "Yuanquan Hu",
      "Zhiru Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Wu_Multiview_Vehicle_Tracking_by_Graph_Matching_Model_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Wu_Multiview_Vehicle_Tracking_by_Graph_Matching_Model_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Using multiple visual cameras to sensing traffic, especially tracking of vehicles, is a challenging task because of the large number of vehicle models, non-overlapping views, occlusion, viewchange and time-consuming algorithms. All of them remain obstacles in real world deployment. In this work, we propose a novel and flexible vehicle tracking framework, which formulates matching problem as a graph matching problem and solve it from the bottom up. In our framework, many restrictions can be added into the graph uniformly and simply. Moreover, we introduced an iterative Graph Matching Solver algorithm which can divide and reduce the graph matching problem's scale efficiently. Additionally, We also take the advantage of geographic information and make a combination with deep ReID features, motion and temporal information. The result shows that our algorithm achieves a 9th place at the AI City Challenge 2019."
  },
  "cvpr2019_ai_city_supervisedjointdomainlearningforvehiclere-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Supervised Joint Domain Learning for Vehicle Re-Identification",
    "authors": [
      "Chih-Ting Liu",
      "Man-Yu Lee",
      "Chih-Wei Wu",
      "Bo-Ying Chen",
      "Tsai-Shien Chen",
      "Yao-Ting Hsu",
      "Shao-Yi Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Liu_Supervised_Joint_Domain_Learning_for_Vehicle_Re-Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Liu_Supervised_Joint_Domain_Learning_for_Vehicle_Re-Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle Re-Identification (Re-ID), which aims at matching vehicle identities across different cameras, is a critical technique for traffic analysis in a smart city. It suffers from varying image quality and challenging visual appearance characteristics. A solution for enhancing the feature robustness is by training Convolutional Neural Networks on multiple datasets simultaneously. However, the larger set of training data does not guarantee performance improvement due to misaligned feature distribution between domains. To mitigate the domain gap, we propose a Joint Domain Re-Identification Network (JDRN) to improve the feature by disentangling domain-invariant information and encourage a shared feature space between domains. With our JDRN, we perform favorably against state-of-the-arts methods on the public VeRi-776 dataset and obtain promising results on the 2019 AI City Challenge."
  },
  "cvpr2019_ai_city_multi-taskmutuallearningforvehiclere-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Multi-Task Mutual Learning for Vehicle Re-Identification",
    "authors": [
      "Aytac Kanaci",
      "Minxian Li",
      "Shaogang Gong",
      "Georgia Rajamanoharan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Kanaci_Multi-Task_Mutual_Learning_for_Vehicle_Re-Identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Kanaci_Multi-Task_Mutual_Learning_for_Vehicle_Re-Identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle re-identification (Re-ID) aims to search a specific vehicle instance across non-overlapping camera views. The main challenge of vehicle Re-ID is that the visual appearance of vehicles may drastically changes according to diverse viewpoints and illumination. Most existing vehicle Re-ID models cannot make full use of various complementary vehicle information, e.g. vehicle type and orientation. In this paper, we propose a novel Multi-Task Mutual Learning (MTML) deep model to learn discriminative features simultaneously from multiple branches.Specifically, we design a consensus learning loss function by fusing features from the final convolutional feature maps from all branches. Extensive comparative evaluations demonstrate the effectiveness of our proposed MTML method in comparison to the state-of-the-art vehicle Re-ID techniques on a large-scale benchmark dataset, VeRi-776. We also yield competitive performance on the NVIDIA 2019 AI City Challenge Track 2."
  },
  "cvpr2019_ai_city_deepfeaturefusionwithmultiplegranularityforvehiclere-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Deep Feature Fusion with Multiple Granularity for Vehicle Re-identification",
    "authors": [
      "Peixiang Huang",
      "Runhui Huang",
      "Jianjie Huang",
      "Rushi Yangchen",
      "Zongyao He",
      "Xiying Li",
      "Junzhou Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Huang_Deep_Feature_Fusion_with_Multiple_Granularity_for_Vehicle_Re-identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Huang_Deep_Feature_Fusion_with_Multiple_Granularity_for_Vehicle_Re-identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle re-identification (Re-Id) plays a significant role in modern life. We found that Vehicle Re-Id and Person Re-Id are two very similar tasks in the field of Re-Id. To some extent, the Person Re-Id Networks can be transplanted to the Vehicle Re-Id tasks.In this paper, a Deep Feature Fusion with Multiple Granularity (DFFMG) method for Vehicle Re-Id is proposed for integrating discriminative information with various granularity. DFFMG is based on the Multiple Granularity Network (MGN), the state-of-the-art method from Person Re-Id. We pondered on the discrimination between Vehicle Re-Id and Person Re-Id. And we carefully designed DFFMG: a multi-branch deep network architecture which consists of one branch for global feature representations, two for vertical local feature representations and other two horizontal ones. Besides, several re-ranking methods were tested in our experiments and achieved higher scores. This network is adopted to train and test on the 2019 NVIDIA AI City Dataset [16]"
  },
  "cvpr2019_ai_city_aicitychallenge2019--city-scalevideoanalyticsforsmarttransportation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "AI City Challenge 2019 -- City-Scale Video Analytics for Smart Transportation",
    "authors": [
      "Ming-Ching Chang",
      "Jiayi Wei",
      "Zheng-An Zhu",
      "Yan-Ming Chen",
      "Chan-Shuo Hu",
      "Ming-Xiu Jiang",
      "Chen-Kuo Chiang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Chang_AI_City_Challenge_2019_--_City-Scale_Video_Analytics_for_Smart_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Chang_AI_City_Challenge_2019_--_City-Scale_Video_Analytics_for_Smart_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Understanding large-scale video traffic big data is the new frontier of today's AI smart transportation advancement. The AI City Challenge 2019 is the third sequel of a yearly event that draws significantly growing attention and participation. This paper presents works contributed to the three Challenges Tracks. In Track 1 City-Scale Multi-Camera Vehicle Tracking, we developed a new multi-camera fusion method by extending the state-of-the-art single-camera tracking-by-detection with site calibrations. Our approach jointly optimizes the matching of vehicle image features and geometrical factors including trajectory continuity, vehicle moving directions and travel duration across views, to effectively fuse tracks and identify vehicles across 40+ cameras in a city-wide scale. In Track 2 City-Scale Multi-Camera Vehicle Re-Identification, we propose a Pyramid Granularity Attentive Model (PGAM) for ReID by improving the recent Region-Aware deep Model (RAM) with a pyramid design and training strategy improvements. In Track 3 Traffic Anomaly Detection, we improved the 2nd-best method from AIC2018 with refined event recognizers of stalled vehicles with back-tracking to accurately locate event occurrence. The proposed methods achieve compelling performance in the leaderboard among 80+ world-wide participant teams."
  },
  "cvpr2019_ai_city_trafficanomalydetectionviaperspectivemapbasedonspatial-temporalinformationmatrix": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Traffic Anomaly Detection via Perspective Map based on Spatial-temporal Information Matrix",
    "authors": [
      "Shuai Bai",
      "Zhiqun He",
      "Yu Lei",
      "Wei Wu",
      "Chengkai Zhu",
      "Ming Sun",
      "Junjie Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Bai_Traffic_Anomaly_Detection_via_Perspective_Map_based_on_Spatial-temporal_Information_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Bai_Traffic_Anomaly_Detection_via_Perspective_Map_based_on_Spatial-temporal_Information_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Anomaly detection on the road traffic has vast application prospects in urban traffic management and road safety. Due to the impact of many factors such as weather, viewpoints and road conditions in the real-world traffic scene, anomaly detection still faces many challenges. There are many causes for vehicle anomalies, such as crashes, vehicle on fires and vehicle faults, which exhibits different unknown behaviors. In this paper, we propose an anomaly detection system that includes three modules: background modeling module, perspective detection module, and spatial-temporal matrix discriminating module. The background modeling analyses the traffic flow to obtain the road segmentation results, and the vehicle flow superposition is used to obtain the continuous stationary region. The perspective detection module gets the perspective map by the first detection result, through which the image is cropped to uniform scale for different vehicles and re-detection. Finally, we get all anomalies by constructing spatial-temporal information matrix with the detection results. Furthermore, all anomalies are merged through the non maximum suppression (NMS) and the re-identification model, including spatial and temporal dimensions. The experimental results show that our system is effective in the Track3 test-set of NVIDIA AI CITY 2019 CHALLENGE, which finally ranked first in the competition, with a 97.06% F1-score and 5.3058 root mean square error (RMSE)."
  },
  "cvpr2019_ai_city_unsupervisedtrafficanomalydetectionusingtrajectories": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Unsupervised Traffic Anomaly Detection Using Trajectories",
    "authors": [
      "Jianfei Zhao",
      "Zitong Yi",
      "Siyang Pan",
      "Yanyun Zhao",
      "Zhicheng Zhao",
      "Fei Su",
      "Bojin Zhuang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Zhao_Unsupervised_Traffic_Anomaly_Detection_Using_Trajectories_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Zhao_Unsupervised_Traffic_Anomaly_Detection_Using_Trajectories_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Traffic anomaly detection of unsupervised videos has attracted great interests in computer vision field, and this task is very challenging since the scarcity of data and scene diversities. In this work, we present a robust framework for solving unsupervised traffic anomaly detection based on vehicle trajectories. The possible anomalies are detected and tracked from background image sequence of videos. The start time of the abnormal events is located by the decision module based on tracks. In order to better solve the problems of false detections and missed detections caused by the detector, we design a multi-object track (MOT) algorithm suitable for this task. We also present an adaptive unsupervised road mask generation method to filter out false anomalies outside the road area. Our method participated in the evaluation of 2019 AI CITY CHALLENGE Track3 and achieved good result."
  },
  "cvpr2019_ai_city_vehiclere-identifiationandmulti-cameratrackinginchallengingcity-scaleenvironment": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Vehicle Re-Identifiation and Multi-Camera Tracking in Challenging City-Scale Environment",
    "authors": [
      "Jakub Spanhel",
      "Vojtech Bartl",
      "Roman Juranek",
      "Adam Herout"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Spanhel_Vehicle_Re-Identifiation_and_Multi-Camera_Tracking_in_Challenging_City-Scale_Environment_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Spanhel_Vehicle_Re-Identifiation_and_Multi-Camera_Tracking_in_Challenging_City-Scale_Environment_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In our submission to the NVIDIA AI City Challenge, we address vehicle re-identification and vehicle multi-camera tracking. Our approach to vehicle re-identification is based on the extraction of visual features and aggregation of these features in the temporal domain to obtain a single feature descriptor for the whole observed track. For multi-camera tracking, we proposed a method for matching vehicles by the position of trajectory points in real-world space (linear coordinate system). Furthermore, we use CNN for vehicle re-identification task to filter out false matches generated by proposed positional matching method for better results."
  },
  "cvpr2019_ai_city_alocalityawarecity-scalemulti-cameravehicletrackingsystem": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "A Locality Aware City-Scale Multi-Camera Vehicle Tracking System",
    "authors": [
      "Yunzhong Hou",
      "Heming Du",
      "Liang Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Hou_A_Locality_Aware_City-Scale_Multi-Camera_Vehicle_Tracking_System_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Hou_A_Locality_Aware_City-Scale_Multi-Camera_Vehicle_Tracking_System_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle tracking across multiple cameras can be difficult for modern tracking systems.Given unlikely candidates and faulty similarity estimation, data association struggles at city-scale tracking.In order to avoid difficulties in a large scenario, we keep the tracking procedure within a minimal range. The benefit of this smaller scenario idea is two-fold. On the one hand, ruling out most unlikely candidates decrease the possibility of mis-assignment. On the other hand, the system can devote all its discriminative power on the remaining local candidate pool.In fact, our tracking system features two parts to keep the data association within a small range, while at the same time increase the locality awareness for smaller scenarios. First, multiple cues including spatial-temporal information and camera topology are leveraged to restrict the candidate selection. Second, the appearance similarity estimation module is carefully tuned so that it focuses on the smaller local candidate pool. Based on a minimal view for the large scenario, the proposed system finished 5-th place in the 2019 AI-City challenge for city-scale multi-camera vehicle tracking."
  },
  "cvpr2019_ai_city_partitionandreunionatwo-branchneuralnetworkforvehiclere-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Partition and Reunion: A Two-Branch Neural Network for Vehicle Re-identification",
    "authors": [
      "Hao Chen",
      "Benoit Lagadec",
      "Francois Bremond"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Chen_Partition_and_Reunion_A_Two-Branch_Neural_Network_for_Vehicle_Re-identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Chen_Partition_and_Reunion_A_Two-Branch_Neural_Network_for_Vehicle_Re-identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The smart city vision raises the prospect that cities will become more intelligent in various fields, such as more sustainable environment and a better quality of life for residents. As a key component of smart cities, intelligent transportation system highlights the importance of vehicle re-identification (Re-ID). However, as compared to the rapid progress on person Re-ID, vehicle Re-ID advances at a relatively slow pace. Some previous state-of-the-art approaches strongly rely on extra annotation, like attributes (e.g., vehicle color and type) and key-points (e.g., wheels and lamps). Recent work on person Re-ID shows that extracting more local features can achieve a better performance without considering extra annotation. In this paper, we propose an end-to-end trainable two-branch Partition and Reunion Network (PRN) for the challenging vehicle ReID task. Utilizing only identity labels, our proposed method outperforms existing state-of-the-art methods on four vehicle Re-ID benchmark datasets, including VeRi-776, VehicleID, VRIC and CityFlow-ReID by a large margin."
  },
  "cvpr2019_ai_city_multi-cameravehicletrackingwithpowerfulvisualfeaturesandspatial-temporalcue": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Multi-Camera Vehicle Tracking with Powerful Visual Features and Spatial-Temporal Cue",
    "authors": [
      "Zhiqun He",
      "Yu Lei",
      "Shuai Bai",
      "Wei Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/He_Multi-Camera_Vehicle_Tracking_with_Powerful_Visual_Features_and_Spatial-Temporal_Cue_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/He_Multi-Camera_Vehicle_Tracking_with_Powerful_Visual_Features_and_Spatial-Temporal_Cue_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle re-identification and multi-camera multi-object vehicle tracking are important components in the field of intelligent traffic, which is attracting more and more atten- tion. In the NVIDIA AI City Challenge, we propose our solutions to solve this issues. In Track1 task, clustering loss and trajectory consistent loss are introduced into the vehicle re-identification training framework to train more suitable trajectory-based features for cluster task. Besides, spatial- temporal cue is fully excavated to make up the deficiency of appearance feature and constrained hierarchical clus- tering is introduced into the pipeline to get the final clus- ter results. In Track2 task, we propose an effective vehicle training framework and trajectory-based weighted ranking method, which greatly improve the performance. Further- more, an efficient way to mining the additional data to train more robust feature is proposed to enlarge the training data. Finally, our algorithm achieves the state-of-the-art perfor- mance in the competition."
  },
  "cvpr2019_ai_city_spatio-temporalconsistencyandhierarchicalmatchingformulti-targetmulti-cameravehicletracking": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Spatio-temporal Consistency and Hierarchical Matching for Multi-Target Multi-Camera Vehicle Tracking",
    "authors": [
      "Peilun Li",
      "Guozhen Li",
      "Zhangxi Yan",
      "Youzeng Li",
      "Meiqi Lu",
      "Pengfei Xu",
      "Yang Gu",
      "Bing Bai",
      "Yifei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Li_Spatio-temporal_Consistency_and_Hierarchical_Matching_for_Multi-Target_Multi-Camera_Vehicle_Tracking_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Li_Spatio-temporal_Consistency_and_Hierarchical_Matching_for_Multi-Target_Multi-Camera_Vehicle_Tracking_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, many approaches have been addressed to realize Multi-Target Multi-Camera(MTMC) vehicle tracking, which is critical in intelligent transportation system (ITS). Continuous improvements of MTMC have been limited by two modules - trajectory feature representation and feature metric in the city-scale camera condition. In this paper, we propose a spatio-temporal consistency and hierarchical matching method to overcome the challenges. As first step, a popular object detection and object tracking method are implemented to detect vehicles and track them in single camera, thus achieved high performance. The smoothness of trajectory and slice direction of movement make spatio-temporal consistency more confident. As second step, a bottom-up hierarchical match strategy is used to match targets in different cameras. Top performance in City-Scale Multi-Camera Vehicle Tracking task at the NVIDIA AI City Challenge 2019 demonstrated the advantage of our methods."
  },
  "cvpr2019_ai_city_attentiondrivenvehiclere-identificationandunsupervisedanomalydetectionfortrafficunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Attention Driven Vehicle Re-identification and Unsupervised Anomaly Detection for Traffic Understanding",
    "authors": [
      "Pirazh Khorramshahi",
      "Neehar Peri",
      "Amit Kumar",
      "Anshul Shah",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Khorramshahi_Attention_Driven_Vehicle_Re-identification_and_Unsupervised_Anomaly_Detection_for_Traffic_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Khorramshahi_Attention_Driven_Vehicle_Re-identification_and_Unsupervised_Anomaly_Detection_for_Traffic_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Vehicle re-identification and anomaly detection are useful tools in traffic analytics applications. Vehicle re-identification is particularly challenging due to variations in viewpoint, illumination and occlusion. Moreover, the reality of multiple vehicles having the same make and model hinders the design of traditional deep network-based solutions. In this work, we leverage an attention-based model which learns to focus on different parts of a vehicle by conditioning the feature maps on visible key-points. We use triplet embedding to reduce the dimensionality of the features obtained from the ensemble of networks trained using different datasets. To address the problem of anomaly detection, we design an unsupervised algorithm to detect and localize anomalies in traffic scenes. To handle moving cameras, we use the results obtained from tracking to generate anomaly proposals which are then filtered in successive steps. We show the effectiveness of our method on the Nvidia AI City vehicle re-identification dataset, where we obtain mean Average Precision (mAP) score of 60.78% placing us at the 8th position out of 84 participating teams. In addition, we achieved the S3 score of 22.07% for vehicle anomaly detection."
  },
  "cvpr2019_ai_city_comparativestudyonvariouslossesforvehiclere-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Comparative Study on various Losses for Vehicle Re-identification",
    "authors": [
      "Adithya Shankar",
      "Akhil Poojary",
      "Varghese Kollerathu",
      "Chandan Yeshwanth",
      "Sheetal Reddy",
      "Vinay Sudhakaran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Shankar_Comparative_Study_on_various_Losses_for_Vehicle_Re-identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Shankar_Comparative_Study_on_various_Losses_for_Vehicle_Re-identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": "In this paper, we tackle the problem of vehicle re-identification, which has extensive applications in traffic analysis such as anomaly detection, congestion pricing and tolling. While previous methods extract visual features from the images and then use spatio-temporal regularization to further refine the results, our method focuses on extracting purely visual features from vehicle images and then further employs a re-ranking technique to improve results. We evaluate the proposed pipeline on the VeRi and CityFlow (NVIDIA AI City Challenge 2019) datasets. Experiments show that our pipeline achieves state of the art performance on the VeRi dataset. We also perform extensive analysis on each step of the pipeline and demonstrate how they increase overall performance."
  },
  "cvpr2019_ai_city_multi-cameravehicletrackingandre-identificationbasedonvisualandspatial-temporalfeatures": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Multi-camera vehicle tracking and re-identification based on visual and spatial-temporal features",
    "authors": [
      "Xiao Tan",
      "Zhigang Wang",
      "Minyue Jiang",
      "Xipeng Yang",
      "Jian Wang",
      "Yuan Gao",
      "Xiangbo Su",
      "Xiaoqing Ye",
      "Yuchen Yuan",
      "Dongliang He",
      "Shilei Wen",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Tan_Multi-camera_vehicle_tracking_and_re-identification_based_on_visual_and_spatial-temporal_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Tan_Multi-camera_vehicle_tracking_and_re-identification_based_on_visual_and_spatial-temporal_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Due to the heavy occlusions, large variations in different viewing perspectives and low video resolutions, the tracking and re-identification of vehicles under multi-camera become challenging tasks for the intelligent transportation system (ITS). In this work, we propose a novel framework for multi-camera tracking, which integrates visual features, orientation prediction and temporal-spatial information of the trajectories for optimization. In addition, based on the tracking information generated by our framework, we propose a united method for multi-camera re-identification that takes both visual features and tracking information into account. In order to make the visual feature robust for occlusion and perspective variation, our method adopts various features that are extracted from global image, regions and areas around key points, and the tracking information are also used to refine the retrieval results generated by the visual features. Our algorithm achieves the first place in vehicle re-identification at the NVIDIA AI City Challenge 2019."
  },
  "cvpr2019_ai_city_vehiclere-identificationpushingthelimitsofre-identification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Vehicle Re-Identification: Pushing the limits of re-identification",
    "authors": [
      "Abner Ayala-Acevedo",
      "Akash Devgun",
      "Sadri Zahir",
      "Sid Askary"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Ayala-Acevedo_Vehicle_Re-Identification_Pushing_the_limits_of_re-identification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Ayala-Acevedo_Vehicle_Re-Identification_Pushing_the_limits_of_re-identification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present a series of techniques which help push the limits of vehicle re-identification. First, we establish a strong baseline by using one of the best person re-identification models and applying them to vehicle re-identification. Secondly, we show improvements in four key components of re-identification: 1) detection, 2) tracking, 3) model, 4) loss function. Finally, our improvements lead to the state-of-the-art in the vehicle re-identification dataset VeRi-776, with 85.20 mean Average Precision (mAP) and 96.60% Rank-1 accuracy. This represents a +17.65 mAP and +6.37 Rank-1 improvement over the literature."
  },
  "cvpr2019_ai_city_acomparativestudyoffasterr-cnnmodelsforanomalydetectionin2019aicitychallenge": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "A Comparative Study of Faster R-CNN Models for Anomaly Detection in 2019 AI City Challenge",
    "authors": [
      "Linu Shine",
      "Anitha Edison",
      "Jiji C. V."
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Shine_A_Comparative_Study_of_Faster_R-CNN_Models_for_Anomaly_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Shine_A_Comparative_Study_of_Faster_R-CNN_Models_for_Anomaly_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Traffic anomaly detection forms an integral part of intelligent traffic monitoring and management system. Timely detection of anomalies is crucial in providing necessary assistance to accident victims. Track 3 of 2019 AI city challenge addresses traffic anomaly detection problem. We propose an unsupervised method to tackle this problem.Proposed system consists of three stages. The first stage is a background extraction stage which isolates the stalled vehicles from moving vehicles. An anomaly detection is the second stage that identifies the stalled vehicles in the background and finally anomaly confirmation module confirms anomaly and determines the start time. We have used faster RCNN (FRCNN) with Inception v2 and ResNet 101 to detect stalled vehicles and confirm possible anomalies. A comparative study shows that FRCNN with Inception v2 gives superior performance."
  },
  "cvpr2019_ai_city_multi-cameravehicletrackingandre-identificationonaicitychallenge2019": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Multi-camera Vehicle Tracking and Re-identification on AI City Challenge 2019",
    "authors": [
      "Yucheng Chen",
      "Longlong Jing",
      "Elahe Vahdani",
      "Ling Zhang",
      "Mingyi He",
      "Yingli Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Chen_Multi-camera_Vehicle_Tracking_and_Re-identification_on_AI_City_Challenge_2019_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Chen_Multi-camera_Vehicle_Tracking_and_Re-identification_on_AI_City_Challenge_2019_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work, we present our solutions to the image-based vehicle re-identification (ReID) track and multi-camera vehicle tracking (MVT) tracks on AI City Challenge 2019 (AIC2019). For the ReID track, we propose an enhanced multi-granularity network with multiple branches to extract visual features for vehicles with different levels of grains. With the help of these multi-grained features, the proposed framework outperforms the current state-of-the-art vehicle ReID method by 16.3% on Veri dataset. For the MVT track, we first generate tracklets by Kernighan-Lin graph partitioning algorithm with feature and motion correlation, then combine tracklets to trajectories by proposed progressive connection strategy, finally match trajectories under different camera views based on the annotated road boundaries. Our MVT and ReID algorithms are ranked the 10 and 23 in MVT and ReID tracks respectively at the NVIDIA AI City Challenge 2019."
  },
  "cvpr2019_ai_city_vpulabparticipationataicitychallenge2019": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "VPULab participation at AI City Challenge 2019",
    "authors": [
      "Elena Luna",
      "Paula Moral",
      "Juan C. SanMiguel",
      "Alvaro Garcia-Martin",
      "Jose M. Martinez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Luna_VPULab_participation_at_AI_City_Challenge_2019_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Luna_VPULab_participation_at_AI_City_Challenge_2019_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present an approach for Multi-target and Multi-Camera Vehicle Tracking and another approach for Vehicle Re-Identification (ReID) across multiple cameras. We evaluate both approaches over \"CityFlow: A City- Scale Benchmark\" participating in Track 1 and Track 2 of 2019 AI City Challenge Workshop. The proposed tracking approach is based on applying detection and tracking of multiple moving vehicles for each camera. Afterwards, we cluster such results (detections of vehicles) obtained from multiple cameras with overlapped fields of view. The clustering is based on appearance and spatial distances on a common plane for all camera views. The optimal number of clusters is obtained by using validation indexes. Then, a spatio-temporal linkage of the obtained clusters is performed to obtain the trajectories of each moving vehicle in the scene. We tested different combinations for the input of the proposed approach (detector and tracker) and provide sample results for selected scenarios of \"CityFlow: A City- Scale Benchmark\". The proposed re-identification system is based on the combination of adapted deep learning feature embedding representations and a distance metric learning process. We also include the vehicle tracking information provided by the \"CityFlow: A City-Scale Benchmark\" in order to improve the results. We tested different combinations of features, metric learning and the use of tracking information and provide sample results for the CityFlow- ReID dataset."
  },
  "cvpr2019_ai_city_vehiclere-identificationwithlearnedrepresentationandspatialverificationandabnormalitydetectionwithmulti-adaptivevehicledetectorsfortrafficvideoanalysis": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Vehicle Re-identification with Learned Representation and Spatial Verification and Abnormality Detection with Multi-Adaptive Vehicle Detectors for Traffic Video Analysis",
    "authors": [
      "Khac-Tuan Nguyen",
      "Trung-Hieu Hoang",
      "Minh-Triet Tran",
      "Trung-Nghia Le",
      "Ngoc-Minh Bui",
      "Trong-Le Do",
      "Viet-Khoa Vo-Ho",
      "Quoc-An Luong",
      "Mai-Khiem Tran",
      "Thanh-An Nguyen",
      "Thanh-Dat Truong",
      "Vinh-Tiep Nguyen",
      "Minh N. Do"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Nguyen_Vehicle_Re-identification_with_Learned_Representation_and_Spatial_Verification_and_Abnormality_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Nguyen_Vehicle_Re-identification_with_Learned_Representation_and_Spatial_Verification_and_Abnormality_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Traffic flow analysis is essential for intelligent transportation systems. In this paper, we propose methods for two challenging problems in traffic flow analysis: vehicle re-identification and abnormal event detection. For the first problem, we propose to combine learned high-level features for vehicle instance representation with hand-crafted local features for spatial verification. For the second problem, we propose to use multiple adaptive vehicle detectors for anomaly proposal and use heuristics properties extracted from anomaly proposals to determine anomaly events. Experiments on the datasets of traffic flow analysis from AI City Challenge 2019 show that our methods achieve mAP of 0.4008 for vehicle re-identification in Track 2, and can detect abnormal events with very high accuracy (F1 = 0.9429) in Track 3."
  },
  "cvpr2019_ai_city_anomalycandidateidentificationandstartingtimeestimationofvehiclesfromtrafficvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Anomaly Candidate Identification and Starting Time Estimation of Vehicles from Traffic Videos",
    "authors": [
      "Gaoang Wang",
      "Xinyu Yuan",
      "Aotian Zheng",
      "Hung-Min Hsu",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Wang_Anomaly_Candidate_Identification_and_Starting_Time_Estimation_of_Vehicles_from_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Wang_Anomaly_Candidate_Identification_and_Starting_Time_Estimation_of_Vehicles_from_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Anomaly event detection on road traffic has been a challenging field mainly due to lack of training data and a wide variety of anomaly cases.In this paper, we propose a novel two-stage framework for anomaly event detection inroad traffic based on anomaly candidate identification and starting time estimation of vehicles. First, we use Gaussian mixture models (GMMs) to generate the foreground mask and background image to identify the anomaly candidates. Foreground mask is used to produce the region of interest (ROI) to filter out the noise from the object detector, YOLOv3, in the background image.Then, we apply the TrackletNet Tracker (TNT) to extract the trajectory of anomaly candidate to estimate the anomaly starting time. Experimental results, with achieved S3 score performanceof93.62%,on the Track3testing set of CVPRAICityChallenge 2019 City Flow dataset, show the effectiveness of the proposed framework and its robustness in different real scenes."
  },
  "cvpr2019_ai_city_vehiclere-identificationwithlocationandtimestamps": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Vehicle Re-Identification with Location and Time Stamps",
    "authors": [
      "Kai Lv",
      "Heming Du",
      "Yunzhong Hou",
      "Weijian Deng",
      "Hao Sheng",
      "Jianbin Jiao",
      "Liang Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Lv_Vehicle_Re-Identification_with_Location_and_Time_Stamps_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Lv_Vehicle_Re-Identification_with_Location_and_Time_Stamps_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper focuses on the problem of vehicle re-identification (Re-ID). In our attempt, we propose a re-identification framework by exploiting vehicle location and time stamps. The location and time information have the potential to cover the shortage of appearance-based feature representations. First, we introduce an ensemble technique to combine the informative cues of multiple Re-ID models effectively. To further improve the accuracy, we then build up a system to acquire the vehicle location and time stamps. Specifically, we utilize the detected results to obtain the needed information. With the help of the proposed system, we can remove irrelevant images from a given ranking list.Our system finished 3rd place in the 2019 AI-City challenge for city-scale multi-camera vehicle re-identification. "
  },
  "cvpr2019_ai_city_multi-cameratrackingofvehiclesbasedondeepfeaturesre-idandtrajectory-basedcameralinkmodels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Multi-Camera Tracking of Vehicles based on Deep Features Re-ID and Trajectory-Based Camera Link Models",
    "authors": [
      "Hung-Min Hsu",
      "Tsung-Wei Huang",
      "Gaoang Wang",
      "Jiarui Cai",
      "Zhichao Lei",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Hsu_Multi-Camera_Tracking_of_Vehicles_based_on_Deep_Features_Re-ID_and_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Hsu_Multi-Camera_Tracking_of_Vehicles_based_on_Deep_Features_Re-ID_and_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Due to the exponential growth of traffic camera networks, the need for multi-camera tracking (MCT) for intelligent transportation has received more and more attention. The challenges of MCT include similar vehicle models, significant feature variation in different orientations, color variation of the same car due to lighting conditions, small object sizes and frequent occlusion, as well as the varied resolutions of videos. In this work, we propose an MCT system, which combines single-camera tracking (SCT) and inter-camera tracking (ICT) which includes trajectory-based camera link model and deep feature reidentification. For SCT, we use a TrackletNet Tracker (TNT), which effectively generates the moving trajectories of all detected vehicles by exploiting temporal and appearance information of multiple tracklets that are created by associating bounding boxes of detected vehicles. The tracklets are generated based on CNN feature matching and intersection-over-union (IOU) in every single-camera view. In terms of deep feature re-identification, we exploit the temporal attention model to extract the most discriminant feature of each trajectory. In addition, we propose the trajectory-based camera link models with order constraint to efficiently leverage the spatial and temporal information for ICT. The proposed method is evaluated on CVPR AI City Challenge2019 City Flow dataset, achieving IDF1 70.59%, which outperforms competing methods."
  },
  "cvpr2019_ai_city_multi-viewvehiclere-identificationusingtemporalattentionmodelandmetadatare-ranking": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "Multi-View Vehicle Re-Identification using Temporal Attention Model and Metadata Re-ranking",
    "authors": [
      "Tsung-Wei Huang",
      "Jiarui Cai",
      "Hao Yang",
      "Hung-Min Hsu",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Huang_Multi-View_Vehicle_Re-Identification_using_Temporal_Attention_Model_and_Metadata_Re-ranking_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Huang_Multi-View_Vehicle_Re-Identification_using_Temporal_Attention_Model_and_Metadata_Re-ranking_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Object re-identification (ReID) is an arduous task which requires matching an object across different nonoverlapping camera views. Recently, many researchers are working on person ReID by taking advantages of appearance, human pose, temporal constrains, etc. However, vehicle ReID is even more challenging because vehicles have fewer discriminant features than human due to viewpoint orientation, changes in lighting condition and inter-class similarity. In this paper, we propose a viewpoint-aware temporal attention model for vehicle ReID utilizing deep learning features extracted from consecutive frames with vehicle orientation and metadata attributes (i.e., type, brand, color) being taken into consideration. In addition, re-ranking with soft decision boundary is applied as post-processing for result refinement. The proposed method is evaluated on CVPR AI City Challenge 2019 dataset, achieving mAP of 79:17% with the second place ranking in the competition."
  },
  "cvpr2019_ai_city_the2019aicitychallenge": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "AI_City",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - AI City",
    "title": "The 2019 AI City Challenge",
    "authors": [
      "Milind Naphade",
      "Zheng Tang",
      "Ming-Ching Chang",
      "David C. Anastasiu",
      "Anuj Sharma",
      "Rama Chellappa",
      "Shuo Wang",
      "Pranamesh Chakraborty",
      "Tingting Huang",
      "Jenq-Neng Hwang",
      "Siwei Lyu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/AI_City/Naphade_The_2019_AI_City_Challenge_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/AI City/Naphade_The_2019_AI_City_Challenge_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The AI City Challenge has been created to accelerate intelligent video analysis that helps make cities smarter and safer. With millions of traffic video cameras acting as sensors around the world, there is a significant opportunity for real-time and batch analysis of these videos to provide actionable insights. These insights will benefit a wide variety of agencies, from traffic control to public safety. The 2019 AI City Challenge is the third annual edition in the AI City Challenge series with significant growing attention and participation. AI City Challenge 2019 enabled 334 academic and industrial research teams from 44 countries to solve real-world problems using real city-scale traffic camera video data. The Challenge was launched with three tracks. Track 1 addressed city-scale multi-camera vehicle tracking, Track 2 addressed city-scale vehicle re-identification, and Track 3 addressed traffic anomaly detection. Each track was chosen in consultation with departments of transportation focusing on problems of greatest public value. With the largest available dataset for such tasks, and ground truth for each track, the 2019 AI City Challenge received 129 submissions from 96 individuals teams (there were 22, 84, 23 team submissions from Tracks 1, 2, and 3 respectively). Participation in this challenge has grown five-fold this year as tasks have become more relevant to traffic optimization and challenging to the computer vision community. Results observed strongly underline the value AI brings to city-scale video analysis for traffic optimization."
  },
  "cvpr2019_vision_meets_cognition_camera_ready_half&halfnewtasksandbenchmarksforstudyingvisualcommonsense": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_Meets_Cognition_Camera_Ready",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision Meets Cognition",
    "title": "Half&Half: New Tasks and Benchmarks for Studying Visual Common Sense",
    "authors": [
      "Ashish Singh",
      "Hang Su",
      "SouYoung Jin",
      "Huaizu Jiang",
      "Chetan Manjesh",
      "Geng Luo",
      "Ziwei He",
      "Li Hong",
      "Erik Learned-Miller",
      "Rosie Cowell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_Meets_Cognition_Camera_Ready/Singh_HalfHalf_New_Tasks_and_Benchmarks_for_Studying_Visual_Common_Sense_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Singh_HalfHalf_New_Tasks_and_Benchmarks_for_Studying_Visual_Common_Sense_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Suppose you're in an unfamiliar apartment looking for a TV. You're more likely to look in a room with a couch than with a sink, or in a room with carpeting than with a tile floor. Making such intelligent decisions about unseen objects given only partial observations is a fundamental component of visual common sense. These capabilities can take various forms and can benefit an intelligent agent in different scenarios. For example, for an agent to find an object efficiently, it must ask questions such as: (1) is the current direction a promising choice? And if not, (2) given observations toward other directions, which one is preferred? A less specific but nonetheless essential capability is to directly predict the next visual observations, which can enable an agent to prepare for imminent encounters. In this work, we formalize three specific prediction tasks critical to visual common sense and introduce benchmarks--the Half&Half benchmarks--to measure an agent's ability to perform these tasks. We show that it is possible to modify pre-existing data sets to develop large training and test set to learn these new tasks with minimal effort. Our trained models exhibit large improvements over naive baselines. Preliminary evaluations on the task on simple visual navigation scenarios demonstrate the utility of our models and the potential power of future intelligent agents equipped with visual common sense. "
  },
  "cvpr2019_vision_meets_cognition_camera_ready_cookingwithblocksarecipeforvisualreasoningonimage-pairs": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_Meets_Cognition_Camera_Ready",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision Meets Cognition",
    "title": "Cooking With Blocks : A Recipe for Visual Reasoning on Image-Pairs",
    "authors": [
      "Tejas Gokhale",
      "Shailaja Sampat",
      "Zhiyuan Fang",
      "Yezhou Yang",
      "Chitta Baral"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The ability of identifying changes or transformations in a scene and to reason about their causes and effects, is a key aspect of intelligence. In this work we go beyond recent advances in computational perception, and introduce a more challenging task, Image-based Event-Sequencing (IES). In IES, the task is to predict a sequence of actions required to rearrange objects from the configuration in an input source image to the one in the target image. IES also requires systemsto possess inductive generalizability. Motivated from evidence in cognitive development, we compile the first IES dataset, the Blocksworld Image Reasoning Dataset (BIRD) which contains images of wooden blocks in different configurations, and the sequence of moves to rearrange one configuration to the other. We first explore the use of existing deep learning architectures and show that these end-to-end methods under-perform in inferring temporal event-sequences and fail at inductive generalization. We propose a modular two-step approach: Visual Perception followed by Event-Sequencing, and demonstrate improved performance by combining learning and reasoning. Finally, by showing an extension of our approach on natural images, we seek to pave the way for future research on event sequencing for real world scenes."
  },
  "cvpr2019_vision_meets_cognition_camera_ready_onhuman-likeperformanceartificialintelligenceademonstrationusinganatarigame": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_Meets_Cognition_Camera_Ready",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision Meets Cognition",
    "title": "On Human-like Performance Artificial Intelligence: A Demonstration Using an Atari Game",
    "authors": [
      "Seng-Beng Ho",
      "Xiwen Yang",
      "Therese Quieta",
      "Gangeshwar Krishnamurthy",
      "Fiona Liausvia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_Meets_Cognition_Camera_Ready/Ho_On_Human-like_Performance_Artificial_Intelligence_A_Demonstration_Using_an_Atari_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Ho_On_Human-like_Performance_Artificial_Intelligence_A_Demonstration_Using_an_Atari_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Despite the progress made in AI, especially in the successful deployment of deep learning for many useful tasks, the systems involved typically require a huge number of training instances, and hence a long time for training. As a result, these systems are not able to rapidly adapt to changing rules and constraints in the environment. This is unlike humans, who are usually able to learn with only a handful of experiences. This hampers the deployment of, say, an adaptive robot that can learn and act rapidly in the ever-changing environment of a home, office, factory, or disaster area. Thus, it is necessary for an AI or robotic system to achieve human performance not only in terms of the \"level\" or \"score\" (e.g., success rate in classification, score in Atari game playing, etc.) but also in terms of the speed with which the level or score can be achieved. In contrast with earlier DeepMind's effort on Atari games, we describe a system that is able to learn causal rules rapidly in an Atari game environment and achieve human-like performance in terms of both score and time."
  },
  "cvpr2019_vision_meets_cognition_camera_ready_visualsimilarityfromoptimizingfeatureandmemoryonahypersphere": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_Meets_Cognition_Camera_Ready",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision Meets Cognition",
    "title": "Visual Similarity from Optimizing Feature and Memory On A Hypersphere",
    "authors": [
      "Xinlei Pan",
      "Rudrasis Chakraborty",
      "Stella Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_Meets_Cognition_Camera_Ready/Pan_Visual_Similarity_from_Optimizing_Feature_and_Memory_On_A_Hypersphere_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Pan_Visual_Similarity_from_Optimizing_Feature_and_Memory_On_A_Hypersphere_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Supervised learning of classification from annotated images develops a latent feature representation that captures semantic visual similarity.We propose an unsupervised metric learning method that develops apparent visual similarity from images alone.Our method maps high-dimensional visual data onto a low-dimensional hyper-sphere and consolidate such feature representations into a visual memory representation. Optimizing the feature mapping and visual memory on a hypersphere achieves maximal discrimination among instances. Our formulation and solution is not only more principled in theory than closely related unsupervised instance discrimination algorithms, but also better in practice in terms of classification accuracy, convergence rate, and feature transferability.We also show that our learned feature can be very useful for vision-based reinforcement learning tasks to improve sample efficiency. "
  },
  "cvpr2019_vision_meets_cognition_camera_ready_learninggeneralizablefinal-statedynamicsof3drigidobjects": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_Meets_Cognition_Camera_Ready",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision Meets Cognition",
    "title": "Learning Generalizable Final-State Dynamics of 3D Rigid Objects",
    "authors": [
      "Davis Rempe",
      "Srinath Sridhar",
      "He Wang",
      "Leonidas Guibas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_Meets_Cognition_Camera_Ready/Rempe_Learning_Generalizable_Final-State_Dynamics_of_3D_Rigid_Objects_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Rempe_Learning_Generalizable_Final-State_Dynamics_of_3D_Rigid_Objects_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Humans have a remarkable ability to predict the effect of physical interactions on the dynamics of objects. Endowing machines with this ability would allow important applications in areas like robotics and autonomous vehicles. In this work, we focus on predicting the dynamics of 3D rigid objects, in particular an object's final resting position and total rotation when subjected to an impulsive force. Different from previous work, our approach is capable of generalizing to unseen object shapes---an important requirement for real-world applications. To achieve this, we represent object shape as a 3D point cloud that is used as input to a neural network, making our approach agnostic to appearance variation. The design of our network is informed by an understanding of physical laws. We train our model with data from a physics engine that simulates the dynamics of a large number of shapes. Experiments show that we can accurately predict the resting position and total rotation for unseen object geometries."
  },
  "cvpr2019_vision_meets_cognition_camera_ready_learningfeaturerepresentationsforlook-alikeimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_Meets_Cognition_Camera_Ready",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision Meets Cognition",
    "title": "Learning Feature Representations for Look-Alike Images",
    "authors": [
      "Ayca Takmaz",
      "Thomas Probst",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_Meets_Cognition_Camera_Ready/Takmaz_Learning_Feature_Representations_for_Look-Alike_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Takmaz_Learning_Feature_Representations_for_Look-Alike_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Human perception of visual similarity relies on information varying from low-level features such as texture and color, to high-level features such as objects and elements. While generic features learned for image or face recognition tasks somewhat correlate with the perceived visual similarity, they are found to be inadequate for matching look-alike images. In this paper, we learn the 'look-alike feature' embedding, capable of representing the perceived image similarity, by fusing low- and high-level features within a modified CNN encoder architecture. This encoder is trained using the triplet loss paradigm on look-alike image pairs. Our findings demonstrate that combining features from different layers across the network is beneficial for look-alike image matching, and clearly outperforms the standard pretrained networks followed by finetuning. Furthermore, we show that the learned similarities are meaningful, and capture color, shape, facial or holistic appearance patterns, depending upon context and image modalities."
  },
  "cvpr2019_vision_meets_cognition_camera_ready_drivingsceneretrievalbyexamplefromlarge-scaledata": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_Meets_Cognition_Camera_Ready",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision Meets Cognition",
    "title": "Driving Scene Retrieval by Example from Large-Scale Data",
    "authors": [
      "Sascha Hornauer",
      "Baladitya Yellapragada",
      "Arian Ranjbar",
      "Stella Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_Meets_Cognition_Camera_Ready/Hornauer_Driving_Scene_Retrieval_by_Example_from_Large-Scale_Data_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Hornauer_Driving_Scene_Retrieval_by_Example_from_Large-Scale_Data_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Many machine learning approaches train networks with input from large datasets to reach high task performance. Collected datasets, such as Berkeley Deep Drive Video (BDD-V) for autonomous driving, contain a large variety of scenes and hence features. However, depending on the task, subsets, containing certain features more densely, support training better than others. For example, training networks on tasks such as image segmentation, bounding box detection or tracking requires an ample amount of objects in the input data. When training a network to perform optical flow estimation from first-person video, over-proportionally many straight driving scenes in the training data may lower generalization to turns. Even though some scenes of the BDD-V dataset are labeled with scene, weather or time of day information, these may be too coarse to filter the dataset best for a particular training task. Furthermore, even defining an exhaustive list of good label-types is complicated as it requires choosing the most relevant concepts of the natural world for a task. Alternatively, we investigate how to use examples of desired data to retrieve more similar data from a large-scale dataset. Following the paradigm of \"I know it when I see it\", we present a method to use driving examples for retrieving similar scenes from the BDD-V dataset. Our method leverages only automatically collected labels. We show how we can reliably vary time of the day or objects in our query examples and retrieve nearest neighbors from the dataset. Using this approach, already collected data could be filtered to remove bias from a dataset, removing scenes regarded too redundant to train on."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_progressivedomainadaptationforobjectdetection": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Progressive Domain Adaptation for Object Detection",
    "authors": [
      "Han-Kai Hsu",
      "Wei-Chih Hung",
      "Hung-Yu Tseng",
      "Chun-Han Yao",
      "Yi-Hsuan Tsai",
      "Maneesh Singh",
      "Ming-Hsuan Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Hsu_Progressive_Domain_Adaptation_for_Object_Detection_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Hsu_Progressive_Domain_Adaptation_for_Object_Detection_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent deep learning methods for object detection rely on a large amount of bounding box annotations. Collecting these annotations is laborious and costly, yet supervised models do not generalize well when testing on images from a different distribution. Domain adaptation provides a solution by adapting existing labels to the target testing data. However, a large gap between domains could make adaptation a challenging task, which leads to unstable training processes and sub-optimal solutions. In this paper, we pro- pose to bridge the domain gap with an intermediate domain and then progressively solve easier adaptation subtasks. Experimental results show that our method performs favorably against the state-of-the-art method in terms of the model test performance on the target domain."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_cats2colorandthermalstereosceneswithsemanticlabels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "CATS 2: Color And Thermal Stereo Scenes with Semantic Labels",
    "authors": [
      "Wayne Treible",
      "Philip Saponaro",
      "Yi Liu",
      "Agnijit Das Gupta",
      "Vinit Veerendraveer",
      "Scott Sorensen",
      "Chandra Kambhamettu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Treible_CATS_2_Color_And_Thermal_Stereo_Scenes_with_Semantic_Labels_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Treible_CATS_2_Color_And_Thermal_Stereo_Scenes_with_Semantic_Labels_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The CATS dataset introduced a new set of diverse indoor and outdoor scenes with ground truth disparity information for testing stereo matching algorithms in color and thermal imagery. These scenes included nighttime, foggy, low light, and complex lighting in scenes. To extend the usefulness of the CATS dataset we add pixel- and instance-level semantic labels. This includes labels for both color and thermal imagery, and the labels also apply to 3D point clouds as a result of the existing 2D-3D alignment. We compare the new CATS 2.0 dataset against other similar datasets and show it is similar in scope to the KITTI-360 and WildDash datasets, but with the addition of both thermal and 3D information. Additionally, we run a benchmark pedestrian detection algorithm on a set of scenes containing pedestrians."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_recoveringtheunseenbenchmarkingthegeneralizationofenhancementmethodstorealworlddatainheavyfog": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Recovering the Unseen: Benchmarking the Generalization of Enhancement Methods to Real World Data in Heavy Fog",
    "authors": [
      "Mario Bijelic",
      "Paula Kysela",
      "Tobias Gruber",
      "Werner Ritter",
      "Klaus Dietmayer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Bijelic_Recovering_the_Unseen_Benchmarking_the_Generalization_of_Enhancement_Methods_to_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Bijelic_Recovering_the_Unseen_Benchmarking_the_Generalization_of_Enhancement_Methods_to_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Due to the ill-posed problem of collecting data within adverse weather scenarios, especially within fog, most approaches in the field of image de-hazing are based on synthetic datasets and standard metrics that mostly originate from general tasks as image denoising or deblurring. To be able to evaluate the performance of such a system, it is necessary to have real data and an adequate metric. We introduce a novel calibrated benchmark dataset recorded in real, well defined weather conditions. The aim is to give a possibility to test developed approaches on real fog data. Furthermore, we claim to be the first showing an investigation of heavy fog conditions up to a total degradation of the considered images. We present a newly developed metric providing more interpretable insights into the system behavior and show how it is superior to several current evaluation methods as PSNR and SSIM. For this purpose, we evaluate current state-of-the-art methods from the area of image de-fogging and verify the proposed dataset and our developed evaluation framework."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_automaticadaptationofobjectdetectorstonewdomainsusingself-training": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Automatic adaptation of object detectors to new domains using self-training",
    "authors": [
      "Aruni RoyChowdhury",
      "Prithvijit Chakrabarty",
      "Ashish Singh",
      "SouYoung Jin",
      "Huaizu Jiang",
      "Liangliang Cao",
      "Erik Learned-Miller"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/RoyChowdhury_Automatic_adaptation_of_object_detectors_to_new_domains_using_self-training_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/RoyChowdhury_Automatic_adaptation_of_object_detectors_to_new_domains_using_self-training_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This work addresses the unsupervised adaptation of an existing object detector to a new target domain. We assume that a large number of unlabeled videos from this domain are readily available. We automatically obtain labels on the target data by using high-confidence detections from the existing detector, augmented with hard (misclassified) examples acquired by exploiting temporal cues using a tracker. These automatically-obtained labels are then used for re-training the original model. A modified knowledge distillation loss is proposed, and we investigate several ways of assigning soft-labels to the training examples from the target domain. Our approach is empirically evaluated on challenging face and pedestrian detection tasks: a face detector trained on WIDER-Face, which consists of high-quality images crawled from the web, is adapted to a large-scale surveillance data set; a pedestrian detector trained on clear, daytime images from the BDD-100K driving data set is adapted to all other scenarios such as rainy, foggy, night-time. Our results demonstrate the usefulness of incorporating hard examples obtained from tracking, the advantage of using soft-labels via distillation loss versus hard-labels, and show promising performance as a simple method for unsupervised domain adaptation of object detectors, with minimal dependence on hyper-parameters."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_modelvulnerabilitytodistributionalshiftsoverimagetransformationsets": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Model Vulnerability to Distributional Shifts over Image Transformation Sets",
    "authors": [
      "Riccardo Volpi",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Volpi_Model_Vulnerability_to_Distributional_Shifts_over_Image_Transformation_Sets_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Volpi_Model_Vulnerability_to_Distributional_Shifts_over_Image_Transformation_Sets_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We provide a summary of the pre-print 'Model Vulnerability to Distributional Shifts over Image Transformation Sets' (arxiv.org/abs/1903.11900) [22]."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_heavyrainimagerestorationintegratingphysicsmodelandconditionaladversariallearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Heavy Rain Image Restoration: Integrating Physics Model and Conditional Adversarial Learning",
    "authors": [
      "Ruoteng Li",
      "Loong-Fah Cheong",
      "Robby T. Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Li_Heavy_Rain_Image_Restoration_Integrating_Physics_Model_and_Conditional_Adversarial_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Li_Heavy_Rain_Image_Restoration_Integrating_Physics_Model_and_Conditional_Adversarial_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Most deraining works focus on rain streaks removal but they cannot deal adequately with heavy rain images. In heavy rain, streaks are strongly visible, dense rain accumulation or rain veiling effect significantly washes out the image, further scenes are relatively more blurry, etc. In this paper, we propose a novel method to address these problems. We put forth a 2-stage network: a physics-based back-bone followed by a depth-guided GAN refinement. The first stage estimates the rain streaks, the transmission, and the atmospheric light governed by the underlying physics. To tease out these components more reliably, a guided filtering framework is used to decompose the image into its low- and high-frequency components. This filtering is guided by a rain-free residue image -- its content is used to set the passbands for the two channels in a spatially-variant manner so that the background details do not get mixed up with the rain-streaks. For the second stage, the refinement stage, we put forth a depth-guided GAN to recover the background details failed to be retrieved by the first stage, as well as correcting artefacts introduced by that stage. We have evaluated our method against the state of the art methods. Extensive experiments show that our method outperforms them on real rain image data, recovering visually clean images with good details."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_bidirectionaldeepresiduallearningforhazeremoval": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Bidirectional Deep Residual learning for Haze Removal",
    "authors": [
      "Guisik Kim",
      "Jinhee Park",
      "Suhyeon Ha",
      "Junseok Kwon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Kim_Bidirectional_Deep_Residual_learning_for_Haze_Removal_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Kim_Bidirectional_Deep_Residual_learning_for_Haze_Removal_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recently, low-level vision problems has been addressed using residual learning that can learn a discrepancy be- tween hazy and haze-free images. Following this approach, in this paper, we present a new dehazing method based on the proposed bidirectional residual learning. Our method is implemented by generative adversarial networks (GANs), consisting of two components, namely, haze-removal and haze-reconstruction passes. The method alternates between removal and reconstruction of hazy regions using the residual to produce more accurate haze-free images. For efficient training, we adopt a feature fusion strategy based on extended tree-structures to include more spatial information and apply spectral normalization techniques to GANs. The effectiveness of our method is empirically demonstrated by quantitative and qualitative experiments, indicating that our method outperforms recent state-of-the-art dehazing algorithms. In particular, our approach can be easily used to solve other low-level vision problems such as deraining."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_isitrainingoutside?detectionofrainfallusinggeneral-purposesurveillancecameras": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Is it Raining Outside?  Detection of Rainfall using General-Purpose Surveillance Cameras",
    "authors": [
      "Joakim Bruslund Haurum",
      "Chris H. Bahnsen",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Haurum_Is_it_Raining_Outside__Detection_of_Rainfall_using_General-Purpose_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Haurum_Is_it_Raining_Outside__Detection_of_Rainfall_using_General-Purpose_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In integrated surveillance systems based on visual cam- eras, the mitigation of adverse weather conditions is an active research topic. Within this field, rain removal algorithms have been developed that artificially remove rain streaks from images or video. In order to deploy such rain removal algorithms in a surveillance setting, one must detect if rain is present in the scene. In this paper, we design a system for the detection of rainfall by the use of surveillance cameras. We reimplement the former state-of-the-art method for rain detection and compare it against a modern CNN-based method by utilizing 3D convolutions. The two methods are evaluated on our new AAU Visual Rain Dataset (VIRADA) that consists of 215 hours of general-purpose surveillance video from two traffic crossings. The results show that the proposed 3D CNN outperforms the previous state-of-the-art method by a large margin on all metrics, for both of the traffic crossings. Finally, it is shown that the choice of region-of-interest has a large influence on performance when trying to generalize the investigated methods. The AAU VIRADA dataset and our implementation of the two rain detection algorithms are publicly available at https://bitbucket.org/aauvap/aau-virada."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_gottaadaptemalljointpixelandfeature-leveldomainadaptationforrecognitioninthewild": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Gotta Adapt 'Em All: Joint Pixel and Feature-Level Domain Adaptation for Recognition in the Wild",
    "authors": [
      "Luan Tran",
      "Kihyuk Sohn",
      "Xiang Yu",
      "Xiaoming Liu",
      "Manmohan Chandraker"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Tran_Gotta_Adapt_Em_All_Joint_Pixel_and_Feature-Level_Domain_Adaptation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Tran_Gotta_Adapt_Em_All_Joint_Pixel_and_Feature-Level_Domain_Adaptation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent developments in deep domain adaptation have allowed knowledge transfer from a labeled source domain to an unlabeled target domain at the level of intermediate features or input pixels. We propose that advantages may be derived by combining them, in the form of different insights that lead to a novel design and complementary properties that result in better performance. At the feature level, in- spired by insights from semi-supervised learning, we propose a classification-aware domain adversarial neural network that brings target examples into more classifiable regions of source domain. Next, we posit that computer vision insights are more amenable to injection at the pixel level. In particular, we use 3D geometry and image synthesis based on a generalized appearance flow to preserve identity across pose transformations, while using an attribute-conditioned Cycle-GAN to translate a single source into multiple target images that differ in lower-level properties such as lighting. Besides standard UDA benchmark, we validate on a novel and apt problem of car recognition in unlabeled surveillance images using labeled images from the web, handling explicitly specified, nameable factors of variation through pixel-level and implicit, unspecified factors through feature-level adaptation. "
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_fastdrawaddressingthelongtailoflanedetectionbyadaptingasequentialpredictionnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network",
    "authors": [
      "Jonah Philion"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Philion_FastDraw_Addressing_the_Long_Tail_of_Lane_Detection_by_Adapting_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Philion_FastDraw_Addressing_the_Long_Tail_of_Lane_Detection_by_Adapting_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The search for predictive models that generalize to the long tail of sensor inputs is the central difficulty when developing data-driven models for autonomous vehicles. In this paper, we use lane detection to study modeling and training techniques that yield better performance on real world test drives. On the modeling side, we introduce a novel fully convolutional model of lane detection that learns to decode lane structures instead of delegating structure inference to post-processing. In contrast to previous works, our convolutional decoder is able to represent an arbitrary number of lanes per image, preserves the polyline representation of lanes without reducing lanes to polynomials, and draws lanes iteratively without requiring the computational and temporal complexity of recurrent neural networks. Because our model includes an estimate of the joint distribution of neighboring pixels belonging to the same lane, our formulation includes a natural and computationally cheap definition of uncertainty. On the training side, we demonstrate a simple yet effective approach to adapt the model to new environments using unsupervised style transfer. By training FastDraw to make predictions of lane structure that are invariant to low-level stylistic differences between images, we achieve strong performance at test time in weather and lighting conditions that deviate substantially from those of the annotated datasets that are publicly available. We quantitatively evaluate our approach on the CVPR 2017 Tusimple lane marking challenge, difficult CULane datasets [8], and a small labeled dataset of our own and achieve competitive accuracy while running at 90 FPS. "
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_reducingsteganographyincycle-consistencygans": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Reducing Steganography In Cycle-consistency GANs",
    "authors": [
      "Horia Porav",
      "Valentina Musat",
      "Paul Newman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Porav_Reducing_Steganography_In_Cycle-consistency_GANs_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Porav_Reducing_Steganography_In_Cycle-consistency_GANs_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this work we present a simple method of improving the suitability of data generated using cycle-consistency GANs in the context of day-to-night domain adaptation. While CycleGANs produce visually pleasing outputs, they also encode hidden (steganographic) information about the source domain in the generated images, which makes them less suitable as training data generators. We reduce the amount of steganographic information hidden in the generated images by introducing an end-to-end differentiable image de-noiser in between the two generators. The role of the de-noiser is to strip away the high frequency, low amplitude encoded information, making it harder for the generators to hide information that is invisible to the discriminator. We benchmark the suitability of data generated using our simple method in the context of simple domain adaptation for semantic segmentation, comparing with standard Cycle- GAN, MUNIT and DRIT and show that our method yields better performance."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_attribute-controlledtrafficdataaugmentationusingconditionalgenerativemodels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "Attribute-Controlled Traffic Data Augmentation Using Conditional Generative Models",
    "authors": [
      "Amitangshu Mukherjee",
      "Ameya Joshi",
      "Soumik Sarkar",
      "Chinmay Hegde"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Mukherjee_Attribute-Controlled_Traffic_Data_Augmentation_Using_Conditional_Generative_Models_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Mukherjee_Attribute-Controlled_Traffic_Data_Augmentation_Using_Conditional_Generative_Models_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Perception systems of self-driving vehicles require large amounts of diverse data to be robust against adverse lighting and weather conditions. Collection and annotation of such traffic data is resource-intensive and expensive. To circumvent this challenge, we introduce an approach where we train attribute-based generative models conditioned on the time-of-day labels to reconstruct semantically valid transformed versions of the original data. We further show the generalization capabilities of our model where they are able to reconstruct full traffic scenes despite having only being trained on constrained crops of the original images. Finally, we present a new dataset derived from an original traffic scene dataset augmented with data generated by our attribute-based conditional generative models."
  },
  "cvpr2019_vision_for_all_seasons_bad_weather_and_nighttime_procsyproceduralsyntheticdatasetgenerationtowardsinfluencefactorstudiesofsemanticsegmentationnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Vision_for_All_Seasons_Bad_Weather_and_Nighttime",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Vision for All Seasons Bad Weather and Nighttime",
    "title": "ProcSy: Procedural Synthetic Dataset Generation Towards Influence Factor Studies Of Semantic Segmentation Networks",
    "authors": [
      "Samin Khan",
      "Buu Phan",
      "Rick Salay",
      "Krzysztof Czarnecki"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Vision_for_All_Seasons_Bad_Weather_and_Nighttime/Khan_ProcSy_Procedural_Synthetic_Dataset_Generation_Towards_Influence_Factor_Studies_Of_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Vision for All Seasons Bad Weather and Nighttime/Khan_ProcSy_Procedural_Synthetic_Dataset_Generation_Towards_Influence_Factor_Studies_Of_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Real-world, large-scale semantic segmentation datasets are expensive and time-consuming to create. Thus, the research community has explored the use of video game worlds and simulator environments to produce large-scale synthetic datasets, mainly to supplement the real-world ones for training deep neural networks. Another use of synthetic datasets is to enable highly controlled and repeatable experiments, thanks to the ability to manipulate the content and rendering of synthesized imagery. To this end, we outline a method to generate an arbitrarily large, semantic segmentation dataset reflecting real-world features, while minimizing required cost and man-hours. We demonstrate its use by generating ProcSy, a synthetic dataset for semantic segmentation, which is modeled on a real-world urban environment and features a range of variable influence factors, such as weather and lighting. Our experiments investigate impact of the factors on performance of a state-of-the-art deep network. Among others, we show that including as little as 3% of rainy images in the training set, improved the mIoU of the network on rainy images by about 10%, while training with more than 15% rainy images has diminishing returns. We provide ProcSy dataset, along with generated 3D assets and code, as supplementary material."
  },
  "cvpr2019_image_matching_local_features_and_beyond_learningdata-adaptiveinterestpointsthroughepipolaradaptation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Image_Matching_Local_Features_and_Beyond",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Image Matching Local Features and Beyond",
    "title": "Learning Data-Adaptive Interest Points through Epipolar Adaptation",
    "authors": [
      "Guandao Yang",
      "Tomasz Malisiewicz",
      "Serge Belongie"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Image_Matching_Local_Features_and_Beyond/Yang_Learning_Data-Adaptive_Interest_Points_through_Epipolar_Adaptation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Image Matching Local Features and Beyond/Yang_Learning_Data-Adaptive_Interest_Points_through_Epipolar_Adaptation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Interest point detection and description have been cornerstones of many computer vision applications. Handcrafted methods like SIFT and ORB focus on generic interest points and do not lend themselves to data-driven adaptation. Recent deep learning models are generally either supervised using expensive 3D information or with synthetic 2D transformations such as homographies that lead to improper handling of nuisance features such as occlusion junctions. In this paper, we propose an alternative form of supervision that leverages the epipolar constraint associated with the fundamental matrix. This approach brings useful 3D information to bear without requiring full depth estimation of all points in the scene. Our proposed approach, Epipolar Adaptation, fine-tunes both the interest point detector and descriptor using a supervision signal provided by the epipolar constraint. We show that our method can improve upon the baseline in a target dataset annotated with epipolar constraints, and the epipolar adapted models learn to remove correspondence involving occlusion junctions correctly."
  },
  "cvpr2019_image_matching_local_features_and_beyond_boostinglocalmatcheswithconvolutionalco-segmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Image_Matching_Local_Features_and_Beyond",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Image Matching Local Features and Beyond",
    "title": "Boosting Local Matches with Convolutional Co-Segmentation",
    "authors": [
      "Erez Farhan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Image_Matching_Local_Features_and_Beyond/Farhan_Boosting_Local_Matches_with_Convolutional_Co-Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Image Matching Local Features and Beyond/Farhan_Boosting_Local_Matches_with_Convolutional_Co-Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Matching corresponding local patches between images is a fundamental building block in many computer-vision algorithms. Most matching methods are composed of two main stages: feature extraction, typically done independently on each image, and feature matching which is done on processed representations. This strategy tends to create large amounts of matches, typically describing small, highly-textured regions within each image. In many cases, large portions of the corresponding images have a simple geometric relationship. We exploit this fact and reformulate the matching procedure to an estimation stage, where we extract large domains roughly related by local transformations, and a convolutional Co-Segmentation stage, for densely detecting accurate matches in every domain. Consequently, we represent the geometrical relation- ship between images with a concise list of accurately co-segmented domains, preserving the geometrical flexibility stemmed from local analysis. We show how the proposed co-segmentation improves the matching coverage to accurately include many low-textured domains."
  },
  "cvpr2019_image_matching_local_features_and_beyond_anend-to-enddeepconvolutionalneuralnetworkforamulti-scaleimagematchingandlocalizationproblem": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Image_Matching_Local_Features_and_Beyond",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Image Matching Local Features and Beyond",
    "title": "An End-to-end Deep Convolutional Neural Network for a Multi-scale Image Matching and Localization Problem",
    "authors": [
      "Sungsoo Ha",
      "Yuewei Lin",
      "Xiaojing Huang",
      "Hanfei Yan",
      "Wei Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Image_Matching_Local_Features_and_Beyond/Ha_An_End-to-end_Deep_Convolutional_Neural_Network_for_a_Multi-scale_Image_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Image Matching Local Features and Beyond/Ha_An_End-to-end_Deep_Convolutional_Neural_Network_for_a_Multi-scale_Image_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Diverse imaging techniques are utilized in many scientific domains to acquire a rich description of the subject under study and to further discover a variety of its properties. Especially, in sample systems, by probing with optical, electron or x-ray beams, the captured images describe the sample in an extremely large range of length scales. This makes the correlation from one image to another very difficult in addition to the intrinsic appearance complexity of those scientific images. In this paper, we aim to tackle this multi-scale image matching and localization problem by proposing an end-to-end deep convolutional neural network. Our proposed network is designed to first generate different filters according to the two queried images originated from different length scales. Then, to compute the correlation map, we use these filters to predict the correspondence between the two images. For the training and evaluation, we collect a number of electron microscopy experiments to form a multi-scaled image patch dataset comprised of various material structures. We observe about 90% accuracy for multi-scale image matching and localization while a triplet-based network shows about 78% accuracy."
  },
  "cvpr2019_autonomous_driving_complexeryoloreal-time3dobjectdetectionandtrackingonsemanticpointclouds": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Complexer YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds",
    "authors": [
      "Martin Simon",
      "Karl Amende",
      "Andrea Kraus",
      "Jens Honer",
      "Timo Saemann",
      "Hauke Kaulbersch",
      "Stefan Milz",
      "Horst-Michael Gross"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Simon_Complexer_YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Simon_Complexer_YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20% and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection."
  },
  "cvpr2019_autonomous_driving_multinet++multi-streamfeatureaggregationandgeometriclossstrategyformulti-tasklearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "MultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning",
    "authors": [
      "Sumanth Chennupati",
      "Ganesh Sistu",
      "Senthil Yogamani",
      "Samir Rawashdeh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Chennupati_MultiNet_Multi-Stream_Feature_Aggregation_and_Geometric_Loss_Strategy_for_Multi-Task_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Chennupati_MultiNet_Multi-Stream_Feature_Aggregation_and_Geometric_Loss_Strategy_for_Multi-Task_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Multi-task learning is commonly used in autonomous driving for solving various visual perception tasks. It offers significant benefits in terms of both performance and computational complexity. Current work on multi-task learning networks focus on processing a single input image and there is no known implementation of multi-task learning handling a sequence of images. In this work, we propose a multi-stream multi-task network to take advantage of using feature representations from preceding frames in a video sequence for joint learning of segmentation, depth, and motion. The weights of the current and previous encoder are shared so that features computed in the previous frame can be leveraged without additional computation. In addition, we propose to use the geometric mean of task losses as a better alternative to the weighted average of task losses. The proposed loss function facilitates better handling of the difference in convergence rates of different tasks. Experimental results on KITTI, Cityscapes and SYNTHIA datasets demonstrate that the proposed strategies outperform various existing multi-task learning solutions. "
  },
  "cvpr2019_autonomous_driving_unsuperviseddomainadaptationforsemanticsegmentationofurbanscenes": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Unsupervised Domain Adaptation for Semantic Segmentation of Urban Scenes",
    "authors": [
      "Matteo Biasetton",
      "Umberto Michieli",
      "Gianluca Agresti",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Biasetton_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Biasetton_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The semantic understanding of urban scenes is one of the key components for an autonomous driving system. Complex deep neural networks for this task require to be trained with a huge amount of labeled data, which is difficult and expensive to acquire. A recently proposed workaround is the usage of synthetic data, however the differences between real world and synthetic scenes limit the performance. We propose an unsupervised domain adaptation strategy to adapt a synthetic supervised training to real world data. The proposed learning strategy exploits three components: a standard supervised learning on synthetic data, an adversarial learning strategy able to exploit both labeled synthetic data and unlabeled real data and finally a self-teaching strategy working on unlabeled data only. The last component is guided by the segmentation confidence, estimated by the fully convolutional discriminator of the adversarial learning module, helping to further reduce the domain shift between synthetic and real data. Furthermore we weighted this loss on the basis of the class frequencies to enhance the performance on less common classes. Experimental results prove the effectiveness of the proposed strategy in adapting a segmentation network trained on synthetic datasets, like GTA5 and SYNTHIA, to a real dataset as Cityscapes."
  },
  "cvpr2019_autonomous_driving_railsem19adatasetforsemanticrailsceneunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "RailSem19: A Dataset for Semantic Rail Scene Understanding",
    "authors": [
      "Oliver Zendel",
      "Markus Murschitz",
      "Marcel Zeilinger",
      "Daniel Steininger",
      "Sara Abbasi",
      "Csaba Beleznai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Zendel_RailSem19_A_Dataset_for_Semantic_Rail_Scene_Understanding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Zendel_RailSem19_A_Dataset_for_Semantic_Rail_Scene_Understanding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Solving tasks for autonomous road vehicles using computer vision is a dynamic and active research field.However, one aspect of autonomous transportation has received little contributions: the rail domain. In this paper, we introduce the first public dataset for semantic scene understanding for trains and trams: RailSem19. This dataset consists of 8500 annotated short sequences from the ego-perspective of trains, including over 1000 examples with railway crossings and 1200 tram scenes.Since it is the first image dataset targeting the rail domain, a novel label policy has been designed from scratch. It focuses on rail-specific labels not covered by any other datasets. In addition to manual annotations in the form of geometric shapes, we also supply dense pixel-wise semantic labeling. The dense labeling is a semantic-aware combination of (a) the geometric shapes and (b) weakly supervised annotations generated by existing semantic segmentation networks from the road domain. Finally, multiple experiments give a first impression on how the new dataset can be used to improve semantic scene understanding in the rail environment. We present prototypes for the image-based classification of trains, switches, switch states, platforms, buffer stops, rail traffic signs and rail traffic lights.Applying transfer learning, we present an early prototype for pixel-wise semantic segmentation on rail scenes. The resulting predictions show that this new data also significantly improves scene understanding in situations where cars and trains interact."
  },
  "cvpr2019_autonomous_driving_sensorfusionforjoint3dobjectdetectionandsemanticsegmentation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation",
    "authors": [
      "Greg Meyer",
      "Jake Charland",
      "Darshan Hegde",
      "Ankit Laddha",
      "Carlos Vallespi-Gonzalez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Meyer_Sensor_Fusion_for_Joint_3D_Object_Detection_and_Semantic_Segmentation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Meyer_Sensor_Fusion_for_Joint_3D_Object_Detection_and_Semantic_Segmentation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present an extension to LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector. We propose a method for fusing image data with the LiDAR data and show that this sensor fusion method improves the detection performance of the model especially at long ranges. The addition of image data is straightforward and does not require image labels. Furthermore, we expand the capabilities of the model to perform 3D semantic segmentation in addition to 3D object detection. On a large benchmark dataset, we demonstrate our approach achieves state-of-the-art performance on both object detection and semantic segmentation while maintaining a low runtime."
  },
  "cvpr2019_autonomous_driving_6d-vnetend-to-end6-dofvehicleposeestimationfrommonocularrgbimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "6D-VNet: End-to-End 6-DoF Vehicle Pose Estimation From Monocular RGB Images",
    "authors": [
      "Di Wu",
      "Zhaoyong Zhuang",
      "Canqun Xiang",
      "Wenbin Zou",
      "Xia Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Wu_6D-VNet_End-to-End_6-DoF_Vehicle_Pose_Estimation_From_Monocular_RGB_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Wu_6D-VNet_End-to-End_6-DoF_Vehicle_Pose_Estimation_From_Monocular_RGB_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a conceptually simple framework for 6DoF object pose estimation, especially for autonomous driving scenario. Our approach efficiently detects traffic participants in a monocular RGB image while simultaneously regressing their 3D translation and rotation vectors. The method, called 6D-VNet, extends Mask R-CNN by adding customised heads for predicting vehicle's finer class, rotation and translation. The proposed 6D-VNet is trained end-to-end compared to previous methods. Furthermore, we show that the inclusion of translational regression in the joint losses is crucial for the 6DoF pose estimation task, where object translation distance along longitudinal axis varies significantly, e.g., in autonomous driving scenarios. Additionally, we incorporate the mutual information between traffic participants via a modified non-local block. As opposed to the original non-local block implementation, the proposed weighting modification takes the spatial neighbouring information into consideration whilst counteracting the effect of extreme gradient values. Our 6D-VNet reaches the 1 st place in ApolloScape challenge 3D Car Instance task. Code has been made available at: https://github.com/stevenwudi/6DVNET ."
  },
  "cvpr2019_autonomous_driving_rgb-dindoormappingusingdeepfeatures": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "RGB-D Indoor Mapping Using Deep Features",
    "authors": [
      "Oguzhan Guclu",
      "Ali Caglayan",
      "Ahmet Burak Can"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Guclu_RGB-D_Indoor_Mapping_Using_Deep_Features_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Guclu_RGB-D_Indoor_Mapping_Using_Deep_Features_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " RGB-D indoor mapping has been an active research topic in the last decade with the advance of depth sensors. However, despite the great success of deep learning techniques on various problems, similar approaches for SLAM have not been much addressed yet. In this work, an RGB-D SLAM system using a deep learning approach for mapping indoor environments is proposed. A pre-trained CNN model with multiple random recursive structures is utilized to acquire deep features in an efficient way with no need for training. Deep features present strong representations from color frames and enable better data association. To increase computational efficiency, deep feature vectors are considered as points in a high dimensional space and indexed in a priority search k-means tree. The search precision is improved by employing an adaptive mechanism. For motion estimation, a sparse feature based approach is adopted by employing a robust keypoint detector and descriptor combination. The system is assessed on TUM RGB-D benchmark using the sequences recorded in medium and large sized environments. The experimental results demonstrate the accuracy and robustness of the proposed system over the state-of-the-art, especially in large sequences."
  },
  "cvpr2019_autonomous_driving_distancenetestimatingtraveleddistancefrommonocularimagesusingarecurrentconvolutionalneuralnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "DistanceNet: Estimating Traveled Distance From Monocular Images Using a Recurrent Convolutional Neural Network",
    "authors": [
      "Robin Kreuzig",
      "Matthias Ochs",
      "Rudolf Mester"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Kreuzig_DistanceNet_Estimating_Traveled_Distance_From_Monocular_Images_Using_a_Recurrent_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Kreuzig_DistanceNet_Estimating_Traveled_Distance_From_Monocular_Images_Using_a_Recurrent_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Classical monocular vSLAM/VO methods suffer from the scale ambiguity problem. Hybrid approaches solve this problem by adding deep learning methods, for example by using depth maps which are predicted by a CNN. We suggest that it is better to base scale estimation on estimating the traveled distance for a set of subsequent images. In this paper, we propose a novel end-to-end many-to-one traveled distance estimator. By using a deep recurrent convolutional neural network (RCNN), the traveled distance between the first and last image of a set of consecutive frames is estimated by our DistanceNet. Geometric features are learned in the CNN part of our model, which are subsequently used by the RNN to learn dynamics and temporal information. Moreover, we exploit the natural order of distances by using ordinal regression to predict the distance. The evaluation on the KITTI dataset shows that our approach outperforms current state-of-the-art deep learning pose estimators and classical mono vSLAM/VO methods in terms of distance prediction. Thus, our DistanceNet can be used as a component to solve the scale problem and help improve current and future classical mono vSLAM/VO methods."
  },
  "cvpr2019_autonomous_driving_roadsrandomizationforobstacleavoidanceanddrivinginsimulation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "ROADS: Randomization for Obstacle Avoidance and Driving in Simulation",
    "authors": [
      "Samira Pouyanfar",
      "Muneeb Saleem",
      "Nikhil George",
      "Shu-Ching Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Pouyanfar_ROADS_Randomization_for_Obstacle_Avoidance_and_Driving_in_Simulation_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Pouyanfar_ROADS_Randomization_for_Obstacle_Avoidance_and_Driving_in_Simulation_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " End-to-end deep learning has emerged as a simple and promising approach for autonomous driving recently. However, collecting large-scale real-world data representing the full spectrum of road scenarios and rare events remains the main hurdle in this area. For this purpose, this paper addresses the problem of end-to-end collision-free deep driving using only simulation data. It extends the idea of domain randomization to bridge the reality gap between simulation and the real world. Using a range of domain randomization flavors in a primitive simulation, it is shown that a model can learn to drive in realistic environments without seeing any real or photo-realistic images. The proposed work dramatically reduces the need for collecting large real-world or high-fidelity simulated datasets, along with allowing for the creation of rare events in the simulation. Finally, this is the first time domain randomization is used for the application of \"deep driving\" which can avoid obstacles. The effectiveness of the proposed method is demonstrated with extensive experiments on both simulation and real-world datasets. "
  },
  "cvpr2019_autonomous_driving_real-timephysics-basedremovalofshadowsandshadingfromroadsurfaces": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Real-Time Physics-Based Removal of Shadows and Shading From Road Surfaces",
    "authors": [
      "Bruce Maxwell",
      "Casey Smith",
      "Maan Qraitem",
      "Spencer Whitt",
      "Ross Messing",
      "Nicholas Thien",
      "Richard Friedhoff"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Maxwell_Real-Time_Physics-Based_Removal_of_Shadows_and_Shading_From_Road_Surfaces_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Maxwell_Real-Time_Physics-Based_Removal_of_Shadows_and_Shading_From_Road_Surfaces_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a real-time physics-based system for generating an illumination free representation of road surfaces that maintains the distinction between asphalt and painted road markings.Cast shadows on road surfaces can create false features and modify the color of road markings, potentially masking important information for vehicle vision systems. We demonstrate a novel method for identifying the relative spectral properties of the direct and ambient illumination conditions and for using that to create an illumination-free 2D chromaticity space in log RGB. We then show how that representation can be used to generate an illumination-free greyscale representation that distinguishes road, white paint, and yellow paint, making it suitable for further analysis and classification. The entire process runs faster than 30Hz on current automotive-grade embedded processing systems.We evaluate the system on a paint detection task, comparing two types of learned classifiers, random forests and convolutional neural networks. For each type, one classifier is trained on the original images, and the other is trained on the illumination-free greyscale output. The classifiers are of identical complexity and trained on the same size data set. For both types, the classifier trained on the illumination-free outputs performs better, even on images with no cast shadows. The gap in performance is indicative of the cost of forcing a classifier to learn a task in the presence of the confounding illumination signal."
  },
  "cvpr2019_autonomous_driving_spatialsamplingnetworkforfastsceneunderstanding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Spatial Sampling Network for Fast Scene Understanding",
    "authors": [
      "Davide Mazzini",
      "Raimondo Schettini"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Mazzini_Spatial_Sampling_Network_for_Fast_Scene_Understanding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Mazzini_Spatial_Sampling_Network_for_Fast_Scene_Understanding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a network architecture to perform efficient scene understanding. This work presents three main novelties: the first is a module named Improved Guided Upsampling Module that can replace in toto the decoder part in common semantic segmentation networks. Our second contribution is the introduction of a new module based on spatial sampling to perform Instance Segmentation. It provides a very fast instance segmentation needing only a simple post-processing step at inference time. Finally, we propose a novel efficient network design that includes the new modules and test it against different datasets for outdoor scene understanding. To our knowledge,our network is one of the most efficient architectures for scene understanding published to date, furthermore being 8.6% more accurate than the fastest competitor on semantic segmentation and almost five times faster than the most efficient network for instance segmentation."
  },
  "cvpr2019_autonomous_driving_attentionalpointnetfor3d-objectdetectioninpointclouds": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Attentional PointNet for 3D-Object Detection in Point Clouds",
    "authors": [
      "Anshul Paigwar",
      "Ozgur Erkent",
      "Christian Wolf",
      "Christian laugier"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate detection of objects in 3D point clouds is a central problem for autonomous navigation. Most existing methods use techniques of hand-crafted features representation or multi-sensor approaches prone to sensor failure. Approaches like PointNet that directly operate on sparse point data have shown good accuracy in the classification of single 3D objects. However, LiDAR sensors on Autonomous Vehicles generate a large scale point cloud. Real-time object detection in such a cluttered environment still remains a challenge. In this study, we propose Attentional PointNet, which is a novel end-to-end trainable deep architecture for object detection in point clouds. We extend the theory of visual attention mechanisms to 3D point clouds and introduce a new recurrent 3D Localization Network module. Rather than processing the whole point cloud, the network learns where to look (finding regions of interest), which significantly reduces the number of points to be processed and inference time. Evaluation on KITTI car detection benchmark shows that our Attentional PointNet achieves comparable results with the state-of-the-art LiDAR-based 3D detection methods in detection and speed."
  },
  "cvpr2019_autonomous_driving_accuratevisuallocalizationforautomotiveapplications": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Accurate Visual Localization for Automotive Applications",
    "authors": [
      "Eli Brosh",
      "Matan Friedmann",
      "Ilan Kadar",
      "Lev Yitzhak  Lavy",
      "Elad Levi",
      "Shmuel Rippa",
      "Yair  Lempert",
      "Bruno Fernandez-Ruiz",
      "Roei Herzig",
      "Trevor Darrell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Brosh_Accurate_Visual_Localization_for_Automotive_Applications_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Brosh_Accurate_Visual_Localization_for_Automotive_Applications_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Accurate vehicle localization is a crucial step towards building effective Vehicle-to-Vehicle networks and automotive applications. Yet standard grade GPS data, such as that provided by mobile phones, is often noisy and exhibits significant localization errors in many urban areas. Approaches for accurate localization from imagery often rely on structure-based techniques, and thus are limited in scale and are expensive to compute. In this paper, we present a scalable visual localization approach geared for real-time performance. We propose a hybrid coarse-to-fine approach that leverages visual and GPS location cues. Our solution uses a self-supervised approach to learn a compact road image representation. This representation enables efficient visual retrieval and provides coarse localization cues, which are fused with vehicle ego-motion to obtain high accuracy location estimates.As a benchmark to evaluate the performance of our visual localization approach, we introduce a new large-scale driving dataset based on video and GPS data obtained from a large-scale network of connected dash-cams. Our experiments confirm that our approach is highly effective in challenging urban environments, reducing localization error by an order of magnitude."
  },
  "cvpr2019_autonomous_driving_dscnetreplicatinglidarpointcloudswithdeepsensorcloning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "DSCnet: Replicating Lidar Point Clouds With Deep Sensor Cloning",
    "authors": [
      "Paden Tomasello",
      "Sammy Sidhu",
      "Anting Shen",
      "Matthew Moskewicz",
      "Nobie Redmon",
      "Gayatri Joshi",
      "Romi Phadte",
      "Paras Jain",
      "Forrest Iandola"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Tomasello_DSCnet_Replicating_Lidar_Point_Clouds_With_Deep_Sensor_Cloning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Tomasello_DSCnet_Replicating_Lidar_Point_Clouds_With_Deep_Sensor_Cloning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional neural networks (CNNs) have become increasingly popular for solving a variety of computer vision tasks, ranging from image classification to image segmentation. Recently, autonomous vehicles have created a demand for depth information, which is often obtained using hardware sensors such as Light detection and ranging (LIDAR). Although it can provide precise distance measurements, most LIDARs are still far too expensive to sell in mass-produced consumer vehicles, which has motivated methods to generate depth information from commodity automotive sensors like cameras.In this paper, we propose an approach called Deep Sensor Cloning (DSC). The idea is to use Convolutional Neural Networks in conjunction with inexpensive sensors to replicate the 3D point-clouds that are created by expensive LIDARs. To accomplish this, we develop a new dataset (DSCdata) and a new family of CNN architectures (DSCnets). While previous tasks such as KITTI depth prediction use interpolated RGB-D images as ground-truth for training, we instead use DSCnets to directly predict LIDAR point-clouds.When we compare the output of our models to a 75,000 LIDAR, we find that our most accurate DSCnet achieves a relative error of 5.77% using a single camera and 4.69% using stereo cameras. "
  },
  "cvpr2019_autonomous_driving_attention-basedhierarchicaldeepreinforcementlearningforlanechangebehaviorsinautonomousdriving": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Attention-Based Hierarchical Deep Reinforcement Learning for Lane Change Behaviors in Autonomous Driving",
    "authors": [
      "Yilun Chen",
      "Chiyu Dong",
      "Palanisamy Praveen",
      "Mudalige Priyantha",
      "Katherina Muelling",
      "John Dolan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Chen_Attention-Based_Hierarchical_Deep_Reinforcement_Learning_for_Lane_Change_Behaviors_in_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Chen_Attention-Based_Hierarchical_Deep_Reinforcement_Learning_for_Lane_Change_Behaviors_in_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Performing safe and efficient lane changes is a crucial feature for creating fully autonomous vehicles. Recent advances have demonstrated successful lane following behavior using deep reinforcement learning, yet the interactions with other vehicles on road for lane changes are rarely considered. In this paper, we design a hierarchical Deep Reinforcement Learning (DRL) algorithm to learn lane change behaviors in dense traffic. By breaking down overall behavior to sub-policies, faster and safer lane change actions can be learned. We also apply temporal and spatial attention to the DRL architecture, which helps the vehicle focus more on surrounding vehicles and leads to smoother lane change behavior. We conduct our experiments in the TORCS simulator and the results outperform the state-of-art deep reinforcement learning algorithm in various lane change scenarios."
  },
  "cvpr2019_autonomous_driving_arguingmachineshumansupervisionofblackboxaisystemsthatmakelife-criticaldecisions": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "Autonomous_Driving",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Autonomous Driving",
    "title": "Arguing Machines: Human Supervision of Black Box AI Systems That Make Life-Critical Decisions",
    "authors": [
      "Lex Fridman",
      "Li Ding",
      "Benedikt Jenik",
      "Bryan Reimer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/Autonomous_Driving/Fridman_Arguing_Machines_Human_Supervision_of_Black_Box_AI_Systems_That_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/Autonomous Driving/Fridman_Arguing_Machines_Human_Supervision_of_Black_Box_AI_Systems_That_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We consider the paradigm of a black box AI system that makes life-critical decisions. We propose an \"arguing machines\" framework that pairs the primary AI system with a secondary one that is independently trained to perform the same task. We show that disagreement between the two systems, without any knowledge of underlying system design or operation, is sufficient to arbitrarily improve the accuracy of the overall decision pipeline given human supervision over disagreements. We demonstrate this system in two applications: (1) an illustrative example of image classification and (2) on large-scale real-world semi-autonomous driving data.For the first application, we apply this framework to image classification achieving a reduction from 8.0% to 2.8% top-5 error on ImageNet. For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4% of system disengagements that were labeled by human annotators as challenging and needing human supervision."
  },
  "cvpr2019_cv4gc_buildinghighresolutionmapsforhumanitarianaidanddevelopmentwithweakly-andsemi-supervisedlearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Building High Resolution Maps for Humanitarian Aid and Development with Weakly- and Semi-Supervised Learning",
    "authors": [
      "Derrick Bonafilia",
      "James Gill",
      "Saikat Basu",
      "David Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Bonafilia_Building_High_Resolution_Maps_for_Humanitarian_Aid_and_Development_with_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Bonafilia_Building_High_Resolution_Maps_for_Humanitarian_Aid_and_Development_with_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Detailed maps help governments and NGOs plan infrastructure development and mobilize relief around the world. Mapping is an open-ended task with a seemingly endless number of potentially useful features to be mapped. In this work, we focus on mapping buildings and roads. We do so with techniques that could easily extend to other features such as land use and land classification. We discuss real-world use cases of our maps by NGOs and humanitarian organizations around the world---from sustainable infrastructure planning to disaster relief. We investigate the pitfalls of existing datasets for building detection and road segmentation and highlight the way that models trained on these datasets---which tend to be highly specific to particular regions---produce worse results in regions of the world not adequately represented in the training set. We explain how we used data from OpenStreetMap (OSM) to train more generalizable models. These models outperform those trained on existing datasets, even in regions in which those models are overfit, and produce these same high-quality results for a diverse range of geographic areas. We utilize a combination of weakly-supervised and semi-supervised learning techniques that allow us to train on the noisy, crowdsourced data in OSM for building detection, which we formulate as a binary classification problem. We then show how weakly supervised learning techniques in conjunction with simple heuristics allowed us to train a semantic segmentation model for road extraction on noisy and never pixel-perfect training data from OSM."
  },
  "cvpr2019_cv4gc_creatingxbdadatasetforassessingbuildingdamagefromsatelliteimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Creating xBD: A Dataset for Assessing Building Damage from Satellite Imagery",
    "authors": [
      "Ritwik Gupta",
      "Bryce Goodman",
      "Nirav Patel",
      "Ricky Hosfelt",
      "Sandra Sajeev",
      "Eric Heim",
      "Jigar Doshi",
      "Keane Lucas",
      "Howie Choset",
      "Matthew Gaston"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a preliminary report for xBD, a new large-scale dataset for the advancement of change detection and building damage assessment for humanitarian assistance and disaster recovery research. Logistics, resource planning, and damage estimation are difficult tasks after a disaster, and putting first responders into post-disaster situations is dangerous and costly. Using passive methods, such as analysis on satellite imagery, to perform damage assessment saves manpower, lowers risk, and expedites an otherwise dangerous process. xBD provides pre- and post-event multi-band satellite imagery from a variety of disaster events with building polygons, classification labels for damage types, ordinal labels of damage level, and corresponding satellite metadata. Furthermore, the dataset contains bounding boxes and labels for environmental factors such as water, smoke, and lava. xBD will be the largest building damage assessment dataset to date, containing ~700,000 building annotations across over 5,000 km\\textsuperscript 2of imagery from 15 countries."
  },
  "cvpr2019_cv4gc_weaklylabelingtheantarcticthepenguincolonycase": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Weakly Labeling the Antarctic: The Penguin Colony Case",
    "authors": [
      "Hieu M Le",
      "Bento Goncalves",
      "Dimitris Samaras",
      "Heather Lynch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Le_Weakly_Labeling_the_Antarctic_The_Penguin_Colony_Case_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Le_Weakly_Labeling_the_Antarctic_The_Penguin_Colony_Case_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Antarctic penguins are important ecological indicators -- especially in the face of climate change. In this work, we present a deep learning based model for semantic segmentation of Adelie penguin colonies in high-resolution satellite imagery. To train our segmentation models, we take advantage of the Penguin Colony Dataset: a unique dataset with 2044 georeferenced cropped images from 193 Adelie penguin colonies in Antarctica. In the face of a scarcity of pixel-level annotation masks, we propose a weakly-supervised framework to effectively learn a segmentation model from weak labels. We use a classification network to filter out data unsuitable for the segmentation network. This segmentation network is trained with a specific loss function, based on the average activation, to effectively learn from the data with the weakly-annotated labels. Our experiments show that adding weakly-annotated training examples significantly improves segmentation performance, increasing the mean Intersection-over-Union from 42.3 to 60.0% on the Penguin Colony Dataset."
  },
  "cvpr2019_cv4gc_towardsautonomousminingviaintelligentexcavators": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Towards Autonomous Mining via Intelligent Excavators",
    "authors": [
      "Hooman Shariati",
      "Anuar Yeraliyev",
      "Burhan Terai",
      "Shahram Tafazoli",
      "Mahdi Ramezani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Shariati_Towards_Autonomous_Mining_via_Intelligent_Excavators_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Shariati_Towards_Autonomous_Mining_via_Intelligent_Excavators_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper we present our first step solution towards global challenge of safety, productivity, profitability and energy-efficiency in mining. Our solution (intelligent excavator) provides complete monitoring solution for excavators that relies on deep neural networks to produce accurate, actionable data for mine. Our solution helps mines to increase shovel efficiency, reduce unexpected downtime cost, enable planned maintenance. We use a multi-frame convolutional LSTM-based object detection approach to accumulate valuable information across video frames without significant computational overhead. Our experiments on dataset captured in several mines across the world show that we can detect objects of interest with accuracy of more than 90% on 10 FPS. Furthermore, we show that our approach generalizes well to mining sites and equipment types not encountered in our training set. Finally, our work on detecting the types of objects encountered in a mining equipment could be used as a first step in developing a perception module that could provide autonomous excavators with the required knowledge of their environment in order to make optimal decisions."
  },
  "cvpr2019_cv4gc_displacenetrecognisingdisplacedpeoplefromimagesbyexploitingdominancelevel": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level",
    "authors": [
      "Grigorios Kalliatakis",
      "Shoaib Ehsan",
      "Maria Fasli",
      "Klaus D McDonald-Maier"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Kalliatakis_DisplaceNet_Recognising_Displaced_People_from_Images_by_Exploiting_Dominance_Level_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Kalliatakis_DisplaceNet_Recognising_Displaced_People_from_Images_by_Exploiting_Dominance_Level_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Every year millions of men, women and children are forced to leave their homes and seek refuge from wars, human rights violations, persecution, and natural disasters. The number of forcibly displaced people came at a record rate of 44,400 every day throughout 2017, raising the cumulative total to 68.5 million at the years end, overtaken the total population of the United Kingdom. Up to 85% of the forcibly displaced find refuge in low- and middle-income countries, calling for increased humanitarian assistance worldwide. To reduce the amount of manual labour required for human-rights-related image analysis, we introduce DisplaceNet, a novel model which infers potential displaced people from images by integrating the control level of the situation and conventional convolutional neural network (CNN) classifier into one framework for image classification. Experimental results show that DisplaceNet achieves up to 4% coverage-the proportion of a data set for which a classifier is able to produce a prediction-gain over the sole use of a CNN classifier. Our dataset, codes and trained models will be available online at https: //github.com/GKalliatakis/DisplaceNet"
  },
  "cvpr2019_cv4gc_tiny-inception-resnet-v2usingdeeplearningforeliminatingbondedlaborsofbrickkilnsinsouthasia": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Tiny-Inception-ResNet-v2: Using Deep Learning for Eliminating Bonded Labors of Brick Kilns in South Asia",
    "authors": [
      "Usman Nazir",
      "Numan Khurshid",
      "Muhammad Ahmed Bhimra",
      "Murtaza Taj"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Nazir_Tiny-Inception-ResNet-v2_Using_Deep_Learning_for_Eliminating_Bonded_Labors_of_Brick_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Nazir_Tiny-Inception-ResNet-v2_Using_Deep_Learning_for_Eliminating_Bonded_Labors_of_Brick_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper proposes to employ a Inception-ResNet inspired deep learning architecture called Tiny-Inception-ResNet-v2 to eliminate bonded labor by identifying brick kilns within \"Brick-Kiln-Belt\" of South Asia. The framework is developed by training a network on the satellite imagery consisting of 11 different classes of South Asian region. The dataset developed during the process includes the geo-referenced images of brick kilns, houses, roads, tennis courts, farms, black farms, dense trees, orchards, parking lots, parks and barren lands. The dataset is made publicly available for further research. Our proposed network architecture with very fewer learning parameters outperforms all state-of-the-art architectures employed for recognition of brick kilns. Our proposed solution would enable regional monitoring and evaluation mechanisms for the Sustainable Development Goals."
  },
  "cvpr2019_cv4gc_deeplandscapefeaturesforimprovingvector-bornediseaseprediction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Deep Landscape Features for Improving Vector-borne Disease Prediction",
    "authors": [
      "Nabeel Abdur Rehman",
      "Umar Saif",
      "Rumi Chunara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Rehman_Deep_Landscape_Features_for_Improving_Vector-borne_Disease_Prediction_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Rehman_Deep_Landscape_Features_for_Improving_Vector-borne_Disease_Prediction_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The global population at risk of mosquito-borne diseases such as dengue, yellow fever, chikungunya and Zika is expanding. Infectious disease models commonly incorporate environmental measures like temperature and precipitation. Given increasing availability of high-resolution satellite imagery, here we consider including landscape features from satellite imagery into infectious disease prediction models. To do so, we implement a Convolutional Neural Network (CNN) model trained on Imagenet data and labelled landscape features in satellite data from London. We then incorporate landscape features from satellite image data from Pakistan, labelled using the CNN, in a well-known Susceptible-Infectious-Recovered epidemic model, alongside dengue case data from 2012-2016 in Pakistan. We study improvement of the prediction model for each of the individual landscape features, and assess the feasibility of using image labels from a different place. We find that incorporating satellite-derived landscape features can improve prediction of outbreaks, which is important for proactive and strategic surveillance and control programmes."
  },
  "cvpr2019_cv4gc_doesobjectrecognitionworkforeveryone?": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Does Object Recognition Work for Everyone?",
    "authors": [
      "Terrance de Vries",
      "Ishan Misra",
      "Changhan Wang",
      "Laurens van der Maaten"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The paper analyzes the accuracy of publicly available object-recognition systems on a geographically diverse dataset. This dataset contains household items and was designed to have a more representative geographical coverage than commonly used image datasets in object recognition. We find that the systems perform relatively poorly on household items that commonly occur in countries with a low household income. Qualitative analyses suggest the drop in performance is primarily due to appearance differences within an object class (e.g., dish soap) and due to items appearing in a different context (e.g., toothbrushes appearing outside of bathrooms). The results of our study suggest that further work is needed to make object-recognition systems work equally well for people across different countries and income levels."
  },
  "cvpr2019_cv4gc_towardsequitableaccesstoinformationandopportunityforallmappingschoolswithhigh-resolutionsatelliteimageryandmachinelearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Towards equitable access to information and opportunity for all: mapping schools with high-resolution Satellite Imagery and Machine Learning",
    "authors": [
      "Zhuangfang Yi",
      "Naroa Zurutuza",
      "Drew Bollinger",
      "Manuel Garcia-Herranz",
      "Dohyung Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Yi_Towards_equitable_access_to_information_and_opportunity_for_all_mapping_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Yi_Towards_equitable_access_to_information_and_opportunity_for_all_mapping_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Having accurate data about schools is key for organizations to provide quality education and promote lifelong learning, listed as UN sustainable development goal 4 (SDG4), ensure equal access to opportunity (SDG10) and eventually, reduce poverty (SDG1). However, this is a challenging task since educational facilities' records are often inaccurate, incomplete or non-existent. By leveraging machine learning and high-resolution imagery, we are able to determine school detection at the national scale.Infant-Prints: Fingerprints for Reducing Infant Mortality"
  },
  "cvpr2019_cv4gc_semanticsegmentationofcroptypeinafricaanoveldatasetandanalysisofdeeplearningmethods": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Semantic Segmentation of Crop Type in Africa: A Novel Dataset and Analysis of Deep Learning Methods",
    "authors": [
      "Rose M Rustowicz",
      "Robin Cheong",
      "Lijing Wang",
      "Stefano Ermon",
      "Marshall Burke",
      "David Lobell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Rustowicz_Semantic_Segmentation_of_Crop_Type_in_Africa_A_Novel_Dataset_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Rustowicz_Semantic_Segmentation_of_Crop_Type_in_Africa_A_Novel_Dataset_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Automatic, accurate crop type maps can provide unprecedented information for understanding food systems, especially in developing countries where ground surveys are infrequent. However, little work has applied existing methods to these data scarce environments, which also have unique challenges of irregularly shaped fields, frequent cloud coverage, small plots, and a severe lack of training data. To address this gap in the literature, we provide the first crop type semantic segmentation dataset of small holder farms, specifically in Ghana and South Sudan. We are also the first to utilize high resolution, high frequency satellite data in segmenting small holder farms. Despite the challenges, we achieve an average F1 score and overall accuracy of 57.3 and 60.9% in Ghana and 69.7 and 85.3% in South Sudan. Additionally, our approach outperforms the state-of-the-art method in a data-rich setting of Germany by over 8 points in F1 and 6 points in accuracy. Code and a link to the dataset are publicly available at https://github.com/roserustowicz/crop-type-mapping."
  },
  "cvpr2019_cv4gc_detectingroadsfromsatelliteimageryinthedevelopingworld": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Detecting Roads from Satellite Imagery in the Developing World",
    "authors": [
      "Yoni Nachmany",
      "Hamed Alemohammad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Nachmany_Detecting_Roads_from_Satellite_Imagery_in_the_Developing_World_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Nachmany_Detecting_Roads_from_Satellite_Imagery_in_the_Developing_World_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Advances in computer vision are improving the ability to accurately extract structured information from frequent and high-resolution satellite imagery, shedding light on global challenges and furthering Sustainable Development Goals. While these advances, along with increased availability of high capacity computational resources, result in improved models, lack of diverse training data significantly limits applications of these models to certain geographical regions. We review state-of-the-art models for road detection using satellite imagery, and compare predictions of two models (one trained in Las Vegas, USA and another in Khartoum, Sudan) in Khartoum. This comparison shows the need for regionally trained models using local training data. Finally, we outline a roadmap to use transfer learning and regional models in cities that do not have human verified labels."
  },
  "cvpr2019_cv4gc_predictingcitypovertyusingsatelliteimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Predicting City Poverty Using Satellite Imagery",
    "authors": [
      "Simone Piaggesi",
      "Laetitia Gauvin",
      "Michele Tizzoni",
      "Ciro Cattuto",
      "Natalia Adler",
      "Stefaan Verhulst",
      "Andrew Young",
      "Rhiannan Price",
      "Leo Ferres",
      "Andre Panisson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Piaggesi_Predicting_City_Poverty_Using_Satellite_Imagery_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Piaggesi_Predicting_City_Poverty_Using_Satellite_Imagery_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Reliable data about socio-economic conditions of individuals, such as health indexes, consumption expenditures and wealth assets, remain scarce for most countries. Traditional methods to collect such data include on site surveys that can be expensive and labour intensive. On the other hand, remote sensing data, such as high-resolution satellite imagery, are becoming largely available. To circumvent the lack of socio-economic data at high granularity, computer vision has already been applied successfully to raw satellite imagery sampled from resource poor countries. "
  },
  "cvpr2019_cv4gc_infant-printsfingerprintsforreducinginfantmortality": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "Infant-Prints: Fingerprints for Reducing Infant Mortality",
    "authors": [
      "Joshua J Engelsma",
      "Debayan Deb",
      "Anil Jain",
      "Anjoo Bhatnagar",
      "Prem Sewak Sudhish"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Engelsma_Infant-Prints_Fingerprints_for_Reducing_Infant_Mortality_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Engelsma_Infant-Prints_Fingerprints_for_Reducing_Infant_Mortality_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In developing countries around the world, a multitude of infants continue to suffer and die from vaccine-preventable diseases, and malnutrition. Lamentably, the lack of any official identification documentation makes it exceedingly difficult to prevent these infant deaths. To solve this global crisis, we propose Infant-Prints which is comprised of (i) a custom, compact, low-cost (85 USD), high-resolution (1,900 ppi) fingerprint reader, (ii) a high-resolution fingerprint matcher, and (iii) a mobile application for search and verification for the infant fingerprint. Using Infant-Prints, we have collected a longitudinal database of infant fingerprints and demonstrate its ability to perform accurate and reliable recognition of infants enrolled at the ages 0-3 months, in time for effective delivery of critical vaccinations and nutritional supplements (TAR=90% @ FAR = 0.1% for infants older than 8 weeks)."
  },
  "cvpr2019_cv4gc_seethee-waste!trainingvisualintelligencetoseedensecircuitboardsforrecycling": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "cv4gc",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - COMPUTER VISION FOR GLOBAL CHALLENGES",
    "title": "See the E-Waste! Training Visual Intelligence to See Dense Circuit Boards for Recycling",
    "authors": [
      "Ali Jahanian",
      "Quang H. Le",
      "Kamal Youcef-Toumi",
      "Dzmitry Tsetserukou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/cv4gc/Jahanian_See_the_E-Waste_Training_Visual_Intelligence_to_See_Dense_Circuit_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/cv4gc/Jahanian_See_the_E-Waste_Training_Visual_Intelligence_to_See_Dense_Circuit_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The state-of-the-art semantic segmentation and object detection deep learning models are taking the leap to generalize and leverage automation, but have yet to be useful in real-world tasks such as those in dense circuit board robotic manipulation. Consider a cellphone circuit board that because of small components and a couple of hundred microns gaps between them challenges any manipulation task. For effective automation and robotics usage in manufacturing, we tackle this problem by building a convolutional neural networks optimized for multi-task learning of instance semantic segmentation and detection while accounting for crisp boundaries of small components inside dense boards. We explore the feature learning mechanism, and add the auxiliary task of boundary detection to encourage the network to learn the objects' geometric properties along with the other objectives. We examine the performance of the networks in the visual tasks (separately and all together), and the extent of generalization on the recycling phone dataset. Our network outperformed the state-of-the-art in the visual tasks while maintaining the high speed of computation. To facilitate this globally concerning topic, we provide a benchmark for Ewaste visual tasks research, and publicize our collected dataset and code, as well as demos on our in-lab robot at https://github.com/MIT-MRL/recybot."
  },
  "cvpr2019_dynavis_3dhumanposeestimationfrommultipersonstereo360scenes": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DynaVis",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DynaVis: The First International Workshop on Dynamic Scene Reconstruction",
    "title": "3D Human Pose Estimation From Multi Person Stereo 360 Scenes",
    "authors": [
      "Matthew Shere",
      "Hansung Kim",
      "Adrian Hilton"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DynaVis/Shere_3D_Human_Pose_Estimation_From_Multi_Person_Stereo_360_Scenes_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DynaVis - The First International Workshop on Dynamic Scene Reconstruction/Shere_3D_Human_Pose_Estimation_From_Multi_Person_Stereo_360_Scenes_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper presents a human tracking and 3D pose estimation algorithm for use with a pair of 360 cameras. We identify and track an individual throughout complex, multi-person scenes in both indoor and outdoor environments using appearance models and positional data, and produce a temporally consistent 3D skeleton by optimising a skeleton of realistic joint lengths over joint positions produce by Convolutional Pose Machines (CPMs). Our results show an average improvement of 22.67% over state of the art deep learning approaches for tracking, as well as reasonable estimates for pose using just two cameras."
  },
  "cvpr2019_dynavis_livereconstructionoflarge-scaledynamicoutdoorworlds": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DynaVis",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DynaVis: The First International Workshop on Dynamic Scene Reconstruction",
    "title": "Live Reconstruction of Large-Scale Dynamic Outdoor Worlds",
    "authors": [
      "Ondrej Miksik",
      "Vibhav Vineet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DynaVis/Miksik_Live_Reconstruction_of_Large-Scale_Dynamic_Outdoor_Worlds_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DynaVis - The First International Workshop on Dynamic Scene Reconstruction/Miksik_Live_Reconstruction_of_Large-Scale_Dynamic_Outdoor_Worlds_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Standard 3D reconstruction pipelines assume stationary world, therefore suffer from \"ghost artifacts\" whenever dynamic objects are present in the scene. Recent approaches has started tackling this issue, however, they typically either only discard dynamic information, represent it using bounding boxes or per-frame depth or rely on approaches that are inherently slow and not suitable to online settings. We propose an end-to-end system for live reconstruction of large-scale outdoor dynamic environments. We leverage recent advances in computationally efficient data-driven approaches for 6-DoF object pose estimation to segment the scene into objects and stationary \"background\". This allows us to represent the scene using a time-dependent(dynamic) map, in which each object is explicitly represented as a separate instance and reconstructed in its own volume. For each time step, our dynamic map maintains a relative pose of each volume with respect to the stationary background. Our system operates in incremental manner which is essential for on-line reconstruction, handles large-scale environments with objects at large distances and runs in (near) real-time. We demonstrate the efficacy of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense 3D reconstructions of a number of dynamic scenes."
  },
  "cvpr2019_3dwiddget_semi-supervisedthree-dimensionalreconstructionframeworkwithgenerativeadversarialnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "3DWidDGET",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3DWidDGET",
    "title": "Semi-supervised Three-dimensional Reconstruction Framework with Generative Adversarial Networks",
    "authors": [
      "Chong Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/3DWidDGET/Yu_Semi-supervised_Three-dimensional_Reconstruction_Framework_with_Generative_Adversarial_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/3D-WidDGET/Yu_Semi-supervised_Three-dimensional_Reconstruction_Framework_with_Generative_Adversarial_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Because of the intrinsic complexity in computation, three-dimensional (3D) reconstruction is an essential and challenging topic in computer vision research and applications. The existing methods for 3D reconstruction often produce holes, distortions and obscure parts in the reconstructed 3D models, or can only reconstruct voxelized 3D models for simple isolated objects. So they are not adequate for real usage. From 2014, the Generative Adversarial Network (GAN) is widely used in generating unreal datasets and semi-supervised learning. So the focus of this paper is to achieve high-quality 3D reconstruction performance by adopting the GAN principle. We propose a novel semi-supervised 3D reconstruction framework, namely SS-3D-GAN, which can iteratively improve any raw 3D reconstruction models by training the GAN models to converge. This new model only takes real-time 2D observation images as the weak supervision and doesn't rely on prior knowledge of shape models or any referenced observations. Finally, through the qualitative and quantitative experiments & analysis, this new method shows compelling advantages over the current state-of-the-art methods on the Tanks & Temples reconstruction benchmark dataset."
  },
  "cvpr2019_3dwiddget_aconditionalgenerativeadversarialnetworkforrenderingpointclouds": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "3DWidDGET",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3DWidDGET",
    "title": "A Conditional Generative Adversarial Network for Rendering Point Clouds",
    "authors": [
      "Rowel Atienza"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/3DWidDGET/Atienza_A_Conditional_Generative_Adversarial_Network_for_Rendering_Point_Clouds_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/3D-WidDGET/Atienza_A_Conditional_Generative_Adversarial_Network_for_Rendering_Point_Clouds_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In computer graphics, point clouds from laser scanning devices are difficult to render into photo-realistic images due to lack of information they carry about color, normal, lighting, and connection between points. Rendering a point cloud after surface mesh reconstruction generally results into poor image quality with many noticeable artifacts. In this paper, we propose a conditional generative adversarial network that directly renders a point cloud given the azimuth and elevation angles of camera viewpoint. The proposed method, called pc2pix, renders point clouds into objects with higher class similarity with the ground truth as compared to images from surface reconstruction. pc2pix is also significantly faster, more robust to noise and can operate on a lower number of points. The code is available at: https://github.com/roatienza/pc2pix."
  },
  "cvpr2019_3dwiddget_differmovingbeyond3dreconstructionwithdifferentiablefeaturerendering": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "3DWidDGET",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3DWidDGET",
    "title": "DIFFER: Moving Beyond 3D Reconstruction with Differentiable Feature Rendering",
    "authors": [
      "K L Navaneet",
      "Priyanka Mandikal",
      "Varun Jampani",
      "Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/3DWidDGET/K_L_Navaneet_DIFFER_Moving_Beyond_3D_Reconstruction_with_Differentiable_Feature_Rendering_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/3D-WidDGET/K_L_Navaneet_DIFFER_Moving_Beyond_3D_Reconstruction_with_Differentiable_Feature_Rendering_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Perception of 3D object properties from 2D images form one of the core computer vision problems. In this work, we propose a deep learning system that can simultaneously reason about 3D shape as well as associated properties (such as color, semantic part segments) directly from a single 2D image. We devise a novel depth-aware differentiable feature rendering module (DIFFER) that is used to train our model by using only 2D supervision. Experiments on both synthetic ShapeNet dataset and the real-world Pix3D dataset demonstrate that our 2D supervised DIFFER model performs on par or sometimes even outperforms existing 3D supervised models."
  },
  "cvpr2019_3dwiddget_coarse-to-fine3dfacereconstruction": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "3DWidDGET",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3DWidDGET",
    "title": "Coarse-to-Fine 3D Face Reconstruction",
    "authors": [
      "Leonardo Galteri",
      "Claudio Ferrari",
      "Giuseppe Lisanti",
      "Stefano Berretti",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/3DWidDGET/Leonardo_Galteri_Coarse-to-Fine_3D_Face_Reconstruction_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/3D-WidDGET/Leonardo_Galteri_Coarse-to-Fine_3D_Face_Reconstruction_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Reconstructing accurate 3D shapes of human faces from a single 2D image is a highly challenging Computer Vision problem that is studied since decades. Statistical modeling techniques, such as the 3D Morphable Model (3DMM), have been widely employed because of their capability of reconstructing a plausible model grounding on the prior knowledge of the facial shape. However, most of them derive a and smooth approximation of the real shape, without accounting for the surface details. In this work, we propose an approach based on a Conditional Generative Adversarial Network (CGAN) for refining the reconstruction provided by a 3DMM. The latter is represented as a three-channel image, where the pixel intensities represent, respectively, the depth and the azimuth and elevation angles of the surface normals. The network architecture is an encoder-decoder, which is trained progressively, starting from the lower-resolution layers; this technique allows a more stable training, which led to the generation of high-quality outputs even when high-resolution images are fed during the training. Experimental results show that our method is able to produce detailed realistic reconstructions and obtain lower errors with respect to the 3DMM. Finally, a comparison with a state-of-the-art solution evidences competitive performance and a clear improvement in the quality of the generated models."
  },
  "cvpr2019_3dwiddget_learningtogeneratetextureson3dmeshes": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "3DWidDGET",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - 3DWidDGET",
    "title": "Learning to Generate Textures on 3D Meshes",
    "authors": [
      "Amit Raj",
      "Cusuh Ham",
      "Connelly Barnes",
      "Vladimir Kim",
      "Jingwan Lu",
      "James Hays"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/3DWidDGET/Amit_Raj_Learning_to_Generate_Textures_on_3D_Meshes_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/3D-WidDGET/Amit_Raj_Learning_to_Generate_Textures_on_3D_Meshes_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent years have seen a great deal of work in photorealistic neural image synthesis from 2D image datasets. However, there are only a few works that exploit 3D shape information to aid in image synthesis. To this end, we leverage data from 2D image datasets as well as 3D model corpora to generate textured 3D models. We propose a framework for texture generation for meshes from multiview images. Our framework first uses 2.5D information rendered using the 3D models, along with user inputs to generate an intermediate view dependent representation. These intermediate representations are then used to generate realistic textures for particular views in an unpaired manner. Finally, we use a differentiable renderer to combine the generated multiview texture into a single textured mesh. We demonstrate results of realistic texture synthesis on cars."
  },
  "cvpr2019_mmlv_groundedvideodescription": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MMLV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multi-Modal Learning from Videos",
    "title": "Grounded Video Description",
    "authors": [
      "Luowei Zhou",
      "Yannis Kalantidis",
      "Xinlei Chen",
      "Jason Corso",
      "Marcus Rohrbach"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MMLV/Zhou_Grounded_Video_Description_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MMLV/Zhou_Grounded_Video_Description_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Video description is one of the most challenging problems in vision and language understanding due to the large variability both on the video and language side. Models, hence, typically shortcut the difficulty in recognition and generate plausible sentences that are based on priors but are not necessarily grounded in the video. In this work, we explicitly link the sentence to the evidence in the video by annotating each noun phrase in a sentence with the corresponding bounding box in one of the frames of a video. Our dataset, ActivityNet-Entities, augments the challenging ActivityNet Captions dataset with 158k bounding box annotations, each grounding a noun phrase. This allows training video description models with this data, and importantly, evaluate how grounded or \"true\" such model are to the video they describe. To generate grounded captions, we propose a novel video description model which is able to exploit these bounding box annotations. We demonstrate the effectiveness of our model on our dataset, but also show how it can be applied to image description on the Flickr30k Entities dataset. We achieve state-of-the-art performance on video description, video paragraph description, and image description and demonstrate our generated sentences are better grounded in the video."
  },
  "cvpr2019_mmlv_theemotionallyintelligentrobotimprovingsocially-awarehumanpredictionincrowdedenvironments": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MMLV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multi-Modal Learning from Videos",
    "title": "The Emotionally Intelligent Robot: Improving Socially-aware Human Prediction in Crowded Environments",
    "authors": [
      "Aniket Bera",
      "Tanmay Randhavane",
      "Dinesh  Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MMLV/Bera_The_Emotionally_Intelligent_Robot_Improving_Socially-aware_Human_Prediction_in_Crowded_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MMLV/Bera_The_Emotionally_Intelligent_Robot_Improving_Socially-aware_Human_Prediction_in_Crowded_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present a real-time emotion-aware navigation algorithm for social robots. Our approach estimates time-varying emotional behaviors of pedestrians from their faces and trajectories using a combination of Bayesian-inference, CNN-based learning, and the PAD (Pleasure-Arousal-Dominance) model from psychology. These PAD characteristics are used for generating proxemic constraints for each pedestrian. We use a multi-channel model to classify pedestrian characteristics into four emotion categories (happy, sad, angry, neutral) =. In our validation results, we observe an emotion detection accuracy of 85.33%. We formulate emotion-based proxemic constraints to perform socially-aware robot navigation in low- to medium-density environments. We demonstrate the benefits of our algorithm in simulated environments with tens of pedestrians as well as in a real-world setting with Pepper, a social humanoid robot."
  },
  "cvpr2019_mmlv_self-supervisedsegmentationandsourceseparationonvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MMLV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multi-Modal Learning from Videos",
    "title": "Self-Supervised Segmentation and Source Separation on Videos",
    "authors": [
      "Andrew Rouditchenko",
      "Hang Zhao",
      "Chuang Gan",
      "Josh McDermott",
      "Antonio Torralba"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MMLV/Rouditchenko_Self-Supervised_Segmentation_and_Source_Separation_on_Videos_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MMLV/Rouditchenko_Self-Supervised_Segmentation_and_Source_Separation_on_Videos_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Semantic segmentation of images [11, 3] and sound source separation in audio [8, 4, 1] are two important and popular tasks in the computer vision and computational audition communities. Traditional approaches have relied on large, labeled datasets, but recent work has leveraged the natural correspondence between vision and sound to apply supervised learning without explicit labels. In this paper, we develop a neural network model for visual object segmentation and sound source separation that learns from natural videos through self-supervision. The model is an extension of recently proposed work that maps image pixels to sounds [9]. This paper is a workshop edit of Rouditchenko et al. 2019 [5]."
  },
  "cvpr2019_mmlv_adversarialinferenceformulti-sentencevideodescription": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "MMLV",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Multi-Modal Learning from Videos",
    "title": "Adversarial Inference for Multi-Sentence Video Description",
    "authors": [
      "Jae Sung Park",
      "Marcus Rohrbach",
      "Trevor Darrell",
      "Anna Rohrbach"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/MMLV/Park_Adversarial_Inference_for_Multi-Sentence_Video_Description_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/MMLV/Park_Adversarial_Inference_for_Multi-Sentence_Video_Description_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " While significant progress has been made in the image captioning task, video description is still in its infancy due to the complex nature of video data. Among the main issues are the fluency and coherence of the generated descriptions, and their relevance to the video. Recently, reinforcement and adversarial learning based methods have been explored to improve the image captioning models; however, both types of methods suffer from a number of issues, e.g. poor readability and high redundancy for RL and stability issues for GANs. In this work, we instead propose to apply adversarial techniques during inference, designing a discriminator which encourages better multi-sentence video description. In addition, we find that a multi-discriminator \"hybrid\" design, where each discriminator targets one aspect of a description, leads to the best results. Specifically, we decouple the discriminator to evaluate on three criteria: 1) visual relevance to the video, 2) language diversity and fluency, and 3) coherence across sentences. Our approach results in more accurate, diverse, and coherent multi-sentence video descriptions, as shown by automatic as well as human evaluation on the popular ActivityNet Captions dataset."
  },
  "cvpr2019_clic_2019_abettercolorspaceconversionbasedonlearnedvariancesforimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "A Better Color Space Conversion Based on Learned Variances For Image Compression",
    "authors": [
      "Ming Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Li_A_Better_Color_Space_Conversion_Based_on_Learned_Variances_For_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Li_A_Better_Color_Space_Conversion_Based_on_Learned_Variances_For_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Modern image coders, especially the lossy ones, encode the YCbCr channels separately.Processing the Y channel is always much more sophisticated than the Cb/Cr. The raw image retrieved from the camera sensor is of Bayer-RGB[??] or 3-color RGB[??] format, and the conversion between RGB and YCbCr format normally follows the ITU-R BT.601[??] standard, which essentially defines a fixed 3x3 space conversion matrix with offsets. The algorithm presented in this paper, however, learns a better color space conversion algorithm tailored for each image, squeezing more information into the Y channel before encoding. In order to achieve this goal, the principle component analysis (PCA)[??] algorithm has been trained, to find the image's primary axes giving the highest variance. The PCA algorithm is carried out onto the AC values of each 16x16 pixel block (RGB values minus the block DC). During decoding, the least square method (LSM) is proposed, to estimate the optimal inverse conversion and to compensate for the coding noise. Overhead of the proposed algorithm is negligible 12 coefficients per image only, around 0.00019 bit per pixel for an image of size 2M bytes. The image after PCA conversion is coded by the latest H.266 codec running in INTRA mode, with a binary arithmetic coding engine as the entropy coder. Experiments on the CLIC2019's valid dataset has shown a significant RGB-PSNR performance boost: 0.26db or 7.4% bitrate save@0.145bpp, and 1.2db/22.5%@1.0bpp. The choice on Cb/Cr axis and the channel range are also studied. The proposed algorithm also outperforms the YCoCg[??] conversion algorithm, and is more robust than the YCoCg/BT.601 algorithm."
  },
  "cvpr2019_clic_2019_contentadaptiveoptimizationforneuralimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Content Adaptive Optimization for Neural Image Compression",
    "authors": [
      "Joaquim Campos",
      "Simon Meierhans",
      "Abdelaziz Djelouah",
      "Christopher Schroers"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Campos_Content_Adaptive_Optimization_for_Neural_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Campos_Content_Adaptive_Optimization_for_Neural_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The field of neural image compression has witnessed exciting progress as recently proposed architectures already surpass the established transform coding based approaches. While, so far, research has mainly focused on architecture and model improvements, in this work we explore content adaptive optimization. To this end, we introduce an iterative procedure which adapts the latent representation to the specific content we wish to compress while keeping the parameters of the network and the predictive model fixed. Our experiments show that this allows for an overall increase in rate-distortion performance, independently of the specific architecture used. Furthermore, we also evaluate this strategy in the context of adapting a pre-trained network to other content that is different in visual appearance or resolution. Here, our experiments show that our adaptation strategy can largely close the gap as compared to models specifically trained for the given content while having the benefit that no additional data in the form of model parameter updates has to be transmitted."
  },
  "cvpr2019_clic_2019_discriminativequantizationforfastsimilaritysearch": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Discriminative Quantization for Fast Similarity Search",
    "authors": [
      "Sepehr Eghbali",
      "Ladan Tahvildari"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Eghbali_Discriminative_Quantization_for_Fast_Similarity_Search_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Eghbali_Discriminative_Quantization_for_Fast_Similarity_Search_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Recent decade has witnessed a growing surge of research on encoding high-dimensional objects with compact discrete codes. In this paper, we present a new supervised quantization technique to learn discriminative and compact codes for large scale retrieval tasks. To achieve fast and accurate search, the proposed algorithm learns a discriminative embedding of the input points and at the same time encodes the embedded points with compact codes to reduce storage cost."
  },
  "cvpr2019_clic_2019_end-to-endlearnedroiimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "End-to-End Learned ROI Image Compression",
    "authors": [
      "Hiroaki Akutsu",
      "Takahiro Naruko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Akutsu_End-to-End_Learned_ROI_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Akutsu_End-to-End_Learned_ROI_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present the effectiveness of image compression based on a convolutional auto encoder (CAE) with region of interest (ROI) for quality control. We use road images used to check damaged parts in the road. Our evaluation reveals that BPG does not provide adequate quality for the road damaged parts at a low bit rate (1.0 bpp or less). We propose a method that adapts image quality for prioritized parts and non-prioritized parts for CAE-based compression. The proposed method uses annotation information for the distortion weights of the MS-SSIM-based loss function. Experimental results show that the proposed method implemented for CAE-based compression from F. Mentzer et al. learns the characteristics of the road damaged parts by end-to-end training with the weighted loss function and reduces bpp by 31% compared to the original method while meeting quality requirements that an average weighted MS-SSIM for the road damaged parts be larger than 0.97 and an average weighted MS-SSIM for the other parts be larger than 0.95."
  },
  "cvpr2019_clic_2019_learningpatternsoflatentresidualforimprovingvideocompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Learning Patterns of Latent Residual for Improving Video Compression",
    "authors": [
      "Yen-Chung Chen",
      "Keng-Jui Chang",
      "Yi-Hsuan Tsai",
      "Wei-Chen Chiu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Chen_Learning_Patterns_of_Latent_Residual_for_Improving_Video_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Chen_Learning_Patterns_of_Latent_Residual_for_Improving_Video_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We tackle the problem of reducing compression artifacts. Specifically, we focus on transmitting the residual from the original video, i.e. difference between a compressed video and its corresponding original/uncompressed one, together with the compressed video during video transmission. Our video compression pipeline is capable of diminishing the overall cost of transmitting the residual and simultaneously achieving comparable video quality with respect to a state-of-the-art baseline. We provide experimental results on several datasets, including the one with great diversity, to substantiate the capacity of our pipeline in improving video compression."
  },
  "cvpr2019_clic_2019_deepresiduallearningforimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Deep Residual Learning for Image Compression",
    "authors": [
      "Zhengxue Cheng",
      "Heming Sun",
      "Masaru Takeuchi",
      "Jiro Katto"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Cheng_Deep_Residual_Learning_for_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Cheng_Deep_Residual_Learning_for_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we provide a detailed description on our approach designed for CVPR 2019 Workshop and Challenge on Learned Image Compression (CLIC). Our approach mainly consists of two proposals, i.e. deep residual learning for image compression and sub-pixel convolution as up-sampling operations. Experimental results have indicated that our approaches, Kattolab, Kattolabv2 and KattolabSSIM, achieve 0.972 in MS-SSIM at the rate constraint of 0.15bpp with moderate complexity during the validation phase."
  },
  "cvpr2019_clic_2019_decodersidecolorimagequalityenhancementusingawavelettransformbased3-stageconvolutionalneuralnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Decoder Side Color Image Quality Enhancement using a Wavelet Transform based 3-stage Convolutional Neural Network",
    "authors": [
      "Kai Cui",
      "Eckehard Steinbach"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Cui_Decoder_Side_Color_Image_Quality_Enhancement_using_a_Wavelet_Transform_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Cui_Decoder_Side_Color_Image_Quality_Enhancement_using_a_Wavelet_Transform_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we describe our submission to the workshop and challenge on learned image compression (CLIC) hosted at CVPR 2019. Lossy compressed images usually suffer from unpleasant artifacts, especially when the bit-rate is low. In order to improve the image quality without spending extra bit-rate, decoder side quality enhancement becomes necessary. Most approaches focus on spatial information exploration and the quality enhancement is usually only performed on the luminance component, which leads to the neglect of inter-channel correlation. In addition, since compressed images mainly lose the high-frequency components, high-frequency and low-frequency components show different characteristics. Motivated by the characteristics of compressed images, a wavelet transform based 3-stage CNN is proposed in this paper. With the RGB image as input, the proposed network exploits the latent inter-channel correlations and enhances the low-frequency and high-frequency sub-band separately. Both objective and subjective evaluations show the noticeable quality improvements compared to Better Portable Graphics (BPG) and previous approaches."
  },
  "cvpr2019_clic_2019_vimicroabcnetanimagecodercombiningabettercolorspaceconversionalgorithmandapostenhancingnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "VimicroABCnet: An Image Coder Combining A Better Color Space Conversion Algorithm and A Post Enhancing Network",
    "authors": [
      "Ming Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Li_VimicroABCnet_An_Image_Coder_Combining_A_Better_Color_Space_Conversion_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Li_VimicroABCnet_An_Image_Coder_Combining_A_Better_Color_Space_Conversion_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The framework of combining a better color space conversion (ABC) algorithm,and a post enhancing network for image coding, called VimicroABCnet[??] , is described in this paper. The ABC algorithm employs the principle component analysis[??] method, to find a new primary base axis offering the highest variance for each individual image. The RGB values of each pixel are pre-processed by a 64x64 template filtering. The pixels are then converted by the proposed ABC algorithm, before being encoded by an open source coder[??]. During decoding, the least square method (LSM) has been introduced to estimate the optimal inverse conversion, instead of using a matrix inversion directly. Another feature of the VimicroABCnet is the enhancing network, which adopts the architecture of a classic ResNet[??], and post-processes the decoded RGB image after ABC. Experiments on the CLIC2019 valid dataset have shown significant RGB-PSNR boost of 0.26db or 7.4% bits save@0.145bpp, and 1.2db/22.5%@1.0bpp, making use of the ABC algorithm; and a RGB-PSNR boost of 0.30db@0.15bpp, making use of the enhancing network, respectively. Combining both techniques, an improvement of 0.56db or 12% bits save@0.15bpp; and a decrease in the compressed file size of about 17.8% are achieved in the transparent track. It is noted that each of the two techniques contributes equally. Methods to speed up the decoder model are also discussed."
  },
  "cvpr2019_clic_2019_animagecoderwithcnnoptimizations": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "An Image Coder With CNN Optimizations",
    "authors": [
      "hu jianhua"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Jianhua_An_Image_Coder_With_CNN_Optimizations_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Jianhua_An_Image_Coder_With_CNN_Optimizations_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Convolutional neural networks (CNNs) has achieved great success in image processing and computer vision, especially in high level vision applications, such as classification and image compression. In this paper, CNN based optimizations have been proposed to improve the performance of an open source image coder, and the coding gain mainly comes from three modules: firstly, a classification CNN is employed to generate a region of interest (ROI) map, highlighting the part of the image containing more visual information that might be more sensitive to coding loss than other part, and thus guiding the bit allocation; secondly, a remedy CNN is introduced on the reconstructioned YUV image, to learn and compensate for the coding loss; thirdly, adaptive loop filter(ALF alorithm is applied to carry out color space conversion, and to minimize the color information loss during conversion. The improvement of the proposed optimizations, both objectively and subjectively, has been demonstrated on the CLIC validation data set."
  },
  "cvpr2019_clic_2019_efficientlearningbasedsub-pixelimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Efficient Learning Based Sub-pixel Image Compression",
    "authors": [
      "Chunlei Cai",
      "Guo Lu",
      "Qiang Hu",
      "Li Chen",
      "Zhiyong Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Cai_Efficient_Learning_Based_Sub-pixel_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Cai_Efficient_Learning_Based_Sub-pixel_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose an efficient learning based sub-pixel image compression algorithm. Our framework builds upon the previous variational auto-encoder architecture and reduces the computational complexity significantly. Specifically, we propose an end-to-end optimized image compression framework to utilize the powerful non-linear representation ability of neural networks. This framework follows the widely used variational auto-encoder architecture and is optimized based on the rate-distortion balance. More importantly, a sub-pixel image compression framework is exploited to reduce the spatial resolution of image and improve the inference speed. Experimental results demonstrate the effectiveness of our method. Compared with the baseline algorithm, our encoder is 2 times faster with negligible performance decrease. The decoding speed of our method for the CLIC dataset is 1.85 fps on GTX 1080Ti, which makes our codec one of the fastest learning based image compression algorithm."
  },
  "cvpr2019_clic_2019_rdo-basedsecondarypredictionschemeforimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "RDO-based Secondary Prediction Scheme for Image Compression",
    "authors": [
      "Hongkui Wang",
      "Junhui Liang",
      "Yamei Chen",
      "Hailang Yang",
      "Shengwei Wang",
      "Li Yu",
      "Ning Wang",
      "Zhengang Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Wang_RDO-based_Secondary_Prediction_Scheme_for_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Wang_RDO-based_Secondary_Prediction_Scheme_for_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Image compression plays an important role in information transmission. To further improve its efya...iency, this paper proposes an rate-distortion optimization (RDO) based secondary prediction scheme. In this scheme, the block is predicted twice and the optimal prediction is chosen out with a content-based RDO. Speciya...ally, the content property is introduced into prediction and a content-based RDO is proposed at ya> st. Then, a secondary prediction scheme is developed and the block is predicted once again with a distance-based bi-directional prediction method. In order to verify the effectiveness of our scheme, it is incorporated into the latest test model of versatile video coding standard for image compression. Compared with existing image codecs, experimental results show that our codec has the lowest decoding complexity and achieves best quality at the similar compression ratio."
  },
  "cvpr2019_clic_2019_attentionbasedimagecompressionpost-processingconvlutionalneuralnetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Attention Based Image Compression Post-Processing Convlutional Neural Network",
    "authors": [
      "Yuyang Xue"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Xue_Attention_Based_Image_Compression_Post-Processing_Convlutional_Neural_Network_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Xue_Attention_Based_Image_Compression_Post-Processing_Convlutional_Neural_Network_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The traditional image compressors, e.g., BPG and H.266, have achieved great image and video compression quality. Recently, Convolutional Neural Network has been used widely in image compression. We proposed an attention-based convolutional neural network for low bit-rate compression to post-process the output of traditional image compression decoder. Across the experimental results on validation sets, the post-processing module trained by MAE and MS-SSIM losses yields the highest PSNR of 32.10 on average at the bit-rate of 0.15."
  },
  "cvpr2019_clic_2019_acompressionobjectiveandacyclelossforneuralimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "A Compression Objective and a Cycle Loss for Neural Image Compression",
    "authors": [
      "Caglar Aytekin",
      "Francesco Cricri",
      "Antti Hallapuro",
      "Jani Lainema",
      "Emre Aksu",
      "Miska Hannuksela"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Aytekin_A_Compression_Objective_and_a_Cycle_Loss_for_Neural_Image_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Aytekin_A_Compression_Objective_and_a_Cycle_Loss_for_Neural_Image_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this manuscript we propose two loss terms for neural image compression: a compression objective and a cycle loss. These terms are applied on the encoder output of an autoencoder and are used in combination with reconstruction losses. The compression objective encourages sparsity and low entropy in the activations. The cycle loss term represents the distortion between encoder outputs computed from the original image and from the reconstructed image (code-domain distortion). We train different autoencoders by using the compression objective in combination with different losses: a) MSE, b) MSE and MS-SSIM, c) MSE, MS-SSIM and cycle loss. We observe that images encoded by these differently-trained autoencoders fall into different points of the perception-distortion curve (while having similar bit-rates). In particular, MSE-only training favors low image-domain distortion, whereas cycle loss training favors high perceptual quality."
  },
  "cvpr2019_clic_2019_jointlearnedandtraditionalimagecompressionfortransparentcoding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Joint learned and traditional image compression for transparent coding",
    "authors": [
      "timmy wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Wang_Joint_learned_and_traditional_image_compression_for_transparent_coding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Wang_Joint_learned_and_traditional_image_compression_for_transparent_coding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper proposes a novel image compression framework, which consists of a CNN-based method and a versatile video coding (VVC) based method. The CNN-based method uses the auto-encoder to learn the quantized latent representation of the image and joints the autoregressive and hierarchical priors to exploit the probabilistic structure. We also design a post-processing network for VVC to further improve the quality of compressed images. We find that CNN-based method and VVC-based method are complementary to each other in terms of MS-SSIM and PSNR. Thus, we combine the two methods together to obtained better coding performance. Furthermore, to select the best compression parameter, an optimal coding mode selection algorithm is introduced. Experimental results indicate that the proposed image compression scheme can achieve significantly better rate-distortion (RD) performance than other methods."
  },
  "cvpr2019_clic_2019_learning-basedimagecompressionusingconvolutionalautoencoderandwaveletdecomposition": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Learning-Based Image Compression using Convolutional Autoencoder and Wavelet Decomposition",
    "authors": [
      "Pinar Akyazi",
      "Touradj Ebrahimi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Akyazi_Learning-Based_Image_Compression_using_Convolutional_Autoencoder_and_Wavelet_Decomposition_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Akyazi_Learning-Based_Image_Compression_using_Convolutional_Autoencoder_and_Wavelet_Decomposition_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, a learning-based image compression method that employs wavelet decomposition as a preprocessing step is presented. The proposed convolutional autoencoder is trained end-to-end to yield a target bitrate smaller than 0.15 bits per pixel across the full CLIC2019 test set. Objective results show that the proposed model is able to outperform legacy JPEG compression, as well as a similar convolutional autoencoder that excludes the proposed preprocessing. The presented architecture shows that wavelet decomposition is beneficial in adjusting the frequency characteristics of the compressed image and helps increase the performance of learning-based image compression models."
  },
  "cvpr2019_clic_2019_learnedimagecompressionwithresidualcoding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Learned Image Compression with Residual Coding",
    "authors": [
      "Wei-Cheng Lee",
      "David Alexandre",
      "Chih-Peng Chang",
      "Wen-Hsiao Peng",
      "Cheng-Yen Yang",
      "Hsueh-Ming Hang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Lee_Learned_Image_Compression_with_Residual_Coding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Lee_Learned_Image_Compression_with_Residual_Coding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a two-layer image compression system consisting of a base-layer BPG codec and a learning-based residual layer codec. This proposal is submitted to the Challenge on Learned Image Compression (CLIC) in April 2019. Our contribution is to integrate several known components together to produce a result better than the original individual components. Also, unlike the conventional two-layer coding, our encoder and decoder take inputs also from the base-layer decoder. In addition, we create a refinement network to integrate the residual-layer decoded residual image and the base-layer decoded image together to form the final reconstructed image. Our simulation results indicate that the transmitted feature maps are fairly uncorrelated to the original image because the object boundary information can be provided by base-layer image. The experiments show that the proposed system achieves better performance than BPG subjectively at the given 0.15 bit-per-pixel constraint."
  },
  "cvpr2019_clic_2019_practicalstackednon-localattentionmodulesforimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Practical Stacked Non-local Attention Modules for Image Compression",
    "authors": [
      "Haojie Liu",
      "Tong Chen",
      "Qiu Shen",
      "Zhan Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Liu_Practical_Stacked_Non-local_Attention_Modules_for_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Liu_Practical_Stacked_Non-local_Attention_Modules_for_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we proposed a stacked non-local attention based variational autoencoder (VAE) for learned image compression. We use a non-local module to capture global correlations effectively that can't be offered by traditional convolutional neural networks (CNNs). Meanwhile, layer-wise self-attention mechanisms are widely used to activate/preserve important and challenging regions. We jointly take the hyperpriors and autoregressive priors for conditional probability estimation. For practical application, we have implemented a sparse non-local processing via maxpooling to greatly reduce the memory consumption, and masked 3D convolutions to support parallel processing for autoregressive priors based probability prediction. A post-processing network is then concatenated and trained with decoder jointly for quality enhancement. We have evaluated our model using public CLIC2019 validation and test dataset, offering averaged 0.9753 and 0.9733 respectively when evaluated using multi-scale structural similarity (MS-SSIM) with bit rate less than 0.15 bits per pixel (bpp)."
  },
  "cvpr2019_clic_2019_multi-scaleandcontext-adaptiveentropymodelforimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Multi-scale and Context-adaptive Entropy Model for Image Compression",
    "authors": [
      "Jing Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Zhou_Multi-scale_and_Context-adaptive_Entropy_Model_for_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Zhou_Multi-scale_and_Context-adaptive_Entropy_Model_for_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose an end-to-end trainable image compression framework with a multi-scale and context-adaptive entropy model, especially for low bitrate compression. Due to the success of autoregressive priors in probabilistic generative model, the complementary combination of autoregressive and hierarchical priors can estimate the distribution of each latent representation accurately. Based on this combination, we firstly propose a multi-scale masked convolutional network as our autoregressive model. Secondly, for the significant computational penalty of generative model, we focus on decoded representations covered by receptive field, and skip full zero latents in arithmetic codec. At last, according to the low-rate compression's constraint in CLIC-2019, we use a method to maximize MS-SSIM by allocating bitrate for each image."
  },
  "cvpr2019_clic_2019_learnedpriorinformationforimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Learned Prior Information for Image Compression",
    "authors": [
      "Huang Ching Chun",
      "Phat Nguyen",
      "Chen-Tung Lai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Chun_Learned_Prior_Information_for_Image_Compression_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Chun_Learned_Prior_Information_for_Image_Compression_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a method for image compression by integrating a deep neural network (DNN) with the better portable graphics (BPG) codec. As DNN can learn the prior information from image data, it will reduce the transmission information through BPG codec and achieves a good visual quality for the decompressed image. The proposed method includes three parts: the BPG codec, the artifact reduction network and the colorization network. First, image is converted to the CIE Lab color space. Then the BPG codec compresses L component and color hint extracted from the a, b components. To satisfy the file size, the suitable QP values of BPG compression will be found for each image by binary search. Next, the decompressed L will be improved by the artifact reduction network. Finally, the colorization will predict a and b components from the decompressed L and the color hint. We evaluate the proposed method upon the Kodak image sets by the quantitative metrics (PSNR, MS-SSIM). The comparison with BPG is also presented."
  },
  "cvpr2019_clic_2019_lowbit-rateimagecompressionbasedonpost-processingwithgroupedresidualdensenetwork": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Low Bit-rate Image Compression based on Post-processing with Grouped Residual Dense Network",
    "authors": [
      "Seunghyun Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Cho_Low_Bit-rate_Image_Compression_based_on_Post-processing_with_Grouped_Residual_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Cho_Low_Bit-rate_Image_Compression_based_on_Post-processing_with_Grouped_Residual_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, an image compression method implemented for CVPR 2019 Challenge on Learned Image Compression (CLIC) is introduced. It is designed to satisfy both requirements of image compression, \"higher compression ratio\" and \"better quality\", at the same time. To this end, a neural network based image quality enhancement is incorporated into the most recent traditional image/video coding technique. The decoders, ETRIDGU, ETRIDGUlite, and ETRIDGUfast, which implement the proposed image compression method are designed to have different degrees of complexity and compression efficiency. ETRIDGU, which provides the highest compression efficiency, is reported to achieve the 2nd highest PSNR in the lowrate track of CLIC. ETRIDGUlite, which compromises between the compression efficiency and the complexity, is reported to be the fastest one among the decoders with high mean opinion score (MOS) in the same track."
  },
  "cvpr2019_clic_2019_extendedend-to-endoptimizedimagecompressionmethodbasedonacontext-adaptiveentropymodel": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Extended End-to-End optimized Image Compression Method based on a Context-Adaptive Entropy Model",
    "authors": [
      "Jooyoung Lee",
      "Seunghyun Cho",
      "Se Yoon Jeong",
      "Hyoungjin Kwon",
      "Hyunsuk Ko",
      "Hui Yong Kim",
      "Jin Soo Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Lee_Extended_End-to-End_optimized_Image_Compression_Method_based_on_a_Context-Adaptive_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Lee_Extended_End-to-End_optimized_Image_Compression_Method_based_on_a_Context-Adaptive_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we propose an extended compression method using a context-adaptive entropy model. Based on the Lee et al.Ju Huapproach, we extend the network structure so that compression and quality enhancement methods are jointly optimized. In terms of contexts for estimating distributions, we additionally use offset information. By exploiting the extended structure and the additional con-texts, we obtain substantially improved compression performance, in terms of multi-scale structural similarity (MS-SSIM) index, compared to the model without the extensions."
  },
  "cvpr2019_clic_2019_descriptionofchallengeproposalbynctuanautoencoder-basedimagecompressorwithprinciplecomponentanalysisandsoft-bitrateestimation": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Description of Challenge Proposal by NCTU: An Autoencoder-based Image Compressor with Principle Component Analysis and Soft-Bit Rate Estimation",
    "authors": [
      "Chih-Peng Chang",
      "David Alexandre",
      "Wen-Hsiao Peng",
      "Hsueh-Ming Hang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Chang_Description_of_Challenge_Proposal_by_NCTU_An_Autoencoder-based_Image_Compressor_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Chang_Description_of_Challenge_Proposal_by_NCTU_An_Autoencoder-based_Image_Compressor_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " This paper describes the technology proposal by NCTU for learning-based image compression. The selected technologies include an autoencoder that incorporates (1) a principal component analysis (PCA) layer for energy compaction, (2) a uniform, scalar quantizer for lossy compression, (3) a context-adaptive bitplane coder for entropy coding, and (4) a soft-bit-based rate estimator. The PCA layer includes 1 x1 eigen kernels derived from the sample covariance of co-located feature samples across channels. The bitplane coder compresses PCA-transformed feature samples based on their quantized, fixed-point representations, of which the soft bits provide a differentiable approximation for context-adaptive rate estimation. The training of our compression system proceeds in two alternating phases: one for updating the rate estimator and the other for fine tuning the autoencoder regularized by the rate estimator. The proposed method outperforms BPG in terms of both PSNR and MS-SSIM. Several bug fixes have been made since the submission of our decoder. This paper presents the up-to-date results."
  },
  "cvpr2019_clic_2019_learnedimagerestorationforvvcintracoding": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Learned Image Restoration for VVC Intra Coding",
    "authors": [
      "Ming Lu",
      "Tong Chen",
      "Haojie Liu",
      "Zhan Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Lu_Learned_Image_Restoration_for_VVC_Intra_Coding_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Lu_Learned_Image_Restoration_for_VVC_Intra_Coding_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We propose a learned image restoration network as the post-processing module for emerging Versatile Video Coding (VVC) Intra Profile (https://jvet.hhi.fraunhofer.de) based image coding to further improve the reconstructed image quality. The image restoration network is designed using multi-scale spatial priors to effectively alleviate compression artifacts in the decoded images induced by the quantization based lossy compression algorithms. Experimental results demonstrate the performance gains of our proposed post-porcessing network with VVC Intra coding, offering about 6.5% Bjontegaard-Delta Rate (BD-Rate) reduction for YUV 4:4:4 and 12.2% for YUV 4:2:0, against the VVC Intra without our restoration network on the Test Dataset P/M released by the Computer Vision Lab of ETH Zurich, where the distortion is Peak Signal to Noise Ratio (PSNR)."
  },
  "cvpr2019_clic_2019_variationalautoencoderbasedimagecompressionwithpyramidalfeaturesandcontextentropymodel": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Variational Autoencoder based Image Compression with Pyramidal Features and Context Entropy Model",
    "authors": [
      "Sihan Wen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Wen_Variational_Autoencoder_based_Image_Compression_with_Pyramidal_Features_and_Context_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Wen_Variational_Autoencoder_based_Image_Compression_with_Pyramidal_Features_and_Context_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Variational autoencoder with the potential to address an increasing need for flexible lossy image compression, has recently be investigated as a promising direction for advancing the state-of-the-art. Based on this effective framework, we present an end-to-end image compression method with a multi-scale encoder, residual decoder, and separate entropy model. The encoder uses a pyramidal resize module and inception network to leverage the priors at different resolution scales to improve the efficiency of the compressed latents. The decoder utilizes a residual network to synthesize the images with more nonlinearity. The separate entropy model is adopted to better predict the prior probability model of the latent representation. The final experiment results show that our approach yields a state-of-the-art image compression system."
  },
  "cvpr2019_clic_2019_compressingweight-updatesforimageartifactsremovalneuralnetworks": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Compressing Weight-updates for Image Artifacts Removal Neural Networks",
    "authors": [
      "Yat Hong Lam",
      "Alireza Zare",
      "Caglar Aytekin",
      "Francesco Cricri",
      "Jani Lainema",
      "Emre Aksu",
      "Miska Hannuksela"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Lam_Compressing_Weight-updates_for_Image_Artifacts_Removal_Neural_Networks_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Lam_Compressing_Weight-updates_for_Image_Artifacts_Removal_Neural_Networks_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " In this paper, we present a novel approach for fine-tuning a decoder-side neural network in the context of image compression, such that the weight-updates are better compressible. At encoder side, we fine-tune a pre-trained artifact removal network on target data by using a compression objective applied on the weight-update. In particular, the compression objective encourages weight-updates which are sparse and closer to quantized values. This way, the final weight-update can be compressed more efficiently by pruning and quantization, and can be included into the encoded bitstream together with the image bitstream of a traditional codec. We show that this approach achieves reconstruction quality which is on-par or slightly superior to a traditional codec, at comparable bitrates. To our knowledge, this is the first attempt to combine image compression and neural network's weight update compression."
  },
  "cvpr2019_clic_2019_end-to-endoptimizedimagecompressionwithattentionmechanism": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "End-to-end Optimized Image Compression with Attention Mechanism",
    "authors": [
      "Lei Zhou",
      "Zhenhong Sun",
      "Xiangji Wu",
      "Junmin Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Zhou_End-to-end_Optimized_Image_Compression_with_Attention_Mechanism_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Zhou_End-to-end_Optimized_Image_Compression_with_Attention_Mechanism_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " We present an end-to-end trainable image compression framework for low bit-rate and transparent image compression. Our method is based on variational autoencoder, which consists of a nonlinear encoder transformation, a soft quantizer, a nonlinear decoder transformation and a entropy estimation module. The prior probability of the latent representations is modeled by combining a hyperprior autoencoder and a Pixelcnn++ based context module and they are trained jointly with the transformation autoencoder with attention mechanism. In order to improve the compression performance, a non-local convolution based attention mechanism is designed for allocating bits adaptively. Finally, a novel rate allocation algorithm based on linear optimization is used to assign the bits for each image dynamically, considering the bits constraint of the challenge. Across the experimental results on validation and test sets, the optimized framework can generate the highest PSNR and MS-SSIM for low bit-rate compression competition, and cost the lowest bytes for transparent 40db competition."
  },
  "cvpr2019_clic_2019_ahybridapproachbetweenadversarialgenerativenetworksandactor-criticpolicygradientforlowratehigh-resolutionimagecompression": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "CLIC_2019",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "A Hybrid Approach Between Adversarial Generative Networks and Actor-Critic Policy Gradient for Low Rate High-Resolution Image Compression",
    "authors": [
      "Nicolo Savioli"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/CLIC_2019/Savioli_A_Hybrid_Approach_Between_Adversarial_Generative_Networks_and_Actor-Critic_Policy_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/CLIC 2019/Savioli_A_Hybrid_Approach_Between_Adversarial_Generative_Networks_and_Actor-Critic_Policy_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Image compression is an essential approach for decreasing the size in bytes of the image without deteriorating the quality of it. Typically, classic algorithms are used but recently deep-learning has been successfully applied. In this work, is presented a deep super-resolution work-flow for image compression that maps low-resolution JPEG image to the high-resolution. The pipeline consists of two components: first, an encoder-decoder neural network learns how to transform the downsampling JPEG images to high resolution. Second, a combination between Generative Adversarial Networks (GANs) and reinforcement learning Actor-Critic (A3C) loss pushes the encoder-decoder to indirectly maximize High Peak Signal-to-Noise Ratio (PSNR). Although PSNR is a fully differentiable metric, this work opens the doors to new solutions for maximizing non-differential metrics through an end-to-end approach between encoder-decoder networks and reinforcement learning policy gradient methods."
  },
  "cvpr2019_ug2_prize_challenge_all-in-oneunderwaterimageenhancementusingdomain-adversariallearning": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UG2_Prize_Challenge",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UG2+ Prize Challenge",
    "title": "All-in-One Underwater Image Enhancement Using Domain-Adversarial Learning",
    "authors": [
      "Pritish M Uplavikar",
      "Zhenyu Wu",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UG2_Prize_Challenge/Uplavikar_All-in-One_Underwater_Image_Enhancement_Using_Domain-Adversarial_Learning_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UG2+ Prize Challenge/Uplavikar_All-in-One_Underwater_Image_Enhancement_Using_Domain-Adversarial_Learning_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Raw underwater images are degraded due to wavelength dependent light attenuation and scattering, limiting their applicability in vision systems. Another factor that makes enhancing underwater images particularly challenging is the diversity of the water types in which they are captured. For example, images captured in deep oceanic waters have a different distribution from those captured in shallow coastal waters. Such diversity makes it hard to train a single model to enhance underwater images. In this work, we propose a novel model which nicely handles the diversity of water during the enhancement, by adversarially learning the content features of the images by disentangling the unwanted nuisances corresponding to water types (viewed as different domains). We use the learned domain agnostic features to generate enhanced underwater images. We train our model on a dataset consisting images of 10 Jerlov water types [1]. Experimental results show that the proposed model not only outperforms the previous methods in SSIM and PSNR scores for almost all Jerlov water types but also generalizes well on real-world datasets. The performance of a high-level vision task (object detection) also shows improvement using enhanced images with our model."
  },
  "cvpr2019_ug2_prize_challenge_pyramidconvolutionalnetworkforsingleimagederaining": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UG2_Prize_Challenge",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UG2+ Prize Challenge",
    "title": "Pyramid Convolutional Network for Single Image Deraining",
    "authors": [
      "Jing Zhao",
      "Jiyu Xie",
      "Ruiqin Xiong",
      "Siwei Ma",
      "Tiejun Huang",
      "Wen Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UG2_Prize_Challenge/Zhao_Pyramid_Convolutional_Network_for_Single_Image_Deraining_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UG2+ Prize Challenge/Zhao_Pyramid_Convolutional_Network_for_Single_Image_Deraining_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Restoring images corrupted by rain streaks is important for many computer vision applications in outdoor scenes. Benefiting from the fast inference and excellent feature representation capability, deep convolutional neural networks (CNN) have achieved significant performance improvement for image deraining and attracted considerable attention recently. However, for the images with complex background, the performance of these CNN-based methods is still unsatisfactory. Addressing this issue, we develop a new pyramid convolutional neural network, which is composed of multiple subnets, for image deraining, and name it PDRNet. To take full advantage of multi-scale redundancy, the network decomposes the rainy images into multi-scale subbands via a hierarchical wavelet transform and then process them by several sub-networks respectively. In particular, wavelet transform also plays the role of downsampling and enlarges the receptive field without increasing depth or sacrificing efficiency of network. Experimental results show that our PDRNet can not only achieve promising deraining performance quantitatively and qualitatively, but also benefit high-level computer vision tasks."
  },
  "cvpr2019_ug2_prize_challenge_depthimagequalityassessmentforviewsynthesisbasedonweightededgesimilarity": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UG2_Prize_Challenge",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UG2+ Prize Challenge",
    "title": "Depth Image Quality Assessment for View Synthesis Based on Weighted Edge Similarity",
    "authors": [
      "Leida Li",
      "Xi Chen",
      "Yu Zhou",
      "Jinjian Wu",
      "Guangming Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UG2_Prize_Challenge/Li_Depth_Image_Quality_Assessment_for_View_Synthesis_Based_on_Weighted_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UG2+ Prize Challenge/Li_Depth_Image_Quality_Assessment_for_View_Synthesis_Based_on_Weighted_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With the increasing prevalence of multi-view and freeview displays, virtual view synthesis has been extensively researched. In view synthesis, texture and depth images are typically fed into a depth-image-based-rendering (DIBR) algorithm to generate the new viewpoints. In contrast to the enormous amount of research effort on the quality assessment of texture images and rendering process, much less effort has been dedicated to the quality evaluation of depth images. To fill this gap, this paper presents a quality metric of depth images for view synthesis. Depth image represents information relating to the distance of the surfaces of scene objects from a viewpoint, and edge conveys key location information in depth image, which is extremely important in view rendering. Therefore, the proposed metric is developed with emphasis on measuring the edge characteristics of depth images. Firstly, a similarity map is computed between the distorted and reference depth images by combining intensity and gradient information. Then an adaptive weighting map is calculated by integrating depth distance and location characteristics in the depth image. Finally, an edge indication map is computed and utilized to guide the pooling process, producing the overall depth quality score. Extensive experiments and comparisons on the public MCL-3D database demonstrate that the proposed metric outperforms the relevant state-of-the-art quality metrics."
  },
  "cvpr2019_ug2_prize_challenge_density-adaptivesamplingforheterogeneouspointcloudobjectsegmentationinautonomousvehicleapplications": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UG2_Prize_Challenge",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UG2+ Prize Challenge",
    "title": "Density-Adaptive Sampling for Heterogeneous Point Cloud Object Segmentation in Autonomous Vehicle Applications",
    "authors": [
      "Hasan A Arief",
      "Mansur Arief",
      "Manoj Bhat",
      "Ulf Indahl",
      "Havard Tveite",
      "Ding Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UG2_Prize_Challenge/Arief_Density-Adaptive_Sampling_for_Heterogeneous_Point_Cloud_Object_Segmentation_in_Autonomous_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UG2+ Prize Challenge/Arief_Density-Adaptive_Sampling_for_Heterogeneous_Point_Cloud_Object_Segmentation_in_Autonomous_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Robust understanding of the driving scene is among the key steps for accurate object detection and reliable autonomous driving. Accomplishing these tasks with a high level of precision, however, is not trivial. One of the challenges come from dealing with the heterogeneous density distribution and massively imbalanced class representation in the point cloud data, making the crude implementation of deep learning architectures for point cloud data from other domains less effective. In this paper, we propose a densityadaptive sampling method that can deal with the point density problem while preserving point-object representation. The method works by balancing the point density of pregridded point cloud data using oversampling, and then empirically sample points from the balanced grid. Using the KITTI Vision 3D Benchmark dataset for point cloud segmentation and PointCNN as the classifier of choice, our proposal provides superior results compared to the original PointCNN implementation, improving the performance from 82.73% using voxel-based sampling to 92.25% using our proposed density-adaptive sampling in terms of per class accuracy."
  },
  "cvpr2019_ug2_prize_challenge_preselectionbasedsubjectivepreferenceevaluationforthequalityofunderwaterimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "UG2_Prize_Challenge",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - UG2+ Prize Challenge",
    "title": "Preselection Based Subjective Preference Evaluation for the Quality of Underwater Images",
    "authors": [
      "Miao Yang",
      "Yixiang Du",
      "Yue Huang",
      "Hantao Liu",
      "Zhiqiang Wei",
      "Jintong Hu",
      "Ke Hu",
      "Zhibin Sheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/UG2_Prize_Challenge/Yang_Preselection_Based_Subjective_Preference_Evaluation_for_the_Quality_of_Underwater_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/UG2+ Prize Challenge/Yang_Preselection_Based_Subjective_Preference_Evaluation_for_the_Quality_of_Underwater_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Underwater images contain an interactive mixture of distortions due to the physicochemical property of water and the instability of imaging systems, which differ from those in natural images. We cannot obtain the pristine underwater image as the reference applied in the traditional benchmark databases, and the groups of gradual distortions either. In this paper, a novel preselection based preference label evaluation method is proposed to construct a combined subjective test procedure for an extended preference judgment dataset of underwater images. To the best of our knowledge, this is the first subjective evaluation procedure for underwater images, and also a solution for an expanding visual preference benchmark database. We demonstrate the excellent correlation of the proposed subjective evaluation with the traditional image quality assessment. It is also proven that the proposed subjective evaluation procedure could reflect the slight change of image quality and the authentic quality of a picture more accurately better than the traditional methods."
  },
  "cvpr2019_doai_multi-scaleweightedbranchnetworkforremotesensingimageclassification": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DOAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DOAI",
    "title": "Multi-Scale Weighted Branch Network for Remote Sensing Image Classification",
    "authors": [
      "Kunping Yang",
      "Zicheng Liu",
      "Qikai Lu",
      "Gui-Song Xia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DOAI/Yang_Multi-Scale_Weighted_Branch_Network_for_Remote_Sensing_Image_Classification_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DOAI/Yang_Multi-Scale_Weighted_Branch_Network_for_Remote_Sensing_Image_Classification_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Remote sensing image classification aims to assign semantic label for each input pixel. In this paper, we propose a Multi-Scale Weighted Branch Network (MSWBN) for this dense prediction task. Inspired by attention module, which is commonly adopted to enhance the informative features among the dense feature maps in the deep network, we firstly introduce a Hierarchical Weighted branch Module (HWM). The HWM is designed to extract multi-scale information from the backbone simultaneously with a weighted branches architecture, whose branch weights are generated from lower layers of the backbone. Then, a Low level features Branch Module (LBM) is proposed to embed information with high resolution, where the weighted sum of output from the HWM and low level features is calculated as the dense prediction of the proposed Multi-Scale Weighted Branch Network. The proposed method outperforms extisting best models on the large scale remote sensing image classification dataset (GID) in terms of both efficiency and accuracy."
  },
  "cvpr2019_doai_windowdetectioninfacadesforaerialtexturefilesof3dcitygmlmodels": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DOAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DOAI",
    "title": "Window Detection in Facades for Aerial Texture Files of 3D CityGML Models",
    "authors": [
      "Franziska Lippoldt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DOAI/Lippoldt_Window_Detection_in_Facades_for_Aerial_Texture_Files_of_3D_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DOAI/Lippoldt_Window_Detection_in_Facades_for_Aerial_Texture_Files_of_3D_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " The author inspects the optimal way to extract geometric facade features of windows from aerial texture files of CityGML models. The following method can be integrated and used for aerial texture modifications or 3D modeling details of 3D CityGML models. The author uses the Mask R-CNN with different configurations and backbone graphs to be tested on two data sets. As to improve the scores on the data sets, two traditional solutions to adjust the results are used: The author tests to integrate the more traditional approach of dbscan clustering to correct the results. Further the author also uses the texture coordinates available from the 3D CityGML file to correct our predictions. As those 3D model textures origin from aerial photos, but are essentially smaller crops of a bigger image, facing typical challenges associated with low-level vision problems and bad image resolution and quality. This application can detect windows and facades from the Berlin CityGML model, extract the windows and doors and adjust the 3D model to integrate those. In addition, it is possible to replace the original windows and doors and insert black counterparts or standard models. The latter procedure will play a crucial role in privacy, as those elements might reveal private objects or persons next to the windows and can be automatically replaced."
  },
  "cvpr2019_doai_learningobject-wisesemanticrepresentationfordetectioninremotesensingimagery": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DOAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DOAI",
    "title": "Learning Object-Wise Semantic Representation for Detection in Remote Sensing Imagery",
    "authors": [
      "Chengzheng Li",
      "Chunyan Xu",
      "Zhen Cui",
      "Dan Wang",
      "Zequn Jie",
      "Tong Zhang",
      "Jian Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DOAI/Li_Learning_Object-Wise_Semantic_Representation_for_Detection_in_Remote_Sensing_Imagery_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DOAI/Li_Learning_Object-Wise_Semantic_Representation_for_Detection_in_Remote_Sensing_Imagery_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " With the upgrade of remote sensing technology, object detection in remote sensing imagery becomes a critical but also challenging problem in the field of computer vision. To deal with highly complex background and extreme variation of object scales, we propose to learn a novel object-wise semantic representation for boosting the performance of detection task in remote sensing imagery. An enhanced feature pyramid network is first designed to better extract hierarchical discriminative visual features. To suppress background clutter as well as better estimate proposals, next we specifically introduce a semantic segmentation module to guide horizontal proposals detection. Finally, a ROI module which can fuses multiple-level features is proposed to further promote object detection performance for both horizontal and rotate bounding boxes. With the proposed approach, we achieve 79.5% mAP and 76.6% mAP in horizontal bounding boxes (HBB) and oriented bounding boxes (OBB) tasks of DOTA-v1.5 dataset, which takes the first and second place in the DOAI2019 challenge, respectively."
  },
  "cvpr2019_doai_isaidalarge-scaledatasetforinstancesegmentationinaerialimages": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DOAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DOAI",
    "title": "iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images",
    "authors": [
      "Syed Waqas Zamir",
      "Aditya Arora",
      "Akshita Gupta",
      "Salman  Khan",
      "Guolei Sun",
      "Fahad Shahbaz Khan",
      "Fan Zhu",
      "Ling Shao",
      "Gui-Song Xia",
      "Xiang Bai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DOAI/Zamir_iSAID_A_Large-scale_Dataset_for_Instance_Segmentation_in_Aerial_Images_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DOAI/Zamir_iSAID_A_Large-scale_Dataset_for_Instance_Segmentation_in_Aerial_Images_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Existing Earth Vision datasets are either suitable for semantic segmentation or object detection. In this work, we introduce the first benchmark dataset for instance segmentation in aerial imagery that combines instance-level object detection and pixel-level segmentation tasks. In comparison to instance segmentation in natural scenes, aerial images present unique challenges e.g., huge number of instances per image, large object-scale variations and abundant tiny objects. Our large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (IS-AID) comes with 655,451 object instances for 15 categories across 2,806 high-resolution images. Such precise per-pixel annotations for each instance ensure accurate localization that is essential for detailed scene analysis. Compared to existing small-scale aerial image based instance segmentation datasets, IS-AID contains 15x the number of object categories and 5x the number of instances. We benchmark our dataset using two popular instance segmentation approaches for natural images, namelyMask R-CNN and PANet.In our experiments we show that direct application of off-the-shelf Mask R-CNN and PANet on aerial images provide sub-optimal instance segmentation results, thus requiring specialized solutions from the research community. "
  },
  "cvpr2019_doai_arcticnetadeeplearningsolutiontoclassifythearcticarea": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DOAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DOAI",
    "title": "ArcticNet: A Deep Learning Solution to Classify the Arctic Area",
    "authors": [
      "Ziyu Jiang",
      "Kate Von Ness",
      "Julie Loisel",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DOAI/Jiang_ArcticNet_A_Deep_Learning_Solution_to_Classify_the_Arctic_Area_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DOAI/Jiang_ArcticNet_A_Deep_Learning_Solution_to_Classify_the_Arctic_Area_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Arctic environments are rapidly changing under the warming climate. Of particular interest are wetlands, a type of ecosystem that constitutes the most effective terrestrial long-term carbon store. As permafrost thaws, the carbon that was locked in these wetland soils for millennia becomes available for aerobic and anaerobic decomposition, which releases carbon dioxide CO2 and methane CH4, respectively, back to the atmosphere. As CO2 and CH4 are potent greenhouse gases, this transfer of carbon from the land to the atmosphere further contributes to global warming, thereby increasing the rate of permafrost degradation in a positive feedback loop. Therefore, monitoring Arctic wetland health and dynamics is a key scientific task that is also of importance for policy. However, the identification and delineation of these important wetland ecosystems, remain incomplete and often inaccurate.Mapping the extent of Arctic wetlands remains a challenge for the scientific community. Conventional, coarser remote sensing methods are inadequate at distinguishing the diverse and micro-topographically complex non-vascular vegetation that characterize Arctic wetlands, presenting the need for better identification methods. To tackle this challenging problem, we constructed and annotated the first-of-its-kind Arctic Wetland Dataset (AWD). Based on that, we present ArcticNet, a deep neural network that exploits the multi-spectral, high-resolution imagery captured from nanosatellites (Planet Dove CubeSats) with additional Digital Elevation Model (DEM) from the ArcticDEM project, to semantically label a Arctic study area into six types, in which three Arctic wetland functional types are included. We present multi-fold efforts to handle the arising challenges, including class imbalance, and the choice of fusion strategies. Preliminary results endorse the high promise of ArcticNet, achieving 93.12% in labelling a hold-out set of regions in our Arctic study area."
  },
  "cvpr2019_doai_theaerialelephantdatasetanewpublicbenchmarkforaerialobjectdetection.": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DOAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DOAI",
    "title": "The Aerial Elephant Dataset: A New Public Benchmark for Aerial Object Detection.",
    "authors": [
      "Johannes Naude",
      "Deon Joubert"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DOAI/Naude_The_Aerial_Elephant_Dataset_A_New_Public_Benchmark_for_Aerial_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DOAI/Naude_The_Aerial_Elephant_Dataset_A_New_Public_Benchmark_for_Aerial_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Aerial surveying is a key tool for effective wildlife management. However, the high costs associated with large scale surveys means that this tool is often underutilized. We believe that computer vision can be used to dramatically decrease the costs associated with surveying, while at the same time improving the consistency of results. We present the Aerial Elephant Dataset, a challenging dataset to enable research on game detection under real-world conditions. The dataset consists of 2101 images containing a total of 15 511 African bush elephants in their natural habitats, imaged with a consistent methodology over a range of background types, resolutions and times-of-day. A baseline algorithm for elephant detection is trained and tested to demonstrate the feasibility of the proposed task. The algorithm is used in a larger system, where false positive rejection and counting of densely spaced individuals is aided by a human-in-the-loop. We evaluate the performance of this system against traditional methods by performing surveys in tandem with professional human surveying crews and comparing results in terms of detections missed, man-hours spent and cost."
  },
  "cvpr2019_doai_multi-cuevehicledetectionforsemanticvideocompressioningeoregisteredaerialvideos": {
    "conf_id": "CVPR2019",
    "conf_sub_id": "DOAI",
    "is_workshop": true,
    "conf_name": "CVPR2019_workshops - DOAI",
    "title": "Multi-Cue Vehicle Detection for Semantic Video Compression in Georegistered Aerial Videos",
    "authors": [
      "NOOR AL-SHAKARJI",
      "Filiz Bunyak",
      "Hadi Aliakbarpour",
      "Guna Seetharaman",
      "Kannappan Palaniappan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/html/DOAI/Al-Shakarji_Multi-Cue_Vehicle_Detection_for_Semantic_Video_Compression_in_Georegistered_Aerial_CVPRW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2019_workshops/../content_CVPRW_2019/papers/DOAI/Al-Shakarji_Multi-Cue_Vehicle_Detection_for_Semantic_Video_Compression_in_Georegistered_Aerial_CVPRW_2019_paper.pdf",
    "published": "2019-06",
    "summary": " Detection of moving objects especially vehicles in videos acquired from an airborne camera is very useful for video analytics applications including traffic flow, urban planning, surveillance, law enforcement and disaster response. Using fast low power algorithms for onboard moving object detection would also provide region of interest-based semantic information for very high image compression. Despite recent advances in both UAV platforms and imaging sensor technologies, vehicle detection from aerial video remains challenging due to the relatively small object sizes, appearance changes, platform motion and camera jitter, obscurations and the scene and environment complexity. This paper proposes an approach for moving vehicle detection which synergistically fuses both appearance and motionbased detections in a complementary manner using deep learning combined with flux tensor spatio-temporal filtering [28]). We use deep learning as an appearance-based approach to detect basically all vehicles (both moving and stationary) present in the scene. For detecting moving objects a spatiotemporal filtering is used (Flux tensor [28]) which detects any type of motion including real moving objects and also spurious motions (i.e. parallax motions caused by buildings and non-flat scene structures and magnified by the platform motion). Our proposed pipeline is able to detect the moving vehicles and filter out the false positives caused by parked cars, through fusion of both appearance and motion based techniques. Experimental results show the effectiveness of the proposed method."
  }
}