{
  "cvpr2016_w3_uav-basedautonomousimageacquisitionwithmulti-viewstereoqualityassurancebyconfidenceprediction": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "UAV-Based Autonomous Image Acquisition With Multi-View Stereo Quality Assurance by Confidence Prediction",
    "authors": [
      "Christian Mostegel",
      "Markus Rumpler",
      "Friedrich Fraundorfer",
      "Horst Bischof"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Mostegel_UAV-Based_Autonomous_Image_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Mostegel_UAV-Based_Autonomous_Image_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we present an autonomous system for acquiring close-range high-resolution images that maximize the quality of a later-on 3D reconstruction with respect to coverage, ground resolution and 3D uncertainty. In contrast to previous work, our system uses the already acquired images to predict the confidence in the output of a dense multi-view stereo approach without executing it. This confidence encodes the likelihood of a successful reconstruction with respect to the observed scene and potential camera constellations. Our prediction module runs in real-time and can be trained without any externally recorded ground truth. We use the confidence prediction for on-site quality assurance and for planning further views that are tailored for a specific multi-view stereo approach with respect to the given scene. We demonstrate the capabilities of our approach with an autonomous Unmanned Aerial Vehicle (UAV) in a challenging outdoor scenario."
  },
  "cvpr2016_w3_mobiledevicebasedoutdoornavigationwithon-linelearningneuralnetworkacomparisonwithconvolutionalneuralnetwork": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "Mobile Device Based Outdoor Navigation With On-Line Learning Neural Network: A Comparison With Convolutional Neural Network",
    "authors": [
      "Zejia Zhengj",
      "Juyang Weng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Zhengj_Mobile_Device_Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Zhengj_Mobile_Device_Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Outdoor navigation is challenging with its dynamic environments. Traditional autonomous navigation systems construct 3D driving scenes to recognize open and occupied voxels by using laser range scanners, which are not available on mobile devices. Existing image-based navigation methods are costly in computation thus cannot be deployed onto a mobile device. We present an on-line learning neural network for real-time outdoor navigation using only the computational resources available on a standard mobile device. The network is trained to recognize the most relevant object in current navigation setting and make corresponding decisions. The network is compared with state of the art image classifier, the Convolutional Neural Network. Comparisons show that our network requires a minimal number of updates and converges significantly faster to better performance. The network successfully navigated in long-duration testing and blindfolded testing under sunny and cloudy weather conditions."
  },
  "cvpr2016_w3_thehcibenchmarksuitestereoandflowgroundtruthwithuncertaintiesforurbanautonomousdriving": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "The HCI Benchmark Suite: Stereo and Flow Ground Truth With Uncertainties for Urban Autonomous Driving",
    "authors": [
      "Daniel Kondermann",
      "Rahul Nair",
      "Katrin Honauer",
      "Karsten Krispin",
      "Jonas Andrulis",
      "Alexander Brock",
      "Burkhard Gussefeld",
      "Mohsen Rahimimoghaddam",
      "Sabine Hofmann",
      "Claus Brenner",
      "Bernd Jahne"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Kondermann_The_HCI_Benchmark_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Kondermann_The_HCI_Benchmark_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Recent advances in autonomous driving require more and more highly realistic reference data, even for difficult situations such as low light and bad weather. We present a new stereo and optical flow dataset to complement existing benchmarks. It was specifically designed to be representative for urban autonomous driving, including realistic, systematically varied radiometric and geometric challenges which were previously unavailable.The accuracy of the ground truth is evaluated based on Monte Carlo simulations yielding full, per-pixel distributions. Interquartile ranges are used as uncertainty measure to create binary masks for arbitrary accuracy thresholds and show that we achieved uncertainties better than those reported for comparable outdoor benchmarks. Binary masks for all dynamically moving regions are supplied with estimated stereo and flow values. An initial public benchmark dataset of 55 manually selected sequences between 19 and 100 frames long are made available in a dedicated website featuring interactive tools for database search, visualization, comparison and benchmarking."
  },
  "cvpr2016_w3_monocularlong-termtargetfollowingonuavs": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "Monocular Long-Term Target Following on UAVs",
    "authors": [
      "Rui Li",
      "Minjian Pang",
      "Cong Zhao",
      "Guyue Zhou",
      "Lu Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Li_Monocular_Long-Term_Target_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Li_Monocular_Long-Term_Target_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we investigate the challenging long-term visual tracking problem and its implementation on Unmanned Aerial Vehicles (UAVs). By exploiting the inherent correlation between Frequency tracker And Spatial detector, we propose a novel tracking algorithm, denoted as FAST. As can be theoretically and analytically shown, the superior performance of FAST originates from: 1) robustness -- by transforming from frequency tracker to spatial detector, FAST owns comprehensive detector to cover consequential temporal variance/invariance information that inherently retained in tracker; 2) efficiency -- the coarse-to-fine redetection scheme avoids the training of extra classifier and exhaustive search of location and scale. Experiments testified on tracking benchmarks demonstrate the impressive improvement of FAST. In particular, we successfully implement FAST on quadrotor platform to tackle with indoor and outdoor practical scenarios, achieving real-time, automatic, smooth, and long-term target following on UAVs."
  },
  "cvpr2016_w3_deeplanesend-to-endlanepositionestimationusingdeepneuralnetworksa": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "DeepLanes: End-To-End Lane Position Estimation Using Deep Neural Networksa",
    "authors": [
      "Alexandru Gurghian",
      "Tejaswi Koduri",
      "Smita V. Bailur",
      "Kyle J. Carey",
      "Vidya N. Murali"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Gurghian_DeepLanes_End-To-End_Lane_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Gurghian_DeepLanes_End-To-End_Lane_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Camera-based lane detection algorithms are one of the key enablers for many semi-autonomous and fully-autonomous systems, ranging from lane keep assist to level-5 automated vehicles. Positioning a vehicle between lane boundaries is the core navigational aspect of a self-driving car. Even though this should be trivial, given the clarity of lane markings on most standard roadway systems, the process is typically mired with tedious pre-processing and computational effort. We present an approach to estimate lane positions directly using a deep neural network that operates on images from laterally-mounted down-facing cameras. To create a diverse training set, we present a method to generate semi-artificial images. Besides the ability to distinguish whether there is a lane-marker present or not, the network is able to estimate the position of a lane marker with sub-centimeter accuracy at an average of 100 frames/s on an embedded automotive platform, requiring no pre- or post-processing. This system can be used not only to estimate lane position for navigation, but also provide an efficient way to validate the robustness of driver-assist features which depend on lane information."
  },
  "cvpr2016_w3_multiplescalefaster-rcnnapproachtodriverscell-phoneusageandhandsonsteeringwheeldetection": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "Multiple Scale Faster-RCNN Approach to Driver's Cell-Phone Usage and Hands on Steering Wheel Detection",
    "authors": [
      "T. Hoang Ngan Le",
      "Yutong Zheng",
      "Chenchen Zhu",
      "Khoa Luu",
      "Marios Savvides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Le_Multiple_Scale_Faster-RCNN_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Le_Multiple_Scale_Faster-RCNN_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we present an advanced deep learning based approach to automatically determine whether a driver is using a cell-phone as well as detect if his/her hands are on the steering wheel (i.e. counting the number of hands on the wheel). To robustly detect small objects such as hands, we propose Multiple Scale Faster-RCNN (MS-FRCNN) approach that uses a standard Region Proposal Network (RPN) generation and incorporates feature maps from shallower convolution feature maps, i.e. conv3 and conv4, for ROI pooling. In our driver distraction detection framework, we first make use of the proposed MS-FRCNN to detect individual objects, namely, a hand, a cell-phone, and a steering wheel. Then, the geometric information is extracted to determine if a cell-phone is being used or how many hands are on the wheel. The proposed approach is demonstrated and evaluated on the Vision for Intelligent Vehicles and Applications (VIVA) Challenge database and the challenging Strategic Highway Research Program (SHRP-2) face view videos that was acquired to monitor drivers under naturalistic driving conditions. The experimental results have shown that our method archives better performance than Faster R-CNN on both hands on wheel detection and cell-phone usage detection while remaining at similar testing cost. Compare to the state-of-the-art cell-phone usage detection, our approach obtains higher accuracy, is less time consuming and is independent to landmarking. The groundtruth database will be publicly available."
  },
  "cvpr2016_w3_dr(eye)veadatasetforattention-basedtaskswithapplicationstoautonomousandassisteddriving": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "DR(Eye)Ve: A Dataset for Attention-Based Tasks With Applications to Autonomous and Assisted Driving",
    "authors": [
      "Stefano Alletto",
      "Andrea Palazzi",
      "Francesco Solera",
      "Simone Calderara",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Alletto_DREyeVe_A_Dataset_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Alletto_DREyeVe_A_Dataset_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Autonomous and assisted driving are undoubtedly hot topics in computer vision. However, the driving task is extremely complex and a deep understanding of drivers' behavior is still lacking. Several researchers are now investigating the attention mechanism in order to define computational models for detecting salient and interesting objects in the scene. Nevertheless, most of these models only refer to bottom up visual saliency and are focused on still images. Instead, during the driving experience the temporal nature and peculiarity of the task influence the attention mechanisms, leading to the conclusion that real life driving data is mandatory. In this paper we propose a novel and publicly available dataset acquired during actual driving. Our dataset, composed by more than 500,000 frames, contains drivers' gaze fixations and their temporal integration providing task-specific saliency maps. Geo-referenced locations, driving speed and course complete the set of released data. To the best of our knowledge, this is the first publicly available dataset of this kind and can foster new discussions on better understanding, exploiting and reproducing the driver's attention process in the autonomous and assisted cars of future generations. "
  },
  "cvpr2016_w3_alow-costmirror-basedactiveperceptionsystemforeffectivecollisionfreeunderwaterroboticnavigation": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision in Vehicle Technology",
    "title": "A Low-Cost Mirror-Based Active Perception System for Effective Collision Free Underwater Robotic Navigation",
    "authors": [
      "Noel Cortes Perez",
      "Luz Abril Torres Mendez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/html/Perez_A_Low-Cost_Mirror-Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w3/papers/Perez_A_Low-Cost_Mirror-Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This ongoing research work presents a servo actuated mirror-based design that allows a fixed front-view visual system mounted in an underwater robotic system to extend its field of view by controlling its gaze.We are interested in the autonomous underwater exploration of coral reefs. This type of exploration must involve a cautious and collision-free navigation to avoid damaging the marine ecosystem. Generally, vision systems of underwater vehicles are carefully isolated with mechanical seals to prevent the water from entering. However, this fact causes a strictly dependence between the angle ofview of the camera and the pose of the vehicle. Furthermore, the addition of a system to control camera orientation may result in a significantly reduction of useful load capacity and the movement of the vision system could carry undesirable trusting effects, especially at higher speeds. Our design of servo actuated mirror system changes the angle of view of the camera in two degrees of freedom: pan and tilt, and reaches viewing angles from the sides, bottom top and even rear views of the robot, thusenabling a more effective navigation with obstacle avoidance."
  },
  "cvpr2016_w4_whatdoyoudowhenyouknowthatyoudontknow?": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "What Do You Do When You Know That You Don't Know?",
    "authors": [
      "Abhijit Bendale",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Bendale_What_Do_You_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Bendale_What_Do_You_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Real-world biometrics recognition problems often have two unknowns: the person be recognized, as well as a hidden unknown - missing data. If we choose to ignore data that is occasionally missing, we sacrifice accuracy.In this paper, we present a novel technique to address the problem of handling missing data in biometrics systems without having to make implicit assumptions on the distribution of the underlying data. We introduce the concept of \"operational adaptation\" for biometric systems and formalize the problem. We present a solution for handling missing data based on refactoring on Support Vector Machines for large scale face recognition tasks. We also develop a general approach to estimating SVM refactoring risk. We present experiments on large-scale face recognition based on describable visual attributes on LFW dataset. Our approach consistently outperforms state-of-the-art methods designed to handle missing data. "
  },
  "cvpr2016_w4_deepsecureencodingforfacetemplateprotection": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Deep Secure Encoding for Face Template Protection",
    "authors": [
      "Rohit Kumar Pandey",
      "Yingbo Zhou",
      "Bhargava Urala Kota",
      "Venu Govindaraju"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Pandey_Deep_Secure_Encoding_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Pandey_Deep_Secure_Encoding_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we present a framework for secure identification using deep neural networks, and apply it to the task of template protection for face password authentication. We use deep convolutional neural networks (CNNs) to learn a mapping from face images to maximum entropy binary (MEB) codes. The mapping is robust enough to tackle the problem of exact matching, yielding the same code for new samples of a user as the code assigned during training. These codes are then hashed using any hash function that follows the random oracle model (like SHA-512) to generate protected face templates. The algorithm makes no unrealistic assumptions and offers high template security, cancelability, and matching performance comparable to the state-of-the-art. The efficacy of the approach is shown on CMU-PIE, Extended Yale B, and Multi-PIE face databases. We achieve high ( 95%) genuine accept rates (GAR) at zero false accept rate (FAR) while maintaining a high level of template security."
  },
  "cvpr2016_w4_featurevectorcompressionbasedonleasterrorquantization": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Feature Vector Compression Based on Least Error Quantization",
    "authors": [
      "Tomokazu Kawahara",
      "Osamu Yamaguchi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Kawahara_Feature_Vector_Compression_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Kawahara_Feature_Vector_Compression_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a distinctive feature vector compression method based on least error quantization. This method can be applied to several biometrics methods using feature vectors, and allows us to significantly reduce the memory size of feature vectors without degrading the recognition performance. In this paper, we prove that minimizing quantization error between the compressed and original vectors is most effective to control the performance in face recognition. A conventional method uses non-uniform quantizer which minimizes the quantization error in terms of L2-distance. However, face recognition methods often use metrics other than L2-distance. Our method can calculate the quantized vectors in arbitrary metrics such as Lp-distance (0 < p <= infinity) and the quantized subspace basis. Furthermore, we also propose a fast algorithm calculating Lp-distances between two quantized vectors without decoding them. We evaluate the performance of our method on FERET, LFW and large face datasets with LBP (Lp-distance), Mutual Subspace Method and deep feature. The results show that the recognition rate using the quantized feature vectors is as accurate as that of the method using the original vectors even though the memory size of the vectors is reduced to 1/5 - 1/10. In particular, applying our method to the state-of-the-art feature, we are able to obtain the high performance feature whose size is very small. "
  },
  "cvpr2016_w4_weaklysupervisedfacialanalysiswithdensehyper-columnfeatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Weakly Supervised Facial Analysis With Dense Hyper-Column Features",
    "authors": [
      "Chenchen Zhu",
      "Yutong Zheng",
      "Khoa Luu",
      "T. Hoang Ngan Le",
      "Chandrasekhar Bhagavatula",
      "Marios Savvides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Zhu_Weakly_Supervised_Facial_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Zhu_Weakly_Supervised_Facial_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Weakly supervised methods have recently become one of the most popular machine learning methods since they are able to be used on large-scale datasets without the critical requirement of richly annotated data. In this paper, we present a novel, self-taught, discriminative facial feature analysis approach in the weakly supervised framework. Our method can find regions which are discriminative across classes yet consistent within a class and can solve many face related problems. The proposed method first trains a deep face model with high discriminative capability to extract facial features. The hypercolumn features are then used to give pixel level representation for better classification performance along with discriminative region detection. In addition, calibration approaches are proposed to enable the system to deal with multi-class and mixed-class problems. The system is also able to detect multiple discriminative regions from one image. Our uniform method is able to achieve competitive results in various face analysis applications, such as occlusion detection, face recognition, gender classification, twins verification and facial attractiveness analysis."
  },
  "cvpr2016_w4_acomprehensiveanalysisofdeeplearningbasedrepresentationforfacerecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition",
    "authors": [
      "Mostafa Mehdipour Ghazi",
      "Hazim Kemal Ekenel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Ghazi_A_Comprehensive_Analysis_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Ghazi_A_Comprehensive_Analysis_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10% of the interocular distance."
  },
  "cvpr2016_w4_two-streamcnnsforgesture-basedverificationandidentificationlearninguserstyle": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Two-Stream CNNs for Gesture-Based Verification and Identification: Learning User Style",
    "authors": [
      "Jonathan Wu",
      "Prakash Ishwar",
      "Janusz Konrad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Wu_Two-Stream_CNNs_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Wu_Two-Stream_CNNs_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Recently, gestures have been proposed as an alternative biometric modality to traditional biometrics such as face, fingerprint, iris and gait. As a biometric, gesture is a short body motion that contains static anatomical information and changing behavioral (dynamic) information. We consider two types of gestures: full-body gestures, such as a wave of the arms, and hand gestures, such as a subtle curl of the fingers and palm. Most prior work in this area evaluates gestures in the context of a \"password,\" where each user has a single, chosen gesture motion. Contrary to prior work, we instead aim to learn a user's gesture \"style\" from a set of training gestures. We use two-stream convolutional neural networks, a form of deep learning, to learn this gesture style. First, we evaluate the generalization performance during testing of our approach against gestures or users that have not been seen during training. Then, we study the importance of dynamics by suppressing dynamic information in training and testing. We find that we are able to outperform state-of-the-art methods in identification and verification for two biometrics-oriented gesture datasets for body and in-air hand gestures."
  },
  "cvpr2016_w4_deeptattoorecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Deep Tattoo Recognition",
    "authors": [
      "Xing Di",
      "Vishal M. Patel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Di_Deep_Tattoo_Recognition_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Di_Deep_Tattoo_Recognition_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Tattoo is the soft biometric that indicates discriminative characteristics of a person such as beliefs and personalities.Automatic detection and recognition of tattoo images is a difficult problem.We present deep convolutional neural network-based methods for automatic matching of tattoo images based on the recently introduced AlexNet and Siamese networks.Furthermore, we show that rather than using a simple contrastive loss function, triplet loss function can significantly improve the performance of a tattoo matching system.Extensive experiments on a recently introduced Tatt-C dataset show that our method is able to capture the meaningful structure of tattoos and performs significantly better than many competitive tattoo recognition algorithms."
  },
  "cvpr2016_w4_poolingfacestemplatebasedfacerecognitionwithpooledfaceimages": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Pooling Faces: Template Based Face Recognition With Pooled Face Images",
    "authors": [
      "Tal Hassner",
      "Iacopo Masi",
      "Jungyeon Kim",
      "Jongmoo Choi",
      "Shai Harel",
      "Prem Natarajan",
      "Gerard Medioni"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Hassner_Pooling_Faces_Template_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Hassner_Pooling_Faces_Template_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a novel approach to template based face recognition. Our dual goal is to both increase recognition accuracy and reduce the computational and storage costs of template matching. To do this, we leverage on an approach which was proven effective in many other domains, but, to our knowledge, never fully explored for face images: average pooling of face photos. We show how (and why!) the space of a template's images can be partitioned and then pooled based on image quality and head pose and the effect this has on accuracy and template size. We perform extensive tests on the IJB-A and Janus CS2 template based face identification and verification benchmarks. These show that not only does our approach outperform published state of the art despite requiring far fewer cross template comparisons, but also, surprisingly, that image pooling performs on par with deep feature pooling."
  },
  "cvpr2016_w4_deepgenderocclusionandlowresolutionrobustfacialgenderclassificationviaprogressivelytrainedconvolutionalneuralnetworkswithattention": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "DeepGender: Occlusion and Low Resolution Robust Facial Gender Classification via Progressively Trained Convolutional Neural Networks With Attention",
    "authors": [
      "Felix Juefei-Xu",
      "Eshan Verma",
      "Parag Goel",
      "Anisha Cherodian",
      "Marios Savvides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Juefei-Xu_DeepGender_Occlusion_and_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Juefei-Xu_DeepGender_Occlusion_and_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this work, we have undertaken the task of occlusion and low-resolution robust facial gender classification. Inspired by the trainable attention model via deep architecture, and the fact that the periocular region is proven to be the most salient region for gender classification purposes, we are able to design a progressive convolutional neural network training paradigm to enforce the attention shift during the learning process. The hope is to enable the network to attend to particular high-profile regions (e.g. the periocular region) without the need to change the network architecture itself. The network benefits from this attention shift and becomes more robust towards occlusions and low-resolution degradations. With the progressively trained CNN models, we have achieved better gender classification results on the large-scale PCSO mugshot database with 400K images under occlusion and low-resolution settings, compared to the one undergone traditional training. In addition, our progressively trained network is sufficiently generalized so that it can be robust to occlusions of arbitrary types and at arbitrary locations, as well as low resolution."
  },
  "cvpr2016_w4_real-timefaceidentificationviacnnandboostedhashingforest": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Real-Time Face Identification via CNN and Boosted Hashing Forest",
    "authors": [
      "Yuri Vizilter",
      "Vladimir Gorbatsevich",
      "Andrey Vorotnikov",
      "Nikita Kostromov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Vizilter_Real-Time_Face_Identification_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Vizilter_Real-Time_Face_Identification_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The family of real-time face representations is obtained via Convolutional Network with Hashing Forest (CNHF). We learn the CNN, then transform CNN to the multiple convolution architecture and finally learn the output hashing transform via new Boosted Hashing Forest (BHF) technique. This BHF generalizes the Boosted SSC approach for hashing learning with joint optimization of face verification and identification. CNHF is trained on CASIA-WebFace dataset and evaluated on LFW dataset. We code the output of single CNN with 97% on LFW. For Hamming embedding we get CBHF-200 bit (25 byte) code with 96.3% and 2000-bit code with 98.14% on LFW. CNHF with 2000x7-bit hashing trees achieves 93% rank-1 on LFW relative to basic CNN 89.9% rank-1. CNHF generates templates at the rate of 40+ fps with CPU Core i7 and 120+ fps with GPU GeForce GTX 650."
  },
  "cvpr2016_w4_gaussianconditionalrandomfieldsforfacerecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Gaussian Conditional Random Fields for Face Recognition",
    "authors": [
      "Jonathon M. Smereka",
      "B. V. K. Vijaya Kumar",
      "Andres Rodriguez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Smereka_Gaussian_Conditional_Random_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Smereka_Gaussian_Conditional_Random_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a Gaussian Conditional Random Field (GCRF) approach to modeling the non-stationary distortions that are introduced from changing facial expressions during acquisition. While previous work employed a Gaussian Markov Random Field (GMRF) to perform deformation tolerant matching of periocular images, we show that the approach is not well-suited for facial images, which can contain significantly larger and more complex deformations across the image. Like the GMRF, the GCRF tries to find the maximum scoring assignment between a match pair in the presence of non-stationary deformations. However, unlike the GMRF, the GCRF directly computes the posterior probability that the observed deformation is consistent with the distortions exhibited in other authentic match pairs. The difference is the inclusion of a derived mapping between an input comparison and output deformation score. We evaluate performance on the CMU Multi-PIE facial dataset across all sessions and expressions, finding that the GCRF is significantly more effective at capturing naturally occurring large deformations than the previous GMRF approach."
  },
  "cvpr2016_w4_grouperoptimizingcrowdsourcedfaceannotations": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Grouper: Optimizing Crowdsourced Face Annotations",
    "authors": [
      "Jocelyn C. Adams",
      "Kristen C. Allen",
      "Timothy Miller",
      "Nathan D. Kalka",
      "Anil K. Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Adams_Grouper_Optimizing_Crowdsourced_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Adams_Grouper_Optimizing_Crowdsourced_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This study focuses on the problem of extracting consistent and accurate face bounding box annotations from crowdsourced workers. Aiming to provide benchmark datasets for facial recognition training and testing, we create a `gold standard' set against which consolidated face bounding box annotations can be evaluated. An evaluation methodology based on scores for several features of bounding box annotations is presented and is shown to predict consolidation performance using information gathered from crowdsourced annotations. Based on this foundation, we present \"Grouper,\" a method leveraging density-based clustering to consolidate annotations by crowd workers. We demonstrate that the proposed consolidation scheme, which should be extensible to any number of region annotation consolidations, improves upon metadata released with the IARPA Janus Benchmark-A. Finally, we compare FR performance using the originally provided IJB-A annotations and Grouper and determine that similarity to the gold standard as measured by our evaluation metric does predict recognition performance."
  },
  "cvpr2016_w4_paraphpresentationattackrejectionbyanalyzingpolarizationhypotheses": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "PARAPH: Presentation Attack Rejection by Analyzing Polarization Hypotheses",
    "authors": [
      "Ethan M. Rudd",
      "Manuel Gunther",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Rudd_PARAPH_Presentation_Attack_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Rudd_PARAPH_Presentation_Attack_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " For applications such as airport border control, biometric technologies that can process many capture subjects quickly, efficiently, with weak supervision, and with minimal discomfort are desirable. Facial recognition is particularly appealing because it is minimally invasive yet offers relatively good recognition performance. Unfortunately, the combination of weak supervision and minimal invasiveness makes even highly accurate facial recognition systems susceptible to spoofing via presentation attacks. Thus, there is great demand for an effective and low cost system capable of rejecting such attacks. To this end we introduce PARAPH -- a novel hardware extension that exploits different measurements of light polarization to yield an image space in which presentation media are readily discernible from Bona Fide facial characteristics. The PARAPH system is inexpensive with an added cost of less than 10 US dollars. The system makes two polarization measurements in rapid succession, allowing them to be approximately pixel-aligned, with a frame rate limited by the camera, not the system. There are no moving parts above the molecular level, due to the efficient use of twisted nematic liquid crystals. We present evaluation images using three presentation attack media next to an actual face -- high quality photos on glossy and matte paper and a video of the face on an LCD. In each case, the actual face in the image generated by PARAPH is structurally discernible from the presentations, which appear either as noise (print attacks) or saturated images (replay attacks)."
  },
  "cvpr2016_w4_heterogeneousfacerecognitionusinginter-sessionvariabilitymodelling": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Heterogeneous Face Recognition Using Inter-Session Variability Modelling",
    "authors": [
      "Tiago de Freitas Pereira",
      "Sebastien Marcel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/de_Freitas_Pereira_Heterogeneous_Face_Recognition_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/de_Freitas_Pereira_Heterogeneous_Face_Recognition_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The task of Heterogeneous Face Recognition consists into match face images that were sensed in different modalities, such as sketches to photographs, thermal images to photographs or near infrared to photographs. In this preliminary work we introduce a novel and generic approach based on Inter-session Variability Modelling to handle this task. The experimental evaluation conducted with two different image modalities showed an average rank-1 identification rates of 96.93% and 72.39% for the CUHK-CUFS (Sketches) and CASIA NIR-VIS 2.0 (Near infra-red) respectively. This work is totally reproducible and all the source code for this approach is made publicly available."
  },
  "cvpr2016_w4_apolarimetricthermaldatabaseforfacerecognitionresearch": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "A Polarimetric Thermal Database for Face Recognition Research",
    "authors": [
      "Shuowen Hu",
      "Nathaniel J. Short",
      "Benjamin S. Riggan",
      "Christopher Gordon",
      "Kristan P. Gurton",
      "Matthew Thielke",
      "Prudhvi Gurram",
      "Alex L. Chan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Hu_A_Polarimetric_Thermal_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Hu_A_Polarimetric_Thermal_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present a polarimetric thermal face database, the first of its kind, for face recognition research.This database was acquired using a polarimetric longwave infrared imager, specifically a division-of-time spinning achromatic retarder system.A corresponding set of visible spectrum imagery was also collected, to facilitate cross-spectrum (also referred to as heterogeneous) face recognition research.The database consists of imagery acquired at three distances under two experimental conditions: neutral/baseline condition, and expressions condition.Annotations (spatial coordinates of key fiducial points) are provided for all images.Cross-spectrum face recognition performance on the database is benchmarked using three techniques: partial least squares, deep perceptual mapping, and coupled neural networks."
  },
  "cvpr2016_w4_calipercontinuousauthenticationlayeredwithintegratedpkiencodingrecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "CALIPER: Continuous Authentication Layered With Integrated PKI Encoding Recognition",
    "authors": [
      "Ethan M. Rudd",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Rudd_CALIPER_Continuous_Authentication_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Rudd_CALIPER_Continuous_Authentication_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Architectures relying on continuous authentication require a secure way to challenge the user's identity without trusting that the Continuous Authentication Subsystem (CAS) has not been compromised, i.e., that the response to the layer which manages service/application access is not fake. In this paper, we introduce the CALIPER protocol, in which a separate Continuous Access Verification Entity (CAVE) directly challenges the user's identity in a continuous authentication regime. Instead of simply returning authentication probabilities or confidence scores, CALIPER's CAS uses live hard and soft biometric samples from the user to extract a cryptographic private key embedded in a challenge posed by the CAVE. The CAS then uses this key to sign a response to the CAVE. CALIPER supports multiple modalities, key lengths, and security levels and can be applied in two scenarios:one where the CAS must authenticate its user to a CAVE running on a remote server (device-server) for access to remote application data, and another where the CAS must authenticate its user to a locally running trusted computing module (TCM) for access to local application data (device-TCM). We further demonstrate that CALIPER can leverage device hardware resources to enable privacy and security even when the device's kernel is compromised, and we show how this authentication protocol can even be expanded to obfuscate direct kernel object manipulation (DKOM) malwares."
  },
  "cvpr2016_w4_frequencymapbystructuretensorinlogarithmicscalespaceandforensicfingerprints": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Frequency Map by Structure Tensor in Logarithmic Scale Space and Forensic Fingerprints",
    "authors": [
      "Josef Bigun",
      "Anna Mikaelyan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Bigun_Frequency_Map_by_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Bigun_Frequency_Map_by_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Increasingly, absolute frequency and orientation maps are needed, e.g. for forensics. We introduce a non-linear scale space via the logarithm of trace of the Structure Tensor. Therein, frequency estimation becomes an orientation estimation problem. We show that this offers significant advantages, including construction of efficient isotropic estimations of dense maps of frequency. In fingerprints, both maps are shown to improve each other in an enhancement scheme via Gabor filtering. We suggest a novel continuous ridge counting method, relying only on dense absolute frequency and orientation maps, without ridge detection, thinning, etc. Furthermore, we present new evidence that frequency maps are useful attributes of minutiae. We verify that the suggested method compares favorably with state of the art using forensic fingerprints as test bed, and test images where the ground truth is known. In evaluations, we use public data sets and published methods only."
  },
  "cvpr2016_w4_latentfingerprintimagesegmentationusingfractaldimensionfeaturesandweightedextremelearningmachineensemble": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Latent Fingerprint Image Segmentation Using Fractal Dimension Features and Weighted Extreme Learning Machine Ensemble",
    "authors": [
      "Jude Ezeobiejesi",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Ezeobiejesi_Latent_Fingerprint_Image_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Ezeobiejesi_Latent_Fingerprint_Image_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Latent fingerprints are fingerprints unintentionally left at a crime scene. Due to the poor quality and often complex image background and overlapping patterns characteristic of latent fingerprint images, separating the fingerprint region-of-interest from complex image background and overlapping patterns is a very challenging problem. In this paper, we propose a latent fingerprint segmentation algorithm based on fractal dimension features and weighted extreme learning machine. We build feature vectors from the local fractal dimension features and use them as input to a weighted extreme learning machineensemble classifier. The patches are classified into fingerprint and non-fingerprint classes. We evaluated the proposed segmentation algorithm by comparing the results with the published results from the state of the art latent fingerprint segmentation algorithms. The experimental results of our proposed approach show significant improvement in both the false detection rate (FDR) and overall segmentation accuracy compared to the existing approaches."
  },
  "cvpr2016_w4_gmm-svmfingerprintverificationbasedonminutiaeonly": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "GMM-SVM Fingerprint Verification Based on Minutiae Only",
    "authors": [
      "Berkay Topcu",
      "Yusuf Ziya Isik",
      "Hakan Erdogan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Topcu_GMM-SVM_Fingerprint_Verification_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Topcu_GMM-SVM_Fingerprint_Verification_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Most fingerprint recognition systems use minutiae information, which is an unordered collection of minutiae locations and orientations. Template protection algorithms such as fuzzy commitment and other modern cryptographic alternatives based on homomorphic encryption require a fixed size binary template. However, such a template is not directly applicable to fingerprint minutiae representation which by its nature is of variable size. In this study, we introduce a novel method to represent a minutiae set with a rotation invariant fixed-length vector. We represent each minutia according to its geometric relation with neighbors and use Gaussian mixture model (GMM) to model its feature distribution. A two-class linear SVM is used to create a model template for the enrollment fingerprint sample, which discriminates impressions of the same finger from other fingers. We evaluated the verification performance of our method on the FVC2002DB1 database."
  },
  "cvpr2016_w4_acomparisonofhumanandautomatedfaceverificationaccuracyonunconstrainedimagesets": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "A Comparison of Human and Automated Face Verification Accuracy on Unconstrained Image Sets",
    "authors": [
      "Austin Blanton",
      "Kristen C. Allen",
      "Timothy Miller",
      "Nathan D. Kalka",
      "Anil K. Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Blanton_A_Comparison_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Blanton_A_Comparison_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Automatic face recognition technologies have seen significant improvements in performance due to a combination of advances in deep learning and availability of larger data sets for training deep networks. Since recognizing faces is a task that humans are believed to be very good at, it is only natural to compare the relative performance of automated face recognition and humans when processing fully unconstrained facial imagery. In this work, we expand on previous studies of the recognition accuracy of humans and automated systems by performing several novel analyses utilizing unconstrained face imagery. We examine the impact on performance when human recognizers are presented with varying amounts of imagery per subject, immutable attributes such as gender, and circumstantial attributes such as occlusion, illumination, and pose. Results indicate that humans greatly outperform state of the art automated face recognition algorithms on the challenging IJB-A dataset."
  },
  "cvpr2016_w4_soft-marginlearningformultiplefeature-kernelcombinationswithdomainadaptation,forrecognitioninsurveillancefacedataset": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Soft-Margin Learning for Multiple Feature-Kernel Combinations With Domain Adaptation, for Recognition in Surveillance Face Dataset",
    "authors": [
      "Samik Banerjee",
      "Sukhendu Das"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Banerjee_Soft-Margin_Learning_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Banerjee_Soft-Margin_Learning_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Face recognition (FR) is the most preferred mode for biometric-based surveillance, due to its passive nature of detecting subjects, amongst all different types of biometric traits. FR under surveillance scenario does not give satisfactory performance due to low contrast, noise and poor illumination conditions on probes, as compared to the training samples. A state-of-the-art technology, Deep Learning, even fails to perform well in these scenarios. We propose a novel soft-margin based learning method for multiple feature-kernel combinations, followed by feature transformed using Domain Adaptation, which outperforms many recent state-of-the-art techniques, when tested using three real-world surveillance face datasets."
  },
  "cvpr2016_w4_simultaneoussemi-coupleddictionarylearningformatchingrgbddata": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Simultaneous Semi-Coupled Dictionary Learning for Matching RGBD Data",
    "authors": [
      "Nilotpal Das",
      "Devraj Mandal",
      "Soma Biswas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Das_Simultaneous_Semi-Coupled_Dictionary_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Das_Simultaneous_Semi-Coupled_Dictionary_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Matching with hidden information which are available only during training and not during testing has recently become an important research problem. Matching data from two different modalities, known as cross-modal matching is another challenging problem due to the large variations in the data coming from different modalities. Often, these are treated as two independent problems. But for applications like matching RGBD data, when only one modality is available during testing, it can reduce to either of the two problems. In this work, we propose a framework which can handle both these scenarios seamlessly with applications to matching RGBD data of Lambertian objects. The proposed approach jointly uses the RGB and depth data to learn an illumination invariant canonical version of the objects. Dictionaries are learnt for the RGB, depth and the canonical data, such that the transformed sparse coefficients of the RGB and the depth data is equal to that of the canonical data. Given RGB or depth data, their sparse coefficients corresponding to their canonical version is computed which can be directly used for matching using a Mahalanobis metric. Extensive experiments on three datasets, EURECOM, VAP RGB-D-T and Texas 3D Face Recognition database show the effectiveness of the proposed framework."
  },
  "cvpr2016_w4_offlinesignatureverificationbasedonbag-of-visualwordsmodelusingkazefeaturesandweightingschemes": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Offline Signature Verification Based on Bag-Of-Visual Words Model Using KAZE Features and Weighting Schemes",
    "authors": [
      "Manabu Okawa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Okawa_Offline_Signature_Verification_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Okawa_Offline_Signature_Verification_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The familiar use of handwritten signatures in various applications (e.g., credit card authentication) increases the need for automated verification methods. However, there is still room for improvement in the performance of automated systems under various writing conditions compared to human beings, especially forensic document examiners (FDEs). Furthermore, even with modern techniques, obtaining as much information as possible from the limited samples available remains challenging task. Therefore, further research is required to improve the performance of automated systems. In this study, to improve the performance of offline signature verification, a new approach based on a bag-of-visual words (BoVW) model is adopted. The novelty features of the proposed approach are following: 1) considering the cognitive processing of visual information by FDEs to improve the performance of offline signature verification, 2) using an approach based on the BoVW model to implement the FDEs' cognitive process for feature extraction, 3) incorporating weighting schemes based on term frequency-inverse document frequency to enhance the discriminative power of each visual word, 4) adopting KAZE features in the BoVW model to consider the contour information of strokes more effectively, and 5) detecting the KAZE features in both the strokes and background space to introduce not only the stroke itself but also the various relations between strokes. The promising performance of the proposed approach is shown by using an evaluation method with a popular CEDAR signature dataset."
  },
  "cvpr2016_w4_implementationoffixed-lengthtemplateprotectionbasedonhomomorphicencryptionwithapplicationtosignaturebiometrics": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biometrics",
    "title": "Implementation of Fixed-Length Template Protection Based on Homomorphic Encryption With Application to Signature Biometrics",
    "authors": [
      "Marta Gomez-Barrero",
      "Julian Fierrez",
      "Javier Galbally",
      "Emanuele Maiorana",
      "Patrizio Campisi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/html/Gomez-Barrero_Implementation_of_Fixed-Length_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w4/papers/Gomez-Barrero_Implementation_of_Fixed-Length_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Given the high sensitivity of biometric data, any information leakage poses severe security and privacy risks. This fact raises the need to protect biometric templates so that no information can be learned from them, preserving at the same time the unprotected system's performance and speed. We propose a new efficient biometric template protection scheme based on homomorphic probabilistic encryption for fixed-length templates, where only encrypted data is handled. Under a fully reproducible experimental framework, we analyse different distance measures for the particular case of on-line signature, showing that all requirements described in the ISO/IEC 24745 standard on biometric information protection are met with no performance degradation and at a low computational cost. Furthermore, the proposed approach is robust to hill-climbing and inverse-biometrics attacks."
  },
  "cvpr2016_w9_learningcross-spectralsimilaritymeasureswithdeepconvolutionalneuralnetworks": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Learning Cross-Spectral Similarity Measures With Deep Convolutional Neural Networks",
    "authors": [
      "Cristhian A. Aguilera",
      "Francisco J. Aguilera",
      "Angel D. Sappa",
      "Cristhian Aguilera",
      "Ricardo Toledo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Aguilera_Learning_Cross-Spectral_Similarity_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Aguilera_Learning_Cross-Spectral_Similarity_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The simultaneous use of images from different spectra can be helpful to improve the performance of many computer vision tasks. The core idea behind the usage of cross-spectral approaches is to take advantage of the strengths of each spectral band providing a richer representation of a scene, which cannot be obtained with just images from one spectral band. In this work we tackle the cross-spectral image similarity problem by using Convolutional Neural Networks (CNNs). We explore three different CNN architectures to compare the similarity of cross-spectral image patches. Specifically, we train each network with images from the visible and the near-infrared spectrum, and then test the result with two public cross-spectral datasets. Experimental results show that CNN approaches outperform the current state-of-art on both cross-spectral datasets. Additionally,our experiments show that some CNN architectures are capable of generalizing between different cross-spectral domains."
  },
  "cvpr2016_w9_distinguishingweatherphenomenafrombirdmigrationpatternsinradarimagery": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Distinguishing Weather Phenomena From Bird Migration Patterns in Radar Imagery",
    "authors": [
      "Aruni RoyChowdhury",
      "Daniel Sheldon",
      "Subhransu Maji",
      "Erik Learned-Miller"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/RoyChowdhury_Distinguishing_Weather_Phenomena_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/RoyChowdhury_Distinguishing_Weather_Phenomena_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Data archived by the United States radar network for weather surveillance is useful in studying ecological phenomena such as the migration patterns of birds.However, all such methods require a manual screening stage from domain experts to eliminate radar signatures of weather phenomena, since the radar beam picks up both biological and non-biological targets.Automating this screening step would be of significant help to the large scale study of ecological phenomenon from radar data. We apply several techniques to this novel task, comparing the performance of Convolutional Neural Networks (CNNs) models against a baseline of the Fisher Vector model on SIFT descriptors.We compare the performance of deeper and shallower network architectures, deep texture models versus the regular CNN model and the effect of fine-tuning ImageNet pre-trained networks on radar imagery.Fine-tuning the networks on the radar imagery provides a significant boost, and we achieve a final accuracy of 94.4%."
  },
  "cvpr2016_w9_amodularnmfmatchingalgorithmforradiationspectra": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Modular NMF Matching Algorithm for Radiation Spectra",
    "authors": [
      "Melissa Koudelka",
      "Daniel J. Dorsey"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Koudelka_A_Modular_NMF_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Koudelka_A_Modular_NMF_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In real-world object identification systems, the operational mission may change daily.For example, a target recognition system may search for heavy armor one day, and surface-to-air assets the next, or a radiation detection system may be detecting medical isotopes in one instance, and special nuclear material in another.To accommodate this \"mission of the day\" scenario, the underlying object database must be able to adjust to changing target sets. Traditional dimensionality reduction algorithms rely on a single unifying basis set that is derived from the complete set of objects of interest, making mission-specific adjustment a significant task.We describe a method that instead uses limited-size individual basis sets to represent objects of interest.We demonstrate the modular identification system on the problem of identifying radioisotopes from their gamma ray spectra using nonnegative matrix factorization. "
  },
  "cvpr2016_w9_evaluationoffeaturechannelsforcorrelation-filter-basedvisualobjecttrackingininfraredspectrum": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Evaluation of Feature Channels for Correlation-Filter-Based Visual Object Tracking in Infrared Spectrum",
    "authors": [
      "Erhan Gundogdu",
      "Aykut Koc",
      "Berkan Solmaz",
      "Riad I. Hammoud",
      "A. Aydin Alatan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Gundogdu_Evaluation_of_Feature_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Gundogdu_Evaluation_of_Feature_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Correlation filters for visual object tracking in visible imagery has been well studied. Most of the correlation-filter-based methods use either raw image intensities or feature maps of gradient orientations or color channels. However, well-known features designed for visible spectrum may not be ideal for infrared object tracking, since infrared and visible spectra have dissimilar characteristics in general. We assess the performance of two state-of-the-art correlation-filter- based object tracking methods on Linkoping Thermal InfraRed (LTIR) dataset of medium wave and longwave infrared videos, using deep convolutional neural networks (CNN) features as well as other traditional hand-crafted descriptors. The deep CNN features are trained on an infrared dataset consisting of 16K objects for a supervised classification task. The highest performance in terms of the overlap metric is achieved when these deep CNN features are utilized in a correlation-filter-based tracker."
  },
  "cvpr2016_w9_adaptiveobjectclassificationusingcomplexsarsignatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Adaptive Object Classification Using Complex SAR Signatures",
    "authors": [
      "Firooz Sadjadi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Sadjadi_Adaptive_Object_Classification_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Sadjadi_Adaptive_Object_Classification_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper addresses the problem associated with the classification of signatures of objects obtained by coherent sensors whereby the signatures are complex valued. Individual phase and amplitude component of a signature are combined optimally and the resulting fused signature is used in a sparsity-based learning classifier. The results of application of this approach are then compared with the corresponding results using only the amplitudes of the signatures. To test the concept public-domain radar signatures of several land vehicles obtained at different aspect angles are used. The performance improvement, based on confusion matrices, is shown to be significant when both phase and amplitudes are used."
  },
  "cvpr2016_w9_scaleinvarianthumanactiondetectionfromdepthcamerasusingclasstemplates": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Scale Invariant Human Action Detection From Depth Cameras Using Class Templates",
    "authors": [
      "Kartik Gupta",
      "Arnav Bhavsar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Gupta_Scale_Invariant_Human_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Gupta_Scale_Invariant_Human_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We consider the problem of detecting and localizing a human action from continuous action video from depth cameras. We believe that this problem is more challenging than the problem of traditional action recognition as we do not have the information about the starting and ending frames of an action class. Another challenge which makes the problem difficult, is the latency in detection of actions. In this paper, we introduce a greedy approach to detect the action class, invariant of their temporal scale in the testing sequences using class templates and basic skeleton based feature representation from the depth stream data generated using Microsoft Kinect. We evaluate the proposed method on the standard G3D and UTKinect-Action datasets consisting of five and ten actions, respectively. Our results demonstrate that the proposed approach performs well for action detection and recognition under different temporal scales, and is able to outperform the state of the art methods at low latency."
  },
  "cvpr2016_w9_real-timephysiologicalmeasurementandvisualizationusingasynchronizedmulti-camerasystem": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Real-Time Physiological Measurement and Visualization Using a Synchronized Multi-Camera System",
    "authors": [
      "Otkrist Gupta",
      "Dan McDuff",
      "Ramesh Raskar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Gupta_Real-Time_Physiological_Measurement_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Gupta_Real-Time_Physiological_Measurement_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Remote physiological measurement has widespread implications in healthcare and affective computing. This paper presents an efficient system for remotely measuring heart rate and heart rate variability using multiple low-cost digital cameras in real-time. We combine an RGB camera, monochrome camera with color filter and a thermal camera to recover the blood volume pulse (BVP). We show that using multiple cameras in synchrony yields the most accurate recovery of the BVP signal. The RGB combination is not optimal. We show that the thermal camera improves performance of measurement under dynamic ambient lighting but thermal camera alone is not enough and accuracy can be improved by adding more spectral channels. We present a real-time prototype that allows accurate physiological measurement combined with a novel user interface to visualize changes in heart rate and heart rate variability. Finally, we propose how this system might be used for applications such as patient monitoring."
  },
  "cvpr2016_w9_seeingtheforestfromthetreesaholisticapproachtonear-infraredheterogeneousfacerecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Seeing the Forest From the Trees: A Holistic Approach to Near-Infrared Heterogeneous Face Recognition",
    "authors": [
      "Christopher Reale",
      "Nasser M. Nasrabadi",
      "Heesung Kwon",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Reale_Seeing_the_Forest_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Reale_Seeing_the_Forest_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Heterogeneous face recognition is the problem of identifying a person from a face image acquired with a non-traditional sensor by matching it to a visible gallery. Most approaches to this problem involve modeling the relationship between corresponding images from the visible and sensing domains. This is typically done at the patch level and/or with shallow models with the aim to prevent overfitting. In this work, rather than modeling local patches or using a simple model, we propose to use a complex, deep model to learn the relationship between the entirety of cross-modal face images. We describe a deep convolutional neural network based method that leverages a large visible image face dataset to prevent overfitting. We present experimental results on two benchmark datasets showing its effectiveness."
  },
  "cvpr2016_w9_non-planarinfrared-visibleregistrationforuncalibratedstereopairs": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "Non-Planar Infrared-Visible Registration for Uncalibrated Stereo Pairs",
    "authors": [
      "Dinh-Luan Nguyen",
      "Pierre-Luc St-Charles",
      "Guillaume-Alexandre Bilodeau"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Nguyen_Non-Planar_Infrared-Visible_Registration_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Nguyen_Non-Planar_Infrared-Visible_Registration_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Thermal infrared-visible video registration for non-planar scenes is a new area in visual surveillance. It allows the combination of information from two spectra for better human detection and segmentation. In this paper, we present a novel online framework for visible and thermal infrared registration in non-planar scenes that includes foreground segmentation, feature matching, rectification and disparity calculation. Our proposed approach is based on sparse correspondences of contour points. The key ideas of the proposed framework are the removal of spurious regions at the beginning of videos and a registration methodology for non-planar scenes. Besides, a new non-planar dataset with an associated evaluation protocol is also proposed as a standard assessment. We evaluate our method on both public planar and non-planar datasets. Experimental results reveal that the proposed method can not only successfully handle non-planar scenes but also gets state-of-the-art results on planar ones."
  },
  "cvpr2016_w9_anovelvisualizationtoolforevaluatingtheaccuracyof3dsensingandreconstructionalgorithmsforautomaticdormantpruningapplications": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Novel Visualization Tool for Evaluating the Accuracy of 3D Sensing and Reconstruction Algorithms for Automatic Dormant Pruning Applications",
    "authors": [
      "Fangda Li",
      "Somrita Chattopadhyay",
      "Shayan A. Akbar",
      "Noha M. Elfiky",
      "Avinash Kak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Li_A_Novel_Visualization_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Li_A_Novel_Visualization_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Pruning is one of the most essential procedures in specialty crop production. During winter, skilled workers have to remove certain primary branches of the dormant trees in order to increase the productivity of those trees. The process is particularly challenging as it needs huge workforce and costs, besides, the unavailability of skilled seasonal workers. In this paper, we introduce a novel multifunctional graphical user interface which operates in two different modes, namely, manual and automatic, and serve the following purposes - (1) validating 3D reconstruction algorithms, (2) automating the decision making process for selecting candidate branches to be pruned, (3) hence, providing an easy interface with any robotic pruners, (4) training unskilled seasonal pruners, and (5) disseminating the knowledge. The paper describes the operating modes of the proposed tool and highlights various features and functionalities of the software by conducting experiments on an exemplar apple tree."
  },
  "cvpr2016_w9_anovelbenchmarkrgbddatasetfordormantappletreesanditsapplicationtoautomaticpruning": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Novel Benchmark RGBD Dataset for Dormant Apple Trees and Its Application to Automatic Pruning",
    "authors": [
      "Shayan A. Akbar",
      "Somrita Chattopadhyay",
      "Noha M. Elfiky",
      "Avinash Kak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/html/Akbar_A_Novel_Benchmark_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w9/papers/Akbar_A_Novel_Benchmark_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Dormant pruning is a necessary procedure in the field of specialty crop production. In order to mitigate the need of huge labor, automation of this pruning process has become a topic of utmost importance in the field of horticulture. 3D modeling and reconstruction is a major step in such robotics precision agriculture. In this paper, we introduce a new public dataset which can be used for reconstructing dormant apple trees. Our dataset comprises of 9 different apple trees in both indoor and outdoor environment. The images are collected using a portable Kinect2 sensor. To the best of our knowledge, this is the first publicly available dataset for the application like 3D modeling of dormant trees. We hope that the dataset will provide the entire research community working towards mechanizing dormant pruning a baseline benchmark for evaluating different 3D reconstruction and modeling algorithms."
  },
  "cvpr2016_w12_jointlearningofconvolutionalneuralnetworksandtemporallyconstrainedmetricsfortrackletassociation": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - DeepVision: Deep Learning in Computer Vision",
    "title": "Joint Learning of Convolutional Neural Networks and Temporally Constrained Metrics for Tracklet Association",
    "authors": [
      "Bing Wang",
      "Li Wang",
      "Bing Shuai",
      "Zhen Zuo",
      "Ting Liu",
      "Kap Luk Chan",
      "Gang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/html/Wang_Joint_Learning_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we study the challenging problem of multi-object tracking in a complex scene captured by a single camera. Different from the existing tracklet association-based tracking methods, we propose a novel and efficient way to obtain discriminative appearance-based tracklet affinity models. Our proposed method jointly learns the convolutional neural networks (CNNs) and temporally constrained metrics. In our method, a siamese convolutional neural network (CNN) is first pre-trained on the auxiliary data. Then the siamese CNN and temporally constrained metrics are jointly learned online to construct the appearance-based tracklet affinity models. The proposed method can jointly learn the hierarchical deep features and temporally constrained segment-wise metrics under a unified framework. For reliable association between tracklets, a novel loss function incorporating temporally constrained multi-task learning mechanism is proposed. By employing the proposed method, tracklet association can be accomplished even in challenging situations. Moreover, a large-scale dataset with 40 fully annotated sequences is created to facilitate the tracking evaluation. Experimental results on five public datasets and the new large-scale dataset show that our method outperforms several state-of-the-art approaches in multi-object tracking."
  },
  "cvpr2016_w12_fasterr-cnnfeaturesforinstancesearch": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - DeepVision: Deep Learning in Computer Vision",
    "title": "Faster R-CNN Features for Instance Search",
    "authors": [
      "Amaia Salvador",
      "Xavier Giro-i-Nieto",
      "Ferran Marques",
      "Shin'ichi Satoh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/html/Salvador_Faster_R-CNN_Features_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/papers/Salvador_Faster_R-CNN_Features_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Image representations derived from pre-trained Convolutional Neural Networks (CNNs) have become the new state of the art in computer vision tasks such as instance retrieval. This work explores the suitability for instance retrieval of image- and region-wise representations pooled from an object detection CNN such as Faster R-CNN. We take advantage of the object proposals learned by a Region Proposal Network (RPN) and their associated CNN features to build an instance search pipeline composed of a first filtering stage followed by a spatial reranking. We further investigate the suitability of Faster R-CNN features when the network is fine-tuned for the same objects one wants to retrieve. We assess the performance of our proposed system with the Oxford Buildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013, achieving competitive results."
  },
  "cvpr2016_w12_deepend2endvoxel2voxelprediction": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - DeepVision: Deep Learning in Computer Vision",
    "title": "Deep End2End Voxel2Voxel Prediction",
    "authors": [
      "Du Tran",
      "Lubomir Bourdev",
      "Rob Fergus",
      "Lorenzo Torresani",
      "Manohar Paluri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/html/Tran_Deep_End2End_Voxel2Voxel_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/papers/Tran_Deep_End2End_Voxel2Voxel_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video.Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive pre-processing or post-processing methods.In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains."
  },
  "cvpr2016_w12_adversarialdiversityandhardpositivegeneration": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - DeepVision: Deep Learning in Computer Vision",
    "title": "Adversarial Diversity and Hard Positive Generation",
    "authors": [
      "Andras Rozsa",
      "Ethan M. Rudd",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/html/Rozsa_Adversarial_Diversity_and_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/papers/Rozsa_Adversarial_Diversity_and_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " State-of-the-art deep neural networks suffer from a fundamental problem - they misclassify adversarial examples formed by applying small perturbations to inputs. In this paper, we present a new psychometric perceptual adversarial similarity score (PASS) measure for quantifying adversarial images, introduce the notion of hard positive generation, and use a diverse set of adversarial perturbations - not just the closest ones - for data augmentation. We introduce a novel hot/cold approach for adversarial example generation, which provides multiple possible adversarial perturbations for every single image. The perturbations generated by our novel approach often correspond to semantically meaningful image structures, and allow greater flexibility to scale perturbation-amplitudes, which yields an increased diversity of adversarial images. We present adversarial images on several network topologies and datasets, including LeNet on the MNIST dataset, and GoogLeNet and ResidualNet on the ImageNet dataset. Finally, we demonstrate on LeNet and GoogLeNet that fine-tuning with a diverse set of hard positives improves the robustness of these networks compared to training with prior methods of generating adversarial images."
  },
  "cvpr2016_w12_learningbytrackingsiamesecnnforrobusttargetassociation": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - DeepVision: Deep Learning in Computer Vision",
    "title": "Learning by Tracking: Siamese CNN for Robust Target Association",
    "authors": [
      "Laura Leal-Taixe",
      "Cristian Canton-Ferrer",
      "Konrad Schindler"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/html/Leal-Taixe_Learning_by_Tracking_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/papers/Leal-Taixe_Learning_by_Tracking_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper introduces a novel approach to the task of data association within the context of pedestrian tracking, by introducing a two-stage learning scheme to match pairs of detections. First, a Siamese convolutional neural network (CNN) is trained to learn descriptors encoding local spatio-temporal structures between the two input image patches, aggregating pixel values and optical flow information. Second, a set of contextual features derived from the position and size of the compared input patches are combined with the CNN output by means of a gradient boosting classifier to generate the final matching probability. This learning approach is validated by using a linear programming based multi-person tracker showing that even a simple and efficient tracker may outperform much more complex models when fed with our learned matching probabilities. Results on publicly available sequences show that our method meets state-of-the-art standards in multiple people tracking."
  },
  "cvpr2016_w12_resegarecurrentneuralnetwork-basedmodelforsemanticsegmentation": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - DeepVision: Deep Learning in Computer Vision",
    "title": "ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation",
    "authors": [
      "Francesco Visin",
      "Marco Ciccone",
      "Adriana Romero",
      "Kyle Kastner",
      "Kyunghyun Cho",
      "Yoshua Bengio",
      "Matteo Matteucci",
      "Aaron Courville"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/html/Visin_ReSeg_A_Recurrent_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/papers/Visin_ReSeg_A_Recurrent_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg."
  },
  "cvpr2016_w12_richimagecaptioninginthewild": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - DeepVision: Deep Learning in Computer Vision",
    "title": "Rich Image Captioning in the Wild",
    "authors": [
      "Kenneth Tran",
      "Xiaodong He",
      "Lei Zhang",
      "Jian Sun",
      "Cornelia Carapcea",
      "Chris Thrasher",
      "Chris Buehler",
      "Chris Sienkiewicz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/html/Tran_Rich_Image_Captioning_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w12/papers/Tran_Rich_Image_Captioning_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include generating high quality caption with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out-of-domain datasets. We also make the system publicly accessible as a part of the Microsoft Cognitive Services."
  },
  "cvpr2016_w13_bodypartbasedre-identificationfromanegocentricperspective": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Egocentric Vision",
    "title": "Body Part Based Re-Identification From an Egocentric Perspective",
    "authors": [
      "Federica Fergnani",
      "Stefano Alletto",
      "Giuseppe Serra",
      "Joaquim De Mira",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/html/Fergnani_Body_Part_Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/papers/Fergnani_Body_Part_Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " With the spread of wearable cameras, many applications ranging from social tagging to video summarization would greatly benefit from people re-identification methods capable of dealing with the egocentric perspective. In this regard, first-person camera views present such a unique setting that traditional re-identification methods results in poor performance when applied to this scenario. In this paper, we present a simple but effective solution that overcomes the limitations of traditional approaches by dividing people images into meaningful body parts. Furthermore, by taking into account human gaze information concerning where people look at when trying to recognize a person, we devise a meaningful way to weight the contributions of different bodyparts. Experimental results validate the proposal on a novel egocentric re-identification dataset, the first of its kind, showing that the performance increases when compared to current state of the art on egocentric sequences is significant"
  },
  "cvpr2016_w13_discoveringobjectsofjointattentionviafirst-personsensing": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Egocentric Vision",
    "title": "Discovering Objects of Joint Attention via First-Person Sensing",
    "authors": [
      "Hiroshi Kera",
      "Ryo Yonetani",
      "Keita Higuchi",
      "Yoichi Sato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/html/Kera_Discovering_Objects_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/papers/Kera_Discovering_Objects_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The goal of this work is to discover objects of joint attention, i.e., objects being viewed by multiple people using head-mounted cameras and eye trackers. Such objects of joint attention are expected to act as an important cue for understanding social interactions in everyday scenes. To this end, we develop a commonality-clustering method tailored to first-person videos combined with points-of-gaze sources. The proposed method uses multiscale spatiotemporal tubes around points of gaze as a candidate of objects, making it possible to deal with various sizes of objects observed in the first-person videos. We also introduce a new dataset of multiple pairs of first-person videos and points-of-gaze data. Our experimental results show that our approach can outperform several state-of-the-art commonality-clustering methods."
  },
  "cvpr2016_w13_apointinggesturebasedegocentricinteractionsystemdataset,approachandapplication": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Egocentric Vision",
    "title": "A Pointing Gesture Based Egocentric Interaction System: Dataset, Approach and Application",
    "authors": [
      "Yichao Huang",
      "Xiaorui Liu",
      "Xin Zhang",
      "Lianwen Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/html/Huang_A_Pointing_Gesture_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/papers/Huang_A_Pointing_Gesture_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Natural hand-based human device interaction is essential for wearable camera development. This paper presents a solution for the point gesture based interaction in the egocentric vision and its application. Firstly, a dataset named EgoFinger is established focusing on the pointing gesture for the egocentric vision. We discuss the dataset collection in detail as well as in depth analysis of this dataset. The analysis shows that the dataset covers substantial data samples in various environments and dynamic hand shapes. Furthermore, we propose a two-stage Faster-RCNN based hand detection and dual-target fingertip detection framework. Comparing with state-of-art tracking and detection algorithms, it performs the best. Finally, using the fingertip detection result, we design and implement an input system for the egocentric vision, i.e., Ego-Air-Writing. By considering the fingertip as a pen, the user with wearable glass can write character in the air and interact with system. "
  },
  "cvpr2016_w13_multimodalmulti-streamdeeplearningforegocentricactivityrecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Egocentric Vision",
    "title": "Multimodal Multi-Stream Deep Learning for Egocentric Activity Recognition",
    "authors": [
      "Sibo Song",
      "Vijay Chandrasekhar",
      "Bappaditya Mandal",
      "Liyuan Li",
      "Joo-Hwee Lim",
      "Giduthuri Sateesh Babu",
      "Phyo Phyo San",
      "Ngai-Man Cheung"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/html/Song_Multimodal_Multi-Stream_Deep_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w13/papers/Song_Multimodal_Multi-Stream_Deep_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we propose a multimodal multi-stream deep learning framework to tackle the egocentric activity recognition problem, using both the video and sensor data. First, we experiment and extend a multi-stream Convolutional Neural Network to learn the spatial and temporal features from egocentric videos. Second,we propose amulti-stream Long Short-Term Memory architecture to learn the features from multiple sensor streams (accelerometer, gyroscope, etc.). Third, we propose to use a two-level fusion technique and experiment different pooling techniques to compute the prediction results. Experimental results using a multimodal egocentric dataset show that our proposed method can achieve very encouraging performance, despite the constraint that the scale of the existing egocentric datasets is still quite limited."
  },
  "cvpr2016_w14_adiverselowcosthighperformanceplatformforadvanceddriverassistancesystem(adas)applications": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "A Diverse Low Cost High Performance Platform for Advanced Driver Assistance System (ADAS) Applications",
    "authors": [
      "Prashanth Viswanath",
      "Kedar Chitnis",
      "Pramod Swami",
      "Mihir Mody",
      "Sujith Shivalingappa",
      "Soyeb Nagori",
      "Manu Mathew",
      "Kumar Desappan",
      "Shyam Jagannathan",
      "Deepak Poddar",
      "Anshu Jain",
      "Hrushikesh Garud",
      "Vikram Appia",
      "Mayank Mangla",
      "Shashank Dabral"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Viswanath_A_Diverse_Low_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Viswanath_A_Diverse_Low_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Advanced driver assistance systems (ADAS) are becoming more and more popular. Lot of the ADAS applications such as Lane departure warning (LDW), Forward Collision Warning (FCW), Automatic Cruise Control (ACC), Auto Emergency Braking (AEB), Surround View (SV) that were present only in high-end cars in the past have trickled down to the low and mid end vehicles. Lot of these applications are also mandated by safety authorities such as EUNCAP and NHTSA. In order to make these applications affordable in the low and mid end vehicles, it is important to have a cost effective, yet high performance and low power solution. Texas Instruments (TI's) TDA3x is an ideal platform which addresses these needs. This paper illustrates mapping of different algorithms such as SV, LDW, Object detection (OD), Structure From Motion (SFM) and Camera-Monitor Systems (CMS) to the TDA3x device, thereby demonstrating its compute capabilities. We also share the performance for these embedded vision applications."
  },
  "cvpr2016_w14_avisualattentionalgorithmdesignedforcoupledoscillatoracceleration": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "A Visual Attention Algorithm Designed for Coupled Oscillator Acceleration",
    "authors": [
      "Christopher Thomas",
      "Adriana Kovashka",
      "Donald Chiarulli",
      "Steven Levitan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Thomas_A_Visual_Attention_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Thomas_A_Visual_Attention_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present a new top-down and bottom-up saliency algorithm designed to exploit the capabilities of coupled oscillators: an ultra-low-power, high performance, non-boolean computer architecture designed to serve as a special purpose embedded vision accelerator. To do this, we extend a widely used bottom-up saliency pipeline by introducing a top-down channel which looks for objects of a particular type. The proposed channel relies on a segmentation of the input image to identify exemplar object segments resembling those encountered in training. The channel leverages pre-computed bottom-up feature maps to produce a novel scale-invariant descriptor for each segment with little computational overhead. We also introduce a new technique to automatically determine exemplar segments during training, without the need for annotations per segment. We evaluate our method on both NeoVision2 DARPA challenge datasets, illustrating significant gains in performance compared to all baseline approaches."
  },
  "cvpr2016_w14_embeddedmotiondetectionvianeuralresponsemixturebackgroundmodeling": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "Embedded Motion Detection via Neural Response Mixture Background Modeling",
    "authors": [
      "Mohammad Javad Shafiee",
      "Parthipan Siva",
      "Paul Fieguth",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Shafiee_Embedded_Motion_Detection_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Shafiee_Embedded_Motion_Detection_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Recent studies have shown that deep neural networks (DNNs) can outperform state-of-the-art for a multitude of computer vision tasks. However, the ability to leverage DNNs for near real-time performance on embedded systems have been all but impossible so far without requiring specialized processors or GPUs.In this paper, we present a new motion detection algorithm that leverages the power of DNNs while maintaining low computational complexity needed for near real-time embedded performance without specialized hardware. The proposed Neural Response Mixture (NeRM) model leverages rich deep features extracted from the neural responses of an efficient, stochastically-formed deep neural network for constructing Gaussian mixture models to detect moving objects in a scene. NeRM was implemented on an embedded system on an Axis surveillance camera, and results demonstrated that the proposed NeRM approach can strong motion detection accuracy while operating at near real-time performance."
  },
  "cvpr2016_w14_ascalablehigh-performancehardwarearchitectureforreal-timestereovisionbysemi-globalmatching": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "A Scalable High-Performance Hardware Architecture for Real-Time Stereo Vision by Semi-Global Matching",
    "authors": [
      "Jaco Hofmann",
      "Jens Korinth",
      "Andreas Koch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Hofmann_A_Scalable_High-Performance_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Hofmann_A_Scalable_High-Performance_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Perceiving distance from two camera images, a task called stereo vision, is fundamental for many applications in robotics or automation. However, algorithms that compute this information at high accuracy have a high computational complexity. One such algorithm, Semi Global Matching (SGM), performs well in many stereo vision benchmarks, while maintaining a manageable computational complexity. Nevertheless, CPU and GPU implementations of this algorithm often fail to achieve real-time processing of camera images, especially in power-constrained embedded environments. This work presents a novel architecture to calculate disparities through SGM. The proposed architecture is highly scalable and applicable for low-power embedded as well as high-performance multi-camera high-resolution applications. "
  },
  "cvpr2016_w14_visionbasedautonomousorientationalcontrolforaerialmanipulationviaon-boardfpga": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "Vision Based Autonomous Orientational Control for Aerial Manipulation via On-Board FPGA",
    "authors": [
      "Leewiwatwong Suphachart",
      "Shouhei Shimahara",
      "Robert Ladig",
      "Kazuhiro Shimonomura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Suphachart_Vision_Based_Autonomous_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Suphachart_Vision_Based_Autonomous_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We describe an FPGA-based on-board control system for autonomous orientation of an aerial robot to assist aerial manipulation tasks. The system is able to apply yaw control to aid an operator to precisely position a drone when it is nearby a bar-like object. This is achieved by applying parallel Hough transform enhanced with a novel image space separation method, enabling highly reliable results in various circumstances combined with high performance. The feasibility of this approach is shown by applying the system to a multi-rotor aerial robot equipped with an upward directed robotic hand on top of the airframe developed for high altitude manipulation tasks. In order to grasp a bar-like object, orientation of the bar object is observed from the image data obtained by a monocular camera mounted on the robot. This data is then analyzed by the on-board FPGA system to control yaw angle of the aerial robot. In experiments, reliable yaw-orientation control of the aerial robot is achieved."
  },
  "cvpr2016_w14_embeddedvisionsystemforatmosphericturbulencemitigation": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "Embedded Vision System for Atmospheric Turbulence Mitigation",
    "authors": [
      "Ajinkya Deshmukh",
      "Gaurav Bhosale",
      "Swarup Shanti",
      "Karthik Reddy",
      "Hemanthkumar P.",
      "Chandrasekhar A.",
      "Kirankumar P.",
      "Vijaysagar K."
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Deshmukh_Embedded_Vision_System_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Deshmukh_Embedded_Vision_System_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Outdoor surveillance systems that involve farfield operations often encounter atmospheric turbulence perturbations due to a series of randomized reflections and refraction effecting incoming light rays. The resulting distortions make it hard to discriminate between true moving objects and turbulence induced motion. Current algorithms are not effective in detecting true moving objects in the scene and also rely on computationally complex warping methods. In this paper, we describe a real time embedded solution connected with traditional cameras to both rectify turbulence distortions and reliably detect and track true moving targets. Our comparisons with other methods shows better turbulence rectification with less false and miss detections. FPGA-DSP based embedded realization of our algorithm achieves nearly 15x speed-up along with lesser memory requirement over a quad core PC implementation. The proposed system is suitable for persistence surveillance systems and optical sight devices."
  },
  "cvpr2016_w14_approximatedpredictionstrategyforreducingpowerconsumptionofconvolutionalneuralnetworkprocessor": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "Approximated Prediction Strategy for Reducing Power Consumption of Convolutional Neural Network Processor",
    "authors": [
      "Takayuki Ujiie",
      "Masayuki Hiromoto",
      "Takashi Sato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Ujiie_Approximated_Prediction_Strategy_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Ujiie_Approximated_Prediction_Strategy_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Convolutional neural network (CNN) is becoming popular because of its great ability for accurate image recognition. However, the computational cost is extremely high, which increases power consumption of embedded CV systems. This paper proposes an efficient computing method, LazyConvPool (LCP), and its hardware architecture to reduce power consumption of CNN-based image recognition. The LCP exploits redundancy of operations in CNN and only executes essential convolutions by an approximated prediction technique. We also propose Sign Connect, which is a low computational-cost approximated prediction without any multiplications. The experimental evaluation using image classification dataset shows that the proposed method reduces the power consumption by 17.8%-20.2% and energy consumption by 11.4%-14.1% while retaining recognition performance."
  },
  "cvpr2016_w14_visualmonocularobstacleavoidanceforsmallunmannedvehicles": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "Visual Monocular Obstacle Avoidance for Small Unmanned Vehicles",
    "authors": [
      "Levente Kovacs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Kovacs_Visual_Monocular_Obstacle_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Kovacs_Visual_Monocular_Obstacle_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper presents and extensively evaluates a visual obstacle avoidance method using frames of a single camera, intended for application on small devices (ground or aerial robots or even smartphones). It is based on image region classification using so called relative focus maps, it does not require a priori training, and it is applicable in both indoor and outdoor environments, which we demonstrate through evaluations using both simulated and real data."
  },
  "cvpr2016_w14_real-time,embeddedsceneinvariantcrowdcountingusingscale-normalizedhistogramofmovinggradients(homg)": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "Real-Time, Embedded Scene Invariant Crowd Counting Using Scale-Normalized Histogram of Moving Gradients (HoMG)",
    "authors": [
      "Parthipan Siva",
      "Mohammad Javad Shafiee",
      "Michael Jamieson",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Siva_Real-Time_Embedded_Scene_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Siva_Real-Time_Embedded_Scene_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Automated crowd counting has garnered significant interest for video surveillance. This paper proposes a novel scene invariant crowd counting algorithm designed for high accuracy yet low computational complexity in order to facilitate widespread use in real-time embedded video analytics systems.A novel low-complexity, scale-normalized feature called Histogram of Moving Gradients (HoMG) is introduced for highly effective spatiotemporal representation of crowds within a video.Real-time crowd region detection is achieved via boosted cascade of weak classifiers based on HoMG features. Based on the detected crowd regions, linear support vector regression (SVR) of crowd-region HoMG features is introduced for real-time crowd counting.Experimental results using a multi-scene crowd dataset show that the proposed algorithm outperforms state-of-the-art crowd counting algorithms while embedded on modern surveillance cameras."
  },
  "cvpr2016_w14_3dcapture3dreconstructionforasmartphone": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "3DCapture: 3D Reconstruction for a Smartphone",
    "authors": [
      "Oleg Muratov",
      "Yury Slynko",
      "Vitaly Chernov",
      "Maria Lyubimtseva",
      "Artem Shamsuarov",
      "Victor Bucha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Muratov_3DCapture_3D_Reconstruction_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Muratov_3DCapture_3D_Reconstruction_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a method of reconstruction of 3D representation (a mesh with a texture) of an object on a smartphone with a monocular camera. The reconstruction consists of two parts - real-time scanning around the object and post processing. At the scanning stage IMU sensors data are acquired along with tracks of features in video. A special care is taken to comply with 360 scan requirement. All these data are used to build a camera trajectory at offline stage using bundle adjustment techniques. This trajectory is used in calculation of depth maps, which are used to construct a polygonal mesh with overlaid textures. The proposed method allows online tracking at 30 fps on a modern smartphone while the offline part is completed within 1 minute using an OpenCL compatible mobile GPU. In addition, we show that with a few modifications this algorithm can be adopted for human face reconstruction."
  },
  "cvpr2016_w14_embeddedcomputingframeworkforvision-basedreal-timesurroundthreatanalysisanddriverassistance": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Embedded Vision",
    "title": "Embedded Computing Framework for Vision-Based Real-Time Surround Threat Analysis and Driver Assistance",
    "authors": [
      "Frankie Lu",
      "Sean Lee",
      "Ravi Kumar Satzoda",
      "Mohan Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/html/Lu_Embedded_Computing_Framework_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w14/papers/Lu_Embedded_Computing_Framework_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we present a distributed embedded vision system that enables surround scene analysis and vehicle threat estimation. The proposed system analyzes the surroundings of the ego-vehicle using four cameras, each connected to a separate embedded processor. Each processor runs a set of optimized vision-based techniques to detect surrounding vehicles, so that the entire system operates at real-time speeds. This setup has been demonstrated on multiple vehicle testbeds with high levels of robustness under real-world driving conditions and is scalable to additional cameras. Finally, we present a detailed evaluation which shows over 95% accuracy and operation at nearly 15 frames per second. "
  },
  "cvpr2016_w15_editorialfortheworkshoponbiomedicalimageregistration(wbir)2016": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Editorial for the Workshop on Biomedical Image Registration (WBIR) 2016",
    "authors": [
      "Julia A. Schnabel",
      "Kensaku Mori",
      "Ben Glocker",
      "Mattias P. Heinrich"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Schnabel_Editorial_for_the_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Schnabel_Editorial_for_the_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " WBIR 2016 is the seventh international Workshop on Biomedical Image Registration. It aims to bring together leading researchers in the area of biomedical image regis- tration to present and discuss recent developments in the field. In recent years, medical imaging researchers have established several unique approaches to deal with the often very complex, high dimensional and multi-modal nature of the image registration problem. In addition, they often draw inspirations from concepts originally introduced by the much larger computer vision and pattern recognition community, such as graphical models, machine learning, superpixels and many more. WBIR 2016 is therefore held for the very first time in conjunction with the Conference on Computer Vision and Pattern Recognition (CVPR) at Caesar's Palace in Las Vegas, Nevada, to emphasize the strong links between the two communities, and to foster fruitful scientific exchange and communication among researchers that share common interests."
  },
  "cvpr2016_w15_discreteoptimisationforgroup-wisecorticalsurfaceatlasing": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Discrete Optimisation for Group-Wise Cortical Surface Atlasing",
    "authors": [
      "Emma C. Robinson",
      "Ben Glocker",
      "Martin Rajchl",
      "Daniel Rueckert"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Robinson_Discrete_Optimisation_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Robinson_Discrete_Optimisation_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper presents a novel method for cortical surface atlasing. Groupwise registration is performed through a discrete optimisation framework that seeks to simultaneously improve pairwise correspondences between surface feature sets, whilst minimising a global cost relating to the rank of the feature matrix. It is assumed that when fully aligned, features will be highly linearly correlated, and thus have low rank. The framework is regularised through use of multi-resolution control point grids and higher-order smoothness terms, calculated by considering deformation strain for displacements of triplets of points. Accordingly the discrete framework is solved through high-order clique reduction.The framework is tested on cortical folding based alignment, using data from the Human Connectome Project.Results show that group-wise alignment improves folding correspondences, relative to registration between all pairwise combinations, and registration to a global average template. "
  },
  "cvpr2016_w15_sparsekernelmachinesfordiscontinuousregistrationandnonstationaryregularization": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Sparse Kernel Machines for Discontinuous Registration and Nonstationary Regularization",
    "authors": [
      "Christoph Jud",
      "Nadia Mori",
      "Philippe C. Cattin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Jud_Sparse_Kernel_Machines_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Jud_Sparse_Kernel_Machines_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present a novel approach where we address image registration with the concept of a sparse kernel machine. We formulate the registration problem as a regularized minimization functional where a reproducing kernel Hilbert space is used as transformation model. The regularization comprises a sparsity inducing l1-type norm and a well known l2 norm. We prove a representer theorem for this type of functional to guarantee a finite dimensional solution. The presented method brings the advantage of flexibly defining the admissible transformations by choosing a positive definite kernel jointly with an efficient sparse representation of the solution. As such, we introduce a new type of kernel function, which enables discontinuities in the transformation and simultaneously has nice interpolation properties. In addition, location-dependent smoothness is achieved within the same framework to further improve registration results. Finally, we make use of an adaptive grid refinement scheme to optimize on multiple scales and for a finer control point grid at locations of high gradients. We evaluate our new method with a public thoracic 4DCT dataset."
  },
  "cvpr2016_w15_accuratesmalldeformationexponentialapproximanttointegratelargevelocityfieldsapplicationtoimageregistration": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Accurate Small Deformation Exponential Approximant to Integrate Large Velocity Fields: Application to Image Registration",
    "authors": [
      "Sebastiano Ferraris",
      "Marco Lorenzi",
      "Pankaj Daga",
      "Marc Modat",
      "Tom Vercauteren"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Ferraris_Accurate_Small_Deformation_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Ferraris_Accurate_Small_Deformation_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " One of the basic components of diffeomorphic image registration algorithms based on velocity fields is the numerical method used to integrate velocity parameters and obtain spatial transformations as displacement fields. When the input velocity field does not depend on the time parameter, the solution is often referred to as the Lie exponential of the velocity field.In this work, we present an integration method for its numerical computation based both on a generalization of the scaling and squaring algorithm and on a class of numerical integrators aimed to solve systems of ordinary differential equations called exponential integrators. This new method led to the introduction of three numerical integrators, and the subsequent validation are performed on synthetic deformations and real medical images."
  },
  "cvpr2016_w15_fastdeformableimageregistrationwithnon-smoothdualoptimization": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Fast Deformable Image Registration With Non-Smooth Dual Optimization",
    "authors": [
      "Martin Rajchl",
      "John S.H Baxter",
      "Wu Qiu",
      "Ali R. Khan",
      "Aaron Fenster",
      "Terry M. Peters",
      "Daniel Rueckert",
      "Jing Yuan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Rajchl_Fast_Deformable_Image_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Rajchl_Fast_Deformable_Image_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Optimization techniques have been widely used in deformable registration, allowing for the incorporation of similarity metrics with regularization mechanisms. These regularization mechanisms are designed to mitigate the effects of trivial solutions to ill-posed registration problems and to otherwise ensure the resulting deformation fields are well-behaved. This paper introduces a novel deformable registration (DR) algorithm, RANCOR, which uses iterative convexification to address deformable registration problems under non-smooth total-variation regularization. Initial comparative results against four state-of-the-art registration algorithms and under smooth regularization, respectively, are presented using the Internet Brain Segmentation Repository (IBSR) database."
  },
  "cvpr2016_w15_imageregistrationforplacentareconstruction": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Image Registration for Placenta Reconstruction",
    "authors": [
      "Floris Gaisser",
      "Pieter P. Jonker",
      "Toshio Chiba"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Gaisser_Image_Registration_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Gaisser_Image_Registration_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we introduce a method to handle the challenges posed by image registration for placenta reconstruction from fetoscopic video as used in the treatment of Twin-to-Twin Transfusion Syndrome (TTTS). Panorama reconstruction of the placenta greatly supports the surgeon in obtaining a complete view of the placenta to localize vascular anastomoses. The found shunts can subsequently be blocked by coagulation in the correct order.By using similarity learning in training a Convolutional Neural Network we created a novel feature extraction method, allowing robust matching of keypoints for image registration and therefore taking the most critical step in placenta reconstruction from fetoscopic video.The fetoscopic video we used for our experiments was acquired from a training simulator for TTTS surgery. We compared our method with state-of-the-art methods. The matching performance of our method is up to three times better while the mean projection error is reduced with 64% for the registered images. Our image registration method provides the ground work for a complete panorama reconstruction of the placenta."
  },
  "cvpr2016_w15_tissue-volumepreservingdeformableimageregistrationfor4dctpulmonaryimages": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Tissue-Volume Preserving Deformable Image Registration for 4DCT Pulmonary Images",
    "authors": [
      "Bowen Zhao",
      "Gary E. Christensen",
      "Joo Hyun Song",
      "Yue Pan",
      "Sarah E. Gerard",
      "Joseph M. Reinhardt",
      "Kaifang Du",
      "Taylor Patton",
      "John M. Bayouth",
      "Geoffrey D. Hugo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Zhao_Tissue-Volume_Preserving_Deformable_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Zhao_Tissue-Volume_Preserving_Deformable_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a 4D (three spatial dimensions plus time) tissue-volume preserving non-rigid image registration algorithm for pulmonary 4D computed tomography (4DCT) data sets to provide relevant information for radiation therapy and estimate pulmonary ventilation. The sum of squared tissue volume difference (SSTVD) similarity cost takes into account the CT intensity changes of spatially corresponding voxels, which is caused by the variations of fraction of tissue within voxels throughout the respiratory cycle. The proposed 4D SSTVD registration scheme considers the entire dynamic 4D data set simultaneously, using both spatial and temporal information. We employed a uniform 4D cubic B-spline parametrization of the transform and a temporally extended linear elasticity regularization of deformation field to ensure temporal smoothness and thus biological plausibility of estimated deformation. We used a multi-resolution multi-grid registration framework with limited-memory Broyden Fletcher Goldfarb Shanno (L-BFGS) optimization procedure for rapid convergence and limited memory consumption. We conducted experiments using synthetic 2D+t images and clinical 4DCT pulmonary data sets and evaluated accuracy and temporal smoothness of the proposed method via manually annotated landmarks."
  },
  "cvpr2016_w15_registeringretinalvesselimagesfromlocaltoglobalviamultiscaleandmulticyclefeatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Registering Retinal Vessel Images From Local to Global via Multiscale and Multicycle Features",
    "authors": [
      "Haiyong Zheng",
      "Lin Chang",
      "Tengda Wei",
      "Xinxin Qiu",
      "Ping Lin",
      "Yangfan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Zheng_Registering_Retinal_Vessel_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Zheng_Registering_Retinal_Vessel_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a comprehensive method using multiscale and multicycle features for retinal vessel image registration with a local and global strategy. The multiscale vessel maps generated by multiwavelet kernels and multiscale hierarchical decomposition contain segmentation results at varying image resolutions in different levels of vessel details. Then the multicycle feature composed of various combinations of cycle structures with different numbers of vertices is extracted. The cycle structure consisting of vessel bifurcation points, crossover points of arteries and veins, and the connected vessels can be found by our Angle-based Depth-First Search (ADFS) algorithm. Local initial registration is implemented by the matched Cycle-Vessel feature points and global final registration is completed by the Cycle-Vessel-Bifurcation feature points using similarity transformation. Finally, our Skeleton Alignment Error Measure (SAEM) is calculated for optimal scale and cycle feature selection, yielding the best registration result intelligently. Experimental results show that our method outperforms state-of-the-art methods on retinal vessel image registration using different features in terms of accuracy and robustness."
  },
  "cvpr2016_w15_thedesignofsuperelastix--aunifyingframeworkforawiderangeofimageregistrationmethodologies": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "The Design of SuperElastix -- A Unifying Framework for a Wide Range of Image Registration Methodologies",
    "authors": [
      "Floris F. Berendsen",
      "Kasper Marstal",
      "Stefan Klein",
      "Marius Staring"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Berendsen_The_Design_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Berendsen_The_Design_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " A large diversity of image registration methodologies has emerged from the research community. The scattering of methods over toolboxes impedes rigorous comparison to select the appropriate method for a given application. Toolboxes typically tailor their implementations to a mathematical registration paradigm, which makes internal functionality nonexchangeable. Subsequently, this forms a barrier for adoption of registration technology in the clinic. We therefore propose a unifying, role-based software design that can integrate a broad range of functional registration components. These components can be configured into an algorithmic network via a single high-level user interface. A generic component handshake mechanism provides users feedback on incompatibilities. We demonstrate the viability of our design by incorporating two paradigms from different code bases. The implementation is done in C++ and is available as open source. The progress of embedding more paradigms can be followed via https://github.com/kaspermarstal/SuperElastix"
  },
  "cvpr2016_w15_tumorgrowthestimationviaregistrationofdce-mriderivedtumorspecificdescriptors": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Tumor Growth Estimation via Registration of DCE-MRI Derived Tumor Specific Descriptors",
    "authors": [
      "Thais Roque",
      "Bartlomiej W. Papiez",
      "Veerle Kersemans",
      "Sean Smart",
      "Danny Allen",
      "Michael Chappell",
      "Julia A. Schnabel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Roque_Tumor_Growth_Estimation_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Roque_Tumor_Growth_Estimation_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) provides information on changes occurring during tumor growth in the tumor micro-environment and -vasculature. In the present paper, tumor voxel-wise estimates of tumor descriptors including total cell number, proliferative cell number, hypoxic cell number, necrotic cell number and oxygen level derived from DCE-MRI data are used to guide the deformable registration of subsequent time points over the tumor growth cycle, evaluating their predictive value for tumor growth. The analysis of three pre-clinical colon carcinoma longitudinal cases shows that using physiologically meaningful measures of tumor as guidance information can improve non-rigid registration of longitudinal tumor imaging data when compared to a state-of-the-art local correlation coefficient Demons approach. Moreover, using the determinant of the Jacobian of the estimated displacement field as an indicator of volume change allows us to observe a correlation between the tumor descriptor values and tumor growth, especially when maps of hypoxic cells and level of oxygen were used to aid registration. To the best of our knowledge, this work demonstrates for the first time the feasibility of using biologically meaningful tumor descriptors (total cell number, proliferative cell number, hypoxic cell number, necrotic cell number and oxygen level) derived from DCE-MRI to aid non-rigid registration of longitudinal tumor data as well as to estimate tumor growth. "
  },
  "cvpr2016_w15_graph-constrainedsurfaceregistrationbasedontutteembedding": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Graph-Constrained Surface Registration Based on Tutte Embedding",
    "authors": [
      "Wei Zeng",
      "Yi-Jun Yang",
      "Muhammad Razib"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Zeng_Graph-Constrained_Surface_Registration_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Zeng_Graph-Constrained_Surface_Registration_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This work presents an efficient method to compute the registration between surfaces with consistent graph constraints based on Tutte graph embedding. Most natural objects have consistent anatomical structures, extracted as isomorphic feature graphs. For genus zero surfaces with >=1 boundaries, the graphs are planar and usually 3-connected. By using Tutte embedding, each feature graph isembedded as a convex subdivision of a planar convex domain. Using the convex subdivision as constraint, surfaces are mapped onto convex subdivision domains and the registration is then computed over them. The computation is based on constrained harmonic maps to minimize the stretching energy, where curvy graph constraints become linear ones. This method is theoretically rigorous. The algorithm solves sparse linear systems and is computationally efficient and robust. The resulting mappings are proved to be unique and diffeomorphic. Experiments on various facial surface data demonstrate its efficiency and practicality."
  },
  "cvpr2016_w15_acombinedemandvisualtrackingprobabilisticmodelforrobustmosaickingapplicationtofetoscopy": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "A Combined EM and Visual Tracking Probabilistic Model for Robust Mosaicking: Application to Fetoscopy",
    "authors": [
      "Marcel Tella-Amo",
      "Pankaj Daga",
      "Francois Chadebecq",
      "Stephen Thompson",
      "Dzhoshkun I. Shakir",
      "George Dwyer",
      "Ruwan Wimalasundera",
      "Jan Deprest",
      "Danail Stoyanov",
      "Tom Vercauteren",
      "Sebastien Ourselin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Tella-Amo_A_Combined_EM_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Tella-Amo_A_Combined_EM_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Twin-to-Twin Transfusion Syndrome (TTTS) is a progressive pregnancy complication in which inter-twin vascular connections in the shared placenta result in a blood flow imbalance between the twins. The most effective therapy is to sever these connections by laser photo-coagulation. However, the limited field of view of the fetoscope hinders their identification.A potential solution is to augment the surgeon's view by creating a mosaic image of the placenta. State-of-the-art mosaicking methods use feature-based approaches, which have three main limitations: (i) they are not robust against corrupt data e.g. blurred frames, (ii) temporal information is not used, (iii) the resulting mosaic suffers from drift. We introduce a probabilistic temporal model that incorporates electromagnetic and visual tracking data to achieve a robust mosaic with reduced drift. By assuming planarity of the imaged object, the nRT decomposition can be used to parametrize the state vector. Finally, we tackle the non-linear nature of the problem in a numerically stable manner by using the Square Root Unscented Kalman Filter. We show an improvement in performance in terms of robustness as well as a reduction of the drift in comparison to state-of-the-art methods in synthetic, phantom and ex vivo datasets."
  },
  "cvpr2016_w15_reducingdriftinmosaicingslit-lampretinalimages": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Reducing Drift in Mosaicing Slit-Lamp Retinal Images",
    "authors": [
      "Kristina Prokopetc",
      "Adrien Bartoli"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Prokopetc_Reducing_Drift_in_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Prokopetc_Reducing_Drift_in_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The construction of seamless and accurate mosaics from long slit-lamp retinal video sequences is an important and challenging task in navigated Pan-Retinal Photocoagulation. The main difficulty is accumulated registration drift due to the small number of features away from the optic nerve and the distortion induced by the geometry of the eye and the contact lens. We present a new approach to reduce the drift. Our main idea is to create long-term high precision point correspondences by associating a simple global model with local correction and perform key-frame based Bundle Adjustment. We evaluate the method's performance compared to state-of-the-art. The results obtained with our method show significantly lower accumulated error."
  },
  "cvpr2016_w15_howtobuildanaveragemodelwhensamplesarevariablyincomplete?applicationtofossildata": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "How to Build an Average Model When Samples Are Variably Incomplete? Application to Fossil Data",
    "authors": [
      "Jean Dumoncel",
      "Gerard Subsol",
      "Stanley Durrleman",
      "Jean-Pierre Jessel",
      "Amelie Beaudet",
      "Jose Braga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Dumoncel_How_to_Build_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Dumoncel_How_to_Build_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In paleontology, incomplete samples with small or large missing parts are frequently encountered. For example, dental crowns, which are widely studied in paleontology because of their potential interest in taxonomic and phylogenetic analyses, are nearly systematically affected by a variable degree of wear that alters considerably their shape. It is then difficult to compute a significant reference surface model based on classical methods which are used to build atlases from set of samples. In this paper, we present a general approach to deal with the problem of estimating an average model from a set of incomplete samples. Our method is based on a state-of-the-art non-rigid surface registration algorithm. In a first step, we detect missing parts which allows one to focus only on the common parts to get an accurate registration result. In a second step, we try to build average model of the missing parts by using information which is available in a subset of the samples. We specifically apply our method on teeth, and more precisely on the surface in between dentine and enamel tissues (EDJ). We investigate the robustness and accuracy properties of the methods on a set of artificial samples representing a high degree of incompleteness. We compare the reconstructed complete shape to a ground-truth dataset. We then show some results on real data."
  },
  "cvpr2016_w15_populationshapecollapseinlargedeformationregistrationofmrbrainimages": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Population Shape Collapse in Large Deformation Registration of MR Brain Images",
    "authors": [
      "Wei Shao",
      "Gary E. Christensen",
      "Hans J. Johnson",
      "Joo Hyun Song",
      "Oguz C. Durumeric",
      "Casey P. Johnson",
      "Joseph J. Shaffer",
      "Vincent A. Magnotta",
      "Jess G. Fiedorowicz",
      "John A. Wemmie"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Shao_Population_Shape_Collapse_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Shao_Population_Shape_Collapse_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper examines the shape collapse problem that occurs when registering a pair of images or a population of images of the brain to a reference (target) image coordinate system using diffeomorphic image registration. Shape collapse occurs when a foreground or background structure in an image with non-zero volume is transformed into a set of zero or near zero volume as measured on a discrete voxel lattice in the target image coordinate system. Shape collapse may occur during image registration when the moving image has a structure that is either missing or does not sufficiently overlap the corresponding structure in the target image. Such a problem is common in image registration algorithms with large degrees of freedom such as many diffeomorphic image registration algorithms. Shape collapse is a concern when mapping functional data. For example, loss of signal may occur when mapping functional data such as fMRI, PET, SPECT using a transformation with a shape collapse if the functional signal occurs at the collapse region. This paper proposes an novel shape collapse measurement algorithm to detect the regions of shape collapse after image registration in pairwise registration. We further compute the shape collapse for a population of pairwise transformations such as occurs when registering many images to a common atlas coordinate system. Experiments are presented using the SyN diffeomorphic image registration algorithm. We demonstrate how changing the input parameters to the SyN registration algorithm can mitigate some of the collapse image registration artifacts."
  },
  "cvpr2016_w15_registrationofdevelopmentalimagesequenceswithmissingdata": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Registration of Developmental Image Sequences With Missing Data",
    "authors": [
      "Istvan Csapo",
      "Yundi Shi",
      "Mar Sanchez",
      "Martin Styner",
      "Marc Niethammer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Csapo_Registration_of_Developmental_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Csapo_Registration_of_Developmental_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Longitudinal image registration is commonly used to establish spatial correspondence between images when investigating temporal changes in brain morphology. Most image registration methods have been developed to align images that are similar in appearance or structure. If such similarity is not given (e.g., in the case of neurodevelopmental studies, which is the target application of this paper), (i) local similarity measures, (ii) metamorphosis approaches, or (iii) methods modeling longitudinal intensity change can be used. Methods modeling longitudinal intensity change have the advantage of not treating images as independent static samples. However, missing or incomplete data can lead to poor model estimation and, in turn,poor registration. Therefore, incomplete longitudinal data sets are often excluded from analysis. Here, we propose a method to build a longitudinal atlas of intensity change and incorporate it as a prior into an existing model-based registration method. We show that using the prior can guide the deformable registration of longitudinal images of brain development with missing data and produce comparable registration results to complete data sets."
  },
  "cvpr2016_w15_current-andvarifold-basedregistrationoflungvesselandairwaytrees": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Current- and Varifold-Based Registration of Lung Vessel and Airway Trees",
    "authors": [
      "Yue Pan",
      "Gary E. Christensen",
      "Oguz C. Durumeric",
      "Sarah E. Gerard",
      "Joseph M. Reinhardt",
      "Geoffrey D. Hugo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Pan_Current-_and_Varifold-Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Pan_Current-_and_Varifold-Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Registering lung CT images is an important problem for many applications including tracking lung motion over the breathing cycle, tracking anatomical and function changes over time, and detecting abnormal mechanical properties of the lung. This paper compares and contrasts current- and varifold-based diffeomorphic image registration approaches for registering tree-like structures of the lung. In these approaches, curve-like structures in the lung---for example, the skeletons of vessels and airways segmentation---are represented by currents or varifolds in the dual space of a Reproducing Kernel Hilbert Space (RKHS). Current and varifold representations are discretized and are parameterized via of a collection of momenta. A momenta corresponds to a line segment via the coordinates of the center of the line segment and the tangent direction of the line segment at the center. A varifold-based registration approach is similar to currents except that two varifold representations are aligned independent of the tangent vector orientation. An advantage of varifolds over currents is that the orientation of the tangent vectors can be difficult to determine especially when the vessel and airway trees are not connected. In this paper, we examine the image registration sensitivity and accuracy of current- and varifold-based registration as a function of the number and location of momentum used to represent tree like-structures in the lung. The registrations presented in this paper were generated using the Deformetrica software package."
  },
  "cvpr2016_w15_simpleelastixauser-friendly,multi-linguallibraryformedicalimageregistration": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "SimpleElastix: A User-Friendly, Multi-Lingual Library for Medical Image Registration",
    "authors": [
      "Kasper Marstal",
      "Floris Berendsen",
      "Marius Staring",
      "Stefan Klein"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Marstal_SimpleElastix_A_User-Friendly_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Marstal_SimpleElastix_A_User-Friendly_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we present SimpleElastix, an extension of SimpleITK designed to bring the Elastix medical image registration library to a wider audience. Elastix is a modular collection of robust C++ image registration algorithms that is widely used in the literature. However, its command-line interface introduces overhead during prototyping, experimental setup, and tuning of registration algorithms. By integrating Elastix with SimpleITK, Elastix can be used as a native library in Python, Java, R, Octave, Ruby, Lua, Tcl and C# on Linux, Mac and Windows. This allows Elastix to intregrate naturally with many development environments so the user can focus more on the registration problem and less on the underlying C++ implementation. As means of demonstration, we show how to register MR images of brains and natural pictures of faces using minimal amount of code. SimpleElastix is open source, licensed under the permissive Apache License Version 2.0 and available at https://github.com/kaspermarstal/SimpleElastix."
  },
  "cvpr2016_w15_effectsofresolutionandregistrationalgorithmontheaccuracyofepivnavsforrealtimeheadmotioncorrectioninmri": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Effects of Resolution and Registration Algorithm on the Accuracy of EPI vNavs for Real Time Head Motion Correction in MRI",
    "authors": [
      "Yingzhuo Zhang",
      "Iman Aganj",
      "Andre J. van der Kouwe",
      "M. Dylan Tisdall"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Zhang_Effects_of_Resolution_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Zhang_Effects_of_Resolution_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Low-resolution, EPI-based Volumetric Navigators (vNavs) have been used as a prospective motion-correction system in a variety of MRI neuroimaging pulse sequences. The use of low-resolution volumes represents a trade-off between motion tracking accuracy and acquisition time. However, this means that registration must be accurate on the order of 0.2 voxels or less to be effective for motion correction. While vNavs have shown promising results in clinical and research use, the choice of navigator and registration algorithm have not previously been systematically evaluated. In this work we experimentally evaluate the accuracy of vNavs, and possible design choices for future improvements to the system, using real human data. We acquired navigator volumes at three isotropic resolutions (6.4 mm, 8 mm, and 10 mm) with known rotations and translations. The vNavs were then rigidly registered using trilinear, tricubic, and cubic B-spline interpolation. We demonstrate a novel refactoring of the cubic B-spline algorithm that stores pre-computed coefficients to reduce the per-interpolation time to be identical to tricubic interpolation. Our results show that increasing vNav resolution improves registration accuracy, and that cubic B-splines provide the highest registration accuracy at all vNav resolutions. Our results also suggest that the time required by vNavs may be reduced by imaging at 10 mm resolution, without substantial cost in registration accuracy."
  },
  "cvpr2016_w15_graphcuts-basedregistrationrevisitedanovelapproachforlungimageregistrationusingsupervoxelsandimage-guidedfiltering": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Graph Cuts-Based Registration Revisited: A Novel Approach for Lung Image Registration Using Supervoxels and Image-Guided Filtering",
    "authors": [
      "Adam Szmul",
      "Bartlomiej W. Papiez",
      "Russell Bates",
      "Andre Hallack",
      "Julia A. Schnabel",
      "Vicente Grau"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Szmul_Graph_Cuts-Based_Registration_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Szmul_Graph_Cuts-Based_Registration_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This work revisits the concept of graph cuts as an efficient optimization technique in image registration. Previously, due to the computational burden involved, the use of graph cuts in this context has been mainly limited to 2D applications. Here we show how combining graph cuts with supervoxels, resulting in a sparse, yet meaningful graph-based image representation, can overcome previous limitations. Additionally, we show that a relaxed graph representation of the image allows for `sliding' motion modeling and provides anatomically plausible estimation of the deformations. This is achieved by using image-guided filtering of the estimated sparse deformation field. We evaluate our method on a publicly available CT lung data set and show that our new approach compares very favourably with state-of-the-art in continuous and discrete image registration."
  },
  "cvpr2016_w15_multi-atlasbasedpseudo-ctsynthesisusingmultimodalimageregistrationandlocalatlasfusionstrategies": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Multi-Atlas Based Pseudo-CT Synthesis Using Multimodal Image Registration and Local Atlas Fusion Strategies",
    "authors": [
      "Johanna Degen",
      "Mattias P. Heinrich"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Degen_Multi-Atlas_Based_Pseudo-CT_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Degen_Multi-Atlas_Based_Pseudo-CT_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The synthesis of pseudo-CT images is of particular in- terest in the development of hybrid PET-MRI devices and MRI-guided radiotherapy. These images can be used for attenuation correction during PET image reconstruc- tion. Furthermore, using MRI-based radiotherapy planning would enable a more accurate dosimetry planning due to the superior soft tissue contrast of the scans. The previ- ously proposed methods for pseudo-CT synthesis are char- acterised by mainly two drawbacks. First, most proposed methods are limited to the head and neck region and there- fore not feasible in case of whole body applications. Sec- ond, the presence of aligned training pairs of both MRI and CT scans for a number of subjects is assumed. In this work, we present preliminary results for atlas-based approaches using multiple CT atlas scans (from different patients) to synthesise a pseudo-CT image for a new patient using only their MRI data. This application requires ac- curate and robust deformable multimodal registration. We employed a recent discrete optimisation registration frame- work together with a self-similarity-based metric to accu- rately match the CT atlases to the anatomy of the patient. The registered atlases are then jointly combined by means of local fusion strategies. We apply our method to different 3D whole body MRI scans and a total of 18 3D whole body CT atlases. In addition to intensity fusion, the proposed methods can also be used for label fusion. Since evaluation based directly on synthesised intensity values is problem- atic, we use the Dice overlap after the fusion of segmenta- tion labels as a proxy measure. Our proposed new method, which uses MIND descriptors for multimodal label fusion shows overall the best results."
  },
  "cvpr2016_w15_afastdrrgenerationschemefor3d-2dimageregistrationbasedontheblockprojectionmethod": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "A Fast DRR Generation Scheme for 3D-2D Image Registration Based on the Block Projection Method",
    "authors": [
      "Zhiping Mu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Mu_A_Fast_DRR_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Mu_A_Fast_DRR_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In three-dimensional to two-dimensional (3D-2D) image registration, DRR (digitally reconstructed radiograph) generation is often a bottleneck in computation. In this article, a novel fast DRR generation scheme is proposed based on the recently introduced Block Projection method and Slab algorithm that reuse building blocks of DRRs previously generated for known poses. The scheme is flexible as exemplified in pose grid design and slab binding, and upper bounds in projection error exist and can be estimated. Experiments were conducted to evaluate DRR quality and sensitivity to pose difference; computing time and error bounds were reported. The results showed that on a conventional computer the proposed scheme generated high quality, pose-preserving DRRs of size 512x512 in 6 ms with slab binding, demonstrating its potential to be a viable solution to fast, high quality DRR generation for 3D-2D image registration."
  },
  "cvpr2016_w15_optimalestimationofdiffusionindw-mribyhigh-ordermrf-basedjointdeformableregistrationanddiffusionmodeling": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Optimal Estimation of Diffusion in DW-MRI by High-Order MRF-Based Joint Deformable Registration and Diffusion Modeling",
    "authors": [
      "Evgenios N. Kornaropoulos",
      "Evangelia I. Zacharaki",
      "Pierre Zerbib",
      "Chieh Lin",
      "Alain Rahmouni",
      "Nikos Paragios"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Kornaropoulos_Optimal_Estimation_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Kornaropoulos_Optimal_Estimation_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Over the last years, the apparent diffusion coefficient (ADC), computed from diffusion-weighted magnetic resonance (DW-MR) images, has become an important imaging biomarker for evaluating and managing patients with neoplastic or cerebrovascular disease. Standard methods for the calculation of ADC ignore the presence of noise and motion between successive (in time) DW-MR images acquired by changing the b-value. In order to accurately quantify the diffusion process during image acquisition, we introduce a method based on a high-order Markov Random Field (MRF) formulation that jointly registers the DW-MR images and models the spatiotemporal diffusion. Spatial smoothness on the ADC map, as well as spatiotemporal deformation smoothness, is imposed towards producing anatomically meaningful representations. The high-order dependencies in our MRF model are handled through Dual Decomposition. Performance of registration is compared to a state-of-the art registration approach in terms of obtained fitting error of the diffusion model in the core of the tumor. We also assess the clinical significance of the proposed method by examining the ability of the mean ADC value inside the tumor to act as imaging surrogate for lymphoma staging. Preliminary results reveal a marginally better performance of our method when compared against the standard ADC map used in clinical practice, which indicates its potential as a means for extracting imaging biomarkers."
  },
  "cvpr2016_w15_totalcorrelation-basedgroupwiseimageregistrationforquantitativemri": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Total Correlation-Based Groupwise Image Registration for Quantitative MRI",
    "authors": [
      "Jean-Marie Guyader",
      "Wyke Huizinga",
      "Valerio Fortunati",
      "Dirk H. J. Poot",
      "Matthijs van Kranenburg",
      "Jifke F. Veenland",
      "Margarethus M. Paulides",
      "Wiro J. Niessen",
      "Stefan Klein"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Guyader_Total_Correlation-Based_Groupwise_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Guyader_Total_Correlation-Based_Groupwise_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In quantitative magnetic resonance imaging (qMRI), quantitative tissue properties can be estimated by fitting a signal model to the voxel intensities of a series of images acquired with different settings. To obtain reliable quantitative measures, it is necessary that the qMRI images are spatially aligned so that a given voxel corresponds in all images to the same anatomical location. The objective of the present study is to describe and evaluate a novel automatic groupwise registration technique using a dissimilarity metric based on an approximated form of total correlation. The proposed registration method is applied to five qMRI datasets of various anatomical locations, and the obtained registration performances are compared to these of a conventional pairwise registration based on mutual information. The results show that groupwise total correlation yields better registration performances than pairwise mutual information. This study also establishes that the formulation of approximated total correlation is quite analogous to two other groupwise metrics based on principal component analysis (PCA). Registration performances of total correlation and these two PCA-based techniques are therefore compared. The results show that total correlation yields performances that are analogous to these of the PCA-based techniques. However, compared to these PCA-based metrics, total correlation has two main advantages. Firstly, it is directly derived from a multivariate form of mutual information, while the PCA-based metrics were obtained empirically. Secondly, total correlation has the advantage of requiring no user-defined parameter."
  },
  "cvpr2016_w15_multimodalwholebrainregistrationmriandhighresolutionhistology": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Biomedical Image Registration",
    "title": "Multimodal Whole Brain Registration: MRI and High Resolution Histology",
    "authors": [
      "Maryana Alegro",
      "Edson Amaro-Jr",
      "Burlen Loring",
      "Helmut Heinsen",
      "Eduardo Alho",
      "Lilla Zollei",
      "Daniela Ushizima",
      "Lea T. Grinberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/html/Alegro_Multimodal_Whole_Brain_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w15/papers/Alegro_Multimodal_Whole_Brain_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Three-dimensional brain imaging through cutting-edge MRI technology allows assessment of physical and chemical tissue properties at sub-millimeter resolution. In order to improve brain understanding as part of diagnostic tasks using MRI images, other imaging modalities to obtain deep cerebral structures and cytoarchitectural boundaries have been investigated. Under availability of postmortem samples, the fusion of MRI to brain histology supports more accurate description of neuroanatomical structures since it preserves microscopic entities and reveal fine anatomical details, unavailable otherwise. Nonetheless, histological processing causes severe tissue deformation and loss of the brain original 3D conformation, preventing direct comparisons between MRI and histology. This paper proposes an interactive computational pipeline designed to register multimodal brain data and enable direct histology-MRI correlation. Our main contribution is to develop schemes for brain data fusion, distortion corrections, using appropriate diffeomorphic mappings to align the 3D histological and MRI volumes. We describe our pipeline and preliminary developments of scalable processing schemes for high-resolution images. Tests consider a postmortem human brain, and include qualitatively and quantitatively results, such as 3D visualizations and the Dice coefficient (DC) between brain structures. Preliminary results show promising DC values when comparing our scheme results to manually labeled neuroanatomical regions defined by a neurosurgeon on MRI and histology data sets. DC was computed for the left caudade gyrus (LC), right hippocampus (RH) and lateral ventricles (LV). "
  },
  "cvpr2016_w16_depthcamerabasedoncolor-codedaperture": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computational Cameras and Displays",
    "title": "Depth Camera Based on Color-Coded Aperture",
    "authors": [
      "Vladimir Paramonov",
      "Ivan Panchenko",
      "Victor Bucha",
      "Andrey Drogolyub",
      "Sergey Zagoruyko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/html/Paramonov_Depth_Camera_Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/papers/Paramonov_Depth_Camera_Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we present a single-lens single-frame passive depth sensor based on conventional imaging system with minor hardware modifications. It is based on color-coded aperture approach and has high light-efficiency which allows capturing images even with handheld devices with small cameras. The sensor measures depth in millimeters in the whole frame, in contrast to prior-art approaches. Contributions of this paper are: (1) introduction of novel light-efficient coded aperture designs and corresponding algorithm modification; (2) depth sensor calibration procedure and disparity to depth conversion method; (3) a number of color-coded aperture based depth sensor implementations including a DSLR based prototype, a smartphone based prototype and a compact camera based prototype; (4) applications including real-time 3D scene reconstruction and depth based image effects."
  },
  "cvpr2016_w16_sparklegeometryglitterimagingfor3dpointtracking": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computational Cameras and Displays",
    "title": "SparkleGeometry: Glitter Imaging for 3D Point Tracking",
    "authors": [
      "Abigail Stylianou",
      "Robert Pless"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/html/Stylianou_SparkleGeometry_Glitter_Imaging_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/papers/Stylianou_SparkleGeometry_Glitter_Imaging_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We consider a geometric inference problem for an imaging system consisting of a camera that views the world through a planar, rectangular sheet of glitter.We describe a procedure to calibrate this imaging geometry as a generalized camera which characterizes the subset of the light field viewed through each piece of glitter.We propose an easy to construct physical prototype and characterize its performance for estimating the 3D position of a moving point light source just by viewing the changing sparkle patterns visible on the glitter sheet."
  },
  "cvpr2016_w16_time-offsetconversationsonalife-sizedautomultiscopicprojectorarray": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computational Cameras and Displays",
    "title": "Time-Offset Conversations on a Life-Sized Automultiscopic Projector Array",
    "authors": [
      "Andrew Jones",
      "Jonas Unger",
      "Koki Nagano",
      "Jay Busch",
      "Xueming Yu",
      "Hsuan-Yueh Peng",
      "Joseph Barreto",
      "Oleg Alexander",
      "Mark Bolas",
      "Paul Debevec"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/html/Jones_Time-Offset_Conversations_on_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/papers/Jones_Time-Offset_Conversations_on_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present a system for creating and displaying interactive life-sized 3D digital humans based on pre-recorded interviews.We use 30 cameras and an extensive list of questions to record a large set of video responses.Users access videos through a natural conversation interface that mimics face-to-face interaction. Recordings of answers, listening and idle behaviors are linked together to create a persistent visual image of the person throughout the interaction.The interview subjects are rendered using flowed light fields and shown life-size on a special rear-projection screen with an array of 216 video projectors. The display allows multiple users to see different 3D perspectives of the subject in proper relation to their viewpoints, without the need for stereo glasses. The display is effective for interactive conversations since it provides 3D cues such as eye gaze and spatial hand gestures. "
  },
  "cvpr2016_w16_avoidingthedeconvolutionframeworkorientedcolortransferforenhancinglow-lightimages": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computational Cameras and Displays",
    "title": "Avoiding the Deconvolution: Framework Oriented Color Transfer for Enhancing Low-Light Images",
    "authors": [
      "Laura Florea",
      "Corneliu Florea",
      "Ciprian Ionascu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/html/Florea_Avoiding_the_Deconvolution_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/papers/Florea_Avoiding_the_Deconvolution_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we introduce a novel color transfer method to address the underexposed image amplification problem. Targeted scenario implies a dual acquisition, containing a normally exposed, possibly blurred, image and an underexposed/low-light but sharp one. The problem of enhancing the low-light image is addressed as a color transfer problem. To properly solve the color transfer, the scene is split into perceptual frameworks and we propose a novel piece-wise approximation. The proposed method is shown to lead to robust results from both an objective and a subjective point of view."
  },
  "cvpr2016_w16_power-efficientcamerasusingnaturalimagestatistics": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computational Cameras and Displays",
    "title": "Power-Efficient Cameras Using Natural Image Statistics",
    "authors": [
      "Roni Feldman",
      "Yair Weiss",
      "Yonina C. Eldar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/html/Feldman_Power-Efficient_Cameras_Using_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/papers/Feldman_Power-Efficient_Cameras_Using_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Abstract : Motivated by recent results on compressed sensing cameras we consider cameras that perform an analog linear transformation Phi on the signal, followed by scalar quantization. Specifically we ask: is it better to use compressed sensing (Phi is an under-sampling random matrix) or direct sensing (Phi is the sparsifying basis)? We compare the two approaches using their energy-distortion tradeoffs: assuming most of the energy consumed by such systems is in the ADC and the energy of the quantizer doubles with each bit, which system will give lower distortion for the same energy consumption? We present analytic expressions for the energy-distortion curves for three signal models: signals residing in a known subspace, sparse signals and power-law signals. For all of these models, our analysis shows that direct sensing results in lower distortion for a given energy consumption. We also present simulation results for natural images showing that direct sensing of Haar wavelet coefficients is preferable for these signals. Given the assumptions of our model, direct sensing of Haar wavelets can achieve high quality imaging (PSNR of 40 dB) with 6% the power consumption of standard cameras using 8 bits per channel."
  },
  "cvpr2016_w16_strategiesforresolvingcamerametamersusing3+1channel": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computational Cameras and Displays",
    "title": "Strategies for Resolving Camera Metamers Using 3+1 Channel",
    "authors": [
      "Dilip K. Prasad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/html/Prasad_Strategies_for_Resolving_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w16/papers/Prasad_Strategies_for_Resolving_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper discusses the problem of reducing camera metamerism by using a fourth spectral channel in addition to the typical red, green, and blue channels of the tristimulus sensor used in the commercial consumer cameras. Specifically, we consider three options for this fourth channel. The first option is to use a color channel from another camera to reduce metamerism. The second option is to use a color channel from an image captured by the same camera but with a color filter as the fourth channel. The third option is to design a specific spectral channel to be fabricated with the existing camera sensor. This option uses the metameric black space to design the channel. The commercial cameras' original metamerism is typically more than 20%, as observed in a dataset of 335 spectral images captured in 5 different indoor illuminations. Our results show that the third option is the best since it reduces metamerism down to about 5%. Among the first and second options, the first option is more effective and it reduces metamerism down to about 15%. The channel designed using the third option can be used for advanced applications such as distinguishing objects with different spectral reflectances but similar colors."
  },
  "cvpr2016_w17_fastandaccurateregistrationofstructuredpointcloudswithsmalloverlaps": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Large Scale 3D Data: Acquisition, Modelling and Analysis",
    "title": "Fast and Accurate Registration of Structured Point Clouds With Small Overlaps",
    "authors": [
      "Yanxin Ma",
      "Yulan Guo",
      "Jian Zhao",
      "Min Lu",
      "Jun Zhang",
      "Jianwei Wan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/html/Ma_Fast_and_Accurate_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/papers/Ma_Fast_and_Accurate_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " To perform registration of structured point clouds with large rotation and small overlaps, this paper presents an algorithm based on the direction angles and the projection information of dense points. This algorithm fully employs the geometric information of structured environment. It consists of two parts: rotation estimation and translation estimation. For rotation estimation, a direction angle is defined for a point cloud and then the rotation matrix is obtained by comparing the difference between the distributions of angles. For translation estimation, the point clouds are projected onto three orthogonal planes and then a correlation operation is performed on the projection images to calculate the translation vector. Experiments have been conducted on several datasets. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art approaches in terms of both accuracy and efficiency."
  },
  "cvpr2016_w17_comprehensiveautomated3durbanenvironmentmodellingusingterrestriallaserscanningpointcloud": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Large Scale 3D Data: Acquisition, Modelling and Analysis",
    "title": "Comprehensive Automated 3D Urban Environment Modelling Using Terrestrial Laser Scanning Point Cloud",
    "authors": [
      "Pouria Babahajiani",
      "Lixin Fan",
      "Joni-Kristian Kamarainen",
      "Moncef Gabbouj"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/html/Babahajiani_Comprehensive_Automated_3D_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/papers/Babahajiani_Comprehensive_Automated_3D_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we present a novel street scene modelling framework, which takes advantage of 3D point cloud captured by a high definition LiDAR laser scanner. We propose an automatic and robust approach to detect, segment and classify urban objects from point clouds hence reconstructing a comprehensive 3D urban environment model. Our system first automatically segments grounds point cloud. Then building facades will be detected by using binary range image processing. Remained point cloud will be grouped into voxels and subsequently transformed into super voxels. Local 3D features are extracted from super voxels and classified by trained boosted decision trees. Given labeled point cloud the proposed algorithm reconstructs the realistic model in two phases. Firstly building facades will be rendered by ShadVis algorithm. In the second step we apply a novel and fast method for fitting the solid predefined template mesh models to non-building labeled point cloud. "
  },
  "cvpr2016_w17_rgbddatasetspast,presentandfuture": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Large Scale 3D Data: Acquisition, Modelling and Analysis",
    "title": "RGBD Datasets: Past, Present and Future",
    "authors": [
      "Michael Firman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/html/Firman_RGBD_Datasets_Past_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/papers/Firman_RGBD_Datasets_Past_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Since the launch of the Microsoft Kinect, scores of RGBD datasets have been released. These have propelled advances in areas from reconstruction to gesture recognition. In this paper we explore the field, reviewing datasets across eight categories: semantics, object pose estimation, camera tracking, scene reconstruction, object tracking, human actions, faces and identification. By extracting relevant information in each category we help researchers to find appropriate data for their needs, and we consider which datasets have succeeded in driving computer vision forward and why.Finally, we examine the future of RGBD datasets. We identify key areas which are currently underexplored, and suggest that future directions may include synthetic data and dense reconstructions of static and dynamic scenes."
  },
  "cvpr2016_w17_realtimecompletedensedepthreconstructionforamonocularcamera": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Large Scale 3D Data: Acquisition, Modelling and Analysis",
    "title": "Real Time Complete Dense Depth Reconstruction for a Monocular Camera",
    "authors": [
      "Xiaoshui Huang",
      "Lixin Fan",
      "Jian Zhang",
      "Qiang Wu",
      "Chun Yuan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/html/Huang_Real_Time_Complete_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w17/papers/Huang_Real_Time_Complete_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we aim to solve the problem of estimating complete dense depth maps from a monocular moving camera. By 'complete', we mean depth information is estimated for every pixel and detailed reconstruction is achieved. Although this problem has previously been attempted, the accuracy of complete dense depth reconstruction is a remaining problem. We propose a novel system which produces accurate complete dense depth map. The new system consists of two subsystems running in separated threads, namely, dense mapping and sparse patch-based tracking. For dense mapping, a new projection error computation method is proposed to enhance the gradient component in estimated depth maps. For tracking, a new sparse patch-based tracking method estimates camera pose by minimizing a normalized error term. The experiments demonstrate that the proposed method obtains improved performance in terms of completeness and accuracy compared to three state-of-the-art dense reconstruction methods."
  },
  "cvpr2016_w18_chalearnlookingatpeopleandfacesoftheworldfaceanalysisworkshopandchallenge2016": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge 2016",
    "authors": [
      "Sergio Escalera",
      "Mercedes Torres Torres",
      "Brais Martinez",
      "Xavier Baro",
      "Hugo Jair Escalante",
      "Isabelle Guyon",
      "Georgios Tzimiropoulos",
      "Ciprian Corneou",
      "Marc Oliu",
      "Mohammad Ali Bagheri",
      "Michel Valstar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Escalera_ChaLearn_Looking_at_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Escalera_ChaLearn_Looking_at_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present the 2016 ChaLearn Looking at People and Faces of the World Challenge and Workshop, which ran three competitions on the common theme of face analysis from still images. The first one, Looking at People, ad- dressed age estimation, while the second and third com- petitions, Faces of the World, addressed accessory classi- fication and smile and gender classification, respectively. We present two crowd-sourcing methodologies used to col- lect manual annotations. A custom-build application was used to collect and label data about the apparent age of people (as opposed to the real age). For the Faces of the World data, the citizen-science Zooniverse platform was used. This paper summarizes the three challenges and the data used, as well as the results achieved by the partici- pants of the competitions. Details of the ChaLearn LAP FotW competitions can be found at http://gesture. chalearn.org ."
  },
  "cvpr2016_w18_apparentageestimationusingensembleofdeeplearningmodels": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Apparent Age Estimation Using Ensemble of Deep Learning Models",
    "authors": [
      "Refik Can Malli",
      "Mehmet Aygun",
      "Hazim Kemal Ekenel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Malli_Apparent_Age_Estimation_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Malli_Apparent_Age_Estimation_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we address the problem of apparent age estimation. Different from estimating the real age of individuals, in which each face image has a single age label, in this problem, face images have multiple age labels, corresponding to the ages perceived by the annotators, when they look at these images. This provides an intriguing computer vision problem, since in generic image or object classification tasks, it is typical to have a single ground truth label per class. To account for multiple labels per image, instead of using average age of the annotated face image as the class label, we have grouped the face images that are within a specified age range.Using these age groups and their age-shifted groupings, we have trained an ensemble of deep learning models. Before feeding an input face image to a deep learning model, five facial landmark points are detected and used for 2-D alignment.Proposed method achieves 0.3668 error in the final ChaLearn LAP 2016 challenge test set."
  },
  "cvpr2016_w18_deepagedistributionlearningforapparentageestimation": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Deep Age Distribution Learning for Apparent Age Estimation",
    "authors": [
      "Zengwei Huo",
      "Xu Yang",
      "Chao Xing",
      "Ying Zhou",
      "Peng Hou",
      "Jiaqi Lv",
      "Xin Geng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Huo_Deep_Age_Distribution_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Huo_Deep_Age_Distribution_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Apparent age estimation has attracted more and more researchers since its potential applications in the real world. Apparent age estimation differs from chronological age estimation that in apparent age estimation each facial image is labelled by multiple individuals, the mean age is the ground truth age and the uncertainty is introduced by the standard deviation. In this paper, we propose a novel method called Deep Age Distribution Learning(DADL) to deal with such situation. According to the given mean age and the standard deviation, we generate a Gaussian age distribution for each facial image as the training target. DADL first detects the facial region and aligns the facial image. Then, it uses deep Convolutional Neural Network(CNN) pre-trained based on the VGGFace to extract the predicted age distribution. Finally it uses ensemble method to get the result. Our DADL method got a good performance in ChaLearn LAP 2016-Track 1: Age Estimation and ranked the 2nd place."
  },
  "cvpr2016_w18_structuredoutputsvmpredictionofapparentage,genderandsmilefromdeepfeatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Structured Output SVM Prediction of Apparent Age, Gender and Smile From Deep Features",
    "authors": [
      "Michal Uricar",
      "Radu Timofte",
      "Rasmus Rothe",
      "Jiri Matas",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Uricar_Structured_Output_SVM_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Uricar_Structured_Output_SVM_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose structured output SVM for predicting the apparent age as well as gender and smile from a single face image represented by deep features. We pose the problem of apparent age estimation as an instance of the multi-class structured output SVM classifier followed by a softmax expected value refinement. The gender and smile are treated as binary classification problems. The proposed solution first detects the face in the image and then extracts deep features from the cropped image around the detected face. We use a convolutional neural network with VGG-16 architecture for learning deep features. The network is pretrained on the ImageNet database and then fine-tunned on IMDB-WIKI and ChaLearn 2015 LAP datasets. We validate our methods on the ChaLearn 2016 LAP dataset. Our structured output SVMs are trained solely on ChaLearn 2016 LAP data. We achieve excellent results for both apparent age prediction and gender and smile classification. "
  },
  "cvpr2016_w18_genderandsmileclassificationusingdeepconvolutionalneuralnetworks": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Gender and Smile Classification Using Deep Convolutional Neural Networks",
    "authors": [
      "Kaipeng Zhang",
      "Lianzhi Tan",
      "Zhifeng Li",
      "Yu Qiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Zhang_Gender_and_Smile_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Zhang_Gender_and_Smile_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Facial gender and smile classification in unconstrained environment is challenging due to the invertible and large variations of face images. In this paper, we propose a deep model composed of GNet and SNet for these two tasks. We leverage the multi-task learning and the general-to-specific fine-tuning scheme to enhance the performance of our model. Our strategies exploit the inherent correlation between face identity, smile, gender and other face attributes to relieve the problem of over-fitting on small training set and improve the classification performance. We also propose the tasks-aware face cropping scheme to extract attribute-specific regions. The experimental results on the ChaLearn'16 FotW dataset for gender and smile classification demonstrate the effectiveness of our proposed methods."
  },
  "cvpr2016_w18_deepbelearningdeepbinaryencodingformulti-labelclassification": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "DeepBE: Learning Deep Binary Encoding for Multi-Label Classification",
    "authors": [
      "Chenghua Li",
      "Qi Kang",
      "Guojing Ge",
      "Qiang Song",
      "Hanqing Lu",
      "Jian Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Li_DeepBE_Learning_Deep_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Li_DeepBE_Learning_Deep_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The track 2 and track 3 of ChaLearn 2016 can be considered as Multi-Label Classification problems. We present a framework of learning deep binary encoding (DeepBE) to deal with multi-label problems by transforming multi-labels to single labels. The transformation of DeepBE is in a hidden pattern, which can be well addressed by deep convolutions neural networks (CNNs). Furthermore, we adopt an ensemble strategy to enhance the learning robustness. This strategy is inspired by its effectiveness in fine-grained image recognition (FGIR) problem, while most of face related tasks such as track 2 and track 3 are also FGIR problems. By DeepBE, we got 5.45% and 10.84% mean square error for track 2 and track 3 respectively. Additionally, we proposed an algorithm adaption method to treat the multiple labels of track 2 directly and got 6.84% mean square error. "
  },
  "cvpr2016_w18_facialattributesclassificationusingmulti-taskrepresentationlearning": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Facial Attributes Classification Using Multi-Task Representation Learning",
    "authors": [
      "Max Ehrlich",
      "Timothy J. Shields",
      "Timur Almaev",
      "Mohamed R. Amer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Ehrlich_Facial_Attributes_Classification_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Ehrlich_Facial_Attributes_Classification_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper presents a new approach for facial attribute classification using multi-task learning model. Our model learns a shared feature representation that is well-suited for multiple attribute classification. For learning this shared feature representation we use a Restricted Boltzmann Machines based model and enhance it with a factored multi-task component to become Multi-Task Restricted Boltzmann Machines. We operate on faces and facial landmark points and learn a joint feature representation for all attributes. We use an iterative learning approach consisting of a bottom-up/top-down pass to learn the shared representation of our multi-task model and at inference we use a bottom-up pass to predict the different tasks. Our approach is not restricted to any type of attributes, however, for this paper we focus only on facial attributes. We evaluate our approach on three publicly available datasets and show superior classification performance improvement over the state-of-the-art."
  },
  "cvpr2016_w18_chalearnlookingatpeoplergb-disolatedandcontinuousdatasetsforgesturerecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "ChaLearn Looking at People RGB-D Isolated and Continuous Datasets for Gesture Recognition",
    "authors": [
      "Jun Wan",
      "Yibing Zhao",
      "Shuai Zhou",
      "Isabelle Guyon",
      "Sergio Escalera",
      "Stan Z. Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Wan_ChaLearn_Looking_at_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Wan_ChaLearn_Looking_at_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we present two large video multi-modal datasets for RGB and RGB-D gesture recognition: the ChaLearn LAP RGB-D Isolated Gesture Dataset (IsoGD) and the Continuous Gesture Dataset (ConGD). Both datasets are derived from the ChaLearn Gesture Dataset (CGD) that has a total of more than 50000 gestures for the \"one-shot-learning\" competition. To increase the potential of the old dataset, we designed new well curated datasets composed of 249 gesture labels, and including 47933 gestures manually labeled the begin and end frames in sequences. Using these datasets we will open two competitions on the CodaLab platform so that researchers can test and compare their methods for \"user independent\" gesture recognition. The first challenge is designed for gesture spotting and recognition in continuous sequences of gestures while the second one is designed for gesture classification from segmented data. The baseline method based on the bag of visual words (BoVW) model is also presented."
  },
  "cvpr2016_w18_dominantcodewordsselectionwithtopicmodelforactionrecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Dominant Codewords Selection With Topic Model for Action Recognition",
    "authors": [
      "Hirokatsu Kataokai",
      "Kenji Iwata",
      "Yutaka Satoh",
      "Masaki Hayashi",
      "Yoshimitsu Aok",
      "Slobodan Ilic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Kataokai_Dominant_Codewords_Selection_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Kataokai_Dominant_Codewords_Selection_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we propose a framework for recognizing human activities that uses only in-topic dominant codewords and a mixture of intertopic vectors. Latent Dirichlet allocation (LDA) is used to develop approximations of human motion primitives; these are mid-level representations, and they adaptively integrate dominant vectors when classifying human activities. In LDA topic modeling, action videos (documents) are represented by a bag-of-words (input from a dictionary), and these are based on improved dense trajectories. The output topics correspond to human motion primitives, such as finger moving or subtle leg motion. We eliminate the impurities, such as missed tracking or changing light conditions, in each motion primitive. The assembled vector of motion primitives is an improved representation of the action. We demonstrate our method on four different datasets."
  },
  "cvpr2016_w18_inferringvisualpersuasionviabodylanguage,setting,anddeepfeatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Inferring Visual Persuasion via Body Language, Setting, and Deep Features",
    "authors": [
      "Xinyue Huang",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Huang_Inferring_Visual_Persuasion_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Huang_Inferring_Visual_Persuasion_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The computer vision community has reached a point when it can start considering high-level reasoning tasks such as the \"communicative intents\" of images, or in what light an image portrays its subject. For example, an image might imply that a politician is competent, trustworthy, or energetic. We explore a variety of features for predicting these communicative intents. We study a number of facial expressions and body poses as cues for the implied nuances of the politician's personality. We also examine how the setting of an image (e.g. kitchen or hospital) influences the audience's perception of the portrayed politician. Finally, we improve the performance of an existing approach on this problem, by learning intermediate cues using convolutional neural networks. We show state of the art results on the Visual Persuasion dataset of Joo et al. [11]."
  },
  "cvpr2016_w18_kernelelmandcnnbasedfacialageestimation": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Kernel ELM and CNN Based Facial Age Estimation",
    "authors": [
      "Furkan Gurpinar",
      "Heysem Kaya",
      "Hamdi Dibeklioglu",
      "Ali Salah"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Gurpinar_Kernel_ELM_and_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Gurpinar_Kernel_ELM_and_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a two-level system for apparent age estimation from facial images. Our system first classifies samples into overlapping age groups. Within each group, the apparent age is estimated with local regressors, whose outputs are then fused for the final estimate. We use a deformable parts model based face detector, and features from a pre-trained deep convolutional network. Kernel extreme learning machines are used for classification. We evaluate our system on the ChaLearn Looking at People 2016 - Apparent Age Estimation challenge dataset, and report 0.3740 normal score on the sequestered test set."
  },
  "cvpr2016_w18_person-independent3dgazeestimationusingfacefrontalization": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Person-Independent 3D Gaze Estimation Using Face Frontalization",
    "authors": [
      "Laszlo A. Jeni",
      "Jeffrey F. Cohn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Jeni_Person-Independent_3D_Gaze_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Jeni_Person-Independent_3D_Gaze_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimate the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model. Next, the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation. The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets. Precision for head pose and gaze averaged 4 degrees or less for pitch, yaw, and roll. The algorithm outperformed alternative methods in both datasets."
  },
  "cvpr2016_w18_apparentageestimationfromfaceimagescombininggeneralandchildren-specializeddeeplearningmodels": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Apparent Age Estimation From Face Images Combining General and Children-Specialized Deep Learning Models",
    "authors": [
      "Grigory Antipov",
      "Moez Baccouche",
      "Sid-Ahmed Berrani",
      "Jean-Luc Dugelay"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Antipov_Apparent_Age_Estimation_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Antipov_Apparent_Age_Estimation_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This work describes our solution in the second edition of the ChaLearn LAP competition on Apparent Age Estimation. We train VGG-16 convolutional neural network on the huge IMDB-Wiki dataset for biological age estimation and then fine-tune it for apparent age estimation using the relatively small competition dataset. We show that the precise age estimation of children is the cornerstone of the competition. Therefore, we integrate a separate \"children\" VGG-16 network for apparent age estimation of children between 0 and 12 years old in our final solution. The \"children\" network is fine-tuned from the \"general\" one. We employ different age encoding strategies for training \"general\" and \"children\" networks: the soft one (label distribution encoding) for the \"general\" network and the strict one (0/1 classification encoding) for the \"children\" network. Our resulting solution wins the 1st place in the competition significantly outperforming the runner-up."
  },
  "cvpr2016_w18_identifyingsamepersonsfromtemporallysynchronizedvideostakenbymultiplewearablecameras": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - ChaLearn Looking at People and Faces of the World: Face Analysis Workshop and Challenge",
    "title": "Identifying Same Persons From Temporally Synchronized Videos Taken by Multiple Wearable Cameras",
    "authors": [
      "Kang Zheng",
      "Hao Guo",
      "Xiaochuan Fan",
      "Hongkai Yu",
      "Song Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/html/Zheng_Identifying_Same_Persons_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w18/papers/Zheng_Identifying_Same_Persons_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Video-based human action recognition benefits from multiple cameras which can provide temporally synchronized, multi-view videos. Cross-video person identification, i.e., determining whether at a given time, persons tracked in different videos are the same person or not, is a key step to integrate multi-view information for collaborative action recognition. For fixed cameras, this step is relatively easy since they can be calibrated. In this paper, we study cross-video person identification for wearable cameras, which are constantly moving with the wearers. Specifically, we take tracked persons from different videos to be the same person if their 3D poses are the same, given these videos are synchronized. We adapt an existing algorithm to estimate the tracked person's 3D poses in each 2D video using motion-based features. Experiments show that, although 3D pose estimation is not perfect, it can still lead to better cross-video person identification than using appearance information."
  },
  "cvpr2016_w19_semanticsegmentationofsmallobjectsandmodelingofuncertaintyinurbanremotesensingimagesusingdeepconvolutionalneuralnetworks": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Visual Analysis of Satellite to Street Imagery",
    "title": "Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks",
    "authors": [
      "Michael Kampffmeyer",
      "Arnt-Borre Salberg",
      "Robert Jenssen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w19/html/Kampffmeyer_Semantic_Segmentation_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w19/papers/Kampffmeyer_Semantic_Segmentation_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a deep Convolutional Neural Network (CNN) for land cover mapping in remote sensing images, with a focus on urban areas. In remote sensing, class imbalance represents often a problem for tasks like land cover mapping, as small objects get less prioritised in an effort to achieve the best overall accuracy. We propose a novel approach to achieve high overall accuracy, while still achieving good accuracy for small objects. Quantifying the uncertainty on a pixel scale is another challenge in remote sensing, especially when using CNNs. In this paper we use recent advances in measuring uncertainty for CNNs and evaluate their quality both qualitatively and quantitatively in a remote sensing context. We demonstrate our ideas on different deep architectures including patch-based and so-called pixel-to-pixel approaches, as well as their combination, by classifying each pixel in a set of aerial images covering Vaihingen, Germany. The results show that we obtain an overall classification accuracy of 87%. The corresponding F1-score for the small object class \"car\" is 80.6%, which is higher than state-of-the art for this dataset. "
  },
  "cvpr2016_w19_automaticalignmentofindoorandoutdoorbuildingmodelsusing3dlinesegments": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Visual Analysis of Satellite to Street Imagery",
    "title": "Automatic Alignment of Indoor and Outdoor Building Models Using 3D Line Segments",
    "authors": [
      "Tobias Koch",
      "Marco Korner",
      "Friedrich Fraundorfer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w19/html/Koch_Automatic_Alignment_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w19/papers/Koch_Automatic_Alignment_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper presents an approach for automatically aligning the non-overlapping interior and exterior parts of a 3D building model computed from image based 3D reconstructions. We propose a method to align the 3d reconstructions by identifying corresponding 3D structures that are part of the interior and exterior model (e.g. openings like windows).In this context, we point out the potential of using 3D line segments to enrich the information of point clouds generated by SfMs and show how this can be used for interpreting the scene and matching individual reconstructions."
  },
  "cvpr2016_w19_thetum-dlrmultimodalearthobservationevaluationbenchmark": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Visual Analysis of Satellite to Street Imagery",
    "title": "The TUM-DLR Multimodal Earth Observation Evaluation Benchmark",
    "authors": [
      "Tobias Koch",
      "Pablo d'Angelo",
      "Franz Kurz",
      "Friedrich Fraundorfer",
      "Peter Reinartz",
      "Marco Korner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w19/html/Koch_The_TUM-DLR_Multimodal_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w19/papers/Koch_The_TUM-DLR_Multimodal_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present a new dataset for development, benchmarking, and evaluation of remote sensing and earth observation approaches with special focus on converging perspectives. In order to provide data with different modalities, we observed the same scene using satellites, airplanes, unmanned aerial vehicles (UAV), and smartphones. The dataset is further complemented by ground-truth information and baseline results for different application scenarios.The provided data can be freely used by anybody interested in remote sensing and earth observation and will be continuously augmented and updated."
  },
  "cvpr2016_w20_pets2016datasetandchallenge": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "PETS 2016: Dataset and Challenge",
    "authors": [
      "Luis Patino",
      "Tom Cane",
      "Alain Vallee",
      "James Ferryman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Patino_PETS_2016_Dataset_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Patino_PETS_2016_Dataset_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper describes the datasets and computer vision challenges that form part of the PETS 2016 workshop. PETS 2016 addresses the application of on-board multi sensor surveillance for protection of mobile critical assets.The sensors (visible and thermal cameras) are mounted on the asset itself and surveillance is performed around the asset.Two datasets are provided: (1) a multi sensor dataset as used for the PETS2014 challenge which addresses protection of trucks (the ARENA Dataset);and (2) a new dataset - the IPATCH Dataset - addressing the application of multi sensor surveillance to protect a vessel at sea from piracy. The dataset specifically addresses several vision challenges set in the PETS 2016 workshop, andcorresponding to different steps in a video understanding system: Low-Level Video Analysis (object detection and tracking), Mid-Level Video Analysis ('simple' event detection: the behaviour recognition of a single actor) and High-Level Video Analysis ('complex' event detection: the behaviour and interaction recognition of several actors)."
  },
  "cvpr2016_w20_channelcodeddistributionfieldtrackingforthermalinfraredimagery": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "Channel Coded Distribution Field Tracking for Thermal Infrared Imagery",
    "authors": [
      "Amanda Berg",
      "Jorgen Ahlberg",
      "Michael Felsberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Berg_Channel_Coded_Distribution_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Berg_Channel_Coded_Distribution_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We address short-term, single-object tracking, a topic that is currently seeing fast progress for visual video, for the case of thermal infrared (TIR) imagery. The fast progress has been possible thanks to the development of new template-based tracking methods with online template updates, methods which have not been explored for TIR tracking. Instead, tracking methods used for TIR are often subject to a number of constraints, e.g., warm objects, low spatial resolution, and static camera. As TIR cameras become less noisy and get higher resolution these constraints are less relevant, and for emerging civilian applications, e.g., surveillance and automotive safety, new tracking methods are needed.Due to the special characteristics of TIR imagery, we argue that template-based trackers based on distribution fields should have an advantage over trackers based on spatial structure features. In this paper, we propose a template-based tracking method (ABCD) designed specifically for TIR and not being restricted by any of the constraints above. In order to avoid background contamination of the object template, we propose to exploit background information for the online template update and to adaptively select the object region used for tracking. Moreover, we propose a novel method for estimating object scale change. The proposed tracker is evaluated on the VOT-TIR2015 and VOT2015 datasets using the VOT evaluation toolkit and a comparison of relative ranking of all common participating trackers in the challenges is provided. Further, the proposed tracker, ABCD, and the VOT-TIR2015 winner SRDCFir are evaluated on maritime data. Experimental results show that the ABCD tracker performs particularly well on thermal infrared sequences. "
  },
  "cvpr2016_w20_saliency-baseddetectionformaritimeobjecttracking": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "Saliency-Based Detection for Maritime Object Tracking",
    "authors": [
      "Tom Cane",
      "James Ferryman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Cane_Saliency-Based_Detection_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Cane_Saliency-Based_Detection_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper presents a new method for object detection and tracking based on visual saliency as a way of mitigating against challenges present in maritime environments. Object detection is based on adaptive hysteresis thresholding of a saliency map generated with a modified version of the Boolean Map Saliency (BMS) approach. We show that the modification reduces false positives by suppressing detection of wakes and surface glint. Tracking is performed by matching detections frame to frame and smoothing trajectories with a Kalman filter. The proposed approach is evaluated on the PETS 2016 challenge dataset on detecting and tracking boats around a vessel at sea."
  },
  "cvpr2016_w20_robustvisualtrackingwithdeepconvolutionalneuralnetworkbasedobjectproposalsonpets": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "Robust Visual Tracking With Deep Convolutional Neural Network Based Object Proposals on PETS",
    "authors": [
      "Gao Zhu",
      "Fatih Porikli",
      "Hongdong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Zhu_Robust_Visual_Tracking_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Zhu_Robust_Visual_Tracking_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Object tracking has been widely used yet still a challenge for surveillance as the drastic size change, deformation and occlusion present. While it is hard to design such an online classifier that adapts to all those changes, in this paper, we employ an object proposal network to generate a small set of bounding box candidates. In a new frame, only these \"object-like\" candidates are necessary for the classifier to test, which excludes spurious false positives. We also use them to update and improve the discriminative power of the classifier as those proposals are likely to be the background distractions. The novelly proposed approach is robust to object deformation and size change as they are handled naturally during the object proposal stage. We evaluate it on the PETS 2016 dataset, comparing to state-of-the-art trackers."
  },
  "cvpr2016_w20_onlinemulti-objecttrackingbasedonhierarchicalassociationframework": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "Online Multi-Object Tracking Based on Hierarchical Association Framework",
    "authors": [
      "Jaeyong Ju",
      "Daehun Kim",
      "Bonhwa Ku",
      "David K. Han",
      "Hanseok Ko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Ju_Online_Multi-Object_Tracking_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Ju_Online_Multi-Object_Tracking_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Online multi-object tracking is one of the crucial tasks in time-critical computer vision applications. In this paper, the problem of online multi-object tracking in complex scenes from a single, static, un-calibrated camera is addressed. In complex scenes, it is still challenging due to frequent and prolonged occlusions, abrupt motion change of objects, unreliable detections, and so on. To handle these difficulties, this paper proposes a four-stage hierarchical association framework based on online tracking-by-detection strategy. For this framework, tracks and detections are divided into several groups depending on several cues obtained from association results with the proposed track confidence. In each association stage, different sets of tracks and detections are associated to handle the following problems simultaneously: track generation, progressive trajectory construction, track drift and fragmentation. The experimental results show the robustness and effectiveness of the proposed method compared with other state-of-the-art methods."
  },
  "cvpr2016_w20_semanticmodellingforbehaviourcharacterisationandthreatdetection": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "Semantic Modelling for Behaviour Characterisation and Threat Detection",
    "authors": [
      "Luis Patino",
      "James Ferryman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Patino_Semantic_Modelling_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Patino_Semantic_Modelling_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Threat detection in computer vision can be achieved by extraction of behavioural cues. To achieve recognition of such cues, we propose to work with Semantic Models of behaviours. Semantic Models correspond to the translation of Low-Level information (tracking information) into High-Level semantic description. The model is then similar to a naturally spoken description of the event.We have built semantic models for the behaviours and threats addressed in the PETS 2016 IPATCH dataset. Semantic models can trigger a threat alarm by themselves or give situation awareness. We describe in this paper how semantic models are built from Low-Level trajectory features and how they are recognised. The current results are promising."
  },
  "cvpr2016_w20_realtimeanomalydetectionusingtrajectory-levelcrowdbehaviorlearning": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "Realtime Anomaly Detection Using Trajectory-Level Crowd Behavior Learning",
    "authors": [
      "Aniket Bera",
      "Sujeong Kim",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Bera_Realtime_Anomaly_Detection_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Bera_Realtime_Anomaly_Detection_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present an interactive crowd behavior learning algorithm that can be used for analyzing crowd videos to detect anomalies in realtime for surveillance related applications. Our formulation combines online tracking algorithms from computer vision, non-linear pedestrian motion models from computer graphics, and machine learning techniques to automatically compute the trajectory-level pedestrian behaviors for each agent in the video. These learned behaviors are used to automatically perform motion segmentation to detect anomalous behaviors. We demonstrate the interactive performance using the PETS 2016 ARENA dataset and various indoor and outdoor crowd video benchmarks consisting of tens of human agents. "
  },
  "cvpr2016_w20_abnormaleventrecognitionahybridapproachusingsemanticweb": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Performance Evaluation of Tracking and Surveillance",
    "title": "Abnormal Event Recognition: A Hybrid Approach Using Semantic Web",
    "authors": [
      "Luca Greco",
      "Pierluigi Ritrovato",
      "Alessia Saggese",
      "Mario Vento"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/html/Greco_Abnormal_Event_Recognition_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w20/papers/Greco_Abnormal_Event_Recognition_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Video surveillance systems generated about 65% of the Universe Big Data in 2015.The development of systems for intelligent analysis of such a large amountof data is among the most investigated topics in the academia and commercial world.Recent outcomes in knowledge management and computationalintelligence demonstrate the effectiveness of semantic technologies in several fields like image and text analysis, hand writing and speech recognition.In this paper a solution that, starting from the outputof a people tracking algorithm, is able to recognize simpleevents (person falling to the ground) and complex ones (person aggression) is presented.The proposed solution uses semantic web technologies for automatically annotating the output produced by the tracking algorithm; a sets of rules for reasoning on these annotated data are also proposed. Such rules allow to define complex analytics functionsdemonstrating the effectiveness of hybrid approaches for event recognition."
  },
  "cvpr2016_w21_skeleton-baseddynamichandgesturerecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Observing and Understanding Hands in Action",
    "title": "Skeleton-Based Dynamic Hand Gesture Recognition",
    "authors": [
      "Quentin De Smedt",
      "Hazem Wannous",
      "Jean-Philippe Vandeborre"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/html/De_Smedt_Skeleton-Based_Dynamic_Hand_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/papers/De_Smedt_Skeleton-Based_Dynamic_Hand_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, a new skeleton-based approach is proposed for 3D hand gesture recognition. Specifically, we exploit the geometric shape of the hand to extract an effective descriptor from connected joints of the hand skeleton returned by the Intel RealSense depth camera. Each descriptor is then encoded by a Fisher Vector representation obtained using a Gaussian Mixture Model. A multi-level representation of Fisher Vectors and other skeleton-based geometric features is guaranteed by a temporal pyramid to obtain the final feature vector, used later to achieve the classification by a linear SVM classifier.The proposed approach is evaluated on a challenging hand gesture dataset containing 14 gestures, performed by 20 participants performing the same gesture with two different numbers of fingers. Experimental results show that our skeleton-based approach consistently achieves superior performance over a depth-based approach. "
  },
  "cvpr2016_w21_learningmarginalizationthroughregressionforhandorientationinference": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Observing and Understanding Hands in Action",
    "title": "Learning Marginalization Through Regression for Hand Orientation Inference",
    "authors": [
      "Muhammad Asad",
      "Gregory Slabaugh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/html/Asad_Learning_Marginalization_Through_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/papers/Asad_Learning_Marginalization_Through_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present a novel marginalization method for multi-layered Random Forest based hand orientation regression. The proposed model is composed of two layers, where the first layer consists of a marginalization weights regressor while the second layer contains expert regressors trained on subsets of our hand orientation dataset. We use a latent variable space to divide our dataset into subsets. Each expert regressor gives a posterior probability for assigning a given latent variable to the input data. Our main contribution comes from the regression based marginalization of these posterior probabilities. We use a Kullback-Leibler divergence based optimization for estimating the weights that are used to train our marginalization weights regressor. In comparison to the state-of-the-art of both hand orientation inference and multi-layered Random Forest marginalization, our proposed method proves to be more robust."
  },
  "cvpr2016_w21_hiddenhandstrackinghandswithanocclusionawaretracker": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Observing and Understanding Hands in Action",
    "title": "Hidden Hands: Tracking Hands With an Occlusion Aware Tracker",
    "authors": [
      "Akshay Rangesh",
      "Eshed Ohn-Bar",
      "Mohan M. Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/html/Rangesh_Hidden_Hands_Tracking_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/papers/Rangesh_Hidden_Hands_Tracking_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This work presents an occlusion aware hand tracker to reliably track both hands of a person using a monocular RGB camera. To demonstrate its robustness, we evaluate the tracker on a challenging, occlusion-ridden naturalistic driving dataset, where hand motions of a driver are to be captured reliably. The proposed framework additionally encodes and learns tracklets corresponding to complex (yet frequently occurring) hand interactions offline, and makes an informed choice during data association. This provides positional information of the left and right hands with no intrusion (through complete or partial occlusions) over long, unconstrained video sequences in an online manner. The tracks thus obtained may find use in domains such as human activity analysis, gesture recognition, and higher-level semantic categorization."
  },
  "cvpr2016_w21_effectivenessofgraspattributesandmotion-constraintsforfine-grainedrecognitionofobjectmanipulationactions": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Observing and Understanding Hands in Action",
    "title": "Effectiveness of Grasp Attributes and Motion-Constraints for Fine-Grained Recognition of Object Manipulation Actions",
    "authors": [
      "Kartik Gupta",
      "Darius Burschka",
      "Arnav Bhavsar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/html/Gupta_Effectiveness_of_Grasp_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w21/papers/Gupta_Effectiveness_of_Grasp_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this work, we consider the problem of recognition of object manipulation actions. This is a challenging task for real everyday actions, as the same object can be grasped and moved in different ways depending on its functions and geometric constraints of the task. We propose to leverage grasp and motion-constraints information, using a suitable representation, to recognize and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi-class voting scheme, and b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints. Our results clearly demonstrate the usefulness of grasp characteristics and motion-constraints, to understand actions intended with an object."
  },
  "cvpr2016_w23_theassignmentmanifoldasmoothmodelforimagelabeling": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "The Assignment Manifold: A Smooth Model for Image Labeling",
    "authors": [
      "Freddie Astrom",
      "Stefania Petra",
      "Bernhard Schmitzer",
      "Christoph Schnorr"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Astrom_The_Assignment_Manifold_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Astrom_The_Assignment_Manifold_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We introduce a novel geometric approach to the image labeling problem. A general objective function is defined on a manifold of stochastic matrices, whose elements assign prior data that are given in any metric space, to observed image measurements. The corresponding Riemannian gradient flow entails a set of replicator equations, one for each data point, that are spatially coupled by geometric averaging on the manifold. Starting from uniform assignments at the barycenter as natural initialization, the flow terminates at some global maximum, each of which corresponds to an image labeling that uniquely assigns the prior data. No tuning parameters are involved, except for two parameters setting the spatial scale of geometric averaging and scaling globally the numerical range of features, respectively. Our geometric variational approach can be implemented with sparse interior-point numerics in terms of parallel multiplicative updates that converge efficiently."
  },
  "cvpr2016_w23_astatisticalframeworkforelasticshapeanalysisofspatio-temporalevolutionsofplanarclosedcurves": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Statistical Framework for Elastic Shape Analysis of Spatio-Temporal Evolutions of Planar Closed Curves",
    "authors": [
      "Chafik Samir",
      "Sebastian Kurtek",
      "Justin Strait",
      "Shantanu H. Joshi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Samir_A_Statistical_Framework_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Samir_A_Statistical_Framework_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a new statistical framework for spatio-temporal modeling of elastic planar, closed curves. This approach combines two recent frameworks for elastic functional data analysis and elastic shape analysis. The proposed trajectory registration framework enables matching and averagingto quantify spatio-temporal deformations while taking into account their dynamic specificities. A key ingredient of this framework is atracking method that optimizes the evolution of curves extracted from sequences of consecutive images to estimate the spatial-temporal deformation fields.Automatic estimation of such spatio-temporal deformations (including spatial changes or strain and dynamic temporal changes or phase) was tested on several simulated examples and real myocardial trajectories. Experimental results showedsignificant improvements in the spatio-temporal structure of trajectory comparisons and averages using the proposed framework."
  },
  "cvpr2016_w23_testingstationarityofbrainfunctionalconnectivityusingchange-pointdetectioninfmridata": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Testing Stationarity of Brain Functional Connectivity Using Change-Point Detection in fMRI Data",
    "authors": [
      "Mengyu Dai",
      "Zhengwu Zhang",
      "Anuj Srivastava"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Dai_Testing_Stationarity_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Dai_Testing_Stationarity_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper studies two questions: (1)Does the functional connectivity (FC) in a human brain remain stationary during performance of a task? (2) If it is non-stationary, how can one evaluate and estimate dynamic FC?The framework presented here relies on pre-segmented brain regions to represent instantaneous FC as symmetric, positive-definite matrices (SPDMs), with entries denoting covariances of fMRI signals across regions.The time series of such SPDMs is tested for change point detection using two important ideas: (1) a convenient Riemannian structure on the space of SPDMs for calculating geodesic distances and sample statistics, and (2) a graph-based approach, for testing similarity of distributions, that uses pairwise distances and a minimal spanning tree. This hypothesis test results in a temporal segmentation of observation interval into parts with stationary connectivity and an estimation ofgraph displaying FC during each such interval. "
  },
  "cvpr2016_w23_partialmatchingsandgrowthmappedevolutionsinshapespaces": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Partial Matchings and Growth Mapped Evolutions in Shape Spaces",
    "authors": [
      "Irene Kaltenmark",
      "Alain Trouve"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Kaltenmark_Partial_Matchings_and_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Kaltenmark_Partial_Matchings_and_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The definition of shape spaces as homogeneous spaces under the action diffeomorphism equipped with a right invariant metric has been successful in providing theoretically sound and numerically efficient tools for registering and comparing shapes in the context of computational anatomy and leading to the so called diffeomorphometry. However, when considering not only shapes but shapes evolution or growth modelling through time, what could be the equivalent shape evolution spaces if any and what can be the natural group actions ? This paper proposes a principled framework in this direction on stratified shapes."
  },
  "cvpr2016_w23_humanobjectinteractionrecognitionusingrate-invariantshapeanalysisofinterjointdistancestrajectories": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Human Object Interaction Recognition Using Rate-Invariant Shape Analysis of Inter Joint Distances Trajectories",
    "authors": [
      "Meng Meng",
      "hassen Drira",
      "Mohamed Daoudi",
      "Jacques Boonaert"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Meng_Human_Object_Interaction_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Meng_Human_Object_Interaction_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Human action recognition has emerged as one of the most challenging and active areas of research in the computer vision domain. In addition to pose variation and scale variability, high complexity of human motions and the variability of object interactions represent additional significant challenges. In this paper, we present an approach for human-object interaction modeling and classification. Towards that goal, we adopt relevant frame-level features; the inter-joint distances and joint-object distances. These proposed features are efficiently insensitive to position and pose variation. The evolution of the these distances in time is modeled by trajectories anda shape analysis framework is used to model and compares the trajectories corresponding to human-object interaction in a Riemannian manifold. The experiments conducted following state-of-the-art settings and results demonstrate the strength of the proposed method. "
  },
  "cvpr2016_w23_riemanniangeometricapproachesformeasuringmovementquality": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Riemannian Geometric Approaches for Measuring Movement Quality",
    "authors": [
      "Anirudh Som",
      "Rushil Anirudh",
      "Qiao Wang",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Som_Riemannian_Geometric_Approaches_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Som_Riemannian_Geometric_Approaches_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " A growing set of applications in home-based interactive physical therapy require the ability to monitor, inform and assess the quality of everyday movements. Interactive therapy requires both real-time feedback of movement quality, as well as summative feedback of quality over a period of time. Obtaining labeled data from trained experts is the main limitation, since it is both expensive and time consuming. Motivated by recent studies in motor-control, we propose an unsupervised approach that measures movement quality of simple actions by considering the deviation of a trajectory from an ideal movement path in the configuration space. We use two different configuration spaces to demonstrate this idea - the product space S^1 x S^1 to model the interaction of two joint angles, and SE(3) x SE(3) to model the movement of two joints, for two different applications in movement quality estimation. We also describe potential applications of these ideas to assess quality in real-time."
  },
  "cvpr2016_w23_differentialgeometryboostsconvolutionalneuralnetworksforobjectdetection": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Differential Geometry Boosts Convolutional Neural Networks for Object Detection",
    "authors": [
      "Chu Wang",
      "Kaleem Siddiqi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Convolutional neural networks (CNNs) have had dramatic success in appearance based object recognition tasks such as the ImageNet visual recognition challenge. However, their application to object recognition and detection thus far has focused largely on appearance images as inputs. Motivated by demonstrations that depth can enhance the performance of CNN-based approaches, we consider the benefits of adding differential geometric shape features in a principled manner. This elementary idea of using zeroth order (depth), first-order (surface normal) and second-order (surface curvature) features boosts the performance of a CNN that has been pretrained on a color image database. In an object detection task involving 19 categories we improve on the current state-of-the-art detection accuracy on the NYUv2 dataset of 35.6% by Gupta et al. by 10.4% to a new result of 39.3%. Our results provide strong evidence that the abstraction of surface shape benefits object detection and recognition."
  },
  "cvpr2016_w23_ontime-seriestopologicaldataanalysisnewdataandopportunities": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "On Time-Series Topological Data Analysis: New Data and Opportunities",
    "authors": [
      "Lee M. Seversky",
      "Shelby Davis",
      "Matthew Berger"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Seversky_On_Time-Series_Topological_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Seversky_On_Time-Series_Topological_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This work introduces a new dataset and framework for the exploration of topological data analysis (TDA) techniques applied to time-series data. We examine the end-to-end TDA processing pipeline for persistent homology applied to time-delay embeddings of time series - embeddings that capture the underlying system dynamics from which time series data is acquired. In particular, we consider stability with respect to time series length, the approximation accuracy of sparse filtration methods, and the discriminating ability of persistence diagrams as a feature for learning. We explore these properties across a wide range of time-series datasets spanning multiple domains for single source multi-segment signals as well as multi-source single segment signals. We outline the TDA framework and rationale behind the dataset and provide insights into the role of TDA for time-series analysis as well as opportunities for new work."
  },
  "cvpr2016_w23_ariemannianframeworkforstatisticalanalysisoftopologicalpersistencediagrams": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Riemannian Framework for Statistical Analysis of Topological Persistence Diagrams",
    "authors": [
      "Rushil Anirudh",
      "Vinay Venkataraman",
      "Karthikeyan Natesan Ramamurthy",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Anirudh_A_Riemannian_Framework_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Anirudh_A_Riemannian_Framework_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Topological data analysis is a popular way to study high dimensional feature spaces without any contextual clues or assumptions. This paper concerns itself with one popular topological feature -- the number of d-dimensional holes in the dataset, also known as the Betti-d number. The persistence of these Betti numbers using persistence diagrams (PD). A common way to compare PDs is the n-Wasserstein metric. However, a big drawback of this approach is the need to solve correspondence before computing the distance. Instead, we propose to use an entirely new framework built on Riemannian geometry, that models PDs on a Hilbert Sphere. The resulting space is much more intuitive and the distance metric is correspondence-free thereby eliminating the bottleneck. It also enables the use of existing machinery in differential geometry towards statistical analysis of PDs such as computing the mean, geodesics etc. We report competitive results compared with the Wasserstein metric."
  },
  "cvpr2016_w23_asurveyonrotationoptimizationinstructurefrommotion": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Survey on Rotation Optimization in Structure From Motion",
    "authors": [
      "Roberto Tron",
      "Xiaowei Zhou",
      "Kostas Daniilidis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Tron_A_Survey_on_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Tron_A_Survey_on_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We consider the problem of robust rotation optimization in Structure from Motion applications. A number of different approaches have been recently proposed, with solutions that are at times incompatible, and at times complementary. The goal of this paper is to survey and compare these ideas in a unified manner, and to benchmark their robustness against the presence of outliers. In all, we have tested more than forty variants of a these methods (including novel ones), and we find the best performing combination. "
  },
  "cvpr2016_w23_bayesianmodel-basedautomaticlandmarkdetectionforplanarcurves": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Bayesian Model-Based Automatic Landmark Detection for Planar Curves",
    "authors": [
      "Justin Strait",
      "Sebastian Kurtek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Strait_Bayesian_Model-Based_Automatic_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Strait_Bayesian_Model-Based_Automatic_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Identifying landmarks, points of interest on a shape, is crucial for many statistical shape analysis applications. Landmark-based methods dominate early literature; more recently, a method combining continuous shape outlines with landmark constraints was proposed. Unfortunately, methods requiring landmark specification depend on the number selected and their locations; such annotations are tedious for large datasets and subject to human interpretation. This work provides a Bayesian model-based method for automatic landmark selection, based on good approximations of landmark set interpolations. We outline an appropriate prior and likelihood, allowing for efficient posterior inference on landmark locations. The model allows for location uncertainty quantification, an important inferential procedure for further analysis. A method for selecting an appropriate number of landmarks is also discussed. Applications include a simulated example, shapes from the MPEG-7 dataset, and mice vertebrae."
  },
  "cvpr2016_w23_consensus-basedimagesegmentationviatopologicalpersistence": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Consensus-Based Image Segmentation via Topological Persistence",
    "authors": [
      "Qian Ge",
      "Edgar Lobaton"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Ge_Consensus-Based_Image_Segmentation_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Ge_Consensus-Based_Image_Segmentation_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Image segmentation is one of the most important low-level operation in image processing and computer vision. It is unlikely for a single algorithm with a fixed set of parameters to segment various images successfully due to variations between images. However, it can be observed that the desired boundaries are often detected more consistently than other ones in the output of state-of-the-art algorithms. In this paper, we propose a new approach to capture the consensus information from a segmentation set obtained by different algorithms. The present probability of a segment curve is estimated based on our probabilistic segmentation model. A connectivity probability map is constructed and persistent segments are extracted by applying topological persistence to the map. Finally, a robust segmentation is obtained with the detection of certain segment curves guaranteed. The experiments demonstrate our approach is able to consistently capture the curves present within the segmentation set."
  },
  "cvpr2016_w23_robustdomainadaptationonthel1-grassmannianmanifold": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Robust Domain Adaptation on the L1-Grassmannian Manifold",
    "authors": [
      "Sriram Kumar",
      "Andreas Savakis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Kumar_Robust_Domain_Adaptation_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Kumar_Robust_Domain_Adaptation_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Domain adaptation aims to remedy the loss in classification performance that often occurs due to domain shifts between training and testing datasets. This problem is known as the dataset bias attributed to variations across datasets. Domain adaptation methods on Grassmann manifolds are among the most popular, including Geodesic Subspace Sampling and Geodesic Flow Kernel. Grassmann learning facilitates compact characterization by generating linear subspaces and representing them as points on the manifold. However, Grassmannian construction is based on PCA which is sensitive to outliers. This motivates us to find linear projections that are robust to noise, outliers, and dataset idiosyncrasies. Hence, we combine L1-PCA and Grassmann manifolds to perform robust domain adaptation. We present empirical results to validate improvements and robustness for domain adaptation in object class recognition across datasets."
  },
  "cvpr2016_w23_fastdynamicprogrammingforelasticregistrationofcurves": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Fast Dynamic Programming for Elastic Registration of Curves",
    "authors": [
      "Javier Bernal",
      "Gunay Dogan",
      "Charles R. Hagwood"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/html/Bernal_Fast_Dynamic_Programming_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w23/papers/Bernal_Fast_Dynamic_Programming_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Curve registration problems are ubiquitous in data analysis and computer vision. We propose a dynamic programming (DP) algorithm that runs in O(N) time to compute optimal diffeomorphisms for elastic registration of curves with N nodes. Our algorithm compares favorably with other DP algorithms for this problem: the commonly used DP with O(N^2) cost, and the original DP that guarantees a global optimality with O(N^4) cost. We achieve fast run times by reducing our search space, focusing on a strip around an estimate of the optimum, obtained with a multigrid approach: optimal solutions from lower resolutions are progressively projected to ones of higher resolution. Additionally, our algorithm is designed to handle nonuniformly discretized curves, enabling further savings in computations, because we can distribute the curve nodes adaptively and work with fewer nodes. We show its effectiveness on several shape analysis examples."
  },
  "cvpr2016_w24_fastimagegradientsusingbinaryfeatureconvolutions": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Fast Image Gradients Using Binary Feature Convolutions",
    "authors": [
      "Pierre-Luc St-Charles",
      "Guillaume-Alexandre Bilodeau",
      "Robert Bergevin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/St-Charles_Fast_Image_Gradients_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/St-Charles_Fast_Image_Gradients_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The recent increase in popularity of binary feature descriptors has opened the door to new lightweight computer vision applications. Most research efforts thus far have been dedicated to the introduction of new large-scale binary features, which are primarily used for keypoint description and matching. In this paper, we show that the side products of small-scale binary feature computations can efficiently filter images and estimate image gradients. The improved efficiency of low-level operations can be especially useful in time-constrained applications. Through our experiments, we show that efficient binary feature convolutions can be used to mimic various image processing operations, and even outperform Sobel gradient estimation in the edge detection problem, both in terms of speed and F-Measure."
  },
  "cvpr2016_w24_texturecomplexitybasedredundantregionsrankingforobjectproposal": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Texture Complexity Based Redundant Regions Ranking for Object Proposal",
    "authors": [
      "Wei Ke",
      "Tianliang Zhang",
      "Jie Chen",
      "Fang Wan",
      "Qixiang Ye",
      "Zhenjun Han"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Ke_Texture_Complexity_Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Ke_Texture_Complexity_Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Object proposal has been successfully applied in recent visual object detection approaches and shown improved computational efficiency. The purpose of object proposal is to use as few as regions to cover as many as objects. In this paper, we propose a strategy named Texture Complexity based Redundant Regions Ranking (TCR) for object proposal. Our approach first produces rich but redundant regions using a color segmentation approach, i.e. Selective Search. It then uses Texture Complexity (TC) based on complete contour number and Local Binary Pattern (LBP) entropy to measure the objectness score of each region. By ranking based on the TC, it is expected that as many as true object regions are preserved, while the number of the regions is significantly reduced. Experimental results on the PASCAL VOC 2007 dataset show that the proposed TCR significantly improves the baseline approach by increasing AUC (area under recall curve) from 0.39 to 0.48. "
  },
  "cvpr2016_w24_deeplyexploitdepthinformationforobjectdetection": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Deeply Exploit Depth Information for Object Detection",
    "authors": [
      "Saihui Hou",
      "Zilei Wang",
      "Feng Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Hou_Deeply_Exploit_Depth_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Hou_Deeply_Exploit_Depth_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper addresses the issue on how to more effectively coordinate the depth with RGB aiming at boosting the performance of RGB-D object detection. Particularly, we investigate two primary ideas under the CNN model: property derivation and property fusion. Firstly, we propose that the depth can be utilized not only as a type of extra information besides RGB but also to derive more visual properties for comprehensively describing the objects of interest. So a two-stage learning framework consisting of property derivation and fusion is constructed. Secondly, we explore the fusion method of different properties in feature learning, which is boiled down to, under the CNN model, from which layer the properties should be fused together. The analysis shows that different semantic properties should be learned separately and combined before passing into the final classifier. We evaluate the proposed method on the challenging dataset, and have achieved state-of-the-art performance."
  },
  "cvpr2016_w24_efficientdeepfeaturelearningandextractionviastochasticnets": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Efficient Deep Feature Learning and Extraction via StochasticNets",
    "authors": [
      "Mohammad Javad Shafiee",
      "Parthipan Siva",
      "Paul Fieguth",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Shafiee_Efficient_Deep_Feature_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Shafiee_Efficient_Deep_Feature_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Deep neural networks are a powerful tool for feature learning and extraction. One area worth exploring in feature extraction using deep neural networks is efficient neural connectivity formation for faster feature learning and extraction. Motivated by findings of stochastic synaptic connectivity formation in the brain as well as the brain's uncanny ability to efficiently represent information, we propose the efficient learning and extraction of features via StochasticNets, where sparsely-connected deep neural networks can be formed via stochastic connectivity between neurons.Experimental results show that features learned using deep convolutional StochasticNets, with fewer neural connections than conventional deep convolutional neural networks, can allow for better or comparable classification accuracy than conventional deep neural networks.Finally, it was also shown that significant gains in feature extraction speed can be achieved in embedded applications using StochasticNets."
  },
  "cvpr2016_w24_embeddingsequentialinformationintospatiotemporalfeaturesforactionrecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Embedding Sequential Information Into Spatiotemporal Features for Action Recognition",
    "authors": [
      "Yuancheng Ye",
      "YingLi Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Ye_Embedding_Sequential_Information_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Ye_Embedding_Sequential_Information_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we introduce a novel framework for video-based action recognition, In this paper, we introduce a novel framework for video-based action recognition, which incorporates the sequential information with the spatiotemporal features. Specifically, the spatiotemporal features are extracted from the sliced clips of videos, and then a recurrent neural network is applied to embed the sequential information into the final feature representation of the video. In contrast to most current deep learning methods for the video-based tasks, our framework incorporates both long-term dependencies and spatiotemporal information of the clips in the video. To extract the spatiotemporal features from the clips, both dense trajectories (DT) and a newly proposed 3D neural network, C3D, are applied in our experiments. Our proposed framework is evaluated on the benchmark datasets of UCF101 and HMDB51, and achieves comparable performance compared with the state-of-the-art results."
  },
  "cvpr2016_w24_learningdiscriminativefeatureswithclassencoder": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Learning Discriminative Features With Class Encoder",
    "authors": [
      "Hailin Shi",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Shengcai Liao",
      "Stan Z. Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Shi_Learning_Discriminative_Features_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Shi_Learning_Discriminative_Features_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Deep neural networks usually benefit from unsupervised pre-training, e.g. auto-encoders. However, the classifier further needs supervised fine-tuning methods for good discrimination. In this paper, we incorporate the supervised information to propose a novel formulation, namely class-encoder, whose training objective is to reconstruct a sample from another one of which the labels are identical. Class-encoder aims to minimize the intra-class variations in the feature space, and to learn a good discriminative manifolds on a class scale. We impose the class-encoder as a constraint into the softmax for better supervised training, and extend the reconstruction on feature-level to tackle the parameter size issue and translation issue. The experiments show that the class-encoder helps to improve the performance on benchmarks of classification and face recognition. This could also be a promising direction for fast training of face recognition models."
  },
  "cvpr2016_w24_doweneedbinaryfeaturesfor3dreconstruction?": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Do We Need Binary Features for 3D Reconstruction?",
    "authors": [
      "Bin Fan",
      "Qingqun Kong",
      "Wei Sui",
      "Zhiheng Wang",
      "Xinchao Wang",
      "Shiming Xiang",
      "Chunhong Pan",
      "Pascal Fua"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Fan_Do_We_Need_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Fan_Do_We_Need_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Binary features have been incrementally popular in the past few years due to their low memory footprints and the efficient computation of Hamming distance between binary descriptors. They have been shown with promising results on some real time applications, e.g., SLAM, where the matching operations are relative few. However, in computer vision, there are many applications such as 3D reconstruction requiring lots of matching operations between local features. Is the binary feature still a promising solution to this kind of applications? To get the answer, this paper conducts a comparative study of binary features and their matching methods on the context of 3D reconstruction in a recently proposed large scale mutliview stereo dataset. Our evaluations reveal that not all binary features are capable of this task. Most of them are inferior to the classical SIFT based method in terms of reconstruction accuracy and completeness with a not significant better computational performance."
  },
  "cvpr2016_w24_deepfeaturesornottemperatureandtimepredictioninoutdoorscenes": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Deep Features or Not: Temperature and Time Prediction in Outdoor Scenes",
    "authors": [
      "Anna Volokitin",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Volokitin_Deep_Features_or_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Volokitin_Deep_Features_or_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we study the effectiveness of features from CNNs for predicting the ambient temperature as well as the time of the year in an outdoor scene. We follow the benchmark provided by Glasner etal. one of whose findings was that simple hand-crafted features are better than the deep features (from fully connected layers) for temperature prediction. As in their work, we use the VGG16 architecture for our CNNs, pretrained for classification on ImageNet. Our findings on the temperature prediction task are as follows. (i)The pooling layers provide better features than the fully connected layers. (ii)The quality of the features improves little with finetuning of the CNN. (iii)Our best setup significantly improves over the results from Glasner et al. showing that the deep features are successful in turning a camera into a crude temperature sensor. Moreover, we validate our findings also for time prediction and achieve accurate season, month, week, time of the day, and hour prediction."
  },
  "cvpr2016_w24_euclideanandhammingembeddingforimagepatchdescriptionwithconvolutionalnetworks": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Euclidean and Hamming Embedding for Image Patch Description With Convolutional Networks",
    "authors": [
      "Zishun Liu",
      "Zhenxi Li",
      "Juyong Zhang",
      "Ligang Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Liu_Euclidean_and_Hamming_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Liu_Euclidean_and_Hamming_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Local feature descriptors represent image patches as floating-point or binary arrays for computer vision tasks. In this paper, we propose to train Euclidean and Hamming embedding for image patch description with triplet convolutional networks. Thanks to the learning ability of deep ConvNets, the trained local feature generation method, which is called Deeply Learned Feature Transform (DELFT), has good distinctiveness and robustness. Evaluated on the UBC benchmark, we get the state-of-the-art results using floating-point and binary features. Also, the learned features can cooperate with existing nearest neighbor search algorithms in Euclidean and Hamming space. In addition, a new benchmark is constructed to facilitate future related research, which contains 40 million image patches, corresponding to 6.7 million 3D points, being 25 times larger than existing dataset. The distinctiveness and robustness of the proposed method are demonstrated in the experimental results."
  },
  "cvpr2016_w24_robust2dpcaanditsapplication": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Robust 2DPCA and Its Application",
    "authors": [
      "Qianqian Wang",
      "Quanxue Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Wang_Robust_2DPCA_and_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Wang_Robust_2DPCA_and_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Two-dimensional Principal Component Analysis (2DPCA) has been widely used for face image representation and recognition. However, 2DPCA, which is based on F-norm square, is sensitive to the presence of outliers. To enhance the robustness of 2DPCA model, we proposed a novel Robust 2DPCA objective function, called R-2DPCA. The criterion of R-2DPCA is maximizing the covariance of data in the projected subspace, while minimizing the reconstruction error of data. In addition, we use the efficient non-greedy optimization algorithms solving our objective function. Extensive experiments are done on the AR, CMU-PIE, Extended Yale B face image databases, and results illustrate that our method is more effective and robust than other robust 2DPCA algorithms, such as L1-2DPCA, L1-2DPCA-S, and N-2DPCA."
  },
  "cvpr2016_w24_backgroundsubtractionusinglocalsvdbinarypattern": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Background Subtraction Using Local SVD Binary Pattern",
    "authors": [
      "Lili Guo",
      "Dan Xu",
      "Zhenping Qiang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Guo_Background_Subtraction_Using_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Guo_Background_Subtraction_Using_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Background subtraction is a basic problem for change detection in videos and also the first step of high-level computer vision applications. Most background subtraction methods rely on color and texture feature. However, due to illuminations changes in different scenes and affections of noise pixels, those methods often resulted in high false positives in a complex environment. To solve this problem, we propose an adaptive background subtraction model which uses a novel Local SVD Binary Pattern (named LSBP) feature instead of simply depending on color intensity. This feature can describe the potential structure of the local regions in a given image, thus, it can enhance the robustness to illumination variation, noise, and shadows. We use a sample consensus model which is well suited for our LSBP feature. Experimental results on CDnet 2012 dataset demonstrate that our background subtraction method using LSBP feature is more effective than many state-of-the-art methods. "
  },
  "cvpr2016_w24_generatingdiscriminativeobjectproposalsviasubmodularranking": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Generating Discriminative Object Proposals via Submodular Ranking",
    "authors": [
      "Yangmuzi Zhang",
      "Zhuolin Jiang",
      "Xi Chen",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Zhang_Generating_Discriminative_Object_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Zhang_Generating_Discriminative_Object_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " A multi-scale greedy-based object proposal generation approach is presented. Our approach is built on top of a hierarchical segmentation. Object proposals are obtained by selecting a subset from the multi-scale segment pool via maximizing a submodular objective function, which consists of a weighted coverage term, a single-scale diversity term and a multi-scale reward term. The weighted coverage term forces the selected set of object proposals to be representative; single-scale diversity term encourages choosing segments from different exemplar clusters so that they will cover as many object patterns as possible, multi-scale reward term encourages the proposals to be discriminative and selected from multiple layers generated by the hierarchical image segmentation. The experimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012 segmentation dataset demonstrate the accuracy and efficiency of our object proposal model. "
  },
  "cvpr2016_w24_improvinggradienthistogrambaseddescriptorsforpedestriandetectionindatasetswithlargevariations": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Improving Gradient Histogram Based Descriptors for Pedestrian Detection in Datasets With Large Variations",
    "authors": [
      "Prashanth Balasubramanian",
      "Sarthak Pathak",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Balasubramanian_Improving_Gradient_Histogram_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Balasubramanian_Improving_Gradient_Histogram_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Gradient histogram based descriptors, that are constructed using the gradient magnitudes as votes to orientation bins, are successfully used for Pedestrian Detection. However, their performance is hampered when presented with datasets having many variations in properties such as appearance, texture, scale, background, and object pose.Such variations can be reduced by smoothing the images.But, the performance of the descriptors, and their classifiers is affected negatively by this, due to the loss of important gradients along with the noisy ones.In this work, we show that the ranks of gradient magnitudes stay resilient to such a smoothing. We show that a combination of image smoothing and the ranks of gradient magnitudes yields good detection performances, especially when the variations in a dataset are large or the number of training samples is less. Experiments on the challenging Caltech and Daimler Pedestrian datasets, and the Inria Person dataset illustrate these findings."
  },
  "cvpr2016_w24_unsupervisedrobustfeature-basedpartitionensemblingtodiscovercategories": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "Unsupervised Robust Feature-Based Partition Ensembling to Discover Categories",
    "authors": [
      "Roberto J. Lopez-Sastre"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Lopez-Sastre_Unsupervised_Robust_Feature-Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Lopez-Sastre_Unsupervised_Robust_Feature-Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " The design of novel robust image descriptors is still a formidable problem. Different features, with different capabilities, are introduced every year. However, to explore how to combine them is also a fundamental task. This paper proposes two novel strategies for aggregating different feature-based image partitions to tackle the challenging problem of discovering objects in unlabeled image collections. Inspired by consensus clustering models, we introduce the Aggregated Partition (AP) approach, which, starting from a set of weak input partitions, builds a final partition where the disagreements with the input partitions are optimized. We then generalize the AP formulation and derive the Selective AP, which automatically identifies the subset of features and partitions that further improves the precision of the final partition. Experiments on three challenging datasets show how our methods are able to consistently outperform competing methods, reporting state-of-the-art results."
  },
  "cvpr2016_w24_thebestofbothworldscombiningdata-independentanddata-drivenapproachesforactionrecognition": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Robust Features for Computer Vision",
    "title": "The Best of Both Worlds: Combining Data-Independent and Data-Driven Approaches for Action Recognition",
    "authors": [
      "Zhenzhong Lan",
      "Shoou-I Yu",
      "Dezhong Yao",
      "Ming Lin",
      "Bhiksha Raj",
      "Alexander Hauptmann"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/html/Lan_The_Best_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w24/papers/Lan_The_Best_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Motivated by the success of CNNs in object recognition on images, researchers are striving to develop CNN equivalents for learning video features. However, learning video features globally has proven to be quite a challenge due to the difficulty of getting enough labels, processing large-scale video data, and representing motion information. Therefore, we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system. Our contribution is three-fold. First, we explicitly show that local handcrafted features and CNNs share the same convolution-pooling network structure. Second, we propose to use independent subspace analysis (ISA) to learn descriptors for state-of-the-art handcrafted features. Third, we enhance ISA with two new improvements, which make our learned descriptors significantly outperform the handcrafted ones. Experimental results on standard action recognition benchmarks show competitive performance. "
  },
  "cvpr2016_w25_analyzingwheelsofvehiclesinmotionusinglaserscanning": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Automatic Traffic Surveillance",
    "title": "Analyzing Wheels of Vehicles in Motion Using Laser Scanning",
    "authors": [
      "Andreas Mogelmose",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/html/Mogelmose_Analyzing_Wheels_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/papers/Mogelmose_Analyzing_Wheels_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we discuss a lidar-based wheel-width measuring system. Trucks are used for an enormous part of day to day freight delivery across the world, and cause significant wear to the road they use. Road planners and construction engineers rely on traffic statistics to properly design roads. Currently, no system is able to provide them with numbers on the wheel-widths of the vehicles using a particular stretch of road.We present a system which uses a horizontal lidar measuring in a plane close and nearly parallel to the road surface. Input from this is used to detect and analyze tires. A vertical lidar detects passing vehicles so individual tires can be combined into full vehicle models. The system detects 58% of passing vehicles, but correctly counts the number of axles on 85% of detected vehicles. More than 90% of of the axles are correctly classified according to the number of mounted tired (single or dual)."
  },
  "cvpr2016_w25_fastclassificationofemptyandoccupiedparkingspacesusingintegralchannelfeatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Automatic Traffic Surveillance",
    "title": "Fast Classification of Empty and Occupied Parking Spaces Using Integral Channel Features",
    "authors": [
      "Martin Ahrnbom",
      "Kalle Astrom",
      "Mikael Nilsson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/html/Ahrnbom_Fast_Classification_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/papers/Ahrnbom_Fast_Classification_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper we present a novel, fast and accurate system for detecting the presence of cars in parking lots. The system is based on fast integral channel features and machine learning. The methods are well suited for running embedded on low performance platforms. The methods are tested on a database of nearly 700,000 images of parking spaces, where 48.5% are occupied and the rest are free. The experimental evaluation shows improved robustness in comparison to the baseline methods for the dataset."
  },
  "cvpr2016_w25_thecountingapp,orhowtocountvehiclesin500hoursofvideo": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Automatic Traffic Surveillance",
    "title": "The CountingApp, or How to Count Vehicles in 500 Hours of Video",
    "authors": [
      "Adrien Lessard",
      "Francois Belisle",
      "Guillaume-Alexandre Bilodeau",
      "Nicolas Saunier"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/html/Lessard_The_CountingApp_or_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/papers/Lessard_The_CountingApp_or_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper proposes a new method for counting vehicles based on video tracking. The process consists of two main steps: tracking vehicles and processing the output with minimal user input, separating the vehicle positions into sets of trajectories, which correspond to the paths drivers can take. The method allows to rapidly analyze videos from road sections and intersections, and yields detailed results. A large dataset of five hundred hours of traffic videos were processed using this method and the results are promising as mean absolute percentage error (MAPE) can get as low as 14% depending on the conditions and the quality of the video capture. This paper also discusses the factors that affect counting performance and how to improve counting accuracy. "
  },
  "cvpr2016_w25_vehiclere-identificationforautomaticvideotrafficsurveillance": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Automatic Traffic Surveillance",
    "title": "Vehicle Re-Identification for Automatic Video Traffic Surveillance",
    "authors": [
      "Dominik Zapletal",
      "Adam Herout"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/html/Zapletal_Vehicle_Re-Identification_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/papers/Zapletal_Vehicle_Re-Identification_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper proposes an approach to the vehicle re-identification problem in a multiple camera system.We focused on the re-identification itself assuming that the vehicle detection problem is already solved including extraction of a full-fledged 3D bounding box. The re-identification problem is solved by using color histograms and histograms of oriented gradients by a linear regressor.The features are used in separate models in order to get the best results in the shortest CPU computation time. The proposed method works with a high accuracy (60% true positives retrieved with 10% false positive rate on a challenging subset of the test data) in 85 milliseconds of the CPU (Core i7) computation time per one vehicle re-identification assuming the fullHD resolution video input. The applications of this work include finding important parameters such as travel time, traffic flow, or traffic information in a distributed traffic surveillance and monitoring system."
  },
  "cvpr2016_w25_semanticdepthmapfusionformovingvehicledetectioninaerialvideo": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Automatic Traffic Surveillance",
    "title": "Semantic Depth Map Fusion for Moving Vehicle Detection in Aerial Video",
    "authors": [
      "Mahdieh Poostchi",
      "Hadi Aliakbarpour",
      "Raphael Viguier",
      "Filiz Bunyak",
      "Kannappan Palaniappan",
      "Guna Seetharaman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/html/Poostchi_Semantic_Depth_Map_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/papers/Poostchi_Semantic_Depth_Map_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Automatic moving object detection and segmentation is one of the fundamental low-level tasks for many of the urban traffic surveillance applications. We develop an automatic moving vehicle detection system for aerial video based on semantic fusion of trace of the flux tensor and tall structures altitude mask. Trace of the flux tensor provides spatio-temporal information of moving edges including undesirable motion of tall structures caused by parallax effects. The parallax induced motions are filtered out by incorporating buildings altitude masks obtained from available dense 3D point clouds. Using a level-set based geodesic active contours framework, the coarse thresholded building depth masks evolved into the actual building boundaries. Experiments are carried out on a cropped 2kx2k region of interest for 200 frames from Albuquerque urban aerial imagery. An average precision of 83% and recall of 76% have been reported using an object-level detection performance evaluation method."
  },
  "cvpr2016_w25_towardssemanticunderstandingofsurroundingvehicularmaneuversapanoramicvision-basedframeworkforreal-worldhighwaystudies": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Automatic Traffic Surveillance",
    "title": "Towards Semantic Understanding of Surrounding Vehicular Maneuvers: A Panoramic Vision-Based Framework for Real-World Highway Studies",
    "authors": [
      "Miklas S. Kristoffersen",
      "Jacob V. Dueholm",
      "Ravi K. Satzoda",
      "Mohan M. Trivedi",
      "Andreas Mogelmose",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/html/Kristoffersen_Towards_Semantic_Understanding_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w25/papers/Kristoffersen_Towards_Semantic_Understanding_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " This paper proposes the use of multiple low-cost visual sensors to obtain a surround view of the ego-vehicle for semantic understanding. A multi-perspective view will assist the analysis of naturalistic driving studies (NDS), by automating the task of data reduction of the observed sequences into events. A user-centric vision-based framework is presented using a vehicle detector and tracker in each separate perspective. Multi-perspective trajectories are estimated and analyzed to extract 14 different events, including potential dangerous behaviors such as overtakes and cut-ins. The system is tested on ten sequences of real-world data collected on U.S. highways. The results show the potential use of multiple low-cost visual sensors for semantic understanding around the ego-vehicle."
  },
  "cvpr2016_w27_spatiallyawaredictionarylearningandcodingforfossilpollenidentification": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Spatially Aware Dictionary Learning and Coding for Fossil Pollen Identification",
    "authors": [
      "Shu Kong",
      "Surangi Punyasena",
      "Charless Fowlkes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Kong_Spatially_Aware_Dictionary_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Kong_Spatially_Aware_Dictionary_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We propose a robust approach for performing automatic species-level recognition of fossil pollen grains in microscopy images that exploits both global shape and local texture characteristics in a patch-based matching methodology. We introduce a novel criteria for selecting meaningful and discriminative exemplar patches.We optimize this function during training using a greedy submodular function optimization framework that gives a near-optimal solution with bounded approximation error.We use these selected exemplars as a dictionary basis and propose a spatially-aware sparse coding method to match testing images for identification while maintaining global shape correspondence. To accelerate the coding process for fast matching, we introduce a relaxed form that uses spatially-aware soft-thresholding during coding. Finally, we carry out an experimental study that demonstrates the effectiveness and efficiency of our exemplar selection and classification mechanisms, achieving 86.13% accuracy on a difficult fine-grained species classification task distinguishing three types of fossil spruce pollen."
  },
  "cvpr2016_w27_multi-viewmulti-modalfeatureembeddingforendomicroscopymosaicclassification": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Multi-View Multi-Modal Feature Embedding for Endomicroscopy Mosaic Classification",
    "authors": [
      "Yun Gu",
      "Jie Yang",
      "Guang-Zhong Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Gu_Multi-View_Multi-Modal_Feature_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Gu_Multi-View_Multi-Modal_Feature_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Probe-based confocal laser endomicroscopy (pCLE) is an emerging tool for epithelial cancer diagnosis, which enables in vivo microscopic imaging during endoscopic procedures. As a new technique, definite clinical diagnosis is still referenced to the gold standard histology images. In this paper, we propose a Multi-View Multi-Modal Embedding framework (MVMME) to learn representative features for pCLE videos exploiting both pCLE mosaic and histology images.Each pCLE mosaic is represented by multiple feature representations including SIFT, Texton and HoG. A latent space is discovered by embedding the visual features from both mosaics and histology images in a supervised scheme. The features extracted from the latent spaces can make use of multi-modal imaging sources that are more discriminative than unimodal features from mosaics alone. The experiments based on real pCLE datasets demonstrate that our approach outperforms, with statistical significance, several single-view or single-modal methods. A binary classification accuracy of 96% has been achieved."
  },
  "cvpr2016_w27_neuronsegmentationbasedoncnnwithsemi-supervisedregularization": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Neuron Segmentation Based on CNN With Semi-Supervised Regularization",
    "authors": [
      "Kun Xu",
      "Hang Su",
      "Jun Zhu",
      "Ji-Song Guan",
      "Bo Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Xu_Neuron_Segmentation_Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Xu_Neuron_Segmentation_Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Neuron segmentation in two-photon microscopy images is a critical step to investigate neural network activities in vivo. However, it still remains as a challenging problem due to the image qualities, which largely results from the non-linear imaging mechanism and 3D imaging diffusion. To address these issues, we proposed a novel framework by incorporating the convolutional neural network (CNN) with a semi-supervised regularization term, which reduces the human efforts in labeling without sacrificing the performance. Specifically, we generate a putative label for each unlabel sample regularized with a graph-smooth term, which are used as if they were true labels. A CNN model is therefore trained in a supervised fashion with labeled and unlabeled data simultaneously, which is used to detect neuron regions in 2D images. Afterwards, neuron segmentation in a 3D volume is conducted by associating the corresponding neuron regions in each image. Experiments on real-world datasets demonstrate that our approach outperforms neuron segmentation based on the graph-based semi-supervised learning, the supervised CNN and variants of the semi-supervised CNN."
  },
  "cvpr2016_w27_3dstructuremodelingofdensecapillariesbymulti-objectstracking": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "3D Structure Modeling of Dense Capillaries by Multi-Objects Tracking",
    "authors": [
      "Ryoma Bise",
      "Imari Sato",
      "Kentaro Kajiya",
      "Toyonobu Yamashita"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Bise_3D_Structure_Modeling_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Bise_3D_Structure_Modeling_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " A newly developed imaging technique called light-sheet laser microscopy imaging can visualize the detailed 3D structures of capillaries. Capillaries form complicated network structures in the obtained data, and this makes it difficult to model vessel structures by existing methods that implicitly assume simple tree structures for blood vessels. To cope with such dense capillaries with network structures, we propose to track the flow of blood vessels along a base-axis using a multiple-object tracking framework. We first track multiple blood vessels in cross-sectional images along a single axis to make the trajectories of blood vessels, and then connect these blood vessels to reveal their entire structures. This framework is efficient to track densely distributed vessels since it uses only a single cross-sectional plane. The network structure is then generated in the post-processing by connecting blood vessels on the basis of orientations of the trajectories. The results of experiments using a challenging real data-set demonstrate the efficacy of the proposed method, which are capable of modeling dense capillaries."
  },
  "cvpr2016_w27_analysingthestructureofcollagenfibresinsbfsemimages": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Analysing the Structure of Collagen Fibres in SBFSEM Images",
    "authors": [
      "Yassar Almutairi",
      "Timothy Cootes",
      "Karl Kadler"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Almutairi_Analysing_the_Structure_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Almutairi_Analysing_the_Structure_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Collagen fibres form important structures in tissue, and are essential for force transmission, scaffolding and cell addition. Each fibre is long and thin, and large numbers group together into complex networks of bundles, which are little studied as yet.Serial block-face scanning electron microscopy (SBFSEM) can be used to image tissues containing the fibres, but analysing the images manually is almost impossible - there can be over 30,000 fibres in each image slice, and many hundreds of individual image slices in a volume. We describe a system for automatically identifying and tracking the individual fibres, allowing analysis of their paths, how they form bundles and how individual fibres weave from one bundle to another."
  },
  "cvpr2016_w27_clustersensingsuperpixelandgrouping": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Cluster Sensing Superpixel and Grouping",
    "authors": [
      "Rui Li",
      "Lu Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Li_Cluster_Sensing_Superpixel_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Li_Cluster_Sensing_Superpixel_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Superpixel algorithms have shown significant potential in computer vision applications since they can be used to accelerate other computationally demanding algorithms.However, in contrast to the original purpose of superpixels, many upper layer methods still suffer from computational problems when incorporating superpixel for speedup. In this paper, we present a cluster sensing superpixel (CSS) method to efficiently generate superpixel bricks. Based on the insight of pixel density, cluster centers generally have properties of representativeness (i.e., local maximal pixel density) and isolation (i.e., large distance from other cluster centers). Our CSS method efficiently identifies ideal cluster centers via utilizing pixel density. We also integrate superpixel cues into a bipartite graph segmentation framework and apply it to microscopy image segmentation. Extensive experiments show that our CSS method achieves impressive efficiency, being approximately five times faster than the state-of-the-art methods and having comparable performance in terms of the standard metrics. Application on microscopy image segmentation also benefits our efficient implementation."
  },
  "cvpr2016_w27_3dconvolutionalnetworks-basedmitoticeventdetectionintime-lapsephasecontrastmicroscopyimagesequencesofstemcellpopulations": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "3D Convolutional Networks-Based Mitotic Event Detection in Time-Lapse Phase Contrast Microscopy Image Sequences of Stem Cell Populations",
    "authors": [
      "Wei-Zhi Nie",
      "Wei-Hui Li",
      "An-An Liu",
      "Tong Hao",
      "Yu-Ting Su"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Nie_3D_Convolutional_Networks-Based_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Nie_3D_Convolutional_Networks-Based_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " In this paper, we propose a straightforward and effective method for mitotic event detection in time-lapse phase contrast microscopy image sequences of stem cell populations. Different from most of recent methods leveraging temporal modeling to learn the latent dynamics within one mitotic event, we mainly target on the data-driven spatio-temporal visual feature learning for mitotic event representation to bypass the difficulties in both robust hand-crafted feature designing and complicated temporal dynamic learning. Specially, we design the architecture of the convolutional neural networks with 3D filters to extract the holistic feature of the volumetric region where individual mitosis event occurs. Then, the extracted features can be directly feeded into the off-the-shelf classifiers for model learning or inference. Moreover, we prepare a novel and challenging dataset for mitosis detection. The comparison experiments demonstrate the superiority of the proposed method."
  },
  "cvpr2016_w27_segmentationofoverlappingcervicalcellsinmicroscopicimageswithsuperpixelpartitioningandcell-wisecontourrefinement": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Segmentation of Overlapping Cervical Cells in Microscopic Images With Superpixel Partitioning and Cell-Wise Contour Refinement",
    "authors": [
      "Hansang Lee",
      "Junmo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Lee_Segmentation_of_Overlapping_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Lee_Segmentation_of_Overlapping_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Segmentation of cervical cells in microscopic images is an important task for computer-aided diagnosis of cervical cancer. However, their segmentation is challenging due to inhomogeneous cell cytoplasm and the overlap between the cells. In this paper, we propose an automatic segmentation method for multiple overlapping cervical cells in microscopic images using superpixel partitioning and cell-wise contour refinement. First, the cell masses are detected by superpixel generation and triangle thresholding. Then, nuclei of cells are extracted by local thresholding and outlier removal. Finally, cell cytoplasm is initially segmented by superpixel partitioning and refined by cell-wise contour refinement with graph cuts. In experiments, our method showed competitive performances in two public challenge data sets compared to the state-of-the-art methods."
  },
  "cvpr2016_w27_unsupervisedsegmentationofcervicalcellimagesusinggaussianmixturemodel": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Unsupervised Segmentation of Cervical Cell Images Using Gaussian Mixture Model",
    "authors": [
      "Srikanth Ragothaman",
      "Sridharakumar Narasimhan",
      "Madivala G. Basavaraj",
      "Rajan Dewar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Ragothaman_Unsupervised_Segmentation_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Ragothaman_Unsupervised_Segmentation_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Cervical cancer is one of the leading causes of cancer death in women. Screening at early stages using the popular Pap smear test has been demonstrated to reduce fatalities significantly.Cost effective, automated screening methods cansignificantly improve the adoption of these tests worldwide. Automated screening involves image analysis of cervical cells.Gaussian Mixture Models (GMM) are widely used in image processing forsegmentation which isa crucial step in image analysis.In our proposed method, GMM is implemented to segment cell regions to identify cellular features such as nucleus, cytoplasm while addressing shortcomings of existing methods. This method is combined with shape based identification of nucleus to increase the accuracy of nucleus segmentation. This enables the algorithm to accurately trace the cells and nucleus contours from the pap smear images that contain cell clusters. The method also accounts for inconsistent staining, if any. The results that are presented shows that our proposed method performs well even in challenging conditions."
  },
  "cvpr2016_w27_icordintelligentcollectionofredundantdata-adynamicsystemforcrowdsourcingcellsegmentationsaccuratelyandefficiently": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "ICORD: Intelligent Collection of Redundant Data - A Dynamic System for Crowdsourcing Cell Segmentations Accurately and Efficiently",
    "authors": [
      "Sameki Mehrnoosh",
      "Danna Gurari",
      "Margrit Betke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Mehrnoosh_ICORD_Intelligent_Collection_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Mehrnoosh_ICORD_Intelligent_Collection_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Segmentation is a fundamental step in analyzing biological structures in microscopy images. When state-of-the-art automated methods are found to produce inaccurate boundaries, interactive segmentation can be effective. Since the inclusion of domain experts is typically expensive and does not scale, crowdsourcing has been considered.Due to concerns about the quality of crowd work, quality control methods that rely on a fixed number of redundant annotations have been used. We here introduce a collection strategy that dynamically assesses the quality of crowd work. We propose ICORD (Intelligent Collection Of Redundant annotation Data), a system that predicts the accuracy of a segmented region from analysis of (1) its geometric and intensity-based features and (2) the crowd worker's behavioral features. Based on this score, ICORD dynamically determines if the annotation accuracy is satisfactory or if a higher-quality annotation should be sought out in another round of crowdsourcing. We tested ICORD on phase contrast and fluorescence images of 270 cells. We compared the performance of ICORD and a popular baseline method for which we aggregated 1,350 crowd-drawn cell segmentations. Our results show that ICORD collects annotations both accurately and efficiently. Accuracy levels are within 3 percentage points of those of the baseline. More importantly, due to its dynamic nature, ICORD vastly outperforms the baseline method with respect to efficiency. ICORD only uses between 27% and 50% of the resources, i.e., collection time and cost, that the baseline method requires."
  },
  "cvpr2016_w27_fourdimensionalimageregistrationforintravitalmicroscopy": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Four Dimensional Image Registration For Intravital Microscopy",
    "authors": [
      "Chichen Fu",
      "Neeraj Gadgil",
      "Khalid K. Tahboub",
      "Paul Salama",
      "Kenneth W. Dunn",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Fu_Four_Dimensional_Image_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Fu_Four_Dimensional_Image_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Increasingly the behavior of living systems is being evaluated using intravital microscopy since it provides subcellular resolution of biological processes in an intact living organism. Intravital microscopy images are frequently confounded by motion resulting from animal respiration and heartbeat. In this paper we describe an image registration method capable of correcting motion artifacts in three dimensional fluorescence microscopy images collected over time. Our method uses 3D B-Spline non-rigid registration using a coarse-to-fine strategy to register stacks of images collected at different time intervals and 4D rigid registration to register 3D volumes over time. The results show that our proposed method has the ability of correcting global motion artifacts of sample tissues in four dimensional space, thereby revealing the motility of individual cells in the tissue."
  },
  "cvpr2016_w27_methodologyforincreasingthemeasurementaccuracyofimagefeatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Methodology for Increasing the Measurement Accuracy of Image Features",
    "authors": [
      "Michael Majurski",
      "Joe Chalfoun",
      "Steven P. Lund",
      "Peter Bajcsy",
      "Mary Brady"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/html/Majurski_Methodology_for_Increasing_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w27/papers/Majurski_Methodology_for_Increasing_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present an optimization methodology for improving the measurement accuracy of image features for low signal to noise ratio (SNR) images. By superimposing known background noise with high quality images in various proportions, we produce a degraded image set spanning a range of SNRs with reference feature values established from the unmodified high quality images. We then experiment with a variety of image processing spatial filters applied to the degraded images and identify which filter produces an image whose feature values most closely correspond to the reference values. When using the best combination of three filters and six kernel sizes for each feature, the average correlation of feature values between the degraded and high quality images increased from 0.6 (without filtering) to 0.92 (with feature-specific filters), a 53% improvement. Selecting a single filter is more practical than having a separate filter per feature. However, this results in a 1.95% reduction in correlation and a 10% increase in feature residual root mean square error compared to selecting the optimal filter and kernel size per feature. We quantified the tradeoff between a practical solution for all features and feature-specific solution to support decision making. "
  },
  "cvpr2016_w28_extendeddisfadatasetinvestigatingposedandspontaneousfacialexpressions": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Extended DISFA Dataset: Investigating Posed and Spontaneous Facial Expressions",
    "authors": [
      "Mohammad Mavadati",
      "Peyten Sanger",
      "Mohammad H. Mahoor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Mavadati_Extended_DISFA_Dataset_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Mavadati_Extended_DISFA_Dataset_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Automatic facial expression recognition (FER) is an important component of affect-aware technologies. Because of the lack of labeled spontaneous data, majority of existing automated FER systems were trained on posed facial expressions; however in real-world applications we deal with (subtle) spontaneous facial expression. This paper introduces an extension of DISFA, a previously released and well-accepted face dataset. Extended DISFA (DISFA+) has the following features: 1) it contains a large set of posed and spontaneous facial expressions data for a same group of individuals, 2) it provides the manually labeled frame-based annotations of 5-level intensity of twelve FACS facial actions, 3) it provides meta data (i.e. facial landmark points in addition to the self-report of each individual regarding every posed facial expression). This paper introduces and employs DISFA+, to analyze and compare temporal patterns and dynamic characteristics of posed and spontaneous facial expressions."
  },
  "cvpr2016_w28_aframeworkforjointestimationandguidedannotationoffacialactionunitintensity": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "A Framework for Joint Estimation and Guided Annotation of Facial Action Unit Intensity",
    "authors": [
      "Robert Walecki",
      "Ognjen Rudovic",
      "Maja Pantic",
      "Vladimir Pavlovic",
      "Jeffrey F. Cohn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Walecki_A_Framework_for_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Walecki_A_Framework_for_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Manual annotation of facial action units (AUs) is highly tedious and time-consuming. Various methods for automatic coding of AUs have been proposed, however, their performance is still far below of that attained by expert human coders. Several attempts have been made to leverage these methods to reduce the burden of manual coding of AU activations (presence/absence). Nevertheless, this has not been exploited in the context of AU intensity coding, which is a far more difficult task. To this end, we propose an expert-driven probabilistic approach for joint modeling and estimation of AU intensities. Specifically, we introduce a Conditional Random Field model for joint estimation of the AU intensity that updates its predictions in an iterative fashion by relying on expert knowledge of human coders. We show in our experiments on two publicly available datasets of AU intensity (DISFA and FERA2015) that the AU coding process can significantly be facilitated by the proposed approach."
  },
  "cvpr2016_w28_gaussianprocessdomainexpertsformodeladaptationinfacialbehavioranalysis": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis",
    "authors": [
      "Stefanos Eleftheriadis",
      "Ognjen Rudovic",
      "Marc P. Deisenroth",
      "Maja Pantic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Eleftheriadis_Gaussian_Process_Domain_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Eleftheriadis_Gaussian_Process_Domain_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present a novel approach for domain adaptation, that is based upon the framework of Gaussian processes (GPs). We introduce domain-specific GPs as local experts for facial expression classification. The adaptation is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Contrary to the existing approaches, we also learn a target expert from available target data solely. Then, a single classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining of the source classifiers. We evaluate the proposed approach on two datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. In our experiments we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show that the proposed approach consistently outperforms generic classifiers and the state-of-the-art methods on domain adaptation."
  },
  "cvpr2016_w28_automaticrecognitionofemotionsandmembershipingroupvideos": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Automatic Recognition of Emotions and Membership in Group Videos",
    "authors": [
      "Wenxuan Mou",
      "Hatice Gunes",
      "Ioannis Patras"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Mou_Automatic_Recognition_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Mou_Automatic_Recognition_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Automatic affect analysis and understanding has become a well established research area in the last two decades. However, little attention has been paid to the analysis of the affect expressed in group settings, either in the form of affect expressed by the whole group collectively or affect expressed by each individual member of the group. This paper presents a framework which, in group settings automatically classifies the affect expressed by each individual group member along both arousal and valence dimensions. We first introduce a novel vQLZM-FV descriptor to represent the facial behaviours of individuals in the spatio-temporal domain and then propose a method to recognize the group membership of each individual by using their face and body behavioural cues. The experiments show that the proposed vQLZM-FV outperforms the other feature representations in affect recognition, and group membership can be recognized using the non-verbal face and body features."
  },
  "cvpr2016_w28_facialaffectin-the-wildasurveyandanewdatabase": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Facial Affect ``In-The-Wild\": A Survey and a New Database",
    "authors": [
      "Stefanos Zafeiriou",
      "Athanasios Papaioannou",
      "Irene Kotsia",
      "Mihalis Nicolaou",
      "Guoying Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Zafeiriou_Facial_Affect_In-The-Wild_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Zafeiriou_Facial_Affect_In-The-Wild_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Well-established benchmarks have been developed in the past 20 years for automatic facial behaviour analysis. Nevertheless, for some important problems regarding analysis of facial behaviour, such as (a) estimation of affect in a continuous dimensional space (e.g., valence and arousal) in videos displaying spontaneous facial behaviour and (b) detection of the activated facial muscles (i.e., facial action unit detection) well-established \"in-the-wild\" benchmarks do not exist. The majority of the publicly availablecorpora for the above tasks contain samples that have been captured in controlled recording conditions. In this paper, we survey the progress that has been recently made on understanding facial behaviour \"in-the-wild\" and the datasets that have been developed so far, paying particular attention to deep learning techniques for the task. Finally, we make a step further and propose a new benchmark for facial behaviour understanding \"in-the-wild\"."
  },
  "cvpr2016_w28_fusingalignedandnon-alignedfaceinformationforautomaticaffectrecognitioninthewildadeeplearningapproach": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Fusing Aligned and Non-Aligned Face Information for Automatic Affect Recognition in the Wild: A Deep Learning Approach",
    "authors": [
      "Bo-Kyeong Kim",
      "Suh-Yeon Dong",
      "Jihyeon Roh",
      "Geonmin Kim",
      "Soo-Young Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Kim_Fusing_Aligned_and_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Kim_Fusing_Aligned_and_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Face alignment can fail in real-world conditions, negatively impacting the performance of automatic facial expression recognition (FER) systems. In this study, we assume a realistic situation including non-alignable faces due to failures in facial landmark detection. Our proposed approach fuses information about non-aligned and aligned facial states, in order to boost FER accuracy and efficiency. Six experimental scenarios using discriminative deep convolutional neural networks (DCNs) are compared, and causes for performance differences are identified. To handle non-alignable faces better, we further introduce DCNs that learn a mapping from non-aligned facial states to aligned ones, alignment-mapping networks (AMNs). We show that AMNs represent geometric transformations of face alignment, providing features beneficial for FER. Our automatic system based on ensembles of the discriminative DCNs and the AMNs achieves impressive results on a challenging database for FER in the wild."
  },
  "cvpr2016_w28_facialexpressionrecognitionfromworldwildweb": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Facial Expression Recognition from World Wild Web",
    "authors": [
      "Ali Mollahosseini",
      "Behzad Hasani",
      "Michelle J. Salvador",
      "Hojjat Abdollahi",
      "David Chan",
      "Mohammad H. Mahoor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Mollahosseini_Facial_Expression_Recognition_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Mollahosseini_Facial_Expression_Recognition_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Recognizing facial expression in a wild setting has remained a challenging task in computer vision. The World Wide Web is a good source of facial images which most of them are captured in uncontrolled conditions. In fact, the Internet is a Word Wild Web of facial images with expressions. This paper presents the results of a new study on collecting, annotating, and analyzing wild facial expressions from the web. Three search engines were queried using 1250 emotion related keywords in six different languages and the retrieved images were mapped by two annotators to six basic expressions and neutral. Deep neural networks and noise modeling were used in three different training scenarios to find how accurately facial expressions can be recognized when trained on noisy images collected from the web using query terms (e.g. happy face, laughing man, etc)? The results of our experiments show that deep neural networks can recognize wild facial expressions with an accuracy of 82.12%."
  },
  "cvpr2016_w28_facialexpressionrecognitioninthewildusingimproveddensetrajectoriesandfishervectorencoding": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Facial Expression Recognition in the Wild Using Improved Dense Trajectories and Fisher Vector Encoding",
    "authors": [
      "Sadaf Afshar",
      "Albert Ali Salah"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Afshar_Facial_Expression_Recognition_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Afshar_Facial_Expression_Recognition_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Improved dense trajectory features have been successfully used in video-based action recognition problems, but their application to face processing is more challenging. In this paper, we propose a novel system that deals with the problem of emotion recognition in real-world videos, using improved dense trajectory, LGBP-TOP, and geometric features. In the proposed system, we detect the face and facial landmarks from each frame of a video using a combination of two recent approaches, and register faces by means of Procrustes analysis. The improved dense trajectory and geometric features are encoded using Fisher vectors and classification is achieved by extreme learning machines. We evaluate our method on the extended Cohn-Kanade (CK+) and EmotiW 2015 Challenge databases. We obtain state-of-the-art results in both databases."
  },
  "cvpr2016_w28_towardsanin-the-wildemotiondatasetusingagame-basedframework": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Towards An \"In-The-Wild\" Emotion Dataset Using a Game-Based Framework",
    "authors": [
      "Wei Li",
      "Farnaz Abtahi",
      "Christina Tsangouri",
      "Zhigang Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Li_Towards_An_In-The-Wild_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Li_Towards_An_In-The-Wild_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": "In order to create an \"in-the-wild\" dataset of facial emotions with large number of balanced samples, this paper proposes a game-based data collection framework. The framework mainly include three components: a game engine, agame interface, and a data collection and evaluation module. We use a deep learning approach to build an emotion classifier as the game engine. Then a emotion web game to allow gamers to enjoy the games, while the data collection module obtains automatically-labelled emotion images. Using our game, we have collected more than 15,000 images within a month of the test run and built an emotion dataset \"GaMo\". To evaluate the dataset, we compared the performance of two deep learning models trained on both GaMo and CIFE. The results of our experiments show that because of being large and balanced, GaMo can be used to build a more robust emotion detector than the emotion detector trained on CIFE, which was used in the game engine to collect the face images."
  },
  "cvpr2016_w28_recurrentconvolutionalneuralnetworkregressionforcontinuouspainintensityestimationinvideo": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video",
    "authors": [
      "Jing Zhou",
      "Xiaopeng Hong",
      "Fei Su",
      "Guoying Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Zhou_Recurrent_Convolutional_Neural_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Zhou_Recurrent_Convolutional_Neural_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Automatic pain intensity estimation possesses a significant position in healthcare and medical field. Traditional static methods prefer to extract features from frames separately in a video, resulting in unstable changes and peaks among adjacent frames. To overcome this problem, we propose a real-time regression framework based on the recurrent convolutional neural network for automatic frame-level pain intensity estimation. Given vector sequences of AAM-warped facial images, we used a sliding-window strategy to obtain fixed-length input samples. We then carefully design the architecture of the recurrent network to output continuous-valued pain intensity. The proposed end-to-end pain intensity regression framework can predict the pain intensity of each frame by considering a sufficiently large historical frames while limiting the scale of the parameters within the model. Our method achieves promising results in both accuracy and running speed on the published UNBC-McMaster database."
  },
  "cvpr2016_w28_towardsfacialexpressionrecognitioninthewildanewdatabaseanddeeprecognitionsystem": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Towards Facial Expression Recognition in the Wild: A New Database and Deep Recognition System",
    "authors": [
      "Xianlin Peng",
      "Zhaoqiang Xia",
      "Lei Li",
      "Xiaoyi Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Peng_Towards_Facial_Expression_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Peng_Towards_Facial_Expression_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Automatic facial expression recognition (FER) plays an important role in many fields. However, most existing FER techniques are devoted to the tasks in the constrained conditions, which are different from actual emotions. To simulate the spontaneous expression, the number of samples in acted databases is usually small, which limits the ability of facial expression classification. In this paper, a novel database for natural facial expression is constructed leveraging the social images and then a deep model is trained based on the naturalistic dataset. An amount of social labeled images are obtained from the image search engines by using specific keywords. The algorithms of junk image cleansing are then utilized to remove the mislabeled images. Based on the collected images, the deep convolutional neural networks are learned to recognize these spontaneous expressions. Experiments show the advantages of the constructed dataset and deep approach."
  },
  "cvpr2016_w28_a3dmaskfaceanti-spoofingdatabasewithrealworldvariations": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "A 3D Mask Face Anti-Spoofing Database With Real World Variations",
    "authors": [
      "Siqi Liu",
      "Baoyao Yang",
      "Pong C. Yuen",
      "Guoying Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Liu_A_3D_Mask_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Liu_A_3D_Mask_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " 3D mask face spoofing attack becomes a new challenge and attracts more research interests recently. However, due to the deficiency number and limited variations of database, there are few methods be proposed to aim on it. Meanwhile, most of existing databases only concentrate on the anti-spoofing of different kinds of attacks and ignore the environmental changes in real world applications. In this paper, we build a new 3D mask anti-spoofing database with more variations to simulate the real world scenario. The proposed database contains 12 masks from two companies with different appearance quality. 7 cameras from the stationary and mobile devices and 6 lighting settings that cover typical illumination conditions are also included. Therefore, each subject contains 42 (7 cameras * 6 lightings) genuine and 42 mask sequences and the total size is 1008 videos. Future directions are pointed out in experiments. We plan to release the database to evaluate methods under different variations."
  },
  "cvpr2016_w28_sequentialfacealignmentviaperson-specificmodelinginthewild": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Context-Based Affect Recognition and Affective Face In-the-Wild",
    "title": "Sequential Face Alignment via Person-Specific Modeling in the Wild",
    "authors": [
      "Xi Peng",
      "Junzhou Huang",
      "Dimitris N. Metaxas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/html/Peng_Sequential_Face_Alignment_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w28/papers/Peng_Sequential_Face_Alignment_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Sequential face alignment, in essence, deals with non-rigid deformation that changes over time. In this paper, we propose to exploit incremental learning for person-specific alignment. Our approach takes advantage of part-based representation and cascade regression for robust and efficient alignment on each frame. More importantly, it incrementally updates the representation subspace and simultaneously adapts the cascade regressors in parallel using a unified framework. Person-specific modeling is eventually achieved on the fly while the drifting issue is significantly alleviated by erroneous detection using both part and holistic descriptors. Extensive experiments on both controlled and in-the-wild datasets demonstrate the superior performance of our approach compared with state of the arts in terms of fitting accuracy and efficiency."
  },
  "cvpr2016_w29_surveillance(oversight),sousveillance(undersight),andmetaveillance(seeingsightitself)": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Moving Cameras Meet Video Surveillance: From Body-Borne Cameras to Drones",
    "title": "Surveillance (Oversight), Sousveillance (Undersight), and Metaveillance (Seeing Sight Itself)",
    "authors": [
      "Steve Mann (Univ. of Toronto)"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/html/Mann_Surveillance_Oversight_Sousveillance_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/papers/Mann_Surveillance_Oversight_Sousveillance_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Surveillance is an established practice that cameras attached to fixed inanimate objects, or PTZ (Pan Tilt Zoom) cameras at a fixed position. Sur-veillance only provides part of the veillance story, and often only captures a partial truth. Further advances in miniaturization are giving rise to kinematic veillance (\"kineveillance\"): wearable, portable, and mobile cameras, as well as unpersoned aerial vehicles (UAVs). These additional veillances give us a more complete picture: multiple viewpoints from multiple entities bring us closer to the truth. In contrast to the extensive mathematical and conceptual framework developed around surveillance (e.g. background subtraction, frame-differencing, etc.), now that surveillance is no longer the only veillance, we need new mathematical and conceptual understandings of imaging and image processing. One such tool is the veillance wavefunction and metasensing: the sensing of sensors and the sensing of their capacity to sense."
  },
  "cvpr2016_w29_covertvideoclassificationbycodebookgrowingpattern": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Moving Cameras Meet Video Surveillance: From Body-Borne Cameras to Drones",
    "title": "Covert Video Classification by Codebook Growing Pattern",
    "authors": [
      "Liang Du",
      "Haitao Lang",
      "Ying-Li Tian",
      "Chiu C. Tan",
      "Jie Wu",
      "Haibin Ling"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/html/Du_Covert_Video_Classification_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/papers/Du_Covert_Video_Classification_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Recent advances in visual data acquisition and Internet technologies make it convenient and popular to collect and share videos. These activities, however, also raise the issue of privacy invasion. One potential privacy threaten is the unauthorized capturing and/or sharing of covert videos, which are recorded without the awareness of the subject(s) in the video. In this paper, we propose a novel descriptor, codebook growing pattern (CGP), which is derived from latent Dirichlet allocation (LDA) over optical flows. To evaluate the proposed approach, we collected a large covert video dataset, the first such dataset to our knowledge, and tested the proposed method on the dataset. The results show clearly the effectiveness of the proposed approach in comparison with other state-of-the-art video classification algorithms."
  },
  "cvpr2016_w29_detectinganomalousobjectsonmobileplatforms": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Moving Cameras Meet Video Surveillance: From Body-Borne Cameras to Drones",
    "title": "Detecting Anomalous Objects on Mobile Platforms",
    "authors": [
      "Wallace Lawson",
      "Laura Hiatt",
      "Keith Sullivan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/html/Lawson_Detecting_Anomalous_Objects_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/papers/Lawson_Detecting_Anomalous_Objects_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " We present an approach where a robot patrols a fixed path through an environment, autonomously locating suspicious or anomalous objects. To learn, the robot patrols this environment building a dictionary describing what is present. The dictionary is built by clustering features from a deep neural network. The objects present vary depending on the scene, which means that an object that is anomalous in one scene may be completely normal in another. To reason about this, the robot uses a computational cognitive model to learn the dictionary elements that are typically found in each scene. Once the dictionary and model has been built, the robot can patrol the environment matching objects against the dictionary, and querying the model to find the most likely objects present and to determine which objects (if any) are anomalous. We demonstrate our approach by patrolling two indoor and one outdoor environments."
  },
  "cvpr2016_w29_robustdetectionofmovingvehiclesinwideareamotionimagery": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Moving Cameras Meet Video Surveillance: From Body-Borne Cameras to Drones",
    "title": "Robust Detection of Moving Vehicles in Wide Area Motion Imagery",
    "authors": [
      "Michael Teutsch",
      "Michael Grinberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/html/Teutsch_Robust_Detection_of_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/papers/Teutsch_Robust_Detection_of_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Multiple object tracking in Wide Area Motion Imagery (WAMI) data is usually based on initial detections coming from background subtraction or frame differencing. However, these methods are prone to produce split and merged detections. Appearance based vehicle detection can be an alternative but is not well-suited for WAMI data since classifier models are of weak discriminative power for vehicles in top view at low resolution. We introduce a moving vehicle detection algorithm that combines 2-frame differencing with a vehicle appearance model to improve object detection. Our main contributions are (1) integration of robust vehicle detection with split/merge handling and (2) estimation of assignment likelihoods between object hypotheses in consecutive frames using an appearance based similarity measure. Without using any prior knowledge, we achieve state-of-the-art detection rates and produce tracklets that considerably simplify the data association problem for multiple object tracking."
  },
  "cvpr2016_w29_real-timevehicletrackinginaerialvideousinghyperspectralfeatures": {
    "conf_id": "CVPR2016",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2016_workshops - Moving Cameras Meet Video Surveillance: From Body-Borne Cameras to Drones",
    "title": "Real-Time Vehicle Tracking in Aerial Video Using Hyperspectral Features",
    "authors": [
      "Burak Uzkent",
      "Matthew J. Hoffman",
      "Anthony Vodacek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/html/Uzkent_Real-Time_Vehicle_Tracking_CVPR_2016_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2016_workshops/../content_cvpr_2016_workshops/w29/papers/Uzkent_Real-Time_Vehicle_Tracking_CVPR_2016_paper.pdf",
    "published": "2016-06",
    "summary": " Vehicle tracking from a moving aerial platform poses a number of unique challenges including the small number of pixels representing a vehicle, large camera motion, and parallax error. This paper considers a multi-modal sensor to design a real-time persistent aerial tracking system. Wide field of view (FOV) panchromatic imagery is used to remove global camera motion whereas narrow FOV hyperspectral image is used to detect the target of interest (TOI). Hyperspectral features provide distinctive information to reject objects with different reflectance characteristics from the TOI. This way the density of detected vehicles is reduced, which increases tracking consistency. Finally, we use a spatial data based classifier to remove spurious detections. With such framework, parallax effect in non-planar scenes is avoided. The proposed tracking system is evaluated in a dense, synthetic scene and outperforms other state-of-the-art traditional and aerial object trackers."
  }
}