{
  "accv2020_mlcsa_parallel-connectedresidualchannelattentionnetworkforremotesensingimagesuper-resolution": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Parallel-Connected Residual Channel Attention Network for Remote Sensing Image Super-Resolution",
    "authors": [
      "Yinhao Li",
      "Yutaro Iwamoto",
      "Lanfen Lin",
      "Yen-Wei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Li_Parallel-Connected_Residual_Channel_Attention_Network_for_Remote_Sensing_Image_Super-Resolution_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Li_Parallel-Connected_Residual_Channel_Attention_Network_for_Remote_Sensing_Image_Super-Resolution_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "In recent years, convolutional neural networks (CNNs) have obtained promising results in single-image super-resolution (SR) for remote sensing images. However, most existing methods are inadequate for remote sensing image SR due to the high computational cost required. Therefore, enhancing the representation ability with fewer parameters and a shorter prediction time is a challenging and critical task for remote sensing image SR. In this paper, we propose a novel CNN called a parallel-connected residual channel attention network (PCRCAN). Specifically, inspired by group convolution, we propose a parallel module with feature aggregation modules in PCRCAN. The parallel module significantly reduces the model parameters and fully integrates feature maps by widening the network architecture. In addition, to reduce the difficulty of training a complex deep network and improve model performance, we use a residual channel attention block as the basic feature mapping unit instead of a single convolutional layer. Experiments on a public remote sensing dataset UC Merced land-use dataset revealed that PCRCAN achieved higher accuracy, efficiency, and visual improvement than most state-of-the-art methods.",
    "code_link": ""
  },
  "accv2020_mlcsa_spatialandchannelattentionmodulatednetworkformedicalimagesegmentation": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Spatial and Channel Attention Modulated Network for Medical Image Segmentation",
    "authors": [
      "Wenhao Fang",
      "Xian-hua Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Fang_Spatial_and_Channel_Attention_Modulated_Network_for_Medical_Image_Segmentation_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Fang_Spatial_and_Channel_Attention_Modulated_Network_for_Medical_Image_Segmentation_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Medical image segmentation is a fundamental and challenge task in many computer-aided diagnosis and surgery systems, and attracts numerous research attention in computer vision and medical image processing fields. Recently, deep learning based medical image segmentation has been widely investigated and provided state-of-the-art performance for different modalities of medical data. Therein, U-Net consisting of the contracting path for context capturing and the symmetric expanding path for precise localization, has become a meta network architecture for medical image segmentation, and manifests acceptable results even with moderate scale of training data. This study proposes a novel attention modulated network based on the baseline U-Net, and explores embedded spatial and channel attention modules for adaptively highlighting interdependent channel maps and focusing on more discriminant regions via investigating relevant feature association. The proposed spatial and channel attention modules can be used in a plug and play manner and embedded after any learned feature map for adaptively emphasizing discriminant features and neglecting irrelevant information. Furthermore, we propose two aggregation approaches for integrating the learned spatial and channel attentions to the raw feature maps. Extensive experiments on two benchmark medical image datasets validate that our proposed network architecture manifests superior performance compared to the baseline U-Net and its several variants.",
    "code_link": ""
  },
  "accv2020_mlcsa_celldetectionandsegmentationinmicroscopyimageswithimprovedmaskr-cnn": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Cell Detection and Segmentation in Microscopy Images with Improved Mask R-CNN",
    "authors": [
      "Seiya Fujita",
      "Xian-Hua Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Fujita_Cell_Detection_and_Segmentation_in_Microscopy_Images_with_Improved_Mask_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Fujita_Cell_Detection_and_Segmentation_in_Microscopy_Images_with_Improved_Mask_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Analyzing and elucidating the attributes of cells and tissues with an observed microscopy image is a fundamental task in both biological research and clinical practice, and automation of this task to develop computer aided system based on image processing and machine learning technique has been rapidly evolved for providing quantitative evaluation and mitigating burden and time of the biological experts. Automated cell/nuclei detection and segmentation is in general a critical step in automatic system, and is quite challenging due to the existed heterogeneous characteristics of cancer cell such as large variability in size, shape, appearance, and texture of the different cells. This study proposes a novel method for simultaneous detection and segmentation of cells based on the Mask R-CNN, which conducts multiple end-to-end learning tasks by minimizing multi task losses for generic object detection and segmentation. The conventional Mask R-CNN employs cross entropy loss for evaluating the object detection fidelity, and equally treats all training samples in learning procedure regardless to the properties of the objects such as easily or hard degree for detection, which may lead to miss-detection of hard samples. To boost the detection performance of hard samples, this work integrates the focal loss for formulating detection criteria into Mask R-CNN, and investigate a feasible method for balancing the contribution of multiple task losses in network training procedure. Experiments on the benchmark dataset: DSB2018 manifest that our proposed method achieves the promising performance on both cell detection and segmentation.",
    "code_link": ""
  },
  "accv2020_mlcsa_g-gcsnglobalgraphconvolutionshrinkagenetworkforemotionperceptionfromgait": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "G-GCSN: Global Graph Convolution Shrinkage Network for Emotion Perception from Gait",
    "authors": [
      "Yuan Zhuang",
      "Lanfen Lin",
      "Ruofeng Tong",
      "Jiaqing Liu",
      "Yutaro Iwamot",
      "Yen-Wei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Zhuang_G-GCSN_Global_Graph_Convolution_Shrinkage_Network_for_Emotion_Perception_from_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Zhuang_G-GCSN_Global_Graph_Convolution_Shrinkage_Network_for_Emotion_Perception_from_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Recently, emotion recognition through gait, which is more difficult to imitate than other biological characteristics, has aroused extensive attention. Although some deep-learning studies have been conducted in this field, there are still two challenges. First, it is hard to extract the representational features of the gait from video effectively. Second, the input of body joints sequences has noise introduced during dataset collection and feature production. In this work, we propose a global link, which extends the existing skeleton graph (the natural link) to capture the overall state of gait based on spatial-temporal convolution. In addition, we use soft thresholding to reduce noise. The thresholds are learned automatically by a block called shrinkage block. Combined with the global link and shrinkage block, we further propose the global graph convolution shrinkage network (G-GCSN) to capture the emotion-related features. We validate the effectiveness of the proposed method on a public dataset (i.e., Emotion-Gait dataset). The proposed G-GCSN achieves improvements compared with state-of-the-art methods.",
    "code_link": ""
  },
  "accv2020_mlcsa_bdsl36adatasetforbangladeshisignlettersrecognition": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "BdSL36: A Dataset for Bangladeshi Sign Letters Recognition",
    "authors": [
      "Oishee Bintey Hoque",
      "Mohammad Imrul Jubair",
      "Al-Farabi Akash",
      "Saiful Islam"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Hoque_BdSL36_A_Dataset_for_Bangladeshi_Sign_Letters_Recognition_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Hoque_BdSL36_A_Dataset_for_Bangladeshi_Sign_Letters_Recognition_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Bangladeshi Sign Language (BdSL) is a commonly used med\\-ium of communication for the hearing-impaired people in Bangladesh. A real-time BdSL interpreter with no controlled lab environment has a broad social impact and an interesting avenue of research as well. Also, it is a challenging task due to the variation in different subjects (age, gender, color, etc.), complex features, and similarities of signs and clustered backgrounds. However, the existing dataset for BdSL classification task is mainly built in a lab friendly setup which limits the application of powerful deep learning technology. In this paper, we introduce a dataset named BdSL36 which incorporates background augmentation to make the dataset versatile and contains over four million images belonging to 36 categories. Besides, we annotate about 40,000 images with bounding boxes to utilize the potentiality of object detection algorithms. Furthermore, several intensive experiments are performed to establish the baseline performance of our BdSL36. Moreover, we employ beta testing of our classifiers at the user level to justify the possibilities of real-world application with this dataset. We believe our BdSL36 will expedite future research on practical sign letter classification. We make the datasets and all the pre-trained models available for further researcher.",
    "code_link": ""
  },
  "accv2020_mlcsa_unsupervisedmultispectralandhyperspectralimagefusionwithdeepspatialandspectralpriors": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Unsupervised Multispectral and Hyperspectral Image Fusion with Deep Spatial and Spectral Priors",
    "authors": [
      "Zhe Liu",
      "Yinqiang Zheng",
      "Xian-Hua Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Liu_Unsupervised_Multispectral_and_Hyperspectral_Image_Fusion_with_Deep_Spatial_and_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Liu_Unsupervised_Multispectral_and_Hyperspectral_Image_Fusion_with_Deep_Spatial_and_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Hyperspectral (HS) imaging is a promising imaging modality, which can simultaneously acquire various bands of images of the same scene and capture detailed spectral distribution helping for numerous applications. However, existing HS imaging sensor can only obtain images with low spatial resolution. Thus fusing a low resolution hyperspectral (LR-HS) image with a high resolution (HR) RGB (or multispectral) image into a HR-HS image has received much attention. Conventional fusion methods usually employ various hand-crafted priors to regularize the mathematical model formulating the relation between the observations and the HR-HS image, and conduct optimization for pursuing the optimal solution. However, the politic prior would be various for different scenes and is difficult to hammer out for a specific scene. Recently, deep learning-based methods have been widely explored for HS image resolution enhancement, and impressive performance has been validated. As it is known that deep learning-based methods essentially require large-scale training samples, which are hard to obtain due to the limitation of the existing HS cameras, for constructing the model with good generalization. Motivated by the deep image prior that network architecture itself sufficiently captures a great deal of low-level image statistics with arbitrary learning strategy, we investigate the deep learned image prior consisting both spatial structure and spectral attribute instead of hand-crafted priors for unsupervised multispectral (RGB) and HS image fusion, and propose a novel deep spatial and spectral prior learning framework for exploring the underlying structure of the latent HR-HS image with the observed HR-RGB and LR-HS images only. The proposed deep prior learning method has no requirement to prepare massive triplets of the HR-RGB, LR-HS and HR-HS images for network training.We validate the proposed method on two benchmark HS image datasets, and experimental results show that our method is comparable or outperforms the state-of-the-art HS image super-resolution approaches.",
    "code_link": ""
  },
  "accv2020_mlcsa_3dsemanticsegmentationforlarge-scalesceneunderstanding": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "3D Semantic Segmentation for Large-Scale  Scene Understanding",
    "authors": [
      "Kiran Akadas",
      "Shankar Gangisetty"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Akadas_3D_Semantic_Segmentation_for_Large-Scale__Scene_Understanding_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Akadas_3D_Semantic_Segmentation_for_Large-Scale__Scene_Understanding_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "3D semantic segmentation is one of the most challenging events in the robotic vision tasks for detection and identification of var- ious objects in a scene. In this paper, we solve the task of semantic segmentation to classify and assign every point in the scene with an as- sociated label. We propose a lightweight semantic segmentation network for large-scale point clouds which consists of grid subsampling, dilated convolutions, and Gaussian error linear unit activation for gaining better performance. The dilated convolutions increase the receptive field while reducing the number of parameters, making proposed network faster and computationally more efficient with reduced number of parameters. Ad- ditionally, we use conditional random field as post processing method to boost the performance of proposed semantic segmentation network. We perform an exhaustive quantitative analysis of the proposed network on SOTA datasets, namely, SHREC 2020 street scenes dataset [1], S3DIS [2] and SemanticKITTI [3]. We show that proposed semantic segmentation network performs effectively and efficiently compared to SOTA methods.",
    "code_link": "https://github.com/KiranAkadas/GRanDNet"
  },
  "accv2020_mlcsa_aweaklysupervisedconvolutionalnetworkforchangesegmentationandclassification": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Machine Learning and Computing for Visual Semantic Analysis",
    "title": "A Weakly Supervised Convolutional Network for Change Segmentation and Classification",
    "authors": [
      "Philipp Andermatt",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Andermatt_A_Weakly_Supervised_Convolutional_Network_for_Change_Segmentation_and_Classification_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MLCSA/papers/Andermatt_A_Weakly_Supervised_Convolutional_Network_for_Change_Segmentation_and_Classification_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Fully supervised change detection methods require difficult to procure pixel-level labels, while weakly supervised approaches can be trained with image-level labels. However, most of these approaches require a combination of changed and unchanged image pairs for training. Thus, these methods can not directly be used for datasets where only changed image pairs are available. We present W-CDNet, a novel weakly supervised change detection network that can be trained with image-level semantic labels. Additionally, W-CDNet can be trained with two different types of datasets, either containing changed image pairs only or a mixture of changed and unchanged image pairs. Since we use image-level semantic labels for training, we simultaneously create a change mask and label the changed object for single-label images. W-CDNet employs a W-shaped siamese U-net to extract feature maps from an image pair which then get compared in order to create a raw change mask. The core part of our model, the Change Segmentation and Classification (CSC) module, learns an accurate change mask at a hidden layer by using a custom Remapping Block and then segmenting the current input image with the change mask. The segmented image is used to predict the image-level semantic label. The correct label can only be predicted if the change mask actually marks relevant change. This forces the model to learn an accurate change mask. We demonstrate the segmentation and classification performance of our approach and achieve top results on AICD and HRSCD, two public aerial imaging change detection datasets as well as on a Food Waste change detection dataset. Our code is available at: https://github.com/PhiAbs/W-CDNet",
    "code_link": "https://github.com/PhiAbs/W-CDNet"
  },
  "accv2020_mmhau_iterativeself-distillationforprecisefaciallandmarklocalization": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MMHAU",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Multi-visual-Modality Human Activity Understanding",
    "title": "Iterative Self-distillation for Precise Facial Landmark Localization",
    "authors": [
      "Shigenori Nagae",
      "Yamato Takeuchi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/html/Nagae_Iterative_Self-distillation_for_Precise_Facial_Landmark_Localization_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/papers/Nagae_Iterative_Self-distillation_for_Precise_Facial_Landmark_Localization_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "In this paper, we propose a novel training method to improve the precision of facial landmark localization. When a facial landmark localization method is applied to a facial video, the detected landmarks occasionally jitter, whereas the face apparently does not move. We hypothesize that there are two causes that induce the unstable detection: (1) small changes in input images and (2) inconsistent annotations. Corresponding to the causes, we propose (1) two loss terms to make a model robust to changes in the input images and (2) self-distillation training to reduce the effect of the annotation noise. We show that our method can improve the precision of facial landmark localization by reducing the variance using public facial landmark datasets, 300-W and 300-VW. We also show that our method can reduce jitter of predicted landmarks when applied to a video.",
    "code_link": ""
  },
  "accv2020_mmhau_visibleandthermalcamera-basedjaywalkingestimationusingahierarchicaldeeplearningframework": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MMHAU",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Multi-visual-Modality Human Activity Understanding",
    "title": "Visible and Thermal Camera-based Jaywalking Estimation using a Hierarchical Deep Learning Framework",
    "authors": [
      "Vijay John",
      "Ali Boyali",
      "Simon Thompson",
      "Annamalai Lakshmanan",
      "Seiichi Mita"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/html/John_Visible_and_Thermal_Camera-based_Jaywalking_Estimation_using_a_Hierarchical_Deep_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/papers/John_Visible_and_Thermal_Camera-based_Jaywalking_Estimation_using_a_Hierarchical_Deep_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Jaywalking is an abnormal pedestrian behavior which significantly increases the risk of road accidents. Owing to this risk, autonomous driving applications should robustly estimate the jaywalking pedestrians. However, the task of robustly estimating jaywalking is not trivial, especially in the case of visible camera-based estimation. In this work, a two-step hierarchical deep learning formulation using visible and thermal camera is proposed to address these challenges. The two steps are comprised of a deep learning-based scene classifier and two scene-specific semantic segmentation frameworks. The scene classifier classifies the visible-thermal image into legal pedestrian crossing and illegal pedestrian crossing scenes. The two scene-specific segmentation frameworks estimate the normal pedestrians and jaywalking pedestrians. The two segmentation frameworks are individually trained on the legal or illegal crossing scenes. The proposed framework is validated on the FLIR public dataset and compared with baseline algorithms. The experimental results show that the proposed hierarchical strategy reports better accuracy than baseline algorithms in real-time.",
    "code_link": ""
  },
  "accv2020_mmhau_multiviewsimilaritylearningforrobustvisualclustering": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MMHAU",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Multi-visual-Modality Human Activity Understanding",
    "title": "Multiview Similarity Learning for Robust Visual Clustering",
    "authors": [
      "AO LI",
      "Jia jia Chen",
      "Deyun Chen",
      "Guanglu Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/html/Li_Multiview_Similarity_Learning_for_Robust_Visual_Clustering_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/papers/Li_Multiview_Similarity_Learning_for_Robust_Visual_Clustering_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Multiview similarity learning aims to measure the neighbor relationship between each pair of samples, which has been widely used in data mining and presents encouraging performance on lots of applications. Nevertheless, the recent existing multiview similarity learning methods have two main drawbacks. On one hand, the comprehensive consensus similarity is learned based on previous fixed graphs learned from all views separately, which ignores the latent cues hidden in graphs from different views. On the other hand, when the data are contaminated with noise or outlier, the performance of existing methods will decline greatly because the original true data distribution is destroyed. To address the two problems, a Robust Multiview Similarity Learning(RMvSL) method is proposed in this paper. The contributions of RMvSL includes three aspects. Firstly, the recent low-rank representation shows some advantage in removing noise and outliers, which motivates us to introduce the data representation via low-rank constraint in order to generate clean reconstructed data for robust graph learning in each view. Secondly, a multiview scheme is established to learn the consensus similarity by dynamically learned graphs from all views. Meanwhile, the consensus similarity can be used to propagate the latent relationship information from other views to learn each view graph in turn. Finally, the above two processes are put into a unified objective function to optimize the data reconstruction, view graphs learning and consensus similarity graph learning alternatingly, which can help to obtain overall optimal solutions. Experimental results on several visual data clustering demonstrates that RMvSL outperforms the most existing methods on similarity learning and presents great robustness on noisy data.",
    "code_link": ""
  },
  "accv2020_mmhau_real-timespatio-temporalactionlocalizationvialearningmotionrepresentation": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MMHAU",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Multi-visual-Modality Human Activity Understanding",
    "title": "Real-time spatio-temporal action localization via learning motion representation",
    "authors": [
      "Yuanzhong Liu",
      "Zhigang Tu",
      "Liyu Lin",
      "Xing Xie",
      "Qianqing Qin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/html/Liu_Real-time_spatio-temporal_action_localization_via_learning_motion_representation_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/papers/Liu_Real-time_spatio-temporal_action_localization_via_learning_motion_representation_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Most state-of-the-art spatio-temporal (S-T) action localization methods explicitly use optical flow as auxiliary motion information. Although the combination of optical flow and RGB significantly improves the performance, optical flow estimation brings a large amount of computational cost and the whole network is not end-to-end trainable. These shortcomings hinder the interactive fusion between motion information and RGB information, and greatly limit its real-world applications. In this paper, we exploit better ways to use motion information in a unified end-to-end trainable network architecture. First, we use knowledge distillation to enable the 3D-Convolutional branch to learn motion information from RGB inputs. Second, we propose a novel motion cue called short-range-motion (SRM) module to enhance the 2DConvolutional branch to learn RGB information and dynamic motion information. In this strategy, flow computation at test time is avoided. Finally, we apply our methods to learn powerful RGB-motion representations for action classification and localization. Experimental results show that our method outperforms the state-of-the-arts on dataset benchmarks J-HMDB-21 and UCF101-24 with an impressive improvement of 7% and 3%.",
    "code_link": ""
  },
  "accv2020_mmhau_towardslocalitysimilaritypreservingto3dhumanposeestimation": {
    "conf_id": "ACCV2020",
    "conf_sub_id": "MMHAU",
    "is_workshop": true,
    "conf_name": "ACCV2020_workshops - Multi-visual-Modality Human Activity Understanding",
    "title": "Towards Locality Similarity Preserving to 3D Human Pose Estimation",
    "authors": [
      "Shihao Zhou",
      "Mengxi Jiang",
      "Qicong Wang",
      "Yunqi Lei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/html/Zhou_Towards_Locality_Similarity_Preserving_to_3D_Human_Pose_Estimation_ACCVW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2020W/MMHAU/papers/Zhou_Towards_Locality_Similarity_Preserving_to_3D_Human_Pose_Estimation_ACCVW_2020_paper.pdf",
    "published": "2020-11",
    "summary": "Estimating 3D human pose from an annotated or detected 2D pose in a single RGB image is a challenging problem. A successful way to address this problem is the example-based approach. The existing example-based approaches often calculate a global pose error to search a single match 3D pose from the source library. This way fails to capture the local deformations of human pose and highly dependent on a large training set. To alleviate these issues, we propose a simple example-based approach with locality similarity preserving to estimate 3D human pose. Specifically, first of all, we split an annotated or detected 2D pose into 2D body parts with kinematic priors. Then, to recover the 3D pose from these 2D body parts, we recombine a 3D pose by using 3D body parts that are split from the 3D pose candidates. Note that joints in the combined 3D parts are refined by a weighted searching strategy during the inference. Moreover, to increase the search speed, we propose a candidate selecting mechanism to narrow the original source data. We evaluate our approach on three well-design benchmarks, including Human3.6M, HumanEva-I, and MPII. The extensive experimental results show the effectiveness of our approach. Specifically, our approach achieves better performance than compared approaches while using fewer training samples.",
    "code_link": ""
  }
}