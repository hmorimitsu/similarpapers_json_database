{
  "bmvc2017_main_semantics-preservinglocalityembeddingforzero-shotlearning": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Semantics-Preserving Locality Embedding for Zero-Shot Learning",
    "authors": [
      "Shih-Yen Tao",
      "Yi-Ren Yeh",
      "Yu-Chiang Frank Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper003/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper003/paper003.pdf",
    "published": "2017-09",
    "summary": "Zero-shot learning (ZSL) aims at recognizing data as an unseen category, using information learned from the training data of prede\ufb01ned (seen) labels or attributes. In this paper, we propose an effective learning model for solving ZSL, which focuses on relating image and semantic domains with classi\ufb01cation guarantees. In particular, we introduce semantics-preserving locality embedding when associating the above cross-domain data. We show that our ZSL model can be extended from inductive and transductive ZSL settings, if unlabeled data of unseen categories are presented during training.",
    "code_link": ""
  },
  "bmvc2017_main_hyperspectralcnnclassificationwithlimitedtrainingsamples": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Hyperspectral CNN Classification with Limited Training Samples",
    "authors": [
      "Lloyd Windrim",
      "Rishi Ramakrishnan",
      "Arman Melkumyan",
      "Richard Murphy"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper004/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper004/paper004.pdf",
    "published": "2017-09",
    "summary": "Hyperspectral imaging sensors are becoming increasingly popular in robotics applications such as agriculture and mining, and allow per-pixel thematic classi\ufb01cation of materials in a scene based on their unique spectral signatures. Recently, convolutional neural networks have shown remarkable performance for classi\ufb01cation tasks, but require substantial amounts of labelled training data. This data must suf\ufb01ciently cover the variability expected to be encountered in the environment. For hyperspectral data, one of the main variations encountered outdoors is due to incident illumination, which can change in spectral shape and intensity depending on the scene geometry. For example, regions occluded from the sun have a lower intensity and their incident irradiance skewed towards shorter wavelengths. In this work, a data augmentation strategy based on relighting is used during training of a hyperspectral convolutional neural network. It allows training to occur in the outdoor environment given only a small labelled region, which does not need to suf\ufb01ciently represent the geometric variability of the entire scene. This is important for applications where obtaining large amounts of training data is labourious, hazardous or dif\ufb01cult, such as labelling pixels within shadows.",
    "code_link": ""
  },
  "bmvc2017_main_whichisthebetterinpaintedimage?learningwithoutsubjectiveannotation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Which is the better inpainted image? Learning without subjective annotation",
    "authors": [
      "Mariko Isogawa",
      "Dan Mikami",
      "Kosuke Takahashi",
      "Hideaki Kimata"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper005/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper005/paper005.pdf",
    "published": "2017-09",
    "summary": "This paper proposes a learning-based quality evaluation framework for inpainted results that does not require any subjectively annotated training data. Image inpainting, which removes and restores unwanted regions in images, is widely acknowledged as a task whose results are quite dif\ufb01cult to evaluate objectively. Thus, existing learning-based image quality assessment (IQA) methods for inpainting require subjectively annotated data for training. However, subjective annotation requires huge cost and subjects\u2019 judgment occasionally differs from person to person in accordance with the judgment criteria. To overcome these dif\ufb01culties, the proposed framework uses simulated failure results of inpainted images whose subjective qualities are controlled as the training data. This approach enables preference order between pairwise inpainted images to be successfully estimated even if the task is quite subjective.",
    "code_link": ""
  },
  "bmvc2017_main_spatio-temporalconsistencytodetectandsegmentcarriedobjects": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Spatio-Temporal Consistency to Detect and Segment Carried Objects",
    "authors": [
      "Farnoosh Ghadiri",
      "Robert Bergevin",
      "Guillaume-Alexandre Bilodeau"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper006/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper006/paper006.pdf",
    "published": "2017-09",
    "summary": "We present a new method to detect carried objects and to segment them accurately after detection. The proposed method includes several contributions: \ufb01rst, a new superpixel-based descriptor is proposed to identify carried object-like candidate regions using human shape modelling. Second, integrating spatio-temporal information of candidate regions to detect carried objects. We exploit the consistency of recurring carried object candidates viewed over time to detect the \ufb01nal carried object locations based on their motion and location priors. Last, the detected carried object regions are accurately segmented. Compared to existing methods, our approach is not only focusing on detecting carried objects. It takes a step forward and accurately segment them. Our method to carried object segmentation couples local appearance cues with location priors of the detected carried objects to produce accurate segmentation.",
    "code_link": ""
  },
  "bmvc2017_main_improvedimagesegmentationviacostminimizationofmultiplehypotheses": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Improved Image Segmentation via Cost Minimization of Multiple Hypotheses",
    "authors": [
      "Marc Bosch Bosch",
      "Christopher Gifford",
      "Austin Dress",
      "Clare Lau",
      "Jeffrey Skibo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper007/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper007/paper007.pdf",
    "published": "2017-09",
    "summary": "Image segmentation is an important component of many image understanding systems. It aims to group pixels in a spatially and perceptually coherent manner. Typically, these algorithms have a collection of parameters that control the degree of over-segmentation produced. It still remains a challenge to properly select such parameters for human-like perceptual grouping. In this work, we exploit the diversity of segments produced by different choices of parameters. We scan the segmentation parameter space and generate a collection of image segmentation hypotheses (from highly over-segmented to under-segmented). These are fed into a cost minimization framework that produces the \ufb01nal segmentation by selecting segments that: (1) better describe the natural contours of the image, and (2) are more stable and persistent among all the segmentation hypotheses. We compare our algorithm\u2019s performance with state-of-the-art algorithms, showing that we can achieve improved results.",
    "code_link": "https://github.com/fuxiang87/MCL_CCP"
  },
  "bmvc2017_main_faceparsingviarecurrentpropagation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Face Parsing via Recurrent Propagation",
    "authors": [
      "Sifei Liu",
      "Jianping Shi",
      "Liang Ji",
      "Ming-Hsuan Yang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper008/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper008/paper008.pdf",
    "published": "2017-09",
    "summary": "Face parsing is an important problem in computer vision that \ufb01nds numerous applications including recognition and editing. Recently, deep convolutional neural networks (CNNs) have been applied to image parsing and segmentation with the state-of-the-art performance. In this paper, we propose a face parsing algorithm that combines hierarchical representations learned by a CNN, and accurate label propagations achieved by a spatially variant recurrent neural network (RNN). The RNN-based propagation approach enables ef\ufb01cient inference over a global space with the guidance of semantic edges generated by a local convolutional model. Since the convolutional architecture can be shallow and the spatial RNN can have few parameters, the framework is much faster and more light-weighted than the state-of-the-art CNNs for the same task. We apply the proposed model to coarse-grained and \ufb01ne-grained face parsing. For \ufb01ne-grained face parsing, we develop a two-stage approach by \ufb01rst identifying the main regions and then segmenting the detail components, which achieves better performance in terms of accuracy and efficiency.",
    "code_link": ""
  },
  "bmvc2017_main_accuratecameraregistrationinurbanenvironmentsusinghigh-levelfeaturematching": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Accurate Camera Registration in Urban Environments Using High-Level Feature Matching",
    "authors": [
      "Anil Armagan",
      "Martin Hirzer",
      "Peter Roth",
      "Vincent Lepetit"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper009/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper009/paper009.pdf",
    "published": "2017-09",
    "summary": "We propose a method for accurate camera pose estimation in urban environments from single images and 2D maps made of the surrounding buildings\u2019 outlines. Our approach bridges the gap between learning-based approaches and geometric approaches: We use recent semantic segmentation techniques for extracting the buildings\u2019 edges and the fa\u00e7ades\u2019 normals in the images and minimal solvers [14] to compute the camera pose accurately and robustly. We propose two such minimal solvers: one based on three correspondences of buildings\u2019 corners from the image and the 2D map and another one based on two corner correspondences plus one fa\u00e7ade correspondence.",
    "code_link": ""
  },
  "bmvc2017_main_thedevilisinthedecoder": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "The Devil is in the Decoder",
    "authors": [
      "Zbigniew Wojna",
      "Jasper Uijlings",
      "Sergio Guadarrama",
      "Nathan Silberman",
      "Liang-Chieh Chen",
      "Alireza Fathi",
      "Vittorio Ferrari"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper010/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper010/paper010.pdf",
    "published": "2017-09",
    "summary": "Many machine vision applications require predictions for every pixel of the input image (for example semantic segmentation, boundary detection). Models for such problems usually consist of encoders which decreases spatial resolution while learning a high-dimensional representation, followed by decoders who recover the original input resolution and result in low-dimensional predictions. While encoders have been studied rigorously, relatively few studies address the decoder side. Therefore this paper presents an extensive comparison of a variety of decoders for a variety of pixel-wise prediction tasks. Our contributions are: (1) Decoders matter: we observe signi\ufb01cant variance in results between different types of decoders on various problems. (2) We introduce a novel decoder: bilinear additive upsampling. (3) We introduce new residual-like connections for decoders.",
    "code_link": ""
  },
  "bmvc2017_main_virtualtorealreinforcementlearningforautonomousdriving": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Virtual to Real Reinforcement Learning for Autonomous Driving",
    "authors": [
      "Xinlei Pan",
      "Yurong You",
      "Ziyan Wang",
      "Cewu Lu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper011/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper011/paper011.pdf",
    "published": "2017-09",
    "summary": "Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well.",
    "code_link": ""
  },
  "bmvc2017_main_real-timesalientclosedboundarytrackingusingperceptualgroupingandshapepriors": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Real-Time Salient Closed Boundary Tracking using Perceptual Grouping and Shape Priors",
    "authors": [
      "Xuebin Qin",
      "Shida He",
      "Zichen Zhang",
      "Masood Dehghan",
      "Martin Jagersand"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper012/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper012/paper012.pdf",
    "published": "2017-09",
    "summary": "In this paper, we propose a real-time method for accurate salient closed boundary tracking via a combination of shape constraints and perceptual grouping on edge fragments. Particularly, we encode the Gestalt law of proximity and the prior shape constraint in a novel ratio-form grouping cost. The proximity and prior constraint are depicted by the relative gap length and average distance difference along the to-be-tracked boundary with respect to its area. We build a graph using the detected edge fragments and in-between gaps. The grouping problem is formulated as searching for a special cycle in this graph with a minimum grouping cost. To reduce the search space and achieve real-time performance, we propose a set of novel techniques for ef\ufb01cient edge fragments splitting and filtering. We evaluate this method on a public real-world video dataset against other methods.",
    "code_link": "https://github.com/NathanUA/SalientClosedBoundaryTrackingDataset"
  },
  "bmvc2017_main_kinematic-layout-awarerandomforestsfordepth-basedactionrecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Kinematic-Layout-aware Random Forests for Depth-based Action Recognition",
    "authors": [
      "Seungryul Baek",
      "Zhiyuan Shi",
      "Masato Kawade",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper013/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper013/paper013.pdf",
    "published": "2017-09",
    "summary": "This paper tackles the problem of 24 hours monitoring patient actions in a ward such as \u201clying on the bed\u201d, \u201cstretching an arm out of the bed\u201d and \u201cfalling out of the bed\u201d. In the concerned scenario, 3D geometric information (e.g. relations between scene layouts and body kinematics) is important to reveal the actions; however securing them at testing itself is a challenging problem. Especially in our data, securing human skeletal joints at testing time is not easy due to unique and diverse human posture. To address the problem, we propose a kinematic-layout-aware random forest considering the geometry between scene layouts and skeletons (i.e. kinematic-layout), secured in the offline manner, in the training of forests to maximize the discriminant power of depth appearance. We inte- grate the kinematic-layout in the split criteria of random forests to guide the learning process by 1) measuring the usefulness of kinematic-layout information and switching the use of kinematic-layout, and 2) implicitly closing the gap between two distributions obtained by the kinematic-layout and the appearance, if the kinematic-layout appears useful. Experimental evaluations on our new dataset (PATIENT) demonstrate that our method outperforms various state-of-the-arts for this problem. We have also demon- strated accuracy improvements by applying our method to conventional single-view and cross-view action recognition datasets (e.g. CAD-60, UWA3D Multiview Activity II).",
    "code_link": ""
  },
  "bmvc2017_main_totalcapture3dhumanposeestimationfusingvideoandinertialsensors": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors",
    "authors": [
      "Matthew Trumble",
      "Andrew Gilbert",
      "Charles Malleson",
      "Adrian Hilton",
      "John Collomosse"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper014/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper014/paper014.pdf",
    "published": "2017-09",
    "summary": "We present an algorithm for fusing multi-viewpoint video (MVV) with inertial measurement unit (IMU) sensor data to accurately estimate 3D human pose. A 3-D convolutional neural network is used to learn a pose embedding from volumetric probabilistic visual hull data (PVH) derived from the MVV frames. We incorporate this model within a dual stream network integrating pose embeddings derived from MVV and a forward kinematic solve of the IMU data. A temporal model (LSTM) is incorporated within both streams prior to their fusion. Hybrid pose inference using these two complementary data sources is shown to resolve ambiguities within each sensor modality, yielding improved accuracy over prior methods. A further contribution of this work is a new hybrid MVV dataset (TotalCapture) comprising video, IMU and a skeletal joint ground truth derived from a commercial motion capture system. The dataset is available online at http://cvssp.org/data/totalcapture/.",
    "code_link": ""
  },
  "bmvc2017_main_indirectdeepstructuredlearningfor3dhumanbodyshapeandposeprediction": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Indirect deep structured learning for 3D human body shape and pose prediction",
    "authors": [
      "Vince Tan",
      "Ignas Budvytis",
      "Roberto Cipolla"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper015/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper015/paper015.pdf",
    "published": "2017-09",
    "summary": "In this paper we present a novel method for 3D human body shape and pose prediction. Our work is motivated by the need to reduce our reliance on costly-to-obtain ground truth labels. To achieve this, we propose training an encoder-decoder network using a two step procedure as follows. During the \ufb01rst step, a decoder is trained to predict a body silhouette using SMPL [2] (a statistical body shape model) parameters as an input. During the second step, the whole network is trained on real image and corresponding silhouette pairs while the decoder is kept \ufb01xed. Such a procedure allows for an indirect learning of body shape and pose parameters from real images without requiring any ground truth parameter data.",
    "code_link": ""
  },
  "bmvc2017_main_real-timevisual-inertialodometryforeventcamerasusingkeyframe-basednonlinearoptimization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Real-time Visual-Inertial Odometry for Event Cameras using Keyframe-based Nonlinear Optimization",
    "authors": [
      "Henri Rebecq",
      "Timo Horstschaefer",
      "Davide Scaramuzza"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper016/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper016/paper016.pdf",
    "published": "2017-09",
    "summary": "Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer signi\ufb01cant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. We propose a novel, accurate tightly-coupled visual-inertial odometry pipeline for such cameras that leverages their outstanding properties to estimate the camera ego-motion in challenging conditions, such as high-speed motion or high dynamic range scenes. The method tracks a set of features (extracted on the image plane) through time. To achieve that, we consider events in overlapping spatio-temporal windows and align them using the current camera motion and scene structure, yielding motion-compensated event frames. We then combine these feature tracks in a keyframe-based, visual-inertial odometry algorithm based on nonlinear optimization to estimate the camera\u2019s 6-DOF pose, velocity, and IMU biases. The proposed method is evaluated quantitatively on the public Event Camera Dataset [19] and signi\ufb01cantly outperforms the state-of-the-art [28], while being computationally much more ef\ufb01cient: our pipeline can run much faster than real-time on a laptop and even on a smartphone processor.",
    "code_link": ""
  },
  "bmvc2017_main_weaklysupervisedsemanticsegmentationbasedonco-segmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Weakly Supervised Semantic Segmentation Based on Co-segmentation",
    "authors": [
      "Tong Shen",
      "Guosheng Lin",
      "Lingqiao Liu",
      "Chunhua Shen",
      "Ian Reid"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper017/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper017/paper017.pdf",
    "published": "2017-09",
    "summary": "Training a Fully Convolutional Network (FCN) for semantic segmentation requires a large number of masks with pixel level labelling, which involves a large amount of human labour and time for annotation. In contrast, web images and their image-level labels are much easier and cheaper to obtain. In this work, we propose a novel method for weakly supervised semantic segmentation with only image-level labels. The method utilizes the internet to retrieve a large number of images and uses a large scale co-segmentation framework to generate masks for the retrieved images. We \ufb01rst retrieve images from search engines, e.g. Flickr and Google, using semantic class names as queries, e.g. class names in the dataset PASCAL VOC 2012. We then use high quality masks produced by co-segmentation on the retrieved images as well as the target dataset images with image level labels to train segmentation networks. We obtain an IoU score of 56.",
    "code_link": ""
  },
  "bmvc2017_main_semanticsegmentationwithreverseattention": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Semantic Segmentation with Reverse Attention",
    "authors": [
      "Qin Huang",
      "Chihao Wu",
      "Chunyang Xia",
      "Ye Wang",
      "C.-C. Jay Kuo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper018/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper018/paper018.pdf",
    "published": "2017-09",
    "summary": "Recent development in fully convolutional neural network enables ef\ufb01cient end-to-end learning of semantic segmentation. Traditionally, the convolutional classi\ufb01ers are taught to learn the representative semantic features of labeled semantic objects. In this work, we propose a reverse attention network (RAN) architecture that trains the network to capture the opposite concept (i.e., what are not associated with a target class) as well. The RAN is a three-branch network that performs the direct, reverse and reverse-attention learning processes simultaneously. Extensive experiments are conducted to show the effectiveness of the RAN in semantic segmentation. Being built upon the DeepLabv2-LargeFOV, the RAN achieves the state-of-the-art mean IoU score (48.1%) for the challenging PASCAL-Context dataset.",
    "code_link": ""
  },
  "bmvc2017_main_doubleexpansionforoptimizationofmultilabelenergies": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Double Expansion for Optimization of Multilabel Energies",
    "authors": [
      "Yelena Gorelick",
      "Zhengqin Li",
      "Olga Veksler"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper019/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper019/paper019.pdf",
    "published": "2017-09",
    "summary": "We propose a new class of energies for segmentation of multiple foreground objects with a common shape prior. Our energy involves in\ufb01nity constraints. For such energies standard expansion algorithm has no optimality guarantees and in practice gets stuck in bad local minima. Therefore, we develop a new move making algorithm, we call double expansion. In contrast to expansion, the new move allows each pixel to choose a label from a pair of new labels or keep the old label. This results in an algorithm with optimality guarantees and robust performance in practice.",
    "code_link": ""
  },
  "bmvc2017_main_discoveringclass-specificpixelsforweakly-supervisedsemanticsegmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Discovering Class-Specific Pixels for Weakly-Supervised Semantic Segmentation",
    "authors": [
      "Arslan Chaudhry",
      "Puneet Kumar Dokania",
      "Philip Torr"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper020/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper020/paper020.pdf",
    "published": "2017-09",
    "summary": "We propose an approach to discover class-speci\ufb01c pixels for the weakly-supervised semantic segmentation task. We show that properly combining saliency and attention maps allows us to obtain reliable cues capable of signi\ufb01cantly boosting the performance. First, we propose a simple yet powerful hierarchical approach to discover the class-agnostic salient regions, obtained using a salient object detector, which otherwise would be ignored. Second, we use fully convolutional attention maps to reliably localize the class-speci\ufb01c regions in a given image. We combine these two cues to discover class-speci\ufb01c pixels which are then used as an approximate ground truth for training a CNN. While solving the weakly supervised semantic segmentation task, we ensure that the image-level classi\ufb01cation task is also solved in order to enforce the CNN to assign at least one pixel to each object present in the image. Experimentally, on the PASCAL VOC12 val and test sets, we obtain the mIoU of 60.8% and 61.9%, achieving the performance gains of 5.1% and 5.2% compared to the published state-of-the-art results.",
    "code_link": "https://github.com/arslan-chaudhry/dcsp_segmentation"
  },
  "bmvc2017_main_adaptingmodelstosignaldegradationusingdistillation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Adapting Models to Signal Degradation using Distillation",
    "authors": [
      "Jong-Chyi Su",
      "Subhransu Maji"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper021/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper021/paper021.pdf",
    "published": "2017-09",
    "summary": "Model compression and knowledge distillation have been successfully applied for cross-architecture and cross-domain transfer learning. However, a key requirement is that training examples are in correspondence across the domains. We show that in many scenarios of practical importance such aligned data can be synthetically generated using computer graphics pipelines allowing domain adaptation through distillation. We apply this technique to learn models for recognizing low-resolution images using labeled high-resolution images, non-localized objects using labeled localized objects, line-drawings using labeled color images, etc. Experiments on various \ufb01ne-grained recognition datasets demonstrate that the technique improves recognition performance on the low-quality data and beats strong baselines for domain adaptation.",
    "code_link": ""
  },
  "bmvc2017_main_localizingactionsfromvideolabelsandpseudo-annotations": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Localizing Actions from Video Labels and Pseudo-Annotations",
    "authors": [
      "Pascal Mettes",
      "Cees Snoek",
      "Shih-Fu Chang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper022/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper022/paper022.pdf",
    "published": "2017-09",
    "summary": "The goal of this paper is to determine the spatio-temporal location of actions in video. Where training from hard to obtain box annotations is the norm, we propose an intuitive and effective algorithm that localizes actions from their class label only. We are inspired by recent work showing that unsupervised action proposals selected with human point-supervision perform as well as using expensive box annotations. Rather than asking users to provide point supervision, we propose fully automatic visual cues that replace manual point annotations. We call the cues pseudo-annotations, introduce \ufb01ve of them, and propose a correlation metric for automatically selecting and combining them. Thorough evaluation on challenging action localization datasets shows that we reach results comparable to results with full box supervision.",
    "code_link": ""
  },
  "bmvc2017_main_introductiontocoherentdepthfieldsfordensemonocularsurfacerecovery": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Introduction to Coherent Depth Fields for Dense Monocular Surface Recovery",
    "authors": [
      "Vladislav Golyanik",
      "Torben Fetzer",
      "Didier Stricker"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper023/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper023/paper023.pdf",
    "published": "2017-09",
    "summary": "Handling large occlusions in non-rigid structure from motion (NRSfM) currently re- quires either an expensive correspondence correction or estimation of a shape prior on several non-occluded views. To save computational cost and remove the dependency on additional pre-processing steps, this paper introduces the concept of depth fields. With the proposed depth fields, NRSfM is interpreted as an alternating estimation of vector fields with fixed origins on the one side, and estimation of displacements of the origins along the depth dimension on the other. The core of the new energy-based Coherent Depth Fields (CDF) approach is the spatial smoothness coherency term (CT) applied on the depth fields. Having its origins in the Motion Coherence Theory, CT interprets data as a displacement vector field and penalises irregularities in displacements. Not only for handling occlusions but also for unoccluded scenes CT has multiple advantages compared to previously proposed regularisers such as total variation. We show exper- imentally that CDF achieves state-of-the-art in dense NRSfM including scenarios with long and large occlusions, inaccurate correspondences as well as inaccurate initialisa- tions, without requiring any additional pre-processing steps.",
    "code_link": ""
  },
  "bmvc2017_main_real-timefactoredconvnetsextractingthexfactorinhumanparsing": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Real-time Factored ConvNets: Extracting the X Factor in Human Parsing",
    "authors": [
      "James Charles",
      "Ignas Budvytis",
      "Roberto Cipolla"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper024/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper024/paper024.pdf",
    "published": "2017-09",
    "summary": "We propose a real-time and lightweight multi-task style ConvNet (termed a Factored ConvNet) for human body parsing in images or video. Factored ConvNets have isolated areas which perform known sub-tasks, such as object localization or edge detection. We call this area and sub-task pair an X factor. Unlike multi-task ConvNets which have independent tasks, the Factored ConvNet\u2019s sub-task has direct effect on the main task outcome. In this paper we show how to isolate the X factor of foreground/background (f/b) subtraction from the main task of segmenting human body images into 31 different body part types. Knowledge of this X factor leads to a number of bene\ufb01ts for the Factored ConvNet: 1) Ease of network transfer to other image domains, 2) ability to personalize to humans in video and 3) easy model performance boosts. All achieved by either ef\ufb01cient network update or replacement of the X factor whilst avoiding catastrophic forgetting of previously learnt body part dependencies and structure.",
    "code_link": ""
  },
  "bmvc2017_main_holistic,instance-levelhumanparsing": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Holistic, Instance-level Human Parsing",
    "authors": [
      "Qizhu Li",
      "Anurag Arnab",
      "Philip Torr"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper025/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper025/paper025.pdf",
    "published": "2017-09",
    "summary": "Object parsing \u2013 the task of decomposing an object into its semantic parts \u2013 has traditionally been formulated as a category-level segmentation problem. Consequently, when there are multiple objects in an image, current methods cannot count the number of objects in the scene, nor can they determine which part belongs to which object. We address this problem by segmenting the parts of objects at an instance-level, such that each pixel in the image is assigned a part label, as well as the identity of the object it belongs to. Moreover, we show how this approach bene\ufb01ts us in obtaining segmentations at coarser granularities as well. Our proposed network is trained end-to-end given detections, and begins with a category-level segmentation module. Thereafter, a differentiable Conditional Random Field, de\ufb01ned over a variable number of instances for every input image, reasons about the identity of each part by associating it with a human detection.",
    "code_link": ""
  },
  "bmvc2017_main_ctfrommotionvolumetriccaptureofmovingshapesusingx-rays": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "CT from Motion: Volumetric Capture of Moving Shapes using X-rays",
    "authors": [
      "Julien Pansiot",
      "Edmond Boyer"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper026/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper026/paper026.pdf",
    "published": "2017-09",
    "summary": "In this paper, we consider the capture of dense volumetric X-ray attenuation models of non-rigidly moving samples. Traditional 3D medical imaging apparatus, e.g. CT or MRI, do not easily adapt to shapes that deform signi\ufb01cantly such as a moving hand. We propose an approach that simultaneously recovers dense volumetric shape and motion information by combining video and X-ray modalities. Multiple colour images are captured to track shape surfaces while a single X-ray device is used to infer inner attenuations. The approach does not assume prior models which makes it versatile and easy to generalise over different shapes. Results on synthetic and real-life data are presented that demonstrate the approach feasibility with a limited number of X-ray views.",
    "code_link": ""
  },
  "bmvc2017_main_conciseradiometriccalibrationusingthepowerofranking": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Concise Radiometric Calibration Using The Power of Ranking",
    "authors": [
      "Han Gong",
      "Graham Finlayson",
      "Maryam Darrodi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper027/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper027/paper027.pdf",
    "published": "2017-09",
    "summary": "Compared with raw images, the more common JPEG images are less useful for machine vision algorithms and professional photographers because JPEG-sRGB does not preserve a linear relation between pixel values and the light measured from the scene. A camera is said to be radiometrically calibrated if there is a computational model which can predict how the raw linear sensor image is mapped to the corresponding rendered image (e.g. JPEGs) and vice versa. This paper begins with the observation that the rank order of pixel values are mostly preserved post colour correction. We show that this observation is the key to solving for the whole camera pipeline (colour correction, tone and gamut mapping). Our rank-based calibration method is simpler than the prior art and so is parametrised by fewer variables which, concomitantly, can be solved for using less calibration data. Another advantage is that we can derive the camera pipeline from a single pair of raw-JPEG images.",
    "code_link": ""
  },
  "bmvc2017_main_scaleexploitingminimalsolversforrelativeposewithcalibratedcameras": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Scale Exploiting Minimal Solvers for Relative Pose with Calibrated Cameras",
    "authors": [
      "Stephan Liwicki",
      "Christopher Zach"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper028/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper028/paper028.pdf",
    "published": "2017-09",
    "summary": "We present ef\ufb01cient minimal solvers for the estimation of relative pose between two calibrated images. The proposed method exploits known scale ratios of feature matches which are intrinsically provided by scale invariant feature detectors with scale-space pyramids (e.g. SIFT). Since we are using scale, the number of required feature matches is reduced and consequently fewer inliers are needed. In our paper, we derive two solvers each related to one of the two possible minimal sets: Our 1 + 3 point algorithm estimates relative pose from four feature matches with one known scale, while our 2 + 1 point algorithm computes relative pose from three feature matches with two known scales. We embed the proposed methods into RANSAC with two inlier classes, and present an ef\ufb01cient RANSAC modi\ufb01cation for less reliable scales.",
    "code_link": ""
  },
  "bmvc2017_main_multipleinstancecurriculumlearningforweaklysupervisedobjectdetection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Multiple Instance Curriculum Learning for Weakly Supervised Object Detection",
    "authors": [
      "Siyang Li",
      "Xiangxin Zhu",
      "Qin Huang",
      "Hao Xu",
      "C.-C. Jay Kuo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper029/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper029/paper029.pdf",
    "published": "2017-09",
    "summary": "When supervising an object detector with weakly labeled data, most existing approaches are prone to trapping in the discriminative object parts, e.g., \ufb01nding the face of a cat instead of the full body, due to lacking the supervision on the extent of full objects. To address this challenge, we incorporate object segmentation into the detector training, which guides the model to correctly localize the full objects. We propose the multiple instance curriculum learning (MICL) method, which injects curriculum learning (CL) into the multiple instance learning (MIL) framework. The MICL method starts by automatically picking the easy training examples, where the extent of the segmentation masks agree with detection bounding boxes. The training set is gradually expanded to include harder examples to train strong detectors that handle complex images.",
    "code_link": ""
  },
  "bmvc2017_main_fastfeaturefooladataindependentapproachtouniversaladversarialperturbations": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Fast Feature Fool: A data independent approach to universal adversarial perturbations",
    "authors": [
      "Konda Reddy Reddy",
      "Utsav Garg",
      "Venkatesh Babu Radhakrishnan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper030/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper030/paper030.pdf",
    "published": "2017-09",
    "summary": "State-of-the-art object recognition Convolutional Neural Networks (CNNs) are shown to be fooled by image agnostic perturbations, called universal adversarial perturbations. It is also observed that these perturbations generalize across multiple networks trained on the same target data. However, these algorithms require training data on which the CNNs were trained and compute adversarial perturbations via complex optimization. The fooling performance of these approaches is directly proportional to the amount of available training data. This makes them unsuitable for practical attacks since its unreasonable for an attacker to have access to the training data. In this paper, for the \ufb01rst time, we propose a novel data independent approach to generate image agnostic perturbations for a range of CNNs trained for object recognition. We further show that these perturbations are transferable across multiple network architectures trained either on same or di\ufb00erent data. In the absence of data, our method generates universal perturbations e\ufb03ciently via fooling the features learned at multiple layers thereby causing CNNs to misclassify.",
    "code_link": "https://github.com/utsavgarg/fast-feature-fool"
  },
  "bmvc2017_main_3dcolorchartsforcameraspectralsensitivityestimation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "3D color charts for camera spectral sensitivity estimation",
    "authors": [
      "Rada Deeb",
      "Damien Muselet",
      "Mathieu Hebert",
      "Alain Tremeau",
      "Joost van de Weijer"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper031/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper031/paper031.pdf",
    "published": "2017-09",
    "summary": "Estimating spectral data such as camera sensor responses or illuminant spectral power distribution from raw RGB camera outputs is crucial in many computer vision applications. Usually, 2D color charts with various patches of known spectral re\ufb02ectance are used as reference for such purpose. Deducing n-D spectral data (n\u00bb3) from 3D RGB inputs is an ill-posed problem that requires a high number of inputs. Unfortunately, most of the natural color surfaces have spectral re\ufb02ectances that are well described by low-dimensional linear models, i.e. each spectral re\ufb02ectance can be approximated by a weighted sum of the others. It has been shown that adding patches to color charts does not help in practice, because the information they add is redundant with the information provided by the \ufb01rst set of patches. In this paper, we propose to use spectral data of higher dimensionality by using 3D color charts that create inter-re\ufb02ections between the surfaces. These inter-re\ufb02ections produce multiplications between natural spectral curves and so provide non-linear spectral curves.",
    "code_link": ""
  },
  "bmvc2017_main_guidedrobustmatte-modelfittingforacceleratingmulti-lightreflectanceprocessingtechniques": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Guided Robust Matte-Model Fitting for Accelerating Multi-light Reflectance Processing Techniques",
    "authors": [
      "Ruggero Pintus",
      "Andrea Giachetti",
      "Gianni Pintore",
      "Enrico Gobbetti"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper032/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper032/paper032.pdf",
    "published": "2017-09",
    "summary": "The generation of a basic matte model is at the core of many multi-light re\ufb02ectance processing approaches, such as Photometric Stereo or Re\ufb02ectance Transformation Imaging. To recover information on objects\u2019 shape and appearance, the matte model is used directly or combined with specialized methods for modeling high-frequency behaviors. Multivariate robust regression offers a general solution to reliably extract the matte component when source data is heavily contaminated by shadows, inter-re\ufb02ections, specularity, or noise. However, robust multivariate modeling is usually very slow. In this paper, we accelerate robust \ufb01tting by drastically reducing the number of tested candidate solutions using a guided approach. Our method propagates already known solutions to nearby pixels using a similarity-driven \ufb02ood-\ufb01ll strategy, and exploits this knowledge to order possible candidate solutions and to determine convergence conditions.",
    "code_link": ""
  },
  "bmvc2017_main_fastevent-basedcornerdetection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Fast Event-based Corner Detection",
    "authors": [
      "Elias Mueggler",
      "Chiara Bartolozzi",
      "Davide Scaramuzza"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper033/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper033/paper033.pdf",
    "published": "2017-09",
    "summary": "Event cameras offer many advantages over standard frame-based cameras, such as low latency, high temporal resolution, and a high dynamic range. They respond to pixel-level brightness changes and, therefore, provide a sparse output. However, in textured scenes with rapid motion, millions of events are generated per second. Therefore, state-of-the-art event-based algorithms either require massive parallel computation (e.g., a GPU) or depart from the event-based processing paradigm. Inspired by frame-based pre-processing techniques that reduce an image to a set of features, which are typically the input to higher-level algorithms, we propose a method to reduce an event stream to a corner event stream. Our goal is twofold: extract relevant tracking information (corners do not suffer from the aperture problem) and decrease the event rate for later processing stages. Our event-based corner detector is very ef\ufb01cient due to its design principle, which consists of working on the Surface of Active Events (a map with the timestamp of the latest event at each pixel) using only comparison operations. Our method asynchronously processes event by event with very low latency. Our implementation is capable of processing millions of events per second on a single core (less than a micro-second per event) and reduces the event rate by a factor of 10 to 20.",
    "code_link": ""
  },
  "bmvc2017_main_pcnpartandcontextinformationforpedestriandetectionwithcnns": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "PCN: Part and Context Information for Pedestrian Detection with CNNs",
    "authors": [
      "Shiguang Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper034/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper034/paper034.pdf",
    "published": "2017-09",
    "summary": "Pedestrian detection has achieved great improvements in recent years, while complex occlusion handling is still one of the most important problems. To take advantage of the body parts and context information for pedestrian detection, we propose the part and context network (PCN) in this work. PCN specially utilizes two branches which detect the pedestrians through body parts semantic and context information, respectively. In the Part Branch, the semantic information of body parts can communicate with each other via recurrent neural networks. In the Context Branch, we adopt a local competition mechanism for adaptive context scale selection. By combining the outputs of all branches, we develop a strong complementary pedestrian detector with a lower miss rate and better localization accuracy, especially for occlusion pedestrian. Comprehensive evaluations on two challenging pedestrian detection datasets (i.e. Caltech and INRIA) well demonstrated the effectiveness of the proposed PCN.",
    "code_link": ""
  },
  "bmvc2017_main_adaptivelocalcontrastnormalizationforrobustobjectdetectionandposeestimation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Adaptive Local Contrast Normalization for Robust Object Detection and Pose Estimation",
    "authors": [
      "Mahdi Rad",
      "Vincent Lepetit",
      "Peter Roth"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper035/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper035/paper035.pdf",
    "published": "2017-09",
    "summary": "To be robust to illumination changes when detecting objects in images, the current trend is to train a Deep Network with training images captured under many different lighting conditions. Unfortunately, creating such a training set is very cumbersome, or sometimes even impossible, for some applications such as 3D pose estimation of speci\ufb01c objects, which is the application we focus on in this paper. We therefore propose a novel illumination normalization method that lets us learn to detect objects and estimate their 3D pose under challenging illumination conditions from very few training samples. Our key insight is that normalization parameters should adapt to the input image. In particular, we realized this via a Convolutional Neural Network trained to predict the parameters of a generalization of the Difference-of-Gaussians method.",
    "code_link": ""
  },
  "bmvc2017_main_object-extentpoolingforweaklysupervisedsingle-shotlocalization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Object-Extent Pooling for Weakly Supervised Single-Shot Localization",
    "authors": [
      "Amogh Gudi",
      "Nicolai van Rosmalen",
      "Marco Loog",
      "Jan van Gemert"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper036/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper036/paper036.pdf",
    "published": "2017-09",
    "summary": "In the face of scarcity in detailed training annotations, the ability to perform object localization tasks in real-time with weak-supervision is very valuable. However, the computational cost of generating and evaluating region proposals is heavy. We adapt the concept of Class Activation Maps (CAM) [28] into the very \ufb01rst weakly-supervised \u2018single-shot\u2019 detector that does not require the use of region proposals. To facilitate this, we propose a novel global pooling technique called Spatial Pyramid Averaged Max (SPAM) pooling for training this CAM-based network for object extent localisation with only weak image-level supervision. We show this global pooling layer possesses a near ideal \ufb02ow of gradients for extent localization, that offers a good trade-off between the extremes of max and average pooling. Our approach only requires a single network pass and uses a fast-backprojection technique, completely omitting any region proposal steps. To the best of our knowledge, this is the \ufb01rst approach to do so.",
    "code_link": ""
  },
  "bmvc2017_main_transformedanti-sparsehashing": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Transformed Anti-Sparse Hashing",
    "authors": [
      "Zhangyang Wang",
      "Ji Liu",
      "Shuai Huang",
      "Xinchao Wang",
      "Shiyu Chang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper037/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper037/paper037.pdf",
    "published": "2017-09",
    "summary": "Anti-sparse representation was recently considered for unsupervised hashing, due to its remarkable robustness to the binary quantization error. We relax the existing spread property [4, 22] for anti-sparse solutions, to a new Relaxed Spread Property (RSP) that demands milder conditions. We then propose a novel Transformed Anti-Sparse Hashing (TASH) model to overcome several major bottlenecks, that have signi\ufb01cantly limited the effectiveness of anti-sparse hashing models. TASH jointly learns a dimension-reduction transform, a dictionary and the anti-sparse representations in a uni\ufb01ed formulation.",
    "code_link": ""
  },
  "bmvc2017_main_deeplysupervised3drecurrentfcnforsalientobjectdetectioninvideos": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Deeply Supervised 3D Recurrent FCN for Salient Object Detection in Videos",
    "authors": [
      "Trung-Nghia Le",
      "Akihiro Sugimoto"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper038/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper038/paper038.pdf",
    "published": "2017-09",
    "summary": "This paper presents a novel end-to-end 3D fully convolutional network for salient object detection in videos. The proposed network uses 3D \ufb01lters in the spatiotemporal domain to directly learn both spatial and temporal information to have 3D deep features, and transfers the 3D deep features to pixel-level saliency prediction, outputting saliency voxels. In our network, we combine the re\ufb01nement at each layer and deep supervision to ef\ufb01ciently and accurately detect salient object boundaries. The re\ufb01nement module recurrently enhances to learn contextual information into the feature map. Applying deeply-supervised learning to hidden layers, on the other hand, improves details of the intermediate saliency voxel, and thus the saliency voxel is re\ufb01ned progressively to become \ufb01ner and \ufb01ner. Intensive experiments using publicly available benchmark datasets con\ufb01rm that our network outperforms state-of-the-art methods.",
    "code_link": ""
  },
  "bmvc2017_main_robustdensedepthmapsgenerationsfromsparsedvsstereos": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Robust Dense Depth Maps Generations from Sparse DVS Stereos",
    "authors": [
      "Dongqing Zou",
      "Feng Shi",
      "Weiheng Liu",
      "Jia Li",
      "Qiang Wang",
      "Paul-K.J. Park",
      "Hyunsurk Eric Ryu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper039/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper039/paper039.pdf",
    "published": "2017-09",
    "summary": "Real-world depth perception applications require precise reaction to fast motion, and the ability to operate in scenes which contain large intensity differences or high dynamic range. Standard CMOS cameras based methods for depth computing, such as stereo matching, run into the problem of huge power consuming at high frame-rates or inaccurate depths with noise or holes. The event camera, DVS (Dynamic Vision Sensor), aims to be robust to fast motion and light change with low power consumption and sparse representation, offering great potential to replace standard cameras for depth perception. However, it is not trivial to directly apply DVS for stereo matching due to its nature of low latency and sparsity which will result in extremely limited available information and imperfect imaging quality. To overcome these problems and make the DVS available for depth perception, this paper introduces a novel method which can enhance the stream of events and estimate the dense depth through event driven stereo matching. Our event stream enhancement algorithm ef\ufb01ciently buffers events according to time continuous rather than using arti\ufb01cially-chosen time intervals, and our stereo matching method can robust estimate depth for complex scenarios regardless of the motion or light changing. To the best of our knowledge, this is the \ufb01rst algorithm provably able to recover dense depth maps from sparse DVS stereos.",
    "code_link": ""
  },
  "bmvc2017_main_adaptivetemporalpoolingforobjectdetectionusingdynamicvisionsensor": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Adaptive Temporal Pooling for Object Detection using Dynamic Vision Sensor",
    "authors": [
      "Jia Li",
      "Feng Shi",
      "Weiheng Liu",
      "Dongqing Zou",
      "Qiang Wang",
      "Paul-K.J. Park",
      "Hyunsurk Eric Ryu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper040/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper040/paper040.pdf",
    "published": "2017-09",
    "summary": "Dynamic Vision Sensor (DVS) is a kind of asynchronous sensor which can capture visual information with low power consumption. Encouraged by the superior properties of DVS, we propose a novel object detection method for DVS. The imaging of DVS is highly related to the object motion speed. For sparse events induced by slow motions, directly applying existing object detection methods cannot obtain reliable features, leading to loss in performance. For this issue, we introduce a new model to extract motion invariant features for object detection using DVS, namely recursive adaptive temporal pooling. The model works in a recursive manner and adaptively integrates features in varied time interval into a holistic representation for current time-step with respect to the motion speed. The pooled features are then fed into a detection sub-network to produce the \ufb01nal results. Our framework is end-to-end trainable and is highly ef\ufb01cient bene\ufb01tting from the recursive manner and the characteristics of DVS data. Our method is evaluated on a large DVS dataset with about 100k ground-truth images and compared with the state-of-the-art methods.",
    "code_link": ""
  },
  "bmvc2017_main_high-resolutionstereomatchingbasedonsampledphotoconsistencycomputation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "High-Resolution Stereo Matching based on Sampled Photoconsistency Computation",
    "authors": [
      "Chloe Legendre",
      "Konstantinos Batsos",
      "Philippos Mordohai"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper041/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper041/paper041.pdf",
    "published": "2017-09",
    "summary": "We propose an approach to binocular stereo that avoids exhaustive photoconsistency computations at every pixel, since they are redundant and computationally expensive, especially for high resolution images. We argue that developing scalable stereo algorithms is critical as image resolution is expected to continue increasing rapidly. Our approach relies on oversegmentation of the images into superpixels, followed by photoconsistency computation for only a random subset of the pixels of each superpixel. This generates sparse reconstructed points which are used to \ufb01t planes. Plane hypotheses are propagated among neighboring superpixels, and they are evaluated at each superpixel by selecting a random subset of pixels on which to aggregate photoconsistency scores for the competing planes. We performed extensive tests to characterize the performance of this algorithm in terms of accuracy and speed on the full-resolution stereo pairs of the 2014 Middlebury benchmark that contains up to 6-megapixel images. Our results show that very large computational savings can be achieved at a small loss of accuracy.",
    "code_link": "https://github.com/chloelle/SPS"
  },
  "bmvc2017_main_generativeopenmaxformulti-classopensetclassification": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Generative OpenMax for Multi-Class Open Set Classification",
    "authors": [
      "Zongyuan Ge",
      "Sergey Demyanov",
      "Rahil Garnavi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper042/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper042/paper042.pdf",
    "published": "2017-09",
    "summary": "We present a conceptually new and \ufb02exible method for multi-class open set classification. Unlike previous methods where unknown classes are inferred with respect to the feature or decision distance to the known classes, our approach is able to provide explicit modelling and decision score for unknown classes. The proposed method, called Generative OpenMax (G-OpenMax), extends OpenMax by employing generative adversarial networks (GANs) for novel category image synthesis. We validate the proposed method on two datasets of handwritten digits and characters, resulting in superior results over previous deep learning based method OpenMax Moreover, G-OpenMax provides a way to visualize samples representing the unknown classes from open space.",
    "code_link": ""
  },
  "bmvc2017_main_ansacadaptivenon-minimalsampleandconsensus": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "ANSAC: Adaptive Non-Minimal Sample and Consensus",
    "authors": [
      "Victor Fragoso",
      "Christopher Sweeney",
      "Pradeep Sen",
      "Matthew Turk"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper043/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper043/paper043.pdf",
    "published": "2017-09",
    "summary": "While RANSAC-based methods are robust to incorrect image correspondences (outliers), their hypothesis generators are not robust to correct image correspondences (inliers) with positional error (noise). This slows down their convergence because hypotheses drawn from a minimal set of noisy inliers can deviate signi\ufb01cantly from the optimal model. This work addresses this problem by introducing ANSAC, a RANSAC-based estimator that accounts for noise by adaptively using more than the minimal number of correspondences required to generate a hypothesis. ANSAC estimates the inlier ratio (the fraction of correct correspondences) of several ranked subsets of candidate correspondences and generates hypotheses from them. Its hypothesis-generation mechanism prioritizes the use of subsets with high inlier ratio to generate high-quality hypotheses. ANSAC uses an early termination criterion that keeps track of the inlier ratio history and terminates when it has not changed signi\ufb01cantly for a period of time.",
    "code_link": ""
  },
  "bmvc2017_main_colorrestorationofunderwaterimages": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Color Restoration of Underwater Images",
    "authors": [
      "Dana Menaker",
      "Tali Treibitz",
      "Shai Avidan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper044/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper044/paper044.pdf",
    "published": "2017-09",
    "summary": "Underwater images suffer from color distortion and low contrast, because light is attenuated as it propagates through water. The attenuation varies with wavelength and depends both on the properties of the water body in which the image was taken and the 3D structure of the scene, making it dif\ufb01cult to restore the colors. Existing single underwater image enhancement techniques either ignore the wavelength dependency of the attenuation, or assume a speci\ufb01c spectral pro\ufb01le. We propose a new method that takes into account multiple spectral pro\ufb01les of different water types, and restores underwater scenes from a single image. We show that by estimating just two additional global parameters - the attenuation ratios of the blue-red and blue-green color channels - the problem of underwater image restoration can be reduced to single image dehazing, where all color channels have the same attenuation coef\ufb01cients. Since we do not know the water type ahead of time, we try different parameter sets out of an existing library of water types. Each set leads to a different restored image and the one that best satis\ufb01es the Gray-World assumption is chosen. The proposed single underwater image restoration method is fully automatic and is based on a more comprehensive physical image formation model than previously used. We collected a dataset of real images taken in different locations with varying water properties and placed color charts in the scenes. Moreover, to obtain ground truth, the 3D structure of the scene was calculated based on stereo imaging.",
    "code_link": ""
  },
  "bmvc2017_main_fine-grainedimageretrievalthetext/sketchinputdilemma": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Fine-Grained Image Retrieval: the Text/Sketch Input Dilemma",
    "authors": [
      "Jifei Song",
      "Yi-zhe Song",
      "Tony Xiang",
      "Timothy Hospedales"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper045/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper045/paper045.pdf",
    "published": "2017-09",
    "summary": "Fine-grained image retrieval (FGIR) enables a user to search for a photo of an object instance based on a mental picture. Depending on how the object is described by the user, two general approaches exist: sketch-based FGIR or text-based FGIR, each of which has its own pros and cons. However, no attempt has been made to systematically investigate how informative each of these two input modalities is, and more importantly whether they are complementary to each thus should be modelled jointly. In this work, for the \ufb01rst time we introduce a multi-modal FGIR dataset with both sketches and sentences description provided as query modalities. A multi-modal quadruplet deep network is formulated to jointly model the sketch and text input modalities as well as the photo output modality.",
    "code_link": ""
  },
  "bmvc2017_main_cross-domaingenerativelearningforfine-grainedsketch-basedimageretrieval": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Cross-domain Generative Learning for Fine-Grained Sketch-Based Image Retrieval",
    "authors": [
      "Kaiyue Pang",
      "Yi-zhe Song",
      "Tony Xiang",
      "Timothy Hospedales"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper046/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper046/paper046.pdf",
    "published": "2017-09",
    "summary": "The key challenge for learning a \ufb01ne-grained sketch-based image retrieval (FG-SBIR) model is to bridge the domain gap between photo and sketch. Existing models learn a deep joint embedding space with discriminative losses where a photo and a sketch can be compared. In this paper, we propose a novel discriminative-generative hybrid model by introducing a generative task of cross-domain image synthesis. This task enforces the learned embedding space to preserve all the domain invariant information that is useful for cross-domain reconstruction, thus explicitly reducing the domain gap as opposed to existing models.",
    "code_link": ""
  },
  "bmvc2017_main_epipolarplanediffusionanefficientapproachforlightfieldediting": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Epipolar Plane Diffusion: An Efficient Approach for Light Field Editing",
    "authors": [
      "Oriel Frigo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper047/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper047/paper047.pdf",
    "published": "2017-09",
    "summary": "This paper presents a novel approach for light \ufb01eld editing. The problem of propagating an edit from a single view to the remaining light \ufb01eld is solved by a structure tensor driven diffusion on the epipolar plane images. The proposed method is shown to be useful for two applications: light \ufb01eld inpainting and recolorization. While the light \ufb01eld recolorization is obtained with a straightforward diffusion, the inpainting application is particularly challenging, as the structure tensors accounting for disparities are unknown under the occluding mask. We address this issue with a disparity inpainting by means of an interpolation constrained by superpixel boundaries.",
    "code_link": ""
  },
  "bmvc2017_main_groupcost-sensitiveboostingwithmulti-scaledecorrelatedfiltersforpedestriandetection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Group Cost-sensitive Boosting with Multi-scale Decorrelated Filters for Pedestrian Detection",
    "authors": [
      "Chengju Zhou",
      "Meiqing Wu",
      "SiewKei Lam"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper048/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper048/paper048.pdf",
    "published": "2017-09",
    "summary": "We propose a novel two-stage pedestrian detection framework that combines multiscale decorrelated \ufb01lters to extract more discriminative features and a novel group costsensitive boosting algorithm. The proposed boosting algorithm is based on mixture loss to alleviate the in\ufb02uence of annotation errors in training data and explores varying cost for different types of misclassi\ufb01cation. Experiments on Caltech and INRIA datasets show that the proposed framework achieves the best detection performance among all state-of-the-art non-deep learning methods.",
    "code_link": ""
  },
  "bmvc2017_main_movingobjectsegmentationinjitteryvideosbystabilizingtrajectoriesmodeledinkendallsshapespace": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Moving Object Segmentation in Jittery Videos by Stabilizing Trajectories Modeled in Kendall's Shape Space",
    "authors": [
      "Geethu Jacob",
      "Sukhendu Das"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper049/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper049/paper049.pdf",
    "published": "2017-09",
    "summary": "Moving Object Segmentation is a challenging task for jittery/wobbly videos. For jittery videos, the non-smooth camera motion makes discrimination between foreground objects and background layers hard to solve. While most recent works for moving video object segmentation fail in this scenario, our method generates an accurate segmentation of a single moving object. The proposed method performs a sparse segmentation, where frame-wise labels are assigned only to trajectory coordinates, followed by the pixel-wise labeling of frames. The sparse segmentation involving stabilization and clustering of trajectories in a 3-stage iterative process. At the 1st stage, the trajectories are clustered using pairwise Procrustes distance as a cue for creating an af\ufb01nity matrix. The 2nd stage performs a block-wise Procrustes analysis of the trajectories and estimates Frechet means (in Kendall\u2019s shape space) of the clusters. The Frechet means represent the average trajectories of the motion clusters. An optimization function has been formulated to stabilize the Frechet means, yielding stabilized trajectories at the 3rd stage. The accuracy of the motion clusters are iteratively re\ufb01ned, producing distinct groups of stabilized trajectories. Next, the labels obtained from the sparse segmentation are propagated for pixel-wise labeling of the frames, using a GraphCut based energy formulation. Use of Procrustes analysis and energy minimization in Kendall\u2019s shape space for moving object segmentation in jittery videos, is the novelty of this work. Second contribution comes from experiments performed on a dataset formed of 20 real-world natural jittery videos, with manually annotated ground truth. Experiments are done with controlled levels of arti\ufb01cial jitter on videos of SegTrack2 dataset.",
    "code_link": ""
  },
  "bmvc2017_main_urbanimagestitchingusingplanarperspectiveguidance": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Urban Image Stitching using Planar Perspective Guidance",
    "authors": [
      "Joo Ho Ho",
      "Seung-Hwan Baek",
      "Min H. Kim"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper050/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper050/paper050.pdf",
    "published": "2017-09",
    "summary": "Image stitching methods with spatially-varying homographies have been proposed to overcome partial misalignments caused by global perspective projection; however, local warp operators often fracture the coherence of linear structures, resulting in an inconsistent perspective. In this paper, we propose an image stitching method that warps a source image to a target image by local projective warps using planar perspective guidance. We \ufb01rst detect line structures that converge into three vanishing points, yielding line-cluster probability functions for each vanishing point. Then we estimate local homographies that account for planar perspective guidance from the joint probability of planar guidance, in addition to spatial coherence. This allows us to enhance linear perspective structures while warping multiple urban images with grid-like structures.",
    "code_link": ""
  },
  "bmvc2017_main_detectingpartsforactionlocalization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Detecting Parts for Action Localization",
    "authors": [
      "Nicolas Chesneau",
      "Gregory Rogez",
      "Karteek Alahari",
      "Cordelia Schmid"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper051/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper051/paper051.pdf",
    "published": "2017-09",
    "summary": "In this paper, we propose a new framework for action localization that tracks people in videos and extracts full-body human tubes, i.e., spatio-temporal regions localizing actions, even in the case of occlusions or truncations. This is achieved by training a novel human part detector that scores visible parts while regressing full-body bounding boxes. The core of our method is a convolutional neural network which learns part proposals speci\ufb01c to certain body parts. These are then combined to detect people robustly in each frame. Our tracking algorithm connects the image detections temporally to extract full-body human tubes.",
    "code_link": ""
  },
  "bmvc2017_main_cascadedboundaryregressionfortemporalactiondetection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Cascaded Boundary Regression for Temporal Action Detection",
    "authors": [
      "Jiyang Gao",
      "Zhenheng Yang",
      "Ram Nevatia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper052/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper052/paper052.pdf",
    "published": "2017-09",
    "summary": "Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classi\ufb01ers on sliding windows. Although sliding windows may contain an identi\ufb01able portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and speci\ufb01c actions are detected respec- tively in the \ufb01rst and the second stage. CBR uses temporal coordinate regression to re\ufb01ne the temporal boundaries of the sliding windows. The salient aspect of the re\ufb01nement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the re\ufb01ned windows back to the system for further boundary re\ufb01nement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%",
    "code_link": ""
  },
  "bmvc2017_main_confidenceanddiversityforactiveselectionoffeedbackinimageretrieval": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Confidence and Diversity for Active Selection of Feedback in Image Retrieval",
    "authors": [
      "Bhavin Modi",
      "Adriana Kovashka"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper053/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper053/paper053.pdf",
    "published": "2017-09",
    "summary": "Image search is a challenging problem because of the need to model any concept the user might want to retrieve. One recent solution to the problem allows the user to give feedback on the current set of results, by answering questions about how the relative attributes of individual returned images relate to his/her target image. We show how to ask more informative questions. In our active selection formulation that determines about which attribute the system should next ask a question, we account for the con\ufb01dence of relative attribute models. In addition to asking about reliably modeled attributes, the system is also encouraged to ask diverse questions, by computing question diversity on both the attribute and image levels. We show that both of our novel active selection criteria, con\ufb01dence and diversity, help improve search results on three datasets.",
    "code_link": ""
  },
  "bmvc2017_main_shapegenerationusingspatiallypartitionedpointclouds": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Shape Generation using Spatially Partitioned Point Clouds",
    "authors": [
      "Matheus Gadelha",
      "Subhransu Maji",
      "Rui Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper054/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper054/paper054.pdf",
    "published": "2017-09",
    "summary": "We propose a method to generate 3D shapes using point clouds. Given a point-cloud representation of a 3D shape, our method builds a kd-tree to spatially partition the points. This orders them consistently across all shapes, resulting in reasonably good correspondences across all shapes. We then use PCA analysis to derive a linear shape basis across the spatially partitioned points, and optimize the point ordering by iteratively minimizing the PCA reconstruction error. Even with the spatial sorting, the point clouds are inherently noisy and the resulting distribution over the shape coef\ufb01cients can be highly multi-modal. We propose to use the expressive power of neural networks to learn a distribution over the shape coef\ufb01cients in a generative-adversarial framework. Compared to 3D shape generative models trained on voxel-representations, our point-based method is considerably more light-weight and scalable, with little loss of quality. It also out-performs simpler linear factor models such as Probabilistic PCA, both qualitatively and quantitatively, on a number of categories from the ShapeNet dataset.",
    "code_link": ""
  },
  "bmvc2017_main_personre-identificationbylocalizingdiscriminativeregions": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Person Re-Identification by Localizing Discriminative Regions",
    "authors": [
      "Tanzila Rahman",
      "Mrigank Rochan",
      "Yang Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper055/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper055/paper055.pdf",
    "published": "2017-09",
    "summary": "Person re-identi\ufb01cation is a challenging task of matching a person\u2019s image across multiple images captured from different camera views. Recently, deep learning based approaches have been proposed that show promising performance on this task. However, most of these approaches use whole image features to compute the similarity between images. This is not very intuitive since not all the regions in an image contain information about the person identity. In this paper, we introduce an end-to-end Siamese convolutional neural network that \ufb01rstly localizes discriminative salient image regions and then computes the similarity based on these image regions in conjunction with the whole image. We use Spatial Transformer Networks (STN) for localizing salient regions.",
    "code_link": ""
  },
  "bmvc2017_main_adaptingobjectdetectorsfromimagestoweaklylabeledvideos": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Adapting Object Detectors from Images to Weakly Labeled Videos",
    "authors": [
      "Omit Chanda",
      "Eu Wern Teh",
      "Mrigank Rochan",
      "Zhenyu Guo",
      "Yang Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper056/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper056/paper056.pdf",
    "published": "2017-09",
    "summary": "Due to the domain shift between images and videos, standard object detectors trained on images usually do not perform well on videos. At the same time, it is dif\ufb01cult to directly train object detectors from video data due to the lack of labeled video datasets. In this paper, we consider the problem of localizing objects in weakly labeled videos. A video is weakly labeled if we know the presence/absence of an object in a video (or each frame), but we do not know the exact spatial location. In addition to weakly labeled videos, we assume access to a set of fully labeled images. We incorporate domain adaptation in our framework and adapt the information from the labeled images (source domain) to the weakly labeled videos (target domain). Our experimental results on standard benchmark datasets demonstrate the effectiveness of our proposed approach.",
    "code_link": ""
  },
  "bmvc2017_main_bayesiansegnetmodeluncertaintyindeepconvolutionalencoder-decoderarchitecturesforsceneunderstanding": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding",
    "authors": [
      "Alex Kendall",
      "Vijay Badrinarayanan",
      "Roberto Cipolla"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper057/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper057/paper057.pdf",
    "published": "2017-09",
    "summary": "We present a deep learning framework for probabilistic pixelwise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty using Bayesian deep learning. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels.",
    "code_link": ""
  },
  "bmvc2017_main_depthcompreal-timedepthimagecompletionbasedonpriorsemanticscenesegmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "DepthComp: Real-time Depth Image Completion Based on Prior Semantic Scene Segmentation",
    "authors": [
      "Amir Atapour Atapour",
      "Toby Breckon"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper058/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper058/paper058.pdf",
    "published": "2017-09",
    "summary": "We address plausible hole \ufb01lling in depth images in a computationally lightweight methodology that leverages recent advances in semantic scene segmentation. Firstly, we perform such segmentation over a co-registered color image, commonly available from stereo depth sources, and non-parametrically \ufb01ll missing depth values based on a multipass basis within each semantically labeled scene object. Within this formulation, we identify a bounded set of explicit completion cases in a grammar inspired context that can be performed effectively and ef\ufb01ciently to provide highly plausible localized depth continuity via a case-speci\ufb01c non-parametric completion approach. Results demonstrate that this approach has complexity and ef\ufb01ciency comparable to conventional interpolation techniques but with accuracy analogous to contemporary depth \ufb01lling approaches.",
    "code_link": ""
  },
  "bmvc2017_main_deepeyecontactdetectorrobusteyecontactbiddetectionusingconvolutionalneuralnetwork": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "DEEP eye contact detector: Robust eye contact bid detection using convolutional neural network",
    "authors": [
      "Yu Mitsuzumi",
      "Atsushi Nakazawa",
      "Toyoaki Nishida"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper059/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper059/paper059.pdf",
    "published": "2017-09",
    "summary": "Eye contact (mutual gaze) is a foundation of human communication and social interactions; therefore, it is studied in many \ufb01elds such as psychology, social science, and medicine. To support these application of eye-contact detection, much effort has been made for automated eye-contact detection by using image recognition techniques; however, they are dif\ufb01cult to use when facial-landmark tracking is not possible due to facial occlusions. To solve this issue, this paper proposes robust algorithms of detecting eye contacts leveraged using deep neural net to \ufb01nd the eye-contact skills needed for care-givers in dementia care. Since our algorithms do not depend on facial-landmark tracking and only use the images around the eyes, they are robust against facial occlusions and/or image noise. We prepared two eye contact facial image datasets and con\ufb01rmed the performance of the proposed algorithms. The results show that our method are robust against facial occlusion in which a person is wearing a facemask or the person\u2019s face is partially outside the camera viewing area.",
    "code_link": ""
  },
  "bmvc2017_main_bisegsimultaneousinstancesegmentationandsemanticsegmentationwithfullyconvolutionalnetworks": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "BiSeg: Simultaneous Instance Segmentation and Semantic Segmentation with Fully Convolutional Networks",
    "authors": [
      "Viet Pham",
      "Satoshi Ito",
      "Tatsuo Kozakaya"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper060/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper060/paper060.pdf",
    "published": "2017-09",
    "summary": "We present a simple and effective framework for simultaneous semantic segmentation and instance segmentation with Fully Convolutional Networks (FCNs). The method, called BiSeg, predicts instance segmentation as a posterior in Bayesian inference, where semantic segmentation is used as a prior. We extend the idea of position-sensitive score maps used in recent methods to a fusion of multiple score maps at different scales and partition modes, and adopt it as a robust likelihood for instance segmentation inference. As both Bayesian inference and map fusion are performed per pixel, BiSeg is a fully convolutional end-to-end solution that inherits all the advantages of FCNs.",
    "code_link": ""
  },
  "bmvc2017_main_salientobjectdetectionusingacontext-awarerefinementnetwork": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Salient Object Detection using a Context-Aware Refinement Network",
    "authors": [
      "Md Amirul Amirul",
      "Mahmoud Kalash",
      "Mrigank Rochan",
      "Neil Bruce",
      "Yang Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper061/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper061/paper061.pdf",
    "published": "2017-09",
    "summary": "Recently there has been remarkable success in pushing the state of the art in salient object detection. Most of the improvements are driven by employing end-to-end deeper feed-forward networks. However, in many cases precisely detecting salient regions requires representation of \ufb01ne details. Combining high-level and low-level features using skip connections is a strategy that has been proposed, but sometimes fails to select the right contextual features. To overcome this limitation, we propose an end-to-end encoder-decoder network that employs recurrent re\ufb01nement to generate a saliency map in a coarse-to-\ufb01ne fashion by incorporating \ufb01ner details in the detection framework. The proposed approach makes use of re\ufb01nement units within each stage of the decoder that are responsible for re\ufb01ning the saliency map produced by earlier layers by learning context-aware features.",
    "code_link": ""
  },
  "bmvc2017_main_visualodometrywithdrift-freerotationestimationusingindoorsceneregularities": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Visual Odometry with Drift-Free Rotation Estimation Using Indoor Scene Regularities",
    "authors": [
      "Pyojin Kim",
      "Brian Coltin",
      "Hyounjin Kim"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper062/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper062/paper062.pdf",
    "published": "2017-09",
    "summary": "We propose a hybrid visual odometry algorithm to achieve accurate and low-drift state estimation by separately estimating the rotational and translational camera motion. Previous methods usually estimate the six degrees of freedom camera motion jointly without distinction between rotational and translational motion. However, inaccuracy in the rotation estimate is a main source of drift in visual odometry. We design a hybrid visual odometry algorithm which separately estimates the rotational and translational motion to achieve improved accuracy and low drift error. To improve the accuracy of rotational motion estimation, we exploit orthogonal planar structures, such as walls, \ufb02oors, and ceilings, common in man-made environments. We track orthogonal frames with an ef\ufb01cient SO(3)-constrained mean-shift algorithm, resulting in drift-free rotation estimates. Based on the absolute camera orientation, we newly propose a way to compute the translational motion by minimizing the de-rotated reprojection error with the tracked features.",
    "code_link": ""
  },
  "bmvc2017_main_oneforalladaptivelearning-basedtemporaltrackerfor3dheadshapemodels": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "One For All: Adaptive Learning-based Temporal Tracker for 3D Head Shape Models",
    "authors": [
      "David Joseph Joseph",
      "Federico Tombari",
      "Nassir Navab"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper063/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper063/paper063.pdf",
    "published": "2017-09",
    "summary": "We propose a learning-based 3D temporal tracker that estimates the orientation and location of the head in the 3D scene. The algorithm is based on random forest that learns the 6D pose from a class of head models. A unique attribute of our approach is the capacity to adapt the learned tracker for a speci\ufb01c user, i.e., after learning, the tracker can deform the shape of the learned model to a speci\ufb01c instance of the class in order to match the user\u2019s head shape. To \ufb01nd the user\u2019s head shape model, we use a fast calibration method to personalize the model for a speci\ufb01c user. As a consequence, this technique enhances the accuracy of the head pose estimation as the personalized model becomes more detailed, and tracks at 1.",
    "code_link": ""
  },
  "bmvc2017_main_dominantsetclusteringandpoolingformulti-view3dobjectrecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Dominant Set Clustering and Pooling for Multi-View 3D Object Recognition",
    "authors": [
      "Chu Wang",
      "Marcello Pelillo",
      "Kaleem Siddiqi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper064/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper064/paper064.pdf",
    "published": "2017-09",
    "summary": "View based strategies for 3D object recognition have proven to be very successful. The state-of-the-art methods now achieve over 90% correct category level recognition performance on appearance images. We improve upon these methods by introducing a view clustering and pooling layer based on dominant sets. The key idea is to pool information from views which are similar and thus belong to the same cluster. The pooled feature vectors are then fed as inputs to the same layer, in a recurrent fashion. This recurrent clustering and pooling module, when inserted in an off-the-shelf pretrained CNN, boosts performance for multi-view 3D object recognition, achieving a new state of the art test set recognition accuracy of 93.8% on the ModelNet 40 database. We also explore a fast approximate learning strategy for our cluster-pooling CNN, which, while sacri\ufb01cing end-to-end learning, greatly improves its training ef\ufb01ciency with only a slight reduction of recognition accuracy to 93.3%. Our implementation is available at https://github.com/fate3439/dscnn.",
    "code_link": "https://github.com/fate3439/dscnn"
  },
  "bmvc2017_main_ageneralisedformulationforcollaborativerepresentationofimagepatches(gp-crc)": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A Generalised Formulation for Collaborative Representation of Image Patches (GP-CRC)",
    "authors": [
      "Tapabrata Chakraborti",
      "Brendan McCane",
      "Steven Mills",
      "Umapada Pal"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper065/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper065/paper065.pdf",
    "published": "2017-09",
    "summary": "Collaborative Representation based Categorization (CRC) represents query samples collaboratively as an optimal weighted average of training samples across all classes. It has been shown to be e\ufb00ective for recognition problems, but its performance degrades in presence of high variation in image background. We present a generalization mathematical reformulation of a patch based CRC approach. The proposed method (GP-CRC) analytically overcomes the problem in the cost function itself and provides a closed form solution. Experiments are carried out on two face recognition (AR and LFW) and two species recognition (Oxford-102 Flowers and Oxford-IIIT Pets) benchmarks. The proposed method outperforms the original CRC as well as basic patch based CRC consistently across all the datasets (with statistical signi\ufb01cance in majority of the cases) and comparable or marginally higher accuracy than the state of the art probabilistic CRC.",
    "code_link": ""
  },
  "bmvc2017_main_deepshoeamulti-taskview-invariantcnnforstreet-to-shopshoeretrieval": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "DeepShoe: A Multi-Task View-Invariant CNN for Street-to-Shop Shoe Retrieval",
    "authors": [
      "Huijing Zhan",
      "boxin shi",
      "ALEX KOT"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper066/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper066/paper066.pdf",
    "published": "2017-09",
    "summary": "In this paper we aim to \ufb01nd exactly the same shoes from the online shop (shop scenario), given a daily shoe photo (street scenario). There are large visual differences between the street and shop scenario shoe images. To handle the discrepancy of different scenarios, we learn a feature embedding for shoes via a multi-task view-invariant convolutional neural network (MTV-CNN), the feature activations of which re\ufb02ect the inherent similarity between any two shoe images. Speci\ufb01cally, we propose a new loss function that minimizes the distances between images of the same shoes captured from different viewpoints. To train the proposed MTV-CNN in a more ef\ufb01cient way, an attribute-based hard example weighting and mining strategy is developed to smartly select the hard negative examples. To evaluate the performance of the proposed MTV-CNN, we collect a large-scale multi-view shoe dataset with semantic attributes (MVShoe) from the daily life and online shopping websites.",
    "code_link": ""
  },
  "bmvc2017_main_weaklysupervisedsaliencydetectionwithacategory-drivenmapgenerator": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Weakly Supervised Saliency Detection with A Category-Driven Map Generator",
    "authors": [
      "Kuang-Jui Hsu",
      "Yen-Yu Lin",
      "Yung-Yu Chuang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper067/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper067/paper067.pdf",
    "published": "2017-09",
    "summary": "Top-down saliency detection aims to highlight the regions of a speci\ufb01c object category, and typically relies on pixel-wise annotated training data. In this paper, we address the high cost of collecting such training data by presenting a weakly supervised approach to object saliency detection, where only image-level labels, indicating the presence or absence of a target object in an image, are available. The proposed framework is composed of two deep modules, an image-level classi\ufb01er and a pixel-level map generator. While the former distinguishes images with objects of interest from the rest, the latter is learned to generate saliency maps so that the training images masked by the maps can be better predicted by the former. In addition to the top-down guidance from class labels, the map generator is derived by also referring to other image information, including the background prior, area balance and spatial consensus. This information greatly regularizes the training process and reduces the risk of over\ufb01tting, especially when learning deep models with few training data.",
    "code_link": ""
  },
  "bmvc2017_main_saliencydetectionbycompactnessdiffusion": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Saliency Detection by Compactness Diffusion",
    "authors": [
      "Qi Zheng",
      "Peng Zhang",
      "Xinge You"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper068/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper068/paper068.pdf",
    "published": "2017-09",
    "summary": "Most existing methods of salient object segmentation only focus on foreground cues such as contrast, or background cues such as boundary connectivity. Another problem is that they have used redundant information to generate an acceptable saliency map such as variances in different color spaces, multi-scale features and so on. In this paper, we propose saliency detecting with a diffusion model; use optimal seeds generated from foreground statistic cue, i.e., the compactness. Each superpixel is considered as a node and a fully connected graph is constructed to calculate the global compactness of each node. Then the local connected graph is constructed by only considering adjacent nodes, and compactness is diffused by applying a quadratic energy model to generate a coarse saliency map. After that, boundary prior is combined with the coarse saliency map for further eliminating the background. Experiments on three benchmark datasets including MSRA 1000, ECSSD and DUT-OMRON show that compared with other seven state-of-the-art methods, our model achieves stable and excellent performance.",
    "code_link": ""
  },
  "bmvc2017_main_weakly-supervisedlearningofmid-levelfeaturesforpedestrianattributerecognitionandlocalization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Weakly-supervised Learning of Mid-level Features for Pedestrian Attribute Recognition and Localization",
    "authors": [
      "Yang Zhou",
      "Kai Yu",
      "Biao Leng",
      "zhang Zhang",
      "Dangwei Li",
      "Kaiqi Huang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper069/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper069/paper069.pdf",
    "published": "2017-09",
    "summary": "Most existing methods for pedestrian attribute recognition in video surveillance can be formulated as a multi-label image classi\ufb01cation methodology, while attribute localization is usually disregarded due to the low image qualities and large variations of camera viewpoints and human poses. In this paper, we propose a weakly-supervised learning based approaching to implementing multi-attribute classi\ufb01cation and localization simultaneously, without the need of bounding box annotations of attributes. Firstly, a set of mid-level attribute features are discovered by a multi-scale attribute-aware module receiving the outputs of multiple inception layers in a deep Convolution Neural Network (CNN) e.g., GoogLeNet, where a Flexible Spatial Pyramid Pooling (FSPP) operation is performed to acquire the activation maps of attribute features. Subsequently, attribute labels are predicted through a fully-connected layer which performs the regression between the response magnitudes in activation maps and the image-level attribute annotations. Finally, the locations of pedestrian attributes can be inferred by fusing the multiple activation maps, where the fusion weights are estimated as the correlation strengths between attributes and relevant mid-level features. To validate the proposed approach, extensive experiments are performed on the two currently largest pedestrian attribute datasets, i.e. the PETA dataset [4] and the RAP dataset [10]. In comparison with other state-of-theart methods, competitive performance on attribute classification can be achieved. The additional capability of attribute localization is also evaluated.",
    "code_link": ""
  },
  "bmvc2017_main_ast-netanattribute-basedsiamesetemporalnetworkforreal-timeemotionrecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "AST-Net: An Attribute-based Siamese Temporal Network for Real-Time Emotion Recognition",
    "authors": [
      "Shu-Hui Wang",
      "Chiou-Ting Hsu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper070/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper070/paper070.pdf",
    "published": "2017-09",
    "summary": "Predicting continuous facial emotions is essential to many applications in human-computer interaction. In this paper, we focus on predicting the two dimensional emotions: valence and arousal, to interpret the dynamically yet subtly changed facial emotions. We propose an Attribute-based Siamese Temporal Network (AST-Net), which includes a discrete emotion CNN model and a Stacked-LSTM, to incorporate both the spatial facial attributes and the long-term dynamics into the prediction. The discrete emotion CNN model aims to extract attribute-related but pose- and identity-invariant features; and the Stacked-LSTM is used to characterize the dynamic dependency along the temporal domain. Furthermore, in order to stabilize the training procedure and also to derive a smoother and reliable long-term prediction, we propose to jointly learn the model from two temporally-shifted videos under the Siamese network architecture. Experimental results on AVEC2012 dataset show that the proposed AST-Net not only processes in real time (40.1 frames per second) but also achieves the state-of-the-art performance even when using the vision modality alone.",
    "code_link": ""
  },
  "bmvc2017_main_combinedinternalandexternalcategory-specificimagedenoising": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Combined Internal and External Category-Specific Image Denoising",
    "authors": [
      "Saeed Anwar",
      "Cong Huynh",
      "Fatih Porikli"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper071/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper071/paper071.pdf",
    "published": "2017-09",
    "summary": "In this paper, we present a category-speci\ufb01c image denoising algorithm that exploits patch similarity within the input image and between the input image and an external dataset. We rely on standard internal denoising for smooth regions while consulting external images in the same category as the input to denoise textured regions. The external denoising component estimates the latent patches using the statistics, i.e. means and covariance matrices, of external patches, subject to a low-rank constraint. In the \ufb01nal stage, we aggregate results of internal and external denoising using a weighting rule based on the patch SNR measure.",
    "code_link": ""
  },
  "bmvc2017_main_temporalperceptivenetworkforskeleton-basedactionrecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Temporal Perceptive Network for Skeleton-Based Action Recognition",
    "authors": [
      "Yueyu Hu",
      "Chunhui Liu",
      "Yanghao Li",
      "Jiaying Liu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper072/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper072/paper072.pdf",
    "published": "2017-09",
    "summary": "The major challenge for skeleton-based action recognition is to distinguish the difference between various actions. Traditional Recurrent Neural Network (RNN) structure may lead to unsatisfactory results due to the inef\ufb01ciency in capturing local temporal features, especially for large-scale datasets. To address this issue, we propose a novel Temporal Perceptive Network (TPNet) to enable the robust feature learning for action recognition. We design a temporal convolutional subnetwork, which can be embedded between RNN layers, to enhance automatical feature extraction for local temporal dynamics. Experiments show that the proposed method achieves superior performance to other methods and generates new state-of-the-art results.",
    "code_link": ""
  },
  "bmvc2017_main_detectingsemanticpartsonpartiallyoccludedobjects": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Detecting Semantic Parts on Partially Occluded Objects",
    "authors": [
      "Jianyu Wang",
      "Zhishuai Zhang",
      "Cihang Xie",
      "Jun Zhu",
      "Lingxi Xie",
      "Alan Yuille"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper073/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper073/paper073.pdf",
    "published": "2017-09",
    "summary": "In this paper, we address the task of detecting semantic parts on partially occluded objects. We consider a scenario where the model is trained using non-occluded images but tested on occluded images. The motivation is that there are in\ufb01nite number of occlusion patterns in real world, which cannot be fully covered in the training data. So the models should be inherently robust and adaptive to occlusions instead of \ufb01tting / learning the occlusion patterns in the training data. Our approach detects semantic parts by accumulating the con\ufb01dence of local visual cues. Speci\ufb01cally, the method uses a simple voting method, based on log-likelihood ratio tests and spatial constraints, to combine the evidence of local cues. These cues are called visual concepts, which are derived by clustering the internal states of deep networks. We evaluate our voting scheme on the VehicleSemanticPart dataset with dense part annotations. We randomly place two, three or four irrelevant objects onto the target object to generate testing images with various occlusions.",
    "code_link": ""
  },
  "bmvc2017_main_placerecognitioninsemi-densemaps": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Place Recognition in Semi-Dense Maps",
    "authors": [
      "Yawei Ye",
      "Titus Cieslewski",
      "Antonio Loquercio",
      "Davide Scaramuzza"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper074/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper074/paper074.pdf",
    "published": "2017-09",
    "summary": "For robotics and augmented reality systems operating in large and dynamic environments, place recognition and tracking using vision represent very challenging tasks. Additionally, when these systems need to reliably operate for very long time periods, such as months or years, further challenges are introduced by severe environmental changes, that can signi\ufb01cantly alter the visual appearance of a scene. Thus, to unlock long term, large scale visual place recognition, it is necessary to develop new methodologies for improving localization under dif\ufb01cult conditions. As shown in previous work, gains in robustness can be achieved by exploiting the 3D structural information of a scene. The latter, extracted from image sequences, carries in fact more discriminative clues than individual images only. In this paper, we propose to represent a scene\u2019s structure with semi-dense point clouds, due to their highly informative power, and the simplicity of their generation through mature visual odometry and SLAM systems. Then we cast place recognition as an instance of pose retrieval and evaluate several techniques, including recent learning based approaches, to produce discriminative descriptors of semi-dense point clouds. Our proposed methodology, evaluated on the recently published and challenging Oxford Robotcar Dataset, shows to outperform image-based place recognition, with improvements up to 30% in precision across strong appearance changes.",
    "code_link": ""
  },
  "bmvc2017_main_imagecompletionwithintrinsicreflectanceguidance": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Image Completion with Intrinsic Reflectance Guidance",
    "authors": [
      "Soomin Kim",
      "Taeyoung Kim",
      "Min H. Kim",
      "Sung-Eui Yoon"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper075/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper075/paper075.pdf",
    "published": "2017-09",
    "summary": "Patch-based image completion methods often fail in searching patch correspondences of similar materials due to shading caused by scene illumination, resulting in inappropriate image completion with dissimilar materials. We therefore present a novel image completion method that additionally accounts for intrinsic re\ufb02ectance of scene objects, when searching patch correspondences. Our method examines both intrinsic re\ufb02ectances and color image structures to avoid false correspondences of different materials so that our method can search and vote illumination-invariant patches robustly, allowing for image completion mainly with homogeneous materials.",
    "code_link": ""
  },
  "bmvc2017_main_enhancementofssdbyconcatenatingfeaturemapsforobjectdetection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Enhancement of SSD by concatenating feature maps for object detection",
    "authors": [
      "Jisoo Jeong",
      "Hyojin Park",
      "Nojun Kwak"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper076/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper076/paper076.pdf",
    "published": "2017-09",
    "summary": "We propose an object detection method that improves the accuracy of the conventional SSD (Single Shot Multibox Detector), which is one of the top object detection algorithms in both aspects of accuracy and speed. The performance of a deep network is known to be improved as the number of feature maps increases. However, it is dif\ufb01cult to improve the performance by simply raising the number of feature maps. In this paper, we propose and analyze how to use feature maps effectively to improve the performance of the conventional SSD. The enhanced performance was obtained by changing the structure close to the classi\ufb01er network, rather than growing layers close to the input data, e.g., by replacing VGGNet with ResNet. The proposed network is suitable for sharing the weights in the classi\ufb01er networks, by which property, the training can be faster with better generalization power. For the Pascal VOC 2007 test set trained with VOC 2007 and VOC 2012 training sets, the proposed network with the input size of 300\u00d7 300 achieved 78.5% mAP (mean average precision) at the speed of 35.0 FPS (frame per second), while the network with a 512\u00d7 512 sized input achieved 80.8% mAP at 16.6 FPS using Nvidia Titan X GPU. The proposed network shows state-of-the-art mAP, which is better than those of the conventional SSD, YOLO, Faster-RCNN and RFCN.",
    "code_link": ""
  },
  "bmvc2017_main_colourconstancybiologically-inspiredcontrastvariantpoolingmechanism": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Colour Constancy: Biologically-inspired Contrast Variant Pooling Mechanism",
    "authors": [
      "Arash Akbarinia",
      "Raquel Gil Rodr\u00edguez",
      "C. Alejandro Parraga"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper077/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper077/paper077.pdf",
    "published": "2017-09",
    "summary": "Pooling is a ubiquitous operation in image processing algorithms that allows for higher-level processes to collect relevant low-level features from a region of interest. Currently, max-pooling is one of the most commonly used operators in the computational literature. However, it can lack robustness to outliers due to the fact that it relies merely on the peak of a function. Pooling mechanisms are also present in the primate visual cortex where neurons of higher cortical areas pool signals from lower ones. The receptive \ufb01elds of these neurons have been shown to vary according to the contrast by aggregating signals over a larger region in the presence of low contrast stimuli. We hypothesise that this contrast-variant-pooling mechanism can address some of the shortcomings of max-pooling. We modelled this contrast variation through a histogram clipping in which the percentage of pooled signal is inversely proportional to the local contrast of an image. We tested our hypothesis by applying it to the phenomenon of colour constancy where a number of popular algorithms utilise a max-pooling step (e.g. White-Patch, Grey-Edge and Double-Opponency). For each of these methods, we investigated the consequences of replacing their original max-pooling by the proposed contrast-variant-pooling.",
    "code_link": ""
  },
  "bmvc2017_main_superpixel-basedsemanticsegmentationtrainedbystatisticalprocesscontrol": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Superpixel-based semantic segmentation trained by statistical process control",
    "authors": [
      "Hyojin Park",
      "Jisoo Jeong",
      "Youngjoon Yoo",
      "Nojun Kwak"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper078/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper078/paper078.pdf",
    "published": "2017-09",
    "summary": "Semantic segmentation, like other \ufb01elds of computer vision, has seen a remarkable performance advance by the use of deep convolution neural networks. However, considering that neighboring pixels are heavily dependent on each other, both learning and testing of these methods have a lot of redundant operations. To resolve this problem, the proposed network is trained and tested with only 0.37% of total pixels by superpixel-based sampling and largely reduced the complexity of upsampling calculation. The hypercolumn feature maps are constructed by pyramid module in combination with the convolution layers of the base network. Since the proposed method uses a very small number of sampled pixels, the end-to-end learning of the entire network is dif\ufb01cult with a common learning rate for all the layers. In order to resolve this problem, the learning rate after sampling is controlled by statistical process control (SPC) of gradients in each layer.",
    "code_link": ""
  },
  "bmvc2017_main_robustpixel-wisedehazingalgorithmbasedonadvancedhaze-relevantfeatures": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Robust Pixel-wise Dehazing Algorithm based on Advanced Haze-Relevant Features",
    "authors": [
      "Guisik Kim",
      "Junseok Kwon"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper079/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper079/paper079.pdf",
    "published": "2017-09",
    "summary": "The dehazing algorithm aims to remove haze from an image and has been widely used as a pre-processing step in several computer vision applications. The performance of the computer vision algorithm, however, is signi\ufb01cantly affected by weather conditions. Hence, conventional algorithms do not work well when weather conditions vary severely over time. This paper proposes an effective haze removal algorithm based on a single image, which is robust to the varying weather conditions. Unlike conventional methods, which estimate \"global\" atmospheric light, we \ufb01nd the \"local\" atmospheric light by assuming that it is absorbed and emitted differently at each atmospheric particle. We divide the image into two components, namely, illumination and re\ufb02ection, according to the retinex theory, and obtain the pixel-wise atmospheric light from the illumination. To estimate an accurate transmission, we initialize it by using a combination of haze-relevant features that have been proven experimentally to be highly correlated with haze. The aforementioned processes interact with each other in producing accurate dehazing results. We solve the dehazing problem via convex optimization and obtain the optimal solution. The experimental results demonstrate that the proposed algorithm outperforms state-of-the-art methods, using the benchmark dataset with regard to contrast, detail, and visibility.",
    "code_link": ""
  },
  "bmvc2017_main_generaldeepimagecompletionwithlightweightconditionalgenerativeadversarialnetworks": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "General Deep Image Completion with Lightweight Conditional Generative Adversarial Networks",
    "authors": [
      "Ching-Wei Tseng",
      "Hung Jin Lin",
      "Shang-Hong Lai"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper080/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper080/paper080.pdf",
    "published": "2017-09",
    "summary": "Recent image completion researches using deep neural networks approaches have shown remarkable progress by using generative adversarial networks (GANs). However, these approaches still have the problems of large model sizes and lack of generality for various types of corruptions. In addition, the conditional GANs often suffer from the mode collapse and unstable training problems. In this paper, we overcome these shortcomings in the previous models by proposing a lightweight model of conditional GANs with integrating a stable way in adversarial training. Moreover, we present a new training strategy to trigger the model to learn how to complete different types of corruptions or missing regions in images. Experimental results demonstrate qualitatively and quantitatively that the proposed model provides signi\ufb01cant improvement over a number of representative image completion methods on public datasets.",
    "code_link": ""
  },
  "bmvc2017_main_augmentedrealitymeetsdeeplearning": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Augmented Reality meets Deep Learning",
    "authors": [
      "Hassan Abu Abu",
      "Siva Karthik Mustikovela",
      "Lars Mescheder",
      "Andreas Geiger",
      "Carsten Rother"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper081/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper081/paper081.pdf",
    "published": "2017-09",
    "summary": "The success of deep learning in computer vision is based on the availability of large annotated datasets. To lower the need for hand labeled images, virtually rendered 3D worlds have recently gained popularity. Unfortunately, creating realistic 3D content is challenging on its own and requires signi\ufb01cant human effort. In this work, we propose an alternative paradigm which combines real and synthetic data for learning semantic instance segmentation models. Exploiting the fact that not all aspects of the scene are equally important for this task, we propose to augment real-world imagery with virtual objects of the target category. Capturing real-world images at large scale is easy and cheap, and directly provides real background appearances without the need for creating complex 3D models of the environment. We present an ef\ufb01cient procedure to augment these images with virtual objects. This allows us to create realistic composite images which exhibit both realistic background appearance as well as a large number of complex object arrangements. In contrast to modeling complete 3D environments, our data augmentation approach requires only a few user interactions in combination with 3D shapes of the target object category. We demonstrate the utility of the proposed approach for training a state-of-the-art high-capacity deep model for semantic instance segmentation. In particular, we consider the task of segmenting car instances on the KITTI dataset which we have annotated with pixel-accurate ground truth.",
    "code_link": ""
  },
  "bmvc2017_main_improvedir-colorizationusingadversarialtrainingandestuarynetworks": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Improved IR-Colorization using Adversarial Training and Estuary Networks",
    "authors": [
      "Matthias Limmer",
      "Hendrik Lensch"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper082/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper082/paper082.pdf",
    "published": "2017-09",
    "summary": "This paper investigates deep learning based image translation techniques to perform a colorization of near-infrared (NIR) images. Current deep neural network design patterns like inception modules, skip-connections or multi-scalar processing are combined and examined. Adversarial training is used to improve the realism of the resulting image in the translated domain. The presented approach is trained and evaluated on NIR-RGB image pairs from a real-world dataset containing a large amount of road scene images in summer.",
    "code_link": ""
  },
  "bmvc2017_main_realtimenovelviewsynthesiswitheigen-textureregression": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Realtime Novel View Synthesis with Eigen-Texture Regression",
    "authors": [
      "Yuta Nakashima",
      "Fumio Okura",
      "Norihiko Kawai",
      "ryosuke Kimura",
      "Hiroshi Kawasaki",
      "Katsushi Ikeuchi",
      "Ambrosio Blanco"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper083/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper083/paper083.pdf",
    "published": "2017-09",
    "summary": "Realtime novel view synthesis, which generates a novel view of a real object or scene in realtime, enjoys a wide range of applications including augmented reality, telepresence, and immersive telecommunication. Image-based rendering (IBR) with rough geometry can be done using only an off-the-shelf camera and thus can be used by many users. However, IBR from images in the wild (e.g., lighting condition changes or the scene contains objects with specular surfaces) has been a tough problem due to color discontinuity; IBR with rough geometry picks up appropriate images for a given viewpoint, but the image used for a rendering unit (a face or pixel) switches when the viewpoint moves, which may cause noticeable changes in color. We use the eigen-texture technique, which represents images for a certain face using a point in the eigenspace. We propose to regress a new point in this space, which moves smoothly, given a viewpoint so that we can generate an image whose color smoothly changes according to the point. Our regressor is based on a neural network with a single hidden layer and hyperbolic tangent nonlinearity.",
    "code_link": ""
  },
  "bmvc2017_main_humanactionrecognitionusingamulti-modalhybriddeeplearningmodel": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Human Action Recognition Using A Multi-Modal Hybrid Deep Learning Model",
    "authors": [
      "Hany El-Ghaish",
      "Mohamed Hussein",
      "Amin Shoukry"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper084/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper084/paper084.pdf",
    "published": "2017-09",
    "summary": "Human action recognition is a challenging problem, especially in the presence of multiple actors and/or multiple scene views. In this paper, multi-modal integration and a hybrid deep learning architecture are deployed in a uni\ufb01ed action recognition model. The model incorporates two main types of modalities: 3D skeletons and images, which together capture the two main aspects of an action, which are the body motion and part shape. Instead of a mere fusion of the two types of modalities, the proposed model integrates them by focusing on speci\ufb01c parts of the body, whose locations are known from the 3D skeleton data. The proposed model combines both Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) deep learning architectures into a hybrid one. The model is called MCL, for (M)ulti-Modal (C)NN + (L)STM. MCL consists of two sub-models: CL1D and CL2D that simultaneously extract the spatial and temporal patterns for the two sought input modality types. Their decisions are combined to achieve better accuracy. In order to show the ef\ufb01ciency of the MCL model, its performance is evaluated on the large NTU-RGB+D dataset in two different evaluation scenarios: cross-subject and cross-view. The obtained recognition rates, 74.2 % in cross-subject and 81.",
    "code_link": ""
  },
  "bmvc2017_main_probabilisticimagecolorization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Probabilistic Image Colorization",
    "authors": [
      "Amelie Royer",
      "Alexander Kolesnikov",
      "Christoph Lampert"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper085/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper085/paper085.pdf",
    "published": "2017-09",
    "summary": "We develop a probabilistic technique for colorizing grayscale natural images. In light of the intrinsic uncertainty of this task, the proposed probabilistic framework has numerous desirable properties. In particular, our model is able to produce multiple plausible and vivid colorizations for a given grayscale image and is one of the \ufb01rst colorization models to provide a proper stochastic sampling scheme. Moreover, our training procedure is supported by a rigorous theoretical framework that does not require any ad hoc heuristics and allows for ef\ufb01cient modeling and learning of the joint pixel color distribution.",
    "code_link": "https://github.com/ameroyer/PIC"
  },
  "bmvc2017_main_iteratedliftingforrobustcostoptimization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Iterated Lifting for Robust Cost Optimization",
    "authors": [
      "Christopher Zach",
      "Guillaume Bourmaud"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper086/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper086/paper086.pdf",
    "published": "2017-09",
    "summary": "Optimization of latent model parameters using robust formulations usually creates a large number of local minima due to the quasi-convex shape of the underlying robust kernel. Lifting the robust kernel, i.e. embedding the problem into a higher-dimensional space, leads to signi\ufb01cantly better local minima in a range of 3D computer vision problems (e.g. [10, 11, 12]). In this work we propose to iterate this lifting construction to obtain a more gradual lifting scheme for a given target kernel. Thus, a robust kernel is not directly lifted against the (non-robust) quadratic kernel, but initially against a different, less robust kernel. This process is iterated until the quadratic kernel is reached to allow utilization of ef\ufb01cient non-linear least-squares solvers.",
    "code_link": ""
  },
  "bmvc2017_main_slantedstixelsrepresentingsanfranciscossteepeststreets": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Slanted Stixels: Representing San Francisco's Steepest Streets",
    "authors": [
      "Daniel Hernandez-Juarez",
      "Lukas Schneider",
      "Antonio Espinosa",
      "Juan Moure",
      "David Vazquez",
      "Antonio L\u00f3pez",
      "Uwe Franke",
      "Marc Pollefeys"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper087/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper087/paper087.pdf",
    "published": "2017-09",
    "summary": "In this work we present a novel compact scene representation based on Stixels that infers geometric and semantic information. Our approach overcomes the previous rather restrictive geometric assumptions for Stixels by introducing a novel depth model to account for non-\ufb02at roads and slanted objects. Both semantic and depth cues are used jointly to infer the scene representation in a sound global energy minimization formulation. Furthermore, a novel approximation scheme is introduced that uses an extremely ef\ufb01cient over-segmentation. In doing so, the computational complexity of the Stixel inference algorithm is reduced signi\ufb01cantly, achieving real-time computation capabilities with only a slight drop in accuracy. We evaluate the proposed approach in terms of semantic and geometric accuracy as well as run-time on four publicly available benchmark datasets.",
    "code_link": ""
  },
  "bmvc2017_main_deformablepart-basedfullyconvolutionalnetworkforobjectdetection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Deformable Part-based Fully Convolutional Network for Object Detection",
    "authors": [
      "Taylor Mordan",
      "Nicolas Thome",
      "Gilles Henaff",
      "Matthieu Cord"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper088/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper088/paper088.pdf",
    "published": "2017-09",
    "summary": "Existing region-based object detectors are limited to regions with \ufb01xed box geometry to represent objects, even if those are highly non-rectangular. In this paper we introduce DP-FCN, a deep model for object detection which explicitly adapts to shapes of objects with deformable parts. Without additional annotations, it learns to focus on discriminative elements and to align them, and simultaneously brings more invariance for classification and geometric information to refine localization. DP-FCN is composed of three main modules: a Fully Convolutional Network to ef\ufb01ciently maintain spatial resolution, a deformable part-based RoI pooling layer to optimize positions of parts and build invariance, and a deformation-aware localization module explicitly exploiting displacements of parts to improve accuracy of bounding box regression. We experimentally validate our model and show signi\ufb01cant gains. DP-FCN achieves state-of-the-art performances of 83.1% and 80.",
    "code_link": ""
  },
  "bmvc2017_main_multipleinstancevisual-semanticembedding": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Multiple Instance Visual-Semantic Embedding",
    "authors": [
      "Zhou Ren",
      "Hailin Jin",
      "Zhe Lin",
      "Chen Fang",
      "Alan Yuille"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper089/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper089/paper089.pdf",
    "published": "2017-09",
    "summary": "Visual-semantic embedding models have been recently proposed and shown to be effective for image classi\ufb01cation and zero-shot learning. The key idea is that by directly learning a mapping from images into a semantic label space, the algorithm can generalize to a large number of unseen labels. However, existing approaches are limited to single-label embedding, handling images with multiple labels still remains an open problem, mainly due to the complex underlying correspondence between an image and its labels. In this work, we present a novel Multiple Instance Visual-Semantic Embedding (MIVSE) model for multi-label images. Instead of embedding a whole image into the semantic space, our model characterizes the subregion-to-label correspondence, which discovers and maps semantically meaningful image subregions to the corresponding labels.",
    "code_link": ""
  },
  "bmvc2017_main_thinkingoutsidetheboxspatialanticipationofsemanticcategories": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Thinking Outside the Box: Spatial Anticipation of Semantic Categories",
    "authors": [
      "Martin Garbade",
      "Juergen Gall"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper090/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper090/paper090.pdf",
    "published": "2017-09",
    "summary": "For certain applications like autonomous systems it is insuf\ufb01cient to interpret only the observed data. Instead, objects or other semantic categories, which are close but outside the \ufb01eld of view, need to be anticipated as well. In this work, we propose an approach for anticipating the semantic categories that surround the scene captured by a camera sensor. This task goes beyond current semantic labeling tasks since it requires to extrapolate a given semantic segmentation. Using the challenging Cityscapes dataset, we demonstrate how current deep learning architectures are able to learn this extrapolation from data.",
    "code_link": ""
  },
  "bmvc2017_main_real-timetemporalactionlocalizationinuntrimmedvideosbysub-actiondiscovery": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Real-Time Temporal Action Localization in Untrimmed Videos by Sub-Action Discovery",
    "authors": [
      "Rui Hou",
      "Rahul Sukthankar",
      "Mubarak Shah"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper091/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper091/paper091.pdf",
    "published": "2017-09",
    "summary": "This paper presents a computationally ef\ufb01cient approach for temporal action detection in untrimmed videos that outperforms state-of-the-art methods by a large margin. We exploit the temporal structure of actions by modeling an action as a sequence of sub-actions. A novel and fully automatic sub-action discovery algorithm is proposed, where the number of sub-actions for each action as well as their types are automatically determined from the training videos. We \ufb01nd that the discovered sub-actions are semantically meaningful. To localize an action, an objective function combining appearance, duration and temporal structure of sub-actions is optimized as a shortest path problem in a network \ufb02ow formulation. A signi\ufb01cant bene\ufb01t of the proposed approach is that it enables real-time action localization (40 fps) in untrimmed videos.",
    "code_link": ""
  },
  "bmvc2017_main_redreinforcedencoder-decodernetworksforactionanticipation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "RED: Reinforced Encoder-Decoder Networks for Action Anticipation",
    "authors": [
      "Jiyang Gao",
      "Zhenheng Yang",
      "Ram Nevatia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper092/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper092/paper092.pdf",
    "published": "2017-09",
    "summary": "Action anticipation aims to detect an action before it happens. Many real world applications in robotics and surveillance are related to this predictive capability. Current methods address this problem by \ufb01rst anticipating visual representations of future frames and then categorizing the anticipated representations to actions. However, anticipation is based on a single past frame\u2019s representation, which ignores the history trend. Besides, it can only anticipate a \ufb01xed future time. We propose a Reinforced Encoder-Decoder (RED) network for action anticipation. RED takes multiple history representations as input and learns to anticipate a sequence of future representations. One salient aspect of RED is that a reinforcement module is adopted to provide sequence-level supervision; the reward function is designed to encourage the system to make correct predictions as early as possible.",
    "code_link": ""
  },
  "bmvc2017_main_end-to-end,single-streamtemporalactiondetectioninuntrimmedvideos": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos",
    "authors": [
      "Shyamal Buch",
      "Victor Escorcia",
      "Bernard Ghanem",
      "Juan Carlos Niebles"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper093/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper093/paper093.pdf",
    "published": "2017-09",
    "summary": "In this work, we present a new intuitive, end-to-end approach for temporal action detection in untrimmed videos. We introduce our new architecture for Single-Stream Temporal Action Detection (SS-TAD), which effectively integrates joint action detection with its semantic sub-tasks in a single unifying end-to-end framework. We develop a method for training our deep recurrent architecture based on enforcing semantic constraints on intermediate modules that are gradually relaxed as learning progresses. We \ufb01nd that such a dynamic learning scheme enables SS-TAD to achieve higher overall detection performance, with fewer training epochs.",
    "code_link": "https://github.com/shyamal-b/ss-tad"
  },
  "bmvc2017_main_recognizingandcuratingphotoalbumsviaevent-specificimageimportance": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Recognizing and Curating Photo Albums via Event-Specific Image Importance",
    "authors": [
      "Yufei Wang",
      "Zhe Lin",
      "Xiaohui Shen",
      "Radomir Mech",
      "Gavin Miller",
      "Garrison Cottrell"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper094/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper094/paper094.pdf",
    "published": "2017-09",
    "summary": "Automatic organization of personal photos is a problem with many real world applications, and can be divided into two main tasks: recognizing the event type of the photo collection, and selecting interesting images from the collection. In this paper, we attempt to simultaneously solve both tasks: album-wise event recognition and image-wise importance prediction. We collected an album dataset with both event type labels and image importance labels, re\ufb01ned from an existing CUFED dataset. We propose a hybrid system consisting of three parts: A siamese network-based event-speci\ufb01c image importance prediction, a Convolutional Neural Network (CNN) that recognizes the event type, and a Long Short-Term Memory (LSTM)-based sequence level event recognizer. We propose an iterative updating procedure for event type and image importance score prediction.",
    "code_link": ""
  },
  "bmvc2017_main_spatio-temporalactiondetectionwithcascadeproposalandlocationanticipation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Spatio-Temporal Action Detection with Cascade Proposal and Location Anticipation",
    "authors": [
      "Zhenheng Yang",
      "Jiyang Gao",
      "Ram Nevatia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper095/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper095/paper095.pdf",
    "published": "2017-09",
    "summary": "In this work, we address the problem of spatio-temporal action detection in temporally untrimmed videos. It is an important and challenging task as \ufb01nding accurate human actions in both temporal and spatial space is important for analyzing large-scale video data. To tackle this problem, we propose a cascade proposal and location anticipation (CPLA) model for frame-level action detection. There are several salient points of our model: (1) a cascade region proposal network (casRPN) is adopted for action proposal generation and shows better localization accuracy compared with single region proposal network (RPN); (2) action spatio-temporal consistencies are exploited via a location anticipation network (LAN) and thus frame-level action detection is not conducted independently. Frame-level detections are then linked by solving an linking score maximization problem, and temporally trimmed into spatio-temporal action tubes.",
    "code_link": ""
  },
  "bmvc2017_main_videosegmentationwithbackgroundmotionmodels": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Video Segmentation with Background Motion Models",
    "authors": [
      "Scott Wehrwein",
      "Richard Szeliski"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper096/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper096/paper096.pdf",
    "published": "2017-09",
    "summary": "Many of today\u2019s most successful video segmentation methods use long-term feature trajectories as their \ufb01rst processing step. Such methods typically use spectral clustering to segment these trajectories, implicitly assuming that motion is translational in image space. In this paper, we explore the idea of explicitly \ufb01tting more general motion models in order to classify trajectories as foreground or background. We \ufb01nd that homographies are sufficient to model a wide variety of background motions found in real-world videos.",
    "code_link": ""
  },
  "bmvc2017_main_orientation-boostedvoxelnetsfor3dobjectrecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Orientation-boosted Voxel Nets for 3D Object Recognition",
    "authors": [
      "Nima Sedaghat",
      "Mohammadreza Zolfaghari",
      "Ehsan Amiri",
      "Thomas Brox"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper097/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper097/paper097.pdf",
    "published": "2017-09",
    "summary": "Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More speci\ufb01cally, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields signi\ufb01cant improvements in the classi\ufb01cation results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images.",
    "code_link": ""
  },
  "bmvc2017_main_semantic3dreconstructionwithfiniteelementbases": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Semantic 3D Reconstruction with Finite Element Bases",
    "authors": [
      "Audrey Richard",
      "Christoph Vogel",
      "Maros Blaha",
      "Thomas Pock",
      "Konrad Schindler"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper098/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper098/paper098.pdf",
    "published": "2017-09",
    "summary": "We propose a novel framework for the discretisation of multi-label problems on arbitrary, continuous domains. Our work bridges the gap between general FEM discretisations, and labeling problems that arise in a variety of computer vision tasks, including for instance those derived from the generalised Potts model. Starting from the popular formulation of labeling as a convex relaxation by functional lifting, we show that FEM discretisation is valid for the most general case, where the regulariser is anisotropic and non-metric. While our \ufb01ndings are generic and applicable to different vision problems, we demonstrate their practical implementation in the context of semantic 3D reconstruction, where such regularisers have proved particularly bene\ufb01cial.",
    "code_link": ""
  },
  "bmvc2017_main_silnetsingle-andmulti-viewreconstructionbylearningfromsilhouettes": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "SilNet : Single- and Multi-View Reconstruction by Learning from Silhouettes",
    "authors": [
      "Olivia Wiles",
      "Andrew Zisserman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper099/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper099/paper099.pdf",
    "published": "2017-09",
    "summary": "The objective of this paper is 3D shape understanding from single and multiple images. To this end, we introduce a new deep-learning architecture and loss function, SilNet, that can handle multiple views in an order-agnostic manner. The architecture is fully convolutional, and for training we use a proxy task of silhouette prediction, rather than directly learning a mapping from 2D images to 3D shape as has been the target in most recent work. We demonstrate that with the SilNet architecture there is generalisation over the number of views \u2013 for example, SilNet trained on 2 views can be used with 3 or 4 views at test-time; and performance improves with more views. We introduce two new synthetics datasets: a blobby object dataset useful for pretraining, and a challenging and realistic sculpture dataset; and demonstrate on these datasets that SilNet has indeed learnt 3D shape.",
    "code_link": ""
  },
  "bmvc2017_main_adifferentialapproachtoshapefrompolarization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A Differential Approach to Shape from Polarization",
    "authors": [
      "Roberto Mecca",
      "Fotios Logothetis",
      "Roberto Cipolla"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper100/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper100/paper100.pdf",
    "published": "2017-09",
    "summary": "State-of-the-art formulations of the Shape from Polarisation problem consist of several steps based on merging physical principles that prevent this problem being described by a single mathematical framework. In addition, specular and diffuse re\ufb02ections need to be separately considered, making the three-dimensional shape reconstruction not easily applicable to heterogeneous scenes consisting of different materials. In this work we derive a uni\ufb01ed specular/diffuse re\ufb02ection parametrisation of the Shape from Polarisation problem based on a linear partial differential equation capable of recovering the level-set of the surface. The inherent ambiguity of the Shape from Polarization problem becomes evident through the impossibility of reconstructing the whole surface with this differential approach. To overcome this limitation, we consider shading information elegantly embedding this new formulation into a two-lights calibrated photometric stereo approach. Thus we derive an albedo independent and well-posed differential model based on a system of hyperbolic PDEs capable of reconstructing the shape with no ambiguity. We validate the geometrical properties of the new differential model for the Shape from Polarisation problem using synthetic and real data by computing the isocontours of the shape under observation. Lastly, we show the suitability of this new model to elegantly \ufb01t into a variational solver that is able to provide 3D shape reconstructions from synthetic and real data.",
    "code_link": ""
  },
  "bmvc2017_main_fastdensefeatureextractionwithconvolutionalneuralnetworksthathavepoolingorstridinglayers": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Fast dense feature extraction with convolutional neural networks that have pooling or striding layers",
    "authors": [
      "Christian Bailer",
      "tewodros Habtegebrial",
      "Kiran Varanasi",
      "Didier Stricker"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper101/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper101/paper101.pdf",
    "published": "2017-09",
    "summary": "In recent years, many publications showed that convolutional neural network based features can have a superior performance to engineered features. However, not much effort was taken so far to extract local features ef\ufb01ciently for a whole image. In this paper, we present an approach to compute patch-based local feature descriptors ef\ufb01ciently in presence of pooling and striding layers for whole images at once. Our approach is generic and can be applied to nearly all existing network architectures. This includes networks for all local feature extraction tasks like camera calibration, Patchmatching, optical \ufb02ow estimation and stereo matching. In addition, our approach can be applied to other patch-based approaches like sliding window object detection and recognition.",
    "code_link": ""
  },
  "bmvc2017_main_localvisualmicrophonesimprovedsoundextractionfromsilentvideo": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Local Visual Microphones: Improved Sound Extraction from Silent Video",
    "authors": [
      "Mohammad Amin Amin",
      "Mohammad Amin Shabani",
      "Laleh Samadfam"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper102/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper102/paper102.pdf",
    "published": "2017-09",
    "summary": "Sound waves cause small vibrations in nearby objects. A few techniques exist in the literature that can extract sound from video. In this paper we study local vibration patterns at different image locations. We show that different locations in the image vibrate differently. We carefully aggregate local vibrations and produce a sound quality that improves state-of-the-art. We show that local vibrations could have a time delay because sound waves take time to travel through the air. We use this phenomenon to estimate sound direction. We also present a novel algorithm that speeds up sound extraction by two to three orders of magnitude and reaches real-time performance in a 20KHz video.",
    "code_link": ""
  },
  "bmvc2017_main_unsuperviseddeepgenerativehashing": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Unsupervised Deep Generative Hashing",
    "authors": [
      "Yuming Shen",
      "li Liu",
      "Ling Shao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper103/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper103/paper103.pdf",
    "published": "2017-09",
    "summary": "Hashing is regarded as an ef\ufb01cient approach for image retrieval and many other big-data applications. Recently, deep learning frameworks are adopted for image hashing, suggesting an alternative way to formulate the encoding function other than the conventional projections. However, existing deep learning based unsupervised hashing techniques still cannot produce leading performance compared with the non-deep methods, as it is hard to unveil the intrinsic structure of the whole sample space in the framework of mini-batch Stochastic Gradient Descent (SGD). To tackle this problem, in this paper, we propose a novel unsupervised deep hashing model, named Deep Variational Binaries (DVB). The conditional auto-encoding variational Bayesian networks are introduced in this work as the generative model to exploit the feature space structure of the training data using the latent variables. Integrating the probabilistic inference process with hashing objectives, the proposed DVB model estimates the statistics of data representations, and thus produces compact binary codes. Experimental results on three benchmark datasets, i.e., CIFAR-10, SUN-397 and NUS-WIDE, demonstrate that DVB outperforms state-of-the-art unsupervised hashing methods with significant margins.",
    "code_link": ""
  },
  "bmvc2017_main_sampledimagetaggingandretrievalmethodsonusergeneratedcontent": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Sampled Image Tagging and Retrieval Methods on User Generated Content",
    "authors": [
      "Karl Ni",
      "Kyle Zaragoza",
      "Alexander Gude",
      "Yonas Tesfaye",
      "Carmen Carrano",
      "Charles Foster",
      "Barry Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper104/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper104/paper104.pdf",
    "published": "2017-09",
    "summary": "Traditional image tagging and retrieval algorithms have limited generalizability as a result of being trained with heavily curated datasets. These limitations are most evident when arbitrary search words are used that do not intersect with training set labels. Weak labels from user-generated content (UGC) found in the wild (e.g., Google Photos, FlickR, etc.) have an almost unlimited number of unique words in the metadata tags. Prior work on word embeddings successfully leveraged unstructured text with large vocabularies, and our proposed method seeks to apply similar cost functions to open source imagery. Speci\ufb01cally, we train a deep learning image tagging and retrieval system on large-scale UGC using sampling methods and joint optimization of word embeddings. By using the Yahoo! FlickR Creative Commons (YFCC100M) dataset, such an approach builds robustness to common unstructured data issues that include but are not limited to irrelevant tags, misspellings, multiple languages, polysemy, and tag imbalance.",
    "code_link": "https://github.com/lab41/attalos"
  },
  "bmvc2017_main_criteriasliderslearningcontinuousdatabasecriteriaviainteractiveranking": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Criteria Sliders: Learning Continuous Database Criteria via Interactive Ranking",
    "authors": [
      "James Tompkin",
      "Kwang In Kim",
      "Hanspeter Pfister",
      "Christian Theobalt"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper105/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper105/paper105.pdf",
    "published": "2017-09",
    "summary": "Large databases are often organized by hand-labeled metadata\u2014or criteria\u2014which are expensive to collect. We can use unsupervised learning to model database variation, but these models are often high dimensional, complex to parameterize, or require expert knowledge. We learn low-dimensional continuous criteria via interactive ranking, so that the novice user need only describe the relative ordering of examples. This is formed as semi-supervised label propagation in which we maximize the information gained from a limited number of examples. Further, we actively suggest data points to the user to rank in a more informative way than existing work. Our e\ufb03cient approach allows users to interactively organize thousands of data points along 1D and 2D continuous sliders.",
    "code_link": ""
  },
  "bmvc2017_main_nowyouseemedeepfacehallucinationforunviewedsketches": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Now You See Me: Deep Face Hallucination for Unviewed Sketches",
    "authors": [
      "Conghui Hu",
      "Da Li",
      "Yi-zhe Song",
      "Timothy Hospedales"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper106/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper106/paper106.pdf",
    "published": "2017-09",
    "summary": "Face hallucination has been well studied in the last decade because of its useful applications in law enforcement and entertainment. Promising results on the problem of sketch-photo face hallucination have been achieved with classic, and increasingly deep learning-based methods. However, synthesized photos still lack the crisp \ufb01delity of real photos. More importantly, good results have primarily been demonstrated on very constrained datasets where the style variability is very low, and crucially the sketches are perfectly align-able traces of the ground-truth photos. However, realistic applications in entertainment or law enforcement require working with more unconstrained sketches drawn from memory or description, which are not rigidly align-able. In this paper, we develop a new deep learning approach to address these settings. Our image-image regression network is trained with a combination of content and adversarial losses to generate crisp photorealistic images, and it contains an integrated spatial transformer network to deal with non-rigid alignment between the domains. We evaluate face synthesis on classic constrained, as well as unviewed, benchmarks namely CUHK, MGDB, and FSMD.",
    "code_link": ""
  },
  "bmvc2017_main_frombenedictcumberbatchtosherlockholmescharacteridentificationintvserieswithoutascript": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "From Benedict Cumberbatch to Sherlock Holmes: Character Identification in TV series without a Script",
    "authors": [
      "Arsha Nagrani",
      "Andrew Zisserman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper107/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper107/paper107.pdf",
    "published": "2017-09",
    "summary": "The goal of this paper is the automatic identi\ufb01cation of characters in TV and feature \ufb01lm material. In contrast to standard approaches to this task, which rely on the weak supervision afforded by transcripts and subtitles, we propose a new method requiring only a cast list. This list is used to obtain images of actors from freely available sources on the web, providing a form of partial supervision for this task. In using images of actors to recognize characters, we make the following three contributions: (i) We demonstrate that an automated semi-supervised learning approach is able to adapt from the actor\u2019s face to the character\u2019s face, including the face context of the hair; (ii) By building voice models for every character, we provide a bridge between frontal faces (for which there is plenty of actor-level supervision) and pro\ufb01le (for which there is very little or none); and (iii) by combining face context and speaker identi\ufb01cation, we are able to identify characters with partially occluded faces and extreme facial poses. Results are presented on the TV series \u2018Sherlock\u2019 and the feature film \u2018Casablanca\u2019. We achieve the state-of-the-art on the Casablanca benchmark, surpassing previous methods that have used the stronger supervision available from transcripts.",
    "code_link": ""
  },
  "bmvc2017_main_quantifyingfacialagebyposteriorofagecomparisons": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Quantifying Facial Age by Posterior of Age Comparisons",
    "authors": [
      "Yunxuan Zhang",
      "Li Liu",
      "Cheng Li",
      "Chen-Change Loy"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper108/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper108/paper108.pdf",
    "published": "2017-09",
    "summary": "We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the person\u2019s actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the ef\ufb01cient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed \u2018MegaAge\u2019, which consists of 50,000 images. With the dataset, we train a network that jointly performs ordinal hyperplane classi\ufb01cation and posterior distribution learning.",
    "code_link": ""
  },
  "bmvc2017_main_yousaidthat?": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "You said that?",
    "authors": [
      "Joon Son Son",
      "Amir Jamaludin",
      "Andrew Zisserman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper109/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper109/paper109.pdf",
    "published": "2017-09",
    "summary": "We present a method for generating a video of a talking face. The method takes as inputs: (i) still images of the target face, and (ii) an audio speech segment; and outputs a video of the target face lip synched with the audio. The method runs in real time and is applicable to faces and audio not seen at training time. To achieve this we propose an encoder-decoder CNN model that uses a joint embedding of the face and audio to generate synthesised talking face video frames. The model is trained on tens of hours of unlabelled videos. We also show results of re-dubbing videos using speech from a different person.",
    "code_link": ""
  },
  "bmvc2017_main_reflectanceandshapeestimationwithalightfieldcameraundernaturalillumination": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Reflectance and Shape Estimation with a Light Field Camera under Natural Illumination",
    "authors": [
      "Trung Ngo",
      "Hajime Nagahara",
      "Ko Nishino",
      "Rin-ichiro Taniguchi",
      "Yasushi Yagi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper110/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper110/paper110.pdf",
    "published": "2017-09",
    "summary": "Re\ufb02ectance and shape are two important components in visually perceiving the real world. Inferring the re\ufb02ectance and shape of an object through cameras is a fundamental research topic in the \ufb01eld of computer vision. While three-dimensional shape recovery is pervasive with varieties of approaches and practical applications, re\ufb02ectance recovery has only emerged recently. Re\ufb02ectance recovery is a challenging task that is usually conducted in controlled environments, such as a laboratory environment with a special apparatus. However, it is desirable that the re\ufb02ectance be recovered in the \ufb01eld with a handy camera so that re\ufb02ectance can be jointly recovered with the shape. To that end, we present a solution that simultaneously recovers the re\ufb02ectance and shape (i.e., dense depth and normal maps) of an object under natural illumination with commercially available handy cameras. We employ a light \ufb01eld camera to capture one light \ufb01eld image of the object, and a 360-degree camera to capture the illumination.",
    "code_link": ""
  },
  "bmvc2017_main_geneganlearningobjecttransfigurationandobjectsubspacefromunpaireddata": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "GeneGAN: Learning Object Transfiguration and Object Subspace from Unpaired Data",
    "authors": [
      "Shuchang Zhou",
      "Taihong Xiao",
      "Yi Yang",
      "Dieqiao Feng",
      "Qinyao He",
      "Weiran He"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper111/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper111/paper111.pdf",
    "published": "2017-09",
    "summary": "Object Trans\ufb01guration generates diverse novel images by replacing an object in the given image with particular objects from exemplar images. It offers \ufb01ne-grained controls of image generation, and can perform tasks like \u201cput exactly those eyeglasses from image A onto the nose of the person in image B\u201d. However, object trans\ufb01guration often requires disentanglement of objects from backgrounds in feature space, which is challenging and previously requires learning from paired training data: two images sharing the same background but with different objects. In this work, we propose a deterministic generative model that learns disentangled feature subspaces by adversarial training. The training data are two unpaired sets of images: a positive set containing images that have some kind of object, and a negative set being the opposite. The model encodes an image into two complement features: one for the object, and the other for the background. The object and background features from a \u201cpositive\u201d parent and a \u201cnegative\u201d parent, can be recombined to produce four children, of which two are exact reproductions, and the other two are crossbreeds. Minimizing the adversarial loss between crossbreeds and parents will ensure the crossbreeds inherit the speci\ufb01c objects of parents. On the other hand, minimizing the reconstruction loss between reproductions and parents can ensure the completeness of the features. Overall, the object and background features are complete and disentangled representations of images. Moreover, the object features are found to constitute a multidimensional attribute subspace.",
    "code_link": ""
  },
  "bmvc2017_main_pixcolorpixelrecursivecolorization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "PixColor: Pixel Recursive Colorization",
    "authors": [
      "Sergio Guadarrama",
      "Ryan Dahl",
      "David Bieber",
      "Jonathon Shlens",
      "Mohammad Norouzi",
      "Kevin Murphy"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper112/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper112/paper112.pdf",
    "published": "2017-09",
    "summary": "We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We \ufb01rst train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image.",
    "code_link": ""
  },
  "bmvc2017_main_depthestimationandblurremovalfromasingleout-of-focusimage": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Depth Estimation and Blur Removal from a Single Out-of-focus Image",
    "authors": [
      "Saeed Anwar",
      "Zeeshan Hayder",
      "Fatih Porikli"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper113/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper113/paper113.pdf",
    "published": "2017-09",
    "summary": "This paper presents a depth estimation method that leverages rich representations learned from cascaded convolutional and fully connected neural networks operating on a patch-pooled set of feature maps. Our method is very fast and it substantially improves depth accuracy over the state-of-the-art alternatives, and from this, we computationally reconstruct an all-focus image and achieve synthetic re-focusing, all from a single image. Our experiments on benchmark datasets such as Make3D and NYU-v2 demonstrate superior performance in comparison to other available depth estimation methods by reducing the root-mean-squared error by 57% & 46%, and blur removal methods by 0.36 dB & 0.72 dB in PSNR, respectively.",
    "code_link": ""
  },
  "bmvc2017_main_exploringthestructureofareal-time,arbitraryneuralartisticstylizationnetwork": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Exploring the structure of a real-time, arbitrary neural artistic stylization network",
    "authors": [
      "Golnaz Ghiasi",
      "Honglak Lee",
      "Manjunath Kudlur",
      "Vincent Dumoulin",
      "Jonathon Shlens"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper114/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper114/paper114.pdf",
    "published": "2017-09",
    "summary": "In this paper, we present a method which combines the \ufb02exibility of the neural algorithm of artistic style with the speed of fast style transfer networks to allow real-time stylization using any content/style image pair. We build upon recent work leveraging conditional instance normalization for multi-style transfer networks by learning to predict the conditional instance normalization parameters directly from a style image. The model is successfully trained on a corpus of roughly 80,000 paintings and is able to generalize to paintings previously unobserved.",
    "code_link": ""
  },
  "bmvc2017_main_fine-pruningjointfine-tuningandcompressionofaconvolutionalnetworkwithbayesianoptimization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization",
    "authors": [
      "Frederick Tung",
      "Srikanth Muralidharan",
      "Greg Mori"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper115/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper115/paper115.pdf",
    "published": "2017-09",
    "summary": "When approaching a novel visual recognition problem in a specialized image domain, a common strategy is to start with a pre-trained deep neural network and \ufb01ne-tune it to the specialized domain. If the target domain covers a smaller visual space than the source domain used for pre-training (e.g. ImageNet), the \ufb01ne-tuned network is likely to be over-parameterized. However, applying network pruning as a post-processing step to reduce the memory requirements has drawbacks: \ufb01ne-tuning and pruning are performed independently; pruning parameters are set once and cannot adapt over time; and the highly parameterized nature of state-of-the-art pruning methods make it prohibitive to manually search the pruning parameter space for deep networks, leading to coarse approximations. We propose a principled method for jointly \ufb01ne-tuning and compressing a pre-trained convolutional network that overcomes these limitations.",
    "code_link": ""
  },
  "bmvc2017_main_onlineadaptationofconvolutionalneuralnetworksforvideoobjectsegmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Online Adaptation of Convolutional Neural Networks for Video Object Segmentation",
    "authors": [
      "Paul Voigtlaender",
      "Bastian Leibe"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper116/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper116/paper116.pdf",
    "published": "2017-09",
    "summary": "We tackle the task of semi-supervised video object segmentation, i.e. segmenting the pixels belonging to an object in a video using the ground truth pixel mask for the \ufb01rst frame. We build on the recently introduced one-shot video object segmentation (OSVOS) approach which uses a pretrained network and \ufb01ne-tunes it on the \ufb01rst frame. While achieving impressive performance, at test time OSVOS uses the \ufb01ne-tuned network in unchanged form and is not able to adapt to large changes in object appearance. To overcome this limitation, we propose Online Adaptive Video Object Segmentation (OnAVOS) which updates the network online using training examples selected based on the con\ufb01dence of the network and the spatial con\ufb01guration. Additionally, we add a pretraining step based on objectness, which is learned on PASCAL. Our experiments show that both extensions are highly effective and improve the state of the art on DAVIS to an intersection-over-union score of 85.",
    "code_link": ""
  },
  "bmvc2017_main_improvedbilinearpoolingwithcnns": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Improved Bilinear Pooling with CNNs",
    "authors": [
      "Tsung-Yu Lin",
      "Subhransu Maji"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper117/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper117/paper117.pdf",
    "published": "2017-09",
    "summary": "Bilinear pooling of Convolutional Neural Network (CNN) features [22, 23], and their compact variants [10], have been shown to be effective at \ufb01ne-grained recognition, scene categorization, texture recognition, and visual question-answering tasks among others. The resulting representation captures second-order statistics of convolutional features in a translationally invariant manner. In this paper we investigate various ways of normalizing these statistics to improve their representation power. In particular we \ufb01nd that the matrix square-root normalization offers signi\ufb01cant improvements and outperforms alternative schemes such as the matrix logarithm normalization when combined with elementwise square-root and (cid:96)2 normalization. This improves the accuracy by 2-3% on a range of \ufb01ne-grained recognition datasets leading to a new state of the art. We also investigate how the accuracy of matrix function computations effect network training and evaluation. In particular we compare against a technique for estimating matrix square-root gradients via solving a Lyapunov equation that is more numerically accurate than computing gradients via a Singular Value Decomposition (SVD). We \ufb01nd that while SVD gradients are numerically inaccurate the overall effect on the \ufb01nal accuracy is negligible once boundary cases are handled carefully. We present an alternative scheme for computing gradients that is faster and yet it offers improvements over the baseline model.",
    "code_link": ""
  },
  "bmvc2017_main_videototextsummaryjointvideosummarizationandcaptioningwithrecurrentneuralnetworks": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Video to Text Summary: Joint Video Summarization and Captioning with Recurrent Neural Networks",
    "authors": [
      "Bor-Chun Chen",
      "Yan-Ying Chen",
      "Francine Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper118/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper118/paper118.pdf",
    "published": "2017-09",
    "summary": "Video summarization and video captioning are considered two separate tasks in existing studies. For longer videos, automatically identifying the important parts of video content and annotating them with captions will enable a richer and more concise condensation of the video. We propose a general neural network con\ufb01guration that jointly considers two supervisory signals (i.e., an image-based video summary and text-based video captions) in the training phase and generates both a video summary and corresponding captions for a given video in the test phase. Our main idea is that the summary signals can help a video captioning model learn to focus on important frames. On the other hand, caption signals can help a video summarization model to learn better semantic representations. Jointly modeling both the video summarization and the video captioning tasks offers a novel end-to-end solution that generates a captioned video summary enabling users to index and navigate through the highlights in a video.",
    "code_link": ""
  },
  "bmvc2017_main_arecurrentvariationalautoencoderforhumanmotionsynthesis": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A Recurrent Variational Autoencoder for Human Motion Synthesis",
    "authors": [
      "Ikhsanul Habibie",
      "Daniel Holden",
      "Jonathan Schwarz",
      "Joe Yearsley",
      "Taku Komura"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper119/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper119/paper119.pdf",
    "published": "2017-09",
    "summary": "We propose a novel generative model of human motion that can be trained using a large motion capture dataset, and allows users to produce animations from high-level control signals. As previous architectures struggle to predict motions far into the future due to the inherent ambiguity, we argue that a user-provided control signal is desirable for animators and greatly reduces the predictive error for long sequences. Thus, we formulate a framework which explicitly introduces an encoding of control signals into a variational inference framework trained to learn the manifold of human motion. As part of this framework, we formulate a prior on the latent space, which allows us to generate high-quality motion without providing frames from an existing sequence. We further model the sequential nature of the task by combining samples from a variational approximation to the intractable posterior with the control signal through a recurrent neural network (RNN) that synthesizes the motion. We show that our system can predict the movements of the human body over long horizons more accurately than state-of-the-art methods.",
    "code_link": ""
  },
  "bmvc2017_main_adeeplearningpipelineforsemanticfacadesegmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A deep learning pipeline for semantic facade segmentation",
    "authors": [
      "Radwa Fathalla",
      "George Vogiatzis"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper120/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper120/paper120.pdf",
    "published": "2017-09",
    "summary": "We propose an algorithm that provides a pixel-wise classi\ufb01cation of building facades. Building facades provide a rich environment for testing semantic segmentation techniques. They come in a variety of styles that re\ufb02ect both appearance and layout characteristics. On the other hand, they exhibit a degree of stability in the arrangement of structures across different instances. We integrate appearance and layout cues in a single framework. The most likely label based on appearance is obtained through applying the state-of-the-art deep convolution networks. This is further optimized through Restricted Boltzmann Machines (RBM), applied on vertical and horizontal scanlines of facade models. Learning the probability distributions of the models via the RBMs is utilized in two settings. Firstly, we use them in learning from pre-seen facade samples, in the traditional training sense. Secondly, we learn from the test image at hand, in a way the allows the transfer of visual knowledge of the scene from correctly classi\ufb01ed areas to others. Experimentally, we are on par with the reported performance results.",
    "code_link": ""
  },
  "bmvc2017_main_deepreinforcementlearningattentionselectionforpersonre-identification": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Deep Reinforcement Learning Attention Selection For Person Re-Identification",
    "authors": [
      "XU LAN",
      "HangXiao Wang",
      "Shaogang Gong",
      "Xiatian Zhu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper121/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper121/paper121.pdf",
    "published": "2017-09",
    "summary": "Existing person re-identi\ufb01cation (re-id) methods assume the provision of accurately cropped person bounding boxes with minimum background noise, mostly by manually cropping. This is signi\ufb01cantly breached in practice when person bounding boxes must be detected automatically given a very large number of images and/or videos processed. Compared to carefully cropped manually, auto-detected bounding boxes are far less accurate with random amount of background clutter which can degrade notably person re-id matching accuracy. In this work, we develop a joint learning deep model that optimises person re-id attention selection within any auto-detected person bounding boxes by reinforcement learning of background clutter minimisation subject to re-id label pairwise constraints. Speci\ufb01cally, we formulate a novel uni\ufb01ed re-id architecture called Identity DiscriminativE Attention reinforcement Learning (IDEAL) to accurately select re-id attention in auto-detected bounding boxes for optimising re-id performance. Our model can improve re-id accuracy comparable to that from exhaustive human manual cropping of bounding boxes with additional advantages from identity discriminative attention selection that specially bene\ufb01ts re-id tasks beyond human knowledge.",
    "code_link": ""
  },
  "bmvc2017_main_case-basedhistopathologicalmalignancydiagnosisusingconvolutionalneuralnetworks": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Case-Based Histopathological Malignancy Diagnosis using Convolutional Neural Networks",
    "authors": [
      "Qicheng Lao",
      "Thomas Fevens"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper122/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper122/paper122.pdf",
    "published": "2017-09",
    "summary": "In practice, histopathological diagnosis of tumor malignancy often requires a human expert to scan through histopathological images at multiple magni\ufb01cation levels, after which a \ufb01nal diagnosis can be accurately determined. However, previous research on such classi\ufb01cation tasks using convolutional neural networks primarily determine a diagnosis for a single magni\ufb01cation level. In this paper, we propose a case-based approach using deep residual neural networks for histopathological malignancy diagnosis, where a case is de\ufb01ned as a sequence of images from the patient at all available levels of magni\ufb01cation. Effectively, through mimicking what a human expert would actually do, our approach makes a diagnosis decision based on features learned in combination at multiple magni\ufb01cation levels.",
    "code_link": "https://github.com/fchollet/keras"
  },
  "bmvc2017_main_labellesssceneclassificationwithsemanticmatching": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Labelless Scene Classification with Semantic Matching",
    "authors": [
      "Meng Ye",
      "Yuhong Guo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper123/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper123/paper123.pdf",
    "published": "2017-09",
    "summary": "Using high-level representation of images, e.g., objects and discriminative patches, for scene classi\ufb01cation has recently drawn increasing attention. Compared with low-level image features, the high-level features carry rich semantic information that is useful for improving semantic scene classi\ufb01cation. Nevertheless, acquiring scene level annotations remains a bottleneck for automatic scene classi\ufb01cation, although plenty of related auxiliary resources such as images with object tags are free available on the Internet. In this paper we propose a simple and novel methodology that exploits the rich auxiliary image and text resources to perform labelless automatic scene classi\ufb01cation without acquiring training images annotated with scene labels. The key of our methodology is to utilize existing object detectors to represent images in terms of high-level objects and then automatically categorize them based on the semantic relatedness of the object names and scene labels. We further incorporate a label propagation step to re\ufb01ne the automatic scene categorization results. Experiments are conducted on three standard scene classi\ufb01cation datasets.",
    "code_link": ""
  },
  "bmvc2017_main_patch-basedinterferometricphaseestimationviamixtureofgaussiandensitymodelling&non-localaveraginginthecomplexdomain": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Patch-based Interferometric Phase Estimation via Mixture of Gaussian Density Modelling & Non-local Averaging in the Complex Domain",
    "authors": [
      "Joshin Krishnan",
      "Jos\u00e9 Bioucas-Dias"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper124/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper124/paper124.pdf",
    "published": "2017-09",
    "summary": "This paper addresses interferometric phase (InPhase) image denoising, i.e., the denoising of phase modulo-2\u03c0 images from sinusoidal 2\u03c0-periodic and noisy observations. The wrapping discontinuities present in the InPhase images, which are to be preserved carefully, make InPhase denoising a challenging inverse problem. We propose a novel two-step algorithm to tackle this problem by exploiting the non-local self-similarity of the InPhase images. In the \ufb01rst step, the patches of the phase images are modelled using Mixture of Gaussian (MoG) densities in the complex domain. An Expectation Maximization (EM) algorithm is formulated to learn the parameters of the MoG from the noisy data. The learned MoG is used as a prior for estimating the InPhase images from the noisy images using Minimum Mean Square Error (MMSE) estimation. In the second step, an additional exploitation of non-local self-similarity is done by performing a type of non-local mean \ufb01ltering.",
    "code_link": ""
  },
  "bmvc2017_main_featuresequencerepresentationviaslowfeatureanalysisforactionclassification": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Feature Sequence Representation Via Slow Feature Analysis For Action Classification",
    "authors": [
      "Takumi Kobayashi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper125/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper125/paper125.pdf",
    "published": "2017-09",
    "summary": "The recent advances in extracting motion descriptors, such as BoW and CNN features, enable us to effectively convert a video into a sequence of frame-based feature vectors. For improving the action classi\ufb01cation performance, in this paper, we propose an ef\ufb01cient method to represent the feature sequence by exploiting the temporal patterns via slow feature analysis (SFA). The ordinary SFA suffers from small sample size (SSS) problem found in action video clips and thus we propose PCA-SFA to cope with the SSS problem by incorporating the information of PCA subspaces into SFA. The proposed method leverages the PCA-SFA projection vector to describe the sequence of even fewer frames by a \ufb01xed-dimensional video descriptor, capturing the essential temporal dynamics which is a slowly varying pattern embedded in the quickly varying input signals. The computational cost to produce the video descriptor is negligible compared to the feature extraction process such as BoW and CNN since the PCA-SFA is computed in a computationally ef\ufb01cient manner.",
    "code_link": ""
  },
  "bmvc2017_main_aconvolutionaltemporalencoderforvideocaptiongeneration": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A Convolutional Temporal Encoder for Video Caption Generation",
    "authors": [
      "Qingle Huang",
      "Zicheng Liao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper126/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper126/paper126.pdf",
    "published": "2017-09",
    "summary": "We propose a convolutional temporal encoding network for video sequence embedding and caption generation. The mainstream video captioning work is based on recurrent encoder of various forms (e.g. LSTMs and hierarchical encoders). In this work, a multi-layer convolutional neural network encoder is proposed. At the core of this encoder is a gated linear unit (GLU) that performs a linear convolutional transformation of input with a nonlinear gating, which has demonstrated superior performance in natural language modeling. Our model is built on top of this unit for video encoding and integrates several up-to-date tricks including batch normalization, skip connection and soft attention.",
    "code_link": ""
  },
  "bmvc2017_main_marginalizedcnnlearningdeepinvariantrepresentations": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Marginalized CNN: Learning Deep Invariant Representations",
    "authors": [
      "Jian ZHAO",
      "Jianshu Li",
      "Fang Zhao",
      "Xuecheng Nie",
      "Yunpeng Chen",
      "Shuicheng Yan",
      "Jiashi Feng"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper127/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper127/paper127.pdf",
    "published": "2017-09",
    "summary": "Training a deep neural network usually requires suf\ufb01cient annotated samples. The scarcity of supervision samples in practice thus becomes the major bottleneck on performance of the network. In this work, we propose a principled method to circumvent this dif\ufb01culty through marginalizing all the possible transformations over samples, termed as marginalized Convolutional Neural Network (mCNN). mCNN implicitly considers in\ufb01nitely many transformed copies of the training data in every training epoch and therefore is able to learn representations invariant for transformation in an end-to-end way. We prove that such marginalization can be understood as a classic CNN with a special form of regularization and thus is ef\ufb01cient for implementation and not restricted to the CNN module used. Experimental results on the MNIST and affNIST digit number datasets demonstrate that mCNN can match or outperform the original CNN with much fewer training samples. Besides, mCNN also performs well for face recognition on the recently released large-scale MS-Cele-1M dataset and outperforms state-of-the-arts.",
    "code_link": ""
  },
  "bmvc2017_main_correlationhashingnetworkforefficientcross-modalretrieval": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Correlation Hashing Network for Efficient Cross-Modal Retrieval",
    "authors": [
      "Yue Cao",
      "Mingsheng Long",
      "Jianmin Wang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper128/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper128/paper128.pdf",
    "published": "2017-09",
    "summary": "Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation ef\ufb01ciency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods \ufb01rst transform data into low-dimensional feature vectors, and then generate binary codes by another separate quan- tization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes. This paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perceptrons for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes.",
    "code_link": ""
  },
  "bmvc2017_main_sunriseorsunsetselectivecomparisonlearningforsubtleattributerecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition",
    "authors": [
      "Hong-Yu Zhou",
      "Bin-Bin Gao",
      "Jianxin Wu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper129/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper129/paper129.pdf",
    "published": "2017-09",
    "summary": "The dif\ufb01culty of image recognition has gradually increased from general category recognition to \ufb01ne-grained recognition and to the recognition of some subtle attributes such as temperature and geolocation. In this paper, we try to focus on the classi\ufb01cation between sunrise and sunset and hope to give a hint about how to tell the difference in subtle attributes. Sunrise vs. sunset is a dif\ufb01cult recognition task, which is challenging even for humans. Towards understanding this new problem, we \ufb01rst collect a new dataset made up of over one hundred webcams from different places. Since existing algorithmic methods have poor accuracy, we propose a new pairwise learning strategy to learn features from selective pairs of images. Experiments show that our approach surpasses baseline methods by a large margin and achieves better results even compared with humans.",
    "code_link": ""
  },
  "bmvc2017_main_bimballintersectionmultitemplatematching": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "BIM: Ball Intersection Multi Template Matching",
    "authors": [
      "Bat El El"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper130/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper130/paper130.pdf",
    "published": "2017-09",
    "summary": "BIM is a multi-template matching algorithm. As opposed to traditional template matching algorithms that match a single template to a single image, BIM attempts to match multiple templates to a single image at once. A naive approach to multi-template matching would be to run a standard template matching algorithm sequentially with each of the templates and report the best result. Instead, each template processed by BIM further restricts the search space of the following templates, thus speeding up the overall process. In particular, we extend a recently introduced method for single template matching under 2D af\ufb01ne transformation to work with multiple templates at once. As a result, given a library of templates we can ef\ufb01ciently \ufb01nd the best 2D af\ufb01ne transformation for each of them in a target image.",
    "code_link": ""
  },
  "bmvc2017_main_visualtextbooknetworkwatchcarefullybeforeansweringvisualquestions": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Visual Textbook Network: Watch Carefully before Answering Visual Questions",
    "authors": [
      "Difei Gao",
      "Ruiping Wang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper131/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper131/paper131.pdf",
    "published": "2017-09",
    "summary": "Recent deep neural networks have achieved promising results on Visual Question Answering (VQA) tasks. However, many works have shown that a high accuracy does not always guarantee that the VQA system correctly understands the contents of images and questions, which are what we really care about. Attention based models can locate the regions related to answers, and may demonstrate a promising understanding of image and question. However, the key components of generating correct location, i.e. visual semantic alignments and semantic reasoning, are still obscure and invisible. To deal with this problem, we introduce a two-stage model Visual Textbook Network (VTN), which is made up by two modules to produce more reasonable answers. Speci\ufb01cally, in the \ufb01rst stage, a textbook module watches the image carefully by performing a novel task named sentence reconstruction, which encodes a word to a visual region feature, and then decodes the visual feature to the input word. This procedure forces VTN to learn visual semantic alignments without much concerning on question answering. This stage is just like studying from textbooks where people mainly concentrate on the knowledge in the book and pay little attention to the test. At the second stage, we propose a simple network as exam module, which utilizes both the visual features generated by the \ufb01rst module and the question to predict the answer. To validate the effectiveness of our method, we conduct evaluations on Visual7W dataset and show the quantitive and qualitative results on answering questions.",
    "code_link": ""
  },
  "bmvc2017_main_keypersonaidedre-identificationinpartiallyorderedpedestrianset": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Key Person Aided Re-identification in Partially Ordered Pedestrian Set",
    "authors": [
      "Chen Chen",
      "Min Cao",
      "Silong Peng"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper132/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper132/paper132.pdf",
    "published": "2017-09",
    "summary": "Ideally person re-identi\ufb01cation seeks for perfect feature representation and metric model that re-identify all various pedestrians well in non-overlapping views at different locations with different camera con\ufb01gurations, which is very challenging. However, in most pedestrian sets, there always are some outstanding persons who are relatively easy to re-identify. Inspired by the existence of such data devision, we propose a novel key person aided person re-identi\ufb01cation framework based on the re-de\ufb01ned partially ordered pedestrian sets. The outstanding persons, namely \u201ckey persons\u201d, are selected by the K-nearest neighbor based saliency measurement. The partial order de\ufb01ned by pedestrian entering time in surveillance associates the key persons with the query person temporally and helps to locate the possible candidates.",
    "code_link": ""
  },
  "bmvc2017_main_learningconfidencemeasuresinthewild": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Learning confidence measures in the wild",
    "authors": [
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia",
      "Alessio Tonioni",
      "Luigi Di Stefano"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper133/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper133/paper133.pdf",
    "published": "2017-09",
    "summary": "Con\ufb01dence measures for stereo earned increasing popularity in most recent works concerning stereo, being effectively deployed to improve its accuracy. While most measures are obtained by processing cues from the cost volume, top-performing ones usually leverage on random-forests or CNNs to predict match reliability. Therefore, a proper amount of labeled data is required to effectively train such con\ufb01dence measures. Being such ground-truth labels not always available in practical applications, in this paper we propose a methodology suited for training con\ufb01dence measures in a self-supervised manner. Leveraging on a pool of properly selected conventional measures, we automatically detect a subset of very reliable pixels as well as a subset of erroneous samples from the output of a stereo algorithm. This strategy provides labels for training con\ufb01dence measures based on machine-learning technique without ground-truth labels. Compared to state-of-the-art, our method is neither constrained to image sequences nor to image content.",
    "code_link": ""
  },
  "bmvc2017_main_deepview-sensitivepedestrianattributeinferenceinanend-to-endmodel": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Deep View-Sensitive Pedestrian Attribute Inference in an end-to-end Model",
    "authors": [
      "M. Saquib Saquib",
      "Arne Schumann",
      "Yan Wang",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper134/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper134/paper134.pdf",
    "published": "2017-09",
    "summary": "Pedestrian attribute inference is a demanding problem in visual surveillance that can facilitate person retrieval, search and indexing. To exploit semantic relations between attributes, recent research treats it as a multi-label image classi\ufb01cation task. The visual cues hinting at attributes can be strongly localized and inference of person attributes such as hair, backpack, shorts, etc., are highly dependent on the acquired view of the pedestrian. In this paper we assert this dependence in an end-to-end learning framework and show that a view-sensitive attribute inference is able to learn better attribute predictions. Our proposed model jointly predicts the coarse pose (view) of the pedestrian and learns specialized view-speci\ufb01c multi-label attribute predictions.",
    "code_link": ""
  },
  "bmvc2017_main_divideandfuseare-rankingapproachforpersonre-identification": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Divide and Fuse: A Re-ranking Approach for Person Re-identification",
    "authors": [
      "Rui Yu",
      "Zhichao Zhou",
      "Song Bai",
      "Xiang Bai"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper135/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper135/paper135.pdf",
    "published": "2017-09",
    "summary": "As re-ranking is a necessary procedure to boost person re-identi\ufb01cation (re-ID) performance on large-scale datasets, the diversity of feature becomes crucial to person reID for its importance both on designing pedestrian descriptions and re-ranking based on feature fusion. However, in many circumstances, only one type of pedestrian feature is available. In this paper, we propose a \u201cDivide and Fuse\u201d re-ranking framework for person re-ID. It exploits the diversity from different parts of a high-dimensional feature vector for fusion-based re-ranking, while no other features are accessible. Speci\ufb01cally, given an image, the extracted feature is divided into sub-features. Then the contextual information of each sub-feature is iteratively encoded into a new feature. Finally, the new features from the same image are fused into one vector for re-ranking. Experimental results on two person re-ID benchmarks demonstrate the effectiveness of the proposed framework.",
    "code_link": ""
  },
  "bmvc2017_main_jointoptimizationofcodedilluminationandgrayscaleconversionforone-shotrawmaterialclassification": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Joint Optimization of Coded Illumination and Grayscale Conversion for One-Shot Raw Material Classification",
    "authors": [
      "Chao Wang",
      "Takahiro Okabe"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper136/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper136/paper136.pdf",
    "published": "2017-09",
    "summary": "Classifying materials and their surface states is important for machine vision applications such as visual inspection. In this paper, we propose an approach to one-shot per-pixel classi\ufb01cation of raw materials on the basis of spectral BRDFs; a surface of interest is illuminated by multispectral and multidirectional light sources at the same time. Speci\ufb01cally, we achieve two-class classi\ufb01cation from a single color image; it directly \ufb01nds the linear discriminant hyperplane with the maximal margin in the spectral BRDF feature space by jointly optimizing the non-negative coded illumination and the grayscale conversion. In addition, we extend our method to multiclass classi\ufb01cation by exploiting the degree of freedom of the grayscale conversion.",
    "code_link": ""
  },
  "bmvc2017_main_handposelearningcombiningdeeplearningandhierarchicalrefinementfor3dhandposeestimation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Hand Pose Learning: Combining Deep Learning and Hierarchical Refinement for 3D Hand Pose Estimation",
    "authors": [
      "Min-Yu Wu",
      "Pai-Wen Ting",
      "Li-Chen Fu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper137/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper137/paper137.pdf",
    "published": "2017-09",
    "summary": "Hand Pose Estimation aims to predict the position of joints on a hand from an image. This problem is pretty challenging since a hand can perform a variety of poses and tends to cause self-occlusion easily. This paper proposes a hybrid method of training a deep learning model and hierarchical refinement for hand pose estimation in a 3D space using depth images. First, we design a so-called skeleton-difference layer that can allow a convolutional neural network (CNN) training process to effectively learn the shape as well as physical constraints of a hand. Secondly, we employ a refinement method that is capable of hierarchically regressing a hand pose with an energy function. In the experiments we have conducted, the results validate the robustness and the performance of our system, and show that our method is able to predict the joints more accurately.",
    "code_link": ""
  },
  "bmvc2017_main_learningcross-scalecorrespondenceandpatch-basedsynthesisforreference-basedsuper-resolution": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Learning Cross-scale Correspondence and Patch-based Synthesis for Reference-based Super-Resolution",
    "authors": [
      "Haitian Zheng",
      "Mengqi Ji",
      "Lei Han",
      "Ziwei Xu",
      "Haoqian Wang",
      "Yebin Liu",
      "Lu Fang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper138/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper138/paper138.pdf",
    "published": "2017-09",
    "summary": "In this paper, we explore the Reference-based Super-Resolution (RefSR) problem, which aims to super-resolve a low de\ufb01nition (LR) input to a high de\ufb01nition (HR) output, given another HR reference image that shares similar viewpoint or capture time with the LR input. We solve this problem by proposing a learning-based scheme, denoted as RefSR-Net. Speci\ufb01cally, we \ufb01rst design a Cross-scale Correspondence Network (CCNet) to indicate the cross-scale patch matching between reference and LR image. The CC-Net is formulated as a classi\ufb01cation problem which predicts the correct matches from the candidate patches within the search range. Using dilated convolution, the training and feature map generation are ef\ufb01ciently implemented. Given the reference patch selected via CC-Net, we further propose a Super-resolution image Synthesis Network (SS-Net) for the synthesis of the HR output, by fusing the LR patch and the reference patch at multiple scales.",
    "code_link": ""
  },
  "bmvc2017_main_anomalydetectionusingaconvolutionalwinner-take-allautoencoder": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Anomaly Detection using a Convolutional Winner-Take-All Autoencoder",
    "authors": [
      "Hanh Tran",
      "David Hogg"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper139/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper139/paper139.pdf",
    "published": "2017-09",
    "summary": "We propose a method for video anomaly detection using a winner-take-all convolutional autoencoder that has recently been shown to give competitive results in learning for classi\ufb01cation task. The method builds on state of the art approaches to anomaly detection using a convolutional autoencoder and a one-class SVM to build a model of normality. The key novelties are (1) using the motion-feature encoding extracted from a convolutional autoencoder as input to a one-class SVM rather than exploiting reconstruction error of the convolutional autoencoder, and (2) introducing a spatial winner-take-all step after the \ufb01nal encoding layer during training to introduce a high degree of sparsity.",
    "code_link": ""
  },
  "bmvc2017_main_aparallelarchitectureforhighframeratestereousingsemi-globalmatching": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A Parallel Architecture for High Frame Rate Stereo using Semi-Global Matching",
    "authors": [
      "Akshay Jain",
      "Alexander Fell",
      "Saket Anand"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper140/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper140/paper140.pdf",
    "published": "2017-09",
    "summary": "Semi-Global Matching (SGM) is a popular algorithm to calculate depth maps in stereo images offering the best trade-off among accuracy, computational costs and high frame rates. This paper presents two architectural improvements in FPGA implementations of SGM to achieve high frame rates. First, a highly parallel, pipelined and scalable architecture is implemented which stores the intermediate values internally in the BlockRAMs of the FPGA, rendering external, off-chip memory obsolete. The architecture facilitates the parallelization of the cost computations, over the complete disparity range, in every clock cycle. Secondly, a novel SGM architecture based on multi-clock systems is introduced, which allows the integration of both, disparity-level and row-level parallelism and thus obtain even higher FPS rates. Results show that the FPS obtained are higher than any other FPGA based SGM implementation available in literature. On a Virtex-7 FPGA device, for VGA images (640\u00d7 480 pixels) and a disparity range of 128, a rate of 475 FPS is achieved.",
    "code_link": ""
  },
  "bmvc2017_main_acceleratingcomputationofexemplar-svmbybinaryapproximationbasedonmatrixdecomposition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Accelerating Computation of Exemplar-SVM by Binary Approximation based on Matrix Decomposition",
    "authors": [
      "Takato Kurokawa",
      "Yuji Yamauchi",
      "Mitsuru Ambai",
      "Takayoshi YAMASHITA",
      "Hironobu Fujiyoshi"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper141/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper141/paper141.pdf",
    "published": "2017-09",
    "summary": "The Exemplar-SVM (E-SVM) is a learning method based on exemplar that uses only one positive sample and a substantial number of negative samples. In the detection stage, it is possible to detect the location of the target object and estimate the attribute by transferring the attribute of the nearest exemplar. The use of E-SVM classi\ufb01ers leads to very high computational cost because it is necessary to compute the inner products of weight vectors for multiple classi\ufb01ers and an input feature vector. For accelerating the computation of E-SVM, we propose binary approximation based on matrix decomposition. First, we stack the E-SVM\u2019s weight vectors as a matrix. Then, we decompose the matrix into common binary basis vectors and real-valued coef\ufb01cient vectors for computing the approximated inner products by logical operation. We also introduce early rejection by cascade structure classi\ufb01er into the proposed method.",
    "code_link": ""
  },
  "bmvc2017_main_sparseandnoisytodensedepthmapupsamplingbasedonmeshandcolourconsistency": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Sparse and Noisy to Dense Depth Map Upsampling Based on Mesh and Colour Consistency",
    "authors": [
      "Hanshin Lim",
      "Junseok Lee"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper142/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper142/paper142.pdf",
    "published": "2017-09",
    "summary": "The paper presents a mesh- and colour-consistency-based depth map upsampling method from sparse depth information with various sampling structures under noisy conditions. In addition, we applied the proposed method to generate spatially consistent depth maps and a dense 3D point cloud from a sparse and noisy initial 3D point cloud. In the proposed method, triangulation is \ufb01rst performed on an image plane, whose sparse depth information is contaminated by noise and have irregular sampling structures. Then, an iterative discontinuity-preserving noise reduction process is enforced in the triangulation. After the noise reduction, a depth assignment method based on colour consistency and triangulation is used to generate a dense depth map. The experiment results show that the proposed method can provide a more accurate depth map than previous sparse-to-dense depth map upsampling methods.",
    "code_link": ""
  },
  "bmvc2017_main_efficient3dtrackinginurbanenvironmentswithsemanticsegmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Efficient 3D Tracking in Urban Environments with Semantic Segmentation",
    "authors": [
      "Martin Hirzer",
      "Peter Roth",
      "Vincent Lepetit"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper143/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper143/paper143.pdf",
    "published": "2017-09",
    "summary": "In this paper, we present a new 3D tracking approach for self-localization in urban environments. In particular, we build on existing tracking approaches (i.e., visual odometry tracking and SLAM), additionally using the information provided by 2.5D maps of the environment. Since this combination is not straightforward, we adopt ideas from semantic segmentation to \ufb01nd a better alignment between the pose estimated by the tracker and the 2.5D model. Speci\ufb01cally, we show that introducing edges as semantic classes is highly bene\ufb01cial for our task. In this way, we can reduce tracker inaccuracies and prevent drifting, thus increasing the tracker\u2019s stability.",
    "code_link": ""
  },
  "bmvc2017_main_classweightedconvolutionalfeaturesforvisualinstancesearch": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Class Weighted Convolutional Features for Visual Instance Search",
    "authors": [
      "Albert Jimenez",
      "Jose Alvarez",
      "Xavier Giro-i-Nieto"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper144/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper144/paper144.pdf",
    "published": "2017-09",
    "summary": "Image retrieval in realistic scenarios targets large dynamic datasets of unlabeled images. In these cases, training or \ufb01ne-tuning a model every time new images are added to the database is neither ef\ufb01cient nor scalable. Convolutional neural networks trained for image classi\ufb01cation over large datasets have been proven effective feature extractors for image retrieval. The most successful approaches are based on encoding the activations of convolutional layers, as they convey the image spatial information. In this paper, we go beyond this spatial information and propose a local-aware encoding of convolutional features based on semantic information predicted in the target image. To this end, we obtain the most discriminative regions of an image using Class Activation Maps (CAMs). CAMs are based on the knowledge contained in the network and therefore, our approach, has the additional advantage of not requiring external information. In addition, we use CAMs to generate object proposals during an unsupervised re-ranking stage after a \ufb01rst fast search. Our experiments on two public available datasets for instance retrieval, Oxford5k and Paris6k, demonstrate the competitiveness of our approach outperforming the current state-of-the-art when using off-the-shelf models trained on ImageNet. Our code is publicly available at http://imatge-upc.github.io/retrieval-2017-cam/.",
    "code_link": ""
  },
  "bmvc2017_main_dynamicsteerableblocksindeepresidualnetworks": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Dynamic Steerable Blocks in Deep Residual Networks",
    "authors": [
      "J\u00f6rn-Henrik Jacobsen",
      "Bert De Brabandere",
      "Arnold Smeulders"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper145/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper145/paper145.pdf",
    "published": "2017-09",
    "summary": "Filters in convolutional networks are typically parameterized in a pixel basis, that does not take prior knowledge about the visual world into account. We investigate the generalized notion of frames designed with image properties in mind, as alternatives to this parametrization. We show that frame-based ResNets and Densenets can improve performance on Cifar-10+ consistently, while having additional pleasant properties like steerability. By exploiting these transformation properties explicitly, we arrive at dynamic steerable blocks. They are an extension of residual blocks, that are able to seamlessly transform \ufb01lters under pre-de\ufb01ned transformations, conditioned on the input at training and inference time. Dynamic steerable blocks learn the degree of invariance from data and locally adapt \ufb01lters, allowing them to apply a different geometrical variant of the same \ufb01lter to each location of the feature map. When evaluated on the Berkeley Segmentation contour detection dataset, our approach outperforms all competing approaches that do not utilize pre-training.",
    "code_link": "https://github.com/fchollet/keras"
  },
  "bmvc2017_main_towardscompletescenereconstructionfromsingle-viewdepthandhumanmotion": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Towards Complete Scene Reconstruction from Single-View Depth and Human Motion",
    "authors": [
      "Sam Fowler",
      "Hansung Kim",
      "Adrian Hilton"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper146/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper146/paper146.pdf",
    "published": "2017-09",
    "summary": "Complete scene reconstruction from single view RGBD is a challenging task, requiring estimation of scene regions occluded from the captured depth surface. We propose that scene-centric analysis of human motion within an indoor scene can reveal fully occluded objects and provide functional cues to enhance scene understanding tasks. Captured skeletal joint positions of humans, utilised as naturally exploring active sensors, are projected into a human-scene motion representation. Inherent body occupancy is leveraged to carve a volumetric scene occupancy map initialised from captured depth, revealing a more complete voxel representation of the scene. To obtain a structured box model representation of the scene, we introduce unique terms to an object detection optimisation that overcome depth occlusions whilst deriving from the same depth data. The method is evaluated on challenging indoor scenes with multiple occluding objects such as tables and chairs.",
    "code_link": ""
  },
  "bmvc2017_main_flowbasedvideosuper-resolutionwithspatio-temporalpatchsimilarity": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Flow Based Video Super-Resolution with Spatio-temporal Patch Similarity",
    "authors": [
      "Joan Duran",
      "Antoni Buades"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper147/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper147/paper147.pdf",
    "published": "2017-09",
    "summary": "The goal of super-resolution is to fuse several low-resolution images of the same scene into a single one with increased resolution. The classical formulation assumes that the super-resolved image is related to the low-resolution frames by warping, convolution and subsampling. Algorithms divide into those using explicit registration and those avoiding it. The \ufb01rst ones combine for each pixel the information in its estimated trajectory. The second ones exploit both spatial and temporal redundancy. We propose to combine both ideas, making use of optical \ufb02ow and exploiting spatio-temporal redundancy with patch-based techniques. The proposed non-linear \ufb01ltering takes into account patch similarities, automatically correcting the \ufb02ow inaccuracies and avoiding the need of occlusion detection. Total variation and nonlocal regularization are used for the deconvolution stage.",
    "code_link": ""
  },
  "bmvc2017_main_bv-cnnsbinaryvolumetricconvolutionalnetworksfor3dobjectrecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "BV-CNNs: Binary Volumetric Convolutional Networks for 3D Object Recognition",
    "authors": [
      "Chao Ma",
      "Wei An",
      "Yinjie Lei",
      "Yulan Guo"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper148/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper148/paper148.pdf",
    "published": "2017-09",
    "summary": "Though 3D convolutional neural networks (CNNs) have achieved impressive performance for object recognition, they still face challenges in computational and memory cost. In this paper, we propose binary volumetric convolutional neural networks (namely, BV-CNNs) for ef\ufb01cient 3D object recognition. Specially, it transforms the inputs and weights in the network to binary values through binary transformation, then the \ufb02oating-point arithmetic convolutions are replaced with bitwise operations to reduce the computational and memory cost. Three binary volumetric CNNs are introduced from the traditional CNNs using our BV-CNN approach.",
    "code_link": ""
  },
  "bmvc2017_main_combiningedgeimagesanddepthmapsforrobustvisualodometry": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Combining Edge Images and Depth Maps for Robust Visual Odometry",
    "authors": [
      "Fabian Schenk",
      "Friedrich Fraundorfer"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper149/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper149/paper149.pdf",
    "published": "2017-09",
    "summary": "In this work, we propose a robust visual odometry system for RGBD sensors. The core of our method is a combination of edge images and depth maps for joint camera pose estimation. Edges are more stable under varying lighting conditions than raw intensity values and depth maps further add stability in poorly textured environments. This leads to higher accuracy and robustness in scenes, where feature- or photoconsistency-based approaches often fail. We demonstrate the robustness of our method under challenging conditions on various real-world scenarios recorded with our own RGBD sensor. Further, we evaluate on several sequences from standard benchmark datasets covering a wide variety of scenes and camera motions.",
    "code_link": "https://github.com/mp3guy/ICPCUDA"
  },
  "bmvc2017_main_order-adaptiveandillumination-awarevariationalopticalflowrefinement": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Order-Adaptive and Illumination-Aware Variational Optical Flow Refinement",
    "authors": [
      "Daniel Maurer",
      "Michael Stoll",
      "Andres Bruhn"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper150/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper150/paper150.pdf",
    "published": "2017-09",
    "summary": "Variational approaches form an inherent part of most state-of-the-art pipeline approaches for optical \ufb02ow computation. As the \ufb01nal step of the pipeline, the aim is to re\ufb01ne an initial \ufb02ow \ufb01eld typically obtained by inpainting non-dense matches in order to provide highly accurate results. In this paper, we take advantage of recent improvements in variational optical \ufb02ow estimation to construct an advanced variational model for this \ufb01nal re\ufb01nement step. By combining an illumination aware data term with an order adaptive smoothness term, we obtain a highly \ufb02exible model that is able to cope well with a broad variety of different scenarios. Moreover, we propose the use of an additional reduced coarse-to-\ufb01ne scheme instead of an exclusive initialisation scheme, which not only allows to re\ufb01ne the initialisation but also allows to correct larger erroneous displacements. Experiments on recent optical \ufb02ow benchmarks show the advantages of the advanced variational re\ufb01nement and the reduced coarse to \ufb01ne scheme.",
    "code_link": ""
  },
  "bmvc2017_main_large-scalecontinualroadinspectionvisualinfrastructureassessmentinthewild": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Large-scale Continual Road Inspection: Visual Infrastructure Assessment in the Wild",
    "authors": [
      "Ke Ma",
      "Minh Hoai",
      "Dimitris Samaras"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper151/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper151/paper151.pdf",
    "published": "2017-09",
    "summary": "This work develops a method to inspect the quality of pavement conditions based on images captured from moving vehicles. This task is challenging because the appearance of road surfaces varies tremendously, depending on the construction materials (e.g., concrete, asphalt), the weather conditions (e.g., rain, snow), the illumination conditions (e.g., sunny, shadow), and the interference of other structures (e.g., manholes, road marks). This problem is ampli\ufb01ed by the lack of a suf\ufb01ciently large and diverse dataset for training a pavement classi\ufb01er. Our \ufb01rst contribution in this paper is the development of a method to create a large-scale dataset of pavement images. Speci\ufb01cally, using map and GPS information, we match the ratings by government inspectors found in public databases to Google Street View images, creating a dataset containing more than 700K images from 70K street segments. We use the dataset to develop a deep-learning method for road assessment, which is based on Convolutional Neural Networks, Fisher Vector encoding, and UnderBagging random forests. This method achieves an accuracy of 58.",
    "code_link": "https://github.com/fchollet/keras"
  },
  "bmvc2017_main_correlationfiltertrackingbeyondanopen-loopsystem": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Correlation Filter Tracking: Beyond an Open-loop System",
    "authors": [
      "Qingyong Hu",
      "Yulan Guo",
      "Yunjin Chen",
      "Jingjing Xiao",
      "Wei An"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper152/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper152/paper152.pdf",
    "published": "2017-09",
    "summary": "Most existing Correlation Filter (CF) based trackers do not use any feedback from tracking output and can be considered as open-loop systems. They are prone to drifting when the object endures occlusion and large appearance changes. In this paper, we propose a generic self-correction mechanism for CF based trackers by introducing a closed-loop feedback technique. Our mechanism \ufb01rst detects the abnormality in tracking output using the Gaussian shape prior of a response map, and then estimates the tracking error by minimizing the discrepancy of tracking output and the expected response. An optimal offset is \ufb01nally given to regulate the tracking process and prevent tracking drifting through a feedback loop. Extensive experiments have been conducted on four large-scale benchmarks, including OTB-2013, OTB-2015, TC-128, and UAV123@10fps. Experimental results show that our self-correction mechanism can be used to improve the overall performance of most CF based trackers by a large margin.",
    "code_link": ""
  },
  "bmvc2017_main_photorealisticstyletransferwithscreenedpoissonequation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Photorealistic Style Transfer with Screened Poisson Equation",
    "authors": [
      "Roey Mechrez",
      "Eli Shechtman",
      "Lihi Zelnik-Manor"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper153/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper153/paper153.pdf",
    "published": "2017-09",
    "summary": "Recent work has shown impressive success in transferring painterly style to images. These approaches, however, fall short of photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. In this paper we propose an approach that takes as input a stylized image and makes it more photorealistic. It relies on the Screened Poisson Equation, maintaining the \ufb01delity of the stylized image while constraining the gradients to those of the original input image. Our method is fast, simple, fully automatic and shows positive progress in making a stylized image photorealistic.",
    "code_link": ""
  },
  "bmvc2017_main_probabilisticspatialregressionusingadeepfullyconvolutionalneuralnetwork": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Probabilistic Spatial Regression using a Deep Fully Convolutional Neural Network",
    "authors": [
      "S M MASUDUR RAHMAN AL M",
      "Karen Knapp",
      "Greg Slabaugh"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper154/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper154/paper154.pdf",
    "published": "2017-09",
    "summary": "Probabilistic predictions are often preferred in computer vision problems because they can provide a con\ufb01dence of the predicted value. The recent dominant model for computer vision problems, the convolutional neural network, produces probabilistic output for classi\ufb01cation and segmentation problems. But probabilistic regression using neural networks is not well de\ufb01ned. In this work, we present a novel fully convolutional neural network capable of producing a spatial probabilistic distribution for localizing image landmarks. We have introduced a new network layer and a novel loss function for the network to produce a two-dimensional probability map. The proposed network has been used in a novel framework to localize vertebral corners for lateral cervical X-ray images. The framework has been evaluated on a dataset of 172 images consisting 797 vertebrae and 3,188 vertebral corners.",
    "code_link": ""
  },
  "bmvc2017_main_lipreadinginprofile": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Lip Reading in Profile",
    "authors": [
      "Joon Son Son",
      "Andrew Zisserman"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper155/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper155/paper155.pdf",
    "published": "2017-09",
    "summary": "There has been a quantum leap in the performance of automated lip reading recently due to the application of neural network sequence models trained on a very large corpus of aligned text and face videos. However, this advance has only been demonstrated for frontal or near frontal faces, and so the question remains: can lips be read in pro\ufb01le to the same standard? The objective of this paper is to answer that question. We make three contributions: \ufb01rst, we obtain a new large aligned training corpus that contains pro\ufb01le faces, and select these using a face pose regressor network; second, we propose a curriculum learning procedure that is able to extend SyncNet [10] (a network to synchronize face movements and speech) progressively from frontal to pro\ufb01le faces; third, we demonstrate lip reading in pro\ufb01le for unseen videos.",
    "code_link": ""
  },
  "bmvc2017_main_adversarialrobustnesssoftmaxversusopenmax": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Adversarial Robustness: Softmax versus Openmax",
    "authors": [
      "Andras Rozsa",
      "Manuel Gunther",
      "Terrance Boult"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper156/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper156/paper156.pdf",
    "published": "2017-09",
    "summary": "Deep neural networks (DNNs) provide state-of-the-art results on various tasks and are widely used in real world applications. However, it was discovered that machine learning models, including the best performing DNNs, suffer from a fundamental problem: they can unexpectedly and con\ufb01dently misclassify examples formed by slightly perturbing otherwise correctly recognized inputs. Various approaches have been developed for ef\ufb01ciently generating these so-called adversarial examples, but those mostly rely on ascending the gradient of loss. In this paper, we introduce the novel logits optimized targeting system (LOTS) to directly manipulate deep features captured at the penultimate layer. Using LOTS, we analyze and compare the adversarial robustness of DNNs using the traditional Softmax layer with Openmax, which was designed to provide open set recognition by de\ufb01ning classes derived from deep representations, and is claimed to be more robust to adversarial perturbations.",
    "code_link": ""
  },
  "bmvc2017_main_generative3dhandtrackingwithspatiallyconstrainedposesampling": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Generative 3D Hand Tracking with Spatially Constrained Pose Sampling",
    "authors": [
      "Konstantinos Roditakis",
      "Alexandros Makris",
      "Antonis Argyros"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper157/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper157/paper157.pdf",
    "published": "2017-09",
    "summary": "We present a method for 3D hand tracking that exploits spatial constraints in the form of end effector (\ufb01ngertip) locations. The method follows a generative, hypothesize-and-test approach and uses a hierarchical particle \ufb01lter to track the hand. In contrast to state of the art methods that consider spatial constraints in a soft manner, the proposed approach enforces constraints during the hand pose hypothesis generation phase by sampling in the Reachable Distance Space (RDS). This sampling produces hypotheses that respect both the hands\u2019 dynamics and the end effector locations. The data likelihood term is calculated by measuring the discrepancy between the rendered 3D model and the available observations.",
    "code_link": ""
  },
  "bmvc2017_main_efficientonlinesurfacecorrectionforreal-timelarge-scale3dreconstruction": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Efficient Online Surface Correction for Real-time Large-Scale 3D Reconstruction",
    "authors": [
      "Robert Maier",
      "Raphael Schaller",
      "Daniel Cremers"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper158/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper158/paper158.pdf",
    "published": "2017-09",
    "summary": "State-of-the-art methods for large-scale 3D reconstruction from RGB-D sensors usually reduce drift in camera tracking by globally optimizing the estimated camera poses in real-time without simultaneously updating the reconstructed surface on pose changes. We propose an ef\ufb01cient on-the-\ufb02y surface correction method for globally consistent dense 3D reconstruction of large-scale scenes. Our approach uses a dense Visual RGB-D SLAM system that estimates the camera motion in real-time on a CPU and re\ufb01nes it in a global pose graph optimization. Consecutive RGB-D frames are locally fused into keyframes, which are incorporated into a sparse voxel hashed Signed Distance Field (SDF) on the GPU. On pose graph updates, the SDF volume is corrected on-the-\ufb02y using a novel keyframe re-integration strategy with reduced GPU-host streaming. We demonstrate in an extensive quantitative evaluation that our method is up to 93% more runtime ef\ufb01cient compared to the state-of-the-art and requires signi\ufb01cantly less memory, with only negligible loss of surface quality.",
    "code_link": "https://github.com/mp3guy/SurfReg"
  },
  "bmvc2017_main_longrangestereofromsynchronizedmonocularopticalflowstreamss": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Long Range Stereo from Synchronized Monocular Optical Flow Streamss",
    "authors": [
      "Darius Burschka"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper159/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper159/paper159.pdf",
    "published": "2017-09",
    "summary": "We present a processing technique for a robust reconstruction of motion properties for single points in large scale, dynamic environments. We assume that the acquisition camera is moving and that there are other independently moving agents in a large environment, like road scenarios. The separation of direction and magnitude of the reconstructed motion allows for robust reconstruction of the dynamic state of the objects in situations, where conventional binocular systems fail due to a small signal (disparity) from the images due to a constant detection error, and where structure from motion approaches fail due to unobserved motion of other agents between the camera frames.",
    "code_link": ""
  },
  "bmvc2017_main_learningtemporalstructuresforhumanactivityrecognition": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Learning temporal structures for human activity recognition",
    "authors": [
      "Tiantian Xu",
      "Edward Wong"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper160/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper160/paper160.pdf",
    "published": "2017-09",
    "summary": "We propose a hierarchical method for learning temporal structures for the recognition of complex human activities or actions in videos. Low level features (HOG, HOF, MBHx and MBHy) are \ufb01rst computed from video snippets to form concatenated feature vectors. A novel segmentation algorithm based on K-means clustering is then used to divide the video into segments, with each segment corresponding to a sub-action with uniform motion characteristics. Using low level features as inputs, a many-to-one encoder is trained to extract generalized features for the snippets in each segment. A second many-to-one encoder is then used to compute higher-level features from the generalized features. The higher-level features from individual segments are then concatenated together and used to train a third many-to-one encoder to extract a high-level feature representation for the entire video. The \ufb01nal descriptor is the concatenation of higher-level features from individual segments and the high-level feature for the entire video.",
    "code_link": ""
  },
  "bmvc2017_main_end-to-endmulti-viewlipreading": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "END-TO-END MULTI-VIEW LIPREADING",
    "authors": [
      "Stavros Petridis",
      "Yujiang Wang",
      "Zuwei Li",
      "Maja Pantic"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper161/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper161/paper161.pdf",
    "published": "2017-09",
    "summary": "Non-frontal lip views contain useful information which can be used to enhance the performance of frontal view lipreading. However, the vast majority of recent lipreading works, including the deep learning approaches which signi\ufb01cantly outperform traditional approaches, have focused on frontal mouth images. As a consequence, research on joint learning of visual features and speech classi\ufb01cation from multiple views is limited. In this work, we present an end-to-end multi-view lipreading system based on Bidirectional Long-Short Memory (BLSTM) networks. To the best of our knowledge, this is the \ufb01rst model which simultaneously learns to extract features directly from the pixels and performs visual speech classi\ufb01cation from multiple views and also achieves state-of-the-art performance. The model consists of multiple identical streams, one for each view, which extract features directly from different poses of mouth images. The temporal dynamics in each stream/view are modelled by a BLSTM and the fusion of multiple streams/views takes place via another BLSTM. An absolute average improvement of 3% and 3.8% over the frontal view performance is reported on the OuluVS2 database when the best two (frontal and pro\ufb01le) and three views (frontal, pro\ufb01le, 45\u25e6) are combined, respectively. The best three-view model results in a 10.5% absolute improvement over the current multi-view state-of-the-art performance on OuluVS2, without using external databases for training, achieving a maximum classi\ufb01cation accuracy of 96.",
    "code_link": ""
  },
  "bmvc2017_main_supervisedscale-regularizedlinearconvolutionaryfilters": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Supervised Scale-Regularized Linear Convolutionary Filters",
    "authors": [
      "Marco Loog",
      "Francois Lauze"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper162/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper162/paper162.pdf",
    "published": "2017-09",
    "summary": "We start by demonstrating that an elementary learning task\u2014learning a linear \ufb01lter from training data by means of regression can be solved very ef\ufb01ciently for feature spaces of very high dimensionality. In a second step, \ufb01rstly, acknowledging that such high-dimensional learning tasks typically bene\ufb01t from some form of regularization and, secondly, arguing that the problem of scale has not been taken care of in a very satisfactory manner, we come to a combined resolution of both of these shortcomings by proposing a technique that we coin scale regularization. This regularization problem can also be solved relatively ef\ufb01cient. All in all, the idea is to properly control the scale of a trained \ufb01lter, which we solve by introducing a speci\ufb01c regularization term into the overall objective function. We demonstrate, on an arti\ufb01cial \ufb01lter learning problem, the capabilities of our basic \ufb01lter.",
    "code_link": ""
  },
  "bmvc2017_main_sparsedeepfeaturerepresentationforobjectdetectionfromwearablecameras": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Sparse Deep Feature Representation for Object Detection from Wearable Cameras",
    "authors": [
      "Quanfu Fan",
      "Richard Chen"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper163/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper163/paper163.pdf",
    "published": "2017-09",
    "summary": "We propose a novel sparse feature representation for the faster RCNN framework and apply it for object detection from wearable cameras. Two main ideas, sparse convolution and sparse ROI pooling, are developed to reduce model complexity as well as computational cost. Sparse convolution approximates a full kernel by skipping weights in the kernel while sparse ROI pooling performs feature dimensionality reduction on the ROI pooling layer by skipping odd-indexed or even-indexed features. We demonstrate the effectiveness of our approach on two challenging body camera datasets including realistic police-generated clips.",
    "code_link": ""
  },
  "bmvc2017_main_humanactionsegmentationusing3dfullyconvolutionalnetwork": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Human Action Segmentation using 3D Fully Convolutional Network",
    "authors": [
      "Pei Yu",
      "Jiang Wang",
      "Ying Wu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper164/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper164/paper164.pdf",
    "published": "2017-09",
    "summary": "Detailed action analysis, such as action detection, localization and segmentation, has received more and more attention in recent years. Compared to action classi\ufb01cation, action segmentation and localization are more useful in many practical applications that require precise spatio-temporal information of the actions. However, performing action segmentation and localization is more challenging, because determining the pixel-level locations of action not only requires a strong spatial model that captures the visual appearances for the actions, but also calls for a temporal model that characterizes the dy- namics of the actions. Most existing methods either use hand-crafted spatial models, or can only extract short-term motion information. In this paper, we propose a 3D fully convolutional deep network to jointly exploit spatial and temporal information in a unified framework for action segmentation and localization. The proposed deep network is trained to combine both information in an end-to-end fashion.",
    "code_link": ""
  },
  "bmvc2017_main_deepfisherfaces": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Deep fisher faces",
    "authors": [
      "Harald Hanselmann",
      "Shen Yan",
      "Hermann Ney"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper165/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper165/paper165.pdf",
    "published": "2017-09",
    "summary": "Most current state-of-the-art methods for unconstrained face recognition use deep convolutional neural networks. Recently, it has been proposed to augment the typically used softmax cross-entropy loss by adding a center loss trying to minimize the distance between the face images and their class centers. In this work we further extend the center (intra-class) loss with an inter-class loss reminiscent of the popular early face recognition approach Fisherfaces. To this end we add a term that directly optimizes the distances of the class centers appearing in a batch in dependence of the input images. We evaluate the new loss on two popular databases for unconstrained face recognition, the Labeled Faces in the Wild and the Youtube Faces database.",
    "code_link": ""
  },
  "bmvc2017_main_improvingtargettrackingrobustnesswithbayesiandatafusion": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Improving target tracking robustness with Bayesian data fusion",
    "authors": [
      "Yevgeniy Reznichenko",
      "Henry Medeiros"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper166/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper166/paper166.pdf",
    "published": "2017-09",
    "summary": "Intelligent data fusion is an active area of research. Most recent works in data fusion for object tracking employ machine learning techniques that lack \ufb02exibility due to their inability to adapt to changing conditions in the presence of limited amounts of training data. Our work explores a hierarchical Bayesian fusion approach, which aggregates information from multiple tracking algorithms into a more robust estimate and hence outperforms its constituent trackers. This adaptive and general data fusion scheme takes advantage of each tracker\u2019s local statistics and combines them using a global softened majority voting. The widespread availability of high-performance multicore processors has allowed parallel threads to run multiple trackers asynchronously, which means that the algorithm can be executed in real time as it is only limited by the slowest tracker in the ensemble.",
    "code_link": ""
  },
  "bmvc2017_main_one-shotlearningforsemanticsegmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "One-Shot Learning for Semantic Segmentation",
    "authors": [
      "Amirreza Shaban",
      "Shray Bansal",
      "Zhen Liu",
      "Irfan Essa",
      "Byron Boots"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper167/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper167/paper167.pdf",
    "published": "2017-09",
    "summary": "Low-shot learning methods for image classi\ufb01cation support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class.",
    "code_link": ""
  },
  "bmvc2017_main_efficenttraffic-signrecognitionwithscale-awarecnn": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Efficent Traffic-Sign Recognition with Scale-aware CNN",
    "authors": [
      "Yuchen Yang",
      "Shuo Liu",
      "Wei Ma",
      "Qiuyuan Wang",
      "Zheng Liu"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper168/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper168/paper168.pdf",
    "published": "2017-09",
    "summary": "The paper presents a Traf\ufb01c Sign Recognition (TSR) system, which can fast and accurately recognize traf\ufb01c signs of different sizes in images. The system consists of two well-designed Convolutional Neural Networks (CNNs), one for region proposals of traf\ufb01c signs and one for classi\ufb01cation of each region. In the proposal CNN, a Fully Convolutional Network (FCN) with a dual multi-scale architecture is proposed to achieve scale invariant detection. In training the proposal network, a modi\ufb01ed Online Hard Example Mining (OHEM) scheme is adopted to suppress false positives. The classi\ufb01cation network fuses multi-scale features as representation and adopts an Inception module for ef\ufb01ciency. We evaluate the proposed TSR system and its components with extensive experiments. Our method obtains 99.88% precision and 96.61% recall on the Swedish Traf\ufb01c Signs Dataset (STSD), higher than state-of-the-art methods.",
    "code_link": ""
  },
  "bmvc2017_main_plane-aidedvisual-inertialodometryforposeestimationofa3dcamerabasedindoorblindnavigationsystem": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Plane-Aided Visual-Inertial Odometry for Pose Estimation of a 3D Camera based Indoor Blind Navigation System",
    "authors": [
      "He Zhang",
      "cang Ye"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper169/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper169/paper169.pdf",
    "published": "2017-09",
    "summary": "The classic visual-inertial odometry (VIO) method estimates a moving camera\u2019s 6-DOF pose relative to its starting point by fusing the camera\u2019s ego-motion measured by a visual odometry (VO) and the motion measured by an inertial measurement unit (IMU). The VIO attempts to updates the estimates of the IMU\u2019s biases at each step by using the VO\u2019s output so as to improve the accuracy of IMU measurement. This approach works only if an accurate VO output can be identified and used. However, there is no reliable method that can be used to evaluate the accuracy of the VO. In this paper, a new VIO method is introduced for pose estimation of a robotic navigation aid (RNA) that uses a 3D time-of-flight camera for perception. The method, called plane-aided visual-inertial odometry (PAVIO), extracts planes from the 3D point cloud of the current camera view and track them onto the next camera view by using the IMU\u2019s measurement. The tracking result is used to accept the VO output only if it is accurate. The accepted VO outputs, the information of the extracted planes, and the IMU\u2019s measurements over time are used to create a factor graph. By optimizing the graph, the method improves the estimation accuracy of the IMU bias and reduces the camera\u2019s pose error.",
    "code_link": ""
  },
  "bmvc2017_main_dissectingscalefromposeestimationinvisualodometry": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Dissecting scale from pose estimation in visual odometry",
    "authors": [
      "Rong yuan",
      "Hongyi Fan",
      "Benjamin Kimia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper170/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper170/paper170.pdf",
    "published": "2017-09",
    "summary": "Traditional visual odometry approaches often rely on estimating the world in the form a 3D cloud of points from key frames, which are then projected onto other frames to determine their absolute poses. The resulting trajectory is obtained from the integration of these incremental estimates. In this process, both in the initial world reconstruction as well as in the subsequent PnP projection, a rotation matrix and a translation vector are the unknowns that are solved via a numerical process. We observe that the involvement of all these variables in the numerical process is unnecessary, costing both computational time and accuracy. Rather, the relative pose of pairs of frames can be independently estimated from a set of common features, up to scale, with high accuracy. This scale parameter is a free parameter for each pair of frames, whose estimation is the only obstacle in the integration of these local estimates. This paper presents an approach for relating this free parameter for each neighboring pair of frames and therefore integrating the entire estimation process, leaving only a single global scale variable.",
    "code_link": ""
  },
  "bmvc2017_main_automaticimagetransformationforinducingaffect": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Automatic Image transformation for inducing affect",
    "authors": [
      "MOhsen Ali",
      "Afsheen Rafaqat"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper171/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper171/paper171.pdf",
    "published": "2017-09",
    "summary": "Current image transformation and recoloring algorithms try to introduce artistic effects in the photographed images, based on user input of target image(s) or selection of pre-designed \ufb01lters. These manipulations, although intended to enhance the impact of an image on the viewer, do not include the option of image transformation by specifying the affect information. In this paper we present an automatic image-transformation method that transforms the source image such that it can induce an emotional affect on the viewer, as desired by the user. Our proposed novel image emotion transfer algorithm does not require a user-speci\ufb01ed target image. The proposed algorithm uses features extracted from top layers of deep convolutional neural network and the user-speci\ufb01ed emotion distribution to select multiple target images from an image database for color transformation, such that the resultant image has desired emotional impact. Our method can handle more diverse set of photographs than the previous methods. We conducted a detailed user study showing the effectiveness of our proposed method.",
    "code_link": ""
  },
  "bmvc2017_main_subpixelsemanticflow": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Subpixel Semantic Flow",
    "authors": [
      "Berk Sevilmis",
      "Benjamin Kimia"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper172/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper172/paper172.pdf",
    "published": "2017-09",
    "summary": "Dense semantic correspondence is usually cast as a variational optimization problem. Current methods generally focus on obtaining more discriminative features (to improve the data/correspondence term), and adopt a message-passing algorithm for inference, which is generally a variant of loopy belief propagation. One drawback of such optimization is that the \ufb02ow vectors are constrained to be discrete variables resulting in pixel resolution, \u201cblocky\u201d \ufb02ow \ufb01elds. The main hinderance to formulating the problem in continuous space, and hence solving the problem at subpixel resolution, is the use of histogram based descriptors such as SIFT, HOG, etc. Such sparse feature descriptors are distinctive but linearize poorly. In this paper, we revisit a classic dense descriptor, namely Geometric Blur, which is, in contrast, extracted from a linear \ufb01lter (spatially varying Gaussian) response that can be linearized and therefore interpolated at subpixel values. In addition to the data and smoothness terms used in variational models, we also add a term promoting bidirectional \ufb02ow consistency. As there is no longer a \ufb01nite set of values a \ufb02ow vector can take, we use gradient descent based continuous optimization. We present promising results encouraging the use of gradient based continuous optimiza- tion in establishing dense semantic correspondences.",
    "code_link": ""
  },
  "bmvc2017_main_lightcascadedconvolutionalneuralnetworksforaccurateplayerdetection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Light Cascaded Convolutional Neural Networks for Accurate Player Detection",
    "authors": [
      "Keyu Lu",
      "Jianhui Chen",
      "James Little",
      "Hangen He"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper173/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper173/paper173.pdf",
    "published": "2017-09",
    "summary": "Vision based player detection is important in sports applications. Accuracy, ef\ufb01ciency, and low memory consumption are desirable for real-time tasks such as intelligent broadcasting and automatic event classi\ufb01cation. In this paper, we present a cascaded convolutional neural network (CNN) that satis\ufb01es all three of these requirements. Our method \ufb01rst trains a binary (player/non-player) classi\ufb01cation network from labeled image patches. Then, our method ef\ufb01ciently applies the network to a whole image in testing. We conducted experiments on basketball and soccer games. Experimental results demonstrate that our method can accurately detect players under challenging conditions such as varying illumination, highly dynamic camera movements and motion blur. Comparing with conventional CNNs, our approach achieves state-of-the-art accuracy on both games with 1000x fewer parameters (i.e., it is light)",
    "code_link": ""
  },
  "bmvc2017_main_exploitingprotrusioncuesforfastandeffectiveshapemodelingviaellipses": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Exploiting Protrusion Cues for Fast and Effective Shape Modeling via Ellipses",
    "authors": [
      "Alex Wong",
      "Alan Yuille",
      "Brian Taylor"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper174/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper174/paper174.pdf",
    "published": "2017-09",
    "summary": "Modeling objects with a set of geometric primitives is a key problem in computer vision and pattern recognition with numerous applications including object detection and retrieval, tracking, motion and action analysis. In this paper, we attempt to represent 2D object shapes with a number of ellipses with semantic meaning (e.g. one ellipse may correspond to an arm, while two other ellipses may represent a bent leg), while maintaining a high coverage of the shapes. We propose a novel ellipse \ufb01tting method based on psychology and cognitive science studies on shape decomposition and show that our shape coverage compares well with the state of the art methods, while signi\ufb01cantly outperforming them in run-time by as much as 508 times in our evaluation of the methods on over 4000 2D shapes.",
    "code_link": ""
  },
  "bmvc2017_main_acnn-basedapproachforautomaticlicenseplaterecognitioninthewild": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A CNN-Based Approach for Automatic License Plate Recognition in the Wild",
    "authors": [
      "Meng Dong",
      "Dongliang He",
      "Chong Luo",
      "Dong Liu",
      "Wenjun Zeng"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper175/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper175/paper175.pdf",
    "published": "2017-09",
    "summary": "In this paper, we address automatic license plate recognition (ALPR) in the wild. Such an ALPR system takes an arbitrary image as input and outputs the recognized license plate numbers. In the detection stage, we adopt a cascade structure comprising of a fast region proposal network and a R-CNN network. The R-CNN network not only eliminates false alarms but also regresses corner positions for each detected plate. This allows us to estimate an af\ufb01ne transformation matrix to rectify the extracted plates. In the recognition stage, we propose an innovative structure composed of parallel spatial transform networks and shared-weight recognizers. The system is trained and evaluated on a Chinese license plate dataset with over 18K images. Results show that our detector performs better than faster R-CNN (VGG) which is 1.5x slower in testing and 57x larger in model size. The recognizer is also signi\ufb01cantly better than existing solutions, reducing 57.",
    "code_link": ""
  },
  "bmvc2017_main_multi-regionensembleconvolutionalneuralnetworkforhighaccuracyageestimation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Multi-Region Ensemble Convolutional Neural Network for High Accuracy Age Estimation",
    "authors": [
      "Yiliang Chen",
      "zichang Tan",
      "Alex Po Leung",
      "Jun Wan",
      "Jianguo Zhang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper176/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper176/paper176.pdf",
    "published": "2017-09",
    "summary": "In real life, when telling a person\u2019s age from his/her face, we tend to look at his/her whole face \ufb01rst and then focus on certain important regions like eyes. After that we will focus on each particular facial feature individually like the nose or the mouth so that we can decide the age of the person. Similarly, in this paper, we propose a new framework for age estimation, which is based on human face sub-regions. Each sub-network in our framework takes the input of two images each from human facial region. One of them is the global face, and the other is a vital sub-region. Then, we combine the predictions from different sub-regions based on a majority voting method. We call our framework Multi-Region Network Prediction Ensemble (MRNPE) and evaluate our approach using two popular public datasets: MORPH Album II and Cross Age Celebrity Dataset (CACD). Experiments show that our method outperforms the existing state-of-the-art age estimation methods by a signi\ufb01cant margin. The Mean Absolute Errors (MAE) of age estimation are dropped from 3.03 to 2.73 years on the MORPH Album II and 4.79 to 4.",
    "code_link": ""
  },
  "bmvc2017_main_primitive-basedsurfaceregularizationforurban3dreconstruction": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Primitive-based Surface Regularization for Urban 3D Reconstruction",
    "authors": [
      "Thomas Holzmann",
      "Martin Oswald",
      "Marc Pollefeys",
      "Friedrich Fraundorfer",
      "Horst Bischof"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper177/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper177/paper177.pdf",
    "published": "2017-09",
    "summary": "We propose a method for urban 3D reconstruction that is a hybrid between a volumetric 3D reconstruction approach and a plane \ufb01tting approach in order to obtain a denoised and compact representation of the scene. In our hybrid approach, a single global optimization, using visibility as main information, de\ufb01nes whether the \ufb01nal reconstructed surface should align with a detected plane or rather follow the details of the input data. Our method is based on an established tetrahedral occupancy labeling approach which we taylor for urban reconstruction by adding the possibility to favor an alignment of the surface with detected planes. We further add novel regularization terms that favor Manhattan-like structures and which allow to control the level of detail of the output model.",
    "code_link": ""
  },
  "bmvc2017_main_cross-modalretrievalviamemorynetwork": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Cross-modal Retrieval via Memory Network",
    "authors": [
      "Ge Song",
      "Xiaoyang Tan"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper178/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper178/paper178.pdf",
    "published": "2017-09",
    "summary": "With the explosive growth of multimedia data on the Internet, cross-modal retrieval has attracted a great deal of attention in computer vision and multimedia community. However, this task is very challenging due to the heterogeneity gap between different modalities. Current approaches typically involve a common representation learning process that maps different data into a common space by linear or nonlinear functions. Yet most of them 1) only handle the dual-modal situation and generalize poorly to complex cases; 2) require example-level alignment of training data, which is often prohibitively expensive in practical applications; and 3) do not fully exploit prior knowledge about different modalities during the mapping process. In this paper, we address above issues by casting common representation learning as a Question Answer problem via a cross-modal memory neural network (CMMN). Speci\ufb01cally, raw features of all modalities are seemed as \u2019Question\u2019, and extra discriminator is exploited to select high-quality ones as \u2019Statements\u2019 for storage whereby common features are desired \u2019Answer\u2019.",
    "code_link": ""
  },
  "bmvc2017_main_acompactparametricsolutiontodepthsensorcalibration": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "A Compact Parametric Solution to Depth Sensor Calibration",
    "authors": [
      "Andrew Spek",
      "Tom Drummond"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper179/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper179/paper179.pdf",
    "published": "2017-09",
    "summary": "We present a method for calibration of low-cost depth sensors such as the Microsoft Kinect. We show this method is effective at correcting the structured sensor error using a compact parametric solution, that uses only a small fraction of the number of parameters used in many existing approaches. Additionally we provide this calibration as an open-source implementation, with limited required external dependencies. We demonstrate our approach can optimise directly for a geometric depth and radial distortion calibration function in minutes on modern hardware. We demonstrate our approach can optimise directly for a geometric depth and radial distortion calibration function in minutes on modern hardware.",
    "code_link": ""
  },
  "bmvc2017_main_pansharpeningvialocality-constrainedsparserepresentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Pansharpening via Locality-Constrained Sparse Representation",
    "authors": [
      "Songze Tang",
      "Nan zhou",
      "Liang Xiao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper180/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper180/paper180.pdf",
    "published": "2017-09",
    "summary": "Recently, sparse representation based approaches have been shown an effective performance for pansharpening. However, these methods imposed (cid:96)0 or (cid:96)1 -norm constraints on the sparse coef\ufb01cients. The local similarity of sparse coef\ufb01cients was ignored. Motivated by the importance of data locality, in this paper, we propose a locality-constrained sparse representation algorithm for pansharpening, which keeps the data locality during the sparse representation process. The learned dictionary is able to preserve local data structure, resulting in improved data representation. During the sparse coding stage, analytical solutions are provided based on the basis of mathematic deduction.",
    "code_link": ""
  },
  "bmvc2017_main_residualconv-deconvgridnetworkforsemanticsegmentation": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Residual Conv-Deconv Grid Network for Semantic Segmentation",
    "authors": [
      "Damien Fourure",
      "R\u00e9mi Emonet",
      "Elisa Fromont",
      "Damien Muselet",
      "Alain Tremeau",
      "Christian Wolf"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper181/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper181/paper181.pdf",
    "published": "2017-09",
    "summary": "This paper presents GridNet, a new Convolutional Neural Network (CNN) architecture for semantic image segmentation (full scene labelling). Classical neural networks are implemented as one stream from the input to the output with subsampling operators applied in the stream in order to reduce the feature maps size and to increase the receptive \ufb01eld for the \ufb01nal prediction. However, for semantic image segmentation, where the task consists in providing a semantic class to each pixel of an image, feature maps reduction is harmful because it leads to a resolution loss in the output prediction. To tackle this problem, our GridNet follows a grid pattern allowing multiple interconnected streams to work at different resolutions. We show that our network generalizes many well known networks such as conv-deconv, residual or U-Net networks.",
    "code_link": ""
  },
  "bmvc2017_main_deepgrabcutforobjectselection": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Deep GrabCut for Object Selection",
    "authors": [
      "Ning Xu",
      "Brian Price",
      "Scott Cohen",
      "Jimei Yang",
      "Thomas Huang"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper182/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper182/paper182.pdf",
    "published": "2017-09",
    "summary": "Most previous bounding-box-based segmentation methods assume the bounding box tightly covers the object of interest. However it is common that a rectangle input could be too large or too small. In this paper, we propose a novel segmentation approach that uses a rectangle as a soft constraint by transforming it into an Euclidean distance map. A convolutional encoder-decoder network is trained end-to-end by concatenating images with these distance maps as inputs and predicting the object masks as outputs. Our approach gets accurate segmentation results given sloppy rectangles while being general for both interactive segmentation and instance segmentation. We show our network extends to curve-based input without retraining. We further apply our network to instance-level semantic segmentation and resolve any overlap using a conditional random \ufb01eld.",
    "code_link": ""
  },
  "bmvc2017_main_solarpowerplantdetectiononmulti-spectralsatelliteimageryusingweakly-supervisedcnnwithfeedbackfeaturesandm-pcnnfusion": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Solar Power Plant Detection on Multi-Spectral Satellite Imagery using Weakly-Supervised CNN with Feedback Features and m-PCNN Fusion",
    "authors": [
      "Nevrez Imamoglu",
      "Motoki Kimura",
      "Hiroki Miyamoto",
      "Aito Fujita",
      "Ryosuke Nakamura"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper183/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper183/paper183.pdf",
    "published": "2017-09",
    "summary": "Most of the traditional convolutional neural networks (CNNs) implement bottom-up approach (feed-forward) for image classi\ufb01cations. However, many scienti\ufb01c studies demonstrate that visual perception in primates rely on both bottom-up and top-down connections. Therefore, in this work, we propose a CNN network with feedback structure for solar power plant detection on middle-resolution satellite images. To express the strength of the top-down connections, we introduce feedback CNN network (FB-Net) to a baseline CNN model used for solar power plant classi\ufb01cation on multi-spectral satellite data. Moreover, we introduce a method to improve class activation mapping (CAM) to our FB-Net, which takes advantage of multi-channel pulse coupled neural network (m-PCNN) for weakly-supervised localization of the solar power plants from the features of proposed FB-Net.",
    "code_link": ""
  },
  "bmvc2017_main_multiple-kernellocal-patchdescriptor": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Multiple-Kernel Local-Patch Descriptor",
    "authors": [
      "Arun Mukundan",
      "Giorgos Tolias",
      "Ondrej Chum"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper184/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper184/paper184.pdf",
    "published": "2017-09",
    "summary": "We propose a multiple-kernel local-patch descriptor based on ef\ufb01cient match kernels of patch gradients. It combines two parametrizations of gradient position and direction, each parametrization provides robustness to a different type of patch miss-registration: polar parametrization for noise in the patch dominant orientation detection, Cartesian for imprecise location of the feature point.",
    "code_link": ""
  },
  "bmvc2017_main_autoscalerscale-attentionnetworksforvisualcorrespondence": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "AutoScaler: Scale-Attention Networks for Visual Correspondence",
    "authors": [
      "Shenlong Wang",
      "Linjie Luo",
      "Ning Zhang",
      "Jia Li"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper185/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper185/paper185.pdf",
    "published": "2017-09",
    "summary": "Finding visual correspondence between local features is key to many computer vision problems. While de\ufb01ning features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our architecture consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive sizes of equivalent receptive \ufb01eld over different scales of the input. The entire network can be trained end-to-end in a Siamese framework for visual correspondence tasks. Using the latest off-the-shelf architecture for the feature network, our method achieves competitive results compared to state-of-the-art methods on challenging optical \ufb02ow and semantic matching benchmarks, including Sintel, KITTI and CUB-2011. We also show that our attention network alone can be applied to existing hand-crafted feature descriptors (e.g Daisy) and improve their performance on visual correspondence tasks.",
    "code_link": ""
  },
  "bmvc2017_main_cross-viewganbasedvehiclegenerationforre-identification": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Cross-View GAN Based Vehicle Generation for Re-identification",
    "authors": [
      "Yi Zhou",
      "Ling Shao"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper186/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper186/paper186.pdf",
    "published": "2017-09",
    "summary": "Automatic vehicle re-identi\ufb01cation (re-ID) is highly valuable and signi\ufb01cant in public transportation systems, but has not achieved much progress since the visual appearances vary hugely across different viewpoints of a vehicle. Feature matching in this problem is extremely dif\ufb01cult, and traditional person re-ID algorithms cannot be suitably applied to vehicles. However, image generation by convolutional generative adversarial networks (GANs), which has obtained breakthrough progress, inspires us to generate vehicles in different viewpoints from only one visible view to tackle vehicle re-ID. In this work, we propose a new deep architecture, called Cross-View Generative Adversarial Network (XVGAN), to learn the features of vehicle images captured by cameras with disjoint views, and take the features as conditional variables to effectively infer cross-view images. Finally, the features of the original images are combined with the features of generated images in other views to learn distance metrics for vehicle re-ID.",
    "code_link": ""
  },
  "bmvc2017_main_efficientvideosummarizationusingprincipalpersonappearanceforvideo-basedpersonre-identification": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Efficient Video Summarization Using Principal Person Appearance for Video-Based Person Re-Identification",
    "authors": [
      "Seongro Yoon",
      "Furqan Khan",
      "Francois Bremond"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper187/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper187/paper187.pdf",
    "published": "2017-09",
    "summary": "In video-based person re-identi\ufb01cation, while most work has focused on problems of person signature representation and matching between different cameras, intra-sample variance is also a critical issue to be addressed. There are various factors that cause the intra-sample variance such as detection/tracking inconsistency, motion change and background. However, \ufb01nding individual solutions for each factor is dif\ufb01cult and complicated. To deal with the problem collectively, we assume that it is more effective to represent a video with signatures based on a few of the most stable and representative features rather than extract from all video frames. In this work, we propose an ef\ufb01cient approach to summarize a video into a few of discriminative features given those challenges. Primarily, our algorithm learns principal person appearance over an entire video sequence, based on low-rank matrix recovery method. We design the optimizer considering temporal continuity of the person appearance as a constraint on the low-rank based manner. In addition, we introduce a simple but ef\ufb01cient method to represent a video as groups of similar frames using recovered principal appearance.",
    "code_link": ""
  },
  "bmvc2017_main_fine-grainedforensicmatching": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Fine-Grained Forensic Matching",
    "authors": [
      "Bailey Kong",
      "James Supancic",
      "Deva Ramanan",
      "Charless Fowlkes"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper188/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper188/paper188.pdf",
    "published": "2017-09",
    "summary": "We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made dif\ufb01cult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We \ufb01nd that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for these specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance. For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness.",
    "code_link": ""
  },
  "bmvc2017_main_learningaccuratelow-bitdeepneuralnetworkswithstochasticquantization": {
    "conf_id": "BMVC2017",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "BMVC2017",
    "title": "Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization",
    "authors": [
      "Yinpeng Dong",
      "Jianguo Li",
      "Renkun Ni"
    ],
    "page_url": "http://www.bmva.org/bmvc/2017/papers/paper189/index.html",
    "pdf_url": "http://www.bmva.org/bmvc/2017/papers/paper189/paper189.pdf",
    "published": "2017-09",
    "summary": "Low-bit deep neural networks (DNNs) become critical for embedded applications due to their low storage requirement and computing ef\ufb01ciency. However, they suffer much from the non-negligible accuracy drop. This paper proposes the stochastic quantization (SQ) algorithm for learning accurate low-bit DNNs. The motivation is due to the following observation. Existing training algorithms approximate the real-valued elements/\ufb01lters with low-bit representation all together in each iteration. The quantization errors may be small for some elements/\ufb01lters, while are remarkable for others, which lead to inappropriate gradient direction during training, and thus bring notable accuracy drop. Instead, SQ quantizes a portion of elements/\ufb01lters to low-bit with a stochastic probability inversely proportional to the quantization error, while keeping the other portion unchanged with full-precision. The quantized and full-precision portions are updated with corresponding gradients separately in each iteration. The SQ ratio is gradually increased until the whole network is quantized. This procedure can greatly compensate the quantization error and thus yield better accuracy for low-bit DNNs.",
    "code_link": "https://github.com/dongyp13/Stochastic-Quantization"
  }
}