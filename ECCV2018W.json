{
  "eccv2018_w1_thesixthvisualobjecttrackingvot2018challengeresults": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "The sixth Visual Object Tracking VOT2018 challenge results",
    "authors": [
      "Matej Kristan",
      "Ales Leonardis",
      "Jiri Matas",
      "Michael Felsberg",
      "Roman Pflugfelder",
      "Luka Cehovin Zajc",
      "Tomas Vojir",
      "Goutam Bhat",
      "et al."
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Kristan_The_sixth_Visual_Object_Tracking_VOT2018_challenge_results_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Kristan_The_sixth_Visual_Object_Tracking_VOT2018_challenge_results_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a \"real-time\" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking subchallenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new longterm tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website."
  },
  "eccv2018_w1_ontheoptimizationofadvanceddcf-trackers": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "On the Optimization of Advanced DCF-Trackers",
    "authors": [
      "Joakim Johnander",
      "Goutam Bhat",
      "Martin Danelljan",
      "Fahad Shahbaz Khan",
      "Michael Felsberg"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Johnander_On_the_Optimization_of_Advanced_DCF-Trackers_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Johnander_On_the_Optimization_of_Advanced_DCF-Trackers_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Trackers based on discriminative correlation filters (DCF) have recently seen widespread success and in this work we dive into their numerical core. DCF-based trackers interleave learning of the target detector and target state inference based on this detector. Whereas the original formulation includes a closed-form solution for the filter learning, recently introduced improvements to the framework no longer have known closed-form solutions. Instead a large-scale linear least squares problem must be solved each time the detector is updated. We analyze the procedure used to optimize the detector and let the popular scheme introduced with ECO serve as a baseline. The ECO implementation is revisited in detail and several mechanisms are provided with alternatives. With comprehensive experiments we show which configurations are superior in terms of tracking capabilities and optimization performance."
  },
  "eccv2018_w1_channelpruningforvisualtracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "Channel pruning for visual tracking",
    "authors": [
      "Manqiang Che",
      "Runling Wang",
      "Yan Lu",
      "Yan Li",
      "Hui Zhi",
      "Changzhen Xiong"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Che_Channel_pruning_for_visual_tracking_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Che_Channel_pruning_for_visual_tracking_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Deep convolutional feature based Correlation Filter trackers have achieved record-breaking accuracy, but the huge computational complexity limits their application. In this paper, we derive the efficient convolution operators(ECO) tracker which obtains the top rank on VOT-2016. Firstly, we introduce a channel pruned VGG16 model to fast extract most representative channels for deep features. Then an Average Feature Energy Ratio method is put forward to select advantageous convolution channels, and an adaptive iterative strategy is designed to optimize object location. Finally, extensive experimental results on four benchmarks OTB-2013, OTB-2015, VOT-2016 and VOT-2017, demonstrate that our tracker performs favorably against the state-of-the-art methods."
  },
  "eccv2018_w1_waefweightedaggregationwithenhancementfilterforvisualobjecttracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "WAEF: Weighted Aggregation with Enhancement Filter for Visual Object Tracking",
    "authors": [
      "Litu Rout",
      "Deepak Mishra",
      "Rama Krishna Sai Subrahmanyam Gorthi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Rout_WAEF_Weighted_Aggregation_with_Enhancement_Filter_for_Visual_Object_Tracking_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Rout_WAEF_Weighted_Aggregation_with_Enhancement_Filter_for_Visual_Object_Tracking_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In the recent years, convolutional neural networks (CNN) have been extensively employed in various complex computer vision tasks including visual object tracking. In this paper, we study the efficacy of temporal regression with Tikhonov regularization in generic object tracking. Among other major aspects, we propose a different approach to regress in the temporal domain, based on weighted aggregation of distinctive visual features and feature prioritization with entropy estimation in a recursive fashion. We provide a statistics based ensembler approach for integrating the conventionally driven spatial regression results (such as from ECO), and the proposed temporal regression results to accomplish better tracking. Further, we exploit the obligatory dependency of deep architectures on provided visual information, and present an image enhancement filter that helps to boost the performance on popular benchmarks. Our extensive experimentation shows that the proposed weighted aggregation with enhancement filter (WAEF) tracker outperforms the baseline (ECO) in almost all the challenging categories on OTB50 dataset with a cumulative gain of 14.8%. As per the VOT2016 evaluation, the proposed framework offers substantial improvement of 19.04% in occlusion, 27.66% in illumination change, 33.33% in empty, 10% in size change, and 5.28% in average expected overlap."
  },
  "eccv2018_w1_amemorymodelbasedonthesiamesenetworkforlong-termtracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "A Memory Model based on the Siamese Network for Long-term Tracking",
    "authors": [
      "Hankyeol Lee",
      "Seokeon Choi",
      "Changick Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Lee_A_Memory_Model_based_on_the_Siamese_Network_for_Long-term_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Lee_A_Memory_Model_based_on_the_Siamese_Network_for_Long-term_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a novel memory model using deep convolutional features for long-term tracking to handle the challenging issues, including visual deformation or target disappearance. Our memory model is separated into shortand long-term stores inspired by Atkinson-Shiffrin Memory Model (ASMM). In the tracking step, the bounding box of the target is estimated by the Siamese features obtained from both memory stores to accommodate changes in the visual appearance of the target. In the re-detection step, we take features only in the long-term store to alleviate the drift problem. At this time, we adopt a coarse-to-fine strategy to detect the target in the entire image without the dependency of the previous position. In the end, we employ Regional Maximum Activation of Convolutions (R-MAC) as key criteria. Our tracker achieves an F-score of 0.52 on the LTB35 dataset, which is 0.04 higher than the performance of the state-of-the-art algorithm."
  },
  "eccv2018_w1_multiplecontextfeaturesinsiamesenetworksforvisualobjecttracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "Multiple Context Features in Siamese Networks for Visual Object Tracking",
    "authors": [
      "Henrique Morimitsu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Morimitsu_Multiple_Context_Features_in_Siamese_Networks_for_Visual_Object_Tracking_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Morimitsu_Multiple_Context_Features_in_Siamese_Networks_for_Visual_Object_Tracking_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Siamese networks have been successfully utilized to learn a robust matching function between pairs of images. Visual object tracking methods based on siamese networks have been gaining popularity recently due to their robustness and speed. However, existing siamese approaches are still unable to perform on par with the most accurate trackers. In this paper, we propose to extend the SiamFC tracker [1] to extract features at multiple context and semantic levels from very deep networks. We show that our approach effectively extracts complementary features for siamese matching from different layers, which provides a significant performance boost when fused. Experimental results on VOT and OTB datasets show that our multi-context tracker is comparable to the most accurate methods, while still being faster than most of them. In particular, we outperform several other state-of-the-art siamese methods."
  },
  "eccv2018_w1_towardsabettermatchinsiamesenetworkbasedvisualobjecttracker": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "Towards a Better Match in Siamese Network Based Visual Object Tracker",
    "authors": [
      "Anfeng He",
      "Chong Luo",
      "Xinmei Tian",
      "Wenjun Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/He_Towards_a_Better_Match_in_Siamese_Network_Based_Visual_Object_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/He_Towards_a_Better_Match_in_Siamese_Network_Based_Visual_Object_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recently,Siamesenetworkbasedtrackershavereceivedtremendous interest for their fast tracking speed and high performance. Despite the great success, this tracking framework still suffers from several limitations. First, it cannot properly handle large object rotation. Second, tracking gets easily distracted when the background contains salient objects. In this paper, we propose two simple yet effective mechanisms, namely angle estimation and spatial masking, to address these issues. The objective is to extract more representative features so that a better match can be obtained between the same object from different frames. The resulting tracker, named Siam-BM, not only significantly improves the tracking performance, but more importantly maintains the realtime capability. Evaluations on the VOT2017 dataset show that Siam-BM achieves an EAO of 0.335, which makes it the best-performing realtime tracker to date."
  },
  "eccv2018_w1_howtomakeanrgbdtracker?": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "How to Make an RGBD Tracker ?",
    "authors": [
      "Ugur Kart",
      "Joni-Kristian Kamarainen",
      "Jiri Matas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Kart_How_to_Make_an_RGBD_Tracker__ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Kart_How_to_Make_an_RGBD_Tracker__ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a generic framework for converting an arbitrary short-term RGB tracker into an RGBD tracker. The proposed framework has two mild requirements \u00e2\u0080\u0093 the short-term tracker provides a bounding box and its object model update can be stopped and resumed. The core of the framework is a depth augmented foreground segmentation which is formulated as an energy minimization problem solved by graph cuts. The proposed framework offers two levels of integration. The first requires that the RGB tracker can be stopped and resumed according to the decision on target visibility. The level-two integration requires that the tracker accept an external mask (foreground region) in the target update. We integrate in the proposed framework the Discriminative Correlation Filter (DCF), and three state-of-the-art trackers \u00e2\u0080\u0093 Efficient Convolution Operators for Tracking (ECOhc, ECOgpu) and Discriminative Correlation Filter with Channel and Spatial Reliability (CSR-DCF). Comprehensive experiments on Princeton Tracking Benchmark (PTB) show that level-one integration provides significant improvements for all trackers: DCF average rank improves from 18th to 17th, ECOgpu from 16th to 10th, ECOhc from 15th to 5th and CSR-DCF from 19th to 14th. CSR-DCF with level-two integration achieves the top rank by a clear margin on PTB. Our framework is particularly powerful in occlusion scenarios where it provides 13.5% average improvement and 26% for the best tracker (CSR-DCF)."
  },
  "eccv2018_w1_learningarobustsocietyoftrackingpartsusingco-occurrenceconstraints": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - The Visual Object Tracking Challenge Workshop",
    "title": "Learning a Robust Society of Tracking Parts using Co-occurrence Constraints",
    "authors": [
      "Elena Burceanu",
      "Marius Leordeanu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w1/html/Burceanu_Learning_a_Robust_Society_of_Tracking_Parts_using_Co-occurrence_Constraints_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Burceanu_Learning_a_Robust_Society_of_Tracking_Parts_using_Co-occurrence_Constraints_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Object tracking is an essential problem in computer vision that has been researched for several decades. One of the main challenges in tracking is to adapt to object appearance changes over time and avoiding drifting to background clutter. We address this challenge by proposing a deep neural network composed of different parts, which functions as a society of tracking parts. They work in conjunction according to a certain policy and learn from each other in a robust manner, using cooccurrence constraints that ensure robust inference and learning. From a structural point of view, our network is composed of two main pathways. One pathway is more conservative. It carefully monitors a large set of simple tracker parts learned as linear filters over deep feature activation maps. It assigns the parts different roles. It promotes the reliable ones and removes the inconsistent ones. We learn these filters simultaneously in an efficient way, with a single closed-form formulation, for which we propose novel theoretical properties. The second pathway is more progressive. It is learned completely online and thus it is able to better model object appearance changes. In order to adapt in a robust manner, it is learned only on highly confident frames, which are decided using co-occurrences with the first pathway. Thus, our system has the full benefit of two main approaches in tracking. The larger set of simpler filter parts offers robustness, while the full deep network learned online provides adaptability to change. As shown in the experimental section, our approach achieves state of the art performance on the challenging VOT17 benchmark, outperforming the published methods both on the general EAO metric and in the number of fails, by a significant margin."
  },
  "eccv2018_w2_semanticsegmentationoffisheyeimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Semantic Segmentation of Fisheye Images",
    "authors": [
      "Gregor Blott",
      "Masato Takami",
      "Christian Heipke"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Blott_Semantic_Segmentation_of_Fisheye_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Blott_Semantic_Segmentation_of_Fisheye_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Semantic segmentation of fisheye images (e.g., from actioncameras or smartphones) requires different training approaches and data than those of rectilinear images obtained using central projection. The shape of objects is distorted depending on the distance between the principal point and the object position in the image. Therefore, classical semantic segmentation approaches fall short in terms of performance compared to rectilinear data. A potential solution to this problem is the recording and annotation of a new dataset, however this is expensive and tedious. In this study, an alternative approach that modifies the augmentation stage of deep learning training to re-use rectilinear training data is presented. In this way we obtain a considerably higher semantic segmentation performance on the fisheye images: +18.3% intersection over union (IoU) for action-camera test images, +8.3% IoU for artificially generated fisheye data, and +18.0% IoU for challenging security scenes acquired in bird\u00e2\u0080\u0099s eye view."
  },
  "eccv2018_w2_complex-yoloaneuler-region-proposalforreal-time3dobjectdetectiononpointclouds": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Complex-YOLO: An Euler-Region-Proposal for Real-time 3D Object Detection on Point Clouds",
    "authors": [
      "Martin Simony",
      "Stefan Milzy",
      "Karl Amendey",
      "Horst-Michael Gross"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Simony_Complex-YOLO_An_Euler-Region-Proposal_for_Real-time_3D_Object_Detection_on_Point_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Simony_Complex-YOLO_An_Euler-Region-Proposal_for_Real-time_3D_Object_Detection_on_Point_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-RegionProposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTIclasses, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy."
  },
  "eccv2018_w2_itsnotallaboutsizeontheroleofdatapropertiesinpedestriandetection": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "It's Not All About Size: On the Role of Data Properties in Pedestrian Detection",
    "authors": [
      "Amir Rasouli",
      "Iuliia Kotseruba",
      "John K. Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Rasouli_Its_Not_All_About_Size_On_the_Role_of_Data_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Rasouli_Its_Not_All_About_Size_On_the_Role_of_Data_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Pedestrian detection is central in applications such as autonomous driving. The performance of algorithms tailored to solve this problem has been extensively evaluated on benchmark datasets, such as Caltech, which do not adequately represent the diversity of traffic scenes. Consequently, the true performance of algorithms and their limitations in practice remain understudied.To this end, we conduct an empirical study using 7 classical and state-ofthe-art algorithms on the recently proposed JAAD dataset augmented with 16 additional labels for pedestrian attributes. Using this data we show that the relative performance of the algorithms varies depending on the properties of the training data.We analyze the contribution of weather conditions and pedestrian attributes to performance changes and examine the major sources of detection errors. Finally, we show that the diversity of the training data leads to better generalizability of the algorithms across different datasets even with a smaller number of samples."
  },
  "eccv2018_w2_real-timepointcloudalignmentforvehiclelocalizationinahighresolution3dmap": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Real-time point cloud alignment for vehicle localization in a high resolution 3D map",
    "authors": [
      "Balazs Nagy",
      "Csaba Benedek"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Nagy_Real-time_point_cloud_alignment_for_vehicle_localization_in_a_high_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Nagy_Real-time_point_cloud_alignment_for_vehicle_localization_in_a_high_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper we introduce a Lidar based real time and accurate self localization approach for self driving vehicles (SDV) in high resolution 3D point cloud maps of the environment obtained through Mobile Laser Scanning (MLS). Our solution is able to robustly register the sparse point clouds of the SDVs to the dense MLS point cloud data, starting from a GPS based initial position estimation of the vehicle. The main steps of the method are robust object extraction and transformation estimation based on multiple keypoints extracted from the objects, and additional semantic information derived from the MLS based map. We tested our approach on roads with heavy traffic in the downtown of a large city with large GPS positioning errors, and showed that the proposed method enhances the matching accuracy with an order of magnitude. Comparative tests are provided with various keypoint selection strategies, and against a state-of-the-art technique."
  },
  "eccv2018_w2_exploitingsingleimagedepthpredictionformono-stixelestimation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Exploiting single image depth prediction for mono-stixel estimation",
    "authors": [
      "Fabian Brickwedde",
      "Steffen Abraham",
      "Rudolf Mester"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Brickwedde_Exploiting_single_image_depth_prediction_for_mono-stixel_estimation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Brickwedde_Exploiting_single_image_depth_prediction_for_mono-stixel_estimation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The stixel-world is a compact and detailed environment representation specially designed for street scenes and automotive vision applications. A recent work proposes a monocamera based stixel estimation method based on the structure from motion principle and scene model to predict the depth and translational motion of the static and dynamic parts of the scene. In this paper, we propose to exploit the recent advantages in deep learning based single image depth prediction for mono-stixel estimation. In our approach, the mono-stixels are estimated based on the single image depth predictions, a dense optical flow field and semantic segmentation supported by the prior knowledge about the characteristic of typical street scenes. To provide a meaningful estimation, it is crucial to model the statistical distribution of all measurements, which is especially challenging for the single image depth predictions. Therefore, we present a semantic class dependent measurement model of the single image depth prediction derived from the empirical error distribution on the Kitti dataset.Our experiments on the Kitti-Stereo\u00e2\u0080\u00992015 dataset show that we can significantly improve the quality of mono-stixel estimation by exploiting the single image depth prediction. Furthermore, our proposed approach is able to handle partly occluded moving objects as well as scenarios without translational motion of the camera."
  },
  "eccv2018_w2_el-ganembeddinglossdrivengenerativeadversarialnetworksforlanedetection": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection",
    "authors": [
      "Mohsen Ghafoorian",
      "Cedric Nugteren",
      "Nora Baka",
      "Olaf Booij",
      "Michael Hofmann"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Ghafoorian_EL-GAN_Embedding_Loss_Driven_Generative_Adversarial_Networks_for_Lane_Detection_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Ghafoorian_EL-GAN_Embedding_Loss_Driven_Generative_Adversarial_Networks_for_Lane_Detection_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This illposed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing.In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in much more stable training due to having better discriminative information, benefiting from seeing both \u00e2\u0080\u0098fake\u00e2\u0080\u0099 and \u00e2\u0080\u0098real\u00e2\u0080\u0099 predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold."
  },
  "eccv2018_w2_scaledriftcorrectionofcamerageo-localizationusinggeo-taggedimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Scale Drift Correction of Camera Geo-Localization using Geo-Tagged Images",
    "authors": [
      "Kazuya Iwami",
      "Satoshi Ikehata",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Iwami_Scale_Drift_Correction_of_Camera_Geo-Localization_using_Geo-Tagged_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Iwami_Scale_Drift_Correction_of_Camera_Geo-Localization_using_Geo-Tagged_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Camera geo-localization from a monocular video is a fundamental task for video analysis and autonomous navigation. Although 3D reconstruction is a key technique to obtain camera poses, monocular 3D reconstruction in a large environment tends to result in the accumulation of errors in rotation, translation, and especially in scale: a problem known as scale drift. To overcome these errors, we propose a novel framework that integrates incremental structure from motion (SfM) and a scale drift correction method utilizing geo-tagged images, such as those provided by Google Street View. Our correction method begins by obtaining sparse 6-DoF correspondences between the reconstructed 3D map coordinate system and the world coordinate system, by using geo-tagged images. Then, it corrects scale drift by applying pose graph optimization over Sim(3) constraints and bundle adjustment. Experimental evaluations on large-scale datasets show that the proposed framework not only sufficiently corrects scale drift, but also achieves accurate geo-localization in a kilometer-scale environment."
  },
  "eccv2018_w2_distantvehicledetectionhowwellcanregionproposalnetworkscopewithtinyobjectsatlowresolution?": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Distant Vehicle Detection: How Well Can Region Proposal Networks Cope With Tiny Objects at Low Resolution?",
    "authors": [
      "Ann-Katrin Fattal",
      "Michelle Karg",
      "Christian Scharfenberger",
      "J urgen Adamy"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w2/html/Fattal_Distant_Vehicle_Detection_How_Well_Can_Region_Proposal_Networks_Cope_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Fattal_Distant_Vehicle_Detection_How_Well_Can_Region_Proposal_Networks_Cope_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " High-performance faster R-CNN has been applied to many detection tasks. Detecting tiny objects at very low resolution remains a challenge, however, and a few studies addressed explicitly the detection of such objects yet. Focusing on distant object detection at very low resolution images for driver assistance systems, we introduce post-trained net surgery to 1) analyze the network activation patterns, 2) study the potential of prior information to improve localization and binary classification performance, and 3) to support the development of priors for improving the network performance.We use post-trained net surgery to analyze the feature maps used for bounding box regression and classification for RPNs in detail, and to discuss the complexity of the network activation patterns. Using these findings, we show that incorporating prior maps into the network architecture improves the performance of bounding box regression and binary classification for small object detection in low resolution images."
  },
  "eccv2018_w3_deepdepthfromdefocushowcandefocusblurimprove3destimationusingdenseneuralnetworks?": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Deep Depth from Defocus: how can defocus blur improve 3D estimation using dense neural networks?",
    "authors": [
      "Marcela Carvalho",
      "Bertrand Le Saux",
      "Pauline Trouve-Peloux",
      "Andres Almansa",
      "Frederic Champagnat"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Carvalho_Deep_Depth_from_Defocus_how_can_defocus_blur_improve_3D_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Carvalho_Deep_Depth_from_Defocus_how_can_defocus_blur_improve_3D_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Depth estimation is critical interest for scene understanding and accurate 3D reconstruction. Most recent approaches with deep learning exploit geometrical structures of standard sharp images to predict depth maps. However, cameras can also produce images with defocus blur depending on the depth of the objects and camera settings. Hence, these features may represent an important hint for learning to predict depth. In this paper, we propose a full system for single-image depth prediction in the wild using depth-from-defocus and neural networks. We carry out thorough experiments real and simulated defocused images using a realistic model of blur variation with respect to depth. We also investigate the influence of blur on depth prediction observing model uncertainty with a Bayesian neural network approach. From these studies, we show that out-of-focus blur greatly improves the depth-prediction network performances. Furthermore, we transfer the ability learned on a synthetic, indoor dataset to real, indoor and outdoor images. For this purpose, we present a new dataset with real all-focus and defocused images from a DSLR camera, paired with ground truth depth maps obtained with an active 3D sensor for indoor scenes. The proposed approach is successfully validated on both this new dataset and standard ones as NYUv2 or Depth-in-the-Wild. Code and new datasets are available at https://github.com/marcelampc/d3net depth estimation."
  },
  "eccv2018_w3_deepmodularnetworkarchitecturefordepthestimationfromsingleindoorimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Deep Modular Network Architecture for Depth Estimation from Single Indoor Images",
    "authors": [
      "Seiya Ito",
      "Naoshi Kaneko",
      "Yuma Shinohara",
      "Kazuhiko Sumi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Ito_Deep_Modular_Network_Architecture_for_Depth_Estimation_from_Single_Indoor_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Ito_Deep_Modular_Network_Architecture_for_Depth_Estimation_from_Single_Indoor_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a novel deep modular network architecture for indoor scene depth estimation from single RGB images. The proposed architecture consists of a main depth estimation network and two auxiliary semantic segmentation networks. Our insight is that semantic and geometrical structures in a scene are strongly correlated, thus we utilize global (i.e. room layout) and mid-level (i.e. objects in a room) semantic structures to enhance depth estimation. The first auxiliary network, or layout network, is responsible for room layout estimation to infer the positions of walls, floor, and ceiling of a room. The second auxiliary network, or object network, estimates per-pixel class labels of the objects in a scene, such as furniture, to give mid-level semantic cues. Estimated semantic structures are effectively fed into the depth estimation network using newly proposed discriminator networks, which discern the reliability of the estimated structures. The evaluation result shows that our architecture achieves significant performance improvements over previous approaches on the standard NYU Depth v2 indoor scene dataset."
  },
  "eccv2018_w3_generativeadversarialnetworksforunsupervisedmonoculardepthprediction": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Generative Adversarial Networks for unsupervised monocular depth prediction",
    "authors": [
      "Filippo Aleotti",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Aleotti_Generative_Adversarial_Networks_for_unsupervised_monocular_depth_prediction_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Aleotti_Generative_Adversarial_Networks_for_unsupervised_monocular_depth_prediction_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Estimating depth from a single image is a very challenging and exciting topic in computer vision with implications in several application domains. Recently proposed deep learning approaches achieve outstanding results by tackling it as an image reconstruction task and exploiting geometry constraints (e.g., epipolar geometry) to obtain supervisory signals for training. Inspired by these works and compelling results achieved by Generative Adversarial Network (GAN) on image reconstruction and generation tasks, in this paper we propose to cast unsupervised monocular depth estimation within a GAN paradigm. The generator network learns to infer depth from the reference image to generate a warped target image. At training time, the discriminator network learns to distinguish between fake images generated by the generator and target frames acquired with a stereo rig. To the best of our knowledge, our proposal is the first successful attempt to tackle monocular depth estimation with a GAN paradigm and the extensive evaluation on CityScapes and KITTI datasets confirm that it enables to improve traditional approaches. Additionally, we highlight a major issue with data deployed by a standard evaluation protocol widely used in this field and fix this problem using a more reliable dataset recently made available by the KITTI evaluation benchmark."
  },
  "eccv2018_w3_combinationofspatially-modulatedtofandstructuredlightformpi-freedepthestimation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Combination of Spatially-Modulated ToF and Structured Light for MPI-Free Depth Estimation",
    "authors": [
      "Gianluca Agresti",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Agresti_Combination_of_Spatially-Modulated_ToF_and_Structured_Light_for_MPI-Free_Depth_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Agresti_Combination_of_Spatially-Modulated_ToF_and_Structured_Light_for_MPI-Free_Depth_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Multi-path Interference (MPI) is one of the major sources of error in Time-of-Flight (ToF) camera depth measurements. A possible solution for its removal is based on the separation of direct and global light through the projection of multiple sinusoidal patterns. In this work we extend this approach by applying a Structured Light (SL) technique on the same projected patterns. This allows to compute two depth maps with a single ToF acquisition, one with the Time-of-Flight principle and the other with the Structured Light principle. The two depth fields are finally combined using a Maximum-Likelihood approach in order to obtain an accurate depth estimation free from MPI error artifacts. Experimental results demonstrate that the proposed method has very good MPI correction properties with state-of-the-art performances."
  },
  "eccv2018_w3_robuststructuredlightsystemagainstsubsurfacescatteringeffectsachievedbycnn-basedpatterndetectionanddecodingalgorithm": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Robust structured light system against subsurface scattering effects achieved by CNN-based pattern detection and decoding algorithm",
    "authors": [
      "Ryo Furukawa",
      "Daisuke Miyazaki",
      "Masashi Baba",
      "Shinsaku Hiura",
      "Hiroshi Kawasaki"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Furukawa_Robust_structured_light_system_against_subsurface_scattering_effects_achieved_by_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Furukawa_Robust_structured_light_system_against_subsurface_scattering_effects_achieved_by_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " To reconstruct 3D shapes of real objects, a structured-light technique has been commonly used especially for practical purposes, such as inspection, industrial modeling, medical diagnosis, etc, because of simplicity, stability and high precision. Among them, oneshot scanning technique, which requires only single image for reconstruction, becomes important for the purpose of capturing moving objects. One open problem of oneshot scanning technique is its instability, when captured pattern is degraded by some reasons, such as strong specularity, subsurface scattering, inter-reflection and so on. One of important targets for oneshot scan is live animal, which includes human body or tissue of organ, and has subsurface scattering. In this paper, we propose a learning-based approach to solve pattern degradation caused by subsurface scattering for oneshot scan. Since patterns are significantly blurred by subsurface scattering, robust decoding technique is required, which is effectively achieved by separating the decoding process into two parts, such as pattern detection and ID recognition in our technique; both parts are implemented by CNN. To efficiently achieve robust pattern detection, we convert a line detection into segmentation problem. For robust ID recognition, we segment all the region into each ID using U-Net. In the experiments, it is shown that our technique is robust against strong subsurface scattering compared to state of the art technique."
  },
  "eccv2018_w3_robust3dpigmeasurementinpigfarm": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Robust 3D Pig Measurement in Pig Farm",
    "authors": [
      "Kumiko Yoshida",
      "Kikuhito Kawasue"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Yoshida_Robust_3D_Pig_Measurement_in_Pig_Farm_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Yoshida_Robust_3D_Pig_Measurement_in_Pig_Farm_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " On a pig farm, the shipment of pigs of proper weight is very important for increasing profit. However, in order to reduce labor costs, many farmers ship pigs without weighing them. Therefore, an automatic sorting system that selects pigs that have reached the proper weight by measuring the weight of each pig has been developed. In the present paper, a weight estimation system using a camera for pig sorting is introduced. Three-dimensional visual information on a pig captured in a single image is used to estimate its weight. The proposed method is robust and practical for the measurement of a moving animal in a poor environment of pig farms."
  },
  "eccv2018_w3_sconesiameseconstellationembeddingdescriptorforimagematching": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "SConE: Siamese Constellation Embedding Descriptor for Image Matching",
    "authors": [
      "Tomasz Trzcinski",
      "Jacek Komorowski",
      "Lukasz Dabala",
      "Konrad Czarnota",
      "Grzegorz Kurzejamski",
      "Simon Lynen"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Trzcinski_SConE_Siamese_Constellation_Embedding_Descriptor_for_Image_Matching_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Trzcinski_SConE_Siamese_Constellation_Embedding_Descriptor_for_Image_Matching_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Numerous computer vision applications rely on local feature descriptors, such as SIFT, SURF or FREAK, for image matching. Although their local character makes image matching processes more robust to occlusions, it often leads to geometrically inconsistent keypoint matches that need to be filtered out, e.g. using RANSAC. In this paper we propose a novel, more discriminative, descriptor that includes not only local feature representation, but also information about the geometric layout of neighbouring keypoints. To that end, we use a Siamese architecture that learns a low-dimensional feature embedding of keypoint constellation by maximizing the distances between non-corresponding pairs of matched image patches, while minimizing it for correct matches. The 48-dimensional floating point descriptor that we train is built on top of the state-of-the-art FREAK descriptor achieves significant performance improvement over the competitors on a challenging TUM dataset."
  },
  "eccv2018_w3_rgb-dslambasedincrementalcuboidmodeling": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "RGB-D SLAM based Incremental Cuboid Modeling",
    "authors": [
      "Masashi Mishima",
      "Hideaki Uchiyama",
      "Diego Thomas",
      "Rin-ichiro Taniguchi",
      "Rafael Roberto",
      "Jo ao Paulo Lima",
      "Veronica Teichrieb"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Mishima_RGB-D_SLAM_based_Incremental_Cuboid_Modeling_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Mishima_RGB-D_SLAM_based_Incremental_Cuboid_Modeling_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper present a framework for incremental 3D cuboid modeling combined with RGB-D SLAM. While performing RGB-D SLAM, planes are incrementally reconstructed from point clouds. Then, cuboids are detected in the planes by analyzing the positional relationships between the planes; orthogonality, convexity, and proximity. Finally, the position, pose and size of a cuboid are determined by computing the intersection of three perpendicular planes. In addition, the cuboid shapes are incrementally updated to suppress false detections with sequential measurements. As an application of our framework, an augmented reality based interactive cuboid modeling system is introduced. In the evaluation at a cluttered environment, the precision and recall of the cuboid detection are improved with our framework owing to stable plane detection, compared with a batch based method."
  },
  "eccv2018_w3_semi-independentstereovisualodometryfordifferentfieldofviewcameras": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Semi-independent Stereo Visual Odometry for Different Field of View Cameras",
    "authors": [
      "Trong Phuc Truong",
      "Vincent Nozick",
      "Hideo Saito"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Truong_Semi-independent_Stereo_Visual_Odometry_for_Different_Field_of_View_Cameras_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Truong_Semi-independent_Stereo_Visual_Odometry_for_Different_Field_of_View_Cameras_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents a pipeline for stereo visual odometry using cameras with different fields of view. It gives a proof of concept about how a constraint on the respective field of view of each camera can lead to both an accurate 3D reconstruction and a robust pose estimation. Indeed, when considering a fixed resolution, a narrow field of view has a higher angular resolution and can preserve image texture details. On the other hand, a wide field of view allows to track features over longer periods since the overlap between two successive frames is more substantial. We propose a semi-independent stereo system where each camera performs individually temporal multi-view optimization but their initial parameters are still jointly optimized in an iterative framework. Furthermore, the concept of lead and follow camera is introduced to adaptively propagate information between the cameras. We evaluate the method qualitatively on two indoor datasets, and quantitatively on a synthetic dataset to allow the comparison across different fields of view."
  },
  "eccv2018_w3_improvingthinstructuresinsurfacereconstructionfromsparsepointcloud": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Improving thin structures in surface reconstruction from sparse point cloud",
    "authors": [
      "Maxime Lhuillier"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Lhuillier_Improving_thin_structures_in_surface_reconstruction_from_sparse_point_cloud_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Lhuillier_Improving_thin_structures_in_surface_reconstruction_from_sparse_point_cloud_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Methods were proposed to estimate a surface from a sparse cloud of points reconstructed from images. These methods are interesting in several contexts including large scale scenes, limited computational resources and initialization of dense stereo. However they are deficient in presence of thin structures such as posts, which are often present in both urban and natural scenes: these scene components can be partly or even completely removed. Here we reduce this problem by introducing a pre-processing, assuming that (1) some of the points form polygonal chains approximating curves and occluding contours of the scene and (2) the direction of the thin structures is roughly known (e.g. vertical). In the experiments, our pre-processing improves the results of two different surface reconstruction methods applied on videos taken by helmet-held 360 cameras."
  },
  "eccv2018_w3_polygonalreconstructionofbuildinginteriorsfromclutteredpointclouds": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Polygonal reconstruction of building interiors from cluttered pointclouds",
    "authors": [
      "Inge Coudron",
      "Steven Puttemans",
      "Toon Goedeme"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Coudron_Polygonal_reconstruction_of_building_interiors_from_cluttered_pointclouds_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Coudron_Polygonal_reconstruction_of_building_interiors_from_cluttered_pointclouds_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we propose a framework for reconstructing a compact geometric model from point clouds of building interiors. Geometric reconstruction of indoor scenes is especially challenging due to clutter in the scene, such as furniture and cabinets. The clutter may (partially) hide the structural components of the interior. The proposed framework is able to cope with this clutter by using a hypothesizing and selection strategy, in which candidate faces are firstly generated by intersecting the extracted planar primitives. Secondly, an optimal subset of candidate faces is selected by optimizing a binary labeling problem. We formulate the selection problem as a continuous quadratic optimization problem, allowing us to incorporate a cost function specifically for indoor scenes. The obtained polygonal surface is not only 2-manifold but also oriented, meaning that the surface normals of each polygon are consistently oriented towards the interior. All adjacent and coplanar faces that were selected, are merged into a single face in order to obtain a final geometric model that is as compact as possible. This compact model of the room uses less memory and allows for faster processing when used in virtual reality applications. The method of L. Nan et al. was used as a starting point for our proposed framework. Finally, as opposed to other state-of-the-art interior modeling approaches, the only input that is required, is the point cloud itself. We do not rely on viewpoint information, nor do we assume constrained input environments with a 2.5D or, more restrictively, a Manhattan-world structure. To demonstrate the practical applicability of our proposed method, we performed various experiments on actual scan data of building interiors."
  },
  "eccv2018_w3_paired3dmodelgenerationwithconditionalgenerativeadversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Paired 3D Model Generation with Conditional Generative Adversarial Networks",
    "authors": [
      "Cihan  Ong un",
      "Alptekin Temizel"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/un_Paired_3D_Model_Generation_with_Conditional_Generative_Adversarial_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/un_Paired_3D_Model_Generation_with_Conditional_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Generative Adversarial Networks (GANs) are shown to be successful at generating new and realistic samples including 3D object models. Conditional GAN, a variant of GANs, allows generating samples in given conditions. However, objects generated for each condition are different and it does not allow generation of the same object in different conditions. In this paper, we first adapt conditional GAN, which is originally designed for 2D image generation, to the problem of generating 3D models in different rotations. We then propose a new approach to guide the network to generate the same 3D sample in different and controllable rotation angles (sample pairs). Unlike previous studies, the proposed method does not require modification of the standard conditional GAN architecture and it can be integrated into the training step of any conditional GAN. Experimental results and visual comparison of 3D models show that the proposed method is successful at generating model pairs in different conditions."
  },
  "eccv2018_w3_predictingmuscleactivityandjointanglefromskinshape": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction in the Wild",
    "title": "Predicting Muscle Activity and Joint Angle from Skin Shape",
    "authors": [
      "Ryusuke Sagawa",
      "Ko Ayusawa",
      "Yusuke Yoshiyasu",
      "Akihiko Murai"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w3/html/Sagawa_Predicting_Muscle_Activity_and_Joint_Angle_from_Skin_Shape_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Sagawa_Predicting_Muscle_Activity_and_Joint_Angle_from_Skin_Shape_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Muscle of human body can be a clue to recognize the behavior and intention of a person. If the muscle activity is measured only by visual observation, it is useful to estimate the state of the muscle. In this paper, a method of predicting muscle activity and joint angle of human body from skin shape is proposed. Since the muscle activity and the joint angle affect the skin shape, the both factors should be considered simultaneously. The proposed method is a learning-based approach that uses the data set of the skin shape, the muscle activity and the joint angle. It trains a linear regressor for predicting muscle activity and joint angle from skin shape. The deformation of skin shape is calculated as the feature in the active regions, which are extracted from the training data and limits the regions of the skin shape that contribute to the prediction. We acquired a lower limb with simple motion to consider the small number of factors in this paper. In the experiment, the muscle activity and joint angle are predicted even in the case that the both factors change simultaneously. The skin regions that contributes to prediction are given as the result of learning, and the distribution is reasonable from the viewpoint of biomechanics."
  },
  "eccv2018_w4_modelingcameraeffectstoimprovevisuallearningfromsyntheticdata": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Visual Learning and Embodied Agents in Simulation Environments",
    "title": "Modeling Camera Effects to Improve Visual Learning from Synthetic Data",
    "authors": [
      "Alexandra Carlson",
      "Katherine A. Skinner",
      "Ram Vasudevan",
      "Matthew Johnson-Roberson"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w4/html/Carlson_Modeling_Camera_Effects_to_Improve_Visual_Learning_from_Synthetic_Data_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Carlson_Modeling_Camera_Effects_to_Improve_Visual_Learning_from_Synthetic_Data_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recent work has focused on generating synthetic imagery to increase the size and variability of training data for learning visual tasks in urban scenes. This includes increasing the occurrence of occlusions or varying environmental and weather effects. However, few have addressed modeling variation in the sensor domain. Sensor effects can degrade real images, limiting generalizability of network performance on visual tasks trained on synthetic data and tested in real environments. This paper proposes an efficient, automatic, physically-based augmentation pipeline to vary sensor effects \u00e2\u0080\u0093 chromatic aberration, blur, exposure, noise, and color temperature \u00e2\u0080\u0093 for synthetic imagery. In particular, this paper illustrates that augmenting synthetic training datasets with the proposed pipeline reduces the domain gap between synthetic and real domains for the task of object detection in urban driving scenes."
  },
  "eccv2018_w4_answeringvisualwhat-ifquestionsfromactionstopredictedscenedescriptions": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Visual Learning and Embodied Agents in Simulation Environments",
    "title": "Answering Visual What-If Questions: From Actions to Predicted Scene Descriptions",
    "authors": [
      "Misha Wagner",
      "Hector Basevi",
      "Rakshith Shetty",
      "Wenbin Li",
      "Mateusz Malinowski",
      "Mario Fritz",
      "Ales Leonardis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w4/html/Wagner_Answering_Visual_em_What-If_Questions_From_Actions_to_Predicted_Scene_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Wagner_Answering_Visual_em_What-If_Questions_From_Actions_to_Predicted_Scene_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In-depth scene descriptions and question answering tasks have greatly increased the scope of today\u00e2\u0080\u0099s definition of scene understanding. While such tasks are in principle open ended, current formulations primarily focus on describing only the current state of the scenes under consideration. In contrast, in this paper, we focus on the future states of the scenes which are also conditioned on actions. We posit this as a question answering task, where an answer has to be given about a future scene state, given observations of the current scene, and a question that includes a hypothetical action. Our solution is a hybrid model which integrates a physics engine into a question answering architecture in order to anticipate future scene states resulting from object-object interactions caused by an action. We demonstrate first results on this challenging new problem and compare to baselines, where we outperform fully data-driven end-to-end learning approaches."
  },
  "eccv2018_w5_exploringbiasinprimatefacedetectionandrecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bias Estimation in Face Analytics",
    "title": "Exploring Bias in Primate Face Detection and Recognition",
    "authors": [
      "Sanchit Sinha",
      "Mohit Agarwal",
      "Mayank Vatsa",
      "Richa Singh",
      "Saket Anand"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w5/html/Sinha_Exploring_Bias_in_Primate_Face_Detection_and_Recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Sinha_Exploring_Bias_in_Primate_Face_Detection_and_Recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Deforestation and loss of habitat have resulted in rapid decline of certain species of primates in forests. On the other hand, uncontrolled growth of a few species of primates in urban areas has led to safety issues and nuisance for the local residents. Hence, identifying individual primates has become the need of the hour not only for conservation and effective mitigation in the wild but also in zoological parks and wildlife sanctuaries. Primates and human faces share a lot of common features like position and shape of eyes, nose and mouth. It is worth exploring whether the knowledge of human faces and recent methods learned from human face detection and recognition can be extended to primate faces. However, similar challenges relating to bias in human faces will also occur in primates. The quality and orientation of primate images along with different species of primates ranging from monkeys to gorillas and chimpanzees will contribute to bias in effective detection and recognition. Experimental results on a primate dataset of over 80 identities show the effect of bias in this research problem."
  },
  "eccv2018_w5_turningablindeyeexplicitremovalofbiasesandvariationfromdeepneuralnetworkembeddings": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bias Estimation in Face Analytics",
    "title": "Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings",
    "authors": [
      "Mohsan Alvi",
      "Andrew Zisserman",
      "Christoffer Nellaaker"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w5/html/Alvi_Turning_a_Blind_Eye_Explicit_Removal_of_Biases_and_Variation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Alvi_Turning_a_Blind_Eye_Explicit_Removal_of_Biases_and_Variation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Neural networks achieve the state-of-the-art in image classification tasks. However, they can encode spurious variations or biases that may be present in the training data. For example, training an age predictor on a dataset that is not balanced for gender can lead to gender biased predicitons (e.g. wrongly predicting that males are older if only elderly males are in the training set).We present two distinct contributions:1) An algorithm that can remove multiple sources of variation from the feature representation of a network. We demonstrate that this algorithm can be used to remove biases from the feature representation, and thereby improve classification accuracies, when training networks on extremely biased datasets.2) An ancestral origin database of 14,000 images of individuals from East Asia, the Indian subcontinent, sub-Saharan Africa, and Western Europe.We demonstrate on this dataset, for a number of facial attribute classification tasks, that we are able to remove racial biases from the network feature representation."
  },
  "eccv2018_w5_mitigatingbiasingender,ageandethnicityclassificationamulti-taskconvolutionneuralnetworkapproach": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bias Estimation in Face Analytics",
    "title": "Mitigating Bias in Gender, Age and Ethnicity Classification: a Multi-Task Convolution Neural Network Approach",
    "authors": [
      "Abhijit Das",
      "Antitza Dantcheva",
      "Francois Bremond"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w5/html/Das_Mitigating_Bias_in_Gender_Age_and_Ethnicity_Classification_a_Multi-Task_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Das_Mitigating_Bias_in_Gender_Age_and_Ethnicity_Classification_a_Multi-Task_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This work explores joint classification of gender, age and race. Specifically, we here propose a Multi-Task Convolution Neural Network (MTCNN) employing joint dynamic loss weight adjustment towards classification of named soft biometrics, as well as towards mitigation of soft biometrics related bias. The proposed algorithm achieves promising results on the UTKFace and the Bias Estimation in Face Analytics (BEFA) datasets and was ranked first in the the BEFA Challenge of the European Conference of Computer Vision (ECCV) 2018."
  },
  "eccv2018_w6_asummaryofthe4thinternationalworkshoponrecovering6dobjectpose": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "A Summary of the 4th International Workshop on~Recovering 6D Object Pose",
    "authors": [
      "Tomas Hodan",
      "Rigas Kouskouridas",
      "Tae-Kyun Kim",
      "Federico Tombari",
      "Kostas Bekris",
      "Bertram Drost",
      "Thibault Groueix",
      "Krzysztof Walas",
      "Vincent Lepetit",
      "Ales Leonardis",
      "Carsten Steger",
      "Frank Michel",
      "Caner Sahin",
      "Carsten Rother",
      "Jiri Matas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Hodan_A_Summary_of_the_4th_International_Workshop_onRecovering_6D_Object_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Hodan_A_Summary_of_the_4th_International_Workshop_onRecovering_6D_Object_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This document summarizes the 4th International Workshop on Recovering 6D Object Pose which was organized in conjunction with ECCV 2018 in Munich. The workshop featured four invited talks, oral and poster presentations of accepted workshop papers, and an introduction of the BOP benchmark for 6D object pose estimation. The workshop was attended by 100+ people working on relevant topics in both academia and industry who shared up-to-date advances and discussed open problems."
  },
  "eccv2018_w6_image-to-voxelmodeltranslationwithconditionaladversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "Image-to-Voxel Model Translation with Conditional Adversarial Networks",
    "authors": [
      "Vladimir A. Knyaz",
      "Vladimir V. Kniaz",
      "Fabio Remondino"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Knyaz_Image-to-Voxel_Model_Translation_with_Conditional_Adversarial_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Knyaz_Image-to-Voxel_Model_Translation_with_Conditional_Adversarial_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present a single-view voxel model prediction method that uses generative adversarial networks. Our method utilizes correspondences between 2D silhouettes and slices of a camera frustum to predict a voxel model of a scene with multiple object instances. We exploit pyramid shaped voxel and a generator network with skip connections between 2D and 3D feature maps. We collected two datasets VoxelCity and VoxelHome to train our framework with 36,416 images of 28 scenes with ground-truth 3D models, depth maps, and 6D object poses. We made the datasets publicly available4. We evaluate our framework on 3D shape datasets to show that it delivers robust 3D scene reconstruction results that compete with and surpass state-of-the-art in a scene reconstruction with multiple non-rigid objects."
  },
  "eccv2018_w6_3dposeestimationforfine-grainedobjectcategories": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "3D Pose Estimation for Fine-Grained Object Categories",
    "authors": [
      "Yaming Wang",
      "Xiao Tan",
      "Yi Yang",
      "Xiao Liu",
      "Errui Ding",
      "Feng Zhou",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Wang_3D_Pose_Estimation_for_Fine-Grained_Object_Categories_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Wang_3D_Pose_Estimation_for_Fine-Grained_Object_Categories_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Existing object pose estimation datasets are related to generic object types and there is so far no dataset for fine-grained object categories. In this work, we introduce a new large dataset to benchmark pose estimation for fine-grained objects, thanks to the availability of both 2D and 3D fine-grained data recently. Specifically, we augment two popular fine-grained recognition datasets (StanfordCars and CompCars) by finding a fine-grained 3D CAD model for each sub-category and manually annotating each object in images with 3D pose. We show that, with enough training data, a full perspective model with continuous parameters can be estimated using 2D appearance information alone. We achieve this via a framework based on Faster/Mask R-CNN. This goes beyond previous works on category-level pose estimation, which only estimate discrete/continuous viewpoint angles or recover rotation matrices often with the help of key points. Furthermore, with fine-grained 3D models available, we incorporate a dense 3D representation named as location field into the CNN-based pose estimation framework to further improve the performance. The new dataset is available at www.umiacs.umd.edu/ \u00e2\u0088\u00bcwym/3dpose.html"
  },
  "eccv2018_w6_seamlesscolormappingfor3dreconstructionwithconsumer-gradescanningdevices": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "Seamless Color Mapping for 3D Reconstruction with Consumer-Grade Scanning Devices",
    "authors": [
      "Bin Wang",
      "Pan Pan",
      "Qinjie Xiao",
      "Likang Luo",
      "Xiaofeng Ren",
      "Rong Jin",
      "Xiaogang Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Wang_Seamless_Color_Mapping_for_3D_Reconstruction_with_Consumer-Grade_Scanning_Devices_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Wang_Seamless_Color_Mapping_for_3D_Reconstruction_with_Consumer-Grade_Scanning_Devices_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Virtual Reality provides an immersive and intuitive shopping experience for customers. This raises challenging problems of reconstructing real-life products realistically in a cheap way. We present a seamless texturing method for 3D reconstructed objects with inexpensive consumer-grade scanning devices. To this end, we develop a two-step global optimization method to seamlessly texture reconstructed models with color images. We first perform a seam generation optimization based on Markov random field to generate more reasonable seams located at low-frequency color areas. Then, we employ a seam correction optimization that uses local color information around seams to correct the misalignments of images used for texturing. In contrast to previous approaches, the proposed method is more computationally efficient in generating seamless texture maps. Experimental results show that our method can efficiently deliver a seamless and high-quality texture maps even for noisy data."
  },
  "eccv2018_w6_plane-basedhumanoidrobotnavigationandobjectmodelconstructionforgrasping": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "Plane-based Humanoid Robot Navigation and Object Model Construction for Grasping",
    "authors": [
      "Pavel Gritsenko",
      "Igor Gritsenko",
      "Askar Seidakhmet",
      "Bogdan Kwolek"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Gritsenko_Plane-based_Humanoid_Robot_Navigation_and_Object_Model_Construction_for_Grasping_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Gritsenko_Plane-based_Humanoid_Robot_Navigation_and_Object_Model_Construction_for_Grasping_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this work we present an approach to humanoid robot navigation and object model construction for grasping using only RGB-D data from an onboard depth sensor. A plane-based representation is used to provide a high-level model of the workspace, to estimate both the global robot pose and pose with respect to the object, and to determine the object pose as well as its dimensions. A visual feedback is used to achieve the desired robot pose for grasping. In the pre\u00e2\u0080\u0093grasping pose the robot determines the object pose as well as its dimensions. In such a local grasping approach, a simulator with our high-level scene representation and a virtual camera is used to fine-tune the motion controllers as well as to simulate and validate the process of grasping. We present experimental results that were obtained in simulations with virtual camera and robot as well as with real humanoid robot equipped with RGB-D camera, which performed object grasping in low-texture layouts."
  },
  "eccv2018_w6_category-level6dobjectposerecoveryindepthimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "Category-level 6D Object Pose Recovery in Depth Images",
    "authors": [
      "Caner Sahin",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Sahin_Category-level_6D_Object_Pose_Recovery_in_Depth_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Sahin_Category-level_6D_Object_Pose_Recovery_in_Depth_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Intra-class variations, distribution shifts among source and target domains are the major challenges of category-level tasks. In this study, we address category-level full 6D object pose estimation in the context of depth modality, introducing a novel part-based architecture that can tackle the above-mentioned challenges. Our architecture particularly adapts the distribution shifts arising from shape discrepancies, and naturally removes the variations of texture, illumination, pose, etc., so we call it as \"Intrinsic Structure Adaptor (ISA)\". We engineer ISA based on the followings: i) \"Semantically Selected Centers (SSC)\" are proposed in order to define the \"6D pose\" at the level of categories. ii) 3D skeleton structures, which we derive as shape-invariant features, are used to represent the parts extracted from the instances of given categories, and privileged one-class learning is employed based on these parts. iii) Graph matching is performed during training in such a way that the adaptation/generalization capability of the proposed architecture is improved across unseen instances. Experiments validate the promising performance of the proposed architecture using both synthetic and real datasets."
  },
  "eccv2018_w6_onpre-trainedimagefeaturesandsyntheticimagesfordeeplearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "On Pre-Trained Image Features and Synthetic Images for Deep Learning",
    "authors": [
      "Stefan Hinterstoisser",
      "Vincent Lepetit",
      "Paul Wohlhart",
      "Kurt Konolige"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Hinterstoisser_On_Pre-Trained_Image_Features_and_Synthetic_Images_for_Deep_Learning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Hinterstoisser_On_Pre-Trained_Image_Features_and_Synthetic_Images_for_Deep_Learning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " DeepLearningmethodsusuallyrequirehugeamountsoftrainingdata to perform at their full potential, and often require expensive manual labeling. Using synthetic images is therefore very attractive to train object detectors, as the labeling comes for free, and several approaches have been proposed to combine synthetic and real images for training. In this paper, we evaluate if \u00e2\u0080\u0099freezing\u00e2\u0080\u0099 the layers responsible for feature extraction to generic layers pre-trained on real images, and training only the remaining layers with plain OpenGL rendering may allow for training with synthetic images only. Our experiments with very recent deep architectures for object recognition (Faster-RCNN, R-FCN, Mask-RCNN) and image feature extractors (InceptionResnet and Resnet) show this simple approach performs surprisingly well."
  },
  "eccv2018_w6_convolutionalnetworksforobjectcategoryand3dposeestimationfrom2dimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "Convolutional Networks for Object Category and 3D Pose Estimation from 2D Images",
    "authors": [
      "Siddharth Mahendran",
      "Haider Ali",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Mahendran_Convolutional_Networks_for_Object_Category_and_3D_Pose_Estimation_from_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Mahendran_Convolutional_Networks_for_Object_Category_and_3D_Pose_Estimation_from_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Current CNN-based algorithms for recovering the 3D pose of an object in an image assume knowledge about both the object category and its 2D localization in the image. In this paper, we relax one of these constraints and propose to solve the task of joint object category and 3D pose estimation from an image assuming known 2D localization. We design a new architecture for this task composed of a feature network that is shared between subtasks, an object categorization network built on top of the feature network, and a collection of category dependent pose regression networks. We also introduce suitable loss functions and a training method for the new architecture. Experiments on the challenging PASCAL3D+ dataset show state-of-the-art performance in the joint categorization and pose estimation task. Moreover, our performance on the joint task is comparable to the performance of state-of-the-art methods on the simpler 3D pose estimation with known object category task."
  },
  "eccv2018_w6_occlusionresistantobjectrotationregressionfrompointcloudsegments": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "Occlusion Resistant Object Rotation Regression from Point Cloud Segments",
    "authors": [
      "Ge Gao",
      "Mikko Lauri",
      "Jianwei Zhang",
      "Simone Frintrop"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Gao_Occlusion_Resistant_Object_Rotation_Regression_from_Point_Cloud_Segments_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Gao_Occlusion_Resistant_Object_Rotation_Regression_from_Point_Cloud_Segments_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from point cloud segments using a convolutional neural network. Experimental results show that our method achieves competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm."
  },
  "eccv2018_w6_cameratrackingforslamindeformablemaps": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "Camera Tracking for SLAM in Deformable Maps",
    "authors": [
      "Jose Lamarca",
      "J.M.M. Montiel"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/Lamarca_Camera_Tracking_for_SLAM_in_Deformable_Maps_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/Lamarca_Camera_Tracking_for_SLAM_in_Deformable_Maps_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The current SLAM algorithms cannot work without assuming rigidity. We propose the first real-time tracking thread for monocular VSLAM systems that manages deformable scenes. It is based on top of the Shape-from-Template (SfT) methods to code the scene deformation model. Our proposal is a sequential method that manages efficiently large templates, i.e. deformable maps estimating at the same time the camera pose and deformation. It also can be relocated in case of tracking loss. We have created a new dataset to evaluate our system. Our results show the robustness of the method in deformable environments while running in real time with errors under 3% in depth estimation."
  },
  "eccv2018_w6_rpnetanend-to-endnetworkforrelativecameraposeestimation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Recovering 6D Object Pose",
    "title": "RPNet: an End-to-End Network for Relative Camera Pose Estimation",
    "authors": [
      "Sovann En",
      "Alexis Lechervy",
      "Frederic Jurie"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w6/html/En_RPNet_an_End-to-End_Network_for_Relative_Camera_Pose_Estimation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11129/En_RPNet_an_End-to-End_Network_for_Relative_Camera_Pose_Estimation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper addresses the task of relative camera pose estimation from raw image pixels, by means of deep neural networks. The proposed RPNet network takes pairs of images as input and directly infers the relative poses, without the need of camera intrinsic/extrinsic. While state-of-the-art systems based on SIFT + RANSAC, are able to recover the translation vector only up to scale, RPNet is trained to produce the full translation vector, in an end-to-end way. Experimental results on the Cambridge Landmark data set show very promising results regarding the recovery of the full translation vector. They also show that RPNet produces more accurate and more stable results than traditional approaches, especially for hard images (repetitive textures, textureless images, etc.). To the best of our knowledge, RPNet is the first attempt to recover full translation vectors in relative pose estimation."
  },
  "eccv2018_w7_teachinguavstoraceend-to-endregressionofagilecontrolsinsimulation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "Teaching UAVs to Race: End-to-End Regression of Agile Controls in Simulation",
    "authors": [
      "Matthias Muller",
      "Vincent Casser",
      "Neil Smith",
      "Dominik L. Michels",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Muller_Teaching_UAVs_to_Race_End-to-End_Regression_of_Agile_Controls_in_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Muller_Teaching_UAVs_to_Race_End-to-End_Regression_of_Agile_Controls_in_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Automating the navigation of unmanned aerial vehicles (UAVs) indiverse scenarios has gained much attention in recent years. However, teaching UAVs to fly in challenging environments remains an unsolved problem, mainly due to the lack of training data. In this paper, we train a deep neural network to predict UAV controls from raw image data for the task of autonomous UAV racing in a photo-realistic simulation. Training is done through imitation learning with data augmentation to allow for the correction of navigation mistakes. Extensive experiments demonstrate that our trained network (when sufficient data augmentation is used) outperforms state-of-the-art methods and flies more consistently than many human pilots. Additionally, we show that our optimized network architecture can run in real-time on embedded hardware, allowing for efficient onboard processing critical for real-world deployment. From a broader perspective, our results underline the importance of extensive data augmentation techniques to improve robustness in end-to-end learning setups."
  },
  "eccv2018_w7_onboardhyperspectralimagecompressionusingcompressedsensinganddeeplearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "Onboard Hyperspectral Image Compression using Compressed Sensing and Deep Learning",
    "authors": [
      "Saurabh Kumar",
      "Subhasis Chaudhuri",
      "Biplab Banerjee",
      "Feroz Ali"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Kumar_Onboard_Hyperspectral_Image_Compression_using_Compressed_Sensing_and_Deep_Learning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Kumar_Onboard_Hyperspectral_Image_Compression_using_Compressed_Sensing_and_Deep_Learning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a real-time onboard compression scheme for hyperspectral datacube which consists of a very low complexity encoder and a deep learning based parallel decoder architecture for fast decompression. The encoder creates a set of coded snapshots from a given datacube using a measurement code matrix. The decoder decompresses the coded snapshots by using a sparse recovery algorithm. We solve this sparse recovery problem using a deep neural network for fast reconstruction. We present experimental results which demonstrate that our technique performs very well in terms of quality of reconstruction and in terms of computational requirements compared to other transform based techniques with some tradeoff in PSNR. The proposed technique also enables faster inference in compressed domain, suitable for on-board requirements."
  },
  "eccv2018_w7_safeuavlearningtoestimatedepthandsafelandingareasforuavsfromsyntheticdata": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "SafeUAV: Learning to estimate depth and safe landing areas for UAVs from synthetic data",
    "authors": [
      "Alina Marcu",
      "Dragos Costea",
      "Vlad Licaret",
      "Mihai Pirvu",
      "Emil Slusanschi",
      "Marius Leordeanu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Marcu_SafeUAV_Learning_to_estimate_depth_and_safe_landing_areas_for_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Marcu_SafeUAV_Learning_to_estimate_depth_and_safe_landing_areas_for_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The emergence of relatively low cost UAVs has prompted a global concern about the safe operation of such devices. Since most of them can \u00e2\u0080\u0099autonomously\u00e2\u0080\u0099 fly by means of GPS way-points, the lack of a higher logic for emergency scenarios leads to an abundance of incidents involving property or personal injury. In order to tackle this problem, we propose a small, embeddable ConvNet for both depth and safe landing area estimation. Furthermore, since labeled training data in the 3D aerial field is scarce and ground images are unsuitable, we capture a novel synthetic aerial 3D dataset obtained from 3D reconstructions. We use the synthetic data to learn to estimate depth from in-flight images and segment them into \u00e2\u0080\u0099safe-landing\u00e2\u0080\u0099 and \u00e2\u0080\u0099obstacle\u00e2\u0080\u0099 regions. Our experiments demonstrate compelling results in practice on both synthetic data and real RGB drone footage."
  },
  "eccv2018_w7_aerialganerationtowardsrealisticdataaugmentationusingconditionalgans": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "Aerial GANeration: Towards Realistic Data Augmentation Using Conditional GANs",
    "authors": [
      "Stefan Milz",
      "Tobias Rudiger",
      "Sebastian Suss"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Milz_Aerial_GANeration_Towards_Realistic_Data_Augmentation_Using_Conditional_GANs_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Milz_Aerial_GANeration_Towards_Realistic_Data_Augmentation_Using_Conditional_GANs_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Environmental perception for autonomous aerial vehicles is a rising field. Recent years have shown a strong increase of performance in terms of accuracy and efficiency with the aid of convolutional neural networks. Thus, the community has established data sets for benchmarking several kinds of algorithms. However, public data is rare for multi-sensor approaches or either not large enough to train very accurate algorithms. For this reason, we propose a method to generate multi-sensor data sets using realistic data augmentation based on conditional generative adversarial networks (cGAN). cGANs have shown impressive results for image to image translation. We use this principle for sensor simulation. Hence, there is no need for expensive and complex 3D engines. Our method encodes ground truth data, e.g semantics or object boxes that could be drawn randomly, in the conditional image to generate realistic consistent sensor data. Our method is proven for aerial object detection and semantic segmentation on visual data, such as 3D Lidar reconstruction using the ISPRS and DOTA data set. We demonstrate qualitative accuracy improvements for state-of-the-art object detection (YOLO) using our augmentation technique."
  },
  "eccv2018_w7_metricsforreal-timemono-vslamevaluationincludingimuinduceddriftwithapplicationtouavflight": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "Metrics for Real-Time Mono-VSLAM Evaluation including IMU induced Drift with Application to UAV Flight",
    "authors": [
      "Alexander Hardt-Stremayr",
      "Matthias Schorghuber",
      "Stephan Weiss",
      "Martin Humenberger"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Hardt-Stremayr_Metrics_for_Real-Time_Mono-VSLAM_Evaluation_including_IMU_induced_Drift_with_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Hardt-Stremayr_Metrics_for_Real-Time_Mono-VSLAM_Evaluation_including_IMU_induced_Drift_with_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Vision based algorithms became popular for state estimation and subsequent (local) control of mobile robots. Currently a large variety of such algorithms exists and their performance is often characterized through their drift relative to the total trajectory traveled. However, this metric has relatively low relevance for local vehicle control/stabilization. In this paper, we propose a set of metrics which allows to evaluate a vision based algorithm with respect to its usability for state estimation and subsequent (local) control of highly dynamic autonomous mobile platforms such as multirotor UAVs. As such platforms usually make use of inertial measurements to mitigate the relatively low update rate of the visual algorithm, we particularly focus on a new metric taking the expected IMU-induced drift between visual readings into consideration based on the probabilistic properties of the sensor. We demonstrate this set of metrics by comparing ORB-SLAM, LSD-SLAM and DSO on different datasets."
  },
  "eccv2018_w7_shuffledetreal-timevehicledetectionnetworkinon-boardembeddeduavimagery": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "ShuffleDet: Real-Time Vehicle Detection Network in On-board Embedded UAV Imagery",
    "authors": [
      "Seyed Majid Azimi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Azimi_ShuffleDet_Real-Time_Vehicle_Detection_Network_in_On-board_Embedded_UAV_Imagery_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Azimi_ShuffleDet_Real-Time_Vehicle_Detection_Network_in_On-board_Embedded_UAV_Imagery_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " On-board real-time vehicle detection is of great significance for UAVs and other embedded mobile platforms. We propose a computationally inexpensive detection network for vehicle detection in UAV imagery which we call ShuffleDet. In order to enhance the speed-wise performance, we construct our method primarily using channel shuffling and grouped convolutions. We apply inception modules and deformable modules to consider the size and geometric shape of the vehicles. ShuffleDet is evaluated on CARPK and PUCPR+ datasets and compared against the state-of-the-art real-time object detection networks. ShuffleDet achieves 3.8 GFLOPs while it provides competitive performance on test sets of both datasets. We show that our algorithm achieves real-time performance by running at the speed of 14 frames per second on NVIDIA Jetson TX2 showing high potential for this method for real-time processing in UAVs."
  },
  "eccv2018_w7_jointexploitationoffeaturesandopticalflowforreal-timemovingobjectdetectionondrones": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "Joint Exploitation of Features and Optical Flow for Real-Time Moving Object Detection on Drones",
    "authors": [
      "Hazal Lezki",
      "I. Ahu Ozturk",
      "M. Akif Akpinar",
      "M. Kerim Yucel",
      "K. Berker Logoglu",
      "Aykut Erdem",
      "Erkut Erdem"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Lezki_Joint_Exploitation_of_Features_and_Optical_Flow_for_Real-Time_Moving_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Lezki_Joint_Exploitation_of_Features_and_Optical_Flow_for_Real-Time_Moving_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning."
  },
  "eccv2018_w7_uav-gestureadatasetforuavcontrolandgesturerecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition",
    "authors": [
      "Asanka G. Perera",
      "Yee Wei Law",
      "Javaan Chahl"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Perera_UAV-GESTURE_A_Dataset_for_UAV_Control_and_Gesture_Recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Perera_UAV-GESTURE_A_Dataset_for_UAV_Control_and_Gesture_Recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Current UAV-recorded datasets are mostly limited to action recognition and object tracking, whereas the gesture signals datasets were mostly recorded in indoor spaces. Currently, there is no outdoor recorded public video dataset for UAV commanding signals. Gesture signals can be effectively used with UAVs by leveraging the UAVs visual sensors and operational simplicity. To fill this gap and enable research in wider application areas, we present a UAV gesture signals dataset recorded in an outdoor setting. We selected 13 gestures suitable for basic UAV navigation and command from general aircraft handling and helicopter handling signals. We provide 119 high-definition video clips consisting of 37151 frames. The overall baseline gesture recognition performance computed using Pose-based Convolutional Neural Network (PCNN) is 91.9 %. All the frames are annotated with body joints and gesture classes in order to extend the dataset\u00e2\u0080\u0099s applicability to a wider research area including gesture recognition, action recognition, human pose recognition and situation awareness."
  },
  "eccv2018_w7_changenetadeeplearningarchitectureforvisualchangedetection": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Computer Vision for UAVs",
    "title": "ChangeNet: A Deep Learning Architecture for Visual Change Detection",
    "authors": [
      "Ashley Varghese",
      "Jayavardhana Gubbi",
      "Akshaya Ramaswamy",
      "P. Balamuralidhar"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w7/html/Varghese_ChangeNet_A_Deep_Learning_Architecture_for_Visual_Change_Detection_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Varghese_ChangeNet_A_Deep_Learning_Architecture_for_Visual_Change_Detection_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The increasing urban population in cities necessitates the need for the development of smart cities that can offer better services to its citizens. Drone technology plays a crucial role in the smart city environment and is already involved in a number of functions in smart cities such as traffic control and construction monitoring. A major challenge in fast growing cities is the encroachment of public spaces. A robotic solution using visual change detection can be used for such purposes. For the detection of encroachment, a drone can monitor outdoor urban areas over a period of time to infer the visual changes. Visual change detection is a higher level inference task that aims at accurately identifying variations between a reference image (historical) and a new test image depicting the current scenario. In case of images, the challenges are complex considering the variations caused by environmental conditions that are actually unchanged events. Human mind interprets the change by comparing the current status with historical data at intelligence level rather than using only visual information. In this paper, we present a deep architecture called ChangeNet for detecting changes between pairs of images and express the same semantically (label the change). A parallel deep convolutional neural network (CNN) architecture for localizing and identifying the changes between image pair has been proposed in this paper. The architecture is evaluated with VL-CMU-CD street view change detection, TSUNAMI and Google Street View (GSV) datasets that resemble drone captured images. The performance of the model for different lighting and seasonal conditions are experimented quantitatively and qualitatively. The result shows that ChangeNet outperforms the state of the art by achieving 98.3% pixel accuracy, 77.35% object based Intersection over Union (IoU) and 88.9% area under Receiver Operating Characteristics (RoC) curve."
  },
  "eccv2018_w8_deesildeep-shallowincrementallearning.": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 5th Transferring and Adapting Source Knowledge in Computer Vision and 2nd VisDA Challenge",
    "title": "DeeSIL: Deep-Shallow Incremental Learning.",
    "authors": [
      "Eden Belouadah",
      "Adrian Popescu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w8/html/Belouadah_DeeSIL_Deep-Shallow_Incremental_Learning._ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Belouadah_DeeSIL_Deep-Shallow_Incremental_Learning._ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Incremental Learning (IL) is an interesting AI problem when the algorithm is assumed to work on a budget. This is especially true when IL is modeled using a deep learning approach, where two complex challenges arise due to limited memory, which induces catastrophic forgetting and delays related to the retraining needed in order to incorporate new classes. Here we introduce DeeSIL, an adaptation of a known transfer learning scheme that combines a fixed deep representation used as feature extractor and learning independent shallow classifiers to increase recognition capacity. This scheme tackles the two aforementioned challenges since it works well with a limited memory budget and each new concept can be added within a minute. Moreover, since no deep retraining is needed when the model is incremented, DeeSIL can integrate larger amounts of initial data that provide more transferable features. Performance is evaluated on ImageNet LSVRC 2012 against three state of the art algorithms. Results show that, at scale, DeeSIL performance is 23 and 33 points higher than the best baseline when using the same and more initial data respectively."
  },
  "eccv2018_w8_dynamicadaptationonnon-stationaryvisualdomains": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 5th Transferring and Adapting Source Knowledge in Computer Vision and 2nd VisDA Challenge",
    "title": "Dynamic Adaptation on Non-Stationary Visual Domains",
    "authors": [
      "Sindi Shkodrani",
      "Michael Hofmann",
      "Efstratios Gavves"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w8/html/Shkodrani_Dynamic_Adaptation_on_Non-Stationary_Visual_Domains_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Shkodrani_Dynamic_Adaptation_on_Non-Stationary_Visual_Domains_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Domain adaptation aims to learn models on a supervised source domain that perform well on an unsupervised target. Prior work has examined domain adaptation in the context of stationary domain shifts, i.e. static data sets. However, with large-scale or dynamic data sources, data from a defined domain is not usually available all at once. For instance, in a streaming data scenario, dataset statistics effectively become a function of time. We introduce a framework for adaptation over non-stationary distribution shifts applicable to large-scale and streaming data scenarios. The model is adapted sequentially over incoming unsupervised streaming data batches. This enables improvements over several batches without the need for any additionally annotated data. To demonstrate the effectiveness of our proposed framework, we modify associative domain adaptation to work well on source and target data batches with unequal class distributions. We apply our method to several adaptation benchmark datasets for classification and show improved classifier accuracy not only for the currently adapted batch, but also when applied on future stream batches. Furthermore, we show the applicability of our associative learning modifications to semantic segmentation, where we achieve competitive results."
  },
  "eccv2018_w8_domainadaptivesemanticsegmentationthroughstructureenhancement": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 5th Transferring and Adapting Source Knowledge in Computer Vision and 2nd VisDA Challenge",
    "title": "Domain Adaptive Semantic Segmentation through Structure Enhancement",
    "authors": [
      "Fengmao Lv",
      "Qing Lian",
      "Guowu Yang",
      "Guosheng Lin",
      "Sinno Jialin Pan",
      "Lixin Duan"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w8/html/Lv_Domain_Adaptive_Semantic_Segmentation_through_Structure_Enhancement_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Lv_Domain_Adaptive_Semantic_Segmentation_through_Structure_Enhancement_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Although fully convolutional networks have recently achieved great advances in semantic segmentation, the performance leaps heavily rely on supervision with pixel-level annotations which are extremely expensive and time-consuming to collect. Training models on synthetic data is a feasible way to relieve the annotation burden. However, the domain shift between synthetic and real images usually lead to poor generalization performance. In this work, we propose an effective method to adapt the segmentation network trained on synthetic images to real scenarios in an unsupervised fashion. To improve the adaptation performance for semantic segmentation, we enhance the structure information of the target images at both the feature level and the output level. Specifically, we enforce the segmentation network to learn a representation that encodes the target images\u00e2\u0080\u0099 visual cues through image reconstruction, which is beneficial to the structured prediction of the target images. Further more, we implement adversarial training at the output space of the segmentation network to align the structured prediction of the source and target images based on the similar spatial structure they share. To validate the performance of our method, we conduct comprehensive experiments on the \"GTA5 to Cityscapes\" dataset which is a standard domain adaptation benchmark for semantic segmentation. The experimental results clearly demonstrate that our method can effectively bridge the synthetic and real image domains and obtain better adaptation performance compared with the existing state-of-the-art methods."
  },
  "eccv2018_w8_addingnewtaskstoasinglenetworkwithweighttransformationsusingbinarymasks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 5th Transferring and Adapting Source Knowledge in Computer Vision and 2nd VisDA Challenge",
    "title": "Adding New Tasks to a Single Network with Weight Transformations using Binary Masks",
    "authors": [
      "Massimiliano Mancini",
      "Elisa Ricci",
      "Barbara Caputo",
      "Samuel Rota Bulo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w8/html/Mancini_Adding_New_Tasks_to_a_Single_Network_with_Weight_Transformations_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Mancini_Adding_New_Tasks_to_a_Single_Network_with_Weight_Transformations_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Visual recognition algorithms are required today to exhibit adaptive abilities. Given a deep model trained on a specific, given task, it would be highly desirable to be able to adapt incrementally to new tasks, preserving scalability as the number of new tasks increases, while at the same time avoiding catastrophic forgetting issues. Recent work has shown that masking the internal weights of a given original conv-net through learned binary variables is a promising strategy. We build upon this intuition and take into account more elaborated affine transformations of the convolutional weights that include learned binary masks. We show that with our generalization it is possible to achieve significantly higher levels of adaptation to new tasks, enabling the approach to compete with fine tuning strategies by requiring slightly more than 1 bit per network parameter per additional task. Experiments on two popular benchmarks showcase the power of our approach, that achieves the new state of the art on the Visual Decathlon Challenge."
  },
  "eccv2018_w8_generatingsharedlatentvariablesforrobotstoimitatehumanmovementsandunderstandtheirphysicallimitations": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 5th Transferring and Adapting Source Knowledge in Computer Vision and 2nd VisDA Challenge",
    "title": "Generating Shared Latent Variables for Robots to Imitate Human Movements and  Understand their Physical Limitations",
    "authors": [
      "Maxime Devanne",
      "Sao Mai Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w8/html/Devanne_Generating_Shared_Latent_Variables_for_Robots_to_Imitate_Human_Movements_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Devanne_Generating_Shared_Latent_Variables_for_Robots_to_Imitate_Human_Movements_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Assistive robotics and particularly robot coaches may be very helpful for rehabilitation healthcare. In this context, we propose a method based on Gaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a physiotherapist, a robot coach and a patient. Our model is able to map visual human body features to robot data in order to facilitate the robot learning and imitation. In addition, we propose to extend the model to adapt the robots\u00e2\u0080\u0099 understanding to patients\u00e2\u0080\u0099 physical limitations during assessment of rehabilitation exercises. Experimental evaluation demonstrates promising results for both robot imitation and model adaptation according to patients\u00e2\u0080\u0099 limitations."
  },
  "eccv2018_w8_modelselectionforgeneralizedzero-shotlearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 5th Transferring and Adapting Source Knowledge in Computer Vision and 2nd VisDA Challenge",
    "title": "Model Selection for Generalized Zero-shot Learning",
    "authors": [
      "Hongguang Zhang",
      "Piotr Koniusz"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w8/html/Zhang_Model_Selection_for_Generalized_Zero-shot_Learning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Zhang_Model_Selection_for_Generalized_Zero-shot_Learning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In the problem of generalized zero-shot learning, the datapoints from unknown classes are not available during training. The main challenge for generalized zero-shot learning is the unbalanced data distribution which makes it hard for the classifier to distinguish if a given testing sample comes from a seen or unseen class. However, using Generative Adversarial Network (GAN) to generate auxiliary datapoints by the semantic embeddings of unseen classes alleviates the above problem. Current approaches combine the auxiliary datapoints and original training data to train the generalized zero-shot learning model and obtain state-of-the-art results. Inspired by such models, we propose to feed the generated data via a model selection mechanism. Specifically, we leverage two sources of datapoints (observed and auxiliary) to train some classifier to recognize which test datapoints come from seen and which from unseen classes. This way, generalized zero-shot learning can be divided into two disjoint classification tasks, thus reducing the negative influence of the unbalanced data distribution. Our evaluations on four publicly available datasets for generalized zero-shot learning show that our model obtains state-of-the-art results."
  },
  "eccv2018_w9_multi-domainposenetworkformulti-personposeestimationandtracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PoseTrack Challenge: Articulated People Tracking in the Wild",
    "title": "Multi-Domain Pose Network for Multi-Person Pose Estimation and Tracking",
    "authors": [
      "Hengkai Guo",
      "Tang Tang",
      "Guozhong Luo",
      "Riwei Chen",
      "Yongchen Lu",
      "Linfu Wen"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w9/html/Guo_Multi-Domain_Pose_Network_for_Multi-Person_Pose_Estimation_and_Tracking_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Guo_Multi-Domain_Pose_Network_for_Multi-Person_Pose_Estimation_and_Tracking_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Multi-person human pose estimation and tracking in the wild is important and challenging. For training a powerful model, large-scale training data are crucial. While there are several datasets for human pose estimation, the best practice for training on multi-dataset has not been investigated. In this paper, we present a simple network called MultiDomain Pose Network (MDPN) to address this problem. By treating the task as multi-domain learning, our methods can learn a better representation for pose prediction. Together with prediction heads fine-tuning and multi-branch combination, it shows significant improvement over baselines and achieves the best performance on PoseTrack ECCV 2018 Challenge without additional datasets other than MPII and COCO."
  },
  "eccv2018_w9_enhancedtwo-stagemulti-personposeestimation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PoseTrack Challenge: Articulated People Tracking in the Wild",
    "title": "Enhanced Two-stage Multi-Person Pose Estimation",
    "authors": [
      "Hiroto Honda",
      "Tomohiro Kato",
      "Yusuke Uchida"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w9/html/Honda_Enhanced_Two-stage_Multi-Person_Pose_Estimation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Honda_Enhanced_Two-stage_Multi-Person_Pose_Estimation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper we introduce an enhanced multi-person pose estimation method for the competition of the PoseTrack [6] workshop in ECCV 2018. We employ a two-stage human pose detector, where human region detection and keypoint detection are separately performed. A strong encoder-decoder network for keypoint detection has achieved 70.4% mAP for PoseTrack 2018 validation dataset."
  },
  "eccv2018_w9_multi-personposeestimationforposetrackingwithenhancedcascadedpyramidnetwork": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PoseTrack Challenge: Articulated People Tracking in the Wild",
    "title": "Multi-Person Pose Estimation for Pose Tracking with Enhanced Cascaded Pyramid Network",
    "authors": [
      "Dongdong Yu",
      "Kai Su",
      "Jia Sun",
      "Changhu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w9/html/Yu_Multi-Person_Pose_Estimation_for_Pose_Tracking_with_Enhanced_Cascaded_Pyramid_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Yu_Multi-Person_Pose_Estimation_for_Pose_Tracking_with_Enhanced_Cascaded_Pyramid_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Multi-person pose estimation is a fundamental yet challenging task in machine learning. In parallel, recent development of pose estimation has increased interests on pose tracking in recent years. In this work, we propose an efficient and powerful method to locate and track human pose. Our proposed method builds upon the state-of-theart single person pose estimation system (Cascaded Pyramid Network), and adopts the IOU-tracker module to identify the people in the wild. We conduct experiments on the released multi-person video pose estimation benchmark(PoseTrack2018) to validate the effectiveness of our network. Our model achieves an accuracy of 80.9% on the validation and 77.1% on the test set using the Mean Average Precision (MAP) metric, an accuracy of 64.0% on the validation and 57.4% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric."
  },
  "eccv2018_w9_atop-downapproachtoarticulatedhumanposeestimationandtracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PoseTrack Challenge: Articulated People Tracking in the Wild",
    "title": "A Top-down Approach to Articulated Human Pose Estimation and Tracking",
    "authors": [
      "Guanghan Ning",
      "Ping Liu",
      "Xiaochuan Fan",
      "Chi Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w9/html/Ning_A_Top-down_Approach_to_Articulated_Human_Pose_Estimation_and_Tracking_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Ning_A_Top-down_Approach_to_Articulated_Human_Pose_Estimation_and_Tracking_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Both the tasks of multi-person human pose estimation and pose tracking in videos are quite challenging. Existing methods can be categorized into two groups: top-down and bottom-up approaches. In this paper, following the top-down approach, we aim to build a strong baseline system with three modules: human candidate detector, singleperson pose estimator and human pose tracker. Firstly, we choose a generic object detector among state-of-the-art methods to detect human candidates. Then, cascaded pyramid network is used to estimate the corresponding human pose. Finally, we use a flow-based pose tracker to render keypoint-association across frames, i.e., assigning each human candidate a unique and temporally-consistent id, for the multi-target pose tracking purpose. We conduct extensive ablative experiments to validate various choices of models and configurations. We take part in two ECCV\u00e2\u0080\u009918 PoseTrack challenges1: pose estimation and pose tracking."
  },
  "eccv2018_w10_deepfusionnetworkforsplicingforgerylocalization": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Objectionable Content and Misinformation",
    "title": "Deep fusion network for splicing forgery localization",
    "authors": [
      "Bo Liu",
      "Chi-Man Pun"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w10/html/Liu_Deep_fusion_network_for_splicing_forgery_localization_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Liu_Deep_fusion_network_for_splicing_forgery_localization_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Digital splicing is a common type of image forgery: some regions of an image are replaced with contents from other images. To locate altered regions in a tampered picture is a challenging work because the difference is unknown between the altered regions and the original regions and it is thus necessary to search the large hypothesis space for a convincing result. In this paper, we proposed a novel deep fusion network to locate tampered area by tracing its border. A group of deep convolutional neural networks called Base-Net were firstly trained to response the certain type of splicing forgery respectively. Then, some layers of the Base-Net are selected and combined as a deep fusion neural network (Fusion-Net). After fine-tuning by a very small number of pictures, Fusion-Net is able to discern whether an image block is synthesized from different origins. Experiments on the benchmark datasets show that our method is effective in various situations and outperform state-of-the-art methods."
  },
  "eccv2018_w10_imagesplicinglocalizationviasemi-globalnetworkandfullyconnectedconditionalrandomfields": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Objectionable Content and Misinformation",
    "title": "Image Splicing Localization via Semi-Global Network and Fully Connected Conditional Random Fields",
    "authors": [
      "Xiaodong Cun",
      "Chi-Man Pun"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w10/html/Cun_Image_Splicing_Localization_via_Semi-Global_Network_and_Fully_Connected_Conditional_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Cun_Image_Splicing_Localization_via_Semi-Global_Network_and_Fully_Connected_Conditional_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We address the problem of image splicing localization: given an input image, localizing the spliced region which is cut from another image. We formulate this as a classification task but, critically, instead of classifying the spliced region by local patch, we leverage the features from whole image and local patch together to classify patch. We call this structure Semi-Global Network. Our approach exploits the observation that the spliced region should not only highly relate to local features (spliced edges), but also global features (semantic information, illumination, etc.) from the whole image. Furthermore, we first integrate Fully Connected Conditional Random Fields as post-processing technique in image splicing to improve the consistency between the input image and the output of the network. We show that our method outperforms other state-of-the-art methods in three popular datasets."
  },
  "eccv2018_w10_bridgingmachinelearningandcryptographyindefenceagainstadversarialattacks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Objectionable Content and Misinformation",
    "title": "Bridging machine learning and cryptography in defence against adversarial attacks",
    "authors": [
      "Olga Taran",
      "Shideh Rezaeifar",
      "Slava Voloshynovskiy"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w10/html/Taran_Bridging_machine_learning_and_cryptography_in_defence_against_adversarial_attacks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Taran_Bridging_machine_learning_and_cryptography_in_defence_against_adversarial_attacks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This questions the security of deep neural networks (DNN) for many securityand trust-sensitive domains. The majority of the proposed existing adversarial attacks are based on the differentiability of the DNN cost function. Defence strategies are mostly based on machine learning and signal processing principles that either try to detect-reject or filter out the adversarial perturbations and completely neglect the classical cryptographic component in the defence.In this work, we propose a new defence mechanism based on the second Kerckhoffs\u00e2\u0080\u0099s cryptographic principle which states that the defence and classification algorithm are supposed to be known, but not the key.To be compliant with the assumption that the attacker does not have access to the secret key, we will primarily focus on a gray-box scenario and do not address a white-box one. More particularly, we assume that the attacker does not have direct access to the secret block, but (a) he completely knows the system architecture, (b) he has access to the data used for training and testing and (c) he can observe the output of the classifier for each given input. We show empirically that our system is efficient against most famous state-of-the-art attacks in black-box and gray-box scenarios."
  },
  "eccv2018_w10_bidirectionalconvolutionallstmforthedetectionofviolenceinvideos": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Objectionable Content and Misinformation",
    "title": "Bidirectional Convolutional LSTM for the Detection of Violence in Videos",
    "authors": [
      "Alex Hanson",
      "Koutilya PNVR",
      "Sanjukta Krishnagopal",
      "Larry Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w10/html/Hanson_Bidirectional_Convolutional_LSTM_for_the_Detection_of_Violence_in_Videos_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Hanson_Bidirectional_Convolutional_LSTM_for_the_Detection_of_Violence_in_Videos_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The field of action recognition has gained tremendous traction in recent years. A subset of this, detection of violent activity in videos, is of great importance, particularly in unmanned surveillance or crowd footage videos. In this work, we explore this problem on three standard benchmarks widely used for violence detection: the Hockey Fights, Movies, and Violent Flows datasets. To this end, we introduce a Spatiotemporal Encoder, built on the Bidirectional Convolutional LSTM (BiConvLSTM) architecture. The addition of bidirectional temporal encodings and an elementwise max pooling of these encodings in the Spatiotemporal Encoder is novel in the field of violence detection. This addition is motivated by a desire to derive better video representations via leveraging long-range information in both temporal directions of the video. We find that the Spatiotemporal network is comparable in performance with existing methods for all of the above datasets. A simplified version of this network, the Spatial Encoder is sufficient to match state-of-the-art performance on the Hockey Fights and Movies datasets. However, on the Violent Flows dataset, the Spatiotemporal Encoder outperforms the Spatial Encoder."
  },
  "eccv2018_w10_areyoutamperingwithmydata?": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Objectionable Content and Misinformation",
    "title": "Are You Tampering With My Data?",
    "authors": [
      "Michele Alberti",
      "Vinaychandran Pondenkandath",
      "Marcel Wursch",
      "Manuel Bouillon",
      "Mathias Seuret",
      "Rolf Ingold",
      "Marcus Liwicki"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w10/html/Alberti_Are_You_Tampering_With_My_Data_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Alberti_Are_You_Tampering_With_My_Data_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a novel approach towards adversarial attacks on neural networks (NN), focusing on tampering the data used for training instead of generating attacks on trained models. Our network-agnostic method creates a backdoor during training which can be exploited at test time to force a neural network to exhibit abnormal behaviour. We demonstrate on two widely used datasets (CIFAR-10 and SVHN) that a universal modification of just one pixel per image for all the images of a class in the training set is enough to corrupt the training procedure of several state-of-the-art deep neural networks, causing the networks to misclassify any images to which the modification is applied. Our aim is to bring to the attention of the machine learning community, the possibility that even learning-based methods that are personally trained on public datasets can be subject to attacks by a skillful adversary."
  },
  "eccv2018_w10_adversarialexamplesdetectioninfeaturesdistancespaces": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Workshop on Objectionable Content and Misinformation",
    "title": "Adversarial examples detection in features distance spaces",
    "authors": [
      "Fabio Carrara",
      "Rudy Becarelli",
      "Roberto Caldelli",
      "Fabrizio Falchi",
      "Giuseppe Amato"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w10/html/Carrara_Adversarial_examples_detection_in_features_distance_spaces_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Carrara_Adversarial_examples_detection_in_features_distance_spaces_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Maliciously manipulated inputs for attacking machine learning methods \u00e2\u0080\u0093 in particular deep neural networks \u00e2\u0080\u0093 are emerging as a relevant issue for the security of recent artificial intelligence technologies, especially in computer vision. In this paper, we focus on attacks targeting image classifiers implemented with deep neural networks, and we propose a method for detecting adversarial images which focuses on the trajectory of internal representations (i.e. hidden layers neurons activation, also known as deep features) from the very first, up to the last. We argue that the representations of adversarial inputs follow a different evolution with respect to genuine inputs, and we define a distance-based embedding of features to efficiently encode this information. We train an LSTM network that analyzes the sequence of deep features embedded in a distance space to detect adversarial examples. The results of our preliminary experiments are encouraging: our detection scheme is able to detect adversarial inputs targeted to the ResNet-50 classifier pretrained on the ILSVRC\u00e2\u0080\u009912 dataset and generated by a variety of crafting algorithms."
  },
  "eccv2018_w11_giveeartomyfacemodellingmultimodalattentiontosocialinteractions": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Give ear to my face: modelling multimodal attention to  social interactions",
    "authors": [
      "Giuseppe Boccignone",
      "Vittorio Cuculo",
      "Alessandro D'Amelio",
      "Giuliano Grossi",
      "Raffaella Lanzarotti"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Boccignone_Give_ear_to_my_face_modelling_multimodal_attention_to__ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Boccignone_Give_ear_to_my_face_modelling_multimodal_attention_to__ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We address the deployment of perceptual attention to social interactions as displayed in conversational clips, when relying on multimodal information (audio and video). A probabilistic modelling framework is proposed that goes beyond the classic saliency paradigm while integrating multiple information cues. Attentional allocation is determined not just by stimulus-driven selection but, importantly, by social value as modulating the selection history of relevant multimodal items. Thus, the construction of attentional priority is the result of a sampling procedure conditioned on the potential value dynamics of socially relevant objects emerging moment to moment within the scene. Preliminary experiments on a publicly available dataset are presented."
  },
  "eccv2018_w11_investigatingdepthdomainadaptationforefficienthumanposeestimation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Investigating Depth Domain Adaptation for Efficient Human Pose Estimation",
    "authors": [
      "Angel Martinez-Gonzalez",
      "Michael Villamizar",
      "Olivier Canevet",
      "Jean-Marc Odobez"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Martinez-Gonzalez_Investigating_Depth_Domain_Adaptation_for_Efficient_Human_Pose_Estimation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Martinez-Gonzalez_Investigating_Depth_Domain_Adaptation_for_Efficient_Human_Pose_Estimation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " ConvolutionalNeuralNetworks(CNN)aretheleadingmodelsforhuman body landmark detection from RGB vision data. However, as such models require high computational load, an alternative is to rely on depth images which, due to their more simple nature, can allow the use of less complex CNNs and hence can lead to a faster detector. As learning CNNs from scratch requires large amounts of labeled data, which are not always available or expensive to obtain, we propose to rely on simulations and synthetic examples to build a large training dataset with precise labels. Nevertheless, the final performance on real data will suffer from the mismatch between the training and test data, also called domain shift between the source and target distributions. Thus in this paper, our main contribution is to investigate the use of unsupervised domain adaptation techniques to fill the gap in performance introduced by these distribution differences. The challenge lies in the important noise differences (not only gaussian noise, but many missing values around body limbs) between synthetic and real data, as well as the fact that we address a regression task rather than a classification one. In addition, we introduce a new public dataset of synthetically generated depth images to cover the cases of multi-person pose estimation. Our experiments show that domain adaptation provides some improvement, but that further network fine-tuning with real annotated data is worth including to supervise the adaptation process."
  },
  "eccv2018_w11_fillingthegapspredictingmissingjointsofhumanposesusingdenoisingautoencoders": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Filling the Gaps: Predicting Missing Joints of Human Poses Using Denoising Autoencoders",
    "authors": [
      "Nicolo Carissimi",
      "Paolo Rota",
      "Cigdem Beyan",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Carissimi_Filling_the_Gaps_Predicting_Missing_Joints_of_Human_Poses_Using_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Carissimi_Filling_the_Gaps_Predicting_Missing_Joints_of_Human_Poses_Using_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " State of the art pose estimators are able to deal with different challenges present in real-world scenarios, such as varying body appearance, lighting conditions and rare body poses. However, when body parts are severely occluded by objects or other people, the resulting poses might be incomplete, negatively affecting applications where estimating a full body pose is important (e.g. gesture and pose-based behavior analysis). In this work, we propose a method for predicting the missing joints from incomplete human poses. In our model we consider missing joints as noise in the input and we use an autoencoder-based solution to enhance the pose prediction. The method can be easily combined with existing pipelines and, by using only 2D coordinates as input data, the resulting model is small and fast to train, yet powerful enough to learn a robust representation of the low dimensional domain. Finally, results show improved predictions over existing pose estimation algorithms."
  },
  "eccv2018_w11_poseguidedhumanimagesynthesisbyviewdisentanglementandenhancedweightingloss": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Pose guided human image synthesis by view disentanglement and enhanced weighting loss",
    "authors": [
      "Mohamed Ilyes Lakhal",
      "Oswald Lanz",
      "Andrea Cavallaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Lakhal_Pose_guided_human_image_synthesis_by_view_disentanglement_and_enhanced_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Lakhal_Pose_guided_human_image_synthesis_by_view_disentanglement_and_enhanced_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " View synthesis aims at generating a novel, unseen view of an object. This is a challenging task in the presence of occlusions and asymmetries. In this paper, we present View-Disentangled Generator (VDG), a two-stage deep network for pose-guided human-image generation that performs coarse view prediction followed by a refinement stage. In the first stage, the network predicts the output from a target human pose, the source-image and the corresponding human pose, which are processed in different branches separately. This enables the network to learn a disentangled representation from the source and target view. In the second stage, the coarse output from the first stage is refined by adversarial training. Specifically, we introduce a masked version of the structural similarity loss that facilitates the network to focus on generating a higher quality view. Experiments on Market-1501 and DeepFashion demonstrate the effectiveness of the proposed generator."
  },
  "eccv2018_w11_asemi-superviseddataaugmentationapproachusing3dgraphicalengines": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "A Semi-Supervised Data Augmentation Approach using 3D Graphical Engines",
    "authors": [
      "Shuangjun Liu",
      "Sarah Ostadabbas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Liu_A_Semi-Supervised_Data_Augmentation_Approach_using_3D_Graphical_Engines_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Liu_A_Semi-Supervised_Data_Augmentation_Approach_using_3D_Graphical_Engines_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Deep learning approaches have been rapidly adopted across a wide range of fields because of their accuracy and flexibility, but require large labeled training datasets. This presents a fundamental problem for applications with limited, expensive, or private data (i.e. small data), such as human pose and behavior estimation/tracking which could be highly personalized. In this paper, we present a semi-supervised data augmentation approach that can synthesize large scale labeled training datasets using 3D graphical engines based on a physically-valid low dimensional pose descriptor. To evaluate the performance of our synthesized datasets in training deep learning-based models, we generated a large synthetic human pose dataset, called ScanAva using 3D scans of only 7 individuals based on our proposed augmentation approach. A state-of-the-art human pose estimation deep learning model then was trained from scratch using our ScanAva dataset and could achieve the pose estimation accuracy of 91.2% at PCK0.5 criteria after applying an efficient domain adaptation on the synthetic images, in which its pose estimation accuracy was comparable to the same model trained on large scale pose data from real humans such as MPII dataset and much higher than the model trained on other synthetic human dataset such as SURREAL."
  },
  "eccv2018_w11_towardslearningarealisticrenderingofhumanbehavior": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Towards Learning a Realistic Rendering of Human Behavior",
    "authors": [
      "Patrick Esser",
      "Johannes Haux",
      "Timo Milbich",
      "Bj orn Ommer"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Esser_Towards_Learning_a_Realistic_Rendering_of_Human_Behavior_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Esser_Towards_Learning_a_Realistic_Rendering_of_Human_Behavior_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Realistic rendering of human behavior is of great interest for applications such as video animations, virtual reality and gaming engines. Commonly animations of persons performing actions are rendered by articulating explicit 3D models based on sequences of coarse body shape representations simulating a certain behavior. While the simulation of natural behavior can be efficiently learned, the corresponding 3D models are typically designed in manual, laborious processes or reconstructed from costly (multi-)sensor data. In this work, we present an approach towards a holistic learning framework for rendering human behavior in which all components are learned from easily available data. To enable control over the generated behavior, we utilize motion capture data and generate realistic motions based on user inputs. Alternatively, we can directly copy behavior from videos and learn a rendering of characters using RGB camera data only. Our experiments show that we can further improve data efficiency by training on multiple characters at the same time. Overall our approach shows a new path towards easily available, personalized avatar creation."
  },
  "eccv2018_w11_humanactionrecognitionbasedontemporalposecnnandmulti-dimensionalfusion": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Human Action Recognition Based on Temporal Pose CNN  and Multi-Dimensional Fusion",
    "authors": [
      "Yi Huang",
      "Shang-Hong Lai",
      "Shao-Heng Tai"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Huang_Human_Action_Recognition_Based_on_Temporal_Pose_CNN__and_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Huang_Human_Action_Recognition_Based_on_Temporal_Pose_CNN__and_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " To take advantage of recent advances in human pose estimation from images, we develop a deep neural network model for action recognition from videos by computing temporal human pose features with a 3D CNN model. The proposed temporal pose features can provide more discriminative human action information than previous video features, such as appearance and short-term motion. In addition, we propose a novel fusion network that combines temporal pose, spatial and motion feature maps for the classification by bridging the gap between the dimension difference between 3D and 2D CNN feature maps. We show that the proposed action recognition system provides superior accuracy compared to the previous methods through experiments on Sub-JHMDB and PennAction datasets."
  },
  "eccv2018_w11_renderingrealisticsubject-dependentexpressionimagesbylearning3dmmdeformationcoefficients": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Rendering Realistic Subject-Dependent Expression Images by Learning 3DMM Deformation Coefficients",
    "authors": [
      "Claudio Ferrari",
      "Stefano Berretti",
      "Pietro Pala",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Ferrari_Rendering_Realistic_Subject-Dependent_Expression_Images_by_Learning_3DMM_Deformation_Coefficients_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Ferrari_Rendering_Realistic_Subject-Dependent_Expression_Images_by_Learning_3DMM_Deformation_Coefficients_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Automatic analysis of facial expressions is now attracting an increasing interest, thanks to the many potential applications it can enable. However, collecting images with labeled expression for large sets of images or videos is a quite complicated operation that, in most of the cases, requires substantial human intervention. In this paper, we propose a solution that, starting from a neutral image of a subject, is capable of producing a realistic expressive face image of the same subject. This is possible thanks to the use of a particular 3D morphable model (3DMM) that can effectively and efficiently fit to 2D images, and then deform itself under the action of deformation parameters learned expression-by-expression in a subject-independent manner. Ultimately, the application of such deformation parameters to the neutral model of a subject allows the rendering of realistic expressive images of the subject. Experiments demonstrate that such deformation parameters can be learned from a small set of training data using simple statistical tools; despite this simplicity, very realistic subject-dependent expression renderings can be obtained. Furthermore, robustness to cross dataset tests is also evidenced."
  },
  "eccv2018_w11_deepmultitaskgazeestimationwithaconstrainedlandmark-gazemodel": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Deep Multitask Gaze Estimation with a Constrained Landmark-Gaze Model",
    "authors": [
      "Yu Yu",
      "Gang Liu",
      "Jean-Marc Odobez"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Yu_Deep_Multitask_Gaze_Estimation_with_a_Constrained_Landmark-Gaze_Model_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Yu_Deep_Multitask_Gaze_Estimation_with_a_Constrained_Landmark-Gaze_Model_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones."
  },
  "eccv2018_w11_photorealisticfacialsynthesisinthedimensionalaffectspace": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Photorealistic Facial Synthesis in the Dimensional Affect Space",
    "authors": [
      "Dimitrios Kollias",
      "Shiyang Cheng",
      "Maja Pantic",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Kollias_Photorealistic_Facial_Synthesis_in_the_Dimensional_Affect_Space_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Kollias_Photorealistic_Facial_Synthesis_in_the_Dimensional_Affect_Space_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents a novel approach for synthesizing facial affect, which is based on our annotating 600,000 frames of the 4DFAB database in terms of valence and arousal. The input of this approach is a pair of these emotional state descriptors and a neutral 2D image of a person to whom the corresponding affect will be synthesized. Given this target pair, a set of 3D facial meshes is selected, which is used to build a blendshape model and generate the new facial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting is performed and the reconstructed face is deformed to generate the target facial expressions. Last, the new face is rendered into the original image. Both qualitative and quantitative experimental studies illustrate the generation of realistic images, when the neutral image is sampled from a variety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE, AFEW-VA, BU3DFE, Bosphorus."
  },
  "eccv2018_w11_generatingsyntheticvideosequencesbyexplicitlymodelingobjectmotion": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Generating Synthetic Video Sequences by Explicitly Modeling Object Motion",
    "authors": [
      "S. Palazzo",
      "C. Spampinato",
      "P. D'Oro",
      "D. Giordano",
      "M. Shah"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Palazzo_Generating_Synthetic_Video_Sequences_by_Explicitly_Modeling_Object_Motion_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Palazzo_Generating_Synthetic_Video_Sequences_by_Explicitly_Modeling_Object_Motion_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recent GAN-based video generation approaches model videos as the combination of a time-independent scene component and a time-varying motion component, thus factorizing the generation problem into generating background and foreground separately. One of the main limitations of current approaches is that both factors are learned by mapping one source latent space to videos, which complicates the generation task as a single data point must be informative of both background and foreground content. In this paper we propose a GAN framework for video generation that, instead, employs two latent spaces in order to structure the generative process in a more natural way: 1) a latent space to generate the static visual content of a scene (background), which remains the same for the whole video, and 2) a latent space where motion is encoded as a trajectory between sampled points and whose dynamics are modeled through an RNN encoder (jointly trained with the generator and the discriminator) and then mapped by the generator to visual objects\u00e2\u0080\u0099 motion. Performance evaluation showed that our approach is able to control effectively the generation process as well as to synthesize more realistic videos than state-of-the-art methods."
  },
  "eccv2018_w11_asemi-superviseddeepgenerativemodelforhumanbodyanalysis": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "A Semi-supervised Deep Generative Model for Human Body Analysis",
    "authors": [
      "Rodrigo de Bem",
      "Arnab Ghosh",
      "Thalaiyasingam Ajanthan",
      "Ondrej Miksik",
      "N. Siddharth",
      "Philip Torr"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/de_A_Semi-supervised_Deep_Generative_Modelfor_Human_Body_Analysis_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/de_A_Semi-supervised_Deep_Generative_Modelfor_Human_Body_Analysis_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Deep generative modelling for human body analysis is an emerging problem with many interesting applications. However, the latent space learned by such models is typically not interpretable, resulting in less flexible models. In this work, we adopt a structured semisupervised approach and present a deep generative model for human body analysis where the body pose and the visual appearance are disentangled in the latent space. Such a disentanglement allows independent manipulation of pose and appearance, and hence enables applications such as pose-transfer without being explicitly trained for such a task. In addition, our setting allows for semi-supervised pose estimation, relaxing the need for labelled data. We demonstrate the capabilities of our generative model on the Human3.6M and on the DeepFashion datasets."
  },
  "eccv2018_w11_roleofgrouplevelaffecttofindthemostinfluentialpersoninimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Role of Group Level Affect to Find The Most Influential Person in Images",
    "authors": [
      "Shreya Ghosh",
      "Abhinav Dhall"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Ghosh_Role_of_Group_Level_Affect_to_Find_The_Most_Influential_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Ghosh_Role_of_Group_Level_Affect_to_Find_The_Most_Influential_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Groupaffectanalysisisanimportantcueforpredictingvariousgroup traits. Generally, the estimation of the group affect, emotional responses, eye gaze and position of people in images are the important cues to identify an important person from a group of people. The main focus of this paper is to explore the importance of group affect in finding the representative of a group. We call that person the \"Most Influential Person\" (for the first impression) or \"leader\" of a group. In order to identify the main visual cues for \"Most Influential Person\", we conducted a user survey. Based on the survey statistics, we annotate the \"influential persons\" in 1000 images of Group AFfect database (GAF 2.0) via LabelMe toolbox and propose the \"GAF-personage database\". In order to identify \"Most Influential Person\", we proposed a DNN based Multiple Instance Learning (Deep MIL) method which takes deep facial features as input. To leverage the deep facial features, we first predict the individual emotion probabilities via CapsNet and rank the detected faces on the basis of it. Then, we extract deep facial features of the top-3 faces via VGG-16 network. Our method performs better than maximum facial area and saliency-based importance methods and achieves the human-level perception of \"Most Influential Person\" at group-level."
  },
  "eccv2018_w11_residualstackedrnnsforactionrecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 9th International Workshop on Human Behavior Understanding",
    "title": "Residual Stacked RNNs for Action Recognition",
    "authors": [
      "Mohamed Ilyes Lakhal",
      "Albert Clapes",
      "Sergio Escalera",
      "Oswald Lanz",
      "Andrea Cavallaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w11/html/Lakhal_Residual_Stacked_RNNs_for_Action_Recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Lakhal_Residual_Stacked_RNNs_for_Action_Recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Action recognition pipelines that use Recurrent Neural Networks (RNN) are currently 5 \u00e2\u0088\u0092 10% less accurate than Convolutional Neural Networks (CNN). While most works that use RNNs employ a 2D CNN on each frame to extract descriptors for action recognition, we extract spatiotemporal features from a 3D CNN and then learn the temporal relationship of these descriptors through a stacked residual recurrent neural network (Res-RNN). We introduce for the first time residual learning to counter the degradation problem in multi-layer RNNs, which have been successful for temporal aggregation in two-stream action recognition pipelines. Finally, we use a late fusion strategy to combine RGB and optical flow data of the two-stream Res-RNN. Experimental results show that the proposed pipeline achieves competitive results on UCF-101 and state of-the-art results for RNN-like architectures on the challenging HMDB-51 dataset."
  },
  "eccv2018_w12_semanticallyselectiveaugmentationfordeepcompactpersonre-identification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Person in Context Workshop and Challenge",
    "title": "Semantically Selective Augmentation for Deep Compact Person Re-Identification",
    "authors": [
      "Victor Ponce-Lopez",
      "Tilo Burghardt",
      "Sion Hannunna",
      "Dima Damen",
      "Alessandro Masullo",
      "Majid Mirmehdi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w12/html/Ponce-Lopez_Semantically_Selective_Augmentation_for_Deep_Compact_Person_Re-Identification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Ponce-Lopez_Semantically_Selective_Augmentation_for_Deep_Compact_Person_Re-Identification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present a deep person re-identification approach that combines semantically selective, deep data augmentation with clusteringbased network compression to generate high performance, light and fast inference networks. In particular, we propose to augment limited training data via sampling from a deep convolutional generative adversarial network (DCGAN), whose discriminator is constrained by a semantic classifier to explicitly control the domain specificity of the generation process. Thereby, we encode information in the classifier network which can be utilized to steer adversarial synthesis, and which fuels our CondenseNet ID-network training. We provide a quantitative and qualitative analysis of the approach and its variants on a number of datasets, obtaining results that outperform the state-of-the-art on the LIMA dataset for long-term monitoring in indoor living spaces."
  },
  "eccv2018_w12_recognizingpeopleinblindspotsbasedonsurroundingbehavior": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Person in Context Workshop and Challenge",
    "title": "Recognizing people in blind spots based on surrounding behavior",
    "authors": [
      "Kensho Hara",
      "Hirokatsu Kataoka",
      "Masaki Inaba",
      "Kenichi Narioka",
      "Yutaka Satoh"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w12/html/Hara_Recognizing_people_in_blind_spots_based_on_surrounding_behavior_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Hara_Recognizing_people_in_blind_spots_based_on_surrounding_behavior_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recentadvancesincomputervisionhaveachievedremarkableperformance improvements. These technologies mainly focus on recognition of visible targets. However, there are many invisible targets in blind spots in real situations. Humans may be able to recognize such invisible targets based on contexts (e.g. visible human behavior and environments) around the targets, and used such recognition to predict situations in blind spots on a daily basis. As the first step towards recognizing targets in blind spots captured in videos, we propose a convolutional neural network that recognizes whether or not there is a person in a blind spot. Based on the experiments that used the volleyball dataset, which includes various interactions of players, with artificial occlusions, our proposed method achieved 90.3% accuracy in the recognition."
  },
  "eccv2018_w12_visualrelationshippredictionvialabelclusteringandincorporationofdepthinformation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Person in Context Workshop and Challenge",
    "title": "Visual Relationship Prediction via Label Clustering and Incorporation of Depth Information",
    "authors": [
      "Hsuan-Kung Yang",
      "An-Chieh Cheng",
      "Kuan-Wei Ho",
      "Tsu-Jui Fu",
      "Chun-Yi Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w12/html/Yang_Visual_Relationship_Prediction_via_Label_Clustering_and_Incorporation_of_Depth_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Yang_Visual_Relationship_Prediction_via_Label_Clustering_and_Incorporation_of_Depth_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we investigate the use of an unsupervised label clustering technique and demonstrate that it enables substantial improvements in visual relationship prediction accuracy on the Person in Context (PIC) dataset. We propose to group object labels with similar patterns of relationship distribution in the dataset into fewer categories. Label clustering not only mitigates both the large classification space and class imbalance issues, but also potentially increases data samples for each clustered category. We further propose to incorporate depth information as an additional feature into the instance segmentation model. The additional depth prediction path supplements the relationship prediction model in a way that bounding boxes or segmentation masks are unable to deliver. We have rigorously evaluated the proposed techniques and performed various ablation analysis to validate the benefits of them."
  },
  "eccv2018_w12_human-centricvisualrelationsegmentationusingmaskr-cnnandvtranse": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Person in Context Workshop and Challenge",
    "title": "Human-centric Visual Relation Segmentation Using Mask R-CNN and VTransE",
    "authors": [
      "Fan Yu",
      "Xin Tan",
      "Tongwei Ren",
      "Gangshan Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w12/html/Yu_Human-centric_Visual_Relation_Segmentation_Using_Mask_R-CNN_and_VTransE_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Yu_Human-centric_Visual_Relation_Segmentation_Using_Mask_R-CNN_and_VTransE_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we propose a novel human-centric visual relation segmentation method based on Mask R-CNN model and VTransE model. We first retain the Mask R-CNN model, and segment both human and object instances. Because Mask R-CNN may omit some human instances in instance segmentation, we further detect the omitted faces and extend them to localize the corresponding human instances. Finally, we retrain the last layer of VTransE model, and detect the visual relations between each pair of human instance and human/object instance. The experimental results show that our method obtains 0.4799, 0.4069, and 0.2681 on the criteria of R@100 with the m-IoU of 0.25, 0.50 and 0.75, respectively, which outperforms other methods in Person in Context Challenge."
  },
  "eccv2018_w12_learningspatiotemporal3dconvolutionwithvideoorderself-supervision": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Person in Context Workshop and Challenge",
    "title": "Learning Spatiotemporal 3D Convolution with Video Order Self-Supervision",
    "authors": [
      "Tomoyuki Suzuki",
      "Takahiro Itazuri",
      "Kensho Hara",
      "Hirokatsu Kataoka"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w12/html/Suzuki_Learning_Spatiotemporal_3D_Convolution_with_Video_Order_Self-Supervision_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Suzuki_Learning_Spatiotemporal_3D_Convolution_with_Video_Order_Self-Supervision_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The purpose of this work is to explore self-supervised learning (SSL) strategy to capture a better feature with spatiotemporal 3D convolution. Although one of the next frontier in video recognition must be spatiotemporal 3D CNN, the convergence of the 3D convolutions is really difficult because of their enormous parameters or missing temporal(motion) feature. One of the effective solutions is to collect a 105-order video database such as Kinetics/Moments in Time. However, this is not an efficient with burden of manual annotations. In the paper, we train 3D CNN on wrong video-sequence detection tasks in a self-supervised manner (without any manual annotation). The shuffling and verification of consecutive video-frame-order is effective for 3D CNN to capture temporal feature and get a good start point of parameters to be fine-tuned. In the experimental section, we verify that our pretrained 3D CNN on wrong clip detection improves the level of performance on UCF101 (+3.99% better than baseline, namely training 3D convolution from scratch)."
  },
  "eccv2018_w13_whatwasmonetseeingwhilepainting?translatingartworkstophoto-realisticimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "What was Monet seeing while painting? Translating artworks to photo-realistic images",
    "authors": [
      "Matteo Tomei",
      "Lorenzo Baraldi",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Tomei_What_was_Monet_seeing_while_painting_Translating_artworks_to_photo-realistic_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Tomei_What_was_Monet_seeing_while_painting_Translating_artworks_to_photo-realistic_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " State of the art Computer Vision techniques exploit the availability of large-scale datasets, most of which consist of images captured from the world as it is. This brings to an incompatibility between such methods and digital data from the artistic domain, on which current techniques under-perform. A possible solution is to reduce the domain shift at the pixel level, thus translating artistic images to realistic copies. In this paper, we present a model capable of translating paintings to photo-realistic images, trained without paired examples. The idea is to enforce a patch level similarity between real and generated images, aiming to reproduce photo-realistic details from a memory bank of real images. This is subsequently adopted in the context of an unpaired image-to-image translation framework, mapping each image from one distribution to a new one belonging to the other distribution. Qualitative and quantitative results are presented on Monet, Cezanne and Van Gogh paintings translation tasks, showing that our approach increases the realism of generated images with respect to the CycleGAN approach."
  },
  "eccv2018_w13_saliency-drivenvariationalretargetingforhistoricalmaps": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "Saliency-driven Variational Retargeting for Historical Maps",
    "authors": [
      "Filippo Bergamasco",
      "Arianna Traviglia",
      "Andrea Torsello"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Bergamasco_Saliency-driven_Variational_Retargeting_for_Historical_Maps_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Bergamasco_Saliency-driven_Variational_Retargeting_for_Historical_Maps_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We study the problem of georeferencing artistic historical maps. Since they were primarily conceived as work of art more than an accurate cartographic tool, the common warping approaches implemented in Geographic Application Systems (GIS) usually lead to an overly-stretched image in which the actual pictorial content (like written text, compass roses, buildings, etc.) is un-naturally deformed. On the other hand, domain transformation of images driven by the perceived salient visual content is a well-known topic known as \"image retargeting\" which has been mostly limited to a change of scale of the image (ie. changing the width and height) rather than a more general controlpoints based warping.In this work we propose a variational image retargeting approach in which the local transformations are estimated to accommodate a set of control points instead of image boundaries. The direction and severity of warping is modulated by a novel tensor-based saliency formulation considering both the visual content and the shape of the underlying features to transform. The optimization includes a flow projection step based on the isotonic regression to avoid singularities and flip overs of the resulting distortion map."
  },
  "eccv2018_w13_deeptransferlearningforartclassificationproblems": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "Deep Transfer Learning for Art Classification Problems",
    "authors": [
      "Matthia Sabatelli",
      "Mike Kestemont",
      "Walter Daelemans",
      "Pierre Geurts"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Sabatelli_Deep_Transfer_Learning_for_Art_Classification_Problems_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Sabatelli_Deep_Transfer_Learning_for_Art_Classification_Problems_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper we investigate whether Deep Convolutional Neural Networks (DCNNs), which have obtained state of the art results on the ImageNet challenge, are able to perform equally well on three different art classification problems. In particular, we assess whether it is beneficial to fine tune the networks instead of just using them as off the shelf feature extractors for a separately trained softmax classifier. Our experiments show how the first approach yields significantly better results and allows the DCNNs to develop new selective attention mechanisms over the images, which provide powerful insights about which pixel regions allow the networks successfully tackle the proposed classification challenges. Furthermore, we also show how DCNNs, which have been fine tuned on a large artistic collection, outperform the same architectures which are pre-trained on the ImageNet dataset only, when it comes to the classification of heritage objects from a different dataset."
  },
  "eccv2018_w13_reflectingonhowartworksareprocessedandanalyzedbycomputervisionsupplementarymaterial": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "Reflecting on How Artworks Are Processed and Analyzed by Computer Vision: Supplementary Material",
    "authors": [
      "Sabine Lang",
      "Bjorn Ommer"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Lang_Reflecting_on_How_Artworks_Are_Processed_and_Analyzed_by_Computer_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Lang_Reflecting_on_How_Artworks_Are_Processed_and_Analyzed_by_Computer_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The intersection between computer vision and art history has resulted in new ways of seeing, engaging and analyzing digital images. Innovative methods and tools have assisted with the evaluation of large datasets, performing tasks such as classification, object detection, image description and style transfer or assisting with a form and content analysis. At this point, in order to progress, past works and established practices must be revisited and evaluated on the ground of their usability for art history. This paper provides a reflection from an art historical perspective to point to erroneous assumptions and where improvements are still needed."
  },
  "eccv2018_w13_seeingtheworldthroughmachiniceyesreflectionsoncomputervisioninthearts": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "Seeing the World Through Machinic Eyes: Reflections on Computer Vision in the Arts",
    "authors": [
      "Marijke Goeting"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Goeting_Seeing_the_World_Through_Machinic_Eyes_Reflections_on_Computer_Vision_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Goeting_Seeing_the_World_Through_Machinic_Eyes_Reflections_on_Computer_Vision_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Today, computer vision is broadly implemented and operates in the background of many systems. For users of these technologies, there is often no visual feedback, making it hard to understand the mechanisms that drive it. When computer vision is used to generate visual representations like Google Earth, it remains difficult to perceive the particular process and principles that went into its creation. This text examines computer vision as a medium and a system of representation by analyzing the work of design studio Onformative, designer Bernhard Hopfenga \u00cc\u0088rtner and artist Clement Valla. By using technical failures and employing computer vision in unforeseen ways, these artists and designers expose the differences between computer vision and human perception. Since computer vision is increasingly used to facilitate (visual) communication, artistic reflections like these help us understand the nature of computer vision and how it shapes our perception of the world."
  },
  "eccv2018_w13_adigitaltooltounderstandthepictorialproceduresof17thcenturyrealism": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "A Digital Tool to Understand the Pictorial Procedures of 17th century Realism",
    "authors": [
      "Francesca Di Cicco",
      "Lisa Wiersma",
      "Maarten Wijntjes",
      "Joris Dik",
      "Jeroen Stumpel",
      "Sylvia Pont"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Di_Cicco_A_Digital_Tool_to_Understand_the_Pictorial_Procedures_of_17textsuperscriptth_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Di_Cicco_A_Digital_Tool_to_Understand_the_Pictorial_Procedures_of_17textsuperscriptth_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " To unveil the mystery of the exquisitely rendered materials in Dutch 17th century paintings, we need to understand the pictorial procedures of this period. We focused on the Dutch master Jan de Heem, known for his highly convincing still-lifes. We reconstructed his systematic multi-layered approach to paint grapes, based on pigment distribution maps, layers stratigraphy, and a 17th century textual source. We digitised the layers reconstruction to access the temporal information of the painting procedure. We combined the layers via optical mixing into a digital tool that can be used to answer \"what if\" art historical questions about the painting composition, by editing the order, weight and colour of the layers."
  },
  "eccv2018_w13_howtoreadpaintingssemanticartunderstandingwithmulti-modalretrieval": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "How to Read Paintings: Semantic Art Understanding with Multi-Modal Retrieval",
    "authors": [
      "Noa Garcia",
      "George Vogiatzis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Garcia_How_to_Read_Paintings_Semantic_Art_Understanding_with_Multi-Modal_Retrieval_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Garcia_How_to_Read_Paintings_Semantic_Art_Understanding_with_Multi-Modal_Retrieval_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Automatic art analysis has been mostly focused on classifying artworks into different artistic styles. However, understanding an artistic representation involves more complex processes, such as identifying the elements in the scene or recognizing author influences. We present SemArt, a multi-modal dataset for semantic art understanding. SemArt is a collection of fine-art painting images in which each image is associated to a number of attributes and a textual artistic comment, such as those that appear in art catalogues or museum collections. To evaluate semantic art understanding, we envisage the Text2Art challenge, a multi-modal retrieval task where relevant paintings are retrieved according to an artistic text, and vice versa. We also propose several models for encoding visual and textual artistic representations into a common semantic space. Our best approach is able to find the correct image within the top 10 ranked images in the 45.5% of the test samples. Moreover, our models show remarkable levels of art understanding when compared against human evaluation."
  },
  "eccv2018_w13_weaklysupervisedobjectdetectioninartworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "Weakly Supervised Object Detection in Artworks",
    "authors": [
      "Nicolas Gonthier",
      "Yann Gousseau",
      "Said Ladjal",
      "Olivier Bonfait"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Gonthier_Weakly_Supervised_Object_Detection_in_Artworks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Gonthier_Weakly_Supervised_Object_Detection_in_Artworks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a method for the weakly supervised detection of objects in paintings. At training time, only image-level annotations are needed. This, combined with the efficiency of our multiple-instance learning method, enables one to learn new classes on-the-fly from globally annotated databases, avoiding the tedious task of manually marking objects. We show on several databases that dropping the instance-level annotations only yields mild performance losses. We also introduce a new database, IconArt, on which we perform detection experiments on classes that could not be learned on photographs, such as Jesus Child or Saint Sebastian. To the best of our knowledge, these are the first experiments dealing with the automatic (and in our case weakly supervised) detection of iconographic elements in paintings. We believe that such a method is of great benefit for helping art historians to explore large digital databases."
  },
  "eccv2018_w13_imagesofimagemachines.visualinterpretabilityincomputervisionforart": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th Workshop on Computer Vision for Art Analysis",
    "title": "Images of Image Machines. Visual Interpretability in Computer Vision for Art",
    "authors": [
      "Fabian Offert"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w13/html/Offert_Images_of_Image_Machines._Visual_Interpretability_in_Computer_Vision_for_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11130/Offert_Images_of_Image_Machines._Visual_Interpretability_in_Computer_Vision_for_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Despite the emergence of interpretable machine learning as a distinct area of research, the role and possible uses of interpretability in digital art history are still unclear. Focusing on feature visualization as the most common technical manifestation of visual interpretability, we argue that in computer vision for art visual interpretability is desirable, if not indispensable. We propose that feature visualization images can be a useful tool if they are used in a non-traditional way that embraces their peculiar representational status. Moreover, we suggest that exactly because of this peculiar representational status, feature visualization images themselves deserve more attention from the computer vision and digital art history communities."
  },
  "eccv2018_w14_deeplearningforautomatedtaggingoffashionimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "Deep Learning for Automated Tagging of Fashion Images",
    "authors": [
      "Patricia Gutierrez",
      "Pierre-Antoine Sondag",
      "Petar Butkovic",
      "Mauro Lacy",
      "Jordi Berges",
      "Felipe Bertrand",
      "Arne Knudson"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Gutierrez_Deep_Learning_for_Automated_Tagging_of_Fashion_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Gutierrez_Deep_Learning_for_Automated_Tagging_of_Fashion_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present 9 deep learning classifiers to predict Fashion attributes in 4 different categories: apparel (dresses and tops), shoes, watches and luggages. Our prediction system hosts several classifiers working at scale to populate a catalogue of millions of products. We provide details of our models as well as the challenges involved in predicting Fashion attributes in a relatively homogeneous problem space."
  },
  "eccv2018_w14_brand>logovisualanalysisoffashionbrands": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "Brand > Logo: Visual Analysis of Fashion Brands",
    "authors": [
      "M. Hadi Kiapour",
      "Robinson Piramuthu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Kiapour_Brand__Logo_Visual_Analysis_of_Fashion_Brands_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Kiapour_Brand__Logo_Visual_Analysis_of_Fashion_Brands_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " While lots of people may think branding begins and ends with a logo, fashion brands communicate their uniqueness through a wide range of visual cues such as color, patterns and shapes. In this work, we analyze learned visual representations by deep networks that are trained to recognize fashion brands. In particular, the activation strength and extent of neurons are studied to provide interesting insights about visual brand expressions. The proposed method identifies where a brand stands in the spectrum of branding strategy, i.e., from trademark-emblazoned goods with bold logos to implicit no logo marketing. By quantifying attention maps, we are able to interpret the visual characteristics of a brand present in a single image and model the general design direction of a brand as a whole. We further investigate versatility of neurons and discover \"specialists\" that are highly brand-specific and \"generalists\" that detect diverse visual features. A human experiment based on three main visual scenarios of fashion brands is conducted to verify the alignment of our quantitative measures with the human perception of brands. This paper demonstrate how deep networks go beyond logos in order to recognize clothing brands in an image."
  },
  "eccv2018_w14_tiereddeepsimilaritysearchforfashion": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "Tiered Deep Similarity Search for Fashion",
    "authors": [
      "Dipu Manandhar",
      "Muhammet Bastan",
      "Kim-Hui Yap"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Manandhar_Tiered_Deep_Similarity_Search_for_Fashion_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Manandhar_Tiered_Deep_Similarity_Search_for_Fashion_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " How similar are two fashion clothing? Fashion apparels demonstrate diverse visual concepts with their designs, styles and brands. Hence, there exist a hierarchy of similarities between fashion clothing, ranging from exact instance or brand to similar attributes, styles. An effective search method, thus, should be able to represent the tiers of similarities. In this paper, we present a deep learning based fashion search framework for learning the tiers of similarity. We propose a new attribute-guided metric learning (AGML) with multitask CNNs that jointly learns fashion attributes and image embeddings while taking category and brand information into account. The two tasks in the framework are linked witha guiding signal. The guiding signal, first, helps in mining informative training samples. Secondly, it helps in treating training samples by their importance to capture the tiers of similarity. We conduct experiments in a new BrandFashion dataset which is richly annotated at different granularities. Experimental results demonstrate that the proposed method is very effective in capturing a tiered similarity search space and outperforms the state-of-the-art fashion search methods."
  },
  "eccv2018_w14_deepfashionanalysiswithfeaturemapupsamplingandlandmark-drivenattention": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "Deep Fashion Analysis with Feature Map Upsampling and Landmark-driven Attention",
    "authors": [
      "Jingyuan Liu",
      "Hong Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Liu_Deep_Fashion_Analysis_with_Feature_Map_Upsampling_and_Landmark-driven_Attention_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Liu_Deep_Fashion_Analysis_with_Feature_Map_Upsampling_and_Landmark-driven_Attention_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we propose an attentive fashion network to address three problems of fashion analysis, namely landmark localization, category classification and attribute prediction. By utilizing a landmark prediction branch with upsampling network structure, we boost the accuracy of fashion landmark localization. With the aid of the predicted landmarks, a landmark-driven attention mechanism is proposed to help improve the precision of fashion category classification and attribute prediction. Experimental results show that our approach outperforms the state-of-the-arts on the DeepFashion dataset."
  },
  "eccv2018_w14_designdesigninspirationfromgenerativenetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "DesIGN: Design Inspiration from Generative Networks",
    "authors": [
      "Othman Sbai",
      "Mohamed Elhoseiny",
      "Antoine Bordes",
      "Yann LeCun",
      "Camille Couprie"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Sbai_DesIGN_Design_Inspiration_from_Generative_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Sbai_DesIGN_Design_Inspiration_from_Generative_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost novelty in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) a new loss function that encourages novelty, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies. We show that our proposed creativity loss yields better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability."
  },
  "eccv2018_w14_fashionsearchnetfashionsearchwithattributemanipulation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "FashionSearchNet: Fashion Search with Attribute Manipulation",
    "authors": [
      "Kenan E. Ak",
      "Ashraf A. Kassim",
      "Joo Hwee Lim",
      "Jo Yew Tham"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Ak_FashionSearchNet_Fashion_Search_with_Attribute_Manipulation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Ak_FashionSearchNet_Fashion_Search_with_Attribute_Manipulation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The focus of this paper is on retrieval of fashion images after manipulating attributes of the query images. This task is particularly useful in search scenarios where the user is interested in small variations of an image, i.e., replacing the mandarin collar with a buttondown. Keeping the desired attributes of the query image while manipulating its other attributes is a challenging problem which is accomplished by our proposed network called FashionSearchNet. FashionSearchNet is able to learn attribute specific representations by leveraging on weakly-supervised localization. The localization module is used to ignore the unrelated features of attributes in the feature map, thus improve the similarity learning. Experiments conducted on two recent fashion datasets show that FashionSearchNet outperforms the other state-of-the-art fashion search techniques."
  },
  "eccv2018_w14_craftcomplementaryrecommendationbyadversarialfeaturetransform": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "CRAFT: Complementary Recommendation by Adversarial Feature Transform",
    "authors": [
      "Cong Phuoc Huynh",
      "Arridhana Ciptadi",
      "Ambrish Tyagi",
      "Amit Agrawal"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Huynh_CRAFT_Complementary_Recommendation_by_Adversarial_Feature_Transform_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Huynh_CRAFT_Complementary_Recommendation_by_Adversarial_Feature_Transform_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a framework that harnesses visual cues in an unsupervised manner to learn the co-occurrence distribution of items in real-world images for complementary recommendation. Our model learns a non-linear transformation between the two manifolds of source and target item categories (e.g., tops and bottoms in outfits). Given a large dataset of images containing instances of co-occurring items, we train a generative transformer network directly on the feature representation by casting it as an adversarial optimization problem. Such a conditional generative model can produce multiple novel samples of complementary items (in the feature space) for a given query item. We demonstrate our framework for the task of recommending complementary top apparel for a given bottom clothing item. The recommendations made by our system are diverse, and are favored by human experts over the baseline approaches."
  },
  "eccv2018_w14_full-bodyhigh-resolutionanimegenerationwithprogressivestructure-conditionalgenerativeadversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "Full-body High-resolution Anime Generation with Progressive Structure-conditional Generative Adversarial Networks",
    "authors": [
      "Koichi Hamada",
      "Kentaro Tachibana",
      "Tianqi Li",
      "Hiroto Honda",
      "Yusuke Uchida"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Hamada_Full-body_High-resolution_Anime_Generation_with_Progressive_Structure-conditional_Generative_Adversarial_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Hamada_Full-body_High-resolution_Anime_Generation_with_Progressive_Structure-conditional_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " WeproposeProgressiveStructure-conditionalGenerativeAdversarial Networks (PSGAN), a new framework that can generate fullbody and high-resolution character images based on structural information. Recent progress in generative adversarial networks with progressive training has made it possible to generate high-resolution images. However, existing approaches have limitations in achieving both high image quality and structural consistency at the same time. Our method tackles the limitations by progressively increasing the resolution of both generated images and structural conditions during training. In this paper, we empirically demonstrate the effectiveness of this method by showing the comparison with existing approaches and video generation results of diverse anime characters at 1024\u00c3\u00971024 based on target pose sequences. We also create a novel dataset containing full-body 1024\u00c3\u00971024 highresolution images and exact 2D pose keypoints using Unity 3D Avatar models."
  },
  "eccv2018_w14_convolutionalphotomosaicgenerationviamulti-scaleperceptuallosses": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - First Workshop on Fashion, Art and Design",
    "title": "Convolutional Photomosaic Generation via Multi-Scale Perceptual Losses",
    "authors": [
      "Matthew Tesfaldet",
      "Nariman Saftarli",
      "Marcus A. Brubaker",
      "Konstantinos G. Derpanis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w14/html/Tesfaldet_Convolutional_Photomosaic_Generation_via_Multi-Scale_Perceptual_Losses_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Tesfaldet_Convolutional_Photomosaic_Generation_via_Multi-Scale_Perceptual_Losses_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Photographic mosaics (or simply photomosaics) are images comprised of smaller, equally-sized image tiles such that when viewed from a distance, the tiled images of the mosaic collectively resemble a perceptually plausible image. In this paper, we consider the challenge of automatically generating a photomosaic from an input image. Although computer-generated photomosaicking has existed for quite some time, none have considered simultaneously exploiting colour/grayscale intensity and the structure of the input across scales, as well as image semantics. We propose a convolutional network for generating photomosaics guided by a multi-scale perceptual loss to capture colour, structure, and semantics across multiple scales. We demonstrate the effectiveness of our multi-scale perceptual loss by experimenting with producing extremely high resolution photomosaics and through the inclusion of ablation experiments that compare with a single-scale variant of the perceptual loss. We show that, overall, our approach produces visually pleasing results, providing a substantial improvement over common baselines."
  },
  "eccv2018_w15_actionanticipationbypredictingfuturedynamicimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Action Anticipation By Predicting Future Dynamic Images",
    "authors": [
      "Cristian Rodriguez",
      "Basura Fernando",
      "Hongdong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Rodriguez_Action_Anticipation_By_Predicting_Future_Dynamic_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Rodriguez_Action_Anticipation_By_Predicting_Future_Dynamic_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Human action-anticipation methods predict what is the future action by observing only a few portion of an action in progress. This is critical for applications where computers have to react to human actions as early as possible such as autonomous driving, human-robotic interaction, assistive robotics among others. In this paper, we present a method for human action anticipation by predicting the most plausible future human motion. We represent human motion using Dynamic Images [1] and make use of tailored loss functions to encourage a generative model to produce accurate future motion prediction. Our method outperforms the currently best performing action-anticipation methods by 4% on JHMDB-21, 5.2% on UT-Interaction and 5.1% on UCF 101-24 benchmarks."
  },
  "eccv2018_w15_predictingactiontubes": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Predicting Action Tubes",
    "authors": [
      "Gurkirt Singh",
      "Suman Saha",
      "Fabio Cuzzolin"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Singh_Predicting_Action_Tubes_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Singh_Predicting_Action_Tubes_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this work, we present a method to predict an entire \u00e2\u0080\u0098action tube\u00e2\u0080\u0099 (a set of temporally linked bounding boxes) in a trimmed video just by observing a smaller subset of it. Predicting where an action is going to take place in the near future is essential to many computer vision based applications such as autonomous driving or surgical robotics. Importantly, it has to be done in realtime and in an online fashion. We propose a Tube Prediction network (TPnet) which jointly predicts the past, present and future bounding boxes along with their action classification scores. At test time TPnet is used in a (temporal) sliding window setting, and its predictions are put into a tube estimation framework to construct/predict the video long action tubes not only for the observed part of the video but also for the unobserved part. Additionally, the proposed action tube predictor helps in completing action tubes for unobserved segments of the video. We quantitatively demonstrate the latter ability, and the fact that TPnet improves state-of-the-art detection performance, on one of the standard action detection benchmarks J-HMDB-21 dataset."
  },
  "eccv2018_w15_forecastinghandsandobjectsinfutureframes": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Forecasting Hands and Objects in Future Frames",
    "authors": [
      "Chenyou Fan",
      "Jangwon Lee",
      "Michael S. Ryoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Fan_Forecasting_Hands_and_Objects_in_Future_Frames_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Fan_Forecasting_Hands_and_Objects_in_Future_Frames_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents an approach to forecast future presence and location of human hands and objects. Given an image frame, the goal is to predict what objects will appear in the future frame (e.g., 5 seconds later) and where they will be located at, even when they are not visible in the current frame. The key idea is that (1) an intermediate representation of a convolutional object recognition model abstracts scene information in its frame and that (2) we can predict (i.e., regress) such representations corresponding to the future frames based on that of the current frame. We present a new two-stream fully convolutional neural network (CNN) architecture designed for forecasting future objects given a video. The experiments confirm that our approach allows reliable estimation of future objects in videos, obtaining much higher accuracy compared to the stateof-the-art future object presence forecast method on public datasets."
  },
  "eccv2018_w15_redasimplebuteffectivebaselinepredictorforthetrajnetbenchmark": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "RED: A simple but effective Baseline Predictor for the TrajNet Benchmark",
    "authors": [
      "Stefan Becker",
      "Ronny Hug",
      "Wolfgang Hubner",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Becker_RED_A_simple_but_effective_Baseline_Predictor_for_the_TrajNet_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Becker_RED_A_simple_but_effective_Baseline_Predictor_for_the_TrajNet_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In recent years, there is a shift from modeling the tracking problem based on Bayesian formulation towards using deep neural networks. Towards this end, in this paper the effectiveness of various deep neural networks for predicting future pedestrian paths are evaluated. The analyzed deep networks solely rely, like in the traditional approaches, on observed tracklets without human-human interaction information. The evaluation is done on the publicly available TrajNet benchmark dataset [39], which builds up a repository of considerable and popular datasets for trajectory prediction. We show how a Recurrent-Encoder with a Dense layer stacked on top, referred to as RED-predictor, is able to achieve toprank at the TrajNet 2018 challenge compared to elaborated models. Further, we investigate failure cases and give explanations for observed phenomena, and give some recommendations for overcoming demonstrated shortcomings."
  },
  "eccv2018_w15_jointfuturesemanticandinstancesegmentationprediction": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Joint Future Semantic and Instance Segmentation Prediction",
    "authors": [
      "Camille Couprie",
      "Pauline Luc",
      "Jakob Verbeek"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Couprie_Joint_Future_Semantic_and_Instance_Segmentation_Prediction_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Couprie_Joint_Future_Semantic_and_Instance_Segmentation_Prediction_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The ability to predict what will happen next from observing the past is a key component of intelligence. Methods that forecast future frames were recently introduced towards better machine intelligence. However, predicting directly in the image color space seems an overly complex task, and predicting higher level representations using semantic or instance segmentation approaches were shown to be more accurate. In this work, we introduce a novel prediction approach that encodes instance and semantic segmentation information in a single representation based on distance maps. Our graph-based modeling of the instance segmentation prediction problem allows us to obtain temporal tracks of the objects as an optimal solution to a watershed algorithm. Our experimental results on the Cityscapes dataset present state-of-theart semantic segmentation predictions, and instance segmentation results outperforming a strong baseline based on optical flow."
  },
  "eccv2018_w15_contextgraphbasedvideoframepredictionusinglocallyguidedobjective": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Context Graph based Video Frame Prediction using Locally Guided Objective",
    "authors": [
      "Prateep Bhattacharjee",
      "Sukhendu Das"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Bhattacharjee_Context_Graph_based_Video_Frame_Prediction_using_Locally_Guided_Objective_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Bhattacharjee_Context_Graph_based_Video_Frame_Prediction_using_Locally_Guided_Objective_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper proposes a feature reconstruction based approach using pixel-graph and Generative Adversarial Networks (GAN) for solving the problem of synthesizing future frames from video scenes. Recent methods of frame synthesis often generate blurry outcomes in case of long-range prediction and scenes involving multiple objects moving at different velocities due to their holistic approach. Our proposed method introduces a novel pixel-graph based context aggregation layer (PixGraph) which efficiently captures long range dependencies. PixGraph incorporates a weighting scheme through which the internal features of each pixel (or a group of neighboring pixels) can be modeled independently of the others, thus handling the issue of separate objects moving in different directions and with very dissimilar speed. We also introduce a novel objective function, the Locally Guided Gram Loss (LGGL), which aides the GAN based model to maximize the similarity between the intermediate features of the ground-truth and the network output by constructing Gram matrices from locally extracted patches over several levels of the generator. Our proposed model is end-to-end trainable and exhibits superior performance compared to the state-of-the-art on four realworld benchmark video datasets."
  },
  "eccv2018_w15_convolutionalneuralnetworkfortrajectoryprediction": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Convolutional Neural Network for Trajectory Prediction",
    "authors": [
      "Nishant Nikhil",
      "Brendan Tran Morris"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Nikhil_Convolutional_Neural_Network_for_Trajectory_Prediction_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Nikhil_Convolutional_Neural_Network_for_Trajectory_Prediction_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Predicting trajectories of pedestrians is quintessential for autonomous robots which share the same environment with humans. In order to effectively and safely interact with humans, trajectory prediction needs to be both precise and computationally efficient. In this work, we propose a convolutional neural network (CNN) based human trajectory prediction approach. Unlike more recent LSTM-based moles which attend sequentially to each frame, our model supports increased parallelism and effective temporal representation. The proposed compact CNN model is faster than the current approaches yet still yields competitive results."
  },
  "eccv2018_w15_actionalignmentfromgazecuesinhuman-humanandhuman-robotinteraction": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Action Alignment from Gaze Cues in Human-Human and Human-Robot Interaction",
    "authors": [
      "Nuno Ferreira Duarte",
      "Mirko Rakovic",
      "Jorge Marques",
      "Jose Santos-Victor"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Duarte_Action_Alignment_from_Gaze_Cues_in_Human-Human_and_Human-Robot_Interaction_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Duarte_Action_Alignment_from_Gaze_Cues_in_Human-Human_and_Human-Robot_Interaction_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Cognitive neuroscience experiments show how people intensify the exchange of non-verbal cues when they work on a joint task towards a common goal. When individuals share their intentions, it creates a social interaction that drives the mutual alignment of their actions and behavior. To understand the intentions of others, we strongly rely on the gaze cues. According to the role each person plays in the interaction, the resulting alignment of the body and gaze movements will be different. This mechanism is key to understand and model the socially dyadic interactions.We focus on the alignment of the leader\u00e2\u0080\u0099s behavior during dyadic interactions. The recorded gaze movements of dyads are used to build a model of the leader\u00e2\u0080\u0099s gaze behavior. The use of the follower\u00e2\u0080\u0099s gaze behavior data is two-fold: (i) to determine whether the follower is involved in the interaction, and (ii) if the follower\u00e2\u0080\u0099s gaze behavior correlates to the type of the action under execution. This information is then used to plan the leader\u00e2\u0080\u0099s actions in order to sustain the leader/follower alignment in the social interaction.The model of the leader\u00e2\u0080\u0099s gaze behavior and the alignment of the intentions is evaluated in a human-robot interaction scenario, with the robot acting as a leader and the human as a follower. During the interaction, the robot (i) emits non-verbal cues consistent with the action performed; (ii) predicts the human actions, and (iii) aligns its motion according to the human behavior."
  },
  "eccv2018_w15_grouplstmgrouptrajectorypredictionincrowdedscenarios": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Anticipating Human Behavior",
    "title": "Group LSTM: Group Trajectory Prediction in Crowded Scenarios",
    "authors": [
      "Niccolo Bisagno",
      "Bo Zhang",
      "Nicola Conci"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w15/html/Bisagno_Group_LSTM_Group_Trajectory_Prediction_in_Crowded_Scenarios_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Bisagno_Group_LSTM_Group_Trajectory_Prediction_in_Crowded_Scenarios_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The analysis of crowded scenes is one of the most challenging scenarios in visual surveillance, and a variety of factors need to be taken into account, such as the structure of the environments, and the presence of mutual occlusions and obstacles. Traditional prediction methods (such as RNN, LSTM, VAE, etc.) focus on anticipating individual\u00e2\u0080\u0099s future path based on the precise motion history of a pedestrian. However, since tracking algorithms are generally not reliable in highly dense scenes, these methods are not easily applicable in real environments. Nevertheless, it is very common that people (friends, couples, family members, etc.) tend to exhibit coherent motion patterns. Motivated by this phenomenon, we propose a novel approach to predict future trajectories in crowded scenes, at the group level. First, by exploiting the motion coherency, we cluster trajectories that have similar motion trends. In this way, pedestrians within the same group can be well segmented. Then, an improved social-LSTM is adopted for future path prediction. We evaluate our approach on standard crowd benchmarks (the UCY dataset and the ETH dataset), demonstrating its efficacy and applicability."
  },
  "eccv2018_w16_scenecoordinateregressionwithangle-basedreprojectionlossforcamerarelocalization": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Scene Coordinate Regression with Angle-Based Reprojection Loss for Camera Relocalization",
    "authors": [
      "Xiaotian Li",
      "Juha Ylioinas",
      "Jakob Verbeek",
      "Juho Kannala"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Li_Scene_Coordinate_Regression_with_Angle-Based_Reprojection_Loss_for_Camera_Relocalization_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Li_Scene_Coordinate_Regression_with_Angle-Based_Reprojection_Loss_for_Camera_Relocalization_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Image-based camera relocalization is an important problem in computer vision and robotics. Recent works utilize convolutional neural networks (CNNs) to regress for pixels in a query image their corresponding 3D world coordinates in the scene. The final pose is then solved via a RANSAC-based optimization scheme using the predicted coordinates. Usually, the CNN is trained with ground truth scene coordinates, but it has also been shown that the network can discover 3D scene geometry automatically by minimizing single-view reprojection loss. However, due to the deficiencies of the reprojection loss, the network needs to be carefully initialized. In this paper, we present a new angle-based reprojection loss, which resolves the issues of the original reprojection loss. With this new loss function, the network can be trained without careful initialization, and the system achieves more accurate results. The new loss also enables us to utilize available multi-view constraints, which further improve performance."
  },
  "eccv2018_w16_deepnormalestimationforautomaticshadingofhand-drawncharacters": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Deep Normal Estimation for Automatic Shading of Hand-Drawn Characters",
    "authors": [
      "Matis Hudon",
      "Mairead Grogan",
      "Rafael Pages",
      "Aljosa Smolic"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Hudon_Deep_Normal_Estimation_for_Automatic_Shading_of_Hand-Drawn_Characters_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Hudon_Deep_Normal_Estimation_for_Automatic_Shading_of_Hand-Drawn_Characters_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present a new fully automatic pipeline for generating shading effects on hand-drawn characters. Our method takes as input a single digitized sketch of any resolution and outputs a dense normal map estimation suitable for rendering without requiring any human input. At the heart of our method lies a deep residual, encoder-decoder convolutional network. The input sketch is first sampled using several equally sized 3-channel windows, with each window capturing a local area of interest at 3 different scales. Each window is then passed through the previously trained network for normal estimation. Finally, network outputs are arranged together to form a full-size normal map of the input sketch. We also present an efficient and effective way to generate a rich set of training data. Resulting renders offer a rich quality without any effort from the 2D artist. We show both quantitative and qualitative results demonstrating the effectiveness and quality of our network and method."
  },
  "eccv2018_w16_3dsurfacereconstructionbypointillism": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "3D Surface Reconstruction by Pointillism",
    "authors": [
      "Olivia Wiles",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Wiles_3D_Surface_Reconstruction_by_Pointillism_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Wiles_3D_Surface_Reconstruction_by_Pointillism_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The objective of this work is to infer the 3D shape of an object from a single image. We use sculptures as our training and test bed, as these have great variety in shape and appearance.To achieve this we build on the success of multiple view geometry (MVG) which is able to accurately provide correspondences between images of 3D objects under varying viewpoint and illumination conditions, and make the following contributions: first, we introduce a new loss function that can harness image-to-image correspondences to provide a supervisory signal to train a deep network to infer a depth map. The network is trained end-to-end by differentiating through the camera. Second, we develop a processing pipeline to automatically generate a large scale multi-view set of correspondences for training the network. Finally, we demonstrate that we can indeed obtain a depth map of a novel object from a single image for a variety of sculptures with varying shape/texture, and that the network generalises at test time to new domains (e.g. synthetic images)."
  },
  "eccv2018_w16_detectingparallel-movingobjectsinthemonocularcaseemployingcnndepthmaps": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Detecting parallel-moving objects in the monocular case employing CNN depth maps",
    "authors": [
      "Nolang Fanani",
      "Matthias Ochs",
      "Rudolf Mester"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Fanani_Detecting_parallel-moving_objects_in_the_monocular_case_employing_CNN_depth_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Fanani_Detecting_parallel-moving_objects_in_the_monocular_case_employing_CNN_depth_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents a method for detecting independently moving objects (IMOs) from a monocular camera mounted on a moving car. We use an existing state of the art monocular sparse visual odome-try/SLAM framework, and specifically attack the notorious problem ofidentifying those IMOs which move parallel to the ego-car motion, thatis, in an \u00e2\u0080\u0099epipolar-conformant\u00e2\u0080\u0099 way. IMO candidate patches are obtained from an existing CNN-based car instance detector. While crossing IMOscan be identified as such by epipolar consistency checks, IMOs that moveparallel to the camera motion are much harder to detect as their epipo-lar conformity allows to misinterpret them as static objects in a wrongdistance. We employ a CNN to provide an appearance-based depth es-timate, and the ambiguity problem can be solved through depth veri-fication. The obtained motion labels (IMO/static) are then propagatedover time using the combination of motion cues and appearance-basedinformation of the IMO candidate patches. We evaluate the performanceof our method on the KITTI dataset."
  },
  "eccv2018_w16_objectposeestimationfrommonocularimageusingmulti-viewkeypointcorrespondence": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Object Pose Estimation from Monocular Image using Multi-View Keypoint Correspondence",
    "authors": [
      "Jogendra Nath Kundu",
      "Rahul M V",
      "Aditya Ganeshan",
      "R Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Kundu_Object_Pose_Estimation_from_Monocular_Image_using_Multi-View_Keypoint_Correspondence_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Kundu_Object_Pose_Estimation_from_Monocular_Image_using_Multi-View_Keypoint_Correspondence_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Understanding the geometry and pose of objects in 2D images is a fundamental necessity for a wide range of real world applications. Driven by deep neural networks, recent methods have brought significant improvements to object pose estimation. However, they suffer due to scarcity of keypoint/pose-annotated real images and hence can not exploit the object\u00e2\u0080\u0099s 3D structural information effectively. In this work, we propose a data-efficient method which utilizes the geometric regularity of intraclass objects for pose estimation. First, we learn pose-invariant local descriptors of object parts from simple 2D RGB images. These descriptors, along with keypoints obtained from renders of a fixed 3D template model are then used to generate keypoint correspondence maps for a given monocular real image. Finally, a pose estimation network predicts 3D pose of the object using these correspondence maps. This pipeline is further extended to a multi-view approach, which assimilates keypoint information from correspondence sets generated from multiple views of the 3D template model. Fusion of multi-view information significantly improves geometric comprehension of the system which in turn enhances the pose estimation performance. Furthermore, use of correspondence framework responsible for the learning of pose invariant keypoint descriptor also allows us to effectively alleviate the data-scarcity problem. This enables our method to achieve state-of-the-art performance on multiple real-image viewpoint estimation datasets, such as Pascal3D+ and ObjectNet3D. To encourage reproducible research, we have released the codes for our proposed approach ."
  },
  "eccv2018_w16_3dcontextnetk-dtreeguidedhierarchicallearningofpointcloudsusinglocalandglobalcontextualcues": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues",
    "authors": [
      "Wei Zeng",
      "Theo Gevers"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Zeng_3DContextNet_K-d_Tree_Guided_Hierarchical_Learning_of_Point_Clouds_Using_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Zeng_3DContextNet_K-d_Tree_Guided_Hierarchical_Learning_of_Point_Clouds_Using_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Classification and segmentation of 3D point clouds are important tasks in computer vision. Because of the irregular nature of point clouds, most of the existing methods convert point clouds into regular 3D voxel grids before they are used as input for ConvNets. Unfortunately, voxel representations are highly insensitive to the geometrical nature of 3D data. More recent methods encode point clouds to higher dimensional features to cover the global 3D space. However, these models are not able to sufficiently capture the local structures of point clouds.Therefore, in this paper, we propose a method that exploits both local and global contextual cues imposed by the k-d tree. The method is designed to learn representation vectors progressively along the tree structure. Experiments on challenging benchmarks show that the proposed model provides discriminative point set features. For the task of 3D scene semantic segmentation, our method significantly outperforms the state-of-the-art on the Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS)."
  },
  "eccv2018_w16_evaluationofcnn-basedsingle-imagedepthestimationmethods": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Evaluation of CNN-based Single-Image Depth Estimation Methods",
    "authors": [
      "Tobias Koch",
      "Lukas Liebel",
      "Friedrich Fraundorfer",
      "Marco Korner"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Koch_Evaluation_of_CNN-based_Single-Image_Depth_Estimation_Methods_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Koch_Evaluation_of_CNN-based_Single-Image_Depth_Estimation_Methods_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " While an increasing interest in deep models for single-image depth estimation (SIDE) can be observed, established schemes for their evaluation are still limited. We propose a set of novel quality criteria, allowing for a more detailed analysis by focusing on specific characteristics of depth maps. In particular, we address the preservation of edges and planar regions, depth consistency, and absolute distance accuracy. In order to employ these metrics to evaluate and compare state-of-the-art SIDE approaches, we provide a new high-quality RGB-D dataset. We used a digital single-lens reflex (DSLR) camera together with a laser scanner to acquire high-resolution images and highly accurate depth maps. Experimental results show the validity of our proposed evaluation protocol."
  },
  "eccv2018_w16_asimpleapproachtointrinsiccorrespondencelearningonunstructured3dmeshes": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "A Simple Approach to Intrinsic Correspondence Learning on Unstructured 3D Meshes",
    "authors": [
      "Isaak Lim",
      "Alexander Dielen",
      "Marcel Campen",
      "Leif Kobbelt"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Lim_A_Simple_Approach_to_Intrinsic_Correspondence_Learning_on_Unstructured_3D_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Lim_A_Simple_Approach_to_Intrinsic_Correspondence_Learning_on_Unstructured_3D_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The question of representation of 3D geometry is of vital importance when it comes to leveraging the recent advances in the field of machine learning for geometry processing tasks. For common unstructured surface meshes state-of-the-art methods rely on patch-based or mapping-based techniques that introduce resampling operations in order to encode neighborhood information in a structured and regular manner. We investigate whether such resampling can be avoided, and propose a simple and direct encoding approach. It does not only increase processing efficiency due to its simplicity \u00e2\u0080\u0093 its direct nature also avoids any loss in data fidelity. To evaluate the proposed method, we perform a number of experiments in the challenging domain of intrinsic, non-rigid shape correspondence estimation. In comparisons to current methods we observe that our approach is able to achieve highly competitive results."
  },
  "eccv2018_w16_learningstructure-from-motionfrommotion": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Learning structure-from-motion from motion",
    "authors": [
      "Clement Pinard",
      "Laure Chevalley",
      "Antoine Manzanera",
      "David Filliat"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Pinard_Learning_structure-from-motion_from_motion_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Pinard_Learning_structure-from-motion_from_motion_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This work is based on a questioning of the quality metrics used by deep neural networks performing depth prediction from a single image, and then of the usability of recently published works on unsupervised learning of depth from videos. These works are all predicting depth from a single image, thus it is only known up to an undetermined scale factor, which is not sufficient for practical use cases that need an absolute depth map, i.e. the determination of the scaling factor. To overcome these limitations, we propose to learn in the same unsupervised manner a depth map inference system from monocular videos that takes a pair of images as input. This algorithm actually learns structure-frommotion from motion, and not only structure from context appearance. The scale factor issue is explicitly treated, and the absolute depth map can be estimated from camera displacement magnitude, which can be easily measured from cheap external sensors. Our solution is also much more robust with respect to domain variation and adaptation via fine tuning, because it does not rely entirely on depth from context. Two use cases are considered, unstabilized moving camera videos, and stabilized ones. This choice is motivated by the UAV (for Unmanned Aerial Vehicle) use case that generally provides reliable orientation measurement. We provide a set of experiments showing that, used in real conditions where only speed can be known, our network outperforms competitors for most depth quality measures. Results are given on the well known KITTI dataset [5], which provides robust stabilization for our second use case, but also contains moving scenes which are very typical of the in-car road context. We then present results on a synthetic dataset that we believe to be more representative of typical UAV scenes. Lastly, we present two domain adaptation use cases showing superior robustness of our method compared to single view depth algorithms, which indicates that it is better suited for highly variable visual contexts."
  },
  "eccv2018_w16_learningspectraltransformnetworkon3dsurfacefornon-rigidshapeanalysis": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Learning Spectral Transform Network on 3D Surface for Non-rigid Shape Analysis",
    "authors": [
      "Ruixuan Yu",
      "Jian Sun",
      "Huibin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Yu_Learning_Spectral_Transform_Network_on_3D_Surface_for_Non-rigid_Shape_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Yu_Learning_Spectral_Transform_Network_on_3D_Surface_for_Non-rigid_Shape_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Designing a network on 3D surface for non-rigid shape analysis is a challenging task. In this work, we propose a novel spectral transform network on 3D surface to learn shape descriptors. The proposed network architecture consists of four stages: raw descriptor extraction, surface second-order pooling, mixture of power function-based spectral transform, and metric learning. The proposed network is simple and shallow. Quantitative experiments on challenging benchmarks show its effectiveness for non-rigid shape retrieval and classification, e.g., it achieved the highest accuracies on SHREC\u00e2\u0080\u009914, 15 datasets as well as the \"range\" subset of SHREC\u00e2\u0080\u009917 dataset."
  },
  "eccv2018_w16_knowwhatyourneighborsdo3dsemanticsegmentationofpointclouds": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds",
    "authors": [
      "Francis Engelmann",
      "Theodora Kontogianni",
      "Jonas Schult",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Engelmann_Know_What_Your_Neighbors_Do_3D_Semantic_Segmentation_of_Point_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Engelmann_Know_What_Your_Neighbors_Do_3D_Semantic_Segmentation_of_Point_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we present a deep learning architecture which addresses the problem of 3D semantic segmentation of unstructured point clouds. Compared to previous work, we introduce grouping techniques which define point neighborhoods in the initial world space and the learned feature space. Neighborhoods are important as they allow to compute local or global point features depending on the spatial extend of the neighborhood. Additionally, we incorporate dedicated loss functions to further structure the learned point feature space: the pairwise distance loss and the centroid loss. We show how to apply these mechanisms to the task of 3D semantic segmentation of point clouds and report state-of-the-art performance on indoor and outdoor datasets."
  },
  "eccv2018_w16_deeplearningformulti-patherrorremovalintofsensors": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Deep Learning for Multi-Path Error Removal in ToF Sensors",
    "authors": [
      "Gianluca Agresti",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Agresti_Deep_Learning_for_Multi-Path_Error_Removal_in_ToF_Sensors_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Agresti_Deep_Learning_for_Multi-Path_Error_Removal_in_ToF_Sensors_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The removal of Multi-Path Interference (MPI) is one of the major open challenges in depth estimation with Time-of-Flight (ToF) cameras. In this paper we propose a novel method for MPI removal and depth refinement exploiting an ad-hoc deep learning architecture working on data from a multi-frequency ToF camera. In order to estimate the MPI we use a Convolutional Neural Network (CNN) made of two subnetworks: a coarse network analyzing the global structure of the data at a lower resolution and a fine one exploiting the output of the coarse network in order to remove the MPI while preserving the small details. The critical issue of the lack of ToF data with ground truth is solved by training the CNN with synthetic information. Finally, the residual zeromean error is removed with an adaptive bilateral filter guided from a noise model for the camera. Experimental results prove the effectiveness of the proposed approach on both synthetic and real data."
  },
  "eccv2018_w16_posix-gangeneratingmultipleposesusingganforpose-invariantfacerecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "PosIX-GAN: Generating multiple poses using GAN for Pose-Invariant Face Recognition",
    "authors": [
      "Avishek Bhattacharjee",
      "Samik Banerjee",
      "Sukhendu Das"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Bhattacharjee_PosIX-GAN_Generating_multiple_poses_using_GAN_for_Pose-Invariant_Face_Recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Bhattacharjee_PosIX-GAN_Generating_multiple_poses_using_GAN_for_Pose-Invariant_Face_Recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Pose-Invariant Face Recognition (PIFR) has been a serious challenge in the general field of face recognition (FR). The performance of face recognition algorithms deteriorate due to various degradations such as pose, illuminaton, occlusions, blur, noise, aliasing, etc. In this paper, we deal with the problem of 3D pose variation of a face. for that we design and propose PosIX Generative Adversarial Network (PosIX-GAN) that has been trained to generate a set of nice (high quality) face images with 9 different pose variations, when provided with a face image in any arbitrary pose as input. The discriminator of the GAN has also been trained to perform the task of face recognition along with the job of discriminating between real and generated (fake) images. Results when evaluated using two benchmark datasets, reveal the superior performance of PosIX-GAN over state-of-the-art shallow as well as deep learning methods."
  },
  "eccv2018_w16_semi-supervisedsemanticmatching.": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Semi-Supervised Semantic Matching.",
    "authors": [
      "Zakaria Laskar",
      "Juho Kannala"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Laskar_Semi-Supervised_Semantic_Matching._ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Laskar_Semi-Supervised_Semantic_Matching._ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Convolutional neural networks (CNNs) have been successfully applied to solve the problem of correspondence estimation between semantically related images. Due to non-availability of large training datasets, existing methods resort to self-supervised or unsupervised training paradigm. In this paper we propose a semi-supervised learning framework that imposes cyclic consistency constraint on unlabeled image pairs. Together with the supervised loss the proposed model achieves state-ofthe-art on a benchmark semantic matching dataset."
  },
  "eccv2018_w16_multi-kerneldiffusioncnnsforgraph-basedlearningonpointclouds": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Multi-Kernel Diffusion CNNs for Graph-Based Learning on Point Clouds",
    "authors": [
      "Lasse Hansen",
      "Jasper Diesel",
      "Mattias P. Heinrich"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Hansen_Multi-Kernel_Diffusion_CNNs_for_Graph-Based_Learning_on_Point_Clouds_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Hansen_Multi-Kernel_Diffusion_CNNs_for_Graph-Based_Learning_on_Point_Clouds_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Graph convolutional networks are a new promising learning approach to deal with data on irregular domains. They are predestined to overcome certain limitations of conventional grid-based architectures and will enable efficient handling of point clouds or related graphical data representations, e.g. superpixel graphs. Learning feature extractors and classifiers on 3D point clouds is still an underdeveloped area and has potential restrictions to equal graph topologies. In this work, we derive a new architectural design that combines rotationally and topologically invariant graph diffusion operators and node-wise feature learning through 1 \u00c3\u0097 1 convolutions. By combining multiple isotropic diffusion operations based on the Laplace-Beltrami operator, we can learn an optimal linear combination of diffusion kernels for effective feature propagation across nodes on an irregular graph. We validated our approach for learning point descriptors as well as semantic classification on real 3D point clouds of human poses and demonstrate an improvement from 85% to 95% in Dice overlap with our multi-kernel approach."
  },
  "eccv2018_w16_attaininghuman-levelperformancewithatlaslocationautocontextforanatomicallandmarkdetectionin3dctdata": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Attaining human-level performance with atlas location autocontext for anatomical landmark detection in 3D CT data",
    "authors": [
      "Alison Q. O'Neil",
      "Antanas Kascenas",
      "Joseph Henry",
      "Daniel Wyeth",
      "Matthew Shepherd",
      "Erin Beveridge",
      "Lauren Clunie",
      "Carrie Sansom",
      "Evelina Seduikyte Keith Muir",
      "Ian Poole"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/ONeil_Attaining_human-level_performance_with_atlas_location_autocontext_for_anatomical_landmark_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/ONeil_Attaining_human-level_performance_with_atlas_location_autocontext_for_anatomical_landmark_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present an efficient neural network method for locating anatomical landmarks in 3D medical CT scans, using atlas location autocontext in order to learn long-range spatial context. Location predictions are made by regression to Gaussian heatmaps, one heatmap per landmark. This system allows patchwise application of a shallow network, thus enabling multiple volumetric heatmaps to be predicted concurrently without prohibitive GPU memory requirements. Further, the system allows inter-landmark spatial relationships to be exploited using a simple overdetermined affine mapping that is robust to detection failures and occlusion or partial views. Evaluation is performed for 22 landmarks defined on a range of structures in head CT scans. Models are trained and validated on 201 scans. Over the final test set of 20 scans which was independently annotated by 2 human annotators, the neural network reaches an accuracy which matches the annotator variability, with similar human and machine patterns of variability across landmark classes."
  },
  "eccv2018_w16_deepfundamentalmatrixestimationwithoutcorrespondences": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "Deep Fundamental Matrix Estimation without Correspondences",
    "authors": [
      "Omid Poursaeed",
      "Guandao Yang",
      "Aditya Prakash",
      "Qiuren Fang",
      "Hanqing Jiang",
      "Bharath Hariharan",
      "Serge Belongie"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Poursaeed_Deep_Fundamental_Matrix_Estimation_without_Correspondences_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Poursaeed_Deep_Fundamental_Matrix_Estimation_without_Correspondences_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Estimating fundamental matrices is a classic problem in computer vision. Traditional methods rely heavily on the correctness of estimated key-point correspondences, which can be noisy and unreliable. As a result, it is difficult for these methods to handle image pairs with large occlusion or significantly different camera poses. In this paper, we propose novel neural network architectures to estimate fundamental matrices in an end-to-end manner without relying on point correspondences. New modules and layers are introduced in order to preserve mathematical properties of the fundamental matrix as a homogeneous rank-2 matrix with seven degrees of freedom. We analyze performance of the proposed models using various metrics on the KITTI dataset, and show that they achieve competitive performance with traditional methods without the need for extracting correspondences."
  },
  "eccv2018_w16_highqualityfacialsurfaceandtexturesynthesisviagenerativeadversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3rd Workshop on Geometry Meets Deep Learning",
    "title": "High Quality Facial Surface and Texture Synthesis via Generative Adversarial Networks",
    "authors": [
      "Ron Slossberg",
      "Gil Shamai",
      "Ron Kimmel"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w16/html/Slossberg_High_Quality_Facial_Surface_and_Texture_Synthesis_via_Generative_Adversarial_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Slossberg_High_Quality_Facial_Surface_and_Texture_Synthesis_via_Generative_Adversarial_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In the past several decades, many attempts have been made to model synthetic realistic geometric data. The goal of such models is to generate plausible 3D geometries and textures. Perhaps the best known of its kind is the linear 3D morphable model (3DMM) for faces. Such models can be found at the core of many computer vision applications such as face reconstruction, recognition and authentication to name just a few.Generative adversarial networks (GANs) have shown great promise in imitating high dimensional data distributions. State of the art GANs are capable of performing tasks such as image to image translation as well as auditory and image signal synthesis, producing novel plausible samples from the data distribution at hand.Geometric data is generally more difficult to process due to the inherent lack of an intrinsic parametrization. By bringing geometric data into an aligned space, we are able to map the data onto a 2D plane using a universal parametrization. This alignment process allows for efficient processing of digitally scanned geometric data via image processing tools. Using this methodology, we propose a novel face synthesis model for generation of realistic facial textures together with their corresponding geometry. A GAN is employed in order to imitate the space of parametrized human textures, while corresponding facial geometries are generated by learning the best 3DMM coefficients for each texture. The generated textures are mapped back onto the corresponding geometries to obtain new generated high resolution 3D faces."
  },
  "eccv2018_w17_recentadvancesatthebrain-drivencomputervisionworkshop2018": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Recent Advances at the Brain-Driven Computer Vision Workshop 2018",
    "authors": [
      "Simone Palazzo",
      "Isaak Kavasidis",
      "Dimitris Kastaniotis",
      "Stavros Dimitriadis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Palazzo_Recent_Advances_at_the_Brain-Driven_Computer_Vision_Workshop_2018_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Palazzo_Recent_Advances_at_the_Brain-Driven_Computer_Vision_Workshop_2018_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The 1st edition of the Brain-Driven Computer Vision Workshop, held in Munich in conjunction with the European Conference on Computer Vision 2018, aimed at attracting, promoting and inspiring research on paradigms, methods and tools for computer vision driven or inspired by the human brain. While successful, in terms of the quality of received submissions and audience present at the event, the workshop emphasized some of the factors that currently limit research in this field. In this report, we discuss the success points of the workshop, the characteristics of the presented works, and our considerations on the state of current research and future directions of research in this topic."
  },
  "eccv2018_w17_capsulegangenerativeadversarialcapsulenetwork": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "CapsuleGAN: Generative Adversarial Capsule Network",
    "authors": [
      "Ayush Jaiswal",
      "Wael AbdAlmageed",
      "Yue Wu",
      "Premkumar Natarajan"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Jaiswal_CapsuleGAN_Generative_Adversarial_Capsule_Network_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Jaiswal_CapsuleGAN_Generative_Adversarial_Capsule_Network_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present Generative Adversarial Capsule Network (CapsuleGAN), a framework that uses capsule networks (CapsNets) instead of the standard convolutional neural networks (CNNs) as discriminators within the generative adversarial network (GAN) setting, while modeling image data. We provide guidelines for designing CapsNet discriminators and the updated GAN objective function, which incorporates the CapsNet margin loss, for training CapsuleGAN models. We show that CapsuleGAN outperforms convolutional-GAN at modeling image data distribution on MNIST and CIFAR-10 datasets, evaluated on the generative adversarial metric and at semi-supervised image classification."
  },
  "eccv2018_w17_navigationalaffordancecorticalresponsesexplainedbyscene-parsingmodel": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Navigational affordance cortical responses explained by scene-parsing model",
    "authors": [
      "Kshitij Dwivedi",
      "Gemma Roig"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Dwivedi_Navigational_affordance_cortical_responses_explained_by_scene-parsing_model_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Dwivedi_Navigational_affordance_cortical_responses_explained_by_scene-parsing_model_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Deep Neural Networks (DNNs) are the leading models for explaining the population responses of neurons in the visual cortex. Recent studies show that responses of some task-specific brain regions can also be explained by a DNN trained for classification. In this work, we propose that responses of task-specific brain regions are better explained by DNNs trained on a similar task. We first show that responses of scene selective visual areas like parahippocampal place area (PPA) and Occipital Place Area (OPA) are better explained by a DNN trained for scene classification than one trained for object classification. Next, we consider a particular case of OPA which has been shown to encode navigational affordances. We argue that a scene parsing task, which predicts the class of each pixel in the scene is more related to navigational affordances than scene classification. Our results show that the responses in OPA are better explained by the scene parsing model than the scene classification model."
  },
  "eccv2018_w17_acontext-awarecapsulenetworkformulti-labelclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "A Context-aware Capsule Network for Multi-label Classification",
    "authors": [
      "Sameera Ramasinghe",
      "C.D. Athuraliya",
      "Salman H. Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Ramasinghe_A_Context-aware_Capsule_Network_for_Multi-label_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Ramasinghe_A_Context-aware_Capsule_Network_for_Multi-label_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recently proposed Capsule Network is a brain inspired architecture that brings a new paradigm to deep learning by modelling input domain variations through vector based representations. Despite being a seminal contribution, CapsNet does not explicitly model structured relationships between the detected entities and among the capsule features for related inputs. Motivated by the working of cortical network in HVS, we seek to resolve CapsNet limitations by proposing several intuitive modifications to the CapsNet architecture. We introduce, (1) a novel routing weight initialization technique, (2) an improved CapsNet design that exploits semantic relationships between the primary capsule activations using a densely connected Conditional Random Field and (3) a Cholesky transformation based correlation module to learn a general priority scheme. Our proposed design allows CapsNet to scale better to more complex problems, such as the multi-label classification task, where semantically related categories co-exist with various interdependencies. We present theoretical bases for our extensions and demonstrate significant improvements on ADE20K scene dataset."
  },
  "eccv2018_w17_brain-inspiredrobustdelineationoperator": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Brain-inspired robust delineation operator",
    "authors": [
      "Nicola Strisciuglio",
      "George Azzopardi",
      "Nicolai Petkov"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Strisciuglio_Brain-inspired_robust_delineation_operator_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Strisciuglio_Brain-inspired_robust_delineation_operator_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper we present a novel filter, based on the existing COSFIRE filter, for the delineation of patterns of interest. It includes a mechanism of push-pull inhibition that improves robustness to noise in terms of spurious texture. Push-pull inhibition is a phenomenon that is observed in neurons in area V1 of the visual cortex, which suppresses the response of certain simple cells for stimuli of preferred orientation but of non-preferred contrast. This type of inhibition allows for sharper detection of the patterns of interest and improves the quality of delineation especially in images with spurious texture.We performed experiments on images from different applications, namely the detection of rose stems for automatic gardening, the delineation of cracks in pavements and road surfaces, and the segmentation of blood vessels in retinal images. Push-pull inhibition helped to improve results considerably in all applications."
  },
  "eccv2018_w17_understandingfakefaces": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Understanding Fake Faces",
    "authors": [
      "Ryota Natsume",
      "Kazuki Inoue",
      "Yoshihiro Fukuhara",
      "Shintaro Yamamoto",
      "Shigeo Morishima",
      "Hirokatsu Kataoka"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Natsume_Understanding_Fake_Faces_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Natsume_Understanding_Fake_Faces_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Face recognition research is one of the most active topics in computer vision (CV), and deep neural networks (DNN) are now filling the gap between human-level and computer-driven performance levels in face verification algorithms. However, although the performance gap appears to be narrowing in terms of accuracy-based expectations, a curious question has arisen; specifically, Face understanding of AI is really close to that of human? In the present study, in an effort to confirm the braindriven concept , we conduct image-based detection, classification, and generation using an in-house created fake face database. This database has two configurations: (i) false positive face detections produced using both the Viola Jones (VJ) method and convolutional neural networks (CNN), and (ii) simulacra that have fundamental characteristics that resemble faces but are completely artificial. The results show a level of suggestive knowledge that indicates the continuing existence of a gap between the capabilities of recent vision-based face recognition algorithms and human-level performance. On a positive note, however, we have obtained knowledge that will advance the progress of face-understanding models."
  },
  "eccv2018_w17_characterizationofvisualobjectrepresentationsinratprimaryvisualcortex": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Characterization of Visual Object Representations in Rat Primary Visual Cortex",
    "authors": [
      "Sebastiano Vascon",
      "Ylenia Parin",
      "Eis Annavini",
      "Mattia D'Andola",
      "Davide Zoccolan",
      "Marcello Pelillo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Vascon_Characterization_of_Visual_Object_Representations_in_Rat_Primary_Visual_Cortex_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Vascon_Characterization_of_Visual_Object_Representations_in_Rat_Primary_Visual_Cortex_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " For most animal species, quick and reliable identification of visual objects is critical for survival. This applies also to rodents, which, in recent years, have become increasingly popular models of visual functions. For this reason in this work we analyzed how various properties of visual objects are represented in rat primary visual cortex (V1). The analysis has been carried out through supervised (classification) and unsupervised (clustering) learning methods. We assessed quantitatively the discrimination capabilities of V1 neurons by demonstrating how photometric properties (luminosity and object position in the scene) can be derived directly from the neuronal responses."
  },
  "eccv2018_w17_learningeventrepresentationsbyencodingthetemporalcontext": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Learning event representations by encoding the temporal context",
    "authors": [
      "Catarina Dias",
      "Mariella Dimiccoli"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Dias_Learning_event_representations_by_encoding_the_temporal_context_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Dias_Learning_event_representations_by_encoding_the_temporal_context_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This work aims at learning image representations suitable for event segmentation, a largely unexplored problem in the computer vision literature. The proposed approach is a self-supervised neural network that captures patterns of temporal overlap by learning to predict the feature vector of neighbor frames, given the one of the current frame. The model is inspired to recent experimental findings in neuroscience, showing that stimuli associated with similar temporal contexts are grouped together in the representational space. Experiments performed on image sequences captured at regular intervals have shown that a representation able to encode the temporal context provides very promising results on the task of temporal segmentation."
  },
  "eccv2018_w17_decodinggenericvisualrepresentationsfromhumanbrainactivityusingmachinelearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Decoding Generic Visual Representations From Human Brain Activity using Machine Learning",
    "authors": [
      "Angeliki Papadimitriou",
      "Nikolaos Passalis",
      "Anastasios Tefas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Papadimitriou_Decoding_Generic_Visual_Representations_From_Human_Brain_Activity_using_Machine_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Papadimitriou_Decoding_Generic_Visual_Representations_From_Human_Brain_Activity_using_Machine_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Among the most impressive recent applications of neural decoding is the visual representation decoding, where the category of an object that a subject either sees or imagines is inferred by observing his/her brain activity. Even though there is an increasing interest in the aforementioned visual representation decoding task, there is no extensive study of the effect of using different machine learning models on the decoding accuracy. In this paper we provide an extensive evaluation of several machine learning models, along with different similarity metrics, for the aforementioned task, drawing many interesting conclusions. That way, this paper a) paves the way for developing more advanced and accurate methods and b) provides an extensive and easily reproducible baseline for the aforementioned decoding task."
  },
  "eccv2018_w17_emop3dabrainlikepyramidaldeepneuralnetworkforemotionrecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "EmoP3D: A Brain like Pyramidal Deep Neural Network for Emotion Recognition",
    "authors": [
      "Emanuel Di Nardo",
      "Alfredo Petrosino",
      "Ihsan Ullah"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Di_Nardo_EmoP3D_A_Brain_like_Pyramidal_Deep_Neural_Network_for_Emotion_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Di_Nardo_EmoP3D_A_Brain_like_Pyramidal_Deep_Neural_Network_for_Emotion_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The paper reports a new model based on the understanding and encompassing intelligence from brain i.e. biological pyramidal neurons, tailored for emotion recognition. Our objective is to introduce and utilize usage of non-Convolutional layers in models and show comparable or state-of-the-art performance for multi-class emotion recognition problem. We open-sourced the optimized code for researchers. Our model shows state-of-the-art performance on two emotion recognition datasets (eNTERFACE and Youtube) enhancing previous best result by 9.47% and 20.8%, respectively."
  },
  "eccv2018_w17_subitizingwithvariationalautoencoders": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Brain-Driven Computer Vision",
    "title": "Subitizing with Variational Autoencoders",
    "authors": [
      "Rijnder Wever",
      "Tom F.H. Runia"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w17/html/Wever_Subitizing_with_Variational_Autoencoders_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Wever_Subitizing_with_Variational_Autoencoders_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Numerosity, the number of objects in a set, is a basic property of a given visual scene. Many animals develop the perceptual ability to subitize: the near-instantaneous identification of the numerosity in small sets of visual items. In computer vision, it has been shown that numerosity emerges as a statistical property in neural networks during unsupervised learning from simple synthetic images. In this work, we focus on more complex natural images using unsupervised hierarchical neural networks. Specifically, we show that variational autoencoders are able to spontaneously perform subitizing after training without supervision on a large amount of images from the Salient Object Subitizing dataset. While our method is unable to outperform supervised convolutional networks for subitizing, we observe that the networks learn to encode numerosity as a basic visual property. Moreover, we find that the learned representations are likely invariant to object area; an observation in alignment with studies on biological neural networks in cognitive neuroscience."
  },
  "eccv2018_w18_thesecondworkshopon3dreconstructionmeetssemanticschallengeresultsdiscussion": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "The Second Workshop on 3D Reconstruction Meets Semantics: Challenge Results Discussion",
    "authors": [
      "Radim Tylecek",
      "Torsten Sattler",
      "Hoang-An Le",
      "Thomas Brox",
      "Marc Pollefeys",
      "Robert B. Fisher",
      "Theo Gevers"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Tylecek_The_Second_Workshop_on_3D_Reconstruction_Meets_Semantics_Challenge_Results_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Tylecek_The_Second_Workshop_on_3D_Reconstruction_Meets_Semantics_Challenge_Results_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper discusses a reconstruction challenge held as a part of the second 3D Reconstruction meets Semantics workshop (3DRMS). The challenge goals and datasets are introduced, including both synthetic and real data from outdoor scenes, here represented by gardens with a variety of bushes, trees, other plants and objects. Both qualitative and quantitative evaluation of the challenge participants\u00e2\u0080\u0099 submissions is given in categories of geometric and semantic accuracy. Finally, comparison of submitted results with baseline methods is given, showing a modest performance increase in some of the categories."
  },
  "eccv2018_w18_adeeperlookat3dshapeclassifiers": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "A Deeper Look at 3D Shape Classifiers",
    "authors": [
      "Jong-Chyi Su",
      "Matheus Gadelha",
      "Rui Wang",
      "Subhransu Maji"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Su_A_Deeper_Look_at_3D_Shape_Classifiers_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Su_A_Deeper_Look_at_3D_Shape_Classifiers_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We investigate the role of representations and architectures for classifying 3D shapes in terms of their computational e\u0000ciency, generalization, and robustness to adversarial transformations. By varying the number of training examples and employing cross-modal transfer learning we study the role of initialization of existing deep architectures for 3D shape classification. Our analysis shows that multiview methods continue to o\u00e2\u0086\u00b5er the best generalization even without pretraining on large labeled image datasets, and even when trained on simplified inputs such as binary silhouettes. Furthermore, the performance of voxel-based 3D convolutional networks and point-based architectures can be improved via cross-modal transfer from image representations. Finally, we analyze the robustness of 3D shape classifiers to adversarial transformations and present a novel approach for generating adversarial perturbations of a 3D shape for multiview classifiers using a di\u00e2\u0086\u00b5erentiable renderer. We find that point-based networks are more robust to point position perturbations while voxel-based and multiview networks are easily fooled with the addition of imperceptible noise to the input."
  },
  "eccv2018_w18_3d-psrnetpartsegmented3dpointcloudreconstructionfromasingleimage": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "3D-PSRNet: Part Segmented 3D Point Cloud Reconstruction From a Single Image",
    "authors": [
      "Priyanka Mandikal",
      "Navaneet K L",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Mandikal_3D-PSRNet_Part_Segmented_3D_Point_Cloud_Reconstruction_From_a_Single_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Mandikal_3D-PSRNet_Part_Segmented_3D_Point_Cloud_Reconstruction_From_a_Single_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a mechanism to reconstruct part annotated 3D point clouds of objects given just a single input image. We demonstrate that jointly training for both reconstruction and segmentation leads to improved performance in both the tasks, when compared to training for each task individually. The key idea is to propagate information from each task so as to aid the other during the training procedure. Towards this end, we introduce a location-aware segmentation loss in the training regime. We empirically show the effectiveness of the proposed loss in generating more faithful part reconstructions while also improving segmentation accuracy. We thoroughly evaluate the proposed approach on different object categories from the ShapeNet dataset to obtain improved results in reconstruction as well as segmentation. Codes are available at https://github.com/val-iisc/3d-psrnet."
  },
  "eccv2018_w18_exploitingmulti-layerfeaturesusingacnn-rnnapproachforrgb-dobjectrecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "Exploiting Multi-Layer Features Using a CNN-RNN Approach for RGB-D Object Recognition",
    "authors": [
      "Ali Caglayan",
      "Ahmet Burak Can"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Caglayan_Exploiting_Multi-Layer_Features_Using_a_CNN-RNN_Approach_for_RGB-D_Object_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Caglayan_Exploiting_Multi-Layer_Features_Using_a_CNN-RNN_Approach_for_RGB-D_Object_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper proposes an approach for RGB-D object recognition by integrating a CNN model with recursive neural networks. It first employs a pre-trained CNN model as the underlying feature extractor to get visual features at different layers for RGB and depth modalities. Then, a deep recursive model is applied to map these features into highlevel representations. Finally, multi-level information is fused to produce a strong global representation of the entire object image. In order to utilize the CNN model trained on large-scale RGB datasets for depth domain, depth images are converted to a representation similar to RGB images. Experimental results on the Washington RGB-D Object dataset show that the proposed approach outperforms previous approaches."
  },
  "eccv2018_w18_temporallyconsistentdepthestimationinvideoswithrecurrentarchitectures": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "Temporally Consistent Depth Estimation in Videos with Recurrent Architectures",
    "authors": [
      "Denis Tananaev",
      "Huizhong Zhou",
      "Benjamin Ummenhofer",
      "Thomas Brox"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Tananaev_Temporally_Consistent_Depth_Estimation_in_Videos_with_Recurrent_Architectures_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Tananaev_Temporally_Consistent_Depth_Estimation_in_Videos_with_Recurrent_Architectures_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Convolutional networks trained on large RGB-D datasets have enabled depth estimation from a single image. Many works on automotive applications rely on such approaches. However, all existing methods work on a frame-by-frame manner when applied to videos, which leads to inconsistent depth estimates over time. In this paper, we introduce for the first time an approach that yields temporally consistent depth estimates over multiple frames of a video. This is done by a dedicated architecture based on convolutional LSTM units and layer normalization. Our approach achieves superior performance on several error metrics when compared to independent frame processing. This also shows in an improved quality of the reconstructed multi-view point clouds."
  },
  "eccv2018_w18_end-to-end6-dofobjectposeestimationthroughdifferentiablerasterization": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "End-to-end 6-DoF Object Pose Estimation through Differentiable Rasterization",
    "authors": [
      "Andrea Palazzi",
      "Luca Bergamini",
      "Simone Calderara",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Palazzi_End-to-end_6-DoF_Object_Pose_Estimation_through_Differentiable_Rasterization_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Palazzi_End-to-end_6-DoF_Object_Pose_Estimation_through_Differentiable_Rasterization_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Here we introduce an approximated differentiable renderer to refine a 6-DoF pose prediction using only 2D alignment information. To this end, a two-branched convolutional encoder network is employed to jointly estimate the object class and its 6-DoF pose in the scene. We then propose a new formulation of an approximated differentiable renderer to re-project the 3D object on the image according to its predicted pose; in this way the alignment error between the observed and the re-projected object silhouette can be measured. Since the renderer is differentiable, it is possible to back-propagate through it to correct the estimated pose at test time in an online learning fashion. Eventually we show how to leverage the classification branch to profitably re-project a representative model of the predicted class (i.e. a medoid) instead. Each object in the scene is processed independently and novel viewpoints in which both objects arrangement and mutual pose are preserved can be rendered."
  },
  "eccv2018_w18_yolo3dend-to-endreal-time3dorientedobjectboundingboxdetectionfromlidarpointcloud": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud",
    "authors": [
      "Waleed Ali",
      "Sherif Abdelkarim",
      "Mahmoud Zidan",
      "Mohamed Zahran",
      "Ahmad El Sallab"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Ali_YOLO3D_End-to-end_real-time_3D_Oriented_Object_Bounding_Box_Detection_from_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Ali_YOLO3D_End-to-end_real-time_3D_Oriented_Object_Bounding_Box_Detection_from_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Object detection and classification in 3D is a key task in Automated Driving (AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction of the surrounding environment, while the task of 3D object bounding box detection in real time remains a strong algorithmic challenge. In this paper, we build on the success of the oneshot regression meta-architecture in the 2D perspective image space and extend it to generate oriented 3D object bounding boxes from LiDAR point cloud. Our main contribution is in extending the loss function of YOLO v2 to include the yaw angle, the 3D box center in Cartesian coordinates and the height of the box as a direct regression problem. This formulation enables real-time performance, which is essential for automated driving. Our results are showing promising figures on KITTI benchmark, achieving real-time performance (40 fps) on Titan X GPU."
  },
  "eccv2018_w18_increasingtherobustnessofcnn-basedhumanbodysegmentationinrangeimagesbymodelingsensor-specificartifacts": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 3D Reconstruction meets Semantics",
    "title": "Increasing the robustness of CNN-based human body segmentation in range images by modeling sensor-specific artifacts",
    "authors": [
      "Lama Seoud",
      "Jonathan Boisvert",
      "Marc-Antoine Drouin",
      "Michel Picard",
      "Guy Godin"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w18/html/Seoud_Increasing_the_robustness_of_CNN-based_human_body_segmentation_in_range_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11131/Seoud_Increasing_the_robustness_of_CNN-based_human_body_segmentation_in_range_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper addresses the problem of human body parts segmentation in range images acquired using a structured-light imaging system. We propose a solution based on a fully convolutional neural network trained on realistic synthetic data that were simulated in a way that closely emulates our structured-light imaging system with its inherent artifacts such as occlusions, noise and missing data. The results on synthetic test data demonstrate quantitatively the performance of our method in identifying 33 body parts, with negligible confusion between the front and back sides of the body and between the left and right limbs. Our experiments highlight the importance of sensor-specific data augmentation in the training set to improve the robustness of the segmentation. Most importantly, when applied to range data actually acquired by our system, the method was capable of accurately segmenting the different body parts with inter-frame consistency in real-time."
  },
  "eccv2018_w19_fastsemanticsegmentationonvideousingblockmotion-basedfeatureinterpolation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Third International Workshop on Video Segmentation",
    "title": "Fast Semantic Segmentation on Video Using Block Motion-Based Feature Interpolation",
    "authors": [
      "Samvit Jain",
      "Joseph E. Gonzalez"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w19/html/Jain_Fast_Semantic_Segmentation_on_Video_Using_Block_Motion-Based_Feature_Interpolation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Jain_Fast_Semantic_Segmentation_on_Video_Using_Block_Motion-Based_Feature_Interpolation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Convolutional networks optimized for accuracy on challenging, dense prediction tasks are often prohibitively slow to run on each frame in a video. The spatial similarity of nearby video frames, however, suggests opportunity to reuse computation. Existing work has explored basic feature reuse and feature warping based on optical flow, but has encountered limits to the speedup attainable with these techniques. In this paper, we present a new, two part approach to accelerating inference on video. First, we propose a fast feature propagation technique that utilizes the block motion vectors present in compressed video (e.g. H.264 codecs) to cheaply propagate features from frame to frame. Second, we develop a novel feature estimation scheme, termed feature interpolation, that fuses features propagated from enclosing keyframes to render accurate feature estimates, even at sparse keyframe frequencies. We evaluate our system on the Cityscapes and CamVid datasets, comparing to both a frame-by-frame baseline and related work. We find that we are able to substantially accelerate semantic segmentation on video, achieving twice the average inference speed as prior work at any target accuracy level."
  },
  "eccv2018_w19_videoobjectsegmentationwithreferringexpressions": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Third International Workshop on Video Segmentation",
    "title": "Video Object Segmentation with Referring Expressions",
    "authors": [
      "Anna Khoreva",
      "Anna Rohrbach",
      "Brent Schiele"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w19/html/Khoreva_Video_Object_Segmentation_with_Referring_Expressions_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Khoreva_Video_Object_Segmentation_with_Referring_Expressions_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Most semi-supervised video object segmentation methods rely on a pixel-accurate mask of a target object provided for the first video frame. However, obtaining a detailed mask is expensive and time-consuming. In this work we explore a more practical and natural way of identifying a target object by employing language referring expressions. Leveraging recent advances of language grounding models designed for images, we propose an approach to extend them to video data, ensuring temporally coherent predictions. To evaluate our approach we augment the popular video object segmentation benchmarks, DAVIS16 and DAVIS17, with language descriptions of target objects. We show that our approach performs on par with the methods which have access to the object mask on DAVIS16 and is competitive to methods using scribbles on challenging DAVIS17."
  },
  "eccv2018_w20_mobileface3dfacereconstructionwithefficientcnnregression": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PeopleCap 2018: capturing and modeling human bodies, faces and hands.",
    "title": "MobileFace: 3D Face Reconstruction with Efficient CNN Regression",
    "authors": [
      "Nikolai Chinaev",
      "Alexander Chigorin",
      "Ivan Laptev"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w20/html/Chinaev_MobileFace_3D_Face_Reconstruction_with_Efficient_CNN_Regression_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Chinaev_MobileFace_3D_Face_Reconstruction_with_Efficient_CNN_Regression_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Estimation of facial shapes plays a central role for face transfer and animation. Accurate 3D face reconstruction, however, often deploys iterative and costly methods preventing real-time applications. In this work we design a compact and fast CNN model enabling real-time face reconstruction on mobile devices. For this purpose, we first study more traditional but slow morphable face models and use them to automatically annotate a large set of images for CNN training. We then investigate a class of efficient MobileNet CNNs and adapt such models for the task of shape regression. Our evaluation on three datasets demonstrates significant improvements in the speed and the size of our model while maintaining state-of-the-art reconstruction accuracy."
  },
  "eccv2018_w20_akinematicchainspaceformonocularmotioncapture": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PeopleCap 2018: capturing and modeling human bodies, faces and hands.",
    "title": "A Kinematic Chain Space for Monocular Motion Capture",
    "authors": [
      "Bastian Wandt",
      "Hanno Ackermann",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w20/html/Wandt_A_Kinematic_Chain_Space_for_Monocular_Motion_Capture_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Wandt_A_Kinematic_Chain_Space_for_Monocular_Motion_Capture_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper deals with motion capture of kinematic chains (e.g. human skeletons) from monocular image sequences taken by uncalibrated cameras. We present a method based on projecting an observation onto a kinematic chain space (KCS). An optimization of the nuclear norm is proposed that implicitly enforces structural properties of the kinematic chain. Unlike other approaches our method is not relying on training data or previously determined constraints such as particular body lengths. The proposed algorithm is able to reconstruct scenes with little or no camera motion and previously unseen motions. It is not only applicable to human skeletons but also to other kinematic chains for instance animals or industrial robots. We achieve state-of-the-art results on different benchmark databases and real world scenes."
  },
  "eccv2018_w20_non-rigid3dshaperegistrationusinganadaptivetemplate": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PeopleCap 2018: capturing and modeling human bodies, faces and hands.",
    "title": "Non-rigid 3D Shape Registration using an Adaptive Template",
    "authors": [
      "Hang Dai",
      "Nick Pears",
      "William Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w20/html/Dai_Non-rigid_3D_Shape_Registration_using_an_Adaptive_Template_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Dai_Non-rigid_3D_Shape_Registration_using_an_Adaptive_Template_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present a new fully-automatic non-rigid 3D shape registration (morphing) framework comprising (1) a new 3D landmarking and pose normalisation method; (2) an adaptive shape template method to improve the convergence of registration algorithms and achieve a better final shape correspondence and (3) a new iterative registration method that combines Iterative Closest Points with Coherent Point Drift (CPD) to achieve a more stable and accurate correspondence establishment than standard CPD. We call this new morphing approach Iterative Coherent Point Drift (ICPD). Our proposed framework is evaluated qualitatively and quantitatively on three datasets: Headspace, BU3D and a synthetic LSFM dataset, and is compared with several other methods. The proposed framework is shown to give state-of-the-art performance."
  },
  "eccv2018_w20_3dhumanbodyreconstructionfromasingleimageviavolumetricregression": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PeopleCap 2018: capturing and modeling human bodies, faces and hands.",
    "title": "3D Human Body Reconstruction from a Single Image via Volumetric Regression",
    "authors": [
      "Aaron S. Jackson",
      "Chris Manafas",
      "Georgios Tzimiropoulos"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w20/html/Jackson_3D_Human_Body_Reconstruction_from_a_Single_Image_via_Volumetric_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Jackson_3D_Human_Body_Reconstruction_from_a_Single_Image_via_Volumetric_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper proposes the use of an end-to-end Convolutional Neural Network for direct reconstruction of the 3D geometry of humans via volumetric regression. The proposed method does not require the fitting of a shape model and can be trained to work from a variety of input types, whether it be landmarks, images or segmentation masks. Additionally, non-visible parts, either self-occluded or otherwise, are still reconstructed, which is not the case with depth map regression. We present results that show that our method can handle both pose variation and detailed reconstruction given appropriate datasets for training."
  },
  "eccv2018_w20_can3dposebelearnedfrom2dprojectionsalone?": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W20",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - PeopleCap 2018: capturing and modeling human bodies, faces and hands.",
    "title": "Can 3D Pose be Learned from 2D Projections Alone?",
    "authors": [
      "Dylan Drover",
      "Rohith MV",
      "Ching-Hang Chen",
      "Amit Agrawal",
      "Ambrish Tyagi",
      "Cong Phuoc Huynh"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w20/html/Drover_Can_3D_Pose_be_Learned_from_2D_Projections_Alone_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Drover_Can_3D_Pose_be_Learned_from_2D_Projections_Alone_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " 3D pose estimation from a single image is a challenging task in computer vision. We present a weakly supervised approach to estimate 3D pose points, given only 2D pose landmarks. Our method does not require correspondences between 2D and 3D points to build explicit 3D priors. We utilize an adversarial framework to impose a prior on the 3D structure, learned solely from their random 2D projections. Given a set of 2D pose landmarks, the generator network hypothesizes their depths to obtain a 3D skeleton. We propose a novel Random Projection layer, which randomly projects the generated 3D skeleton and sends the resulting 2D pose to the discriminator. The discriminator improves by discriminating between the generated poses and pose samples from a real distribution of 2D poses. Training does not require correspondence between the 2D inputs to either the generator or the discriminator. We apply our approach to the task of 3D human pose estimation. Results on Human3.6M dataset demonstrates that our approach outperforms many previous supervised and weakly supervised approaches."
  },
  "eccv2018_w21_towardsafairevaluationofzero-shotactionrecognitionusingexternaldata": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Towards a Fair Evaluation of Zero-Shot Action Recognition using External Data",
    "authors": [
      "Alina Roitberg",
      "Manuel Martinez",
      "Monica Haurilet",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Roitberg_Towards_a_Fair_Evaluation_of_Zero-Shot_Action_Recognition_using_External_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Roitberg_Towards_a_Fair_Evaluation_of_Zero-Shot_Action_Recognition_using_External_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Zero-shot action recognition aims to classify actions not previously seen during training. This is achieved by learning a visual model for the seen source classes and establishing a semantic relationship to the unseen target classes e.g. through the action labels. In order to draw a clear line between zero-shot and conventional supervised classification, the source and target categories must be disjoint. Ensuring this premise is not trivial, especially when the source dataset is external. In this work, we propose an evaluation procedure that enables fair use of external data for zero-shot action recognition. We empirically show that external sources tend to have actions excessively similar to the target classes, strongly influencing the performance and violating the zero-shot premise. To address this, we propose a corrective method to automatically filter out too similar categories by exploiting the pairwise intra-dataset similarity of the labels. Our experiments on the HMDB-51 dataset demonstrate that the zero-shot models consistently benefit from the external sources even under our realistic evaluation, especially when the source categories of internal and external domains are combined."
  },
  "eccv2018_w21_moqa-amulti-modalquestionansweringarchitecture": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "MoQA - A Multi-Modal Question Answering Architecture",
    "authors": [
      "Monica Haurilet",
      "Ziad Al-Halah",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Haurilet_MoQA_-_A_Multi-Modal_Question_Answering_Architecture_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Haurilet_MoQA_-_A_Multi-Modal_Question_Answering_Architecture_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Multi-Modal Machine Comprehension (M3C) deals with extracting knowledge from multiple modalities such as figures, diagrams and text. Particularly, Textbook Question Answering (TQA) focuses on questions based on the school curricula, where the text and diagrams are extracted from textbooks. A subset of questions cannot be answered solely based on diagrams, but requires external knowledge of the surrounding text. In this work, we propose a novel deep model that is able to handle different knowledge modalities in the context of the question answering task. We compare three different information representations encountered in TQA: a visual representation learned from images, a graph representation of diagrams and a language-based representation learned from accompanying text. We evaluate our model on the TQA dataset that contains text and diagrams from the sixth grade material. Even though our model obtains competing results compared to stateof-the-art, we still witness a significant gap in performance compared to humans. We discuss in this work the shortcomings of the model and show the reason behind the large gap to human performance, by exploring the distribution of the multiple classes of mistakes that the model makes."
  },
  "eccv2018_w21_pre-genmetricspredictingcaptionqualitymetricswithoutgeneratingcaptions": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Pre-gen metrics: Predicting caption quality metrics without generating captions",
    "authors": [
      "Marc Tanti",
      "Albert Gatt",
      "Adrian Muscat"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Tanti_Pre-gen_metrics_Predicting_caption_quality_metrics_without_generating_captions_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Tanti_Pre-gen_metrics_Predicting_caption_quality_metrics_without_generating_captions_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Image caption generation systems are typically evaluated against reference outputs. We show that it is possible to predict output quality without generating the captions, based on the probability assigned by the neural model to the reference captions. Such pre-gen metrics are strongly correlated to standard evaluation metrics."
  },
  "eccv2018_w21_quantifyingtheamountofvisualinformationusedbyneuralcaptiongenerators": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Quantifying the amount of visual information used by neural caption generators",
    "authors": [
      "Marc Tanti",
      "Albert Gatt",
      "Kenneth P. Camilleri"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Tanti_Quantifying_the_amount_of_visual_information_used_by_neural_caption_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Tanti_Quantifying_the_amount_of_visual_information_used_by_neural_caption_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper addresses the sensitivity of neural image caption generators to their visual input. A sensitivity analysis and omission analysis based on image foils is reported, showing that the extent to which image captioning architectures retain and are sensitive to visual information varies depending on the type of word being generated and the position in the caption as a whole. We motivate this work in the context of broader goals in the field to achieve more explainability in AI."
  },
  "eccv2018_w21_distinctive-attributeextractionforimagecaptioning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Distinctive-attribute Extraction for Image Captioning",
    "authors": [
      "Boeun Kim",
      "Young Han Lee",
      "Hyedong Jung",
      "Choongsang Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Kim_Distinctive-attribute_Extraction_for_Image_Captioning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Kim_Distinctive-attribute_Extraction_for_Image_Captioning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Image captioning has evolved with the progress of deep neural networks. However, generating qualitatively detailed and distinctive captions is still an open issue. In previous works, a caption involving semantic description can be generated by applying additional information into the RNNs. In this approach, we propose a distinctive-attribute extraction (DaE) method that extracts attributes which explicitly encourage RNNs to generate an accurate caption. We evaluate the proposed method with a challenge data and verify that this method improves the performance, describing images in more detail. The method can be plugged into various models to improve their performance."
  },
  "eccv2018_w21_knowingwheretolook?analysisonattentionofvisualquestionansweringsystem": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Knowing Where to Look? Analysis on Attention of Visual Question Answering System",
    "authors": [
      "Wei Li",
      "Zehuan Yuan",
      "Xiangzhong Fang",
      "Changhu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Li_Knowing_Where_to_Look_Analysis_on_Attention_of_Visual_Question_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Li_Knowing_Where_to_Look_Analysis_on_Attention_of_Visual_Question_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " AttentionmechanismshavebeenwidelyusedinVisualQuestion Answering (VQA) solutions due to their capacity to model deep cross-domain interactions. Analyzing attention maps offers us a perspective to find out limitations of current VQA systems and an opportunity to further improve them. In this paper, we select two state-of-the-art VQA approaches with attention mechanisms to study their robustness and disadvantages by visualizing and analyzing their estimated attention maps. We find that both methods are sensitive to features, and simultaneously, they perform badly for counting and multi-object related questions. We believe that the findings and analytical method will help researchers identify crucial challenges on the way to improve their own VQA systems."
  },
  "eccv2018_w21_knowingwhentolookforwhatandwhereevaluatinggenerationofspatialdescriptionswithadaptiveattention": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Knowing When to Look For What and Where: Evaluating Generation of Spatial Descriptions with Adaptive Attention",
    "authors": [
      "Mehdi Ghanimifard",
      "Simon Dobnik"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Ghanimifard_Knowing_When_to_Look_For_What_and_Where_Evaluating_Generation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Ghanimifard_Knowing_When_to_Look_For_What_and_Where_Evaluating_Generation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We examine and evaluate adaptive attention [17] (which balances the focus on visual features and focus on textual features) in generating image captions in end-to-end neural networks, in particular how adaptive attention is informative for generating spatial relations. We show that the model generates spatial relations more on the basis of textual rather than visual features and therefore confirm the previous observations that the learned visual features are missing information about geometric relations between objects."
  },
  "eccv2018_w21_howcleveristhefilmmodel,andhowclevercanitbe": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "How clever is the FiLM model, and how clever can it be",
    "authors": [
      "Alexander Kuhnle",
      "Huiyuan Xie",
      "Ann Copestake"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Kuhnle_How_clever_is_the_FiLM_model_and_how_clever_can_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Kuhnle_How_clever_is_the_FiLM_model_and_how_clever_can_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The FiLM model achieves close-to-perfect performance on the diagnostic CLEVR dataset and is distinguished from other such models by having a comparatively simple and easily transferable architecture. In this paper, we investigate in more detail the ability of FiLM to learn various linguistic constructions. Our results indicate that (a) FiLM is not able to learn relational statements straight away except for very simple instances, (b) training on a broader set of instances as well as pretraining on simpler instance types can help alleviate these learning difficulties, (c) mixing is less robust than pretraining and very sensitive to the compositional structure of the dataset. Overall, our results suggest that the approach of big all-encompassing datasets and the paradigm of \"the effectiveness of data\" may have fundamental limitations."
  },
  "eccv2018_w21_image-sensitivelanguagemodelingforautomaticspeechrecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Image-sensitive language modeling for automatic speech recognition",
    "authors": [
      "Kata Naszadi",
      "Youssef Oualil",
      "Dietrich Klakow"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Naszadi_Image-sensitive_language_modeling_for_automatic_speech_recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Naszadi_Image-sensitive_language_modeling_for_automatic_speech_recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Typically language models in a speech recognizer just use the previous words as a context. Thus they are insensitive to context from the real world. This paper explores the benefits of introducing the visual modality as context information to automatic speech recognition. We use neural multimodal language models to rescore the recognition results of utterances that describe visual scenes. We provide a comprehensive survey of how much the language model improves when adding the image to the conditioning set. The image was introduced to a purely text-based RNN-LM using three different composition methods. Our experiments show that using the visual modality helps the recognition process by a 7.8% relative improvement, but can also hurt the results because of overfitting to the visual input."
  },
  "eccv2018_w21_addingobjectdetectionskillstovisualdialogueagents": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Shortcomings in Vision and Language",
    "title": "Adding object detection skills to visual dialogue agents",
    "authors": [
      "Gabriele Bani",
      "Davide Belli",
      "Gautier Dagan",
      "Alexander Geenen",
      "Andrii Skliar",
      "Aashish Venkatesh",
      "Tim Baumgartner",
      "Elia Bruni",
      "Raquel Fernandez"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w21/html/Bani_Adding_object_detection_skills_to_visual_dialogue_agents_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Bani_Adding_object_detection_skills_to_visual_dialogue_agents_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Our goal is to equip a dialogue agent that asks questions about a visual scene with object detection skills. We take the first steps in this direction within the GuessWhat?! game. We use Mask R-CNN object features as a replacement for ground-truth annotations in the Guesser module, achieving an accuracy of 57.92%. This proves that our system is a viable alternative to the original Guesser, which achieves an accuracy of 62.77% using ground-truth annotations, and thus should be considered an upper bound for our automated system. Crucially, we show that our system exploits the Mask R-CNN object features, in contrast to the original Guesser augmented with global, VGG features. Furthermore, by automating the object detection in GuessWhat?!, we open up a spectrum of opportunities, such as playing the game with new, non-annotated images and using the more granular visual features to condition the other modules of the game architecture."
  },
  "eccv2018_w22_the2ndyoutube-8mlarge-scalevideounderstandingchallenge": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "The 2nd YouTube-8M Large-Scale Video Understanding Challenge",
    "authors": [
      "Joonseok Lee",
      "Apostol (Paul) Natsev",
      "Walter Reade",
      "Rahul Sukthankar",
      "George Toderici"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Lee_The_2nd_YouTube-8M_Large-Scale_Video_Understanding_Challenge_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Lee_The_2nd_YouTube-8M_Large-Scale_Video_Understanding_Challenge_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We hosted the 2nd YouTube-8M Large-Scale Video Understanding Kaggle Challenge and Workshop at ECCV\u00e2\u0080\u009918, with the task of classifying videos from frame-level and video-level audio-visual features. In this year\u00e2\u0080\u0099s challenge, we restricted the final model size to 1GB or less, encouraging participants to explore representation learning or better architecture, instead of heavy ensembles of multiple models. In this paper, we briefly introduce the YouTube-8M dataset and challenge task, followed by participants statistics and result analysis. We summarize proposed ideas by participants, including architectures, temporal aggregation methods, ensembling and distillation, data augmentation, and more."
  },
  "eccv2018_w22_nextvladanefficientneuralnetworktoaggregateframe-levelfeaturesforlarge-scalevideoclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification",
    "authors": [
      "Rongcheng Lin",
      "Jing Xiao",
      "Jianping Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Lin_NeXtVLAD_An_Efficient_Neural_Network_to_Aggregate_Frame-level_Features_for_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Lin_NeXtVLAD_An_Efficient_Neural_Network_to_Aggregate_Frame-level_Features_for_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper introduces a fast and efficient network architecture, NeXtVLAD, to aggregate frame-level features into a compact feature vector for large-scale video classification. Briefly speaking, the basic idea is to decompose a high-dimensional feature into a group of relatively low-dimensional vectors with attention before applying NetVLAD aggregation over time. This NeXtVLAD approach turns out to be both effective and parameter efficient in aggregating temporal information. In the 2nd Youtube-8M video understanding challenge, a single NeXtVLAD model with less than 80M parameters achieves a GAP score of 0.87846 in private leaderboard. A mixture of 3 NeXtVLAD models results in 0.88722, which is ranked 3rd over 394 teams. The code is publicly available at https://github.com/linrongc/youtube-8m."
  },
  "eccv2018_w22_non-localnetvladencodingforvideoclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Non-local NetVLAD Encoding for Video Classification",
    "authors": [
      "Yongyi Tang",
      "Xing Zhang",
      "Lin Ma",
      "Jingwen Wang",
      "Shaoxiang Chen",
      "Yu-Gang Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Tang_Non-local_NetVLAD_Encoding_for_Video_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Tang_Non-local_NetVLAD_Encoding_for_Video_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper describes our solution for the 2nd YouTube-8M video understanding challenge organized by Google AI. Unlike the video recognition benchmarks, such as Kinetics and Moments, the YouTube8M challenge provides pre-extracted visual and audio features instead of raw videos. In this challenge, the submitted model is restricted to 1GB, which encourages participants focus on constructing one powerful single model rather than incorporating of the results from a bunch of models. Our system fuses six different sub-models into one single computational graph, which are categorized into three families. More specifically, the most effective family is the model with non-local operations following the NetVLAD encoding. The other two family models are Soft-BoF and GRU, respectively. In order to further boost single models performance, the model parameters of different checkpoints are averaged. Experimental results demonstrate that our proposed system can effectively perform the video classification task, achieving 0.88763 on the public test set and 0.88704 on the private set in terms of GAP@20, respectively. We finally ranked at the fourth place in the YouTube-8M video understanding challenge."
  },
  "eccv2018_w22_learnablepoolingmethodsforvideoclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Learnable Pooling Methods for Video Classification",
    "authors": [
      "Sebastian Kmiec",
      "Juhan Bae",
      "Ruijian An"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Kmiec_Learnable_Pooling_Methods_for_Video_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Kmiec_Learnable_Pooling_Methods_for_Video_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We introduce modifications to state-of-the-art approaches to aggregating local video descriptors by using attention mechanisms and function approximations. Rather than using ensembles of existing architectures, we provide an insight on creating new architectures. We demonstrate our solutions in the \"The 2nd YouTube-8M Video Understanding Challenge\", by using frame-level video and audio descriptors. We obtain testing accuracy similar to the state of the art, while meeting budget constraints, and touch upon strategies to improve the state of the art. Model implementations are available in https://github.com/pomonam/LearnablePoolingMethods."
  },
  "eccv2018_w22_constrained-sizetensorflowmodelsforyoutube-8mvideounderstandingchallenge": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Constrained-size Tensorflow Models for YouTube-8M Video Understanding Challenge",
    "authors": [
      "Tianqi Liu",
      "Bo Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Liu_Constrained-size_Tensorflow_Models_for_YouTube-8M_Video_Understanding_Challenge_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Liu_Constrained-size_Tensorflow_Models_for_YouTube-8M_Video_Understanding_Challenge_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents our 7th place solution to the second YouTube-8M video understanding competition which challenges participates to build a constrained-size model to classify millions of YouTube videos into thousands of classes. Our final model consists of four single models aggregated into one Tensorflow graph. For each single model, we use the same network architecture as in the winning solution of the first YouTube-8M video understanding competition, namely Gated NetVLAD. We train the single models separately in Tensorflow\u00e2\u0080\u0099s default float32 precision, then replace weights with float16 precision and ensemble them in the evaluation and inference stages, achieving 48.5% compression rate without loss of precision. Our best model achieved 88.324% GAP on private leaderboard. The code is publicly available at https://github.com/boliu61/youtube-8m"
  },
  "eccv2018_w22_labeldenoisingwithlargeensemblesofheterogeneousneuralnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Label Denoising with Large Ensembles of Heterogeneous Neural Networks",
    "authors": [
      "Pavel Ostyakov",
      "Elizaveta Logacheva",
      "Roman Suvorov",
      "Vladimir Aliev",
      "Gleb Sterkin",
      "Oleg Khomenko",
      "Sergey I. Nikolenko"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Ostyakov_Label_Denoising_with_Large_Ensembles_of_Heterogeneous_Neural_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Ostyakov_Label_Denoising_with_Large_Ensembles_of_Heterogeneous_Neural_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Despite recent advances in computer vision based on various convolutional architectures, video understanding remains an important challenge. In this work, we present and discuss a top solution for the large-scale video classification (labeling) problem introduced as a Kaggle competition based on the YouTube-8M dataset. We show and compare different approaches to preprocessing, data augmentation, model architectures, and model combination. Our final model is based on a large ensemble of videoand frame-level models but fits into rather limiting hardware constraints. We apply an approach based on knowledge distillation to deal with noisy labels in the original dataset and the recently developed mixup technique to improve the basic models."
  },
  "eccv2018_w22_hierarchicalvideoframesequencerepresentationwithdeepconvolutionalgraphnetwork": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network",
    "authors": [
      "Feng Mao",
      "Xiang Wu",
      "Hui Xue",
      "Rong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Mao_Hierarchical_Video_Frame_Sequence_Representation_with_Deep_Convolutional_Graph_Network_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Mao_Hierarchical_Video_Frame_Sequence_Representation_with_Deep_Convolutional_Graph_Network_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " High accuracy video label prediction (classification) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the efficiency for creating models. Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classification method based on a deep convolutional graph neural network(DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation reflecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks."
  },
  "eccv2018_w22_trainingcompactdeeplearningmodelsforvideoclassificationusingcirculantmatrices": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Training compact deep learning models for video classification using circulant matrices",
    "authors": [
      "Alexandre Araujo",
      "Benjamin Negrevergne",
      "Yann Chevaleyre",
      "Jamal Atif"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Araujo_Training_compact_deep_learning_models_for_video_classification_using_circulant_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Araujo_Training_compact_deep_learning_models_for_video_classification_using_circulant_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In real world scenarios, model accuracy is hardly the only factor to consider. Large models consume more memory and are computationally more intensive, which make them difficult to train and to deploy, especially on mobile devices. In this paper, we build on recent results at the crossroads of Linear Algebra and Deep Learning which demonstrate how imposing a structure on large weight matrices can be used to reduce the size of the model. Building on these results, we propose very compact models for video classification based on state-of-the-art network architectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then conduct thorough experiments using the large YouTube-8M video classification dataset. As we will show, the circulant DBoF embedding achieves an excellent trade-off between size and accuracy."
  },
  "eccv2018_w22_towardsgoodpracticesformulti-modalfusioninlarge-scalevideoclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Towards Good Practices for Multi-modal Fusion in Large-scale Video Classification",
    "authors": [
      "Jinlai Liu",
      "Zehuan Yuan",
      "Changhu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Liu_Towards_Good_Practices_for_Multi-modal_Fusion_in_Large-scale_Video_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Liu_Towards_Good_Practices_for_Multi-modal_Fusion_in_Large-scale_Video_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Leveraging both visual frames and audio has been experimentally proven effective to improve large-scale video classification. Previous research on video classification mainly focuses on the analysis of visual content among extracted video frames and their temporal feature aggregation. In contrast, multimodal data fusion is achieved by simple operators like average and concatenation. Inspired by the success of bilinear pooling in the visual and language fusion, we introduce multi-modal factorized bilinear pooling (MFB) to fuse visual and audio representations. We combine MFB with different video-level features and explore its effectiveness in video classification. Experimental results on the challenging Youtube-8M v2 dataset demonstrate that MFB significantly outperforms simple fusion methods in large-scale video classification."
  },
  "eccv2018_w22_buildingasizeconstrainedpredictivemodelforvideoclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Building a Size Constrained Predictive Model for Video Classification",
    "authors": [
      "Miha Skalic",
      "David Austin"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Skalic_Building_a_Size_Constrained_Predictive_Model_for_Video_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Skalic_Building_a_Size_Constrained_Predictive_Model_for_Video_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Herein we present the solution to the 2nd YouTube-8M video understanding challenge which placed 1st. Competition participants were tasked with building a size constrained video labeling model with a model size of less than 1GB. Our final solution consists of several submodels belonging to Fisher vectors, NetVlad, Deep Bag of Frames and Recurrent neural networks model families. To make the classifier efficient under size constraints we introduced model distillation, partial weights quantization and training with exponential moving average."
  },
  "eccv2018_w22_temporalattentionmechanismwithconditionalinferenceforlarge-scalemulti-labelvideoclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Temporal Attention Mechanism with Conditional Inference for Large-Scale Multi-Label Video Classification",
    "authors": [
      "Eun-Sol Kim",
      "Kyoung-Woon On",
      "Jongseok Kim",
      "Yu-Jung Heo",
      "Seong-Ho Choi",
      "Hyun-Dong Lee",
      "Byoung-Tak Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Kim_Temporal_Attention_Mechanism_with_Conditional_Inference_for_Large-Scale_Multi-Label_Video_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Kim_Temporal_Attention_Mechanism_with_Conditional_Inference_for_Large-Scale_Multi-Label_Video_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Here we show neural network based methods, which combine multimodal sequential inputs effectively and classify the inputs into multiple categories. Two key ideas are 1) to select informative frames among a sequence using attention mechanism and 2) to utilize correlation information between labels to solve multi-label classification problems. The attention mechanism is used in both modality (spatio) and sequential (temporal) dimensions to ignore noisy and meaningless frames. Furthermore, to tackle fundamental problems induced by independently predicting each label in conventional multi-label classification methods, the proposed method considers the dependencies among the labels by decomposing joint probability of labels into conditional terms. From the experimental results (5th in the Kaggle competition), we discuss how the suggested methods operate in the YouTube-8M Classification Task, what insights they have, and why they succeed or fail."
  },
  "eccv2018_w22_approachforvideoclassificationwithmulti-labelonyoutube-8mdataset": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Approach for Video Classification with Multi-label on YouTube-8M Dataset",
    "authors": [
      "Kwangsoo Shin",
      "Junhyeong Jeon",
      "Seungbin Lee",
      "Boyoung Lim",
      "Minsoo Jeong",
      "Jongho Nang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Shin_Approach_for_Video_Classification_with_Multi-label_on_YouTube-8M_Dataset_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Shin_Approach_for_Video_Classification_with_Multi-label_on_YouTube-8M_Dataset_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Video traffic is increasing at a considerable rate due to the spread of personal media and advancements in media technology. Accordingly, there is a growing need for techniques to automatically classify moving images. This paper use NetVLAD and NetFV models and the Huber loss function for video classification problem and YouTube-8M dataset to verify the experiment. We tried various attempts according to the dataset and optimize hyperparameters, ultimately obtain a GAP score of 0.8668."
  },
  "eccv2018_w22_learningvideofeaturesformulti-labelclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Learning Video Features for Multi-Label Classification",
    "authors": [
      "Shivam Garg"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Garg_Learning_Video_Features_for_Multi-Label_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Garg_Learning_Video_Features_for_Multi-Label_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper studies some approaches to learn representation of videos. This work was done as a part of Youtube-8M Video Understanding Challenge. The main focus is to analyze various approaches used to model temporal data and evaluate the performance of such approaches on this problem. Also, a model is proposed which reduces the size of feature vector by 70% but does not compromise on accuracy. The first approach is to use recurrent neural network architectures to learn a single video level feature from frame level features and then use this aggregated feature to do multi-label classification. The second approach is to use video level features and deep neural networks to assign the labels."
  },
  "eccv2018_w22_large-scalevideoclassificationwithfeaturespaceaugmentationcoupledwithlearnedlabelrelationsandensembling": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd YouTube-8M Large-Scale Video Understanding Workshop",
    "title": "Large-Scale Video Classification with Feature Space Augmentation coupled with Learned Label Relations and Ensembling",
    "authors": [
      "Choongyeun Cho",
      "Benjamin Antin",
      "Sanchit Arora",
      "Shwan Ashrafi",
      "Peilin Duan",
      "Dang The Huynh",
      "Lee James",
      "Hang Tuan Nguyen",
      "Mojtaba Solgi",
      "Cuong Van Than"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w22/html/Cho_Large-Scale_Video_Classification_with_Feature_Space_Augmentation_coupled_with_Learned_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Cho_Large-Scale_Video_Classification_with_Feature_Space_Augmentation_coupled_with_Learned_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents the Axon AI\u00e2\u0080\u0099s solution to the 2nd YouTube8M Video Understanding Challenge, achieving the final global average precision (GAP) of 88.733% on the private test set (ranked 3rd among 394 teams, not considering the model size constraint), and 87.287% using a model that meets size requirement. Two sets of 7 individual models belonging to 3 different families were trained separately. Then, the inference results on a training data were aggregated from these multiple models and fed to train a compact model that meets the model size requirement. In order to further improve performance we explored and employed data over/sub-sampling in feature space, an additional regularization term during training exploiting label relationship, and learned weights for ensembling different individual models."
  },
  "eccv2018_w23_multi-stylegenerativenetworkforreal-timetransfer": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Multi-style Generative Network for Real-time Transfer",
    "authors": [
      "Hang Zhang",
      "Kristin Dana"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Zhang_Multi-style_Generative_Network_for_Real-time_Transfer_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Zhang_Multi-style_Generative_Network_for_Real-time_Transfer_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Despite the rapid progress in style transfer, existing approaches using feed-forward generative network for multi-style or arbitrary-style transfer are usually compromised of image quality and model flexibility. We find it is fundamentally difficult to achieve comprehensive style modeling using 1-dimensional style embedding. Motivated by this, we introduce CoMatch Layer that learns to match the second order feature statistics with the target styles. With the CoMatch Layer, we build a Multi-style Generative Network (MSG-Net), which achieves real-time performance. In addition, we employ an specific strategy of upsampled convolution which avoids checkerboard artifacts caused by fractionally-strided convolution. Our method has achieved superior image quality comparing to stateof-the-art approaches. The proposed MSG-Net as a general approach for realtime style transfer is compatible with most existing techniques including contentstyle interpolation, color-preserving, spatial control and brush stroke size control. MSG-Net is the first to achieve real-time brush-size control in a purely feedforward manner for style transfer. Our implementations and pre-trained models for Torch, PyTorch and MXNet frameworks will be publicly available1."
  },
  "eccv2018_w23_frustratinglyeasytrade-offoptimizationbetweensingle-stageandtwo-stagedeepobjectdetectors": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Frustratingly Easy Trade-off Optimization between Single-Stage and Two-Stage Deep Object Detectors",
    "authors": [
      "Petru Soviany",
      "Radu Tudor Ionescu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Soviany_Frustratingly_Easy_Trade-off_Optimization_between_Single-Stage_and_Two-Stage_Deep_Object_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Soviany_Frustratingly_Easy_Trade-off_Optimization_between_Single-Stage_and_Two-Stage_Deep_Object_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " There are mainly two types of state-of-the-art object detectors. On one hand, we have two-stage detectors, such as Faster R-CNN (Region-based Convolutional Neural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to generate regions of interests in the first stage and (ii) send the region proposals down the pipeline for object classification and bounding-box regression. Such models reach the highest accuracy rates, but are typically slower. On the other hand, we have single-stage detectors, such as YOLO (You Only Look Once) and SSD (Singe Shot MultiBox Detector), that treat object detection as a simple regression problem, by taking an input image and learning the class probabilities and bounding box coordinates. Such models reach lower accuracy rates, but are much faster than two-stage object detectors. In this paper, we propose and evaluate four simple and straightforward approaches to achieve an optimal trade-off between accuracy and speed in object detection. All the approaches are based on separating the test images in two batches, an easy batch that is fed to a faster single-stage detector and a difficult batch that is fed to a more accurate two-stage detector. The difference between the four approaches is the criterion used for splitting the images in two batches. The criteria are the image difficulty score (easier images go into the easy batch), the number of detected objects (images with less objects go into the easy batch), the average size of the detected objects (images with bigger objects go into the easy batch), and the number of detected objects divided by their average size (images with less and bigger objects go into the easy batch). The first approach is based on an image difficulty predictor, while the other three approaches employ a faster single-stage detector to determine the approximate number of objects and their sizes. Our experiments on PASCAL VOC 2007 show that using image difficulty compares favorably to a random split of the images. However, splitting the images based on the number objects divided by their size, an approach that is frustratingly easy to implement, produces even better results. Remarkably, it shortens the processing time nearly by half, while reducing the mean Average Precision of Faster R-CNN by only 0.5%."
  },
  "eccv2018_w23_targetedkernelnetworksfasterconvolutionswithattentiveregularization": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Targeted Kernel Networks: Faster Convolutions with Attentive Regularization",
    "authors": [
      "Kashyap Chitta"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Chitta_Targeted_Kernel_Networks_Faster_Convolutions_with_Attentive_Regularization_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Chitta_Targeted_Kernel_Networks_Faster_Convolutions_with_Attentive_Regularization_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose Attentive Regularization(AR), a method to constrain the activation maps of kernels in Convolutional Neural Networks (CNNs) to specific regions of interest (ROIs). Each kernel learns a location of specialization along with its weights through standard backpropagation. A differentiable attention mechanism requiring no additional supervision is used to optimize the ROIs. Traditional CNNs of different types and structures can be modified with this idea into equivalent Targeted Kernel Networks (TKNs), while keeping the network size nearly identical. By restricting kernel ROIs, we reduce the number of sliding convolutional operations performed throughout the network in its forward pass, speeding up both training and inference. We evaluate our proposed architecture on both synthetic and natural tasks across multiple domains. TKNs obtain significant improvements over baselines, requiring less computation (around an order of magnitude) while achieving superior performance."
  },
  "eccv2018_w23_smalldefectdetectionusingconvolutionalneuralnetworkfeaturesandrandomforests": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Small Defect Detection Using Convolutional Neural Network Features and Random Forests",
    "authors": [
      "Xinghui Dong",
      "Chris J. Taylor",
      "Tim F. Cootes"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Dong_Small_Defect_Detection_Using_Convolutional_Neural_Network_Features_and_Random_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Dong_Small_Defect_Detection_Using_Convolutional_Neural_Network_Features_and_Random_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We address the problem of identifying small abnormalities in an imaged region, important in applications such as industrial inspection. The goal is to label the pixels corresponding to a defect with a minimum of false positives. A common approach is to run a sliding-window classifier over the image. Recent Fully Convolutional Networks (FCNs), such as U-Net, can be trained to identify pixels corresponding to abnormalities given a suitable training set. However in many application domains it is hard to collect large numbers of defect examples (by their nature they are rare). Although U-Net can work in this scenario, we show that better results can be obtained by replacing the final softmax layer of the network with a Random Forest (RF) using features sampled from the earlier network layers. We also demonstrate that rather than just thresholding the resulting probability image to identify defects it is better to compute Maximally Stable Extremal Regions (MSERs). We apply the approach to the challenging problem of identifying defects in radiographs of aerospace welds."
  },
  "eccv2018_w23_compactdeepaggregationforsetretrieval": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Compact Deep Aggregation for Set Retrieval",
    "authors": [
      "Yujie Zhong",
      "Relja Arandjelovic",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Zhong_Compact_Deep_Aggregation_for_Set_Retrieval_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Zhong_Compact_Deep_Aggregation_for_Set_Retrieval_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The objective of this work is to learn a compact embedding of a set of descriptors that is suitable for efficient retrieval and ranking, whilst maintaining discriminability of the individual descriptors. We focus on a specific example of this general problem \u00e2\u0080\u0093 that of retrieving images containing multiple faces from a large scale dataset of images. Here the set consists of the face descriptors in each image, and given a query for multiple identities, the goal is then to retrieve, in order, images which contain all the identities, all but one, etc.To this end, we make the following contributions: first, we propose a CNN architecture \u00e2\u0080\u0093 SetNet \u00e2\u0080\u0093 to achieve the objective: it learns face descriptors and their aggregation over a set to produce a compact fixed length descriptor designed for set retrieval, and the score of an image is a count of the number of identities that match the query; second, we show that this compact descriptor has minimal loss of discriminability up to two faces per image, and degrades slowly after that \u00e2\u0080\u0093 far exceeding a number of baselines; third, we explore the speed vs. retrieval quality trade-off for set retrieval using this compact descriptor; and, finally, we collect and annotate a large dataset of images containing various number of celebrities, which we use for evaluation and will be publicly released."
  },
  "eccv2018_w23_adversarialnetworkcompression": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Adversarial Network Compression",
    "authors": [
      "Vasileios Belagiannis",
      "Azade Farshad",
      "Fabio Galasso"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Belagiannis_Adversarial_Network_Compression_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Belagiannis_Adversarial_Network_Compression_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Neural network compression has recently received much attention due to the computational requirements of modern deep models. In this work, our objective is to transfer knowledge from a deep and accurate model to a smaller one. Our contributions are threefold: (i) we propose an adversarial network compression approach to train the small student network to mimic the large teacher, without the need for labels during training; (ii) we introduce a regularization scheme to prevent a trivially-strong discriminator without reducing the network capacity and (iii) our approach generalizes on different teacher-student models.In an extensive evaluation on five standard datasets, we show that our student has small accuracy drop, achieves better performance than other knowledge transfer approaches and it surpasses the performance of the same network trained with labels. In addition, we demonstrate state-ofthe-art results compared to other compression strategies."
  },
  "eccv2018_w23_targetawarenetworkadaptationforefficientrepresentationlearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Target Aware Network Adaptation for Efficient Representation Learning",
    "authors": [
      "Yang Zhong",
      "Vladimir Li",
      "Ryuzo Okada",
      "Atsuto Maki"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Zhong_Target_Aware_Network_Adaptation_for_Efficient_Representation_Learning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Zhong_Target_Aware_Network_Adaptation_for_Efficient_Representation_Learning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents an automatic network adaptation method that finds a ConvNet structure well-suited to a given target task, e.g. image classification, for efficiency as well as accuracy in transfer learning. We call the concept target-aware transfer learning. Given only smallscale labeled data, and starting from an ImageNet pre-trained network, we exploit a scheme of removing its potential redundancy for the target task through iterative operations of filter-wise pruning and network optimization. The basic motivation is that compact networks are on one hand more efficient and should also be more tolerant, being less complex, against the risk of overfitting which would hinder the generalization of learned representations in the context of transfer learning. Further, unlike existing methods involving network simplification, we also let the scheme identify redundant portions across the entire network, which automatically results in a network structure adapted to the task at hand. We achieve this with a few novel ideas: (i) cumulative sum of activation statistics for each layer, and (ii) a priority evaluation of pruning across multiple layers. Experimental results by the method on five datasets (Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable accuracies over the related state-of-the-art techniques while enhancing the computational and storage efficiency of the transferred model."
  },
  "eccv2018_w23_learningccarepresentationsformisaligneddata": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Learning CCA Representations for Misaligned Data",
    "authors": [
      "Hichem Sahbi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Sahbi_Learning_CCA_Representations_for_Misaligned_Data_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Sahbi_Learning_CCA_Representations_for_Misaligned_Data_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Canonical correlation analysis (CCA) is a statistical learning method that seeks to build view-independent latent representations from multi-view data. This method has been successfully applied to several pattern analysis tasks such as image-to-text mapping and view-invariant object/action recognition. However, this success is highly dependent on the quality of data pairing (i.e., alignments) and mispairing adversely affects the generalization ability of the learned CCA representations.In this paper, we address the issue of alignment errors using a new variant of canonical correlation analysis referred to as alignment-agnostic (AA) CCA. Starting from erroneously paired data taken from different views, this CCA finds transformation matrices by optimizing a constrained maximization problem that mixes a data correlation term with context regularization; the particular design of these two terms mitigates the effect of alignment errors when learning the CCA transformations. Experiments conducted on multi-view tasks, including multi-temporal satellite image change detection, show that our AA CCA method is highly effective and resilient to mispairing errors."
  },
  "eccv2018_w23_learningrelationship-awarevisualfeatures": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Learning Relationship-aware Visual Features",
    "authors": [
      "Nicola Messina",
      "Giuseppe Amato",
      "Fabio Carrara",
      "Fabrizio Falchi",
      "Claudio Gennaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Messina_Learning_Relationship-aware_Visual_Features_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Messina_Learning_Relationship-aware_Visual_Features_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Relational reasoning in Computer Vision has recently shown impressive results on visual question answering tasks. On the challenging dataset called CLEVR, the recently proposed Relation Network (RN), a simple plug-and-play module and one of the state-of-the-art approaches, has obtained a very good accuracy (95.5%) answering relational questions. In this paper, we define a sub-field of Content-Based Image Retrieval (CBIR) called Relational-CBIR (R-CBIR), in which we are interested in retrieving images with given relationships among objects. To this aim, we employ the RN architecture in order to extract relationaware features from CLEVR images. To prove the effectiveness of these features, we extended both CLEVR and Sort-of-CLEVR datasets generating a ground-truth for R-CBIR by exploiting relational data embedded into scene-graphs. Furthermore, we propose a modification of the RN module \u00e2\u0080\u0093 a two-stage Relation Network (2S-RN) \u00e2\u0080\u0093 that enabled us to extract relation-aware features by using a preprocessing stage able to focus on the image content, leaving the question apart. Experiments show that our RN features, especially the 2S-RN ones, outperform the RMAC state-of-the-art features on this new challenging task."
  },
  "eccv2018_w23_dnnfeaturemapcompressionusinglearnedrepresentationovergf(2)": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "DNN Feature Map Compression using Learned Representation over GF(2)",
    "authors": [
      "Denis Gudovskiy",
      "Alec Hodgkinson",
      "Luca Rigazio"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Gudovskiy_DNN_Feature_Map_Compression_using_Learned_Representation_over_GF2_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Gudovskiy_DNN_Feature_Map_Compression_using_Learned_Representation_over_GF2_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Unlike previous works, the proposed method is based on converting fixed-point activations into vectors over the smallest GF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers embedded into a DNN. Such an end-to-end learned representation finds more compact feature maps by exploiting quantization redundancies within the fixed-point activations along the channel or spatial dimensions. We apply the proposed network architectures derived from modified SqueezeNet and MobileNetV2 to the tasks of ImageNet classification and PASCAL VOC object detection. Compared to prior approaches, the conducted experiments show a factor of 2 decrease in memory requirements with minor degradation in accuracy while adding only bitwise computations."
  },
  "eccv2018_w23_lbp-motivatedcolourtextureclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "LBP-Motivated Colour Texture Classification",
    "authors": [
      "Raquel Bello-Cerezo",
      "Paul Fieguth",
      "Francesco Bianconi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Bello-Cerezo_LBP-Motivated_Colour_Texture_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Bello-Cerezo_LBP-Motivated_Colour_Texture_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper we investigate extensions of Local Binary Patterns (LBP), Improved Local Binary Patterns (ILBP) and Extended Local Binary Patterns (ELBP) to colour textures via two different strategies: intra-/inter-channel features and colour orderings. We experimentally evaluate the proposed methods over 15 datasets of general and biomedical colour textures. Intraand inter-channel features from the RGB space emerged as the best descriptors and we found that the best accuracy was achieved by combining multi-resolution intra-channel features with single-resolution inter-channel features."
  },
  "eccv2018_w23_discriminativefeatureselectionbyoptimalmanifoldsearchforneoplasticimagerecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Discriminative Feature Selection by Optimal Manifold Search for Neoplastic Image Recognition",
    "authors": [
      "Hayato Itoh",
      "Yuichi Mori",
      "Masashi Misawa",
      "Masahiro Oda",
      "Shin-Ei Kudo",
      "Kensaku Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Itoh_Discriminative_Feature_Selection_by_Optimal_Manifold_Search_for_Neoplastic_Image_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Itoh_Discriminative_Feature_Selection_by_Optimal_Manifold_Search_for_Neoplastic_Image_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " An endocytoscope provides ultramagnified observation that enables physicians to achieve minimally invasive and real-time diagnosis in colonoscopy. However, great pathological knowledge and clinical experiences are required for this diagnosis. The computer-aided diagnosis (CAD) system is required that decreases the chances of overlooking neoplastic polyps in endocytoscopy. Towards the construction of a CAD system, we have developed texture-feature-based classification between neoplastic and non-neoplastic images of polyps. We propose a featureselection method that selects discriminative features from texture features for such two-category classification by searching for an optimal manifold. With an optimal manifold, where selected features are distributed, the distance between two linear subspaces is maximised. We experimentally evaluated the proposed method by comparing the classification accuracy before and after the feature selection for texture features and deep-learning features. Furthermore, we clarified the characteristics of an optimal manifold by exploring the relation between the classification accuracy and the output probability of a support vector machine (SVM). The classification with our feature-selection method achieved 84.7% accuracy, which is 7.2% higher than the direct application of Haralick features and SVM."
  },
  "eccv2018_w23_fast,visualandinteractivesemi-superviseddimensionalityreduction": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Fast, Visual and Interactive Semi-supervised Dimensionality Reduction ",
    "authors": [
      "Dimitris Spathis",
      "Nikolaos Passalis",
      "Anastasios Tefas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Spathis_Fast_Visual_and_Interactive_Semi-supervised_Dimensionality_Reduction_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Spathis_Fast_Visual_and_Interactive_Semi-supervised_Dimensionality_Reduction_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recent advances in machine learning allow us to analyze and describe the content of high-dimensional data ranging from images and video to text and audio data. In order to visualize that data in 2D or 3D, usually Dimensionality Reduction (DR) techniques are employed. Most of these techniques produce static projections without taking into account corrections from humans or other data exploration scenarios. In this work, we propose a novel interactive DR framework that is able to learn the optimal projection by exploiting the user interactions with the projected data. We evaluate the proposed method under a widely used interaction scenario in multidimensional projection literature, i.e., project a subset of the data, rearrange them better in classes, and then project the rest of the dataset, and we show that it greatly outperforms competitive baseline and state-of-the-art techniques, while also being able to readily adapt to the computational requirements of different applications."
  },
  "eccv2018_w23_efficienttextureretrievalusingmultiscalelocalextremadescriptorsandcovarianceembedding": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Efficient texture retrieval using multiscale local extrema descriptors and covariance embedding",
    "authors": [
      "Minh-Tan Pham"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Pham_Efficient_texture_retrieval_using_multiscale_local_extrema_descriptors_and_covariance_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Pham_Efficient_texture_retrieval_using_multiscale_local_extrema_descriptors_and_covariance_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present an efficient method for texture retrieval using multiscale feature extraction and embedding based on the local extrema keypoints. The idea is to first represent each texture image by its local maximum and local minimum pixels. The image is then divided into regular overlapping blocks and each one is characterized by a feature vector constructed from the radiometric, geometric and structural information of its local extrema. All feature vectors are finally embedded into a covariance matrix which will be exploited for dissimilarity measurement within retrieval task. Thanks to the method\u00e2\u0080\u0099s simplicity, multiscale scheme can be easily implemented to improve its scale-space representation capacity. We argue that our handcrafted features are easy to implement, fast to run but can provide very competitive performance compared to handcrafted and CNN-based learned descriptors from the literature. In particular, the proposed framework provides highly competitive retrieval rate for several texture databases including 94.95% for MIT Vistex, 79.87% for Stex, 76.15% for Outex TC-00013 and 89.74% for USPtex."
  },
  "eccv2018_w23_extendednon-localfeatureforvisualsaliencydetectioninlowcontrastimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Extended non-local feature for visual saliency detection in low contrast images",
    "authors": [
      "Xin Xu",
      "Jie Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Xu_Extended_non-local_feature_for_visual_saliency_detection_in_low_contrast_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Xu_Extended_non-local_feature_for_visual_saliency_detection_in_low_contrast_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Saliency detection model can substantially facilitate a wide range of applications. Conventional saliency detection models primarily rely on high level features from deep learning and hand-crafted low-level image features. However, they may face great challenges in nighttime scenario, due to the lack of well-defined feature to represent saliency information in low contrast images. This paper proposes a saliency detection model for nighttime scene. This model is capable of extracting non-local feature that is jointly learned with local features under a unified deep learning framework. The key idea of the proposed model is to hierarchically introduce non-local module with local contrast processing blocks, aiming to provide robust representation of saliency information towards low contrast images with low signal-to-noise ratio property. Besides, both nighttime and daytime images are utilized in training to provide complementary information. Extensive experiments have been conducted on five challenging datasets and our nighttime image dataset to evaluate the performance of the proposed model."
  },
  "eccv2018_w23_incompletemulti-viewclusteringviagraphregularizedmatrixfactorization": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Incomplete Multi-view Clustering via Graph Regularized Matrix Factorization",
    "authors": [
      "Jie Wen",
      "Zheng Zhang",
      "Yong Xu",
      "Zuofeng Zhong"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Wen_Incomplete_Multi-view_Clustering_via_Graph_Regularized_Matrix_Factorization_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Wen_Incomplete_Multi-view_Clustering_via_Graph_Regularized_Matrix_Factorization_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Clustering with incomplete views is a challenge in multi-view clustering. In this paper, we provide a novel and simple method to address this issue. Specially, the proposed method simultaneously exploits the local information of each view and the complementary information among views to learn the common latent representation for all samples, which can greatly improve the compactness and discriminability of the obtained representation. Compared with the conventional graph embedding methods, the proposed method does not introduce any extra regularization term and corresponding penalty parameter to preserve the local structure of data, and thus does not increase the burden of extra parameter selection. By imposing the orthogonal constraint on the basis matrix of each view, the proposed method is able to handle the outof-sample. Moreover, the proposed method can be viewed as a unified framework for multi-view learning since it can handle both incomplete and complete multi-view clustering and classification tasks. Extensive experiments conducted on several multi-view datasets prove that the proposed method can significantly improve the clustering performance."
  },
  "eccv2018_w23_ga-basedfilterselectionforrepresentationinconvolutionalneuralnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "GA-based Filter Selection for Representation in Convolutional Neural Networks",
    "authors": [
      "Junbong Kim",
      "Minki Lee",
      "Jongeun Choi",
      "Kisung Seo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Kim_GA-based_Filter_Selection_for_Representation_in_Convolutional_Neural_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Kim_GA-based_Filter_Selection_for_Representation_in_Convolutional_Neural_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " One of the deep learning models, a convolutional neural network (CNN) has been very successful in a variety of computer vision tasks. Features of a CNN are automatically generated, however, they can be further optimized since they often require large scale parallel operations and there exist the possibility of overlapping redundant features. The aim of this paper is to use feature selection via evolutionary algorithms to remove the irrelevant deep features. This will minimize the computational complexity and the amount of overfitting while maintaining a good quality of representation. We demonstrate the improvement of the filter representation by performing experiments on three data sets of CIFAR10, metal surface defects, and variation of MNIST and by analyzing the classification performance as well as the variance of the filter."
  },
  "eccv2018_w23_activedescriptorlearningforfeaturematching": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Active Descriptor Learning for Feature Matching",
    "authors": [
      "Aziz Kocanaogullari",
      "Esra Ataer-Cansizoglu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Kocanaogullari_Active_Descriptor_Learning_for_Feature_Matching_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Kocanaogullari_Active_Descriptor_Learning_for_Feature_Matching_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Feature descriptor extraction lies at the core of many computer vision tasks including image retrieval and registration. In this paper, we present an active learning method for extracting efficient features to be used in matching image patches. We train a Siamese deep neural network by optimizing a triplet loss function. We develop a more efficient and faster training procedure compared to the state-of-the-art methods by increasing difficulty during batch training. We achieve this by adjusting the margin in the loss and picking harder samples over time. The experiments are carried out on Photo Tourism dataset. The results show a significant improvement on matching performance and faster convergence in training."
  },
  "eccv2018_w23_ajointgenerativemodelforzero-shotlearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 2nd International Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "A Joint Generative Model for Zero-Shot Learning",
    "authors": [
      "Rui Gao",
      "Xingsong Hou",
      "Jie Qin",
      "Li Liu",
      "Fan Zhu",
      "Zhao Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w23/html/Gao_A_Joint_Generative_Model_for_Zero-Shot_Learning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Gao_A_Joint_Generative_Model_for_Zero-Shot_Learning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Zero-shot learning (ZSL) is a challenging task due to the lack of data from unseen classes during training. Existing methods tend to have the strong bias towards seen classes, which is also known as the domain shift problem. To mitigate the gap between seen and unseen class data, we propose a joint generative model to synthesize features as the replacement for unseen data. Based on the generated features, the conventional ZSL problem can be tackled in a supervised way. Specifically, our framework integrates Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN) conditioned on class-level semantic attributes for feature generation based on element-wise and holistic reconstruction. A categorization network acts as the additional guide to generate features beneficial for the subsequent classification task. Moreover, we propose a perceptual reconstruction loss to preserve semantic similarities. Experimental results on five benchmarks show the superiority of our framework over the state-of-the-art approaches in terms of both conventional ZSL and generalized ZSL settings."
  },
  "eccv2018_w24_wicvateccv2018thefifthwomenincomputervisionworkshop": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "WiCV at ECCV2018: The Fifth Women in Computer Vision Workshop",
    "authors": [
      "Zeynep Akata",
      "Dena Bazazian",
      "Yana Hasson",
      "Angjoo Kanazawa",
      "Hildegard Kuehne",
      "Gul Varol"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Akata_WiCV_at_ECCV2018_The_Fifth_Women_in_Computer_Vision_Workshop_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Akata_WiCV_at_ECCV2018_The_Fifth_Women_in_Computer_Vision_Workshop_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We report a summary of the Women in Computer Vision Workshop (WiCV) at ECCV 2018. WiCV focuses on creating a more inclusive environment for women researchers, a minority in the currently male dominated field of Computer Vision. In fact, despite the incredible progress of computer vision and machine learning and the growing interest towards these topics, the amount of female researchers is still limited both in academia and industry. The workshop is therefore an opportunity to promote collaborations, increase visibility and inclusion, and provide mentoring. Moreover the workshop offers a venue to discuss gender related biases still present throughout the work environments and are often not discussed with the due importance. This was the fifth WiCV workshop in its fourth year, and also the first WiCV held in Europe, in conjunction with ECCV. We have made changes in our program according to lessons learned from previous workshops and were able to obtain an unprecedented attendance exceeding the room capacity. We report a summary of statistics for presenters and attendees, followed by expectations for future iterations of the workshop."
  },
  "eccv2018_w24_gaitenergyimagereconstructionfromdegradedgaitcycleusingdeeplearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Gait Energy Image Reconstruction from Degraded Gait Cycle Using Deep Learning",
    "authors": [
      "Maryam Babaee",
      "Linwei Li",
      "Gerhard Rigoll"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Babaee_Gait_Energy_Image_Reconstruction_from_Degraded_Gait_Cycle_Using_Deep_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Babaee_Gait_Energy_Image_Reconstruction_from_Degraded_Gait_Cycle_Using_Deep_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Gait energy image (GEI) is considered as an effective gait representation for gait-based human identification. In gait recognition, normally, GEI is computed from one full gait cycle. However in many circumstances, such a full gait cycle might not be available due to occlusion. Thus, the GEI is not complete, giving a rise to degrading gait identification rate. In this paper, we address this issue by proposing a novel method to reconstruct a complete GEI from a few frames of gait cycle. To do so, we propose a deep learning-based approach to transform incomplete GEI to the corresponding complete GEI obtained from a full gait cycle. More precisely, this transformation is done gradually by training several fully convolutional networks independently and then combining these as a uniform model. Experimental results on a large public gait dataset, namely OULP demonstrate the validity of the proposed method for gait identification when dealing with very incomplete gait cycles."
  },
  "eccv2018_w24_fine-grainedvehicleclassificationwithunsupervisedpartsco-occurrencelearning": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Fine-Grained Vehicle Classification with Unsupervised Parts Co-occurrence Learning",
    "authors": [
      "Sara Elkerdawy",
      "Nilanjan Ray",
      "Hong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Elkerdawy_Fine-Grained_Vehicle_Classification_with_Unsupervised_Parts_Co-occurrence_Learning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Elkerdawy_Fine-Grained_Vehicle_Classification_with_Unsupervised_Parts_Co-occurrence_Learning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Vehicle fine-grained classification is a challenging research problem with little attention in the field. In this paper, we propose a deep network architecture for vehicles fine-grained classification without the need of parts or 3D bounding boxes annotation. Co-occurrence layer (COOC) layer is exploited for unsupervised parts discovery. In addition, a two-step procedure with transfer learning and fine-tuning is utilized. This enables us to better fine-tune models with pre-trained weights on ImageNet in some layers while having random initialization in some others. Our model achieves 86.5% accuracy outperforming the state of the art methods in BoxCars116K by 4%. In addition, we achieve 95.5% and 93.19% on CompCars on both train-test splits, 70-30 and 50-50, outperforming the other methods by 4.5% and 8% respectively."
  },
  "eccv2018_w24_multiplewaveletpoolingforcnns": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Multiple Wavelet Pooling for CNNs",
    "authors": [
      "Aina Ferra",
      "Eduardo Aguilar",
      "Petia Radeva"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Ferra_Multiple_Wavelet_Pooling_for_CNNs_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Ferra_Multiple_Wavelet_Pooling_for_CNNs_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Pooling layers are an essential part of any Convolutional Neural Network. The most popular pooling methods, as max pooling or average pooling, are based on a neighborhood approach that can be too simple and easily introduce visual distortion. To tackle these problems, recently a pooling method based on Haar wavelet transform was proposed. Following the same line of research, in this work, we explore the use of more sophisticated wavelet transforms (Coiflet, Daubechies) to perform the pooling. Additionally, considering that wavelets work similarly to filters, we propose a new pooling method for Convolutional Neural Network that combines multiple wavelet transforms. The results achieved demonstrate the benefits of our approach, improving the performance on different public object recognition datasets."
  },
  "eccv2018_w24_automatedfacialwrinklesannotator": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Automated Facial Wrinkles Annotator",
    "authors": [
      "Moi Hoon Yap",
      "Jhan Alarifi",
      "Choon-Ching Ng",
      "Nazre Batool",
      "Kevin Walker"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Yap_Automated_Facial_Wrinkles_Annotator_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Yap_Automated_Facial_Wrinkles_Annotator_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents an automated facial wrinkles annotator for coarse wrinkles, fine wrinkles and wrinkle depth map extraction. First we extended Hybrid Hessian Filter by introducing a multi-scale filter to isolate the coarse wrinkles from fine wrinkles. Then we generate a wrinkle probablisitic map. When evaluated on 20 high resolution full face images (10 from our in-house dataset and 10 from FERET dataset), we achieved good accuracy when the result of coarse wrinkles was validated with manual annotation. Furthermore, we visually illustrate the ability of the annotator in detecting fine wrinkles. We advances the field by automate the localisation of the fine wrinkles, which might not be possible to annotate manually. Our automated facial wrinkles annotator will be beneficial to large-scale data annotation and cosmetic applications."
  },
  "eccv2018_w24_deeplearningofappearancemodelsforonlineobjecttracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Deep Learning of Appearance Models for Online Object Tracking",
    "authors": [
      "Mengyao Zhai",
      "Lei Chen",
      "Greg Mori",
      "Mehrsan Javan Roshtkhari"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Zhai_Deep_Learning_of_Appearance_Models_for_Online_Object_Tracking_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Zhai_Deep_Learning_of_Appearance_Models_for_Online_Object_Tracking_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper introduces a deep learning based approach for vision based single target tracking. We address this problem by proposing a network architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability distributions of the positive and negative examples. An online fine-tuning step is carried out at every frame to learn the appearance of the target. The tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results."
  },
  "eccv2018_w24_towardscycle-consistentmodelsfortextandimageretrieval": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Towards Cycle-Consistent Models for Text and Image Retrieval",
    "authors": [
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Hamed R. Tavakoli",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Cornia_Towards_Cycle-Consistent_Models_for_Text_and_Image_Retrieval_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Cornia_Towards_Cycle-Consistent_Models_for_Text_and_Image_Retrieval_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Cross-modal retrieval has been recently becoming an hotspot research, thanks to the development of deeply-learnable architectures. Such architectures generally learn a joint multi-modal embedding space in which text and images could be projected and compared. Here we investigate a different approach, and reformulate the problem of crossmodal retrieval as that of learning a translation between the textual and visual domain. In particular, we propose an end-to-end trainable model which can translate text into image features and vice versa, and regularizes this mapping with a cycle-consistency criterion. Preliminary experimental evaluations show promising results with respect to ordinary visual-semantic models."
  },
  "eccv2018_w24_fromattribute-labelstofacesfacegenerationusingaconditionalgenerativeadversarialnetwork": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "From attribute-labels to faces: face generation using a conditional generative adversarial network",
    "authors": [
      "Yaohui Wang",
      "Antitza Dantcheva",
      "Francois Bremond"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Wang_From_attribute-labels_to_faces_face_generation_using_a_conditional_generative_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Wang_From_attribute-labels_to_faces_face_generation_using_a_conditional_generative_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Facial attributes are instrumental in semantically characterizing faces. Automated classification of such attributes (i.e., age, gender, ethnicity) has been a well studied topic. We here seek to explore the inverse problem, namely given attribute-labels the generation of attributeassociated faces. The interest in this topic is fueled by related applications in law enforcement and entertainment. In this work, we propose two models for attribute-label based facial image and video generation incorporating 2D and 3D deep conditional generative adversarial networks (DCGAN). The attribute-labels serve as a tool to determine the specific representations of generated images and videos. While these are early results, our findings indicate the methods\u00e2\u0080\u0099 ability to generate realistic faces from attribute labels."
  },
  "eccv2018_w24_optimizingbodyregionclassificationwithdeepconvolutionalactivationfeatures": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Optimizing Body Region Classification With Deep Convolutional Activation Features",
    "authors": [
      "Obioma Pelka",
      "Felix Nensa",
      "Christoph M. Friedrich"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Pelka_Optimizing_Body_Region_Classification_With_Deep_Convolutional_Activation_Features_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Pelka_Optimizing_Body_Region_Classification_With_Deep_Convolutional_Activation_Features_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The goal of this work is to automatically apply generated image keywords as text representations, to optimize medical image classification accuracies of body regions. To create a keyword generative model, a Long Short-Term Memory (LSTM) based Recurrent Neural Network (RNN) is adopted, which is trained with preprocessed biomedical image captions as text representation and visual features extracted using Convolutional Neural Networks (CNN). For image representation, deep convolutional activation features and Bag-of-Keypoints (BoK) features are extracted for each radiograph and combined with the automatically generated keywords. Random Forest models and Support Vector Machines are trained with these multimodal image representations, as well as just visual representation, to predict body regions. Adopting multimodal image features proves to be the better approach, as the prediction accuracy for body regions is increased."
  },
  "eccv2018_w24_efficientinteractivemulti-objectsegmentationinmedicalimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Efficient Interactive Multi-Object Segmentation in Medical Images",
    "authors": [
      "Leissi Margarita Castaneda Leon",
      "Paulo Andre Vechiatto de Miranda"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Leon_Efficient_Interactive_Multi-Object_Segmentation_in_Medical_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Leon_Efficient_Interactive_Multi-Object_Segmentation_in_Medical_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In medical image segmentation, it is common to have several complex objects that are difficult to detect with simple models without user interaction. Hence, interactive graph-based methods are commonly used in this task, where the image is modeled as a connected graph, since graphs can naturally represent the objects and their relationships. In this work, we propose an efficient method for the multiple object segmentation of medical images. For each object, the method constructs an associated weighted digraph of superpixels, attending its individual high-level priors. Then, all individual digraphs are integrated into a hierarchical graph, considering structural relations of inclusion and exclusion. Finally, a single energy optimization is performed in the hierarchical weighted digraph satisfying all the constraints and leading to globally optimal results. The experimental evaluation on 2D medical images indicates promising results comparable to the state-of-the-art methods, with low computational complexity."
  },
  "eccv2018_w24_cross-modalembeddingsforvideoandaudioretrieval": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Cross-modal Embeddings for Video and Audio Retrieval",
    "authors": [
      "Didac Suris",
      "Amanda Duarte",
      "Amaia Salvador",
      "Jordi Torres",
      "Xavier Giro-i-Nieto"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Suris_Cross-modal_Embeddings_for_Video_and_Audio_Retrieval_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Suris_Cross-modal_Embeddings_for_Video_and_Audio_Retrieval_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this work, we explore the multi-modal information provided by the Youtube-8M dataset by projecting the audio and visual features into a common feature space, to obtain joint audio-visual embeddings. These links are used to retrieve audio samples that fit well to a given silent video, and also to retrieve images that match a given query audio. The results in terms of Recall@K obtained over a subset of YouTube-8M videos show the potential of this unsupervised approach for cross-modal feature learning."
  },
  "eccv2018_w24_understandingcenterlossbasednetworkforimageretrievalwithfewtrainingdata": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Understanding Center Loss Based Network for Image Retrieval with Few Training Data",
    "authors": [
      "Pallabi Ghosh",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Ghosh_Understanding_Center_Loss_Based_Network_for_Image_Retrieval_with_Few_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Ghosh_Understanding_Center_Loss_Based_Network_for_Image_Retrieval_with_Few_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Performance of convolutional neural network based image retrieval depends on the characteristics and statistics of the data being used for training. We show that for training datasets with a large number of classes but small number of images per class, the combination of crossentropy loss and center loss works better than either of the losses alone. While cross-entropy loss tries to minimize misclassification of data, center loss minimizes the embedding space distance of each point in a class to its center, bringing together data-points belonging to the same class."
  },
  "eccv2018_w24_end-to-endtrainedcnnencoder-decodernetworksforimagesteganography": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "End-to-End Trained CNN Encoder-Decoder Networks For Image Steganography",
    "authors": [
      "Atique ur Rehman",
      "Rafia Rahim",
      "Shahroz Nadeem",
      "Sibt ul Hussain"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Rehman_End-to-End_Trained_CNN_Encoder-Decoder_Networks_For_Image_Steganography_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Rehman_End-to-End_Trained_CNN_Encoder-Decoder_Networks_For_Image_Steganography_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " All the existing image steganography methods use manually crafted features to hide binary payloads into cover images. This leads to small payload capacity and image distortion. Here we propose a convolutional neural network based encoder-decoder architecture for embedding of images as payload. To this end, we make following three major contributions: (i) we propose a deep learning based generic encoder-decoder architecture for image steganography; (ii) we introduce a new loss function that ensures joint end-to-end training of encoder-decoder networks; (iii) we perform extensive empirical evaluation of proposed architecture on a range of challenging publicly available datasets (MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art payload capacity at high PSNR and SSIM values."
  },
  "eccv2018_w24_cancelableknuckletemplategenerationbasedonlbp-cnn": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "Cancelable knuckle template generation based on LBP-CNN",
    "authors": [
      "Avantika Singh",
      "Shreya Hasmukh Patel",
      "Aditya Nigam"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Singh_Cancelable_knuckle_template_generation_based_on_LBP-CNN_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Singh_Cancelable_knuckle_template_generation_based_on_LBP-CNN_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Security is a prime issue whenever biometric templates are stored in centralized databases. Templates are highly susceptible to varied security and privacy attacks. Unlike passwords, biometric traits are permanently unrecoverable if lost once. In this paper efforts have been made to generate cancelable knuckle print templates. To the best of our knowledge, this is the first attempt for generating secure template for this biometric-trait. Here for learning feature representation of a biometric sample, local binary pattern based CNN is used. The experimental results are evaluated on PolyU FKP knuckle database and demonstrate high performance. The proposed protected template is resilient to various privacy attacks as well as it satisfies one important criteria of cancelable biometrics i.e. revocability."
  },
  "eccv2018_w24_a2.5ddeeplearning-basedapproachforprostatecancerdetectionont2-weightedmagneticresonanceimaging": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "A 2.5D Deep Learning-based Approach For Prostate Cancer Detection on T2-weighted Magnetic Resonance Imaging",
    "authors": [
      "Ruba Alkadi",
      "Ayman El-Baz",
      "Fatma Taher",
      "Naoufel Werghi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Alkadi_A_2.5D_Deep_Learning-based_Approach_For_Prostate_Cancer_Detection_on_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Alkadi_A_2.5D_Deep_Learning-based_Approach_For_Prostate_Cancer_Detection_on_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we propose a fully automatic magnetic resonance image (MRI)-based computer aided diagnosis (CAD) system which simultaneously performs both prostate segmentation and prostate cancer diagnosis. The system utilizes a deep-learning approach to extract high-level features from raw T2-weighted MR volumes. Features are then remapped to the original input to assign a predicted label to each pixel. In the same context, we propose a 2.5D approach which exploits 3D spatial information without a compromise in computational cost. The system is evaluated on a public dataset. Preliminary results demonstrate that our approach outperforms current state-of-the-art in both prostate segmentation and cancer diagnosis."
  },
  "eccv2018_w24_greenwarpsatwo-stagewarpingmodelforstitchingimagesusingdiffeomorphicmeshesandgreencoordinates": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Women in Computer Vision Workshop",
    "title": "GreenWarps: A Two-Stage Warping Model for Stitching Images using Diffeomorphic Meshes and Green Coordinates",
    "authors": [
      "Geethu Miriam Jacob",
      "Sukhendu Das"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w24/html/Jacob_GreenWarps_A_Two-Stage_Warping_Model_for_Stitching_Images_using_Diffeomorphic_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11132/Jacob_GreenWarps_A_Two-Stage_Warping_Model_for_Stitching_Images_using_Diffeomorphic_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Image Stitching is a hard task to solve in the presence of large parallax in the images. Specifically, for a sequence of frames from unconstrained videos which are considerably shaky, recent works fail to align such a sequence of images accurately. The proposed method \"GreenWarps\" aims to accurately align frames/images with large parallax. The method consists of two novel stages, namely, Prewarping and Diffeomorphic Mesh warping. The first stage warps unaligned image to the reference image using Green Coordinates. The second stage of the model refines the alignment by using a demon-based diffeomorphic warping method for mesh deformation termed \"DiffeoMeshes\".The warping is performed using Green Coordinates in both the stages without the assumption of any motion model. The combination of the two stages provide accurate alignment of the images. Experiments were performed on two standard image stitching datasets and one dataset consisting of images created from unconstrained videos. The results show superior performance of our method compared to the state-of-the-art methods."
  },
  "eccv2018_w25_multi--scalerecursiveandperception--distortioncontrollableimagesuper--resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Multi--Scale Recursive and Perception--Distortion Controllable Image Super--Resolution",
    "authors": [
      "Pablo Navarrete Michelini",
      "Dan Zhu",
      "Hanwen Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Michelini_Multi--Scale_Recursive_and_Perception--Distortion_Controllable_Image_Super--Resolution_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Michelini_Multi--Scale_Recursive_and_Perception--Distortion_Controllable_Image_Super--Resolution_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We describe our solution for the PIRM Super\u00e2\u0080\u0093Resolution Challenge 2018 where we achieved the 2nd best perceptual quality for average RMSE \u00f4\u008f\u00b0\u0080 16, 5th best for RMSE \u00f4\u008f\u00b0\u0080 12.5, and 7th best for RMSE \u00f4\u008f\u00b0\u0080 11.5. We modify a recently proposed Multi\u00e2\u0080\u0093Grid Back\u00e2\u0080\u0093 Projection (MGBP) architecture to work as a generative system with an input parameter that can control the amount of artificial details in the output. We propose a discriminator for adversarial training with the following novel properties: it is multi\u00e2\u0080\u0093scale that resembles a progressive\u00e2\u0080\u0093 GAN; it is recursive that balances the architecture of the generator; and it includes a new layer to capture significant statistics of natural images. Finally, we propose a training strategy that avoids conflicts between reconstruction and perceptual losses. Our configuration uses only 281k parameters and upscales each image of the competition in 0.2s in average."
  },
  "eccv2018_w25_bi-gans-stforperceptualimagesuper-resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Bi-GANs-ST for Perceptual Image Super-resolution",
    "authors": [
      "Xiaotong Luo",
      "Rong Chen",
      "Yuan Xie",
      "Yanyun Qu",
      "Cuihua Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Luo_Bi-GANs-ST_for_Perceptual_Image_Super-resolution_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Luo_Bi-GANs-ST_for_Perceptual_Image_Super-resolution_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Image quality measurement is a critical problem for image super-resolution (SR) algorithms. Usually, they are evaluated by some well-known objective metrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results in accordance with the perception of human being. Recently, a more reasonable perception measurement has been proposed in [1], which is also adopted by the PIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a high-quality SR result which balances between the two indices, i.e., the perception index and root-mean-square error (RMSE). To do so, we design a new deep SR framework, dubbed Bi-GANs-ST, by integrating two complementary generative adversarial networks (GAN) branches. One is memory residual SRGAN (MR-SRGAN), which emphasizes on improving the objective performance, such as reducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which obtains the result that favors better subjective perception via a two-stage adversarial training mechanism. Then, to produce final result with excellent perception scores and RMSE, we use soft-thresholding method to merge the results generated by the two GANs. Our method performs well on the perceptual image super-resolution task of the PIRM 2018 challenge. Experimental results on five benchmarks show that our proposal achieves highly competent performance compared with other state-of-the-art methods."
  },
  "eccv2018_w25_multi-modalspectralimagesuper-resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Multi-Modal Spectral Image Super-Resolution",
    "authors": [
      "Fayez Lahoud",
      "Ruofan Zhou",
      "Sabine Susstrunk"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Lahoud_Multi-Modal_Spectral_Image_Super-Resolution_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Lahoud_Multi-Modal_Spectral_Image_Super-Resolution_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recent advances have shown the great power of deep convolutional neural networks (CNN) to learn the relationship between low and high-resolution image patches. However, these methods only take a single-scale image as input and require large amount of data to train without the risk of overfitting. In this paper, we tackle the problem of multi-modal spectral image super-resolution while constraining ourselves to a small dataset. We propose the use of different modalities to improve the performance of neural networks on the spectral superresolution problem. First, we use multiple downscaled versions of the same image to infer a better high-resolution image for training, we refer to these inputs as a multi-scale modality. Furthermore, color images are usually taken at a higher resolution than spectral images, so we make use of color images as another modality to improve the super-resolution network. By combining both modalities, we build a pipeline that learns to super-resolve using multi-scale spectral inputs guided by a color image. Finally, we validate our method and show that it is economic in terms of parameters and computation time, while still producing state-of-the-art results."
  },
  "eccv2018_w25_generativeadversarialnetwork-basedimagesuper-resolutionusingperceptualcontentlosses": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Generative Adversarial Network-based Image Super-Resolution using Perceptual Content Losses",
    "authors": [
      "Manri Cheon",
      "Jun-Hyuk Kim",
      "Jun-Ho Choi",
      "Jong-Seok Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Cheon_Generative_Adversarial_Network-based_Image_Super-Resolution_using_Perceptual_Content_Losses_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Cheon_Generative_Adversarial_Network-based_Image_Super-Resolution_using_Perceptual_Content_Losses_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we propose a deep generative adversarial network for super-resolution considering the trade-off between perception and distortion. Based on good performance of a recently developed model for super-resolution, i.e., deep residual network using enhanced upscale modules (EUSR) [9], the proposed model is trained to improve perceptual performance with only slight increase of distortion. For this purpose, together with the conventional content loss, i.e., reconstruction loss such as L1 or L2, we consider additional losses in the training phase, which are the discrete cosine transform coefficients loss and differential content loss. These consider perceptual part in the content loss, i.e., consideration of proper high frequency components is helpful for the trade-off problem in super-resolution. The experimental results show that our proposed model has good performance for both perception and distortion, and is effective in perceptual super-resolution applications."
  },
  "eccv2018_w25_esrganenhancedsuper-resolutiongenerativeadversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks",
    "authors": [
      "Xintao Wang",
      "Ke Yu",
      "Shixiang Wu",
      "Jinjin Gu",
      "Yihao Liu",
      "Chao Dong",
      "Yu Qiao",
      "Chen Change Loy"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN \u00e2\u0080\u0093 network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN."
  },
  "eccv2018_w25_theunreasonableeffectivenessoftexturetransferforsingleimagesuper-resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "The Unreasonable Effectiveness of Texture Transfer for Single Image Super-resolution",
    "authors": [
      "Muhammad Waleed Gondal",
      "Bernhard Scholkopf",
      "Michael Hirsch"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Gondal_The_Unreasonable_Effectiveness_of_Texture_Transfer_for_Single_Image_Super-resolution_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Gondal_The_Unreasonable_Effectiveness_of_Texture_Transfer_for_Single_Image_Super-resolution_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " While implicit generative models such as GANs have shown impressive results in high quality image reconstruction and manipulation using a combination of various losses, we consider a simpler approach leading to surprisingly strong results. We show that texture loss [1] alone allows the generation of perceptually high quality images. We provide a better understanding of texture constraining mechanism and develop a novel semantically guided texture constraining method for further improvement. Using a recently developed perceptual metric employing \"deep features\" and termed LPIPS [2], the method obtains state-of-theart results. Moreover, we show that a texture representation of those deep features better capture the perceptual quality of an image than the original deep features. Using texture information, off-the-shelf deep classification networks (without training) perform as well as the best performing (tuned and calibrated) LPIPS metrics."
  },
  "eccv2018_w25_perception-enhancedimagesuper-resolutionviarelativisticgenerativeadversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Perception-Enhanced Image Super-Resolution via Relativistic Generative Adversarial Networks",
    "authors": [
      "Thang Vu",
      "Tung M. Luu",
      "Chang D. Yoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Vu_Perception-Enhanced_Image_Super-Resolution_via_Relativistic_Generative_Adversarial_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Vu_Perception-Enhanced_Image_Super-Resolution_via_Relativistic_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper considers a deep Generative Adversarial Networks (GAN) based method referred to as the Perception-Enhanced Super-Resolution (PESR) for Single Image Super Resolution (SISR) that enhances the perceptual quality of the reconstructed images by considering the following three issues: (1) ease GAN training by replacing an absolute with a relativistic discriminator, (2) include in the loss function a mechanism to emphasize difficult training samples which are generally rich in texture and (3) provide a flexible quality control scheme at test time to trade-off between perception and fidelity. Based on extensive experiments on six benchmark datasets, PESR outperforms recent state-of-the-art SISR methods in terms of perceptual quality. The code is available at https://github.com/thangvubk/PESR."
  },
  "eccv2018_w25_analyzingperception-distortiontradeoffusingenhancedperceptualsuper-resolutionnetwork": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network",
    "authors": [
      "Subeesh Vasu",
      "Nimisha Thekke Madam",
      "A. N. Rajagopalan"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Vasu_Analyzing_Perception-Distortion_Tradeoff_using_Enhanced_Perceptual_Super-resolution_Network_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Vasu_Analyzing_Perception-Distortion_Tradeoff_using_Enhanced_Perceptual_Super-resolution_Network_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Convolutional neural network (CNN) based methods have recently achieved great success for image super-resolution (SR). However, most deep CNN based SR models attempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while resulting in poor quantified perceptual quality (e.g. human opinion score, no-reference quality measures such as NIQE). Few works have attempted to improve the perceptual quality at the cost of performance reduction in distortion measures. A very recent study has revealed that distortion and perceptual quality are at odds with each other and there is always a trade-off between the two. Often the restoration algorithms that are superior in terms of perceptual quality, are inferior in terms of distortion measures. Our work attempts to analyze the trade-off between distortion and perceptual quality for the problem of single image SR. To this end, we use the well-known SR architectureenhanced deep super-resolution (EDSR) network and show that it can be adapted to achieve better perceptual quality for a specific range of the distortion measure. While the original network of EDSR was trained to minimize the error defined based on perpixel accuracy alone, we train our network using a generative adversarial network framework with EDSR as the generator module. Our proposed network, called enhanced perceptual super-resolution network (EPSR), is trained with a combination of mean squared error loss, perceptual loss, and adversarial loss. Our experiments reveal that EPSR achieves the state-of-the-art trade-off between distortion and perceptual quality while the existing methods perform well in either of these measures alone."
  },
  "eccv2018_w25_scale-recurrentmulti-residualdensenetworkforimagesuper-resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Scale-Recurrent Multi-Residual Dense Network for Image Super-Resolution",
    "authors": [
      "Kuldeep Purohit",
      "Srimanta Mandal",
      "A. N. Rajagopalan"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Purohit_Scale-Recurrent_Multi-Residual_Dense_Network_for_Image_Super-Resolution_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Purohit_Scale-Recurrent_Multi-Residual_Dense_Network_for_Image_Super-Resolution_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Recent advances in the design of convolutional neural network (CNN) have yielded significant improvements in the performance of image super-resolution (SR). The boost in performance can be attributed to the presence of residual or dense connections within the intermediate layers of these networks. The efficient combination of such connections can reduce the number of parameters drastically while maintaining the restoration quality. In this paper, we propose a scale recurrent SR architecture built upon units containing series of dense connections within a residual block (Residual Dense Blocks (RDBs)) that allow extraction of abundant local features from the image. Our scale recurrent design delivers competitive performance for higher scale factors while being parametrically more efficient as compared to current state-of-the-art approaches. To further improve the performance of our network, we employ multiple residual connections in intermediate layers (referred to as Multi-Residual Dense Blocks), which improves gradient propagation in existing layers. Recent works have discovered that conventional loss functions can guide a network to produce results which have high PSNRs but are perceptually inferior. We mitigate this issue by utilizing a Generative Adversarial Network (GAN) based framework and deep feature (VGG) losses to train our network. We experimentally demonstrate that different weighted combinations of the VGG loss and the adversarial loss enable our network outputs to traverse along the perception-distortion curve. The proposed networks perform favorably against existing methods, both perceptually and objectively (PSNR-based) with fewer parameters."
  },
  "eccv2018_w25_deepnetworksforimage-to-imagetranslationwithmuxanddemuxlayers": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Deep Networks for Image-to-Image Translation with Mux and Demux Layers",
    "authors": [
      "Hanwen Liu",
      "Pablo Navarrete Michelini",
      "Dan Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Liu_Deep_Networks_for_Image-to-Image_Translation_with_Mux_and_Demux_Layers_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Liu_Deep_Networks_for_Image-to-Image_Translation_with_Mux_and_Demux_Layers_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Imageprocessingmethodsusingdeepconvolutionalnetworks have achieved great successes on quantitative and qualitative assessments in many tasks, such as super\u00e2\u0080\u0093resolution, style transfer and and enhancement. Most of these solutions use many layers, many filters and complex architectures. It is difficult to implement them on mobile devices, e.g. smart phones, because of the limited resources. Many applications need to deploy these methods on mobile devices. But it is difficult because of limited resources. In this paper we present a lightweight end\u00e2\u0080\u0093to\u00e2\u0080\u0093end deep learning approach for image enhancement. To improve the performance, we present mux layer and demux layers, which could perform up\u00e2\u0080\u0093sampling and down\u00e2\u0080\u0093sampling by shuffling the pixels without losing any information of feature maps. For further higher performance, denseblocks are used in the models. To ensure the consistency of the output and input, we use weighted L1 loss to increase PSNR. To improve image quality, we use adversarial loss, contextual loss and perceptual loss as parts of the objective functions during training. And NIQE is used for validation to get the best parameters for perceptual quality. Experiments show that, compared to the state\u00e2\u0080\u0093of\u00e2\u0080\u0093the\u00e2\u0080\u0093art, our method could improve both the quantitative and qualitative assessments, as well as the performance. With this system, we get the third place in PIRM Enhancement\u00e2\u0080\u0093 On\u00e2\u0080\u0093Smartphones Challenge 2018(PIRM\u00e2\u0080\u0093EoS Challenge 2018)."
  },
  "eccv2018_w25_carnconvolutionalanchoredregressionnetworkforfastandaccuratesingleimagesuper-resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "CARN: Convolutional Anchored Regression Network for Fast and Accurate Single Image Super-Resolution",
    "authors": [
      "Yawei Li",
      "Eirikur Agustsson",
      "Shuhang Gu",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Li_CARN_Convolutional_Anchored_Regression_Network_for_Fast_and_Accurate_Single_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Li_CARN_Convolutional_Anchored_Regression_Network_for_Fast_and_Accurate_Single_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Althoughtheaccuracyofsuper-resolution(SR)methodsbased on convolutional neural networks (CNN) soars high, the complexity and computation also explode with the increased depth and width of the network. Thus, we propose the convolutional anchored regression network (CARN) for fast and accurate single image super-resolution (SISR). Inspired by locally linear regression methods (A+ and ARN), the new architecture consists of regression blocks that map input features from one feature space to another. Different from A+ and ARN, CARN is no longer relying on or limited by hand-crafted features. Instead, it is an end-to-end design where all the operations are converted to convolutions so that the key concepts, i.e., features, anchors, and regressors, are learned jointly. The experiments show that CARN achieves the best speed and accuracy trade-off among the SR methods. The code is available at https://github.com/ofsoundof/CARN."
  },
  "eccv2018_w25_multipleconnectedresidualnetworkforimageenhancementonsmartphones": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Multiple Connected Residual Network for Image Enhancement on Smartphones",
    "authors": [
      "Jie Liu",
      "Cheolkon Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Liu_Multiple_Connected_Residual_Network_for_Image_Enhancement_on_Smartphones_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Liu_Multiple_Connected_Residual_Network_for_Image_Enhancement_on_Smartphones_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Image enhancement on smartphones needs rapid processing speed with comparable performance. Recently, convolutional neural networks (CNNs) have achieved presentable performance in image processing tasks such as image super-resolution and enhancement. In this paper, we propose a lightweight generator for image enhancement based on CNN to keep a balance between quality and speed, called multi-connected residual network (MCRN). The proposed network consists of one discriminator and one generator. The generator is a two-stage network: 1) The first stage extracts structural features; 2) the second stage focuses on enhancing perceptual visual quality. By utilizing the style of multiple connections, we achieve good performance in image enhancement while making our network converge fast. Experimental results demonstrate that the proposed method outperforms the state-of-the-art approaches in terms of the perceptual quality and runtime. The code will be available at https://github.com/JieLiu95/MCRN."
  },
  "eccv2018_w25_perception-preservingconvolutionalnetworksforimageenhancementonsmartphones": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Perception-Preserving Convolutional Networks for Image Enhancement on Smartphones",
    "authors": [
      "Zheng Hui",
      "Xiumei Wang",
      "Lirui Deng",
      "Xinbo Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Hui_Perception-Preserving_Convolutional_Networks_for_Image_Enhancement_on_Smartphones_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Hui_Perception-Preserving_Convolutional_Networks_for_Image_Enhancement_on_Smartphones_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Although the configuration of smartphone cameras is getting better and better, the quality of smartphone photos still cannot match DSLR camera photos due to the limitation of physical space, hardware and cost. In this work, we present a fast and accurate image enhancement approach based on generative adversarial nets, which elevates the quality of photos on smartphones. We propose the lightweight local residual convolutional network to learn the mapping between ordinary photos and DSLR-quality images. To make the generated images look real, we introduce the perception-preserving measurement error, which comprises content, color, and adversarial losses. Especially, the content loss is constituted of contextual and SSIM losses, which maintains the natural internal statistics and the structure of images. In addition, we introduce the knowledge transfer strategy to ensure the high performance of the proposed network. The experiments demonstrate that our proposed method produces better results compared with the state-of-the-art approaches, both qualitatively and quantitatively. The code is available at https://github.com/Zheng222/PPCN."
  },
  "eccv2018_w25_deepresidualattentionnetworkforspectralimagesuper-resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Deep Residual Attention Network for Spectral Image Super-Resolution",
    "authors": [
      "Zhan Shi",
      "Chang Chen",
      "Zhiwei Xiong",
      "Dong Liu",
      "Zheng-Jun Zha",
      "Feng Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Shi_Deep_Residual_Attention_Network_for_Spectral_Image_Super-Resolution_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Shi_Deep_Residual_Attention_Network_for_Spectral_Image_Super-Resolution_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Spectral imaging sensors often suffer from low spatial resolution, as there exists an essential tradeoff between the spectral and spatial resolutions that can be simultaneously achieved, especially when the temporal resolution needs to be retained. In this paper, we propose a novel deep residual attention network for the spatial super-resolution (SR) of spectral images. The proposed method extends the classic residual network by 1) directly using the 3D low-resolution (LR) spectral image as input instead of upsampling the 2D bandwise images separately, and 2) integrating the channel attention mechanism into the residual network. These two operations fully exploit the correlations across both the spectral and spatial dimensions and greatly promote the performance of spectral image SR. In addition, for the scenario when stereo pairs of LR spectral and high-resolution (HR) RGB measurements are available, we design a fusion framework based on the proposed network. The spatial resolution of the spectral input is enhanced in one branch, while the spectral resolution of the RGB input is enhanced in the other. These two branches are then fused together through the attention mechanism again to reconstruct the final HR spectral image, which achieves further improvement compared to using the single LR spectral input. Experimental results demonstrate the superiority of the proposed method over plain residual networks, and our method is one of the winning solutions in the PIRM 2018 Spectral Super-resolution Challenge."
  },
  "eccv2018_w25_rangescalingglobalu-netforperceptualimageenhancementonmobiledevices": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices",
    "authors": [
      "Jie Huang",
      "Pengfei Zhu",
      "Mingrui Geng",
      "Jiewen Ran",
      "Xingguang Zhou",
      "Chen Xing",
      "Pengfei Wan",
      "Xiangyang Ji"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Huang_Range_Scaling_Global_U-Net_for_Perceptual_Image_Enhancement_on_Mobile_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Huang_Range_Scaling_Global_U-Net_for_Perceptual_Image_Enhancement_on_Mobile_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Perceptual image enhancement on mobile devices\u00e2\u0080\u0094smart phones in particular\u00e2\u0080\u0094has drawn increasing industrial efforts and academic interests recently. Compared to digital single-lens reflex (DSLR) cameras, cameras on smart phones typically capture lower-quality images due to various hardware constraints. Without additional information, it is a challenging task to enhance the perceptual quality of a single image especially when the computation has to be done on mobile devices. In this paper we present a novel deep learning based approach\u00e2\u0080\u0094the Range Scaling Global U-Net (RSGUNet)\u00e2\u0080\u0094for perceptual image enhancement on mobile devices. Besides the U-Net structure that exploits image features at different resolutions, proposed RSGUNet learns a global feature vector as well as a novel range scaling layer that alleviate artifacts in the enhanced images. Extensive experiments show that the RSGUNet not only outputs enhanced images with higher subjective and objective quality, but also takes less inference time. Our proposal wins the 1st place by a great margin in track B of the Perceptual Image Enhancement on Smartphones Challenge (PRIM2018). Code is available at https://github.com/MTlab/ECCV-PIRM2018."
  },
  "eccv2018_w25_fastandefficientimagequalityenhancementviadesubpixelconvolutionalneuralnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Fast and Efficient Image Quality Enhancement via Desubpixel Convolutional Neural Networks",
    "authors": [
      "Thang Vu",
      "Cao Van Nguyen",
      "Trung X. Pham",
      "Tung M. Luu",
      "Chang D. Yoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper considers a convolutional neural network for image quality enhancement referred to as the fast and efficient quality enhancement (FEQE) that can be trained for either image super-resolution or image enhancement to provide accurate yet visually pleasing images on mobile devices by addressing the following three main issues. First, the considered FEQE performs majority of its computation in a lowresolution space. Second, the number of channels used in the convolutional layers is small which allows FEQE to be very deep. Third, the FEQE performs downsampling referred to as desubpixel that does not lead to loss of information. Experimental results on a number of standard benchmark datasets show significant improvements in image fidelity and reduction in processing time of the proposed FEQE compared to the recent state-of-the-art methods. In the PIRM 2018 challenge, the proposed FEQE placed first on the image super-resolution task for mobile devices. The code is available at https://github.com/thangvubk/FEQE.git."
  },
  "eccv2018_w25_fastperceptualimageenhancement": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "Fast Perceptual Image Enhancement",
    "authors": [
      "Etienne de Stoutz",
      "Andrey Ignatov",
      "Nikolay Kobyshev",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/de_Stoutz_Fast_Perceptual_Image_Enhancement_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/de_Stoutz_Fast_Perceptual_Image_Enhancement_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Thevastmajorityofphotostakentodayarebymobilephones. While their quality is rapidly growing, due to physical limitations and cost constraints the mobile phones cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of Ignatov et al., where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3\u00c3\u0097 on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future."
  },
  "eccv2018_w25_pirm2018challengeonspectralimagesuper-resolutiondatasetandstudy": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "PIRM2018 Challenge on Spectral Image Super-Resolution: Dataset and Study",
    "authors": [
      "Mehrdad Shoeiby",
      "Antonio Robles-Kelly",
      "Ran Wei",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Shoeiby_PIRM2018_Challenge_on_Spectral_Image_Super-Resolution_Dataset_and_Study_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Shoeiby_PIRM2018_Challenge_on_Spectral_Image_Super-Resolution_Dataset_and_Study_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper introduces a newly collected and novel dataset (StereoMSI) for example-based single and colour-guided spectral image super-resolution. The dataset was first released and promoted during the PIRM2018 spectral image super-resolution challenge. To the best of our knowledge, the dataset is the first of its kind, comprising 350 registered colour-spectral image pairs. The dataset has been used for the two tracks of the challenge and, for each of these, we have provided a split into training, validation and testing. This arrangement is a result of the challenge structure and phases, with the first track focusing on examplebased spectral image super-resolution and the second one aiming at exploiting the registered stereo colour imagery to improve the resolution of the spectral images. Each of the tracks and splits has been selected to be consistent across a number of image quality metrics. The dataset is quite general in nature and can be used for a wide variety of applications in addition to the development of spectral image super-resolution methods."
  },
  "eccv2018_w25_aibenchmarkrunningdeepneuralnetworksonandroidsmartphones": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "AI Benchmark: Running Deep Neural Networks on Android Smartphones",
    "authors": [
      "Andrey Ignatov",
      "Radu Timofte",
      "William Chou",
      "Ke Wang",
      "Max Wu",
      "Tim Hartley",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Ignatov_AI_Benchmark_Running_Deep_Neural_Networks_on_Android_Smartphones_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Ignatov_AI_Benchmark_Running_Deep_Neural_Networks_on_Android_Smartphones_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Over the last years, the computational power of mobile devices such as smartphones and tablets has grown dramatically, reaching the level of desktop computers available not long ago. While standard smartphone apps are no longer a problem for them, there is still a group of tasks that can easily challenge even high-end devices, namely running artificial intelligence algorithms. In this paper, we present a study of the current state of deep learning in the Android ecosystem and describe available frameworks, programming models and the limitations of running AI on smartphones. We give an overview of the hardware acceleration resources available on four main mobile chipset platforms: Qualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the real-world performance results of different mobile SoCs collected with AI Benchmark6 that are covering all main existing hardware configurations."
  },
  "eccv2018_w25_pirmchallengeonperceptualimageenhancementonsmartphonesreport": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "PIRM Challenge on Perceptual Image Enhancement on Smartphones: Report",
    "authors": [
      "Andrey Ignatov",
      "Radu Timofte",
      "Thang Van Vu",
      "Tung Minh Luu",
      "Trung X Pham",
      "Cao Van Nguyen",
      "Yongwoo Kim",
      "Jae-Seok Choi",
      "et al."
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Ignatov_PIRM_Challenge_on_Perceptual_Image_Enhancement_on_Smartphones_Report_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Ignatov_PIRM_Challenge_on_Perceptual_Image_Enhancement_on_Smartphones_Report_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper reviews the first challenge on efficient perceptual image enhancement with the focus on deploying deep learning models on smartphones. The challenge consisted of two tracks. In the first one, participants were solving the classical image super-resolution problem with a bicubic downscaling factor of 4. The second track was aimed at real-world photo enhancement, and the goal was to map low-quality photos from the iPhone 3GS device to the same photos captured with a DSLR camera. The target metric used in this challenge combined the runtime, PSNR scores and solutions\u00e2\u0080\u0099 perceptual results measured in the user study. To ensure the efficiency of the submitted models, we additionally measured their runtime and memory requirements on Android smartphones. The proposed solutions significantly improved baseline results defining the state-of-the-art for image enhancement on smartphones."
  },
  "eccv2018_w25_the2018pirmchallengeonperceptualimagesuper-resolution": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "The 2018 PIRM Challenge on Perceptual Image Super-Resolution",
    "authors": [
      "Yochai Blau",
      "Roey Mechrez",
      "Radu Timofte",
      "Tomer Michaeli",
      "Lihi Zelnik-Manor"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Blau_2018_PIRM_Challenge_on_Perceptual_Image_Super-resolution_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Blau_2018_PIRM_Challenge_on_Perceptual_Image_Super-resolution_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper reports on the 2018 PIRM challenge on perceptual super-resolution (SR), held in conjunction with the Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018. In contrast to previous SR challenges, our evaluation methodology jointly quantifies accuracy and perceptual quality, therefore enabling perceptualdriven methods to compete alongside algorithms that target PSNR maximization. Twenty-one participating teams introduced algorithms which well-improved upon the existing state-of-the-art methods in perceptual SR, as confirmed by a human opinion study. We also analyze popular image quality measures and draw conclusions regarding which of them correlates best with human opinion scores. We conclude with an analysis of the current trends in perceptual SR, as reflected from the leading submissions."
  },
  "eccv2018_w25_pirm2018challengeonspectralimagesuper-resolutionmethodsandresults": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Perceptual Image Restoration and Manipulation Workshop and Challenge",
    "title": "PIRM2018 Challenge on Spectral Image Super-Resolution: Methods and Results",
    "authors": [
      "Mehrdad Shoeiby",
      "Antonio Robles-Kelly",
      "Radu Timofte",
      "Ruofan Zhou",
      "Fayez Lahoud",
      "Sabine Susstrunk",
      "Zhiwei Xiong",
      "Zhan Shi",
      "et al."
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w25/html/Shoeiby_PIRM2018_Challenge_on_Spectral_Image_Super-Resolution_Methods_and_Results_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Shoeiby_PIRM2018_Challenge_on_Spectral_Image_Super-Resolution_Methods_and_Results_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we describe the Perceptual Image Restoration and Manipulation (PIRM) workshop challenge on spectral image superresolution, motivate its structure and conclude on results obtained by the participants. The challenge is one of the first of its kind, aiming at leveraging modern machine learning techniques to achieve spectral image super-resolution. It comprises of two tracks. The first of these (Track 1) is about example-based single spectral image super-resolution. The second one (Track 2) is on colour-guided spectral image super-resolution. In this manner, Track 1 focuses on the problem of super-resolving the spatial resolution of spectral images given training pairs of low and high spatial resolution spectral images. Track 2, on the other hand, aims to leverage the inherently higher spatial resolution of colour (RGB) cameras and the link between spectral and trichromatic images of the scene. The challenge in both tracks is then to recover a super-resolved image making use of low-resolution imagery at the input. We also elaborate upon the methods used by the participants, summarise the results and discuss their rankings."
  },
  "eccv2018_w26_mamtransferlearningforfullyautomaticvideoannotationandspecializeddetectorcreation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Egocentric Perception, Interaction and Computing",
    "title": "MAM: Transfer Learning for Fully Automatic Video Annotation and Specialized Detector Creation",
    "authors": [
      "Wolfgang Fuhl",
      "Nora Castner",
      "Lin Zhuang",
      "Markus Holzer",
      "Wolfgang Rosenstiel",
      "Enkelejda Kasneci"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w26/html/Fuhl_MAM_Transfer_Learning_for_Fully_Automatic_Video_Annotation_and_Specialized_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Fuhl_MAM_Transfer_Learning_for_Fully_Automatic_Video_Annotation_and_Specialized_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Accurate point detection on image data is an important task for many applications, such as in robot perception, scene understanding, gaze point regression in eye tracking, head pose estimation, or object outline estimation. In addition, it can be beneficial for various object detection tasks where minimal bounding boxes are searched and the method can be applied to each corner. We propose a novel self training method, Multiple Annotation Maturation (MAM) that enables fully automatic labeling of large amounts of image data. Moreover, MAM produces detectors, which can be used online afterward. We evaluated our algorithm on data from different detection tasks for eye, pupil center (head mounted and remote), and eyelid outline point and compared the performance to the state-of-the-art. The evaluation was done on over 300,000 images, and our method shows outstanding adaptability and robustness. In addition, we contribute a new dataset with more than 16,200 accurate manually-labeled images from the remote eyelid, pupil center, and pupil outline detection. This dataset was recorded in a prototype car interior equipped with all standard tools, posing various challenges to object detection such as reflections, occlusion from steering wheel movement, or large head movements. The data set and library are available for download at http://ti.uni-tuebingen.de/Projekte.1801.0.html."
  },
  "eccv2018_w26_leveraginguncertaintytorethinklossfunctionsandevaluationmeasuresforegocentricactionanticipation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Leveraging Uncertainty to Rethink Loss Functions and Evaluation Measures for Egocentric Action Anticipation",
    "authors": [
      "Antonino Furnari",
      "Sebastiano Battiato",
      "Giovanni Maria Farinella"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w26/html/Furnari_Leveraging_Uncertainty_to_Rethink_Loss_Functions_and_Evaluation_Measures_for_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Furnari_Leveraging_Uncertainty_to_Rethink_Loss_Functions_and_Evaluation_Measures_for_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Current action anticipation approaches often neglect the intrinsic uncertainty of future predictions when loss functions or evaluation measures are designed. The uncertainty of future observations is especially relevant in the context of egocentric visual data, which is naturally exposed to a great deal of variability. Considering the problem of egocentric action anticipation, we investigate how loss functions and evaluation measures can be designed to explicitly take into account the natural multi-modality of future events. In particular, we discuss suitable measures to evaluate egocentric action anticipation and study how loss functions can be defined to incorporate the uncertainty arising from the prediction of future events. Experiments performed on the EPIC-KITCHENS dataset show that the proposed loss function allows improving the results of both egocentric action anticipation and recognition methods."
  },
  "eccv2018_w26_pathganvisualscanpathpredictionwithgenerativeadversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Egocentric Perception, Interaction and Computing",
    "title": "PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks",
    "authors": [
      "Marc Assens",
      "Xavier Giro-i-Nieto",
      "Kevin McGuinness",
      "Noel E. O'Connor"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w26/html/Assens_PathGAN_Visual_Scanpath_Prediction_with_Generative_Adversarial_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Assens_PathGAN_Visual_Scanpath_Prediction_with_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We introduce PathGAN, a deep neural network for visual scanpath prediction trained on adversarial examples. A visual scanpath is defined as the sequence of fixation points over an image defined by a human observer with its gaze. PathGAN is composed of two parts, the generator and the discriminator. Both parts extract features from images using off-the-shelf networks, and train recurrent layers to generate or discriminate scanpaths accordingly. In scanpath prediction, the stochastic nature of the data makes it very difficult to generate realistic predictions using supervised learning strategies, but we adopt adversarial training as a suitable alternative. Our experiments prove how PathGAN improves the state of the art of visual scanpath prediction on the iSUN and Salient360! datasets."
  },
  "eccv2018_w26_macnetmulti-scaleatrousconvolutionnetworksforfoodplacesclassificationinegocentricphoto-streams": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W26",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Egocentric Perception, Interaction and Computing",
    "title": "MACNet: Multi-scale Atrous Convolution Networks for Food Places Classification in Egocentric Photo-streams",
    "authors": [
      "Md. Mostafa Kamal Sarker",
      "Hatem A. Rashwan",
      "Estefania Talavera",
      "Syeda Furruka Banu",
      "Petia Radeva",
      "Domenec Puig"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w26/html/Sarker_MACNet_Multi-scale_Atrous_Convolution_Networks_for_Food_Places_Classification_in_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Sarker_MACNet_Multi-scale_Atrous_Convolution_Networks_for_Food_Places_Classification_in_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " First-person(wearable)cameracontinuallycapturesunscripted interactions of the camera user with objects, people, and scenes reflecting his personal and relational tendencies. One of the preferences of peopleis their interaction with food events. The regulation of food intake and its duration has a great importance to protect against diseases. Consequently, this work aims to develop a smart model that is able to determine the recurrences of a person on food places during a day. This model is based on a deep end-to-end model for automatic food places recognition by analyzing egocentric photo-streams. In this paper, we apply multi-scale Atrous convolution networks to extract the key features related to food places of the input images. The proposed model is evaluated on an in-house private dataset called \"EgoFoodPlaces\". Experimental results shows promising results of food places classification in egocentric photo-streams."
  },
  "eccv2018_w27_visdrone-det2018thevisionmeetsdroneobjectdetectioninimagechallengeresults": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Vision Meets Drone: A Challenge",
    "title": "VisDrone-DET2018: The Vision Meets Drone Object Detection in Image Challenge Results",
    "authors": [
      "Pengfei Zhu",
      "Longyin Wen",
      "Dawei Du",
      "Xiao Bian",
      "Haibin Ling",
      "Qinghua Hu",
      "Qinqin Nie",
      "Hao Cheng",
      "et al."
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w27/html/Zhu_VisDrone-DET2018_The_Vision_Meets_Drone_Object_Detection_in_Image_Challenge_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Zhu_VisDrone-DET2018_The_Vision_Meets_Drone_Object_Detection_in_Image_Challenge_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Object detection is a hot topic with various applications in computer vision, e.g., image understanding, autonomous driving, and video surveillance. Much of the progresses have been driven by the availability of object detection benchmark datasets, including PASCAL VOC, ImageNet, and MS COCO. However, object detection on the drone platform is still a challenging task, due to various factors such as view point change, occlusion, and scales. To narrow the gap between current object detection performance and the real-world requirements, we organized the Vision Meets Drone (VisDrone2018) Object Detection in Image challenge in conjunction with the 15th European Conference on Computer Vision (ECCV 2018). Specifically, we release a large-scale drone-based dataset, including 8,599 images (6,471 for training, 548 for validation, and 1,580 for testing) with rich annotations, including object bounding boxes, object categories, occlusion, truncation ratios, etc. Featuring a diverse real-world scenarios, the dataset was collected using various drone models, in different scenarios (across 14 different cities spanned over thousands of kilometres), and under various weather and lighting conditions. We mainly focus on ten object categories in object detection, i.e., pedestrian, person, car, van, bus, truck, motor, bicycle, awning-tricycle, and tricycle. Some rarely occurring special vehicles (e.g., machineshop truck, forklift truck, and tanker) are ignored in evaluation. The dataset is extremely challenging due to various factors, including large scale and pose variations, occlusion, and clutter background. We present the evaluation protocol of the VisDrone-DET2018 challenge and the comparison results of 38 detectors on the released dataset, which are publicly available on the challenge website: http://www.aiskyeye.com/. We expect the challenge to largely boost the research and development in object detection in images on drone platforms."
  },
  "eccv2018_w27_visdrone-sot2018thevisionmeetsdronesingle-objecttrackingchallengeresults": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Vision Meets Drone: A Challenge",
    "title": "VisDrone-SOT2018: The Vision Meets Drone Single-Object Tracking Challenge Results",
    "authors": [
      "Longyin Wen",
      "Pengfei Zhu",
      "Dawei Du",
      "Xiao Bian",
      "Haibin Ling",
      "Qinghua Hu",
      "Chenfeng Liu",
      "Hao Cheng",
      "et al."
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w27/html/Wen_VisDrone-SOT2018_The_Vision_Meets_Drone_Single-Object_Tracking_Challenge_Results_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Wen_VisDrone-SOT2018_The_Vision_Meets_Drone_Single-Object_Tracking_Challenge_Results_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Single-object tracking, also known as visual tracking, on the drone platform attracts much attention recently with various applications in computer vision, such as filming and surveillance. However, the lack of commonly accepted annotated datasets and standard evaluation platform prevent the developments of algorithms. To address this issue, the Vision Meets Drone Single-Object Tracking (VisDrone-SOT2018) Challenge workshop was organized in conjunction with the 15th European Conference on Computer Vision (ECCV 2018) to track and advance the technologies in such field. Specifically, we collect a dataset, including 132 video sequences divided into three non-overlapping sets, i.e., training (86 sequences with 69, 941 frames), validation (11 sequences with 7, 046 frames), and testing (35 sequences with 29, 367 frames) sets. We provide fully annotated bounding boxes of the targets as well as several useful attributes, e.g., occlusion, background clutter, and camera motion. The tracking targets in these sequences include pedestrians, cars, buses, and animals. The dataset is extremely challenging due to various factors, such as occlusion, large scale, pose variation, and fast motion. We present the evaluation protocol of the VisDrone-SOT2018 challenge and the results of a comparison of 22 trackers on the benchmark dataset, which are publicly available on the challenge website: http://www.aiskyeye.com/. We hope this challenge largely boosts the research and development in single object tracking on drone platforms."
  },
  "eccv2018_w27_visdrone-vdt2018thevisionmeetsdronevideodetectionandtrackingchallengeresults": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Vision Meets Drone: A Challenge",
    "title": "VisDrone-VDT2018: The Vision Meets Drone Video Detection and Tracking Challenge Results",
    "authors": [
      "Pengfei Zhu",
      "Longyin Wen",
      "Dawei Du",
      "Xiao Bian",
      "Haibin Ling",
      "Qinghua Hu",
      "Haotian Wu",
      "Qinqin Nie",
      "et al."
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w27/html/Zhu_VisDrone-VDT2018_The_Vision_Meets_Drone_Video_Detection_and_Tracking_Challenge_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Zhu_VisDrone-VDT2018_The_Vision_Meets_Drone_Video_Detection_and_Tracking_Challenge_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Drones equipped with cameras have been fast deployed to a wide range of applications, such as agriculture, aerial photography, fast delivery, and surveillance. As the core steps in those applications, video object detection and tracking attracts much research effort in recent years. However, the current video object detection and tracking algorithms are not usually optimal for dealing with video sequences captured by drones, due to various challenges, such as viewpoint change and scales. To promote and track the development of the detection and tracking algorithms with drones, we organized the Vision Meets Drone Video Detection and Tracking (VisDrone-VDT2018) challenge, which is a subtrack of the Vision Meets Drone 2018 challenge workshop in conjunction with the 15th European Conference on Computer Vision (ECCV 2018). Specifically, this workshop challenge consists of two tasks, (1) video object detection, and (2) multi-object tracking. We present a large-scale video object detection and tracking dataset, which consists of 79 video clips with about 1.5 million annotated bounding boxes in 33,366 frames. We also provide rich annotations, including object categories, occlusion, and truncation ratios for better data usage. Being the largest such dataset ever published, the challenge enables extensive evaluation, investigation and tracking the progress of object detection and tracking algorithms on the drone platform. We present the evaluation protocol of the VisDrone-VDT2018 challenge and the results of the algorithms on the benchmark dataset, which are publicly available on the challenge website: http://www.aiskyeye.com/. We hope the challenge largely boost the research and development in related fields."
  },
  "eccv2018_w28_anend-to-endtreebasedapproachforinstancesegmentation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 11th Perceptual Organization in Computer Vision Workshop on Action, Perception and Organization",
    "title": "An End-to-end Tree based approach for Instance Segmentation",
    "authors": [
      "KV Manohar",
      "Yusuke Niitani"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w28/html/Manohar_An_End-to-end_Tree_based_approach_for_Instance_Segmentation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Manohar_An_End-to-end_Tree_based_approach_for_Instance_Segmentation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper presents an approach for bottom-up hierarchical instance segmentation. We propose an end-to-end model to estimate energies of regions in an hierarchical region tree. To this end, we introduce a Convolutional Tree-LSTM module to leverage the tree-structured network topology. For constructing the hierarchical region tree, we utilize the accurate boundaries predicted from a pre-trained convolutional oriented boundary network. We evaluate our model on PASCAL VOC 2012 dataset showing that we obtain good trade-off between segmentation accuracy and time taken to process a single image."
  },
  "eccv2018_w28_self-supervisedsegmentationbygroupingoptical-flow": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 11th Perceptual Organization in Computer Vision Workshop on Action, Perception and Organization",
    "title": "Self-Supervised Segmentation by Grouping Optical-Flow",
    "authors": [
      "Aravindh Mahendran",
      "James Thewlis",
      "Andrea Vedaldi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w28/html/Mahendran_Self-Supervised_Segmentation_by_Grouping_Optical-Flow_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Mahendran_Self-Supervised_Segmentation_by_Grouping_Optical-Flow_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose to self-supervise a convolutional neural network operating on images using temporal information from videos. The task is to learn a representation of single images and the supervision for this is obtained by learning to group image pixels in such a way that their collective motion is \"coherent\". This learning by grouping approach is used as a pre-training as well as segmentation strategy. Preliminary results suggest that the segments obtained are reasonable and the representation learned transfers well for classification."
  },
  "eccv2018_w28_motionselectivityofneuronsinself-drivingnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 11th Perceptual Organization in Computer Vision Workshop on Action, Perception and Organization",
    "title": "Motion Selectivity of Neurons in Self-Driving Networks",
    "authors": [
      "Baladitya Yellapragada",
      "Alexander Anderson",
      "Stella Yu",
      "Karl Zipser"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w28/html/Yellapragada_Motion_Selectivity_of_Neurons_in_Self-Driving_Networks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Yellapragada_Motion_Selectivity_of_Neurons_in_Self-Driving_Networks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We investigated if optical flow filters were implicitly learned by a neural network trained to drive a vehicle. The network was not trained to predict optical flow across the frames, but, through a series of controlled experiments, we claim that optical flow filters are present in the network. However, this appears to be only the case for sideways flows more relevant for steering predictions. For motor throttle predictions, the network looks at the variance of the pixels over time rather than computing optical flow. In addition, the filters that are likely used for motor throttle predictions dominate primarily in the middle of the network."
  },
  "eccv2018_w29_removalofvisualdisruptioncausedbyrainusingcycle-consistentgenerativeadversarialnetworks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Removal of Visual Disruption Caused by Rain using Cycle-Consistent Generative Adversarial Networks",
    "authors": [
      "Lai Meng Tang",
      "Li Hong Lim",
      "Paul Siebert"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Tang_Removal_of_Visual_Disruption_Caused_by_Rain_using_Cycle-Consistent_Generative_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Tang_Removal_of_Visual_Disruption_Caused_by_Rain_using_Cycle-Consistent_Generative_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper addresses the problem of removing rain disruption from images for outdoor vision systems. The Cycle-Consistent Generative Adversarial Network (CycleGAN) is proposed as a more promising rain removal algorithm, as compared to the state-of-the-art Image Deraining Conditional Generative Adversarial Network (ID-CGAN). The CycleGAN has an advantage in its ability to learn the underlying relationship between the rain and rain-free domain without the need of paired domain examples. Based on rain physical properties and its various phenomena, five broad categories of real rain distortions are proposed in this paper. For a fair comparison, both networks were trained on the same set of synthesized rain-and-ground-truth image-pairs provided by the ID-CGAN work, and subsequently tested on real rain images which fall broadly under these five categories. The comparison results demonstrated that the CycleGAN is superior in removing real rain distortions."
  },
  "eccv2018_w29_real-timedynamicobjectdetectionforautonomousdrivingusingprior3d-maps": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Real-time Dynamic Object Detection for Autonomous Driving using Prior 3D-Maps",
    "authors": [
      "B Ravi Kiran",
      "Luis Roldao",
      "Benat Irastorza",
      "Renzo Verastegui",
      "Sebastian Suss",
      "Senthil Yogamani",
      "Victor Talpaert",
      "Alexandre Lepoutre",
      "Guillaume Trehard"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Kiran_Real-time_Dynamic_Object_Detection_for_Autonomous_Driving_using_Prior_3D-Maps_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Kiran_Real-time_Dynamic_Object_Detection_for_Autonomous_Driving_using_Prior_3D-Maps_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Lidar has become an essential sensor for autonomous driving as it provides reliable depth estimation. Lidar is also the primary sensor used in building 3D maps which can be used even in the case of low-cost systems which do not use Lidar. Computation on Lidar point clouds is intensive as it requires processing of millions of points per second. Additionally there are many subsequent tasks such as clustering, detection, tracking and classification which makes real-time execution challenging. In this paper, we discuss real-time dynamic object detection algorithms which leverages previously mapped Lidar point clouds to reduce processing. The prior 3D maps provide a static background model and we formulate dynamic object detection as a background subtraction problem. Computation and modeling challenges in the mapping and online execution pipeline are described. We propose a rejection cascade architecture to subtract road regions and other 3D regions separately. We implemented an initial version of our proposed algorithm and evaluated the accuracy on CARLA simulator."
  },
  "eccv2018_w29_learningdrivingbehaviorsforautomatedcarsinunstructuredenvironments": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Learning Driving Behaviors for Automated Cars in Unstructured Environments",
    "authors": [
      "Meha Kaushik",
      "K. Madhava Krishna"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Kaushik_Learning_Driving_Behaviors_for_Automated_Cars_in_Unstructured_Environments_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Kaushik_Learning_Driving_Behaviors_for_Automated_Cars_in_Unstructured_Environments_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The core of Reinforcement learning lies in learning from experiences. The performance of the agent is hugely impacted by the training conditions, reward functions and exploration policies. Deep Deterministic Policy Gradient(DDPG) is a well known approach to solve continuous control problems in RL. We use DDPG with intelligent choice of reward function and exploration policy to learn various driving behaviors(Lanekeeping, Overtaking, Blocking, Defensive, Opportunistic) for a simulated car in unstructured environments. In cluttered scenes, where the opponent agents are not following any driving pattern, it is difficult to anticipate their behavior and henceforth decide our agent\u00e2\u0080\u0099s actions. DDPG enables us to propose a solution which requires only the sensor information at current time step to predict the action to be taken. Our main contribution is generating a behavior based motion model for simulated cars, which plans for every instant."
  },
  "eccv2018_w29_multichannelsemanticsegmentationwithunsuperviseddomainadaptation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Multichannel Semantic Segmentation with Unsupervised Domain Adaptation",
    "authors": [
      "Kohei Watanabe",
      "Kuniaki Saito",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Watanabe_Multichannel_Semantic_Segmentation_with_Unsupervised_Domain_Adaptation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Watanabe_Multichannel_Semantic_Segmentation_with_Unsupervised_Domain_Adaptation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Most contemporary robots have depth sensors, and research on semantic segmentation with RGBD images has shown that depth images boost the accuracy of segmentation. Since it is time-consuming to annotate images with semantic labels per pixel, it would be ideal if we could avoid this laborious work by utilizing an existing dataset or a synthetic dataset which we can generate on our own. Robot motions are often tested in a synthetic environment, where multichannel (e.g. , RGB + depth + instance boundary) images plus their pixel-level semantic labels are available. However, models trained simply on synthetic images tend to demonstrate poor performance on real images. In order to address this, we propose two approaches that can efficiently exploit multichannel inputs combined with an unsupervised domain adaptation (UDA) algorithm. One is a fusion-based approach that uses depth images as inputs. The other is a multitask learning approach that uses depth images as outputs. We demonstrated that the segmentation results were improved by using a multitask learning approach with a post-process and created a benchmark for this task."
  },
  "eccv2018_w29_drivingdatacollectionframeworkusinglowcosthardware": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Driving data collection framework using low cost hardware",
    "authors": [
      "Johnny Jacob",
      "Pankaj Rabha"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Jacob_Driving_data_collection_framework_using_low_cost_hardware_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Jacob_Driving_data_collection_framework_using_low_cost_hardware_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Autonomous driving is driven by data. The availability of large and diverse data set from different geographies can help in maturing Autonomous driving technology faster. It is challenging to build a system to collect driving data which is cost intensive especially in emerging economies. Paradoxically these economies have chaotic driving conditions leading to a valuable data set. To address the issue of cost and scale, we have developed a data collection framework. In this paper, we\u00e2\u0080\u0099ll discuss our motive for the framework, performance bottlenecks, a two stage pipeline design and insights on how to tune the system to get maximum throughput."
  },
  "eccv2018_w29_3dboundingboxesforroadvehiclesaone-stage,localizationprioritizedapproachusingsinglemonocularimages.": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "3D Bounding Boxes for Road Vehicles: A One-Stage, Localization Prioritized Approach using Single Monocular Images.",
    "authors": [
      "Ishan Gupta",
      "Akshay Rangesh",
      "Mohan Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Gupta_3D_Bounding_Boxes_for_Road_Vehicles_A_One-Stage_Localization_Prioritized_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Gupta_3D_Bounding_Boxes_for_Road_Vehicles_A_One-Stage_Localization_Prioritized_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Understanding 3D semantics of the surrounding objects is critically important and a challenging requirement from the safety perspective of autonomous driving. We present a localization prioritized approach for effectively localizing the position of the object in the 3D world and fit a complete 3D box around it. Our method requires a single image and performs both 2D and 3D detection in an end to end fashion. Estimating depth of an object from a monocular image is not as generalizable as pose and dimensions. Hence, we approach this problem by effectively localizing the projection of the center of bottom face of 3D bounding box (CBF) to the image. Later in our post processing stage, we use a look up table based approach to reproject the CBF in the 3D world. This stage is a single time setup and simple enough to be deployed in fixed map communities where we can store complete knowledge about the ground plane. The object\u00e2\u0080\u0099s dimension and pose are predicted in multitask fashion using a shared set of features. Experiments show that our method is able to produce smooth tracks for surround objects and outperforms existing image based approaches in 3D localization."
  },
  "eccv2018_w29_masonamodelagnosticobjectnessframework": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "MASON: A Model AgnoStic ObjectNess Framework",
    "authors": [
      "K J Joseph",
      "Rajiv Chunilal Patel",
      "Amit Srivastava",
      "Uma Gupta",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Joseph_MASON_A_Model_AgnoStic_ObjectNess_Framework_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Joseph_MASON_A_Model_AgnoStic_ObjectNess_Framework_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper proposes a simple, yet very effective method to localize dominant foreground objects in an image, to pixel-level precision. The proposed method \u00e2\u0080\u0098MASON\u00e2\u0080\u0099 (Model-AgnoStic ObjectNess) uses a deep convolutional network to generate category-independent and modelagnostic heat maps for any image. The network is not explicitly trained for the task, and hence, can be used off-the-shelf in tandem with any other network or task. We show that this framework scales to a wide variety of images, and illustrate the effectiveness of MASON in three varied application contexts."
  },
  "eccv2018_w29_objectdetectionat200framespersecond": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Object detection at 200 Frames Per Second",
    "authors": [
      "Rakesh Mehta",
      "Cemalettin Ozturk"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Mehta_Object_detection_at_200_Frames_Per_Second_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Mehta_Object_detection_at_200_Frames_Per_Second_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we propose an efficient and fast object detector which can process hundreds of frames per second. To achieve this goal we investigate three main aspects of the object detection framework: network architecture,loss function and training data (labeled and unlabeled). In order to obtain compact network architecture, we introduce various improvements, based on recent work, to develop an architecture which is computationally light-weight and achieves a reasonable performance. To further improve the performance, while keeping the complexity same, we utilize distillation loss function. Using distillation loss we transfer the knowledge of a more accurate teacher network to proposed light-weight student network.We propose various innovations to make distillation efficient for the proposed one stage detector pipeline: objectness scaled distillation loss, feature map non-maximal suppression and a single unified distillation loss function for detection. Finally, building upon the distillation loss, we explore how much can we push the performance by utilizing the unlabeled data. We train our model with unlabeled data using the soft labels of the teacher network. Our final network consists of 10x fewer parameters than the VGG based object detection network and it achieves a speed of more than 200 FPS and proposed changes improve the detection accuracy by 14 mAP over the baseline on Pascal dataset."
  },
  "eccv2018_w29_motionsegmentationusingspectralclusteringonindianroadscenes": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Motion Segmentation Using Spectral Clustering on Indian Road Scenes",
    "authors": [
      "Mahtab Sandhu",
      "Sarthak Upadhyay",
      "Madhava Krishna",
      "Shanti Medasani"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w29/html/Sandhu_Motion_Segmentation_Using_Spectral_Clustering_on_Indian_Road_Scenes_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Sandhu_Motion_Segmentation_Using_Spectral_Clustering_on_Indian_Road_Scenes_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a novel motion segmentation formulation over spatio-temporal depth images obtained from stereo sequences that segments multiple motion models in the scene in an unsupervised manner . The motion segmentation is obtained at frame rates that compete with the speed of the stereo depth computation. This is possible due to a decoupling framework that first delineates spatial clusters and subsequently assigns motion labels to each of these cluster with analysis of a novel motion graph model. A principled computation of the weights of the motion graph that signifies the relative shear and stretch between possible clusters lends itself to a high fidelity segmentation of the motion models in the scene."
  },
  "eccv2018_w30_everypixelcountsunsupervisedgeometrylearningwithholistic3dmotionunderstanding": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - ApolloScape: Vision-based Navigation for Autonomous Driving",
    "title": "Every Pixel Counts: Unsupervised Geometry Learning with Holistic 3D Motion Understanding",
    "authors": [
      "Zhenheng Yang",
      "Peng Wang",
      "Yang Wang",
      "Wei Xu",
      "Ram Nevatia"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w30/html/Yang_Every_Pixel_Counts_Unsupervised_Geometry_Learning_with_Holistic_3D_Motion_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Yang_Every_Pixel_Counts_Unsupervised_Geometry_Learning_with_Holistic_3D_Motion_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network has made significant process recently. Current state-of-the-art (SOTA) methods, are based on the learning framework of rigid structure-from-motion, where only 3D camera ego motion is modeled for geometry estimation. However, moving objects also exist in many videos, e.g. moving cars in a street scene. In this paper, we tackle such motion by additionally incorporating per-pixel 3D object motion into the learning framework, which provides holistic 3D scene flow understanding and helps single image geometry estimation. Specifically, given two consecutive frames from a video, we adopt a motion network to predict their relative 3D camera pose and a segmentation mask distinguishing moving objects and rigid background. An optical flow network is used to estimate dense 2D per-pixel correspondence. A single image depth network predicts depth maps for both images. The four types of information, i.e. 2D flow, camera pose, segment mask and depth maps, are integrated into a differentiable holistic 3D motion parser (HMP), where per-pixel 3D motion for rigid background and moving objects are recovered. We design various losses w.r.t. the two types of 3D motions for training the depth and motion networks, yielding further error reduction for estimated geometry. Finally, in order to solve the 3D motion confusion from monocular videos, we combine stereo images into joint training. Experiments on KITTI 2015 dataset show that our estimated geometry, 3D motion and moving object masks, not only are constrained to be consistent, but also significantly outperforms other SOTA algorithms, demonstrating the benefits of our approach."
  },
  "eccv2018_w30_localisationviadeepimaginationlearnthefeaturesnotthemap": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - ApolloScape: Vision-based Navigation for Autonomous Driving",
    "title": "Localisation via Deep Imagination: learn the features not the map",
    "authors": [
      "Jaime Spencer",
      "Oscar Mendez",
      "Richard Bowden",
      "Simon Hadfield"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w30/html/Spencer_Localisation_via_Deep_Imagination_learn_the_features_not_the_map_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Spencer_Localisation_via_Deep_Imagination_learn_the_features_not_the_map_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " How many times does a human have to drive through the same area to become familiar with it? To begin with, we might first build a mental model of our surroundings. Upon revisiting this area, we can use this model to extrapolate to new unseen locations and imagine their appearance.Based on this, we propose an approach where an agent is capable of modelling new environments after a single visitation. To this end, we introduce \"Deep Imagination\", a combination of classical Visual-based Monte Carlo Localisation and deep learning. By making use of a feature embedded 3D map, the system can \"imagine\" the view from any novel location. These \"imagined\" views are contrasted with the current observation in order to estimate the agent\u00e2\u0080\u0099s current location. In order to build the embedded map, we train a deep Siamese Fully Convolutional U-Net to perform dense feature extraction. By training these features to be generic, no additional training or fine tuning is required to adapt to new environments.Our results demonstrate the generality and transfer capability of our learnt dense features by training and evaluating on multiple datasets. Additionally, we include several visualizations of the feature representations and resulting 3D maps, as well as their application to localisation."
  },
  "eccv2018_w30_interestpointdetectorsstabilityevaluationonapolloscapedataset": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - ApolloScape: Vision-based Navigation for Autonomous Driving",
    "title": "Interest point detectors stability evaluation on ApolloScape dataset",
    "authors": [
      "Jacek Komorowski",
      "Konrad Czarnota",
      "Tomasz Trzcinski",
      "Lukasz Dabala",
      "Simon Lynen"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w30/html/Komorowski_Interest_point_detectors_stability_evaluation_on_ApolloScape_dataset_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Komorowski_Interest_point_detectors_stability_evaluation_on_ApolloScape_dataset_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In the recent years, a number of novel, deep-learning based, interest point detectors, such as LIFT, DELF, Superpoint or LF-Net was proposed. However there\u00e2\u0080\u0099s a lack of a standard benchmark to evaluate suitability of these novel keypoint detectors for real-live applications such as autonomous driving. Traditional benchmarks (e.g. Oxford VGG) are rather limited, as they consist of relatively few images of mostly planar scenes taken in favourable conditions. In this paper we verify if the recent, deep-learning based interest point detectors have the advantage over the traditional, hand-crafted keypoint detectors. To this end, we evaluate stability of a number of hand crafted and recent, learning-based interest point detectors on the street-level view ApolloScape dataset."
  },
  "eccv2018_w30_reliablemultilanedetectionandclassificationbyutilizingcnnasaregressionnetwork": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - ApolloScape: Vision-based Navigation for Autonomous Driving",
    "title": "Reliable multilane detection and classification by utilizing CNN as a regression network",
    "authors": [
      "Shriyash Chougule",
      "Nora Koznek",
      "Asad Ismail",
      "Ganesh Adam",
      "Vikram Narayan",
      "Matthias Schulze"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w30/html/Chougule_Reliable_multilane_detection_and_classification_by_utilizing_CNN_as_a_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11133/Chougule_Reliable_multilane_detection_and_classification_by_utilizing_CNN_as_a_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Reliable lane detection is crucial functionality for autonomous driving. Additionally positional information of ego lanes and side lanes is pivotal for critical tasks like overtaking assistants and path planning. In this work we present a CNN based regression approach for detecting multiple lanes as well as positionally classifying them. Present deep learning approaches for lane detection are inherently CNN semantic segmentation networks, which concentrate on classifying each pixel correctly and require post processing operations to infer lane information. We identify that such segmentation approach is not effective for detecting thin and elongated lane boundaries, which occupy relatively few pixels in the scene and is often occluded by vehicles. We pose the lane detection and classification problem as CNN regression task, which relaxes per pixel classification requirement to a few points along lane boundary. Our networks has better accuracy than the recent CNN based segmentation solution, and does not require any post processing or tracking operations. Particularly we observe improved robustness in occlusions and amidst shadows due to over bridge and trees. We have validated the network on our test vehicle using Nvidia\u00e2\u0080\u0099s PX2 platform, where we observe a promising performance of 25 FPS."
  },
  "eccv2018_w31_deeplearningforassistivecomputervision": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Deep Learning for Assistive Computer Vision",
    "authors": [
      "Marco Leo",
      "Antonino Furnari",
      "Gerard G. Medioni",
      "Mohan Trivedi",
      "Giovanni M. Farinella"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Leo_Deep_Learning_for_Assistive_Computer_Vision_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Leo_Deep_Learning_for_Assistive_Computer_Vision_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper revises the main advances in assistive computer visionrecently fostered by deep learning. To this aim, we first discuss how the application of deep learning in computer vision has contributed to the development of assistive techinologies, then analyze the recentadvances in assistive technologies achieved in five main areas, namely, object classification and localization, scene understanding, human pose estimation and tracking, action/event recognition and anticipation. The paper is concluded with a discussion and insights for future directions."
  },
  "eccv2018_w31_recovering6dobjectposeareviewandmulti-modalanalysis": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Recovering 6D Object Pose: A Review and Multi-modal Analysis",
    "authors": [
      "Caner Sahin",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Sahin_Recovering_6D_Object_Pose_A_Review_and_Multi-modal_Analysis_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Sahin_Recovering_6D_Object_Pose_A_Review_and_Multi-modal_Analysis_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " A large number of studies analyse object detection and pose estimation at visual level in 2D, discussing the effects of challenges such as occlusion, clutter, texture, etc., on the performances of the methods, which work in the context of RGB modality. Interpreting the depth data, the study in this paper presents thorough multi-modal analyses. It discusses the above-mentioned challenges for full 6D object pose estimation in RGB-D images comparing the performances of several 6D detectors in order to answer the following questions: What is the current position of the computer vision community for maintaining \"automation\" in robotic manipulation? What next steps should the community take for improving \"autonomy\" in robotics while handling objects? Our findings include : (i) reasonably accurate results are obtained on textured-objects at varying viewpoints with cluttered backgrounds. (ii) Heavy existence of occlusion and clutter severely affects the detectors, and similar-looking distractors is the biggest challenge in recovering instances\u00e2\u0080\u0099 6D. (iii) Template-based methods and random forest-based learning algorithms underlie object detection and 6D pose estimation. Recent paradigm is to learn deep discriminative feature representations and to adopt CNNs taking RGB images as input. (iv) Depending on the availability of large-scale 6D annotated depth datasets, feature representations can be learnt on these datasets, and then the learnt representations can be customized for the 6D problem."
  },
  "eccv2018_w31_computervisionformedicalinfantmotionanalysisstateoftheartandrgb-ddataset": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Computer Vision for Medical Infant Motion Analysis: State of the Art and RGB-D Data Set",
    "authors": [
      "Nikolas Hesse",
      "Christoph Bodensteiner",
      "Michael Arens",
      "Ulrich G. Hofmann",
      "Raphael Weinberger",
      "A. Sebastian Schroeder"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Hesse_Computer_Vision_for_Medical_Infant_Motion_Analysis_State_of_the_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Hesse_Computer_Vision_for_Medical_Infant_Motion_Analysis_State_of_the_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Assessment of spontaneous movements of infants lets trained experts predict neurodevelopmental disorders like cerebral palsy at a very young age, allowing early intervention for affected infants. An automated motion analysis system requires to accurately capture body movements, ideally without markers or attached sensors to not affect the movements of infants. A vast majority of recent approaches for human pose estimation focuses on adults, leading to a degradation of accuracy if applied to infants. Hence, multiple systems for infant pose estimation have been developed. Due to the lack of publicly available benchmark data sets, a standardized evaluation, let alone a comparison of different approaches is impossible. We fill this gap by releasing the Moving INfants In RGB-D (MINI-RGBD)\u00e2\u0080\u00a0 data set, created using the recently introduced Skinned Multi-Infant Linear body model (SMIL). We map real infant movements to the SMIL model with realistic shapes and textures, and generate RGB and depth images with precise ground truth 2D and 3D joint positions. We evaluate our data set with state-of-the-art methods for 2D pose estimation in RGB images and for 3D pose estimation in depth images. Evaluation of 2D pose estimation results in a PCKh rate of 88.1% and 94.5% (depending on correctness threshold), and PCKh rates of 64.2%, respectively 90.4% for 3D pose estimation. We hope to foster research in medical infant motion analysis to get closer to an automated system for early detection of neurodevelopmental disorders."
  },
  "eccv2018_w31_visionaugmentedrobotfeeding": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Vision Augmented Robot Feeding",
    "authors": [
      "Alexandre Candeias",
      "Travers Rhodes",
      "Manuel Marques",
      "Jo ao P. Costeira",
      "Manuela Veloso"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Candeias_Vision_Augmented_Robot_Feeding_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Candeias_Vision_Augmented_Robot_Feeding_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Researchers have over time developed robotic feeding assistants to help at meals so that people with disabilities can live more autonomous lives. Current commercial feeding assistant robots acquire food without feedback on acquisition success and move to a preprogrammed location to deliver the food. In this work, we evaluate how vision can be used to improve both food acquisition and delivery. We show that using visual feedback on whether food was captured increases food acquisition efficiency. We also show how Discriminative Optimization (DO) can be used in tracking so that the food can be effectively brought all the way to the user's mouth, rather than to a preprogrammed feeding location."
  },
  "eccv2018_w31_human-computerinteractionapproachesfortheassessmentandthepracticeofthecognitivecapabilitiesofelderlypeople.": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Human-computer interaction approaches for the assessment and the practice of the cognitive capabilities of elderly people.",
    "authors": [
      "Manuela Chessa",
      "Chiara Bassano",
      "Elisa Gusai",
      "Alice E. Martis",
      "Fabio Solari"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Chessa_Human-computer_interaction_approaches_for_the_assessment_and_the_practice_of_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Chessa_Human-computer_interaction_approaches_for_the_assessment_and_the_practice_of_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The cognitive assessment of elderly people is usually performed by means of paper-pencil tests, which may not provide an exhaustive evaluation of the cognitive abilities of the subject. Here, we analyze two solutions based on interaction in virtual environments. In particular, we consider a non-immersive exergame based on a standard tablet, and an immersive VR environment based on a head-mounted display. We show the potential use of such tools, by comparing a set of computed metrics with the results of standard clinical tests, and we discuss the potential use of such tools to perform more complex evaluations. In particular, the use of immersive environments, which could be implemented both with head-mounted displays or with configurations of stereoscopic displays, allows us to track the patients' pose, and to analyze his/her movements and posture, when performing Activities of Daily Living, with the aim of having a further way to assess cognitive capabilities."
  },
  "eccv2018_w31_analysisoftheeffectofsensorsforend-to-endmachinelearningodometry": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Analysis of the Effect of Sensors for End-to-End Machine Learning Odometry",
    "authors": [
      "Carlos Marquez Rodriguez-Peral",
      "Dexmont Pe na"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Rodriguez-Peral_Analysis_of_the_Effect_of_Sensors_for_End-to-End_Machine_Learning_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Rodriguez-Peral_Analysis_of_the_Effect_of_Sensors_for_End-to-End_Machine_Learning_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Accurate position and orientation estimations are essential for navigation in autonomous robots. Although it is a well studied problem, existing solutions rely on statistical filters, which usually require good parameter initialization or calibration and are computationally expensive. This paper addresses that problem by using an end-to-end machine learning approach. This work explores the incorporation of multiple sources of data (monocular RGB images and inertial data) to overcome the weaknesses of each source independently. Three different odometry approaches are proposed using CNNs and LSTMs and evaluated against the KITTI dataset and compared with other existing approaches. The obtained results show that the performance of the proposed approaches is similar to the state-of-the-art ones, outperforming some of them at a lower computational cost allowing their execution on resource constrained devices."
  },
  "eccv2018_w31_ramciprobotapersonalroboticassistant;demonstrationofacompleteframework": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "RAMCIP Robot: A Personal Robotic Assistant; Demonstration of a Complete Framework",
    "authors": [
      "Ioannis Kostavelis",
      "Dimitrios Giakoumis",
      "Georgia Peleka",
      "Andreas Kargakos",
      "Evangelos Skartados",
      "Manolis Vasileiadis",
      "Dimitrios Tzovaras"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Kostavelis_RAMCIP_Robot_A_Personal_Robotic_Assistant_Demonstration_of_a_Complete_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Kostavelis_RAMCIP_Robot_A_Personal_Robotic_Assistant_Demonstration_of_a_Complete_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " At the last decades, personal domestic robots are considered as the future for tackling the societal challenge inherent in the growing elderly population. Ageing is typically associated with physical and cognitive decline, altering the way an older person moves around the house, manipulates objects and senses the home environment. This paper aims to demonstrate the RAMCIP robot, which is a Robotic Assistant for patients with Mild Cognitive Impairments (MCI), suitable to provide its services in domestic environments. The use cases that the robot addresses are described herein outlining the necessary requirements that set the basis for the software and hardware architectural components. A short description of the integrated cognitive, perception, manipulation and navigation capabilities of the robot is provided. Robot\u00e2\u0080\u0099s autonomy is enabled through a specific decision making and task planning framework. The robot has been evaluated in ten real home environments of real MCI users exhibiting remarkable performance."
  },
  "eccv2018_w31_anempiricalstudytowardsunderstandinghowdeepconvolutionalnetsrecognizefalls": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "An empirical study towards understanding how deep convolutional nets recognize falls",
    "authors": [
      "Yan Zhang",
      "Heiko Neumann"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Zhang_An_empirical_study_towards_understanding_how_deep_convolutional_nets_recognize_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Zhang_An_empirical_study_towards_understanding_how_deep_convolutional_nets_recognize_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Detecting unintended falls is essential for ambient intelligence and healthcare of elderly people living alone. In recent years, deep convolutional nets are widely used in human action analysis, based on which a number of fall detection methods have been proposed. Despite their highly effective performances, the behaviors of how the convolutional nets recognize falls are still not clear. In this paper, instead of proposing a novel approach, we perform a systematical empirical study, attempting to investigate the underlying fall recognition process. We propose four tasks to investigate, which involve five types of input modalities, seven net instances and different training samples. The obtained quantitative and qualitative results reveal the patterns that the nets tend to learn, and several factors that can heavily influence the performances on fall recognition. We expect that our conclusions are favorable to proposing better deep learning solutions to fall detection systems."
  },
  "eccv2018_w31_assistpersonalizedindoornavigationviamultimodalsensorsandhigh-levelsemanticinformation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "ASSIST: Personalized indoor navigation via multimodal sensors and high-level semantic information",
    "authors": [
      "Vishnu Nair",
      "Manjekar Budhai",
      "Greg Olmschenk",
      "William H. Seiple",
      "Zhigang Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Nair_ASSIST_Personalized_indoor_navigation_via_multimodal_sensors_and_high-level_semantic_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Nair_ASSIST_Personalized_indoor_navigation_via_multimodal_sensors_and_high-level_semantic_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Blind & visually impaired (BVI) individuals and those with Autism Spectrum Disorder (ASD) each face unique challenges in navigating unfamiliar indoor environments. In this paper, we propose an indoor positioning and navigation system that guides a user from point A to point B indoors with high accuracy while augmenting their situational awareness. This system has three major components: location recognition (a hybrid indoor localization app that uses Bluetooth Low Energy beacons and Google Tango to provide high accuracy), object recognition (a body-mounted camera to provide the user momentary situational awareness of objects and people), and semantic recognition (map-based annotations to alert the user of static environmental characteristics). This system also features personalized interfaces built upon the unique experiences that both BVI and ASD individuals have in indoor wayfinding and tailors its multimodal feedback to their needs. Here, the technical approach and implementation of this system are discussed, and the results of human subject tests with both BVI and ASD individuals are presented. In addition, we discuss and show the system\u00e2\u0080\u0099s user-centric interface and present points for future work and expansion."
  },
  "eccv2018_w31_comparingmethodsforassessmentoffacialdynamicsinpatientswithmajorneurocognitivedisorders": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Comparing methods for assessment of facial dynamics in patients with major neurocognitive disorders",
    "authors": [
      "Yaohui Wang",
      "Antitza Dantcheva",
      "Jean-Claude Broutart",
      "Philippe Robert",
      "Francois Bremond",
      "Piotr Bilinski"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Wang_Comparing_methods_for_assessment_of_facial_dynamics_in_patients_with_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Wang_Comparing_methods_for_assessment_of_facial_dynamics_in_patients_with_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Assessing facial dynamics in patients with major neurocognitive disorders and specifically with Alzheimer\u00e2\u0080\u0099s disease (AD) has shown to be highly challenging. Classically such assessment is performed by clinical staff, evaluating verbal and non-verbal language of AD-patients, since they have lost a substantial amount of their cognitive capacity, and hence communication ability.In addition, patients need to communicate important messages, such as discomfort or pain. Automated methods would support the current healthcare system by allowing for telemedicine, i.e., lesser costly and logistically inconvenient examination.In this work we compare methods for assessing facial dynamics such as talking, singing, neutral and smiling in AD-patients, captured during music mnemotherapy sessions.Specifically, we compare 3D ConvNets, Very Deep Neural Network based Two-Stream ConvNets, as well as Improved Dense Trajectories.We have adapted these methods from prominent action recognition methods and our promising results suggest that the methods generalize well to the context of facial dynamics.The Two-Stream ConvNets in combination with ResNet-152 obtains the best performance on our dataset, capturing well even minor facial dynamics andhas thus sparked high interest in the medical community."
  },
  "eccv2018_w31_deepexecutionmonitorforrobotassistivetasks": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Deep execution monitor for robot assistive tasks",
    "authors": [
      "Lorenzo Mauro",
      "Edoardo Alati",
      "Marta Sanzari",
      "Valsamis Ntouskos",
      "Gianluca Massimiani",
      "Fiora Pirri"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Mauro_Deep_execution_monitor_for_robot_assistive_tasks_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Mauro_Deep_execution_monitor_for_robot_assistive_tasks_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We consider a novel approach tohigh-level robot task execution for a robot assistive task.In this work we explore the problem of learning to predict the next subtaskby introducing a deep model for bothsequencinggoals and for visually evaluating the state of a task.We show that deep learning formonitoring robot tasks execution very well supports the interconnection between task-level planningand robot operations.These solutions can also cope with the natural non-determinism of the execution monitor.We show that a deep execution monitor leverages robot performance. We measure the improvement taking into account some robot helping tasks performed at a warehouse."
  },
  "eccv2018_w31_chasingfeetinthewildaproposedegocentricmotion-awaregaitassessmenttool": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Chasing feet in the wild: A proposed egocentric motion-aware gait assessment tool",
    "authors": [
      "Mina Nouredanesh",
      "Aaron W. Li",
      "Alan Godfrey",
      "Jesse Hoey",
      "James Tung"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Nouredanesh_Chasing_feet_in_the_wild_A_proposed_egocentric_motion-aware_gait_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Nouredanesh_Chasing_feet_in_the_wild_A_proposed_egocentric_motion-aware_gait_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Despite advances in gait analysis tools, including optical motion capture and wireless electrophysiology, our understanding of human mobility is largely limited to controlled conditions in a clinic and/or laboratory. In order to examine human mobility under natural conditions, or the \u00e2\u0080\u0099wild\u00e2\u0080\u0099, this paper presents a novel markerless model to obtain gait patterns by localizing feet in the egocentric video data. Based on a beltmounted camera feed, the proposed hybrid FootChaser model consists of: 1) the FootRegionProposer, a ConvNet that proposes regions with high probability of containing feet in RGB frames (global appearance of feet), and 2) LocomoNet, which is sensitive to the periodic gait patterns, and further examines the temporal content in the stacks of optical flow corresponding to the proposed region. The LocomoNet significantly boosted the overall model\u00e2\u0080\u0099s result by filtering out the false positives proposed by the FootRegionProposer. This work advances our long-term objective to develop novel markerless models to extract spatiotemporal gait parameters, particularly step width, to complement existing inertial measurement unit (IMU) based methods."
  },
  "eccv2018_w31_inferringhumanknowledgeabilityfromeyegazeinmobilelearningenvironments": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 6th International Workshop on Assistive Computer Vision and Robotics",
    "title": "Inferring Human Knowledgeability from Eye Gaze in Mobile Learning Environments",
    "authors": [
      "Oya Celiktutan",
      "Yiannis Demiris"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w31/html/Celiktutan_Inferring_Human_Knowledgeability_from_Eye_Gaze_in_Mobile_Learning_Environments_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Celiktutan_Inferring_Human_Knowledgeability_from_Eye_Gaze_in_Mobile_Learning_Environments_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " What people look at during a visual task reflects an interplay between ocular motor functions and cognitive processes. In this paper, we study the links between eye gaze and cognitive states to investigate whether eye gaze reveal information about an individual's knowledgeability. We focus on a mobile learning scenario where a user and a virtual agent play a quiz game using a hand-held mobile device. To the best of our knowledge, this is the first attempt to predict user's knowledgeability from eye gaze using a noninvasive eye tracking method on mobile devices: we perform gaze estimation using front-facing camera of mobile devices in contrast to using specialised eye tracking devices.First, we define a set of eye movement features that are discriminative for inferring user's knowledgeability. Next, we train a model to predict users' knowledgeabilityin the course of responding to a question. We obtain a classification performance of 59.1% achieving human performance, using eye movement features only, which has implications for (1) adapting behaviours of the virtual agent to user's needs (e.g., virtual agent can give hints); (2) personalising quiz questions to the user's perceived knowledgeability."
  },
  "eccv2018_w32_hand-tremorfrequencyestimationinvideos": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Observing and Understanding Hands in Action",
    "title": "Hand-tremor frequency estimation in videos",
    "authors": [
      "Silvia L. Pintea",
      "Jian Zheng",
      "Xilin Li",
      "Paulina J.M. Bank",
      "Jacobus J. van Hilten",
      "Jan C. van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w32/html/Pintea_Hand-tremor_frequency_estimation_in_videos_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Pintea_Hand-tremor_frequency_estimation_in_videos_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We focus on the problem of estimating human hand-tremor frequency from input RGB video data. Estimating tremors from video is important for non-invasive monitoring, analyzing and diagnosing patients suffering from motor-disorders such as Parkinson\u00e2\u0080\u0099s disease. We consider two approaches for hand-tremor frequency estimation: (a) a Lagrangian approach where we detect the hand at every frame in the video, and estimate the tremor frequency along the trajectory; and (b) an Eulerian approach where we first localize the hand, we subsequently remove the large motion along the movement trajectory of the hand, and we use the video information over time encoded as intensity values or phase information to estimate the tremor frequency. We estimate hand tremors on a new human tremor dataset, TIM-Tremor, containing static tasks as well as a multitude of more dynamic tasks, involving larger motion of the hands. The dataset has 55 tremor patient recordings together with: associated ground truth accelerometer data from the most affected hand, RGB video data, and aligned depth data."
  },
  "eccv2018_w32_drawinairalightweightgesturalinterfacebasedonfingertipregression": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Observing and Understanding Hands in Action",
    "title": "DrawInAir: A Lightweight Gestural Interface Based on Fingertip Regression",
    "authors": [
      "Gaurav Garg",
      "Srinidhi Hegde",
      "Ramakrishna Perla",
      "Varun Jain",
      "Lovekesh Vig",
      "Ramya Hebbalaguppe"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w32/html/Garg_DrawInAir_A_Lightweight_Gestural_Interface_Based_on_Fingertip_Regression_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Garg_DrawInAir_A_Lightweight_Gestural_Interface_Based_on_Fingertip_Regression_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Hand gestures form a natural way of interaction on HeadMounted Devices (HMDs) and smartphones. HMDs such as the Microsoft HoloLens and ARCore/ARKit platform enabled smartphones are expensive and are equipped with powerful processors and sensors such as multiple cameras, depth and IR sensors to process hand gestures. To enable mass market reach via inexpensive Augmented Reality (AR) headsets without built-in depth or IR sensors, we propose a real-time, in-air gestural framework that works on monocular RGB input, termed, DrawInAir. DrawInAir uses fingertip for writing in air analogous to a pen on paper. The major challenge in training egocentric gesture recognition models is in obtaining sufficient labeled data for end-to-end learning. Thus, we design a cascade of networks, consisting of a CNN with differentiable spatial to numerical transform (DSNT) layer, for fingertip regression, followed by a Bidirectional Long Short-Term Memory (Bi-LSTM), for a real-time pointing hand gesture classification. We highlight how a model, that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better over end-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model. We show that the framework takes 1.73s to run end-to-end and has a low memory footprint of 14MB while achieving an accuracy of 88.0% on egocentric video dataset."
  },
  "eccv2018_w32_adaptingegocentricvisualhandposeestimationtowardsarobot-controlledexoskeleton": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Observing and Understanding Hands in Action",
    "title": "Adapting Egocentric Visual Hand Pose Estimation Towards a Robot-Controlled Exoskeleton",
    "authors": [
      "Gerald Baulig",
      "Thomas Gulde",
      "Cristobal Curio"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w32/html/Baulig_Adapting_Egocentric_Visual_Hand_Pose_Estimation_Towards_a_Robot-Controlled_Exoskeleton_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Baulig_Adapting_Egocentric_Visual_Hand_Pose_Estimation_Towards_a_Robot-Controlled_Exoskeleton_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The basic idea behind a wearable robotic grasp assistance system is to support people that suffer from severe motor impairments in daily activities. Such a system needs to act mostly autonomously and according to the user\u00e2\u0080\u0099s intent. Vision-based hand pose estimation could be an integral part of a larger control and assistance framework. In this paper we evaluate the performance of egocentric monocular hand pose estimation for a robot-controlled hand exoskeleton in a simulation. For hand pose estimation we adopt a Convolutional Neural Network (CNN). We train and evaluate this network with computer graphics, created by our own data generator. In order to guide further design decisions we focus in our experiments on two egocentric camera viewpoints tested on synthetic data with the help of a 3D-scanned hand model, with and without an exoskeleton attached to it. We observe that hand pose estimation with a wrist-mounted camera performs more accurate than with a head-mounted camera in the context of our simulation. Further, a grasp assistance system attached to the hand alters visual appearance and can improve hand pose estimation. Our experiment provides useful insights for the integration of sensors into a context sensitive analysis framework for intelligent assistance."
  },
  "eccv2018_w32_estimating2dmulti-handposesfromsingledepthimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Observing and Understanding Hands in Action",
    "title": "Estimating 2D Multi-Hand Poses From Single Depth Images",
    "authors": [
      "Le Duan",
      "Minmin Shen",
      "Song Cui",
      "Zhexiao Guo",
      "Oliver Deussen"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w32/html/Duan_Estimating_2D_Multi-Hand_Poses_From_Single_Depth_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Duan_Estimating_2D_Multi-Hand_Poses_From_Single_Depth_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We present a novel framework based on Pictorial Structure (PS) models to estimate 2D multi-hand poses from depth images. Most existing single-hand pose estimation algorithms are either subject to strong assumptions or depend on a weak detector to detect the human hand. We utilize Mask R-CNN to avoid both aforementioned constraints. The proposed framework allows detection of multi-hand instances and localization of hand joints simultaneously. Our experiments show that our method is superior to existing methods."
  },
  "eccv2018_w32_spatial-temporalattentionres-tcnforskeleton-baseddynamichandgesturerecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Observing and Understanding Hands in Action",
    "title": "Spatial-Temporal Attention Res-TCN for Skeleton-based Dynamic Hand Gesture Recognition",
    "authors": [
      "Jingxuan Hou",
      "Guijin Wang",
      "Xinghao Chen",
      "Jing-Hao Xue",
      "Rui Zhu",
      "Huazhong Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w32/html/Hou_Spatial-Temporal_Attention_Res-TCN_for_Skeleton-based_Dynamic_Hand_Gesture_Recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Hou_Spatial-Temporal_Attention_Res-TCN_for_Skeleton-based_Dynamic_Hand_Gesture_Recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Dynamic hand gesture recognition is a crucial yet challenging task in computer vision. The key of this task lies in an effective extraction of discriminative spatial and temporal features to model the evolutions of different gestures. In this paper, we propose an end-to-end Spatial-Temporal Attention Residual Temporal Convolutional Network (STA-Res-TCN) for skeleton-based dynamic hand gesture recognition, which learns different levels of attention and assigns them to each spatialtemporal feature extracted by the convolution filters at each time step. The proposed attention branch assists the networks to adaptively focus on the informative time frames and features while exclude the irrelevant ones that often bring in unnecessary noise. Moreover, our proposed STA-Res-TCN is a lightweight model that can be trained and tested in an extremely short time. Experiments on DHG-14/28 Dataset and SHREC\u00e2\u0080\u009917 Track Dataset show that STA-Res-TCN outperforms stateof-the-art methods on both the 14 gestures setting and the more complicated 28 gestures setting."
  },
  "eccv2018_w32_task-orientedhandmotionretargetingfordexterousmanipulationimitation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Observing and Understanding Hands in Action",
    "title": "Task-Oriented Hand Motion Retargeting for Dexterous Manipulation Imitation",
    "authors": [
      "Dafni Antotsiou",
      "Guillermo Garcia-Hernando",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w32/html/Antotsiou_Task-Oriented_Hand_Motion_Retargeting_for_Dexterous_Manipulation_Imitation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Antotsiou_Task-Oriented_Hand_Motion_Retargeting_for_Dexterous_Manipulation_Imitation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Humanhandactionsarequitecomplex,especiallywhenthey involve object manipulation, mainly due to the high dimensionality of the hand and the vast action space that entails. Imitating those actions with dexterous hand models involves different important and challenging steps: acquiring human hand information, retargeting it to a hand model, and learning a policy from acquired data. In this work, we capture the hand information by using a state-of-the-art hand pose estimator. We tackle the retargeting problem from the hand pose to a 29 DoF hand model by combining inverse kinematics and PSO with a task objective optimisation. This objective encourages the virtual hand to accomplish the manipulation task, relieving the effect of the estimator\u00e2\u0080\u0099s noise and the domain gap. Our approach leads to a better success rate in the grasping task compared to our inverse kinematics baseline, allowing us to record successful human demonstrations. Furthermore, we used these demonstrations to learn a policy network using generative adversarial imitation learning (GAIL) that is able to autonomously grasp an object in the virtual space."
  },
  "eccv2018_w32_hands18methods,techniquesandapplicationsforhandobservation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 4th International Workshop on Observing and Understanding Hands in Action",
    "title": "HANDS18: Methods, Techniques and Applications for Hand Observation",
    "authors": [
      "Iason Oikonomidis",
      "Guillermo Garcia-Hernando",
      "Angela Yao",
      "Antonis Argyros",
      "Vincent Lepetit",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w32/html/Oikonomidis_HANDS18_Methods_Techniques_and_Applications_for_Hand_Observation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Oikonomidis_HANDS18_Methods_Techniques_and_Applications_for_Hand_Observation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This report outlines the proceedings of the Fourth International Workshop on Observing and Understanding Hands in Action (HANDS 2018). The fourth instantiation of this workshop attracted significant interest from both academia and the industry. The program of the workshop included regular papers that are published as the workshop\u00e2\u0080\u0099s proceedings, extended abstracts, invited posters, and invited talks. Topics of the submitted works and invited talks and posters included novel methods for hand pose estimation from RGB, depth, or skeletal data, datasets for special cases and real-world applications, and techniques for hand motion re-targeting and hand gesture recognition. The invited speakers are leaders in their respective areas of specialization, coming from both industry and academia. The main conclusions that can be drawn are the turn of the community towards RGB data and the maturation of some methods and techniques, which in turn has led to increasing interest for real-world applications."
  },
  "eccv2018_w33_automaticclassificationoflow-resolutionchromosomalimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Automatic Classification of Low-Resolution Chromosomal Images",
    "authors": [
      "Swati"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Swati_Automatic_Classification_of_Low-Resolution_Chromosomal_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Swati_Automatic_Classification_of_Low-Resolution_Chromosomal_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Chromosome karyotyping is a two-staged process consisting of segmentation followed by pairing and ordering of 23 pairs of human chromosomes obtained from cell spread images during metaphase stage of cell division. It is carried out by cytogeneticists in clinical labs on the basis of length, centromere position, and banding pattern of chromosomes for the diagnosis of various health and genetic disorders. The entire process demands high domain expertise and considerable amount of manual effort. This motivates us to automate or partially automate karyotyping process which would benefit and aid doctors in the analysis of chromosome images. However, the non-availability of high resolution chromosome images required for classification purpose creates a hindrance in achieving high classification accuracy. To address this issue, we propose a Super-Xception network which takes the low-resolution chromosome images as input and classifies them to one of the 24 chromosome class labels after conversion into high resolution images. In this network, we integrate super-resolution deep models with standard classification networks e.g., Xception network in our case. The network is trained in an end-to-end manner in which the super-resolution layers help in conversion of low-resolution images to high-resolution images which are subsequently passed through deep classification layers for label assigning. We evaluate our proposed network\u00e2\u0080\u0099s efficacy on a publicly available online Bioimage chromosome classification dataset of healthy chromosomes and benchmark it against the baseline models created using traditional deep convolutional neural network, ResNet-50 and Xception network."
  },
  "eccv2018_w33_feature2massvisualfeatureprocessinginlatentspaceforrealisticlabeledmassgeneration": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Feature2Mass: Visual Feature Processing in Latent Space for Realistic Labeled Mass Generation",
    "authors": [
      "Jae-Hyeok Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Lee_Feature2Mass_Visual_Feature_Processing_in_Latent_Space_for_Realistic_Labeled_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Lee_Feature2Mass_Visual_Feature_Processing_in_Latent_Space_for_Realistic_Labeled_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper deals with a method for generating realistic labeled masses. Recently, there have been many attempts to apply deep learning to various bio-image computing fields including computer-aided detection and diagnosis. In order to learn deep network model to be wellbehaved in bio-image computing fields, a lot of labeled data is required. However, in many bioimaging fields, the large-size of labeled dataset is scarcely available. Although a few researches have been dedicated to solving this problem through generative model, there are some problems as follows: 1) The generated bio-image does not seem realistic; 2) the variation of generated bio-image is limited; and 3) additional label annotation task is needed. In this study, we propose a realistic labeled bio-image generation method through visual feature processing in latent space. Experimental results have shown that mass images generated by the proposed method were realistic and had wide expression range of targeted mass characteristics."
  },
  "eccv2018_w33_ordinalregressionwithneuronstick-breakingformedicaldiagnosis": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Ordinal Regression with Neuron Stick-breaking for Medical Diagnosis",
    "authors": [
      "Xiaofeng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Liu_Ordinal_Regression_with_Neuron_Stick-breaking_for_Medical_Diagnosis_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Liu_Ordinal_Regression_with_Neuron_Stick-breaking_for_Medical_Diagnosis_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The classification for medical diagnosis usually involves inherently ordered labels corresponding to the level of health risk. Previous multi-task classifiers on ordinal data often use several binary classification branches to compute a series of cumulative probabilities. However, these cumulative probabilities are not guaranteed to be monotonically decreasing. It also introduces a large number of hyper-parameters to be fine-tuned manually. This paper aims to eliminate or at least largely reduce the effects of those problems. We propose a simple yet efficient way to rephrase the output layer of the conventional deep neural network. We show that our methods lead to the state-of-the-art accuracy on Diabetic Retinopathy dataset and Ultrasound Breast dataset with very little additional cost."
  },
  "eccv2018_w33_multi-levelactivationforsegmentationofhierarchically-nestedclasses": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Multi-level Activation for Segmentation of Hierarchically-nested Classes",
    "authors": [
      "Marie Piraud"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Piraud_Multi-level_Activation_for_Segmentation_of_Hierarchically-nested_Classes_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Piraud_Multi-level_Activation_for_Segmentation_of_Hierarchically-nested_Classes_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Formanybiologicalimagesegmentationtasks,includingtopological knowledge, such as the nesting of classes, can greatly improve results. However, most \u00e2\u0080\u0098out-of-the-box\u00e2\u0080\u0099 CNN models are still blind to such prior information. In this paper, we propose a novel approach to encode this information, through a multi-level activation layer and three compatible losses. We benchmark all of them on nuclei segmentation in bright-field microscopy cell images from the 2018 Data Science Bowl challenge, offering an exemplary segmentation task with cells and nested subcellular structures. Our scheme greatly speeds up learning, and outperforms standard multi-class classification with soft-max activation and a previously proposed method stemming from it, improving the Dice score significantly (p-values < 0.007). Our approach is conceptually simple, easy to implement and can be integrated in any CNN architecture. It can be generalized to a higher number of classes, with or without further relations of containment."
  },
  "eccv2018_w33_detectingsynapselocationandconnectivitybysignedproximityestimationandpruningwithdeepnets": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Detecting Synapse Location and Connectivity by Signed Proximity Estimation and Pruning with Deep Nets",
    "authors": [
      "Toufiq Parag"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Parag_Detecting_Synapse_Location_and_Connectivity_by_Signed_Proximity_Estimation_and_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Parag_Detecting_Synapse_Location_and_Connectivity_by_Signed_Proximity_Estimation_and_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Synaptic connectivity detection is a critical task for neural reconstruction from Electron Microscopy (EM) data. Most of the existing algorithms for synapse detection do not identify the cleft location and direction of connectivity simultaneously. The few methods that computes direction along with contact location have only been demonstrated to work on either dyadic (most common in vertebrate brain) or polyadic (found in fruit fly brain) synapses, but not on both types. In this paper, we present an algorithm to automatically predict the location as well as the direction of both dyadic and polyadic synapses. The proposed algorithm first generates candidate synaptic connections from voxelwise predictions of signed proximity generated by a 3D U-net. A second 3D CNN then prunes the set of candidates to produce the final detection of cleft and connectivity orientation. Experimental results demonstrate that the proposed method outperforms the existing methods for determining synapses in both rodent and fruit fly brain."
  },
  "eccv2018_w33_2dand3dvascularstructuresenhancementviamultiscalefractionalanisotropytensor": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "2D and 3D Vascular Structures Enhancement via Multiscale Fractional Anisotropy Tensor",
    "authors": [
      "Haifa F. Alhasson"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Alhasson_2D_and_3D_Vascular_Structures_Enhancement_via_Multiscale_Fractional_Anisotropy_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Alhasson_2D_and_3D_Vascular_Structures_Enhancement_via_Multiscale_Fractional_Anisotropy_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The detection of vascular structures from noisy images is a fundamental process for extracting meaningful information in many applications. Most well-known vascular enhancing techniques often rely on Hessian-based filters. This paper investigates the feasibility and deficiencies of detecting curve-like structures using a Hessian matrix. The main contribution is a novel enhancement function, which overcomes the deficiencies of established methods. Our approach has been evaluated quantitatively and qualitatively using synthetic examples and a wide range of real 2D and 3D biomedical images. Compared with other existing approaches, the experimental results prove that our proposed approach achieves high-quality curvilinear structure enhancement."
  },
  "eccv2018_w33_improveddictionarylearningwithenrichedinformationforbiomedicalimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Improved Dictionary Learning with Enriched Information for Biomedical Images",
    "authors": [
      "Shengda Luo"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Luo_Improved_Dictionary_Learning_with_Enriched_Information_for_Biomedical_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Luo_Improved_Dictionary_Learning_with_Enriched_Information_for_Biomedical_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " With dictionary learning using k-means or k-means++, the optimal value of k is traditionally determined empirically using a validation set. The optimal k, which should depend on the particular problem, is chosen with previously determined values from prior work. We argue that there is rich information from clustering with a number of values of k. We propose a novel method to extract information from clustering with all reasonable values of k at the same time. It is shown that our method improves the performance of dictionary learning for the popular bag-of-features model in image classification with simple patterns like cells such as biomedical images. Our experiments demonstrate that, our proposed dictionary learning method outperforms popular methods, on two well-known datasets by 12.5% and 8.5% compared to k-means/kmeans++ dictionary learning and by 8.9% and 6.1% compared to sparse coding."
  },
  "eccv2018_w33_visualandquantitativecomparisonofrealandsimulatedbiomedicalimagedata": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Visual and Quantitative Comparison of Real and Simulated Biomedical Image Data",
    "authors": [
      "Tereza Necasova"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Necasova_Visual_and_Quantitative_Comparison_of_Real_and_Simulated_Biomedical_Image_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Necasova_Visual_and_Quantitative_Comparison_of_Real_and_Simulated_Biomedical_Image_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The simulations in biomedical image analysis provide a solution when the real image data are difficult to be annotated or if they are available only in small quantities. The progress in simulations rapidly grows in the recent years. Nevertheless, the comparative techniques for the assessment of the plausibility of generated data are still unsatisfactory or none. This paper aims to point out the problem of insufficient comparison of real and synthetic data, which is done in many cases only by visual inspection or based on subjective measurements. The selected texture features are first compared in a univariate manner by quantilequantile plots and Kolmogorov-Smirnov test. The evaluation is then extended into multivariate assessment using the PCA for a visualization and furthermore for a quantitative measure of similarity by Jaccard index. Two different image datasets were used to show the results and the importance of the validation of simulated data in many aspects."
  },
  "eccv2018_w33_instancesegmentationofneuralcells": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Instance Segmentation of Neural Cells",
    "authors": [
      "Jingru Yi"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Yi_Instance_Segmentation_of_Neural_Cells_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Yi_Instance_Segmentation_of_Neural_Cells_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Instance segmentation of neural cells plays an important role in brain study. However, this task is challenging due to the special shapes and behaviors of neural cells. Existing methods are not precise enough to capture their tiny structures, e.g., filopodia and lamellipodia, which are critical to the understanding of cell interaction and behavior. To this end, we propose a novel deep multi-task learning model to jointly detect and segment neural cells instance-wise. Our method is built upon SSD, with ResNet101 as the backbone to achieve both high detection accuracy and fast speed. Furthermore, unlike existing works which tend to produce wavy and inaccurate boundaries, we embed a deconvolution module into SSD to better capture details. Experiments on a dataset of neural cell microscopic images show that our method is able to achieve better performance in terms of accuracy and efficiency, comparing favorably with current state-of-the-art methods."
  },
  "eccv2018_w33_denselyconnectedstackedu-networkforfilamentsegmentationinmicroscopyimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Densely Connected Stacked U-network for Filament Segmentation in Microscopy Images",
    "authors": [
      "Yi Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Liu_Densely_Connected_Stacked_U-network_for_Filament_Segmentation_in_Microscopy_Images_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Liu_Densely_Connected_Stacked_U-network_for_Filament_Segmentation_in_Microscopy_Images_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Segmenting filamentous structures in confocal microscopy images is important for analyzing and quantifying related biological processes. However, thin structures, especially in noisy imagery, are difficult to accurately segment. In this paper, we introduce a novel deep network architecture for filament segmentation in confocal microscopy images that improves upon the state-of-the-art U-net and SOAX methods. We also propose a strategy for data annotation, and create datasets for microtubule and actin filaments. Our experiments show that our proposed network outperforms state-of-the-art approaches and that our segmentation results are not only better in terms of accuracy, but also more suitable for biological analysis and understanding by reducing the number of falsely disconnected filaments in segmentation."
  },
  "eccv2018_w33_deepconvolutionalneuralnetworksbasedframeworkforestimationofstomatadensityandstructurefrommicroscopicimages": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Deep Convolutional Neural Networks based Framework for Estimation of Stomata Density and Structure from Microscopic Images",
    "authors": [
      "Swati Bhugra"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Bhugra_Deep_Convolutional_Neural_Networks_based_Framework_for_Estimation_of_Stomata_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Bhugra_Deep_Convolutional_Neural_Networks_based_Framework_for_Estimation_of_Stomata_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Analysis of stomata density and its configuration based on scanning electron microscopic (SEM) image of a leaf surface, is an effective way to characterize the plant\u00e2\u0080\u0099s behaviour under various environmental stresses (drought, salinity etc.). Existing methods for phenotyping these stomatal traits are often based on manual or semi-automatic labeling and segmentation of SEM images. This is a low-throughput process when large number of SEM images is investigated for statistical analysis. To overcome this limitation, we propose a novel automated pipeline leveraging deep convolutional neural networks for stomata detection and its quantification. The proposed framework shows a superior performance in contrast to the existing stomata detection methods in terms of precision and recall, 0.91 and 0.89 respectively. Furthermore, the morphological traits (i.e. length & width) obtained at stomata quantification step shows a correlation of 0.95 and 0.91 with manually computed traits, resulting in an efficient and high-throughput solution for stomata phenotyping."
  },
  "eccv2018_w33_afastandscalablepipelineforstainnormalizationofwhole-slideimagesinhistopathology": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "A Fast and Scalable Pipeline for Stain Normalization of Whole-Slide Images in Histopathology",
    "authors": [
      "Milos Stanisavljevic"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Stanisavljevic_A_Fast_and_Scalable_Pipeline_for_Stain_Normalization_of_Whole-Slide_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Stanisavljevic_A_Fast_and_Scalable_Pipeline_for_Stain_Normalization_of_Whole-Slide_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Stainnormalizationisoneofthemaintasksintheprocessing pipeline of computer-aided diagnosis systems in modern digital pathology. Some of the challenges in this tasks are memory and runtime bottlenecks associated with large image datasets. In this work, we present a scalable and fast pipeline for stain normalization using a state-of-the-art unsupervised method based on stain-vector estimation. The proposed system supports single-node and distributed implementations. Based on a highly-optimized engine, our architecture enables high-speed and largescale processing of high-magnification whole-slide images (WSI). We demonstrate the performance of the system using measurements from different datasets. Moreover, by using a novel pixel-sampling optimization we show lower processing time per image than the scanning time of ultrafast WSI scanners with the single-node implementation and additional 3.44 average speed-up with the 4-nodes distributed pipeline."
  },
  "eccv2018_w33_abenchmarkforepithelialcelltracking": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "A Benchmark for Epithelial Cell Tracking",
    "authors": [
      "Jan Funke"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Funke_A_Benchmark_for_Epithelial_Cell_Tracking_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Funke_A_Benchmark_for_Epithelial_Cell_Tracking_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Segmentation and tracking of epithelial cells in light microscopy (LM) movies of developing tissue is an abundant task in celland developmental biology. Epithelial cells are densely packed cells that form a honeycomb-like grid. This dense packing distinguishes membranestained epithelial cells from the types of objects recent cell tracking benchmarks have focused on, like cell nuclei and freely moving individual cells. While semi-automated tools for segmentation and tracking of epithelial cells are available to biologists, common tools rely on classical watershed based segmentation and engineered tracking heuristics, and entail a tedious phase of manual curation. However, a different kind of densely packed cell imagery has become a focus of recent computer vision research, namely electron microscopy (EM) images of neurons. In this work we explore the benefits of two recent neuron EM segmentation methods for epithelial cell tracking in light microscopy. In particular we adapt two different deep learning approaches for neuron segmentation, namely Flood Filling Networks and MALA, to epithelial cell tracking. We benchmark these on a dataset of eight movies with up to 200 frames. We compare to Moral Lineage Tracing, a combinatorial optimization approach that recently claimed state of the art results for epithelial cell tracking. Furthermore, we compare to Tissue Analyzer, an off-the-shelf tool used by Biologists that serves as our baseline."
  },
  "eccv2018_w33_automaticfusionofsegmentationandtrackinglabels": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Automatic Fusion of Segmentation and Tracking Labels",
    "authors": [
      "Cem Emre Akbas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Akbas_Automatic_Fusion_of_Segmentation_and_Tracking_Labels_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Akbas_Automatic_Fusion_of_Segmentation_and_Tracking_Labels_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Labeled training images of high quality are required for developing well-working analysis pipelines. This is, of course, also true for biological image data, where such labels are usually hard to get. We distinguish human labels (gold corpora) and labels generated by computer algorithms (silver corpora). A naturally arising problem is to merge multiple corpora into larger bodies of labeled training datasets. While fusion of labels in static images is already an established field, dealing with labels in time-lapse image data remains to be explored. Obtaining a gold corpus for segmentation is usually very time-consuming and hence expensive. For this reason, gold corpora for object tracking often use object detection markers instead of dense segmentations. If dense segmentations of tracked objects are desired later on, an automatic merge of the detection-based gold corpus with (silver) corpora of the individual time points for segmentation will be necessary. Here we present such an automatic merging system and demonstrate its utility on corpora from the Cell Tracking Challenge. We additionally release all label fusion algorithms as freely available and open plugins for Fiji3."
  },
  "eccv2018_w33_identificationofc.elegansstrainsusingafullyconvolutionalneuralnetworkonbehaviouraldynamics": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Identification of C. elegans strains using a fully convolutional neural network on behavioural dynamics",
    "authors": [
      "Avelino Javer"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Javer_Identification_of_it_C._elegans_strains_using_a_fully_convolutional_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Javer_Identification_of_it_C._elegans_strains_using_a_fully_convolutional_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " The nematode C. elegans is a promising model organism to understand the genetic basis of behaviour due to its anatomical simplicity. In this work, we present a deep learning model capable of discerning genetically diverse strains based only on their recorded spontaneous activity, and explore how its performance changes as different embeddings are used as input. The model outperforms hand-crafted features on strain classification when trained directly on time series of worm postures."
  },
  "eccv2018_w33_towardsautomatedmultiscaleimagingandanalysisintemglomerulusdetectionbyfusionofcnnandlbpmaps": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Towards automated multiscale imaging and analysis in TEM: Glomerulus detection by fusion of CNN and LBP maps",
    "authors": [
      "Elisabeth Wetzer"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Wetzer_Towards_automated_multiscale_imaging_and_analysis_in_TEM_Glomerulus_detection_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Wetzer_Towards_automated_multiscale_imaging_and_analysis_in_TEM_Glomerulus_detection_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Glomerulal structures in kidney tissue have to be analysed at a nanometer scale for several medical diagnoses. They are therefore commonly imaged using Transmission Electron Microscopy. The high resolution produces large amounts of data and requires long acquisition time, which makes automated imaging and glomerulus detection a desired option. This paper presents a deep learning approach for Glomerulus detection, using two architectures, VGG16 (with batch normalization) and ResNet50. To enhance the performance over training based only on intensity images, multiple approaches to fuse the input with texture information encoded in local binary patterns of different scales have been evaluated. The results show a consistent improvement in Glomerulus detection when fusing texture-based trained networks with intensity-based ones at a late classification stage."
  },
  "eccv2018_w33_pre-trainingongrayscaleimagenetimprovesmedicalimageclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - Bioimage Computing",
    "title": "Pre-training on Grayscale ImageNet Improves Medical Image Classification",
    "authors": [
      "Yiting Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w33/html/Xie_Pre-training_on_Grayscale_ImageNet_Improves_Medical_Image_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Xie_Pre-training_on_Grayscale_ImageNet_Improves_Medical_Image_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Deep learning is quickly becoming the de facto standard approach for solving a range of medical image analysis tasks. However, large medical image datasets appropriate for training deep neural network models from scratch are difficult to assemble due to privacy restrictions and expert ground truth requirements, with typical open source datasets ranging from hundreds to thousands of images. A standard approach to counteract limited-size medical datasets is to pre-train models on large datasets in other domains, such as ImageNet for classification of natural images, before fine-tuning on the specific medical task of interest. However, ImageNet contains color images, which introduces artefacts and inefficiencies into models that are intended for single-channel medical images. To address this issue, we pre-trained an Inception-V3 model on ImageNet after converting the images to grayscale through a common transformation. Surprisingly, these models do not show a significant degradation in performance on the original ImageNet classification task, suggesting that color is not a critical feature of natural image classification. Furthermore, models pre-trained on grayscale ImageNet outperformed color ImageNet models in terms of both speed and accuracy when refined on disease classification from chest X-ray images."
  },
  "eccv2018_w34_workshoponinteractiveandadaptivelearninginanopenworld": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Workshop on Interactive and Adaptive Learning in an Open World",
    "title": "Workshop on Interactive and Adaptive Learning in an Open World",
    "authors": [
      "Alexander Freytag",
      "Vittorio Ferrari",
      "Mario Fritz",
      "Uwe Franke",
      "Terrence Boult",
      "Juergen Gall",
      "Walter Scheirer",
      "Angela Yao",
      "Erik Rodner"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w34/html/Freytag_Workshop_on_Interactive_and_Adaptive_Learning_in_an_Open_World_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Freytag_Workshop_on_Interactive_and_Adaptive_Learning_in_an_Open_World_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Next generation machine learning requires stepping away from classical batch learning towards interactive and adaptive learning. This is essential to cope with demanding machine learning applications we have already today. Our workshop at ECCV 2018 in Munich therefore served as a discussion forum for experts in this field and in the following we give a brief overview."
  },
  "eccv2018_w35_workshoponinteractiveandadaptivelearninginanopenworld": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Workshop on Interactive and Adaptive Learning in an Open World",
    "authors": [
      "Alexander Freytag",
      "Vittorio Ferrari",
      "Mario Fritz",
      "Uwe Franke",
      "Terrence Boult",
      "Juergen Gall",
      "Walter Scheirer",
      "Angela Yao",
      "Erik Rodner"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Freytag_Workshop_on_Interactive_and_Adaptive_Learning_in_an_Open_World_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Freytag_Workshop_on_Interactive_and_Adaptive_Learning_in_an_Open_World_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Next generation machine learning requires stepping away from classical batch learning towards interactive and adaptive learning. This is essential to cope with demanding machine learning applications we have already today. Our workshop at ECCV 2018 in Munich therefore served as a discussion forum for experts in this field and in the following we give a brief overview."
  },
  "eccv2018_w35_boostinglidar-basedsemanticlabelingbycross-modaltrainingdatageneration": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Boosting LiDAR-based Semantic Labeling by Cross-Modal Training Data Generation",
    "authors": [
      "Florian Piewak",
      "Peter Pinggera",
      "Manuel Schafer",
      "David Peter",
      "Beate Schwarz",
      "Nick Schneider",
      "Markus Enzweiler",
      "David Pfeiffer",
      "Marius Zollner"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Piewak_Boosting_LiDAR-based_Semantic_Labeling_by_Cross-Modal_Training_Data_Generation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Piewak_Boosting_LiDAR-based_Semantic_Labeling_by_Cross-Modal_Training_Data_Generation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Mobile robots and autonomous vehicles rely on multi-modal sensor setups to perceive and understand their surroundings. Aside from cameras, LiDAR sensors represent a central component of state-of-theart perception systems. In addition to accurate spatial perception, a comprehensive semantic understanding of the environment is essential for efficient and safe operation. In this paper we present a novel deep neural network architecture called LiLaNet for point-wise, multi-class semantic labeling of semi-dense LiDAR data. The network utilizes virtual image projections of the 3D point clouds for efficient inference. Further, we propose an automated process for large-scale cross-modal training data generation called Autolabeling, in order to boost semantic labeling performance while keeping the manual annotation effort low. The effectiveness of the proposed network architecture as well as the automated data generation process is demonstrated on a manually annotated ground truth dataset. LiLaNet is shown to significantly outperform current state-ofthe-art CNN architectures for LiDAR data. Applying our automatically generated large-scale training data yields a boost of up to 14 percentage points compared to networks trained on manually annotated data only."
  },
  "eccv2018_w35_learningtolearnfromwebdatathroughdeepsemanticembeddings": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Learning to Learn from Web Data through Deep Semantic Embeddings",
    "authors": [
      "Raul Gomez",
      "Lluis Gomez",
      "Jaume Gibert",
      "Dimosthenis Karatzas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Gomez_Learning_to_Learn_from_Web_Data_through_Deep_Semantic_Embeddings_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Gomez_Learning_to_Learn_from_Web_Data_through_Deep_Semantic_Embeddings_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper we propose to learn a multimodal image and text embedding from Web and Social Media data, aiming to leverage the semantic knowledge learnt in the text domain and transfer it to a visual model for semantic image retrieval. We demonstrate that the pipeline can learn from images with associated text without supervision and perform a thorough analysis of five different text embeddings in three different benchmarks. We show that the embeddings learnt with Web and Social Media data have competitive performances over supervised methods in the text based image retrieval task, and we clearly outperform state of the art in the MIRFlickr dataset when training in the target data.Further we demonstrate how semantic multimodal image retrieval can be performed using the learnt embeddings, going beyond classical instance-level retrieval problems. Finally, we present a new dataset, InstaCities1M, composed by Instagram images and their associated texts that can be used for fair comparison of image-text embeddings."
  },
  "eccv2018_w35_learningfrombarcelonainstagramdatawhatlocalsandtouristspostaboutitsneighbourhoods": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Learning from Barcelona Instagram data what Locals and Tourists post about its Neighbourhoods",
    "authors": [
      "Raul Gomez",
      "Lluis Gomez",
      "Jaume Gibert",
      "Dimosthenis Karatzas"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Gomez_Learning_from_Barcelona_Instagram_data_what_Locals_and_Tourists_post_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Gomez_Learning_from_Barcelona_Instagram_data_what_Locals_and_Tourists_post_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Massive tourism is becoming a big problem for some cities, such as Barcelona, due to its concentration in some neighborhoods. In this work we gather Instagram data related to Barcelona consisting on images-captions pairs and, using the text as a supervisory signal, we learn relations between images, words and neighborhoods. Our goal is to learn which visual elements appear in photos when people is posting about each neighborhood. We perform a language separate treatment of the data and show that it can be extrapolated to a tourists and locals separate analysis, and that tourism is reflected in Social Media at a neighborhood level. The presented pipeline allows analyzing the differences between the images that tourists and locals associate to the different neighborhoods.The proposed method, which can be extended to other cities or subjects, proves that Instagram data can be used to train multi-modal (image and text) machine learning models that are useful to analyze publications about a city at a neighborhood level. We publish the collected dataset, InstaBarcelona and the code used in the analysis."
  },
  "eccv2018_w35_astructuredlistwiseapproachtolearningtorankforimagetagging": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "A Structured Listwise Approach to Learning to Rank for Image Tagging",
    "authors": [
      "Jorge Sanchez",
      "Franco Luque",
      "Leandro Lichtensztein"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Sanchez_A_Structured_Listwise_Approach_to_Learning_to_Rank_for_Image_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Sanchez_A_Structured_Listwise_Approach_to_Learning_to_Rank_for_Image_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " With the growing quantity and diversity of publicly available image data, computer vision plays a crucial role in understanding and organizing visual information today. Image tagging models are very often used to make this data accessible and useful. Generating image labels and ranking them by their relevance to the visual content is still an open problem. In this work, we use a bilinear compatibility function inspired from zero-shot learning that allows us to rank tags according to their relevance to the image content. We propose a novel listwise structured loss formulation to learn it from data. We leverage captioned image data and propose different \"tags from captions\" schemes meant to capture user attention and intra-user agreement in a simple and effective manner. We evaluate our method on the COCO-Captions, PASCAL-sentences and MIRFlickr-25k datasets showing promising results."
  },
  "eccv2018_w35_visuallyindicatedsoundgenerationbyperceptuallyoptimizedclassification": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Visually Indicated Sound Generation by Perceptually Optimized Classification",
    "authors": [
      "Kan Chen",
      "Chuanxi Zhang",
      "Chen Fang",
      "Zhaowen Wang",
      "Trung Bui",
      "Ram Nevatia"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Chen_Visually_Indicated_Sound_Generation_by_Perceptually_Optimized_Classification_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Chen_Visually_Indicated_Sound_Generation_by_Perceptually_Optimized_Classification_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Visually indicated sound generation aims to predict visually consistent sound from the video content. Previous methods addressed this problem by creating a single generative model that ignores the distinctive characteristics of various sound categories. Nowadays, state-ofthe-art sound classification networks are available to capture semanticlevel information in audio modality, which can also serve for the purpose of visually indicated sound generation. In this paper, we explore generating fine-grained sound from a variety of sound classes, and leverage pre-trained sound classification networks to improve the audio generation quality. We propose a novel Perceptually Optimized Classification based Audio generation Network (POCAN), which generates sound conditioned on the sound class predicted from visual information. Additionally, a perceptual loss is calculated via a pre-trained sound classification network to align the semantic information between the generated sound and its ground truth during training. Experiments show that POCAN achieves significantly better results in visually indicated sound generation task on two datasets."
  },
  "eccv2018_w35_centralnetamultilayerapproachformultimodalfusion": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "CentralNet: a Multilayer Approach for Multimodal Fusion",
    "authors": [
      "Valentin Vielzeuf",
      "Alexis Lechervy",
      "Stephane Pateux",
      "Frederic Jurie"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Vielzeuf_CentralNet_a_Multilayer_Approach_for_Multimodal_Fusion_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Vielzeuf_CentralNet_a_Multilayer_Approach_for_Multimodal_Fusion_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " This paper proposes a novel multimodal fusion approach, aiming to produce best possible decisions by integrating information coming from multiple media. While most of the past multimodal approaches either work by projecting the features of different modalities into the same space, or by coordinating the representations of each modality through the use of constraints, our approach borrows from both visions. More specifically, assuming each modality can be processed by a separated deep convolutional network, allowing to take decisions independently from each modality, we introduce a central network linking the modality specific networks. This central network not only provides a common feature embedding but also regularizes the modality specific networks through the use of multi-task learning. The proposed approach is validated on 4 different computer vision tasks on which it consistently improves the accuracy of existing multimodal fusion approaches."
  },
  "eccv2018_w35_whereandwhatamieating?image-basedfoodmenurecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Where and What Am I Eating? Image-based Food Menu Recognition",
    "authors": [
      "Marc Bola nos",
      "Marc Valdivia",
      "Petia Radeva"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/nos_Where_and_What_Am_I_Eating_Image-based_Food_Menu_Recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/nos_Where_and_What_Am_I_Eating_Image-based_Food_Menu_Recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Food has become a very important aspect of our social activities. Since social networks and websites like Yelp appeared, their users have started uploading photos of their meals to the Internet. This phenomenon opens a whole world of possibilities for developing models for applying food analysis and recognition on huge amounts of real-world data. A clear application could consist in applying image food recognition by using the menu of the restaurants. Our model, based on Convolutional Neural Networks and Recurrent Neural Networks, is able to learn a language model that generalizes on never seen dish names without the need of re-training it. According to the Ranking Loss metric, the results obtained by the model improve the baseline by a 15%."
  },
  "eccv2018_w35_thermalganmultimodalcolor-to-thermalimagetranslationforpersonre-identificationinmultispectraldataset": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "ThermalGAN: Multimodal Color-to-Thermal Image Translation for Person Re-Identification in Multispectral Dataset",
    "authors": [
      "Vladimir V. Kniaz",
      "Vladimir A. Knyaz",
      "Jiri Hladuvka",
      "Walter G. Kropatsch",
      "Vladimir Mizginov"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Kniaz_ThermalGAN_Multimodal_Color-to-Thermal_Image_Translation_for_Person_Re-Identification_in_Multispectral_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Kniaz_ThermalGAN_Multimodal_Color-to-Thermal_Image_Translation_for_Person_Re-Identification_in_Multispectral_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a ThermalGAN framework for cross-modality color-thermal person re-identification (ReID). We use a stack of generative adversarial networks (GAN) to translate a single color probe image to a multimodal thermal probe set. We use thermal histograms and feature descriptors as a thermal signature. We collected a large-scale multispectral ThermalWorld dataset for extensive training of our GAN model. In total the dataset includes 20216 color-thermal image pairs, 516 person ID, and ground truth pixel-level object annotations. We made the dataset freely available4. We evaluate our framework on the ThermalWorld dataset to show that it delivers robust matching that competes and surpasses the state-of-the-art in cross-modality color-thermal ReID."
  },
  "eccv2018_w35_visual-semanticalignmentacrossdomainsusingasemi-supervisedapproach": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Visual-Semantic Alignment Across Domains Using a Semi-Supervised Approach",
    "authors": [
      "Angelo Carraggi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Carraggi_Visual-Semantic_Alignment_Across_Domains_Using_a_Semi-Supervised_Approach_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Carraggi_Visual-Semantic_Alignment_Across_Domains_Using_a_Semi-Supervised_Approach_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Visual-semantic embeddings have been extensively used as a powerful model for cross-modal retrieval of images and sentences. In this setting, data coming from different modalities can be projected in a common embedding space, in which distances can be used to infer the similarity between pairs of images and sentences. While this approach has shown impressive performances on fully supervised settings, its application to semi-supervised scenarios has been rarely investigated. In this paper we propose a domain adaptation model for cross-modal retrieval, in which the knowledge learned from a supervised dataset can be transferred on a target dataset in which the pairing between images and sentences is not known, or not useful for training due to the limited size of the set. Experiments are performed on two target unsupervised scenarios, respectively related to the fashion and cultural heritage domain. Results show that our model is able to effectively transfer the knowledge learned on ordinary visual-semantic datasets, achieving promising results. As an additional contribution, we collect and release the dataset used for the cultural heritage domain."
  },
  "eccv2018_w35_generalizedbayesiancanonicalcorrelationanalysiswithmissingmodalities": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Generalized Bayesian Canonical Correlation Analysis with Missing Modalities",
    "authors": [
      "Toshihiko Matsuura",
      "Kuniaki Saito",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Matsuura_Generalized_Bayesian_Canonical_Correlation_Analysis_with_Missing_Modalities_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Matsuura_Generalized_Bayesian_Canonical_Correlation_Analysis_with_Missing_Modalities_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Multi-modallearningaimstobuildmodelsthatcanrelateinformation from multiple modalities. One challenge of multi-modal learning is the prediction of a target modality based on a set of multiple modalities. However, there are two challenges associated with the goal: Firstly, collecting a large, complete dataset containing all required modalities is difficult; some of the modalities can be missing. Secondly, the features of modalities are likely to be high dimensional and noisy. To deal with these challenges, we propose a method called Generalized Bayesian Canonical Correlation Analysis with Missing Modalities. This method can utilize the incomplete sets of modalities. By including them in the likelihood function during training, it can estimate the relationships among the non-missing modalities and the feature space in the non-missing modality accurately. In addition, this method can work well on high dimensional and noisy features of modalities. This is because, by a probabilistic model based on the prior knowledge, it is strong against outliers and can reduce the amount of data necessary for the model learning even if features of modalities are high dimensional. Experiments with artificial and real data demonstrate our method outperforms conventional methods."
  },
  "eccv2018_w35_unpairedthermaltovisiblespectrumtransferusingadversarialtraining": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - 1st Multimodal Learning and Applications Workshop",
    "title": "Unpaired Thermal to Visible Spectrum Transfer using Adversarial Training",
    "authors": [
      "Adam Nyberg",
      "Abdelrahman Eldesokey",
      "David Bergstrom",
      "David Gustafsson"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w35/html/Nyberg_Unpaired_Thermal_to_Visible_Spectrum_Transfer_using_Adversarial_Training_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Nyberg_Unpaired_Thermal_to_Visible_Spectrum_Transfer_using_Adversarial_Training_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Thermal Infrared (TIR) cameras are gaining popularity in many computer vision applications due to their ability to operate under low-light conditions. Images produced by TIR cameras are usually difficult for humans to perceive visually, which limits their usability. Several methods in the literature were proposed to address this problem by transforming TIR images into realistic visible spectrum (VIS) images. However, existing TIR-VIS datasets suffer from imperfect alignment between TIR-VIS image pairs which degrades the performance of supervised methods. We tackle this problem by learning this transformation using an unsupervised Generative Adversarial Network (GAN) which trains on unpaired TIR and VIS images. When trained and evaluated on KAIST-MS dataset, our proposed methods was shown to produce significantly more realistic and sharp VIS images than the existing stateof-the-art supervised methods. In addition, our proposed method was shown to generalize very well when evaluated on a new dataset of new environments."
  },
  "eccv2018_w36_devondeformablevolumenetworkforlearningopticalflow": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - What is Optical Flow for?",
    "title": "Devon: Deformable Volume Network for Learning Optical Flow",
    "authors": [
      "Yao Lu",
      "Jack Valmadre",
      "Heng Wang",
      "Juho Kannala",
      "Mehrtash Harandi",
      "Philip H. S. Torr"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w36/html/Lu_Devon_Deformable_Volume_Network_for_Learning_Optical_Flow_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Lu_Devon_Deformable_Volume_Network_for_Learning_Optical_Flow_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " We propose a new neural network module, Deformable Cost Volume, for learning large displacement optical flow. The module does not distort the original images or their feature maps and therefore avoids the artifacts associated with warping. Based on this module, a new neural network model is proposed. The full version of this paper can be found online."
  },
  "eccv2018_w36_usingphaseinsteadofopticalflowforactionrecognition": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - What is Optical Flow for?",
    "title": "Using phase instead of optical flow for action recognition",
    "authors": [
      "Omar Hommos",
      "Silvia L. Pintea",
      "Pascal S.M. Mettes",
      "Jan C. van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w36/html/Hommos_Using_phase_instead_of_optical_flow_for_action_recognition_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Hommos_Using_phase_instead_of_optical_flow_for_action_recognition_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Currently, the most common motion representation for action recognition is optical flow. Optical flow is based on particle tracking which adheres to a Lagrangian perspective on dynamics. In contrast to the Lagrangian perspective, the Eulerian model of dynamics does not track, but describes local changes. For video, an Eulerian phase-based motion representation, using complex steerable filters, has been successfully employed recently for motion magnification and video frame interpolation. Inspired by these previous works, here, we proposes learning Eulerian motion representations in a deep architecture for action recognition. We learn filters in the complex domain in an end-to-end manner. We design these complex filters to resemble complex Gabor filters, typically employed for phase-information extraction. We propose a phaseinformation extraction module, based on these complex filters, that can be used in any network architecture for extracting Eulerian representations. We experimentally analyze the added value of Eulerian motion representations, as extracted by our proposed phase extraction module, and compare with existing motion representations based on optical flow, on the UCF101 dataset."
  },
  "eccv2018_w36_eventextractionusingtransportationoftemporalopticalflowfields": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - What is Optical Flow for?",
    "title": "Event Extraction Using Transportation of Temporal Optical Flow Fields",
    "authors": [
      "Itaru Gotoh",
      "Hiroki Hiraoka",
      "Atsushi Imiya"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w36/html/Gotoh_Event_Extraction_Using_Transportation_of_Temporal_Optical_Flow_Fields_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Gotoh_Event_Extraction_Using_Transportation_of_Temporal_Optical_Flow_Fields_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this paper, we develop a method to transform a sequence of images to a sequence of events. Optical flow, which is the vector fields of pointwise motion computed from monocular image sequences, describes pointwise motion in an environment. The method extracts the global smoothness and continuity of motion fields and detects collapses of the smoothness of the motion fields in long-time image sequences using transportation of the temporal optical flow field."
  },
  "eccv2018_w36_asimpleandeffectivefusionapproachformulti-frameopticalflowestimation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - What is Optical Flow for?",
    "title": "A Simple and Effective Fusion Approach for Multi-Frame Optical Flow Estimation",
    "authors": [
      "Zhile Ren",
      "Orazio Gallo",
      "Deqing Sun",
      "Ming-Hsuan Yang",
      "Erik B. Sudderth",
      "Jan Kautz"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w36/html/Ren_A_Simple_and_Effective_Fusion_Approach_for_Multi-Frame_Optical_Flow_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Ren_A_Simple_and_Effective_Fusion_Approach_for_Multi-Frame_Optical_Flow_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " To date, top-performing optical flow estimation methods only take pairs of consecutive frames into account. While elegant and appealing, the idea of using more than two frames has not yet produced state-of-the-art results. We present a simple, yet effective fusion approach for multi-frame optical flow that benefits from longer-term temporal cues. Our method first warps the optical flow from previous frames to the current, thereby yielding multiple plausible estimates. It then fuses the complementary information carried by these estimates into a new optical flow field. At the time of writing, our method ranks first among published results in the MPI Sintel and KITTI 2015 benchmarks."
  },
  "eccv2018_w36_unsupervisedevent-basedopticalflowusingmotioncompensation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - What is Optical Flow for?",
    "title": "Unsupervised Event-based Optical Flow using Motion Compensation",
    "authors": [
      "Alex Zihao Zhu",
      "Liangzhe Yuan",
      "Kenneth Chaney",
      "Kostas Daniilidis"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w36/html/Zhu_Unsupervised_Event-based_Optical_Flow_using_Motion_Compensation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Zhu_Unsupervised_Event-based_Optical_Flow_using_Motion_Compensation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " In this work, we propose a novel framework for unsupervised learning for event cameras that learns to predict optical flow from only the event stream. In particular, we propose an input representation of the events in the form of a discretized 3D volume, which we pass through a neural network to predict the optical flow for each event. This optical flow is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We evaluate this network on the Multi Vehicle Stereo Event Camera dataset (MVSEC), along with qualitative results from a variety of different scenes."
  },
  "eccv2018_w36_moa-netself-supervisedmotionsegmentation": {
    "conf_id": "ECCV2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ECCV2018_workshops - What is Optical Flow for?",
    "title": "MoA-Net: Self-Supervised Motion Segmentation",
    "authors": [
      "Pia Bideau",
      "Rakesh R. Menon",
      "Erik Learned-Miller"
    ],
    "page_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_eccv_2018_workshops/w36/html/Bideau_MoA-Net_Self-Supervised_Motion_Segmentation_ECCVW_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ECCV2018_workshops/../content_ECCVW_2018/papers/11134/Bideau_MoA-Net_Self-Supervised_Motion_Segmentation_ECCVW_2018_paper.pdf",
    "published": "2018-09",
    "summary": " Most recent approaches to motion segmentation use optical flow to segment an image into the static environment and independently moving objects. Neural network based approaches usually require large amounts of labeled training data to achieve state-of-the-art performance. In this work we propose a new approach to train a motion segmentation network in a self-supervised manner. Inspired by visual ecology, the human visual system, and by prior approaches to motion modeling, we break down the problem of motion segmentation into two smaller subproblems: (1) modifying the flow field to remove the observer\u00e2\u0080\u0099s rotation and (2) segmenting the rotation-compensated flow into static environment and independently moving objects. Compensating for rotation leads to essential simplifications that allow us to describe an independently moving object with just a few criteria which can be learned by our new motion segmentation network the Motion Angle Network (MoANet). We compare our network with two other motion segmentation networks and show state-of-the-art performance on Sintel."
  }
}