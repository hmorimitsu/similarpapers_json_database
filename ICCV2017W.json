{
  "iccv2017_w1_solvinglargemulticutproblemsforconnectomicsviadomaindecomposition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Solving Large Multicut Problems for Connectomics via Domain Decomposition",
    "authors": [
      "Constantin Pape",
      "Thorsten Beier",
      "Peter Li",
      "Viren Jain",
      "Davi D. Bock",
      "Anna Kreshuk"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Pape_Solving_Large_Multicut_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Pape_Solving_Large_Multicut_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this contribution we demonstrate how a Multicut- based segmentation pipeline can be scaled up to datasets of hundreds of Gigabytes in size. Such datasets are preva- lent in connectomics, where neuron segmentation needs to be performed across very large electron microscopy image volumes. We show the advantages of a hierarchical block- wise scheme over local stitching strategies and evaluate the performance of different Multicut solvers for the segmenta- tion of the blocks in the hierarchy. We validate the accuracy of our algorithm on a small fully annotated dataset (5x5x5 mm) and demonstrate no significant loss in segmentation quality compared to solving the Multicut problem globally. We evaluate the scalability of the algorithm on a 95x60x60 mm image volume and show that solving the Multicut prob- lem is no longer the bottleneck of the segmentation pipeline.\r"
  },
  "iccv2017_w1_particletrackingaccuracymeasurementbasedoncomparisonoflinearorientedforests": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Particle Tracking Accuracy Measurement Based on Comparison of Linear Oriented Forests",
    "authors": [
      "Martin Maska",
      "Pavel Matula"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Maska_Particle_Tracking_Accuracy_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Maska_Particle_Tracking_Accuracy_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Particle tracking is of fundamental importance in diverse quantitative analyses of dynamic intracellular processes using time-lapse microscopy. Due to frequent impracticability of tracking particles manually, a number of fully automated algorithms have been developed over past decades, carrying out the tracking task in two subsequent phases: (1) particle detection and (2) particle linking. An objective benchmark for assessing the performance of such algorithms was recently established by the Particle Tracking Challenge. Because its performance evaluation protocol finds correspondences between a reference and algorithm-generated tracking result at the level of individual tracks, the performance assessment strongly depends on the algorithm linking capabilities. In this paper, we propose a novel performance evaluation protocol based on a simplified version of the tracking accuracy measure employed in the Cell Tracking Challenge, which establishes the correspondences at the level of individual particle detections, thus allowing one to evaluate the performance of each of the two phases in an isolated, unbiased manner. By analyzing the tracking results of all 14 algorithms competing in the Particle Tracking Challenge using the proposed evaluation protocol, we reveal substantial changes in their detection and linking performance, yielding rankings different from those reported previously.\r"
  },
  "iccv2017_w1_count-ceptioncountingbyfullyconvolutionalredundantcounting": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Count-ception: Counting by Fully Convolutional Redundant Counting",
    "authors": [
      "Joseph Paul Cohen",
      "Genevieve Boucher",
      "Craig A. Glastonbury",
      "Henry Z. Lo",
      "Yoshua Bengio"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Cohen_Count-ception_Counting_by_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Cohen_Count-ception_Counting_by_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% relative improvement (2.9 to 2.3 MAE) over the state of the art method by Xie, Noble, and Zisserman in 2016.\r"
  },
  "iccv2017_w1_dualstructuredconvolutionalneuralnetworkwithfeatureaugmentationforquantitativecharacterizationoftissuehistology": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Dual Structured Convolutional Neural Network With Feature Augmentation for Quantitative Characterization of Tissue Histology",
    "authors": [
      "Mira Valkonen",
      "Kimmo Kartasalo",
      "Kaisa Liimatainen",
      "Matti Nykter",
      "Leena Latonen",
      "Pekka Ruusuvuori"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Valkonen_Dual_Structured_Convolutional_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Valkonen_Dual_Structured_Convolutional_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a dual convolutional neural network (dCNN) architecture for extracting multi-scale features from histological tissue images for the purpose of automated characterization of tissue in digital pathology. The dual structure consists of two identical convolutional neural networks applied to input images with different scales, that are merged together and stacked with two fully connected layers. It has been acknowledged that deep networks can be used to extract higher-order features, and therefore, the network output at final fully connected layer was used as a deep dCNN feature vector. Further, engineered features, shown in previous studies to capture important characteristics of tissue structure and morphology, were integrated to the feature extractor module. The acquired quantitative feature representation can be further utilized to train a discriminative model for classifying tissue types. Machine learning based methods for detection of regions of interest, or tissue type classification will advance the transition to decision support systems and computer aided diagnosis in digital pathology. Here we apply the proposed feature-augmented dCNN method with supervised learning in detecting cancerous tissue from whole slide images. The extracted quantitative representation of tissue histology was used to train a logistic regression model with elastic net regularization. The model was able to accurately discriminate cancerous tissue from normal tissue, resulting in blockwise AUC=0.97, where the total number of analyzed tissue blocks was approximately 8.3 million that constitute the test set of 75 whole slide images.\r"
  },
  "iccv2017_w1_spheroidsegmentationusingmultiscaledeepadversarialnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Spheroid Segmentation Using Multiscale Deep Adversarial Networks",
    "authors": [
      "Sajith Kecheril Sadanandan",
      "Johan Karlsson",
      "Carolina Wahlby"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Sadanandan_Spheroid_Segmentation_Using_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Sadanandan_Spheroid_Segmentation_Using_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this work, we segment spheroids with different sizes, shapes, and illumination conditions from bright-field microscopy images. To segment the spheroids we create a novel multiscale deep adversarial network with different deep feature extraction layers at different scales. We show that linearly increasing the adversarial loss contribution results in a stable segmentation algorithm for our dataset. We qualitatively and quantitatively compare the performance of our deep adversarial network with two other networks without adversarial losses. We show that our deep adversarial network performs better than the other two networks at segmenting the spheroids from our 2D bright-field microscopy images.\r"
  },
  "iccv2017_w1_spatially-variantkernelforopticalflowunderlowsignal-to-noiseratiosapplicationtomicroscopy": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Spatially-Variant Kernel for Optical Flow Under Low Signal-To-Noise Ratios: Application to Microscopy",
    "authors": [
      "Denis Fortun",
      "Noemi Debroux",
      "Charles Kervrann"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Fortun_Spatially-Variant_Kernel_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Fortun_Spatially-Variant_Kernel_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Local and global approaches can be identified as the two main classes of optical flow estimation methods. In this paper, we propose a framework to combine the advantages of these two principles, namely robustness to noise of the local approach and discontinuity preservation of the global approach. This is particularly crucial in biological imaging, where the noise produced by microscopes is one of the main issues for optical flow estimation. The idea is to adapt spatially the local support of the local parametric constraint in the combined local-global model [Bruhn et al. 2005]. To this end, we jointly estimate the motion field and the parameters of the spatial support. We apply our approach to the case of Gaussian filtering, and we derive efficient minimization schemes for usualdata terms. The estimation of a spatially varying standard deviation map prevents from the smoothing of motion discontinuities, while ensuring robustness to noise. We validate our method for a standard model and demonstrate how a baseline approach with pixel-wise data term can be improved when integrated in our framework. The method is evaluated on the Middlebury benchmark with ground truth and on real fluorescence microscopy data. \r"
  },
  "iccv2017_w1_discoveryofrarephenotypesincellularimagesusingweaklysuperviseddeeplearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Discovery of Rare Phenotypes in Cellular Images Using Weakly Supervised Deep Learning",
    "authors": [
      "Heba Sailem",
      "Mar Arias-Garcia",
      "Chris Bakal",
      "Andrew Zisserman",
      "Jens Rittscher"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Sailem_Discovery_of_Rare_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Sailem_Discovery_of_Rare_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " High-throughput microscopy generates a massive amount of images that enables the identification of biological phenotypes resulting from thousands of different genetic or pharmacological perturbations. However, the size of the datasets generated by these studies makes it almost impossible to provide detailed image annotations, e.g. by object bounding box. Furthermore, the variability in cellular responses often results in weak phenotypes that only manifest in a subpopulation of cells. To overcome the burden of providing object-level annotations we propose a deep learning approach that can detect the presence or absence of rare cellular phenotypes from weak annotations. Although, no localization information is provided we demonstrate that our Weakly Supervised Convolutional Neural Network (WSCNN) can reliably estimate the location of the identified rare events. Results on synthetic data set and a data set containing genetically perturbed cells demonstrate the power of our proposed approach.\r"
  },
  "iccv2017_w1_towardsaspatio-temporalatlasof3dcellularparametersduringleafmorphogenesis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Towards a Spatio-Temporal Atlas of 3D Cellular Parameters During Leaf Morphogenesis",
    "authors": [
      "Faical Selka",
      "Thomas Blein",
      "Jasmine Burguet",
      "Eric Biot",
      "Patrick Laufs",
      "Philippe Andrey"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Selka_Towards_a_Spatio-Temporal_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Selka_Towards_a_Spatio-Temporal_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Morphogenesis is a complex process that integrates several mechanisms from the molecular to the organ scales. In plants, division and growth are the two fundamental cellular mechanisms that drive morphogenesis. However, little is known about how these mechanisms are coordinated to establish functional tissue structure. A fundamental bottleneck is the current lack of techniques to systematically quantify the spatio-temporal evolution of 3D cell morphology during organ growth. Using leaf development as a relevant and challenging model to study morphogenesis, we developed a computational framework for cell analysis and quantification from 3D images and for the generation of 3D cell shape atlas. A remarkable feature of leaf morphogenesis being the formation of a laminar-like structure, we propose to automatically separate the cells corresponding to the leaf sides in the segmented leaves, by applying a clustering algorithm. The performance of the proposed pipeline was experimentally assessed on a dataset of 46 leaves in an early developmental state.\r"
  },
  "iccv2017_w1_towardsvirtualh&estainingofhyperspectrallunghistologyimagesusingconditionalgenerativeadversarialnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Towards Virtual H&E Staining of Hyperspectral Lung Histology Images Using Conditional Generative Adversarial Networks",
    "authors": [
      "Neslihan Bayramoglu",
      "Mika Kaakinen",
      "Lauri Eklund",
      "Janne Heikkila"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Bayramoglu_Towards_Virtual_HE_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Bayramoglu_Towards_Virtual_HE_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The microscopic image of a specimen in the absence of staining appears colorless and textureless. Therefore, microscopic inspection of tissue requires chemical staining to create contrast. Hematoxylin and eosin (H&E) is the most widely used chemical staining technique in histopathology. However, such staining creates obstacles for automated image analysis systems. Due to different chemical formulations, different scanners, section thickness, and lab protocols, similar tissues can greatly differ in appearance. This huge variability is one of the main challenges in designing robust and resilient automated image analysis systems. Moreover, staining process is time consuming and its chemical effects deform structures of specimens.In this work, we develop a method to virtually stain unstained specimens. Our method utilizes dimension reduction and conditional adversarial generative networks (cGANs) which build highly non-linear mappings between input and output images. Conditional GANs ability to handle very complex functions and high dimensional data enables transforming unstained hyperspectral tissue image to their H&E equivalent which comprises highly diversified appearance. In the long term, such virtual digital H&E staining could automate some of the tasks in the diagnostic pathology workflow which could be used to speed up the sample processing time, reduce costs, prevent adverse effects of chemical stains on tissue specimens, reduce observer variability, and increase objectivity in disease diagnosis.\r"
  },
  "iccv2017_w1_siamesenetworksforchromosomeclassification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Siamese Networks for Chromosome Classification",
    "authors": [
      "Swati Jindal",
      "Gaurav Gupta",
      "Mohit Yadav",
      "Monika Sharma",
      "Lovekesh Vig"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Jindal_Siamese_Networks_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Jindal_Siamese_Networks_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Karyotying is the process of pairing and ordering 23 pairs of human chromosomes on the basis of size, position of centromere, and banding patterns. Karyotyping during metaphase is often used by clinical cytogeneticists to analyze human chromosomes for diagnostic purposes. It requires experience, domain expertise and considerable manual effort to efficiently perform karyotyping and diagnosis of various disorders. Therefore, automation or even partial automation is highly desirable to assist technicians and reduce the cognitive load necessary for karyotyping. With these motivations, in this paper, we attempt to develop methods for chromosome classification by borrowing the latest ideas from deep learning. More specifically, we perform straightening on chromosomes and feed them into Siamese Networks aimed to closely collate the embeddings of samples coming from the similar label pair. Furthermore, we suggest to perform balanced sampling over datasets while selecting dissimilar training pairs for Siamese Networks, and an MLP based prediction on the top of embeddings obtained from trained Siamese networks. We perform our experiments on a real world dataset of healthy patients collected from a hospital. Also, we exhaustively compare the effect of different straightening techniques, on applying them to chromosome images prior to classification. Results demonstrate that the proposed methods speed up both training and prediction by 83 and 3 folds, respectively; while surpassing the performance of a very competitive baseline created utilizing deep convolutional neural networks.\r"
  },
  "iccv2017_w1_deepconvolutionalneuralnetworksfordetectingcellularchangesduetomalignancy": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Deep Convolutional Neural Networks for Detecting Cellular Changes Due to Malignancy",
    "authors": [
      "Hakan Wieslander",
      "Gustav Forslid",
      "Ewert Bengtsson",
      "Carolina Wahlby",
      "Jan-Michael Hirsch",
      "Christina Runow Stark",
      "Sajith Kecheril Sadanandan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Wieslander_Deep_Convolutional_Neural_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Wieslander_Deep_Convolutional_Neural_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Discovering cancer at an early stage is an effective way to increase the chance of survival. However, since most screening processes are done manually it is time inefficient and thus a costly process. One way of automizing the screening process could be to classify cells using Convolutional Neural Networks. Convolutional Neural Networks have been proven to be accurate for image classification tasks. Two datasets containing oral cells and two datasets containing cervical cells were used. For the cervical cancer dataset the cells were classified by medical experts as normal or abnormal. For the oral cell dataset we only used the diagnosis of the patient. All cells obtained from a patient with malignancy were thus considered malignant even though most of them looked normal. The performance was evaluated for two different network architectures, ResNet and VGG. For the oral datasets the accuracy varied between 78-82% correctly classified cells depending on the dataset and network. For the cervical datasets the accuracy varied between 84-86% correctly classified cells depending on the dataset and network. The results indicate a high potential for detecting abnormalities in oral cavity and in uterine cervix. ResNet was shown to be the preferable network, with a higher accuracy and a smaller standard deviation.\r"
  },
  "iccv2017_w1_synthesisingwiderfieldimagesfromnarrow-fieldretinalvideoacquiredusingalow-costdirectophthalmoscope(arclight)attachedtoasmartphone": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Synthesising Wider Field Images From Narrow-Field Retinal Video Acquired Using a Low-Cost Direct Ophthalmoscope (Arclight) Attached to a Smartphone",
    "authors": [
      "Keylor Daniel Chaves Viquez",
      "Ognjen Arandjelovic",
      "Andrew Blaikie",
      "In Ae Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Viquez_Synthesising_Wider_Field_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Viquez_Synthesising_Wider_Field_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Access to low cost retinal imaging devices in low and middle income countries is limited, compromising progress in preventing needless blindness. The Arclight is a recently developed low-cost solar powered direct ophthalmoscope which can be attached to the camera of a smartphone to acquire retinal images and video. However, the acquired data is inherently limited by the optics of direct ophthalmoscopy, resulting in a narrow field of view with associated corneal reflections, limiting its usefulness. In this work we describe the first fully automatic method utilizing videos acquired using the Arclight attached to a mobile phone camera to create wider view, higher quality still images comparable with images obtained using much more expensive and bulky dedicated traditional retinal cameras.\r"
  },
  "iccv2017_w1_virtualbloodvesselsincomplexbackgroundusingstereox-rayimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Virtual Blood Vessels in Complex Background Using Stereo X-Ray Images",
    "authors": [
      "Qiuyu Chen",
      "Ryoma Bise",
      "Lin Gu",
      "Yinqiang Zheng",
      "Imari Sato",
      "Jenq-Neng Hwang",
      "Nobuaki Imanishi",
      "Sadakazu Aiso"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Chen_Virtual_Blood_Vessels_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Chen_Virtual_Blood_Vessels_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a fully automatic system to reconstruct and visualize 3D blood vessels in Augmented Reality (AR) system from stereo X-ray images with bones and body fat. Currently, typical 3D imaging technologies are expensive and carries the risk of over irradiation exposure. In reduce the potential harm, we only need to take two X-ray images before visualizing the vessels. Our system can effectivelyreconstruct and visualizevessels in following steps. We first conduct initial segmentation using Markov Random Field and then refine segmentation in an entropy based post-process. We parse the segmented vessels by extracting their centerlines and generating trees. We propose a coarse-to-fine scheme for stereo matching, including initial matching using affine transform and dense matching using Hungarian algorithm guided by Gaussian Regression. Finally, we render and visualize the reconstructed model in a HoloLens Based AR system, which can essentially change the way of visualizing medical data. We have evaluated its performance by using synthetic and real stereo X-ray images, and achieved satisfactory quantitative and qualitative results. \r"
  },
  "iccv2017_w1_part-to-wholeregistrationofhistologyandmriusingshapeelements": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Part-To-Whole Registration of Histology and MRI Using Shape Elements",
    "authors": [
      "Jonas Pichat",
      "Eugenio Iglesias",
      "Sotiris Nousias",
      "Tarek Yousry",
      "Sebastien Ourselin",
      "Marc Modat"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Pichat_Part-To-Whole_Registration_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Pichat_Part-To-Whole_Registration_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Image registration between histology and magnetic resonance imaging (MRI) is a challenging task due to differences in structural content and contrast.Too thick and wide specimens cannot be processed all at once and must be cut into smaller pieces. This dramatically increases the complexity of the problem, since each piece should be individually and manually pre-aligned. To the best of our knowledge, no automatic method can reliably locate such piece of tissue within its respective whole in the MRI slice, and align it without any prior information. We propose here a novel automatic approach to the joint problem of multimodal registration between histology and MRI, when only a fraction of tissue is available from histology. The approach relies on the representation of images using their level lines so as to reach contrast invariance. Shape elements obtained via the extraction of bitangents are encoded in a projective-invariant manner, which permits the identification of common pieces of curves between two images. We evaluated the approach on human brain histology and compared resulting alignments against manually annotated ground truths. Considering the complexity of the brain folding patterns, preliminary results are promising and suggest the use of characteristic and meaningful shape elements for improved robustness and efficiency.\r"
  },
  "iccv2017_w1_computer-automatedmalariadiagnosisandquantitationusingconvolutionalneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Computer-Automated Malaria Diagnosis and Quantitation Using Convolutional Neural Networks",
    "authors": [
      "Courosh Mehanian",
      "Mayoore Jaiswal",
      "Charles Delahunt",
      "Clay Thompson",
      "Matt Horning",
      "Liming Hu",
      "Travis Ostbye",
      "Shawn McGuire",
      "Martha Mehanian",
      "Cary Champlin",
      "Ben Wilson",
      "Earl Long",
      "Stephane Proux",
      "Dionicia Gamboa",
      "Peter Chiodini",
      "Jane Carter",
      "Mehul Dhorda",
      "David Isaboke",
      "Bernhards Ogutu",
      "Wellington Oyibo",
      "Elizabeth Villasis",
      "Kyaw Myo Tun",
      "Christine Bachman",
      "David Bell"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Mehanian_Computer-Automated_Malaria_Diagnosis_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Mehanian_Computer-Automated_Malaria_Diagnosis_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The optical microscope remains a widely-used tool for diagnosis and quantitation of malaria. An automated system that can match the performance of well-trained technicians is motivated by a shortage of trained microscopists. We have developed a computer vision system that leverages deep learning to identify malaria parasites in micrographs of standard, field-prepared thick blood films. The prototype application diagnoses P. falciparum with sufficient accuracy to achieve competency level 1 in the World Health Organization external competency assessment, and quantitates with sufficient accuracy for use in drug resistance studies. A suite of new computer vision techniques--global white balance, adaptive nonlinear grayscale, and a novel augmentation scheme--underpin the system's state-of-the-art performance. We outline a rich, global training set; describe the algorithm in detail; argue for patient-level performance metrics for the evaluation of automated diagnosis methods; and provide results for P. falciparum.\r"
  },
  "iccv2017_w1_automatic3dsingleneuronreconstructionwithexhaustivetracing": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Automatic 3D Single Neuron Reconstruction With Exhaustive Tracing",
    "authors": [
      "Zihao Tang",
      "Donghao Zhang",
      "Siqi Liu",
      "Yang Song",
      "Hanchuan Peng",
      "Weidong Cai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Tang_Automatic_3D_Single_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Tang_Automatic_3D_Single_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The digital reconstruction of neuronal morphology from single neurons, also called neuron tracing, is a crucial process to gain a better understanding of the relationship and connections in neuronal networks. However, the fully automation of neuron tracing remains a big challenge due to the biological diversity of the neuronal morphology, varying image qualities captured by different microscopes and large-scale nature of neuron image datasets. A common phenomenon in the low quality neuron images is the broken structures. To tackle this problem, we propose a novel automatic 3D neuron reconstruction framework named exhaustive tracing including distance transform, optimally oriented flux filter, fast-marching and hierarchical pruning. The proposed exhaustive tracing algorithm shows a robust capability of striding over large gaps in the low quality neuron images. It outperforms state-of-the-art neuron tracing algorithms by evaluating the tracing results on the large-scale First-2000 dataset and Gold dataset.\r"
  },
  "iccv2017_w1_botsforsoftware-assistedanalysisofimage-basedtranscriptomics": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Bio-Image Computing",
    "title": "Bots for Software-Assisted Analysis of Image-Based Transcriptomics",
    "authors": [
      "Marcelo Cicconet",
      "Daniel R. Hochbaum",
      "David L. Richmond",
      "Bernardo L. Sabatini"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w1/html/Cicconet_Bots_for_Software-Assisted_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w1/Cicconet_Bots_for_Software-Assisted_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We introduce software assistants -- bots -- for the task of analyzing image-based transcriptomic data. The key steps in this process are detecting nuclei, and counting associated puncta corresponding to labeled RNA. Our main release offers two algorithms for nuclei segmentation, and two for spot detection, to handle data of different complexities. For challenging nuclei segmentation cases, we enable the user to train a stacked Random Forest, which includes novel circularity features that leverage prior knowledge regarding nuclei shape for better instance segmentation. This machine learning model can be trained on a modern CPU-only computer, yet performs comparably with respect to a more hardware-demanding state-of-the-art deep learning approach, as demonstrated through experiments. While the primary motivation for the bots was image-based transcriptomics, we also demonstrate their applicability to the more general problem of scoring 'spots' in nuclei.\r"
  },
  "iccv2017_w2_avariationalstudyonbrdfreconstructioninastructuredlightscanner": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Data-Driven BxDF Models for Computer Vision Applications",
    "title": "A Variational Study on BRDF Reconstruction in a Structured Light Scanner",
    "authors": [
      "Jannik Boll Nielsen",
      "Jonathan Dyssel Stets",
      "Rasmus Ahrenkiel Lyngby",
      "Henrik Aanaes",
      "Anders Bjorholm Dahl",
      "Jeppe Revall Frisvad"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w2/html/Nielsen_A_Variational_Study_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w2/Nielsen_A_Variational_Study_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Time-efficient acquisition of reflectance behavior together with surface geometry is a challenging problem. In this study, we investigate the impact of system parameter uncertainties when incorporating a data-driven BRDF reconstruction approach into the standard pipeline of a structured light scanning system. The parameters investigated include geometric detail of scanned objects; vertex positions and normals; and position and intensity of light sources. To have full control of uncertainties, experiments are carried out in a simulated environment, mimicking an actual structured light scanning setup. Results show that while uncertainties in vertex positions and normals have a high impact on the quality of reconstructed BRDFs, object geometry and light source properties have very little influence on the reconstructed BRDFs. With this analysis, practitioners now have insight in the tolerances required for accurate BRDF acquisition to work.\r"
  },
  "iccv2017_w2_efficientbrdfsamplingusingprojecteddeviationvectorparameterization": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Data-Driven BxDF Models for Computer Vision Applications",
    "title": "Efficient BRDF Sampling Using Projected Deviation Vector Parameterization",
    "authors": [
      "Tanaboon Tongbuasirilai",
      "Jonas Unger",
      "Murat Kurt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w2/html/Tongbuasirilai_Efficient_BRDF_Sampling_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w2/Tongbuasirilai_Efficient_BRDF_Sampling_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents a novel approach for efficient sampling of isotropic Bidirectional Reflectance Distribution Functions (BRDFs). Our approach builds upon a new parameterization, the Projected Deviation Vector parameterization, in which isotropic BRDFs can be described by two 1D functions. We show that BRDFs can be efficiently and accurately measured in this space using simple mechanical measurement setups. To demonstrate the utility of our approach, we perform a thorough numerical evaluation and show that the BRDFs reconstructed from measurements along the two 1D bases produce rendering results that are visually comparable to the reference BRDF measurements which are densely sampled over the 4D domain described by the standard hemispherical parameterization.\r"
  },
  "iccv2017_w2_modelingtheanisotropicreflectanceofasurfacewithmicrostructureengineeredtoobtainvisiblecontrastafterrotation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Data-Driven BxDF Models for Computer Vision Applications",
    "title": "Modeling the Anisotropic Reflectance of a Surface With Microstructure Engineered to Obtain Visible Contrast After Rotation",
    "authors": [
      "Andrea Luongo",
      "Viggo Falster",
      "Mads Brix Doest",
      "Dongya Li",
      "Francesco Regi",
      "Yang Zhang",
      "Guido Tosello",
      "Jannik Boll Nielsen",
      "Henrik Aanaes",
      "Jeppe Revall Frisvad"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w2/html/Luongo_Modeling_the_Anisotropic_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w2/Luongo_Modeling_the_Anisotropic_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Engineering of surface structure to obtain specific anisotropic reflectance properties has interesting applications in large scale production of plastic items. In recent work, surface structure has been engineered to obtain visible reflectance contrast when observing a surface before and after rotating it 90 degrees around its normal axis. We build an analytic anisotropic reflectance model based on the microstructure engineered to obtain such contrast. Using our model to render synthetic images, we predict the above mentioned contrasts and compare our predictions with the measurements reported in previous work. The benefit of an analytical model like the one we provide is its potential to be used in computer vision for estimating the quality of a surface sample. The quality of a sample is indicated by the resemblance of camera-based contrast measurements with contrasts predicted for an idealized surface structure. Our predictive model is also useful in optimization of the microstructure configuration, where the objective for example could be to maximize reflectance contrast.\r"
  },
  "iccv2017_w3_fusinggeometryandappearanceforroadsegmentation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Fusing Geometry and Appearance for Road Segmentation",
    "authors": [
      "Gong Cheng",
      "Yiming Qian",
      "James H. Elder"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Cheng_Fusing_Geometry_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Cheng_Fusing_Geometry_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a novel method for fusing geometric and appearance cues for road surface segmentation. Modeling colour cues using Gaussian mixtures allows the fusion to be performed optimally within a Bayesian framework, avoiding ad hoc weights.Adaptation to different scene conditions is accomplished through nearest-neighbour appearance model selection over a dictionary of mixture models learned from training data, and the thorny problem of selecting the number of components in each mixture is solved through a novel cross-validation approach.Quantitative evaluation reveals that the proposed fusion method significantly improves segmentation accuracy relative to a method that uses geometric cues alone. \r"
  },
  "iccv2017_w3_distantlysupervisedroadsegmentation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Distantly Supervised Road Segmentation",
    "authors": [
      "Satoshi Tsutsui",
      "Tommi Kerola",
      "Shunta Saito"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Tsutsui_Distantly_Supervised_Road_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Tsutsui_Distantly_Supervised_Road_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present an approach for road segmentation that only requires image-level annotations at training time. We leverage distant supervision, which allows us to train our model using images that are different from the target domain. Using large publicly available image databases as distant supervisors, we develop a simple method to automatically generate weak pixel-wise road masks. These are used to iteratively train a fully convolutional neural network, which produces our final segmentation model. We evaluate our method on the Cityscapes dataset, where we compare it with a fully supervised approach. Further, we discuss the trade-off between annotation cost and performance. Overall, our distantly supervised approach achieves 93.8% of the performance of the fully supervised approach, while using orders of magnitude less annotation work.\r"
  },
  "iccv2017_w3_detectingnonexistentpedestrians": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Detecting Nonexistent Pedestrians",
    "authors": [
      "Jui-Ting Chien",
      "Chia-Jung Chou",
      "Ding-Jie Chen",
      "Hwann-Tzong Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Chien_Detecting_Nonexistent_Pedestrians_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Chien_Detecting_Nonexistent_Pedestrians_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We explore beyond object detection and semantic segmentation, and propose to address the problem of estimating the presence probabilities of nonexistent pedestrians in a street scene. Our method builds upon a combination of generative and discriminative procedures to achieve the perceptual capability of figuring out missing visual information. We adopt state-of-the-art inpainting techniques to generate the training data for nonexistent pedestrian detection. The learned detector can predict the probability of observing a pedestrian at some location in image, even if that location exhibits only the background. We evaluate our method by inserting pedestrians into images according to the presence probabilities and conducting user study to determine the `realisticness' of synthetic images. The empirical results show that our method can capture the idea of where the reasonable places are for pedestrians to walk or stand in a street scene.\r"
  },
  "iccv2017_w3_improvingareal-timeobjectdetectorwithcompacttemporalinformation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Improving a Real-Time Object Detector With Compact Temporal Information",
    "authors": [
      "Martin Ahrnbom",
      "Morten Borno Jensen",
      "Kalle Astrom",
      "Mikael Nilsson",
      "Hakan Ardo",
      "Thomas Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Ahrnbom_Improving_a_Real-Time_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Ahrnbom_Improving_a_Real-Time_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Neural networks designed for real-time object detection have recently improvedsignificantly, but in practice, looking at only a single RGB image at the timemay not be ideal. For example, when detecting objects in videos, aforeground detection algorithm can be used to obtain compact temporal data,which can be fed into a neural network alongside RGB images. We propose anapproach for doing this, based on an existing object detector, that re-usespretrained weights for the processing of RGB images. The neural network wastested on the VIRAT dataset with annotations for object detection, a problemthis approach is well suited for. The accuracy was found to improvesignificantly (up to 66%), with a roughly 40% increase in computational time.\r"
  },
  "iccv2017_w3_real-timecategory-basedandgeneralobstacledetectionforautonomousdriving": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Real-Time Category-Based and General Obstacle Detection for Autonomous Driving",
    "authors": [
      "Noa Garnett",
      "Shai Silberstein",
      "Shaul Oron",
      "Ethan Fetaya",
      "Uri Verner",
      "Ariel Ayash",
      "Vlad Goldner",
      "Rafi Cohen",
      "Kobi Horn",
      "Dan Levi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Garnett_Real-Time_Category-Based_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Garnett_Real-Time_Category-Based_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": "Detecting obstacles, both dynamic and static, with near-to-perfect accuracy and low latency, is a crucial enabler of autonomous driving. In recent years obstacle detection methods increasingly rely on cameras instead of Lidars. Camera-based obstacle detection is commonly solved by detecting instances of known categories. However, in many situations the vehicle faces un-categorized obstacles, both static and dynamic. Column-based general obstacle detection covers all 3D obstacles but does not provide object classification, segmentation and motion prediction. In this paper we present a unified deep convolutional network combining these two complementary functions in one computationally efficient framework capable of real-time performance. In addition, we show several improvements to existing column-based obstacle detection, namely an improved network architecture, a new dataset and a major enhancement of the automatic ground truth algorithm.\r"
  },
  "iccv2017_w3_aretheygoingtocross?abenchmarkdatasetandbaselineforpedestriancrosswalkbehavior": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior",
    "authors": [
      "Amir Rasouli",
      "Iuliia Kotseruba",
      "John K. Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Rasouli_Are_They_Going_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Designing autonomous vehicles suitable for urban environments remains an unresolved problem. One of the major dilemmas faced by autonomous cars is how to understand the intention of other road users and communicate with them. The existing datasets do not provide the necessary means for such higher level analysis of traffic scenes. With this in mind, we introduce a novel dataset which in addition to providing the bounding box information for pedestrian detection, also includes the behavioral and contextual annotations for the scenes. This allows combining visual and semantic information for better understanding of pedestrians' intentions in various traffic scenarios. We establish baseline approaches for analyzing the data and show that combining visual and contextual information can improve prediction of pedestrian intention at the point of crossing by at least 20%.\r"
  },
  "iccv2017_w3_goingdeeperautonomoussteeringwithneuralmemorynetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Going Deeper: Autonomous Steering With Neural Memory Networks",
    "authors": [
      "Tharindu Fernando",
      "Simon Denman",
      "Sridha Sridharan",
      "Clinton Fookes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Fernando_Going_Deeper_Autonomous_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Fernando_Going_Deeper_Autonomous_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Although autonomous driving is an area which has been extensively explored in computer vision, current deep learning based methods such as direct image to action mapping approaches, are not able to generate accurate results. This is largely due to the lack of capacity of the current state-of-the-art architectures to capture long term dependencies which can model different human preferences under different contexts. Our work explores a new paradigm in deep autonomous driving where the model incorporates both visual input as well as the steering wheel trajectory and attains a long term planning capacity via neural memory networks. The effectiveness of the proposed architecture is illustrated using two publicly available datasets where in both cases the proposed model demonstrates human like behaviour under challenging situations including illumination variations, discontinuous shoulder lines, lane merges, and divided highways, outperforming the current state-of-the-art. \r"
  },
  "iccv2017_w3_fastvehicledetectorforautonomousdriving": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Fast Vehicle Detector for Autonomous Driving",
    "authors": [
      "Che-Tsung Lin",
      "Patrisia Sherryl Santoso",
      "Shu-Ping Chen",
      "Hung-Jin Lin",
      "Shang-Hong Lai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Lin_Fast_Vehicle_Detector_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Lin_Fast_Vehicle_Detector_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents a fast vehicle detector which can be deployed on NVIDIA DrivePX2 under real-time constraints. The network predicts bounding boxes with different aspect ratio and scale priors from the specifically-designed prediction module given concatenated multi-scale feature map. A new data augmentation strategy is proposed to systematically generate a lot of vehicle training images whose appearance is randomly truncated so our detector could detect occluded vehicles better. Besides, we propose a non-region-based online hard example mining framework which performs fine-tuning by picking (1) hard examples and (2) detection results with insufficient IOU. Compared to other classical object detectors, this work achieves very competitive result in terms of average precision (AP) and computational speed. For the newly-defined vehicle class (car+bus) on VOC2007 test, our detector achieves 85.32 AP and runs at 48 FPS and 30 FPS on NVIDIA Titan X & GP106 (DrivePX2), respectively.\r"
  },
  "iccv2017_w3_largescalelabelledvideodataaugmentationforsemanticsegmentationindrivingscenarios": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Large Scale Labelled Video Data Augmentation for Semantic Segmentation in Driving Scenarios",
    "authors": [
      "Ignas Budvytis",
      "Patrick Sauer",
      "Thomas Roddick",
      "Kesar Breen",
      "Roberto Cipolla"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Budvytis_Large_Scale_Labelled_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Budvytis_Large_Scale_Labelled_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we present an analysis of the effect of large scale video data augmentation for semantic segmentation in driving scenarios. Our work is motivated by a strong correlation between the high performance of most recent deep learning based methods and the availability of large volumes of ground truth labels. To generate additional labelled data, we make use of an occlusion-aware and uncertaintyenabled label propagation algorithm. As a result we increase the availability of high-resolution labelled frames by a factor of 20, yielding in a 6.8% to 10.8% rise in average classification accuracy and/or IoU scores for several semantic segmentation networks. Our key contributions include: (a) augmented CityScapes and CamVid datasets providing 56.2K and 6.5K additional labelled frames of object classes respectively, (b) detailed empirical analysis of the effect of the use of augmented data as well as (c) extension of proposed framework to instance segmentation.\r"
  },
  "iccv2017_w3_ladder-styledensenetsforsemanticsegmentationoflargenaturalimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Ladder-Style DenseNets for Semantic Segmentation of Large Natural Images",
    "authors": [
      "Ivan Kreso",
      "Sinisa Segvic",
      "Josip Krapac"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Kreso_Ladder-Style_DenseNets_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Kreso_Ladder-Style_DenseNets_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recent progress of deep image classification models provides a large potential to improve state-of-the-art performance in the related computer vision tasks. However, the transition towards semantic segmentation is not straight-forward due to strict memory limitations of contemporary GPU cards. The extent of feature map caching required by convolutional backprop poses significant challenges even for moderately sized PASCAL images, while requiring careful architectural considerations when the source resolution is in the megapixel range. To address these concerns we propose a DenseNet-based ladder-style architecture which features a lean representation near the original resolution. The resulting fully convolutional models have few parameters, allow training at megapixel resolution on commodity hardware and display fair semantic segmentation performance even without ImageNet pre-training. We present experiments on Cityscapes and Pascal VOC 2012 datasets and report competitive results.\r"
  },
  "iccv2017_w3_riskyregionlocalizationwithpointsupervision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Risky Region Localization With Point Supervision",
    "authors": [
      "Kazuki Kozuka",
      "Juan Carlos Niebles"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Kozuka_Risky_Region_Localization_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Kozuka_Risky_Region_Localization_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a method for detecting regions with potential risk from images. We focus on images acquired by a front camera mounted on a car with the goal of localizing image regions where pedestrians are likely to enter the scene suddenly. In this case, we define the risk value at every pixel as the likelihood that a pedestrian will occupy those pixels shortly. This task is very challenging because the risk areas are not easily characterized by appearances of single objects, and therefore these regions exhibit large visual variations. Additionally, the boundaries of the risk regions in the image are not easily defined by human annotators, as they do not tend to correspond to object boundaries. This causes the annotation process to be ambiguous and costly. Instead of relying on ambiguous annotations of the boundaries of risk regions, we propose a weakly supervised method for risk region localization and risk value estimation that only requires 1 point supervision at training time. \r"
  },
  "iccv2017_w3_hykoaspectraldatasetforsceneunderstanding": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "HyKo: A Spectral Dataset for Scene Understanding",
    "authors": [
      "Christian Winkens",
      "Florian Sattler",
      "Veronika Adams",
      "Dietrich Paulus"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Winkens_HyKo_A_Spectral_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Winkens_HyKo_A_Spectral_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a novel dataset captured with compact, low-cost, snapshot mosaic (SSM) imaging cameras, which are able to capture a whole spectral cube in one shot. For the best of our knowledge its the first dataset inwhich hyperspectral data was recorded from a moving vehicle enabling hyperspectral scene analysis for road scene understanding. In total, we recorded several hours of traffic scenarios using a variety of sensor modalities such as hyperspectralcameras and 3D laser scanners. We captured and hand labeled diverse scenarios ranging from real-world traffic situations in city scenes to suburban areas. Our data is synchronized and annotated, containing semantic and material labels which allows training classifiers for scene understanding and autonomous driving. The data covers wavelengths from 400 to 1000 nm spanning the visible and near infrared spectral ranges. In this work we describe our recording platforms, the data format and needed utilities to work with the data.\r"
  },
  "iccv2017_w3_eliminatingtheobservereffectshadowremovalinorthomosaicsoftheroadnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Eliminating the Observer Effect: Shadow Removal in Orthomosaics of the Road Network",
    "authors": [
      "Supannee Tanathong",
      "William A. P. Smith",
      "Stephen Remde"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w3/html/Tanathong_Eliminating_the_Observer_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w3/Tanathong_Eliminating_the_Observer_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " High resolution images of the road surface can be obtained using a vehicle equipped with a camera oriented towards the road surface. These images can be stitched into an orthomosaic (i.e. a mosaiced image approximating an orthographic view) providing a virtual top down view of the road network. However, the vehicle casts a shadow onto the road surface that is sometimes visible in the captured images. This causes large artefacts in the stitched orthomosaic. In this paper, we propose a model-based solution to this problem. We capture a 3D model of the vehicle, transform it to a canonical pose and use it to predict shadow masks by ray casting from the sun direction. Shadow masks are precomputed, stored in a look up table and used to generate per-pixel weights for stitching. We integrate this approach into a pipeline for pose estimation and gradient domain stitching that we show is capable of producing shadow-free, high quality orthomosaics from uncontrolled, real world datasets.\r"
  },
  "iccv2017_w5_weblogo-2mscalablelogodetectionbydeeplearningfromtheweb": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "WebLogo-2M: Scalable Logo Detection by Deep Learning From the Web",
    "authors": [
      "Hang Su",
      "Shaogang Gong",
      "Xiatian Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Su_WebLogo-2M_Scalable_Logo_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Su_WebLogo-2M_Scalable_Logo_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Existing logo detection methods usually consider a small number of logo classes and limited images per class with a strong assumption of requiring tedious object bounding box annotations, thus unscalable to real-world applications. This work tackles these challenges by exploring the webly data learning principle without exhaustive manual labelling. Specifically, we propose a novel incremental learning approach, called Scalable Logo Self-Training (SLST), capable of automatically self-discovering informative training images from noisy web data for progressive model update. Moreover, we introduce a very large (1,867,177 images of 194 classes) logo dataset \"WebLogo-2M\" by an automatic web data collection and processing method.Extensive evaluations demonstrate the superiority of the SLST method over state-of-the-art strongly and weakly supervised detection models and webly data learning alternatives.\r"
  },
  "iccv2017_w5_scale-freecontentbasedimageretrieval(ornearlyso)": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Scale-Free Content Based Image Retrieval (or Nearly So)",
    "authors": [
      "Adrian Popescu",
      "Alexandru Ginsca",
      "Herve Le Borgne"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Popescu_Scale-Free_Content_Based_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Popescu_Scale-Free_Content_Based_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " When textual annotations of Web and social media images are poor or missing, content-based image retrieval is an interesting way to access them.Finding an optimal trade-off between accuracy and scalability for CBIR is challenging in practice.We propose a retrieval method whose complexity is nearly independent of the collection scale and does not degrade results quality. Images are represented with sparse semantic features that can be stored as an inverted index. Search complexity is drastically reduced by considering the query feature dimensions independently and thus turning search into a concatenation operation and pruning the index according to a retrieval objective. To improve precision, the inverted index look-up is complemented with an exhaustive search over a fixed size list of intermediary results.We run experiments with three public collections and results show that our much faster method slightly outperforms an exhaustive search done with two competitive baselines.\r"
  },
  "iccv2017_w5_understandingsceneryqualityavisualattentionmeasureanditscomputationalmodel": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Understanding Scenery Quality: A Visual Attention Measure and Its Computational Model",
    "authors": [
      "Yuen Peng Loh",
      "Song Tong",
      "Xuefeng Liang",
      "Takatsune Kumada",
      "Chee Seng Chan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Loh_Understanding_Scenery_Quality_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Loh_Understanding_Scenery_Quality_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Travel photos record tourists' experiences and attentions when visiting a place. We question if they embed any untapped indices, subconsciously created by the tourists, for measuring the scenery quality? By analyzing thousands of such photos and inspired by the psychological theory of \"broaden-and-build\", our study reveals a strong inclination of taking panoramic photos at high rating outdoor tourist spots. Thus, this preference can be a supplementary measure of indexing the scenery quality. However, the task of recognizing panoramic photos is nontrivial. In this paper, we propose a visual attention inspired computational model to address this issue, which mimics human perceptual and cognitive mechanisms by a focus model and a scale model. The experiments on a newly created dataset demonstrate a remarkable performance of our proposal, along with its effectiveness in measuring scenery quality also verified by 10 high rating outdoor spots and 2 lower rating ones from across the world.\r"
  },
  "iccv2017_w5_featurelearningwithrank-basedcandidateselectionforproductsearch": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Feature Learning With Rank-Based Candidate Selection for Product Search",
    "authors": [
      "Yin-Hsi Kuo",
      "Winston H. Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Kuo_Feature_Learning_With_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Kuo_Feature_Learning_With_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Nowadays, more and more people buy products via e-commerce websites. We can not only compare prices from different online retailers but also obtain useful review comments from other customers. Especially, people tend to search for visually similar products when they are looking for possible candidates. The need for product search is emerging. To tackle the problem, recent works integrate different additional information (e.g., attributes, image pairs, category) with deep convolutional neural networks (CNNs) for solving cross-domain image retrieval and product search. Based on the state-of-the-art approaches, we propose a rank-based candidate selection for feature learning. Given a query image, we attempt to push hard negative (irrelevant) images away from queries and make ambiguous positive (relevant) images close to queries. We investigate the effects of global and attention-based local features on the proposed method, and achieve 15.8% relative gain for product search.\r"
  },
  "iccv2017_w5_cross-medialearningforimagesentimentanalysisinthewild": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Cross-Media Learning for Image Sentiment Analysis in the Wild",
    "authors": [
      "Lucia Vadicamo",
      "Fabio Carrara",
      "Andrea Cimino",
      "Stefano Cresci",
      "Felice Dell'Orletta",
      "Fabrizio Falchi",
      "Maurizio Tesconi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Vadicamo_Cross-Media_Learning_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Vadicamo_Cross-Media_Learning_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Much progress has been made in the field of sentiment analysis in the past years. Researchers relied on textual data for this task, while only recently they have started investigating approaches to predict sentiments from multimedia content. With the increasing amount of data shared on social media, there is also a rapidly growing interest in approaches that work \"in the wild\", i.e. that are able to deal with uncontrolled conditions. In this work, we faced the challenge of training a visual sentiment classifier starting from a large set of user-generated and unlabeled contents. In particular, we collected more than 3 million tweets containing both text and images, and we leveraged on the sentiment polarity of the textual contents to train a visual sentiment classifier. We assessed the validity of our model by conducting comparative studies and evaluations on a benchmark for visual sentiment analysis.\r"
  },
  "iccv2017_w5_adaptivepoolinginmulti-instancelearningforwebvideoannotation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Adaptive Pooling in Multi-Instance Learning for Web Video Annotation",
    "authors": [
      "Yizhou Zhou",
      "Xiaoyan Sun",
      "Dong Liu",
      "Zhengjun Zha",
      "Wenjun Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Zhou_Adaptive_Pooling_in_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Zhou_Adaptive_Pooling_in_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Web videos are usually weakly annotated, i.e., a tag is associated to a video once the corresponding concept appears in a frame of this video without indicating when and where it occurs. These weakly annotated tags pose big troubles to many Web video applications, e.g. search and recommendation. In this paper, we present a new Web video annotation approach based on multi-instance learning (MIL) with a learnable pooling function.By formulating the Web video annotation as a MIL problem, we present an end-to-end deep network framework to solve this problem in which the frame (instance) level annotation is estimated from tagsgiven at the video (bag of instances) level via a convolutional neural network. Experimental results demonstrate that our framework is able to not only enhance the accuracy of Web video annotation by outperforming the state-of-the-art Web video annotation methods on the large-scale video dataset FCVID, but also help to infer the most relevant frames in Web videos.\r"
  },
  "iccv2017_w5_attendingtodistinctivemomentsweakly-supervisedattentionmodelsforactionlocalizationinvideo": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Attending to Distinctive Moments: Weakly-Supervised Attention Models for Action Localization in Video",
    "authors": [
      "Lei Chen",
      "Mengyao Zhai",
      "Greg Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Chen_Attending_to_Distinctive_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Chen_Attending_to_Distinctive_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a method for utilizing weakly supervised data for action localization in videos.We focus on sports video analysis, where videos contain scenes of multiple people.Weak supervision gathered from sports website is provided in the form of an action taking place in a video clip, without specification of the person performing the action.Since many frames of a clip can be ambiguous, a novel temporal attention approach is designed to select the most distinctive frames in which to apply the weak supervision.Empirical results demonstrate that leveraging weak supervision can build upon purely supervised localization methods, and utilizing temporal attention further improves localization accuracy.\r"
  },
  "iccv2017_w5_vitsvideotaggingsystemfrommassivewebmultimediacollections": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "ViTS: Video Tagging System From Massive Web Multimedia Collections",
    "authors": [
      "Delia Fernandez",
      "David Varas",
      "Joan Espadaler",
      "Issey Masuda",
      "Jordi Ferreira",
      "Alejandro Woodward",
      "David Rodriguez",
      "Xavier Giro-i-Nieto",
      "Juan Carlos Riveiro",
      "Elisenda Bou"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Fernandez_ViTS_Video_Tagging_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Fernandez_ViTS_Video_Tagging_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The popularization of multimedia content on the Web has arised the need to automatically understand, index and retrieve it. In this paper we present ViTS, an automatic Video Tagging System which learns from videos, their web context and comments shared on social networks. ViTS analyses massive multimedia collections by Internet crawling, and maintains a knowledge base that updates in real time with no need of human supervision. As a result, each video is indexed with a rich set of labels and linked with other related contents. ViTS is an industrial product under exploitation with a vocabulary of over 2.5M concepts, capable of indexing more than 150k videos per month. We compare the quality and completeness of our tags with respect to the ones in the YouTube-8M dataset, and we show how ViTS enhances the semantic annotation of the videos with a larger number of labels (10.04 tags/video), with an accuracy of 80,87%. Extracted tags and video summaries are publicly available.\r"
  },
  "iccv2017_w5_near-duplicatevideoretrievalwithdeepmetriclearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Near-Duplicate Video Retrieval With Deep Metric Learning",
    "authors": [
      "Giorgos Kordopatis-Zilos",
      "Symeon Papadopoulos",
      "Ioannis Patras",
      "Yiannis Kompatsiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Kordopatis-Zilos_Near-Duplicate_Video_Retrieval_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Kordopatis-Zilos_Near-Duplicate_Video_Retrieval_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This work addresses the problem of Near-Duplicate Video Retrieval (NDVR). We propose an efficient video-level NDVR scheme based on deep metric learning that leverages CNN features from intermediate layers to generate discriminative global video representations in tandem with a Deep Metric Learning (DML) framework with two fusion variations, trained to approximate an embedding function for accurate distance calculation between two near-duplicate videos. In contrast to most state-of-the-art methods, which exploit information deriving from the same source of data for both development and evaluation (which usually results to dataset-specific solutions), the proposed model is fed during training with sampled triplets generated from an independent dataset and is thoroughly tested on the widely used CC_WEB_VIDEO dataset. We demonstrate that the proposed approach achieves outstanding performance against the state-of-the-art, either with or without access to the evaluation dataset.\r"
  },
  "iccv2017_w5_set2modelnetworkslearningdiscriminativelytolearngenerativemodels": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Web-Scale Vision and Social Media",
    "title": "Set2Model Networks: Learning Discriminatively to Learn Generative Models",
    "authors": [
      "Alexander Vakhitov",
      "Andrey Kuzmin",
      "Victor Lempitsky"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w5/html/Vakhitov_Set2Model_Networks_Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w5/Vakhitov_Set2Model_Networks_Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a new \"learning-to-learn\"-type approach for small-to-medium sized training sets. At the core lies a deep architecture (a Set2Model network) that maps sets of examples to simple generative probabilistic models such as Gaussians or mixtures of Gaussians in the space of high-dimensional descriptors.The parameters of the embedding into the descriptor space are discriminatively trained in the end-to-end fashion. The main technical novelty of our approach is the derivation of the backprop process through the mixture model fitting. A trained Set2Model network facilitates learning in the cases when no negative examples are available, and whenever the concept being learned is polysemous or represented by noisy training sets. Among other experiments, we demonstrate that these properties allow Set2Model networks to pick visual concepts from the raw outputs of Internet image search engines better than a set of strong baselines.\r"
  },
  "iccv2017_w6_semanticsegmentationofrgbdvideoswithrecurrentfullyconvolutionalneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Semantic Segmentation of RGBD Videos With Recurrent Fully Convolutional Neural Networks",
    "authors": [
      "Ekrem Emre Yurdakul",
      "Yucel Yemez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w6/html/Yurdakul_Semantic_Segmentation_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w6/Yurdakul_Semantic_Segmentation_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Semantic segmentation of videos using neural networks is currently a popular task, the work done in this field is however mostly on RGB videos. The main reason for this is the lack of large RGBD video datasets, annotated with ground truth information at the pixel level. In this work, we use a synthetic RGBD video dataset to investigate the contribution of depth and temporal information to the video segmentation task using convolutional and recurrent neural network architectures. Our experiments show the addition of depth information improves semantic segmentation results and exploiting temporal information results in higher quality output segmentations.\r"
  },
  "iccv2017_w6_mutualforegroundsegmentationwithmultispectralstereopairs": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Mutual Foreground Segmentation With Multispectral Stereo Pairs",
    "authors": [
      "Pierre-Luc St-Charles",
      "Guillaume-Alexandre Bilodeau",
      "Robert Bergevin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w6/html/St-Charles_Mutual_Foreground_Segmentation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w6/St-Charles_Mutual_Foreground_Segmentation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Foreground-background segmentation of video sequences is a low-level process commonly used in machine vision, and highly valued in video content analysis and smart surveillance applications. Its efficacy relies on the contrast between objects observed by the sensor. In this work, we study how the combination of sensors operating in the long-wavelength infrared (LWIR) and visible spectra can improve the performance of foreground-background segmentation methods. As opposed to a classic visible spectrum stereo pair, this multispectral pair is more adequate for object segmentation since it reduces the odds of observing low-contrast regions simultaneously in both images. We show that by alternately minimizing stereo disparity and binary segmentation energies with dynamic priors, we can drastically improve the results of a traditional video segmentation approach applied to each sensor individually. Our implementation is freely available online for anyone wishing to recreate our results.\r"
  },
  "iccv2017_w6_triplet-baseddeepsimilaritylearningforpersonre-identification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Triplet-Based Deep Similarity Learning for Person Re-Identification",
    "authors": [
      "Wentong Liao",
      "Michael Ying Yang",
      "Ni Zhan",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w6/html/Liao_Triplet-Based_Deep_Similarity_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w6/Liao_Triplet-Based_Deep_Similarity_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In recent years, person re-identification (re-id) catches great attention in both computer vision community and industry. In this paper, we propose a new framework for person re-identification with a triplet-based deep similarity learning using convolutional neural networks (CNNs). The network is trained with triplet input: two of them have the same class labels and the other one is different. It aims to learn the deep feature representation, with which the distance within the same class is decreased, while the distance between the different classes is increased as much as possible. Moreover, we trained the model jointly on six different datasets, which differs from common practice - one model is just trained on one dataset and tested also on the same one. \r"
  },
  "iccv2017_w6_accuratecalibrationoflidar-camerasystemsusingordinaryboxes": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Accurate Calibration of LiDAR-Camera Systems Using Ordinary Boxes",
    "authors": [
      "Zoltan Pusztai",
      "Levente Hajder"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w6/html/Pusztai_Accurate_Calibration_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w6/Pusztai_Accurate_Calibration_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper deals with the calibration of a visual system, consisting of RGB cameras and 3D Light Detection And Ranging (LiDAR) sensors. Registering two separate point clouds coming from different modalities is always challenging. We propose a novel and accurate calibration method using simple cardboard boxes with known sizes. Our approach is principally based on the detection of box planes in LiDAR point clouds, thus it can calibrate different LiDAR equipments. Moreover, camera-LiDAR calibration is also possible with minimal manual intervention. The proposed algorithm is validated and compared to state-of-the-art techniques both on synthesized data and real-world measurements taken by a visual system consisting of LiDAR sensors and RGB cameras.\r"
  },
  "iccv2017_w6_multi-tasklearningusingmulti-modalencoder-decodernetworkswithsharedskipconnections": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Multi-Task Learning Using Multi-Modal Encoder-Decoder Networks With Shared Skip Connections",
    "authors": [
      "Ryohei Kuga",
      "Asako Kanezaki",
      "Masaki Samejima",
      "Yusuke Sugano",
      "Yasuyuki Matsushita"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w6/html/Kuga_Multi-Task_Learning_Using_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w6/Kuga_Multi-Task_Learning_Using_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Multi-task learning is a promising approach for efficiently and effectively addressing multiple mutually related recognition tasks. Many scene understanding tasks such as semantic segmentation and depth prediction can be framed as cross-modal encoding/decoding, and hence most of the prior work used multi-modal datasets for multi-task learning. However, the inter-modal commonalities, such as one across image, depth, and semantic labels, have not been fully exploited. We propose a multi-modal encoder-decoder networks to harness the multi-modal nature of multi-task scene recognition. In addition to the shared latent representation among encoder-decoder pairs, our model also has shared skip connections from different encoders. By combining these two representation sharing mechanisms, the proposed method efficiently learns a shared feature representation. Experimental validation show the advantage of our method over baseline encoder-decoder networks and multi-modal auto-encoders.\r"
  },
  "iccv2017_w6_lbp-flowandhybridencodingforreal-timewaterandfireclassification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "LBP-Flow and Hybrid Encoding for Real-Time Water and Fire Classification",
    "authors": [
      "Konstantinos Avgerinakis",
      "Panagiotis Giannakeris",
      "Alexia Briassouli",
      "Anastasios Karakostas",
      "Stefanos Vrochidis",
      "Ioannis Kompatsiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w6/html/Avgerinakis_LBP-Flow_and_Hybrid_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w6/Avgerinakis_LBP-Flow_and_Hybrid_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this work, we focus on the challenging problem of real-world dynamic scene understanding, where videos contain dynamic textures that have been recorded in the 'wild'. These videos feature large illumination variations, complex motion, occlusions, camera motion, as well as significant intra-class differences. We address these issues byintroducing a novel dynamic texture descriptor, the \"Local Binary Pattern-flow\" (LBP-flow), which is shown to be able to accurately classify dynamic scenes whose complex motion patterns are difficult to separate using existing local descriptors, or which cannot be modelled by statistical techniques. The descriptor statistics are encoded with Fisher vector, while a neural network follows to reduce the dimensionality and increase the discriminability of the final descriptor. The proposed algorithm leads to a highly accurate spatio-temporal descriptor which achieves a very low computational cost enabling the deployment of our descriptor in applications\r"
  },
  "iccv2017_w6_registrationofrgbandthermalpointcloudsgeneratedbystructurefrommotion": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Registration of RGB and Thermal Point Clouds Generated by Structure From Motion",
    "authors": [
      "Trong Phuc Truong",
      "Masahiro Yamaguchi",
      "Shohei Mori",
      "Vincent Nozick",
      "Hideo Saito"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w6/html/Truong_Registration_of_RGB_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w6/Truong_Registration_of_RGB_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Thermal imaging has become a valuable tool in various fields for remote sensing and can provide relevant information to perform object recognition or classification. In this paper, we present an automated method to obtain a 3D model fusing data from a visible and a thermal camera. The RGB and thermal point clouds are generated independently by structure from motion. The registration process includes a normalization of the point cloud scale, a global registration based on calibration data and the output of the structure from motion, and a fine registration employing a variant of the Iterative Closest Point optimization. Experimental results demonstrate the accuracy and robustness of the overall process.\r"
  },
  "iccv2017_w8_improvingspeakerturnembeddingbycrossmodaltransferlearningfromfaceembedding": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Audio-Visual Media",
    "title": "Improving Speaker Turn Embedding by Crossmodal Transfer Learning From Face Embedding",
    "authors": [
      "Nam Le",
      "Jean-Marc Odobez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w8/html/Le_Improving_Speaker_Turn_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w8/Le_Improving_Speaker_Turn_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Learning speaker turn embeddings has shown considerable improvement in situations where conventional speaker modeling approaches fail. However, this improvement is relatively limited when compared to the gain observed in face embedding learning, which has proven very successful for face verification and clustering tasks. Assuming that face and voices from the same identities share some latent properties (like age, gender, ethnicity), we propose two transfer learning approaches to leverage the knowledge from the face domain learned from thousands of images and identities for tasks in the speaker domain. These approaches, namely target embedding transfer and clustering structure transfer, utilize the structure of the source face embedding space at different granularities to regularize the target speaker turn embedding space as optimizing terms. Our methods are evaluated on two public broadcast corpora and yield promising advances over competitive baselines in verification and audio clustering tasks, especially when dealing with short speaker utterances. The analysis of the results also gives insight into characteristics of the embedding spaces and shows their potential applications.\r"
  },
  "iccv2017_w8_unsupervisedcross-modaldeep-modeladaptationforaudio-visualre-identificationwithwearablecameras": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Audio-Visual Media",
    "title": "Unsupervised Cross-Modal Deep-Model Adaptation for Audio-Visual Re-Identification With Wearable Cameras",
    "authors": [
      "Alessio Brutti",
      "Andrea Cavallaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w8/html/Brutti_Unsupervised_Cross-Modal_Deep-Model_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w8/Brutti_Unsupervised_Cross-Modal_Deep-Model_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Model adaptation is important for the analysis of audio-visual data from body worn cameras in order to cope with rapidly changing scene conditions, varying object appearance and limited training data. In this paper, we propose a new approach for the on-line and unsupervised adaptation of deep-learning models for audio-visual target re-identification. Specifically, we adapt each mono-modal model using the unsupervised labelling provided by the other modality. To limit the detrimental effects of erroneous labels, we use a regularisation term based on the Kullback--Leibler divergence between the initial model and the one being adapted. The proposed adaptation strategy complements common audio-visual late fusion approaches and is beneficial also when one modality is no longer reliable. We show the contribution of the proposed strategy in improving the overall re-identification performance on a challenging public dataset captured with body worn cameras.\r"
  },
  "iccv2017_w8_exploitingthecomplementarityofaudioandvisualdatainmulti-speakertracking": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Audio-Visual Media",
    "title": "Exploiting the Complementarity of Audio and Visual Data in Multi-Speaker Tracking",
    "authors": [
      "Yutong Ban",
      "Laurent Girin",
      "Xavier Alameda-Pineda",
      "Radu Horaud"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w8/html/Ban_Exploiting_the_Complementarity_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w8/Ban_Exploiting_the_Complementarity_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Multi-speaker tracking is a central problem in humanrobot interaction. In this context, exploiting auditory and visual information is gratifying and challenging at the same time. Gratifying because the complementary nature of auditory and visual information allows us to be more robust against noise and outliers than unimodal approaches. Challenging because how to properly fuse auditory and visual information for multi-speaker tracking is far from being a solved problem. In this paper we propose a probabilistic generative model that tracks multiple speakers by jointly exploiting auditory and visual features in their own representation spaces. Importantly, the method is robust to missing data and is therefore able to track even when observations from one of the modalities are absent. Quantitative and qualitative results on the AVDIAR dataset are reported.\r"
  },
  "iccv2017_w8_improvedspeechreconstructionfromsilentvideo": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Audio-Visual Media",
    "title": "Improved Speech Reconstruction From Silent Video",
    "authors": [
      "Ariel Ephrat",
      "Tavi Halperin",
      "Shmuel Peleg"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w8/html/Ephrat_Improved_Speech_Reconstruction_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w8/Ephrat_Improved_Speech_Reconstruction_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Speechreading is the task of inferring phonetic information from articulatory facial movements by observing them visually, and is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible and natural-sounding acoustic speech signal from silent video frames of a speaking person. We train our model on speakers from the GRID and TCD-TIMIT datasets, and evaluate the quality and intelligibility of reconstructed speech using common objective measurements. We show that speech predictions from the proposed model attain scores which indicate significantly improved quality over existing models. In addition, we show promising results towards reconstructing speech from an unconstrained dictionary.\r"
  },
  "iccv2017_w8_visualmusictranscriptionofclarinetvideorecordingstrainedwithaudio-basedlabelleddata": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Audio-Visual Media",
    "title": "Visual Music Transcription of Clarinet Video Recordings Trained With Audio-Based Labelled Data",
    "authors": [
      "Pablo Zinemanas",
      "Pablo Arias",
      "Gloria Haro",
      "Emilia Gomez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w8/html/Zinemanas_Visual_Music_Transcription_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w8/Zinemanas_Visual_Music_Transcription_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Automatic transcription is a well-known task in the music information retrieval (MIR) domain, and consists on the computation of a symbolic music representation (e.g. MIDI) from an audio recording. In this work, we address the automatic transcription of video recordings when the audio modality is missing or it does not have enough quality, and thus analyze the visual information.We focus on the clarinet which is played by opening/closing a set of holes and keys. We propose a method for automatic visual note estimation by detecting the fingertips of the player and measuring their displacement with respect to the holes and keys of the clarinet. To this aim, we track the clarinet and determine its position on every frame. The relative positions of the fingertips are used as features of a machine learning algorithm trained for note pitch classification. For that purpose, a dataset is built in a semiautomatic way by estimating pitch information from audio signals in an existing collection of 4.5 hours of video recordings from six different songs performed by nine different players. Our results confirm the difficulty of performing visual vs audio automatic transcription mainly due to motion blur and occlusions that cannot be solved with a single view.\r"
  },
  "iccv2017_w9_indefenseofshallowlearnedspectralreconstructionfromrgbimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Physics Based Vision Meets Deep Learning",
    "title": "In Defense of Shallow Learned Spectral Reconstruction From RGB Images",
    "authors": [
      "Jonas Aeschbacher",
      "Jiqing Wu",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w9/html/Aeschbacher_In_Defense_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w9/Aeschbacher_In_Defense_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Very recent Galliani et al. proposed a method using a very deep CNN architecture or learned spectral reconstruction and showed large improvements over the recent sparse coding method of Arad et al. In this paper we defend the shallow learned spectral reconstruction methods by: (i) first, reimplementing Arad and showing that it can achieve significantly better results than those originally reported; (ii) second, introducing a novel shallow method based on A+ of Timofte et al. from super-resolution that substantially improves over Arad and, moreover, provides comparable performance to Galliani's very deep CNN method on three standard benchmarks (ICVL, CAVE, and NUS); and (iii) finally, arguing that the train and runtime efficiency as well as the clear relation between its parameters and the achieved performance makes from our shallow A+ a strong baseline for further research in learned spectral reconstruction from RGB images. Moreover, our shallow A+ (as well as Arad) requires and uses significantly smaller train data than Galliani (and generally the CNN approaches), is robust to overfitting and is easily deployable by fast training to newer cameras.\r"
  },
  "iccv2017_w9_adversarialnetworksforspatialcontext-awarespectralimagereconstructionfromrgb": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Adversarial Networks for Spatial Context-Aware Spectral Image Reconstruction From RGB",
    "authors": [
      "Aitor Alvarez-Gila",
      "Joost van de Weijer",
      "Estibaliz Garrote"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w9/html/Alvarez-Gila_Adversarial_Networks_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w9/Alvarez-Gila_Adversarial_Networks_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Hyperspectral signal reconstruction aims at recovering the original spectral input that produced a certain trichromatic (RGB) response from a capturing device or observer. Given the heavily underconstrained, non-linear nature of the problem, traditional techniques leverage different statistical properties of the spectral signal in order to build informative priors from real world object reflectances for constructing such RGB to spectral signal mapping. However, most of them treat each sample independently, and thus do not benefit from the contextual information that the spatial dimensions can provide. We pose hyperspectral natural image reconstruction as an image to image mapping learning problem, and apply a conditional generative adversarial framework to help capture spatial semantics. This is the first time Convolutional Neural Networks -and, particularly, Generative Adversarial Networks- are used to solve this task. Quantitative evaluation shows a Root Mean Squared Error (RMSE) drop of 44.7% and a Relative RMSE drop of 47.0% on the ICVL natural hyperspectral image dataset.\r"
  },
  "iccv2017_w9_photo-realisticsimulationofroadscenefordata-drivenmethodsinbadweather": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Photo-Realistic Simulation of Road Scene for Data-Driven Methods in Bad Weather",
    "authors": [
      "Kunming Li",
      "Yu Li",
      "Shaodi You",
      "Nick Barnes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w9/html/Li_Photo-Realistic_Simulation_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w9/Li_Photo-Realistic_Simulation_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Modern data-driven computer vision algorithms require a large volume, varied data for validation or evaluation. We utilize computer graphics techniques to generate a large volume foggy image dataset of road scenes with different levels of fog. We compare with other popular synthesized datasets, including data collected both from the virtual world and the real world. In addition, we benchmark recent popular dehazing methods and evaluate their performance on different datasets, which provides us an objectively comparison of their limitations and strengths. To our knowledge, this is the first foggy and hazy dataset with large volume data which can be helpful for computer vision research in the autonomous driving.\r"
  },
  "iccv2017_w9_deepphotometricstereonetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Deep Photometric Stereo Network",
    "authors": [
      "Hiroaki Santo",
      "Masaki Samejima",
      "Yusuke Sugano",
      "Boxin Shi",
      "Yasuyuki Matsushita"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w9/html/Santo_Deep_Photometric_Stereo_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w9/Santo_Deep_Photometric_Stereo_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents a photometric stereo method based on deep learning. One of the major difficulties in photometric stereo is designing an appropriate reflectance model that is both capable of representing real-world reflectances and computationally tractable in terms of deriving surface normal. Unlike previous photometric stereo methods that rely on a simplified parametric image formation model, such as the Lambert's model, the proposed method aims at establishing a flexible mapping between complex reflectance observations and surface normal by the use of a deep neural network. As a result we propose a deep photometric stereo network (DPSN) that takes reflectance observations under varying light directions and infers the corresponding surface normal per pixel. To make the DPSN applicable to real-world objects, a database of measured bidirectional reflectance distribution functions (MERL BRDF database) has been used for training the network. Evaluation using simulation and real-world scenes shows effectiveness of the proposed approach over previous techniques.\r"
  },
  "iccv2017_w9_hierarchicalfeaturedegradationbasedblindimagequalityassessment": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Hierarchical Feature Degradation Based Blind Image Quality Assessment",
    "authors": [
      "Jinjian Wu",
      "Jichen Zeng",
      "Yongxu Liu",
      "Guangming Shi",
      "Weisi Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w9/html/Wu_Hierarchical_Feature_Degradation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w9/Wu_Hierarchical_Feature_Degradation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Though blind image quality assessment (BIQA) is highly demanded for many image processing systems, it is extremely difficult for BIQA to accurately predict the quality without the guide of the reference image. In this paper, we introduce a novel BIQA method with hierarchical feature degradation (HFD). Since the human brain presents hierarchical procedure for visual recognition, we suggest that different levels of distortion generate different degradations on hierarchical features, and propose to consider the degradations on both the low and high level features for quality assessment. Inspired by the orientation selectivity (OS) mechanism in the primary visual cortex, an OS based local visual structure is designed for low-level visual content extraction. Meanwhile, according to the feature integration function of deep neural networks, the deep semantics is extracted with the residual network for high-level visual content representation. Next, by analyzing the degradation on both the local structure and the deep semantics, a HFD based memory (prior knowledge) is learned to represent the generalized quality degradation. Finally, with the guidance of the HFD based memory, a novel HFD-BIQA model is built. Experimental results on the publicly available databases demonstrate the quality prediction accuracy of the proposed HFD-BIQA, and verify that the HFD-BIQA performs highly consistent with the subjective perception.\r"
  },
  "iccv2017_w9_hscnncnn-basedhyperspectralimagerecoveryfromspectrallyundersampledprojections": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Physics Based Vision Meets Deep Learning",
    "title": "HSCNN: CNN-Based Hyperspectral Image Recovery From Spectrally Undersampled Projections",
    "authors": [
      "Zhiwei Xiong",
      "Zhan Shi",
      "Huiqun Li",
      "Lizhi Wang",
      "Dong Liu",
      "Feng Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w9/html/Xiong_HSCNN_CNN-Based_Hyperspectral_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w9/Xiong_HSCNN_CNN-Based_Hyperspectral_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents a unified deep learning framework to recover hyperspectral images from spectrally undersampled projections. Specifically, we investigate two kinds of representative projections, RGB and compressive sensing (CS) measurements. These measurements are first upsampled in the spectral dimension through simple interpolation or CS reconstruction, and the proposed method learns an end-to-end mapping from a large number of upsampled/groundtruth hyperspectral image pairs. The mapping is represented as a deep convolutional neural network (CNN) that takes the spectrally upsampled image as input and outputs the enhanced hyperspetral one. We explore different network configurations to achieve high reconstruction fidelity. Experimental results on a variety of test images demonstrate significantly improved performance of the proposed method over the state-of-the-arts.\r"
  },
  "iccv2017_w9_pvnnaneuralnetworklibraryforphotometricvision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Physics Based Vision Meets Deep Learning",
    "title": "PVNN: A Neural Network Library for Photometric Vision",
    "authors": [
      "Ye Yu",
      "William A. P. Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w9/html/Yu_PVNN_A_Neural_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w9/Yu_PVNN_A_Neural_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we show how a differentiable, physics-based renderer suitable for photometric vision tasks can be implemented as layers in a deep neural network. The layers include geometric operations for representation transformations, reflectance evaluations with arbitrary numbers of light sources and statistical bidirectional reflectance distribution function (BRDF) models. We make an implementation of these layers available as a neural network library (pvnn) for Theano. The layers can be incorporated into any neural network architecture, allowing parts of the photometric image formation process to be explicitly modelled in a network that is trained end to end via backpropagation. As an exemplar application, we show how to train a network with encoder-decoder architecture that learns to estimate BRDF parameters from a single image in an unsupervised manner.\r"
  },
  "iccv2017_w10_multilevelapproximaterobustprincipalcomponentanalysis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Matrix and Tensor Factorization Methods in Computer Vision",
    "title": "Multilevel Approximate Robust Principal Component Analysis",
    "authors": [
      "Vahan Hovhannisyan",
      "Yannis Panagakis",
      "Panos Parpas",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w10/html/Hovhannisyan_Multilevel_Approximate_Robust_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w10/Hovhannisyan_Multilevel_Approximate_Robust_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Robust principal component analysis (RPCA) is currently the method of choice for recovering a low-rank matrix from sparse corruptions that are of unknown value and support by decomposing the observation matrix into low-rank and sparse matrices. RPCA has many applications including background subtraction, learning of robust subspaces from visual data, etc. Nevertheless, the application of SVD in each iteration of optimisation methods renders the application of RPCA challenging in cases when data is large. In this paper, we propose the first, to the best of our knowledge, multilevel approach for solving convex and non-convex RPCA models. The basic idea is to construct lower dimensional models and perform SVD on them instead of the original high dimensional problem. We show that the proposed approach gives a good approximate solution to the original problem for both convex and non-convex formulations, while being many times faster than original RPCA methods in several real world datasets.\r"
  },
  "iccv2017_w10_factorizedconvolutionalneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Matrix and Tensor Factorization Methods in Computer Vision",
    "title": "Factorized Convolutional Neural Networks",
    "authors": [
      "Min Wang",
      "Baoyuan Liu",
      "Hassan Foroosh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w10/html/Wang_Factorized_Convolutional_Neural_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w10/Wang_Factorized_Convolutional_Neural_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose to factorize the standard convolutional layer to reduce the computation. The 3D convolution operation in a convolutional layer can be considered as performing spatial convolution in each channel and linear projection across channels simultaneously. By unravelling them and arranging the spatial convolution sequentially, each layer is composed of a low-cost single intra-channel convolution and a linear channel projection. When combined with residual connection, it can effectively preserve the spatial information and maintain the accuracy with significantly less computation. We also introduce a topological subdivisioning to reduce the connection between the input and output channels. Our experiments demonstrate that the proposed layers outperform the standard convolutional layers on performance/complexity ratio. Our models achieve similar performance to VGG-16, ResNet-34, ResNet-50, ResNet-101 while requiring 42x,7.32x, 4.38x, 5.85x less computation respectively.\r"
  },
  "iccv2017_w10_afactorizationapproachforenablingstructure-from-motion/slamusingintegerarithmetic": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Matrix and Tensor Factorization Methods in Computer Vision",
    "title": "A Factorization Approach for Enabling Structure-From-Motion/SLAM Using Integer Arithmetic",
    "authors": [
      "Nilesh A. Ahuja",
      "Mahesh Subedar",
      "Yeongseon Lee",
      "Omesh Tickoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w10/html/Ahuja_A_Factorization_Approach_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w10/Ahuja_A_Factorization_Approach_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " SLAM and SfM algorithms involve minimization of a cost-function by non-linear least-squares methods. The matrices involved are typically poorly conditioned, making the procedure sensitive to numerical precision effects. Ensuring accuracy therefore entails the use of high-precision floating-point arithmetic. In this work, a factorization approach to EKF-based SfM is presented and is shown to be capable of operating with integer arithmetic - the first such implementation to the best of our knowledge. This is important given the increasing need to implement advanced vision-based capabilities on low-power embedded and mobile processors. An evaluation of the computational complexity shows that the proposed approach typically requires fewer computations than the EKF in practice, resulting in an algorithm that is both numerically more robust and computationally less intensive.\r"
  },
  "iccv2017_w10_accuratestructurerecoveryviaweightednuclearnormalowrankapproachtoshape-from-focus": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Matrix and Tensor Factorization Methods in Computer Vision",
    "title": "Accurate Structure Recovery via Weighted Nuclear Norm: A Low Rank Approach to Shape-From-Focus",
    "authors": [
      "Prashanth Kumar G.",
      "Rajiv Ranjan Sahay"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w10/html/G._Accurate_Structure_Recovery_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w10/G._Accurate_Structure_Recovery_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In recent years, weighted nuclear norm minimization (WNNM) approach has been attracting much interest in computer vision and machine learning. Due to the ability of WNNM to preserve large-scale sharp discontinuities and small-scale fine details more effectively, we propose to use it as a regularizer to recover the 3D structure using shape-from-focus (SFF). Initially, we estimate the Allin- focus image and subsequently 3D structure is recovered using space-variantly blurred observations from the SFF stack. Since estimation of 3D shape is a severely ill-posed problem, we use weighted nuclear norm as a regularizer in the proposed algorithm. Finally, the estimated shape profile is post-processed to compensate for the effect of specular reflections in the observations on shape reconstruction. We conducted several experiments on various synthetic and real-world datasets and our results confirm that the proposed method outperforms other state-of-the-art techniques.\r"
  },
  "iccv2017_w11_backtorgb3dtrackingofhandsandhand-objectinteractionsbasedonshort-baselinestereo": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "Back to RGB: 3D Tracking of Hands and Hand-Object Interactions Based on Short-Baseline Stereo",
    "authors": [
      "Paschalis Panteleris",
      "Antonis Argyros"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Panteleris_Back_to_RGB_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Panteleris_Back_to_RGB_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a novel solution to the problem of 3D tracking of the articulated motion of human hand(s), possibly in interaction with other objects. The vast majority of contemporary relevant work capitalizes on depth information provided by RGBD cameras. In this work, we show that accurate and efficient 3D hand tracking is possible, even for the case of RGB stereo. A straightforward approach for solving the problem based on such input would be to first recover depth and then apply a state of the art depth-based 3D hand tracking method. Unfortunately, this does not work well in practice because the stereo-based, dense 3D reconstruction of hands is far less accurate than the one obtained by RGBD cameras. Our approach bypasses 3D reconstruction and follows a completely different route: 3D hand tracking is formulated as an optimization problem whose solution is the hand configuration that maximizes the color consistency between the two views of the hand. We demonstrate the applicability of our method for real time tracking of a single hand, of a hand manipulating an object and of two interacting hands. The method has been evaluated quantitatively using the same datasets as relevant, state of the art RGBD-based approaches. The obtained results demonstrate that the proposed stereo-based method performs equally well to its RGBD-based competitors, and in some cases, it even outperforms them.\r"
  },
  "iccv2017_w11_deepprior++improvingfastandaccurate3dhandposeestimation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation",
    "authors": [
      "Markus Oberweger",
      "Vincent Lepetit"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Oberweger_DeepPrior_Improving_Fast_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Oberweger_DeepPrior_Improving_Fast_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " DeepPrior is a simple approach based on Deep Learning that predicts the joint 3D locations of a hand given a depth map. Since its publication early 2015, it has been outperformed by several impressive works. Here we show that with simple improvements: adding ResNet layers, data augmentation, and better initial hand localization, we achieve better or similar performance than more sophisticated recent methods on the three main benchmarks (NYU, ICVL, MSRA) while keeping the simplicity of the original method. Our new implementation is available at https://github.com/moberweger/deep-prior-pp.\r"
  },
  "iccv2017_w11_handposeestimationusingdeepstereovisionandmarkov-chainmontecarlo": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "Hand Pose Estimation Using Deep Stereovision and Markov-Chain Monte Carlo",
    "authors": [
      "Rilwan Remilekun Basaru",
      "Greg Slabaugh",
      "Eduardo Alonso",
      "Chris Child"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Basaru_Hand_Pose_Estimation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Basaru_Hand_Pose_Estimation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Hand pose is emerging as an important interface for human-computer interaction. The problem of hand pose estimation from passive stereo inputs has received less attention in the literature compared to active depth sensors. This paper seeks to address this gap by presenting a data-driven method to estimate a hand pose from a stereoscopic camera input, by introducing a stochastic approach to propose potential depth solutions to the observed stereo capture and evaluate these proposals using two convolutional neural networks (CNNs). The first CNN, configured in a Siamese network architecture, evaluates how consistent the proposed depth solution is to the observed stereo capture.The second CNN estimates a hand pose given the proposed depth.Unlike sequential approaches that reconstruct pose from a known depth, our method jointly optimizes the hand pose and depth estimation through Markov-chain Monte Carlo (MCMC) sampling.This way, pose estimation can correct for errors in depth estimation, and vice versa. Experimental results using an inexpensive stereo camera show that the proposed system more accurately measures pose better than competing methods.\r"
  },
  "iccv2017_w11_humanactionrecognitionpose-basedattentiondrawsfocustohands": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "Human Action Recognition: Pose-Based Attention Draws Focus to Hands",
    "authors": [
      "Fabien Baradel",
      "Christian Wolf",
      "Julien Mille"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Baradel_Human_Action_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Baradel_Human_Action_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a new spatio-temporal attention based mechanism for human action recognition able to automatically attend to most important human hands and detect the most discriminative moments in an action. Attention is handled in a recurrent manner employing Recurrent Neural Network (RNN) and is fully-differentiable. In contrast to standard soft-attention based mechanisms, our approach does not use the hidden RNN state as input to the attention model. Instead, attention distributions are drawn using external in- formation: human articulated pose. We performed an ex- tensive ablation study to show the strengths of this approach and we particularly studied the conditioning aspect of the attention mechanism. We evaluate the method on the largest currently available human action recognition dataset, NTU- RGB+D, and report state-of-the-art results. Another ad- vantage of our model are certains aspects of explanability, as the spatial and temporal attention distributions at test time allow to study and verify on which parts of the input data the method focuses.\r"
  },
  "iccv2017_w11_conditionalregressiverandomforeststereo-basedhanddepthrecovery": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "Conditional Regressive Random Forest Stereo-Based Hand Depth Recovery",
    "authors": [
      "Rilwan Remilekun Basaru",
      "Greg Slabaugh",
      "Eduardo Alonso",
      "Chris Child"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Basaru_Conditional_Regressive_Random_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Basaru_Conditional_Regressive_Random_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper introduces Conditional Regressive Random Forest (CRRF), a novel method that combines a closed-form Conditional Random Field (CRF), using learned weights, and a Regressive Random Forest (RRF) that employs adaptively selected expert trees. CRRF is used to estimate a depth image of hand given stereo RGB inputs.CRRF uses a novel superpixel-based regression framework that takes advantage of the smoothness of the hand's depth surface. A RRF unary term adaptively selects different stereo-matching measures as it implicitly determines matching pixels in a coarse-to-fine manner. CRRF also includes a pair-wise term that encourages smoothness between similar adjacent superpixels. Experimental results show that CRRF can produce high quality depth maps, even using an inexpensive RGB stereo camera and produces state-of-the-art results for hand depth estimation.\r"
  },
  "iccv2017_w11_yolseegocentricfingertipdetectionfromsinglergbimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "YOLSE: Egocentric Fingertip Detection From Single RGB Images",
    "authors": [
      "Wenbin Wu",
      "Chenyang Li",
      "Zhuo Cheng",
      "Xin Zhang",
      "Lianwen Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Wu_YOLSE_Egocentric_Fingertip_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Wu_YOLSE_Egocentric_Fingertip_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " With the development of wearable device and augmented reality (AR), the human device interaction in egocentric vision, especially the hand gesture based interaction, has attracted lots of attention among computer vision researchers. In this paper, we build a new dataset named EgoGesture and propose a heatmap-based solution for fingertip detection. Firstly, we discuss the dataset collection detail and as well the comprehensive analysis of this dataset, which shows that the dataset covers substantial data samples in various environments and dynamic hand shapes. Furthermore, we propose a heatmap-based FCN (Fully Convolution Network) named YOLSE (You Only Look what You Should See) for fingertip detection in the egocentric vision from single RGB image. The fingermap is the proposed new probabilistic representation for the multiple fingertip detection, which not only shows the location of fingertip but also indicates whether the fingertip is visible. Comparing with state-of-the-art fingertip detection algorithms, our framework performs the best with limited dependence on the hand detection result. In our experiments, we achieve the fingertip detection error at about 3.69 pixels in 640px x 480px video frame and the average forward time of the YOLSE is about 15.15 ms.\r"
  },
  "iccv2017_w11_lpsnetanovellogpathsignaturefeaturebasedhandgesturerecognitionframework": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "LPSNet: A Novel Log Path Signature Feature Based Hand Gesture Recognition Framework",
    "authors": [
      "Chenyang Li",
      "Xin Zhang",
      "Lianwen Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Li_LPSNet_A_Novel_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Li_LPSNet_A_Novel_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": "Hand gesture recognition is gaining more attentions because it's a natural and intuitive mode of human computer interaction. Hand gesture recognition still faces great challenges for the real-world applications due to the gesture variance and individual difference. In this paper, we propose the LPSNet, an end-to-end deep neural network based hand gesture recognition framework with novel log path signature features. We pioneer a robust feature, path signature (PS) and its compressed version, log path signature (LPS) to extract effective feature of hand gestures. Also, we present a new method based on PS and LPS to effectively combine RGB and depth videos. Further, we propose a statistical method, DropFrame, to enlarge the data set and increase its diversity. By testing on a well-known public dataset, Sheffield Kinect Gesture (SKIG), our method achieves classification rate as 96.7% (only use RGB videos) and 98.7% (combining RGB and Depth videos), which is the best result comparing with state-of-the-art methods.\r"
  },
  "iccv2017_w11_deeplearningbasedhanddetectioninclutteredenvironmentusingskinsegmentation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Observing and Understanding Hands in Action",
    "title": "Deep Learning Based Hand Detection in Cluttered Environment Using Skin Segmentation",
    "authors": [
      "Kankana Roy",
      "Aparna Mohanty",
      "Rajiv R. Sahay"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w11/html/Roy_Deep_Learning_Based_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w11/Roy_Deep_Learning_Based_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Robust detection of hand gestures has remained a challenge due to background clutter encountered in real-world environments. In this work, a two-stage deep learning based approach is presented to detect hands robustly in unconstrained scenarios. We evaluate two recently proposed object detection techniques to initially locate hands in the input images. To further enhance the output of the hand detector we propose a convolutional neural network (CNN) based skin detection technique which reduces occurrences of false positives significantly. We show qualitative and quantitative results of the proposed hand detection algorithm on several public datasets including Oxford, 5-signer and EgoHands dataset. As a case study, we also report hand detection results robust to clutter on a proposed dataset of Indian classical dance (ICD) images.\r"
  },
  "iccv2017_w13_long-term3dlocalizationandposefromsemanticlabellings": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "Long-Term 3D Localization and Pose from Semantic Labellings",
    "authors": [
      "Carl Toft",
      "Carl Olsson",
      "Fredrik Kahl"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/Toft_Long-Term_3D_Localization_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/Toft_Long-Term_3D_Localization_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " One of the major challenges in camera pose estimation and 3D localization is identifying features that are approximately invariant across seasons and in different weather and lighting conditions. In this paper, we present a method for performing accurate and robust six degrees-of-freedom camera pose estimation based only on the pixelwise semantic labelling of a single query image. Localization is performed using a sparse 3D model consisting of semantically labelled points and curves, and an error function based on how well these project onto corresponding curves in the query image is developed.The method is evaluated on the recently released Oxford Robotcar dataset, showing that by minimizing this error function, the pose can be recovered with decimeter accuracy in many cases. \r"
  },
  "iccv2017_w13_skimap++real-timemappingandobjectrecognitionforrobotics": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "SkiMap++: Real-Time Mapping and Object Recognition for Robotics",
    "authors": [
      "Daniele De Gregorio",
      "Tommaso Cavallari",
      "Luigi Di Stefano"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/De_Gregorio_SkiMap_Real-Time_Mapping_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/De_Gregorio_SkiMap_Real-Time_Mapping_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We introduce SkiMap++, an extension to the recently proposed SkiMap mapping framework for robot navigation. The extension deals with enriching the map withsemantic information concerning the presence in the environment of certain objects that may be usefully recognized by the robot, e.g. for the sake of grasping them. More precisely, the map can accommodate information about the spatial locations of certain 3D object features, as determined by matching the visual features extracted from the incoming frames through a random forest learned off-line from a set of object models. Thereby, evidence about the presence of object features is gathered from multiple vantage points alongside with the standard geometric mapping task, so to enable recognizing the objects and estimating their 6 DOF poses. As a result, SkiMap++ can reconstruct the geometry of large scale environments as well as localize some relevant objects therein in real-time on CPU.\r"
  },
  "iccv2017_w13_snapnet-rconsistent3dmulti-viewsemanticlabelingforrobotics": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "SnapNet-R: Consistent 3D Multi-View Semantic Labeling for Robotics",
    "authors": [
      "Joris Guerry",
      "Alexandre Boulch",
      "Bertrand Le Saux",
      "Julien Moras",
      "Aurelien Plyer",
      "David Filliat"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/Guerry_SnapNet-R_Consistent_3D_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/Guerry_SnapNet-R_Consistent_3D_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we present a new approach for semantic recognition in the context of robotics. When a robot evolves in its environment, it gets 3D information given either by its sensors or by its own motion through 3D reconstruction. Our approach uses (i) 3D-coherent synthesis of scene observations and (ii) mix them in a multi-view framework for 3D labeling. (iii) This is efficient locally (for 2D semantic segmentation) and globally (for 3D structure labeling). This allows to add semantics to the observed scene that goes beyond simple image classification, as shown on challenging datasets such as SUNRGBD or the 3DRMS Reconstruction Challenge.\r"
  },
  "iccv2017_w13_3dobjectreconstructionfromasingledepthviewwithadversariallearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "3D Object Reconstruction from a Single Depth View with Adversarial Learning",
    "authors": [
      "Bo Yang",
      "Hongkai Wen",
      "Sen Wang",
      "Ronald Clark",
      "Andrew Markham",
      "Niki Trigoni"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/Yang_3D_Object_Reconstruction_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/Yang_3D_Object_Reconstruction_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects.\r"
  },
  "iccv2017_w13_deeplearninganthropomorphic3dpointcloudsfromasingledepthmapcameraviewpoint": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "Deep Learning Anthropomorphic 3D Point Clouds from a Single Depth Map Camera Viewpoint",
    "authors": [
      "Nolan Lunscher",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/Lunscher_Deep_Learning_Anthropomorphic_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/Lunscher_Deep_Learning_Anthropomorphic_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In footwear, fit is highly dependent on foot shape, which is not fully captured by shoe size. Scanners can be used to acquire better sizing information and allow for more personalized footwear matching, however when scanning an object, many images are usually needed for reconstruction. Semantics such as knowing the kind of object in view can be leveraged to determine the full 3D shape given only one input view. Deep learning methods have been shown to be able to reconstruct 3D shape from limited inputs in highly symmetrical objects such as furniture and vehicles. We apply a deep learning approach to the domain of foot scanning, and present a method to reconstruct a 3D point cloud from a single input depth map. Anthropomorphic body parts can be challenging due to their irregular shapes, difficulty for parameterizing and limited symmetries. We train a view synthesis based network and show that our method can produce foot scans with accuracies of 1.55 mm from a single input depth map.\r"
  },
  "iccv2017_w13_deeplearningforconfidenceinformationinstereoandtofdatafusion": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "Deep Learning for Confidence Information in Stereo and ToF Data Fusion",
    "authors": [
      "Gianluca Agresti",
      "Ludovico Minto",
      "Giulio Marin",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/Agresti_Deep_Learning_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/Agresti_Deep_Learning_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper proposes a novel framework for the fusion of depth data produced by a Time-of-Flight (ToF) camera and a stereo vision system. The key problem of balancing between the two sources of information is solved by extracting confidence maps for both sources using deep learning. We introduce a novel synthetic dataset accurately representing the data acquired by the proposed setup and use it to train a Convolutional Neural Network architecture. The machine learning framework estimates the reliability of both data sources at each pixel location. The two depth fields are finally fusedenforcing the local consistency of depth data taking into account the confidence information. Experimental results show that the proposed approach increases the accuracy of the depth estimation.\r"
  },
  "iccv2017_w13_multi-viewstereowithsingle-viewsemanticmeshrefinement": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "Multi-View Stereo with Single-View Semantic Mesh Refinement",
    "authors": [
      "Andrea Romanoni",
      "Marco Ciccone",
      "Francesco Visin",
      "Matteo Matteucci"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/Romanoni_Multi-View_Stereo_with_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/Romanoni_Multi-View_Stereo_with_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Semantic 3D reconstruction only recently witnessed an increasing share of attention from the Computer Vision community. Semantic annotations allow to enforce class-dependent priors, which can improve 3D reconstruction. Existing methods propose volumetric approaches; even if successful, they do not scale well. In this paper we propose a novel method to refine both the geometry and the semantic labeling of a given mesh. We refine the geometry through a variational method that optimizes a composite energy made of a state-of-the-art pairwise photo-metric term and a novel single-view term that models the semantic consistency between the labels of the 3D mesh and those of the segmented images. We update the semantic labeling through a novel Markov Random Field that, together with the usual data and smoothness terms, takes into account class-specific priors estimated directly from the annotated mesh, in contrast to state-of-the-art methods that are based on handcrafted or learned priors.\r"
  },
  "iccv2017_w13_exploringspatialcontextfor3dsemanticsegmentationofpointclouds": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 3D Reconstruction Meets Semantics",
    "title": "Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds",
    "authors": [
      "Francis Engelmann",
      "Theodora Kontogianni",
      "Alexander Hermans",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w13/html/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Deep learning approaches have made tremendous progress in the field of semantic segmentation over the past few years. However, most current approaches operate in the 2D image space. Direct semantic segmentation of unstructured 3D point clouds is still an open research problem. The recently proposed PointNet architecture presents an interesting step ahead in that it can operate on unstructured point clouds, achieving decent segmentation results. However, it subdivides the input points into a grid of blocks and processes each such block individually. In this paper, we investigate the question how such an architecture can be extended to incorporate larger-scale spatial context. We build upon PointNet and propose two extensions that enlarge the receptive field over the 3D scene. We evaluate the proposed strategies on challenging indoor and outdoor datasets and show improved results in both scenarios.\r"
  },
  "iccv2017_w14_deterministicpolicygradientbasedroboticpathplanningwithcontinuousactionspaces": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Vision in Practice on Autonomous Robots",
    "title": "Deterministic Policy Gradient Based Robotic Path Planning with Continuous Action Spaces",
    "authors": [
      "Somdyuti Paul",
      "Lovekesh Vig"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w14/html/Paul_Deterministic_Policy_Gradient_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w14/Paul_Deterministic_Policy_Gradient_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Path planners for robotic manipulators often require precise target object locationsbased on which inverse kinematics return the required joint-angles for approaching the object. This limits their use in real domains with dynamic relative positions of objects not being readily available.We present a deterministic policy based actor-critic learning framework to encode the path planning strategy irrespective of the robot pose and target object position. This reinforcement learning (RL) agentuses two different views of the environment for planning a path to reach a given target from a random pose. On a physics based simulated environment the proposed planner yielded a 100% success rate from 100 different robot poses, with relatively fewer steps requiredto reach the target. The approach does not require conventional feature matching and triangulation based localization which is often inaccurate, and solves inverse kinematics and depth estimation using only the scene information\r"
  },
  "iccv2017_w14_lightweightmonocularobstacleavoidancebysalientfeaturefusion": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Vision in Practice on Autonomous Robots",
    "title": "Lightweight Monocular Obstacle Avoidance by Salient Feature Fusion",
    "authors": [
      "Andrea Manno-Kovacs",
      "Levente Kovacs"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w14/html/Manno-Kovacs_Lightweight_Monocular_Obstacle_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w14/Manno-Kovacs_Lightweight_Monocular_Obstacle_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a monocular obstacle avoidance method based on a novel image feature map built by fusing robust saliency features, to be used in embedded systems on lightweight autonomous vehicles. The fused salient features are a textural-directional Harris based feature map and a relative focus feature map. We present the generation of the fused salient map, along with its application for obstacle avoidance. Evaluations are performed from a saliency point of view, and for the assessment of the method's applicability for obstacle avoidance in simulated environments. The presented results support the usability of the method in embedded systems on lightweight unmanned vehicles.\r"
  },
  "iccv2017_w14_commonsensescenesemanticsforcognitiveroboticstowardsgroundingembodiedvisuo-locomotiveinteractions": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Vision in Practice on Autonomous Robots",
    "title": "Commonsense Scene Semantics for Cognitive Robotics: Towards Grounding Embodied Visuo-Locomotive Interactions",
    "authors": [
      "Jakob Suchan",
      "Mehul Bhatt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w14/html/Suchan_Commonsense_Scene_Semantics_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w14/Suchan_Commonsense_Scene_Semantics_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a commonsense, qualitative model for the semantic grounding of embodied visuo-spatial and locomotive interactions. The key contribution is an integrative methodology combining low-level visual processing with high-level, human-centred representations of space and motion rooted in artificial intelligence. We demonstrate practical applicability with examples involving object interactions, and indoor movement.\r"
  },
  "iccv2017_w14_isdeeplearningsafeforrobotvision?adversarialexamplesagainsttheicubhumanoid": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Vision in Practice on Autonomous Robots",
    "title": "Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid",
    "authors": [
      "Marco Melis",
      "Ambra Demontis",
      "Battista Biggio",
      "Gavin Brown",
      "Giorgio Fumera",
      "Fabio Roli"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w14/html/Melis_Is_Deep_Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w14/Melis_Is_Deep_Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Deep neural networks have been widely adopted in recent years, exhibiting impressive performances in several application domains. It has however been shown that they can be fooled by adversarial examples, i.e., images altered by a barely-perceivable adversarial noise, carefully crafted to mislead classification.In this work, we aim to evaluate the extent to which robot-vision systems embodying deep-learning algorithms are vulnerable to adversarial examples and propose a computationally efficient countermeasure to mitigate this threat, based on rejecting classification of anomalous inputs. We then provide a clearer understanding of the safety properties of deep networks through an intuitive empirical analysis, showing that the mapping learned by such networks essentially violates the smoothness assumption of learning algorithms. We finally discuss the main limitations of this work, including the creation of real-world adversarial examples, and sketch promising research directions.\r"
  },
  "iccv2017_w14_cadscaleinvariantframeworkforreal-timeobjectdetection": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Vision in Practice on Autonomous Robots",
    "title": "CAD: Scale Invariant Framework for Real-Time Object Detection",
    "authors": [
      "Huajun Zhou",
      "Zechao Li",
      "Chengcheng Ning",
      "Jinhui Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w14/html/Zhou_CAD_Scale_Invariant_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w14/Zhou_CAD_Scale_Invariant_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Real-time detection frameworks that typically utilize end-to-end networks to scan the entire vision range, have shown potential effectiveness in object detection. However, compared to more accurate but time-consuming frameworks, detection accuracy of existing real-time networks are still left far behind. Towards this end, this work proposes a novel CAD framework to improve detection accuracy while preserving the real-time speed. Moreover, to enhance the generalization ability of the proposed framework, we introduce maxout to approximate the correlation between image pixels and network predictions. In addition, the non-maximum weighted (NMW) is employed to eliminate the redundant bounding boxes that are considered as repetitive detections for the same objects. Extensive experiments are conducted on two detection benchmarks to demonstrate that the proposed framework achieves state-of-the-art performance.\r"
  },
  "iccv2017_w14_learningtosegmentaffordances": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Vision in Practice on Autonomous Robots",
    "title": "Learning to Segment Affordances",
    "authors": [
      "Timo Luddecke",
      "Florentin Worgotter"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w14/html/Luddecke_Learning_to_Segment_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w14/Luddecke_Learning_to_Segment_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The goal of this work is to densely predict a comparatively large set of affordances given only single RGB images. We approach this task by using a convolutional neural network based on the well-known ResNet architecture, which we blend with refinement modules recently proposed in the semantic segmentation literature. A novel cost function, capable of handling incomplete data, is introduced, which is necessary because we make use of segmentations of objects and their parts to generate affordance maps.We demonstrate both, quantitatively and qualitatively, that learning a dense predictor of affordances from an object part dataset is indeed possible and show that our model outperforms several baselines.\r"
  },
  "iccv2017_w16_realtimedynamic3dfacialreconstructionformonocularvideoin-the-wild": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "Realtime Dynamic 3D Facial Reconstruction for Monocular Video In-The-Wild",
    "authors": [
      "Shuang Liu",
      "Zhao Wang",
      "Xiaosong Yang",
      "Jianjun Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Liu_Realtime_Dynamic_3D_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Liu_Realtime_Dynamic_3D_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " With the increasing amount of videos recorded using 2D mobile cameras, the technique for recovering the 3D dynamic facial models from these monocular videos has become a necessity for many image and video editing applications. While methods based parametric 3D facial models can reconstruct the 3D shape in dynamic environment, large structural changes are ignored. Structure-from-motion methods can reconstruct these changes but assume the object to be static. To address this problem we present a novel method for realtime dynamic 3D facial tracking and reconstruction from videos captured in uncontrolled environments. Our method can track the deforming facial geometry and reconstruct external objects that protrude from the face such as glasses and hair. It also allows users to move around, perform facial expressions freely without degrading the reconstruction quality.\r"
  },
  "iccv2017_w16_symmetry-factoredstatisticalmodellingofcraniofacialshape": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "Symmetry-Factored Statistical Modelling of Craniofacial Shape",
    "authors": [
      "Hang Dai",
      "William A. P. Smith",
      "Nick Pears",
      "Christian Duncan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Dai_Symmetry-Factored_Statistical_Modelling_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Dai_Symmetry-Factored_Statistical_Modelling_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a new method for symmetry-factored statistical modelling of 3D shape. Our method comprises three novel components. First, a means to symmetrise a 3D mesh, regularised using the Laplace-Beltrami operator. Second, a symmetry-aware variant of Generalized Procrustes Analysis (GPA). Third, a means to compute a linear statistical shape model in which symmetry and asymmetric shape variation are modelled separately. We focus on human head data and build the first 3D morphable model of craniofacial asymmetry. The qualitative and quantitative evaluation demonstrates that the proposed model outperforms a linear model that does not decompose symmetric and asymmetric variation. It also validates that symmetry-aware GPA can improve the data generalisation and reconstruction ability of the standard PCA model. We will make our model and the implementation of our method publicly available.\r"
  },
  "iccv2017_w16_4dmodel-basedspatiotemporalalignmentofscriptedtaijiquansequences": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "4D Model-Based Spatiotemporal Alignment of Scripted Taiji Quan Sequences",
    "authors": [
      "Jesse Scott",
      "Robert Collins",
      "Christopher Funk",
      "Yanxi Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Scott_4D_Model-Based_Spatiotemporal_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Scott_4D_Model-Based_Spatiotemporal_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We develop a computational tool that aligns motion capture (mocap) data to videos of 24-form simplified Taiji (TaiChi) Quan, a scripted motion sequence about 5 minutes long. With only prior knowledge that the subjects in video and mocap perform a similar pose sequence, we establish inter-subject temporal synchronization and spatial alignment of mocap and video based on body joint correspondences. Through time alignment and matching the viewpoint and orientation of the video camera, the 3D body joints from mocap data of subject A can be correctly projected onto the video performance of subject B. Initial quantitative evaluation of this alignment method shows promise in offering the first validated algorithmic treatment for cross-subject comparison of Taiji Quan performances. This work opens the door to subject-specific quantified comparison of long motion sequences beyond Taiji.\r"
  },
  "iccv2017_w16_generatingmultiplediversehypothesesforhuman3dposeconsistentwith2djointdetections": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections",
    "authors": [
      "Ehsan Jahangiri",
      "Alan L. Yuille"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Jahangiri_Generating_Multiple_Diverse_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Jahangiri_Generating_Multiple_Diverse_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a method to generate multiple diverse and valid human pose hypotheses in 3D all consistent with the 2D detection of joints in a monocular RGB image. We use a novel generative model uniform (unbiased) in the space of anatomically plausible 3D poses. Our model is compositional (produces a pose by combining parts) and since it is restricted only by anatomical constraints it can generalize to every plausible human 3D pose. Removing the model bias intrinsically helps to generate more diverse 3D pose hypotheses. We argue that generating multiple pose hypotheses is more reasonable than generating only a single 3D pose based on the 2D joint detection given the depth ambiguity and the uncertainty due to occlusion and imperfect 2D joint detection. We hope that the idea of generating multiple consistent pose hypotheses can give rise to a new line of future work that has not received much attention in the literature. We used the Human3.6M dataset for empirical evaluation.\r"
  },
  "iccv2017_w16_efficientseparationbetweenprojectedpatternsformultipleprojector3dpeoplescanning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "Efficient Separation between Projected Patterns for Multiple Projector 3D People Scanning",
    "authors": [
      "Tomislav Petkovic",
      "Tomislav Pribanic",
      "Matea Donlic",
      "Peter Sturm"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Petkovic_Efficient_Separation_between_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Petkovic_Efficient_Separation_between_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Structured light 3D surface scanners are usually comprised of one projector and of one camera which provide a limited view of the object's surface. Multiple projectors and cameras must be used to reconstruct the whole surface profile. Using multiple projectors in structured light profilometry is a challenging problem due to inter-projector interferences which make pattern separation difficult. We propose the use of sinusoidal fringe patterns where each projector has its own specifically chosen set of temporal phase shifts which together comprise a DFT basis in 2P+1 points, where P is the number of projectors. Such a choice enables simple and efficient separation between projected patterns. The proposed method does not impose a limit on the number of projectors used and does not impose a limit on the projector placement. We demonstrate the applicability of the proposed method on three projectors and six cameras structured light system for human body scanning.\r"
  },
  "iccv2017_w16_abiophysical3dmorphablemodeloffaceappearance": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "A Biophysical 3D Morphable Model of Face Appearance",
    "authors": [
      "Sarah Alotaibi",
      "William A. P. Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Alotaibi_A_Biophysical_3D_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Alotaibi_A_Biophysical_3D_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Skin colour forms a curved manifold in RGB space. The variations in skin colour are largely caused by variations in concentration of the pigments melanin and hemoglobin. Hence, linear statistical models of appearance or skin albedo are insufficiently constrained (they can produce implausible skin tones) and lack compactness (they require additional dimensions to linearly approximate a curved manifold). In this paper, we propose to use a biophysical model of skin colouration in order to transform skin colour into a parameter space where linear statistical modelling can take place. Hence, we propose a hybrid of biophysical and statistical modelling. We present a two parameter spectral model of skin colouration, methods for fitting the model to data captured in a lightstage and then build our hybrid model on a sample of such registered data. We present face editing results and compare our model against a pure statistical model built directly on textures.\r"
  },
  "iccv2017_w16_towardsimplicitcorrespondenceinsigneddistancefieldevolution": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "Towards Implicit Correspondence in Signed Distance Field Evolution",
    "authors": [
      "Miroslava Slavcheva",
      "Maximilian Baust",
      "Slobodan Ilic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Slavcheva_Towards_Implicit_Correspondence_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Slavcheva_Towards_Implicit_Correspondence_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The level set framework is widely used in geometry processing due to its ability to handle topological changes and the readily accessible shape properties it provides, such as normals and curvature. However, its major drawback is the lack of correspondence preservation throughout the level set evolution. Therefore, data associated with the surface, such as colour, is lost. The objective of this paper is a variational approach for signed distance field evolution which implicitly preserves correspondences. We propose an energy functional based on a novel data term, which aligns the lowest-frequency Laplacian eigenfunction representations of the input and target shapes. As these encode information about natural deformations that the shape can undergo, our strategy manages to prevent data diffusion into the volume. We demonstrate that our system is able to preserve texture throughout articulated motion sequences, and evaluate its geometric accuracy on public data.\r"
  },
  "iccv2017_w16_learning-basedinversedynamicsofhumanmotion": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W16",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Capturing and Modeling Human Bodies, Faces and Hands",
    "title": "Learning-Based Inverse Dynamics of Human Motion",
    "authors": [
      "Petrissa Zell",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w16/html/Zell_Learning-Based_Inverse_Dynamics_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w16/Zell_Learning-Based_Inverse_Dynamics_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this work we propose a learning-based algorithm for the inverse dynamics problem of human motion. Our method uses Random Forest regression to predict joint torques and ground reaction forces from motion patterns. For this purpose we extend temporally incomplete force plate data via a direct Random Forest regression from motion parameters to force vectors. Based on the resulting completed data we estimate underlying joint torques using a modified physics-based predictive dynamics approach. The optimization results for model states and controls act as predictors and responses for the final Random Forest regression from motion to joint torques and ground reaction forces. The evaluation of our method includes a comparison to state-of-the-art results and to measured force plate data and a demonstration of the robust performance under influence of noisy and occluded input.\r"
  },
  "iccv2017_w17_vision-as-inverse-graphicsobtainingarich3dexplanationofascenefromasingleimage": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Vision-As-Inverse-Graphics: Obtaining a Rich 3D Explanation of a Scene From a Single Image",
    "authors": [
      "Lukasz Romaszko",
      "Christopher K. I. Williams",
      "Pol Moreno",
      "Pushmeet Kohli"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Romaszko_Vision-As-Inverse-Graphics_Obtaining_a_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Romaszko_Vision-As-Inverse-Graphics_Obtaining_a_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We develop an inverse graphics approach to the problem ofscene understanding, obtaining a rich representation that includes descriptions of the objects in the sceneand their spatial layout, as well as global latent variables like the camera parameters and lighting. The framework's stages include object detection, the prediction of the camera and lighting variables, and prediction of object-specific variables (shape, appearance and pose). This acts like the encoder of an autoencoder, with graphics rendering as the decoder. Importantly the scene representation is interpretable and is of variable dimension to match the detected number of objects plus the global variables. For the prediction of the camera latent variables we introduce a novel architecture termed Probabilistic HoughNets (PHNs), which provides a principled approach to combining information from multiple detections.We demonstrate the quality of the reconstructions obtained quantitatively on synthetic data, and qualitatively on real scenes.\r"
  },
  "iccv2017_w17_semantictextureforrobustdensetracking": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Semantic Texture for Robust Dense Tracking",
    "authors": [
      "Jan Czarnowski",
      "Stefan Leutenegger",
      "Andrew J. Davison"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Czarnowski_Semantic_Texture_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Czarnowski_Semantic_Texture_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We argue that robust dense SLAM systems can make valuable use of the layers of features coming from a standard CNN as a pyramid of 'semantic texture' which is suitable for dense alignment while being much more robust to nuisance factors such as lighting than raw RGB values. We use a straightforward Lucas-Kanade formulation of image alignment, with a schedule of iterations over the coarse-tofine levels of a pyramid, and simply replace the usual image pyramid by the hierarchy of convolutional feature maps from a pre-trained CNN. The resulting dense alignment performance is much more robust to lighting and other variations, as we show by camera rotation tracking experiments on time-lapse sequences captured over many hours. Looking towards the future of scene representation for real-time visual SLAM, we further demonstrate that a selection using simple criteria of a small number of the total set of features output by a CNN gives just as accurate but much more efficient tracking performance.\r"
  },
  "iccv2017_w17_graph-basedclassificationofomnidirectionalimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Graph-Based Classification of Omnidirectional Images",
    "authors": [
      "Renata Khasanova",
      "Pascal Frossard"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Khasanova_Graph-Based_Classification_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Khasanova_Graph-Based_Classification_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Omnidirectional cameras are widely used in such areas as robotics and virtual reality as they provide a wide field of view. Their images are often processed with classical methods, which might unfortunately lead to non-optimal solutions as these methods are designed for planar images that have different geometrical properties than omnidirectional ones. In this paper we study image classification task by taking into account the specific geometry of omnidirectional cameras with graph-based representations. In particular, we extend deep learning architectures to data on graphs; we propose a principled way of graph construction such that convolutional filters respond similarly for the same pattern on different positions of the image regardless of lens distortions. Our experiments show that the proposed method outperforms current techniques for the omnidirectional image classification problem. \r"
  },
  "iccv2017_w17_image-basedlocalizationusinghourglassnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Image-Based Localization Using Hourglass Networks",
    "authors": [
      "Iaroslav Melekhov",
      "Juha Ylioinas",
      "Juho Kannala",
      "Esa Rahtu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Melekhov_Image-Based_Localization_Using_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Melekhov_Image-Based_Localization_Using_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose an encoder-decoder convolutional neural network (CNN) architecture for estimating camera pose (orientation and location) from a single RGB image. The architecture has a hourglass shape consisting of a chain of convolution and up-convolution layers followed by a regression part. The upconvolution layers are introduced to preserve the fine-grained information of the input image. Following the common practice, we train our model in end-to-end manner utilizing transfer learning from large scale classification data. The experiments demonstrate the performance of the approach on data exhibiting different lighting conditions, reflections, and motion blur. The results indicate a clear improvement over the previous state-of-the art even when compared to methods that utilize sequence of test frames instead of a single frame.\r"
  },
  "iccv2017_w17_cascaderesiduallearningatwo-stageconvolutionalneuralnetworkforstereomatching": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Cascade Residual Learning: A Two-Stage Convolutional Neural Network for Stereo Matching",
    "authors": [
      "Jiahao Pang",
      "Wenxiu Sun",
      "Jimmy SJ. Ren",
      "Chengxi Yang",
      "Qiong Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Pang_Cascade_Residual_Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Pang_Cascade_Residual_Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Leveraging on the recent developments in convolutional neural networks (CNNs), matching dense correspondence from a stereo pair has been cast as a learning problem, with performance exceeding traditional approaches. However, it remains challenging to generate high-quality disparities for the inherently ill-posed regions. To tackle this problem, we propose a novel cascade CNN architecture composing of two stages. The first stage advances the recently proposed DispNet by equipping it with extra up-convolution modules, leading to disparity images with more details. The second stage explicitly rectifies the disparity initialized by the first stage; it couples with the first-stage and generates residual signals across multiple scales. The summation of the outputs from the two stages gives the final disparity. As opposed to directly learning the disparity at the second stage, we show that residual learning provides more effective refinement. Moreover, it also benefits the training of the overall cascade network. Experimentation shows that our cascade residual learning scheme provides state-of-the-art performance for matching stereo correspondence. By the time of the submission of this paper, our method ranks first in the KITTI 2015 stereo benchmark, surpassing the prior works by a noteworthy margin.\r"
  },
  "iccv2017_w17_rgb-dobjectrecognitionusingdeepconvolutionalneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "RGB-D Object Recognition Using Deep Convolutional Neural Networks",
    "authors": [
      "Saman Zia",
      "Buket Yuksel",
      "Deniz Yuret",
      "Yucel Yemez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Zia_RGB-D_Object_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Zia_RGB-D_Object_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We address the problem of object recognition from RGB-D images using deep convolutional neural networks (CNNs). We advocate the use of 3D CNNs to fully exploit the 3D spatial information in depth images as well as the use of pretrained 2D CNNs to learn features from RGB-D images. There exists currently no large scale dataset available comprising depth information as compared to those for RGB data. Hence transfer learning from 2D source data is key to be able to train deep 3D CNNs. To this end, we propose a hybrid 2D/3D convolutional neural network that can be initialized with pretrained 2D CNNs and can then be trained over a relatively small RGB-D dataset. We conduct experiments on the Washington dataset involving RGB-D images of small household objects. Our experiments show that the featureslearnt from this hybrid structure, when fused with the features learnt from depth-only and RGB-only architectures, outperform the state of the art on RGB-D category recognition.\r"
  },
  "iccv2017_w17_3dmorphablemodelsasspatialtransformernetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "3D Morphable Models as Spatial Transformer Networks",
    "authors": [
      "Anil Bas",
      "Patrik Huber",
      "William A. P. Smith",
      "Muhammad Awais",
      "Josef Kittler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Bas_3D_Morphable_Models_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Bas_3D_Morphable_Models_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we show how a 3D Morphable Model (i.e. a statistical model of the 3D shape of a class of objects such as faces) can be used to spatially transform input data as a module (a 3DMM-STN) within a convolutional neural network. This is an extension of the original spatial transformer network in that we are able to interpret and normalise 3D pose changes and self-occlusions. The trained localisation part of the network is independently useful since it learns to fit a 3D morphable model to a single image. We show that the localiser can be trained using only simple geometric loss functions on a relatively small dataset yet is able to perform robust normalisation on highly uncontrolled images including occlusion, self-occlusion and large pose changes.\r"
  },
  "iccv2017_w17_homographyestimationfromimagepairswithhierarchicalconvolutionalnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Homography Estimation From Image Pairs With Hierarchical Convolutional Networks",
    "authors": [
      "Farzan Erlik Nowruzi",
      "Robert Laganiere",
      "Nathalie Japkowicz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Nowruzi_Homography_Estimation_From_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Nowruzi_Homography_Estimation_From_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we introduce a hierarchy of twin convolutional regression networks to estimate the homography between a pair of images. In this framework, networks are stacked sequentially in order to reduce error bounds of the estimate. At every convolutional network module, features from each image are extracted independently, given a shared set of kernels, also known as Siamese network model. Later on in the process, they are merged together to estimate the homography. Further, we evaluate and compare effects of various training parameters in this context. We show that given the iterative nature of the framework, highly complicated models are not necessarily required, and high performance is achieved via hierarchical arrangement of simple models. Effectiveness of the proposed method is shown through experiments on MSCOCO dataset, in which it significantly outperforms the state-of-the-art.\r"
  },
  "iccv2017_w17_3dscenemeshfromcnndepthpredictionsandsparsemonocularslam": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "3D Scene Mesh From CNN Depth Predictions and Sparse Monocular SLAM",
    "authors": [
      "Tomoyuki Mukasa",
      "Jiu Xu",
      "Bjorn Stenger"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Mukasa_3D_Scene_Mesh_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Mukasa_3D_Scene_Mesh_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a novel framework for integrating geometrical measurements of monocular visual simultaneous localization and mapping (SLAM) and depth prediction using a convolutional neural network (CNN). In our framework, SLAM-measured sparse features and CNN- predicted dense depth maps are fused to obtain a more accurate dense 3D reconstruction including scale. We continuously update an initial 3D mesh by integrating accurately tracked sparse features points. Compared to prior work on integrating SLAM and CNN estimates [20], there are two main differences: Using a 3D mesh representation allows as-rigid-as-possible update transformations. We further propose a system architecture suitable for mobile devices, where feature tracking and CNN-based depth prediction modules are separated, and only the former is run on the device. We evaluate the framework by comparing the 3D reconstruction result with 3D measurements obtained using an RGBD sensor, showing a reduction in the mean residual error of 38% compared to CNN-based depth map prediction alone.\r"
  },
  "iccv2017_w17_camerarelocalizationbycomputingpairwiserelativeposesusingconvolutionalneuralnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network",
    "authors": [
      "Zakaria Laskar",
      "Iaroslav Melekhov",
      "Surya Kalia",
      "Juho Kannala"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Laskar_Camera_Relocalization_by_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Laskar_Camera_Relocalization_by_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a new deep learning based approach for camera relocalization. Our approach localizes a given query image by using a convolutional neural network (CNN) for first retrieving similar database images and then predicting the relative pose between the query and the database images, whose poses are known. The camera location for the query image is obtained via triangulation from two relative translation estimates using a RANSAC based approach. Each relative pose estimate provides a hypothesis for the camera orientation and they are fused in a second RANSAC scheme. The neural network is trained for relative pose estimation in an end-to-end manner using training image pairs. In contrast to previous work, our approach does not require scene-specific training of the network, which improves scalability, and it can also be applied to scenes which are not available during the training of the network. As another main contribution, we release a challenging indoor localisation dataset covering 5 different scenes registered to a common coordinate frame. We evaluate our approach using both our own dataset and the standard 7 Scenes benchmark. The results show that the proposed approach generalizes well to previously unseen scenes and compares favourably to other recent CNN-based methods.\r"
  },
  "iccv2017_w17_scalingcnnsforhighresolutionvolumetricreconstructionfromasingleimage": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Geometry Meets Deep Learning",
    "title": "Scaling CNNs for High Resolution Volumetric Reconstruction From a Single Image",
    "authors": [
      "Adrian Johnston",
      "Ravi Garg",
      "Gustavo Carneiro",
      "Ian Reid",
      "Anton van den Hengel"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w17/html/Johnston_Scaling_CNNs_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w17/Johnston_Scaling_CNNs_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " One of the long-standing tasks in computer vision is to use a single 2-D view of an object in order to produce its 3-D shape.Recovering the lost dimension in this process has been the goal of classic shape-from-X methods, but often the assumptions made in those works are quite limiting to be useful for general 3-D objects.This problem has been recently addressed with deep learning methods containing a 2-D (convolution) encoder followed by a 3-D (deconvolution) decoder. These methods have been reasonably successful, but memory and run time constraints impose a strong limitation in terms of the resolution of the reconstructed 3-D shapes.In particular, state-of-the-art methodsare able to reconstruct 3-D shapes represented by volumes of at most 32^3 voxels using state-of-the-art desktop computers. In this work, we present a scalable 2-D single view to 3-D volume reconstruction deep learning method, where the 3-D (deconvolution) decoder is replaced by a simple inverse discrete cosine transform (IDCT) decoder. Our simpler architecture has an order of magnitude faster inference when reconstructing 3-D volumes compared to the convolution-deconvolutional model, an exponentially smaller memory complexity while training and testing, and a sub-linear runtime training complexity with respect to the output volume size. We show on benchmark datasets that our method can produce high-resolution reconstructions with state of the art accuracy.\r"
  },
  "iccv2017_w18_class-specificreconstructiontransferlearningviasparselow-rankconstraint": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Class-Specific Reconstruction Transfer Learning via Sparse Low-Rank Constraint",
    "authors": [
      "Shanshan Wang",
      "Lei Zhang",
      "Wangmeng Zuo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Wang_Class-Specific_Reconstruction_Transfer_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Wang_Class-Specific_Reconstruction_Transfer_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Subspace learning and reconstruction has been widely explored in transfer learning. However, existing subspace reconstruction neglect class prior such that the learned transfer function is biased. We propose a novel reconstruction-based method called Class-specific Reconstruction Transfer Learning (CRTL), which optimizes a well-designed transfer loss function without class bias. Using a class-specific reconstruction matrix to align source domain with target domain which provides help for classification with class prior modeling. Furthermore, to keep the intrinsic relationship between data and labels after feature augmentation, a projected HSIC, that measures the dependency between two sets, is first proposed by mapping the data from original space to RKHS. In addition, combining low-rank and sparse constraints on reconstruction matrix, the global and local data structures can be effectively preserved. Extensive experiments demonstrate our method outperforms conventional methods.\r"
  },
  "iccv2017_w18_delugenetsdeepnetworkswithefficientandflexiblecross-layerinformationinflows": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "DelugeNets: Deep Networks With Efficient and Flexible Cross-Layer Information Inflows",
    "authors": [
      "Jason Kuen",
      "Xiangfei Kong",
      "Gang Wang",
      "Yap-Peng Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Kuen_DelugeNets_Deep_Networks_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Kuen_DelugeNets_Deep_Networks_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Deluge Networks (DelugeNets) are deep neural networks which efficiently facilitate massive cross-layer information inflows from preceding layers to succeeding layers. The connections between layers in DelugeNets are established through cross-layer depthwise convolutional layers with learnable filters, acting as a flexible yet efficient selection mechanism. DelugeNets can propagate information across many layers with greater flexibility and utilize network parameters more effectively compared to ResNets, whilst being more efficient than DenseNets. Remarkably, a DelugeNet model with just model complexity of 4.31 GigaFLOPs and 20.2M network parameters, achieve classification errors of 3.76% and 19.02% on CIFAR-10 and CIFAR-100 dataset respectively. Moreover, DelugeNet-122 performs competitively to ResNet-200 on ImageNet dataset, despite costing merely half of the computations needed by the latter.\r"
  },
  "iccv2017_w18_vehiclelogoretrievalbasedonhoughtransformanddeeplearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Vehicle Logo Retrieval Based on Hough Transform and Deep Learning",
    "authors": [
      "Li Huan",
      "Qin Yujian",
      "Wang Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Huan_Vehicle_Logo_Retrieval_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Huan_Vehicle_Logo_Retrieval_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Vehicle logo retrieval is an important problem for the intelligent traffic systems, which is still not reliably accurate for practical applications due to the mutable site conditions. In this paper, a new algorithm based on Hough transform and Deep Learning is proposed. The main steps are as follows: First, the logo region is located according to the prior knowledge for the location of vehicle logo and vehicle license plate. Then, typical shapes in vehicle logos, such as circle and ellipse are detected based on optimized Hough transform; meanwhile the accurate position of the logo can be obtained. Finally, the pattern of logo is classified based on Deep Belief Networks (DBNs). Comparative experiments with the actual traffic monitoring images demonstrate that the algorithm outperforms traditional methods in retrieval accuracy and speed. Moreover, the algorithm is particularly suitable for practical application.\r"
  },
  "iccv2017_w18_p-teluparametrictanhyperboliclinearunitactivationfordeepneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "P-TELU: Parametric Tan Hyperbolic Linear Unit Activation for Deep Neural Networks",
    "authors": [
      "Rahul Duggal",
      "Anubha Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Duggal_P-TELU_Parametric_Tan_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Duggal_P-TELU_Parametric_Tan_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper proposes a new activation function, namely, Parametric Tan Hyperbolic Linear Unit (P-TELU) for deep neural networks. The work is inspired from two recently proposed functions: Parametric RELU (P-RELU) and Exponential Linear Unit (ELU). The specific design of P-TELU allows it to leverage two advantages: (1) the flexibility of tuning parameters from the data distribution similar to P-RELU and (2) better noise robustness similar to ELU. Owing to larger gradient and early saturation of tan hyperbolic compared to exponential function, the proposed activation allows a neuron to reach/exit from the noise robust deactivation state earlier and faster. The performance of the proposed function is evaluated on CIFAR10 and CIFAR100 image dataset using two convolutional neural network (CNN) architectures : KerasNet, a small 6 layer CNN model, and on 76 layer deep ResNet architecture. Results demonstrate enhanced performance of the proposed activation function.\r"
  },
  "iccv2017_w18_learningefficientdeepfeaturerepresentationsviatransgenerationalgenetictransmissionofenvironmentalinformationduringevolutionarysynthesisofdeepneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Learning Efficient Deep Feature Representations via Transgenerational Genetic Transmission of Environmental Information During Evolutionary Synthesis of Deep Neural Networks ",
    "authors": [
      "Mohammad J. Shafiee",
      "Elnaz Barshan",
      "Francis Li",
      "Brendan Chwyl",
      "Michelle Karg",
      "Christian Scharfenberger",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Shafiee_Learning_Efficient_Deep_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Shafiee_Learning_Efficient_Deep_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The computational complexity of deep neural networks for extracting deep features is a significant barrier to widespread adoption, particularly for use in embedded devices.One strategy to addressing the complexity issue is the evolutionary deep intelligence framework, which has been demonstrated to enable the synthesis of highly efficient deep neural networks that retain modeling performance. Here, we introduce the notion of trans-generational genetic transmission into the evolutionary deep intelligence framework, where the intra-generational environmental traumatic stresses are imposed to synapses during training to favor the synthesis of more efficient deep neural networks over successive generations. Results demonstrate the efficacy of the proposed framework for synthesizing networks with significant decreases in synapses (e.g., for SVHN, a 230-fold increase in architectural efficiency) while maintaining modeling accuracy and a significantly more efficient feature representation.\r"
  },
  "iccv2017_w18_large-scalecontent-onlyvideorecommendation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Large-Scale Content-Only Video Recommendation",
    "authors": [
      "Joonseok Lee",
      "Sami Abu-El-Haija"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Lee_Large-Scale_Content-Only_Video_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Lee_Large-Scale_Content-Only_Video_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Traditional recommendation systems using collaborative filtering (CF) approaches work relatively well when the candidate videos are sufficiently popular. With the increase of user-created videos, however, recommending fresh videos gets more and more important, but pure CF-based systems may not perform well in such cold-start situation. In this paper, we model recommendation as a video content-based similarity learning problem, and learn deep video embeddings trained to predict video relationships identified by a co-watch-based system but using only visual and audial content. The system does not depend on availability on video meta-data, and can generalize to both popular and tail content, including new video uploads. We demonstrate performance of the proposed method in large-scale datasets, both quantitatively and qualitatively.\r"
  },
  "iccv2017_w18_efficientfine-grainedclassificationandpartlocalizationusingonecompactnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Efficient Fine-Grained Classification and Part Localization Using One Compact Network",
    "authors": [
      "Xiyang Dai",
      "Ben Southall",
      "Nhon Trinh",
      "Bogdan Matei"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Dai_Efficient_Fine-Grained_Classification_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Dai_Efficient_Fine-Grained_Classification_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Fine-grained classification of objects such as vehicles, natural objects and other classes is an important problem in visual recognition. A key contributor to fine-grained recognition are discriminative parts and regions of objects. We propose a novel compact multi-task network architecture that jointly optimizes both localization of parts and fine-grained class labels by learning from training data. The localization and classification sub-networks share most of the weights, yethave dedicated convolutional layers to capture finer level class specific information. We design our model as memory and computational efficient so that can be easily embedded in mobile applications. We demonstrate the effectiveness of our approach through experiments that achieve a new state-of-the-art 93.1% performance on the Stanford Cars-196 dataset, with a significantly smaller multi-task network (30M parameters) and significantly faster testing speed (78 FPS) compared to recent published results.\r"
  },
  "iccv2017_w18_structuredimagesforrgb-dactionrecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Structured Images for RGB-D Action Recognition",
    "authors": [
      "Pichao Wang",
      "Shuang Wang",
      "Zhimin Gao",
      "Yonghong Hou",
      "Wanqing Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Wang_Structured_Images_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Wang_Structured_Images_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents an effective yet simple video representation for RGB-D based action recognition. It proposes to represent a depth map sequence into three pairs of structured dynamic images at body, part and joint levels respectively through bidirectional rank pooling. Different from previous works that applied one Convolutional Neural Network (ConvNet) for each part/joint separately, one pair of structured dynamic images is constructed from depth maps at each granularity level and serves as the input of a ConvNet. The structured dynamic image not only preserves the spatial-temporal information but also enhances the structure information across both body parts/joints and different temporal scales. In addition, it requires low computational cost and memory to construct. The proposed representation is evaluated on five benchmark datasets, namely, MSRAction3D, G3D, MSRDailyActivity3D, SYSU 3D HOI andUTD-MHAD datasets and achieves the state-of-the-art results on all five datasets.\r"
  },
  "iccv2017_w18_compactfeaturerepresentationforimageclassificationusingelms": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Compact Feature Representation for Image Classification Using ELMs",
    "authors": [
      "Dongshun Cui",
      "Guanghao Zhang",
      "Wei Han",
      "Liyanaarachchi Lekamalage Chamara Kasun",
      "Kai Hu",
      "Guang-Bin Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Cui_Compact_Feature_Representation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Cui_Compact_Feature_Representation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Feature representation/learning is an essential step for many computer vision tasks (like image classification) and is broadly categorized as 1) deep feature representation; 2) shallow feature representation. With the development of deep neural networks, many deep feature representation methods have been proposed and obtained many remarkable results. However, they are limited to real-world applications due to the high demand for storage space and computation ability. In our work, we focus on shallow feature representation (like PCANet) as these algorithms require less storage space and computational resources. In this paper, we have proposed a Compact Feature Representation algorithm (CFR-ELM) which consists of compact feature learning module and a post-processing module. We have tested CFR-ELM on four typical image classification databases, and the results demonstrate that our method outperforms the state-of-the-art methods. \r"
  },
  "iccv2017_w18_improveddescriptorsforpatchmatchingandreconstruction": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Improved Descriptors for Patch Matching and Reconstruction",
    "authors": [
      "Rahul Mitra",
      "Jiakai Zhang",
      "Sanath Narayan",
      "Shuaib Ahmed",
      "Sharat Chandran",
      "Arjun Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Mitra_Improved_Descriptors_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Mitra_Improved_Descriptors_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a convolutional neural network (ConvNet) based approach for learning local image descriptors which can be used for significantlyimproved patch matching and 3D reconstructions. A multi-resolutionConvNet is used for learning keypoint descriptors. We also propose a new dataset consisting of an order of magnitude more number of scenes, images, and positive and negative correspondences compared to the currently available Multi-View Stereo (MVS) dataset. The new dataset also has better coverage of the overall viewpoint, scale, and lighting changes in comparison to the MVS dataset. We evaluate our approach on publicly available datasets, such as Oxford Affine Covariant Regions Dataset (ACRD), MVS, Synthetic and Strecha datasets to quantify the image descriptor performance. We evaluate patch matching performance and 3D reconstruction task. Experiments show that the proposed descriptor outperforms the current state-of-the-art descriptors in both the evaluation tasks.\r"
  },
  "iccv2017_w18_compactcolortexturedescriptorbasedonranktransformandproductorderinginthergbcolorspace": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Compact Color Texture Descriptor Based on Rank Transform and Product Ordering in the RGB Color Space",
    "authors": [
      "Antonio Fernandez",
      "David Lima",
      "Francesco Bianconi",
      "Fabrizio Smeraldi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Fernandez_Compact_Color_Texture_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Fernandez_Compact_Color_Texture_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Color information is generally considered useful for texture analysis. However, an important category of highly effective texture descriptors - namely rank features - has no obvious extension to color spaces, on which no canonical order is defined. In this work, we explore the use of partial orders in conjunction with rank features. We introduce the rank transform based on product ordering, that generalizes the classic rank transform to RGB space by a combined tally of dominated and non-comparable pixels. Experimental results on nine heterogeneous standard databases confirm that our approach outperforms the standard rank transform and its extension to lexicographic and bit mixing total orders, as well as to the preorders based on the Euclidean distance to a reference color. The low computational complexity and compact codebook size of the transform make it suitable for multi-scale approaches.\r"
  },
  "iccv2017_w18_spatial-temporalweightedpyramidusingspatialorthogonalpooling": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Spatial-Temporal Weighted Pyramid Using Spatial Orthogonal Pooling",
    "authors": [
      "Yusuke Mukuta",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Mukuta_Spatial-Temporal_Weighted_Pyramid_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Mukuta_Spatial-Temporal_Weighted_Pyramid_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Feature pooling is a method that summarizes local descriptors in an image using spatial information. Spatial pyramid matching uses the statistics of local features in an image subregion as a global feature. However, the disadvantages of this method are that there is no theoretical guideline for selecting the pooling region, robustness to small image translation is lost around the edges of the pooling region, the information encoded in the different feature pyramids overlaps, and thus recognition performance stagnates as a greater pyramid size is selected. In this research, we propose a novel interpretation that regards feature pooling as an orthogonal projection in the space of functions that maps the image space to the local feature space. Moreover, we propose a novel feature-pooling method that orthogonally projects the function form of local descriptors into the space of low-degree polynomials. Experimental results demonstrate the effectiveness of the proposed methods.\r"
  },
  "iccv2017_w18_double-taskdeepq-learningwithmultipleviews": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Double-Task Deep Q-Learning With Multiple Views",
    "authors": [
      "Jun Chen",
      "Tingzhu Bai",
      "Xiangsheng Huang",
      "Xian Guo",
      "Jianing Yang",
      "Yuxing Yao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Chen_Double-Task_Deep_Q-Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Chen_Double-Task_Deep_Q-Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Deep Reinforcement learning enables autonomous robots to learn large repertories of behavioral skill with minimal human intervention. However, the applications of direct deep reinforcement learning have been restricted. In this paper we introduce a new definition of action space and propose a double-task deep Q-Network with multiple views (DMDQN) based on double-DQN and dueling-DQN. For extension, we define multi-task model for more complex jobs.Moreover data augment policy is applied, which includes auto-sampling and action-overturn. The exploration policy is formed when DMDQN and data augment are combined. For robotic system's steady exploration, we designed the safety constraints according to working condition. Our experiments show that our double-task DQN with multiple views performs better than the single-task and single-view model. Combining our DMDQN anddata augment, the robotic system can reach the object in an exploration way. \r"
  },
  "iccv2017_w18_automaticdiscoveryofdiscriminativepartsasaquadraticassignmentproblem": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Automatic Discovery of Discriminative Parts as a Quadratic Assignment Problem",
    "authors": [
      "Ronan Sicre",
      "Julien Rabin",
      "Yannis Avrithis",
      "Teddy Furon",
      "Frederic Jurie",
      "Ewa Kijak"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Sicre_Automatic_Discovery_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Sicre_Automatic_Discovery_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Part-based image classification consists in representing categories by small sets of discriminative parts upon which a representation of the images is built. This paper addresses the question of how to automatically learn such parts from a set of labeled training images.We propose to cast the training of parts as a quadratic assignment problem in which optimal correspondences between image regions and parts are automatically learned. The paper analyses different assignment strategies and thoroughly evaluates them on two public datasets: Willow actions and MIT 67 scenes. \r"
  },
  "iccv2017_w18_udnetup-downnetworkforcompactandefficientfeaturerepresentationinimagesuper-resolution": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "UDNet: Up-Down Network for Compact and Efficient Feature Representation in Image Super-Resolution",
    "authors": [
      "Chang Chen",
      "Xinmei Tian",
      "Zhiwei Xiong",
      "Feng Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Chen_UDNet_Up-Down_Network_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Chen_UDNet_Up-Down_Network_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recently, image super-resolution (SR) using convolutional neural networks (CNNs) have achieved remarkable performance. However, there is a tradeoff between performance and speed of SR, depending on whether feature representation and learning are conducted in high-resolution (HR) or low-resolution (LR) space. Generally, to pursue real-time SR, the number of parameters in CNNs has to be restricted, which results in performance degradation. In this paper, we propose a compact and efficient feature representation for real-time SR, named up-down network (UDNet). Specifically, a novel hourglass-shape structure is introduced by combining transposed convolution and spatial aggregation. This structure enables the network to transfer the feature representations between LR and HR spaces multiple times to learn a better mapping. Comprehensive experiments demonstrate that, compared with existing CNN models, UDNet achieves real-time SR without performance degradation on widely used benchmarks.\r"
  },
  "iccv2017_w18_enlighteningdeepneuralnetworkswithknowledgeofconfoundingfactors": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Enlightening Deep Neural Networks With Knowledge of Confounding Factors",
    "authors": [
      "Yu Zhong",
      "Gil Ettinger"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Zhong_Enlightening_Deep_Neural_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Zhong_Enlightening_Deep_Neural_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Despite the popularity of deep neural networks, we still strive to better understand the underlying mechanism that drives their success. Motivated by observations that neurons in trained deep nets predict variation explaining factors indirectly related to the training tasks, we recognize that a deep network learns representations more general than the task at hand in order to disentangle impacts of multiple confounding factors governing the data and isolate the effects of the concerning factors. Consequently, we propose to augment training of deep models with information on auxiliary explanatory data factors to boost this disentanglement and improve the generalizability of trained models to compute better feature representations. We adopt this principle to build a pose-aware DCNN and demonstrate that auxiliary pose information improves the classification accuracy. It is readily applicable to improve the recognition and classification performance for various deep-learning applications.\r"
  },
  "iccv2017_w18_consistentiterativemulti-viewtransferlearningforpersonre-identification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Consistent Iterative Multi-View Transfer Learning for Person Re-Identification",
    "authors": [
      "Cairong Zhao",
      "Xuekuan Wang",
      "Yipeng Chen",
      "Can Gao",
      "Wangmeng Zuo",
      "Duoqian Miao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Zhao_Consistent_Iterative_Multi-View_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Zhao_Consistent_Iterative_Multi-View_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Inconsistent data distributions among multiple views is one of the most crucial aspects of person re-identification. To solve the problem, this paper presents a novel strategy called consistent iterative multi-view transfer learning model. The proposed model captures seven groups of multi-view visual words (MvVW) through an unsupervised cluster method (K-means) from human body. For each group of MvVW, a multi-view discriminative common subspace can be obtained by the fusion of transfer learning and discriminative analysis. In these common subspaces, the original samples can be reconstructed based on MvVW under the low-rank and sparse constraints. Then, we solve it via the inexact augmented Lagrange multiplier method. The proposed strategy is performed on three different challenging person re-identification databases (i.e., VIPeR, CUHK01 and PRID450S), which shows that our model outperforms several state-of-the-art models with improving of 6.36%, 7.7% and 4.0% respectively.\r"
  },
  "iccv2017_w18_binary-decomposeddcnnforacceleratingcomputationandcompressingmodelwithoutretraining": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Binary-Decomposed DCNN for Accelerating Computation and Compressing Model Without Retraining",
    "authors": [
      "Ryuji Kamiya",
      "Takayoshi Yamashita",
      "Mitsuru Ambai",
      "Ikuro Sato",
      "Yuji Yamauchi",
      "Hironobu Fujiyoshi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Kamiya_Binary-Decomposed_DCNN_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Kamiya_Binary-Decomposed_DCNN_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The ConvNet has a large number of parameters. This is resulting in increasingly long computation times and large model sizes. To embedding mobile devices, the model size must be compressed and computation must be accelerated.This paper proposes Binary-decomposed DCNN, which resolves these issues without the need for retraining.Our method replaces real-valued inner-product computations with binary inner-product computations in existing network models toaccelerate computation of inference and decrease model size without the need for retraining.Binary computations can be done at high speed using logical operators such as XOR and AND, together with bit counting.In tests using AlexNet with the ImageNet, speed increased by a factor of 1.79, model is compressed by approximately 80%,and increase in error rate was limited to 1.20%.With VGG-16, speed increased by a factor of 2.07, model sizes decreased by 81%, and error increased by only 2.16%.\r"
  },
  "iccv2017_w18_co-localizationwithcategory-consistentfeaturesandgeodesicdistancepropagation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Co-Localization With Category-Consistent Features and Geodesic Distance Propagation",
    "authors": [
      "Hieu Le",
      "Chen-Ping Yu",
      "Gregory Zelinsky",
      "Dimitris Samaras"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Le_Co-Localization_With_Category-Consistent_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Le_Co-Localization_With_Category-Consistent_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Co-localization is the problem of localizing objects of the same class using only the set of images that contain them. This is a challenging task because the object detector must be built without negative examples that can lead to more informative supervision signals. The main idea of our method is to cluster the feature space of a generically pre-trained CNN, to find a set of CNN features that are consistently activated for an object category, which we call category-consistent CNN features. Then, we propagate their combined activation map using superpixel geodesic distances for co-localization. In our first set of experiments, we show that the proposed method achieves state-of-the-art performance on three related benchmarks: PASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that our method is able to detect and localize truly unseen categories, on six held-out ImageNet categories with accuracy that is significantly higher than previous state-of-the-art. \r"
  },
  "iccv2017_w18_end-to-endvisualtargettrackinginmulti-robotsystemsbasedondeepconvolutionalneuralnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "End-To-End Visual Target Tracking in Multi-Robot Systems Based on Deep Convolutional Neural Network",
    "authors": [
      "Yawen Cui",
      "Bo Zhang",
      "Wenjing Yang",
      "Zhiyuan Wang",
      "Yin Li",
      "Xiaodong Yi",
      "Yuhua Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Cui_End-To-End_Visual_Target_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Cui_End-To-End_Visual_Target_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The problem of one-on-one target tracking from a single monocular image acquired from the viewpoint of a follower robot itself is studied in this paper. Previous works mainly depended on locating, onboard sensors with control mechanism, while robot may not carry advanced onboard equipment for localization or GNSS may also fail in GNSS-denied/Indoor environments. In this paper we propose a novel approach based on a deep convolutional neural network called Deep-Track, which trains a supervised image classifier only using images captured by the camera in the follower robot. Specifically, the Deep-Track system can output the estimated velocity of the target as well as the velocity control for the follower, by operating merely on two adjacent frames. In order to verify the effectiveness of Deep-Track, we build up a large-scale dataset in the simulator, in which the performance of the Deep-Track is evaluated and it is shown that a high tracking accuracy is achieved.\r"
  },
  "iccv2017_w18_oceanicscenerecognitionusinggraph-of-words(gow)": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Oceanic Scene Recognition Using Graph-Of-Words (GoW)",
    "authors": [
      "Xinghui Dong",
      "Junyu Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Dong_Oceanic_Scene_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Dong_Oceanic_Scene_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We focus on recognition of oceanic scene images. A new image dataset is collected. Although it is intuitive to use this dataset to train a CNN from scratch, the limited size of it prevents us from doing so. Instead, it has been shown that encoding the words learnt from deep convolutional features outperforms the fully-connected features extracted using a pre-trained CNN. However, these word encoders do not use the spatial layout of words. As known, this type of data is key to representation of long-range characteristics. Considering graphs are able to encode the complicated spatial layout of nodes, we propose an image descriptor: GoW, to capture the higher order spatial relationship between words. This descriptor is also fused with three word encoders to exploit richer characteristics. These descriptors produce promising results in oceanic scene recognition. We attribute these results to that GoW encodes both the short- and long-range higher-order spatial relationship between words.\r"
  },
  "iccv2017_w18_coarse-to-finedeepkernelnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Coarse-To-Fine Deep Kernel Networks",
    "authors": [
      "Hichem Sahbi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Sahbi_Coarse-To-Fine_Deep_Kernel_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Sahbi_Coarse-To-Fine_Deep_Kernel_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we address the issue of efficient computation in deep kernel networks. We propose a novel framework that reduces dramatically the complexity of evaluating these deep kernels.Our method is based on a coarse-to-fine cascade of networks designed for efficient computation; early stages of the cascade are cheap and reject many patterns efficiently whiledeep stagesare more expensive and accurate. The design principle of these reduced complexity networks is based on a variant of the cross-entropy criterion thatreduces the complexity of the networks in the cascade while preserving all the positive responses of the original kernel network.Experiments conducted -- on the challenging and time demanding change detection task, on very large satellite images --show that our proposed coarse-to-fine approach is effective and highly efficient.\r"
  },
  "iccv2017_w18_efficientconvolutionalnetworklearningusingparametriclogbaseddual-treewaveletscatternet": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Efficient Convolutional Network Learning Using Parametric Log Based Dual-Tree Wavelet ScatterNet",
    "authors": [
      "Amarjot Singh",
      "Nick Kingsbury"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Singh_Efficient_Convolutional_Network_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Singh_Efficient_Convolutional_Network_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a DTCWT ScatterNet Convolutional Neural Network (DTSCNN) formed by replacing the first few layers of a CNN network with a parametric log based DTCWT ScatterNet. The ScatterNet extracts edge based invariant representations that are used by the later layers of the CNN to learn high-level features. This improves the training of the network as the later layers can learn more complex patterns from the start of learning because the edge representations are already present. The efficient learning of the DTSCNN network is demonstrated on CIFAR-10 and Caltech-101 datasets. The generic nature of the ScatterNet front-end is shown by an equivalent performance to pre-trained CNN front-ends. A comparison with the state-of-the-art on CIFAR-10 and Caltech-101 datasets is also presented.\r"
  },
  "iccv2017_w18_4deffectvideoclassificationwithshot-awareframeselectionanddeepneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "4D Effect Video Classification With Shot-Aware Frame Selection and Deep Neural Networks",
    "authors": [
      "Thomhert S. Siadari",
      "Mikyong Han",
      "Hyunjin Yoon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Siadari_4D_Effect_Video_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Siadari_4D_Effect_Video_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " A 4D effect video played at cinema or other designated places is a video annotated with physical effects such as motion, vibration, wind, flashlight, water spray, and scent. In order to automate the time-consuming and labor-intensive process of creating such videos, we propose a new method to classify videos into 4D effect types with shot-aware frame selection and deep neural networks (DNNs). Shot-aware frame selection is a process of selecting video frames across multiple shots based on the shot length ratios to subsample every video down to a fixed number of frames for classification. For empirical evaluation, we collect a new dataset of 4D effect videos where most of the videos consist of multiple shots. Our extensive experiments show that the proposed method consistently outperforms DNNs without considering multi-shot aspect by up to 8.8% in terms of mean average precision.\r"
  },
  "iccv2017_w18_max-boost-ganmaxoperationtoboostgenerativeabilityofgenerativeadversarialnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Max-Boost-GAN: Max Operation to Boost Generative Ability of Generative Adversarial Networks",
    "authors": [
      "Xinhan Di",
      "Pengqian Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Di_Max-Boost-GAN_Max_Operation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Di_Max-Boost-GAN_Max_Operation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Generative adversarial networks (GANs) can be used to learn a generation function from a joint probability distribution as an input, and then visual samples with semantic properties can be generated from a marginal probability distribution. In this paper, we propose a novel algorithm named Max-Boost-GAN, which is demonstrated to boost the generative ability of GANs when the error of generation is upper bounded. Moreover, the Max-Boost-GAN can be used to learn the generation functions from two marginal probability distributions as the input, and samples of higher visual quality and variety could be generated from the joint probability distribution. Finally, novel objective functions are proposed for obtaining convergence during training the Max-Boost-GAN. Experiments on the generation of binary digits and RGB human faces show that the Max-Boost-GAN achieves boosted ability of generation as expected.\r"
  },
  "iccv2017_w18_multiplicativenoisechannelingenerativeadversarialnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Multiplicative Noise Channel in Generative Adversarial Networks",
    "authors": [
      "Xinhan Di",
      "Pengqian Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Di_Multiplicative_Noise_Channel_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Di_Multiplicative_Noise_Channel_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Additive Gaussian noise is widely used in generative adversarial networks (GANs). It is shown that the convergence speed is increased through the application of the additive Gaussian noise. However, the performance such as the visual quality of generated samples and semi-classification accuracy is not improved. This is partially due to the high uncertainty introduced by the additive noise. In this paper, we introduce multiplicative noise which has lower uncertainty under technical conditions, and it improves the performance of GANs. To demonstrate its practical use, two experiments including unsupervised human face generation and semi-classification tasks are conducted. The results show that it improves the state-of-art semi-classification accuracy on three benchmarks including CIFAR-10, SVHN and MNIST, as well as the visual quality and variety of generated samples on GANs with the additive Gaussian noise.\r"
  },
  "iccv2017_w18_fastcnn-baseddocumentlayoutanalysis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Fast CNN-Based Document Layout Analysis",
    "authors": [
      "Dario Augusto Borges Oliveira",
      "Matheus Palhares Viana"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Oliveira_Fast_CNN-Based_Document_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Oliveira_Fast_CNN-Based_Document_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Automatic document layout analysis is a crucial step in cognitive computing and processes that extract information out of document images. With the popularization of mobile devices and cloud-based services, the need for approaches that are both fast and economic in data usage is a reality. In this paper we propose a fast one-dimensional approach for automatic document layout analysis considering text, figures and tables based on convolutional neural networks (CNN). We take advantage of the inherently one-dimensional pattern observed in text and table blocks to reduce the dimension analysis from bi-dimensional documents images to 1D signatures, improving significantly the overall performance: we present considerably faster execution times and more compact data usage with no loss in overall accuracy if compared with a classical bi-dimensional CNN approach.\r"
  },
  "iccv2017_w18_textureandstructureincorporatedscatternethybriddeeplearningnetwork(ts-shdl)forbrainmattersegmentation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Texture and Structure Incorporated ScatterNet Hybrid Deep Learning Network (TS-SHDL) for Brain Matter Segmentation",
    "authors": [
      "Amarjot Singh",
      "Devamanyu Hazarika",
      "Aniruddha Bhattacharya"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Singh_Texture_and_Structure_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Singh_Texture_and_Structure_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Automation of brain matter segmentation from MR images is a challenging task due to the irregular boundaries between the grey and white matter regions. In addition, the presence of intensity inhomogeneity in the MR images further complicates the problem. In this paper, we propose a texture and vesselness incorporated version of the ScatterNet Hybrid Deep Learning Network (TS-SHDL) that extracts hierarchical invariant mid-level features, used by fisher vector encoding and a conditional random field (CRF) to perform the desired segmentation. The performance of the proposed network is evaluated by extensive experimentation and comparison with the state-of-the-art methods on several 2D MRI scans taken from the synthetic McGill Brain Web as well as on the MRBrainS dataset of real 3D MRI scans. The advantages of the TS-SHDL network over supervised deep learning networks is also presented in addition to its superior performance over the state-of-the-art.\r"
  },
  "iccv2017_w18_videosummarizationviamulti-viewrepresentativeselection": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Video Summarization via Multi-View Representative Selection",
    "authors": [
      "Jingjing Meng",
      "Suchen Wang",
      "Hongxing Wang",
      "Junsong Yuan",
      "Yap-Peng Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Meng_Video_Summarization_via_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Meng_Video_Summarization_via_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Video contents are inherently heterogeneous. To exploit different feature modalities in a diverse video collection for video summarization, we propose to formulate the task as a multi-view representative selection problem. The goal is to select visual elements that are representative of a video consistently across different views (i.e., feature modalities). We present the multi-view sparse dictionary selection with centroid co-regularization (MSDS-CC), which optimizes the representative selection in each view, and enforces that the view-specific selections to be similar by regularizing them towards a consensus. It can be efficiently solved by an alternating minimizing optimization with the fast iterative shrinkage thresholding algorithm. MSDS-CC can also be applied to category-specific summarization by incorporating visual co-occurrence priors. Experiments on benchmark datasets validate its effectiveness in comparison with other video summarization and representative selection methods.\r"
  },
  "iccv2017_w18_dynamiccomputationaltimeforvisualattention": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Dynamic Computational Time for Visual Attention",
    "authors": [
      "Zhichao Li",
      "Yi Yang",
      "Xiao Liu",
      "Feng Zhou",
      "Shilei Wen",
      "Wei Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Li_Dynamic_Computational_Time_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Li_Dynamic_Computational_Time_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": "We propose a dynamic computational time model to accelerate the average processing time for recurrent visual attention (RAM). Rather than attention with a fixed number of steps for each input image, the model learns to decide when to stop on the fly. To achieve this, we add an additional continue/stop action per time step to RAM and use reinforcement learning to learn both the optimal attention policy and stopping policy. The modification is simple but could dramatically save the average computational time while keeping the same recognition performance as RAM. Experimental results on CUB-200-2011 and Stanford Cars dataset demonstrate the dynamic computational model can work effectively for fine-grained image recognition.\r"
  },
  "iccv2017_w18_rotationinvariantlocalbinaryconvolutionneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Rotation Invariant Local Binary Convolution Neural Networks",
    "authors": [
      "Xin Zhang",
      "Li Liu",
      "Yuxiang Xie",
      "Jie Chen",
      "Lingda Wu",
      "Matti Pietikainen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Zhang_Rotation_Invariant_Local_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Zhang_Rotation_Invariant_Local_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Although CNNs are unprecedentedly powerful to learn effective representations, they are still parameter expensive and limited by the lack of ability to handle with the orientation transformation of the input data. To alleviate this problem, we propose a new differential module, Local Binary orientation Module(LBoM), which is a combination of Local Binary Convolution (LBC)[19] and Active Rotating Filters (ARFs)[38]. With LBoMs, a deep architecture named Rotation Invariant Local Binary Convolution Neural Networks(RI-LBCNNs) is constructed. RI-LBCNNs can be easy implemented and LBoM can be naturally inserted to popular models without any extra modification to the optimisation process. Meanwhile, The proposed RI-LBCNNs thus can be easily trained end to end. Extensive experiments show that the updating with the proposed LBoMs leads to significant reduction of learnable parameters and the reasonable performance on three benchmarks.\r"
  },
  "iccv2017_w18_thematingritualsofdeepneuralnetworkslearningcompactfeaturerepresentationsthroughsexualevolutionarysynthesis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "The Mating Rituals of Deep Neural Networks: Learning Compact Feature Representations Through Sexual Evolutionary Synthesis",
    "authors": [
      "Audrey G. Chung",
      "Mohammad Javad Shafiee",
      "Paul Fieguth",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Chung_The_Mating_Rituals_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Chung_The_Mating_Rituals_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Evolutionary deep intelligence was recently proposed as a method for achieving highly efficient deep neural network architectures over successive generations. Inspired by nature, we propose the incorporation of sexual evolutionary synthesis. Rather than the current asexual synthesis of networks, we aim to produce more compact feature representations by synthesizing more diverse and generalizable offspring networks in subsequent generations via the combination of two parent networks. Experimental results were obtained using the MNIST and CIFAR-10 datasets, and showed improved architectural efficiency and comparable testing accuracy relative to the baseline asexual evolutionary neural networks. In particular, the network synthesized via sexual evolutionary synthesis for MNIST had double the architectural efficiency (cluster efficiency of 34.29x and synaptic efficiency of 258.37x) in comparison to asexual evolutionary synthesis, with both networks achieving a testing accuracy of97%.\r"
  },
  "iccv2017_w18_few-shothashlearningforimageretrieval": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Few-Shot Hash Learning for Image Retrieval",
    "authors": [
      "Yu-Xiong Wang",
      "Liangke Gui",
      "Martial Hebert"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Wang_Few-Shot_Hash_Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Wang_Few-Shot_Hash_Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Current approaches to hash based semantic image retrieval assume a set of pre-defined categories and rely on supervised learning from a large number of annotated samples. The need for labeled samples limits their applicability in scenarios in which a user provides at query time a small set of training images defining a customized novel category. This paper addresses the problem of few-shot hash learning, in the spirit of one-shot learning in image recognition and classification and early work on locality sensitive hashing. More precisely, our approach is based on the insight that universal hash functions can be learned off-line from unlabeled data because of the information implicit in the density structure of a discriminative feature space. We can then select a task-specific combination of hash codes for a novel category from a few labeled samples. The resulting unsupervised generic hashing (UGH) significantly outperforms current supervised and unsupervised hashing approaches.\r"
  },
  "iccv2017_w18_ahandcraftednormalized-convolutionnetworkfortextureclassification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "A Handcrafted Normalized-Convolution Network for Texture Classification",
    "authors": [
      "Vu-Lam Nguyen",
      "Ngoc-Son Vu",
      "Philippe-Henri Gosselin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Nguyen_A_Handcrafted_Normalized-Convolution_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Nguyen_A_Handcrafted_Normalized-Convolution_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a Handcrafted Normalized-Convolution Network (NmzNet) for efficient texture classification. NmzNet is implemented by a three-layer normalized convolution network, which computes successive normalized convolution with a predefined filter bank (Gabor filter bank) and modulus non-linearities. Coefficients from different layers are aggregated by Fisher Vector aggregation to form the final discriminative features. The results of experimental evaluation on three texture datasets UIUC, KTH-TIPS-2a, and KTH-TIPS-2b indicate that our proposed approach achieves the good classification rate compared with other handcrafted methods. The results additionally indicate that only a marginal difference exists between the best classification rate of recent frontiers CNN and that of the proposed method on the experimented datasets.\r"
  },
  "iccv2017_w18_towardsgoodpracticesforimageretrievalbasedoncnnfeatures": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W18",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Towards Good Practices for Image Retrieval Based on CNN Features",
    "authors": [
      "Omar Seddati",
      "Stephane Dupont",
      "Said Mahmoudi",
      "Mahnaz Parian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w18/html/Seddati_Towards_Good_Practices_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w18/Seddati_Towards_Good_Practices_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Convolutional Neural Networks (CNNs) have shown their ability to provide effective descriptors for image retrieval. In this paper, we focus on CNN feature extraction for instance-level image search. We started by studying in depth several methods proposed to improve the Regional Maximal Activation (RMAC) approach. Then, we selected some of these advances and introduced a new approach that combines multi-scale and multi-layer feature extraction with feature selection. We also propose an approach for local RMAC descriptor extraction based on class activation maps. Our parameter-free approach provides short descriptors and achieves state-of-the-art performance without the need of CNN finetuning or additional data in any way . In order to demonstrate the effectiveness of our approach, we conducted extensive experiments on four well known instance-level image retrieval benchmarks (the INRIA Holidays dataset, the University of Kentucky Benchmark, Oxford5k and Paris6k).\r"
  },
  "iccv2017_w19_localgeometryinclusiveglobalshaperepresentation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Image-Based Modeling of Articulated and Deformable Objects",
    "title": "Local Geometry Inclusive Global Shape Representation",
    "authors": [
      "Somenath Das",
      "Suchendra M. Bhandarkar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w19/html/Das_Local_Geometry_Inclusive_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w19/Das_Local_Geometry_Inclusive_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " A local geometry-inclusive global representation of 3D shapes based on the shortest quasi-geodesic paths between all possible pairs of points on the shape manifold is proposed. In the proposed representation, the normal curvature values along the quasi-geodesic paths are shown preserve the local shape geometry. The eigenspectrum of the proposed global representation is exploited to characterize the shape self-symmetry. The commutative property of the shape descriptor spectrum is exploited to address region-based correspondence determination between isometric 3D shapes without requiring prior correspondence maps and to extract stable regions between 3D shapes that differ from one another by a high degree of isometry transformation. Eigenspectrum-based characterization metrics are proposed to quantify the performance of correspondence determination and self-symmetry detection and compare the performance of the proposed 3D shape descriptor with its relevant state-of-the-art counterparts.\r"
  },
  "iccv2017_w19_reliableisometricpointcorrespondencefromdepth": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Image-Based Modeling of Articulated and Deformable Objects",
    "title": "Reliable Isometric Point Correspondence From Depth",
    "authors": [
      "Emel Kupcu",
      "Yucel Yemez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w19/html/Kupcu_Reliable_Isometric_Point_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w19/Kupcu_Reliable_Isometric_Point_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a new iterative isometric point correspondence method that relies on diffusion distance to handle challenges posed by commodity depth sensors, which usuallyprovide incomplete and noisy surface data exhibiting holes and gaps. We formulate the correspondence problem as finding an optimal partial mapping between two given point sets, that minimizes deviation from isometry. Our algorithm starts with an initial rough correspondence between keypoints, obtained via a standard descriptor matching technique. This initial correspondence is then pruned and updated by iterating a perfect matching algorithm until convergence to find as many reliable correspondences as possible. For shapes with intrinsic symmetries such as human models, we additionally provide a symmetry aware extension to improve our formulation. The experiments show that our method provides state of the art performance over depth frames exhibiting occlusions, large deformations and topological noise.\r"
  },
  "iccv2017_w19_mofamodel-baseddeepconvolutionalfaceautoencoderforunsupervisedmonocularreconstruction": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Image-Based Modeling of Articulated and Deformable Objects",
    "title": "MoFA: Model-Based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction",
    "authors": [
      "Ayush Tewari",
      "Michael Zollhofer",
      "Hyeongwoo Kim",
      "Pablo Garrido",
      "Florian Bernard",
      "Patrick Perez",
      "Christian Theobalt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w19/html/Tewari_MoFA_Model-Based_Deep_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w19/Tewari_MoFA_Model-Based_Deep_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this work we propose a novel model-based deep convolutional autoencoder that addresses the highly challenging problem of reconstructing a 3D human face from a single in-the-wild color image. To this end, we combine a convolutional encoder network with an expert-designed generative model that serves as decoder. The core innovation is the differentiable parametric decoder that encapsulates image formation analytically based on a generative model. Our decoder takes as input a code vector with exactly defined semantic meaning that encodes detailed face pose, shape, expression, skin reflectance and scene illumination. For the first time, a CNN encoder and an expert-designed generative model can be trained end-to-end in an unsupervised manner, which renders training on very large (unlabeled) real world data feasible. The obtained reconstructions compare favorably to current state-of-the-art approaches in terms of quality and richness of representation.\r"
  },
  "iccv2017_w19_real-timehandtrackingunderocclusionfromanegocentricrgb-dsensor": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Image-Based Modeling of Articulated and Deformable Objects",
    "title": "Real-Time Hand Tracking Under Occlusion From an Egocentric RGB-D Sensor",
    "authors": [
      "Franziska Mueller",
      "Dushyant Mehta",
      "Oleksandr Sotnychenko",
      "Srinath Sridhar",
      "Dan Casas",
      "Christian Theobalt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w19/html/Mueller_Real-Time_Hand_Tracking_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w19/Mueller_Real-Time_Hand_Tracking_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present an approach for real-time, robust and accurate hand pose estimation from moving egocentric RGB-D cameras in cluttered real environments. Existing methods typically fail for hand-object interactions in cluttered scenes from egocentric viewpoints - common for virtual or augmented reality applications. Our approach uses two subsequent CNNs to localize the hand and regress 3D joint locations. Localization is achieved by estimating the 2D position of the hand center, even in the presence of clutter and occlusions. The localized hand position is used to generate a cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real time. For added accuracy, robustness and temporal stability, we refine the pose estimates using a kinematic pose tracking energy. To train the CNNs, we introduce a new photorealistic dataset that uses a merged reality approach to synthesize large amounts of annotated data of natural hand interaction in cluttered scenes.\r"
  },
  "iccv2017_w21_mommeanofmomentsfeatureforpersonre-identification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Manifold Learning, From Euclid to Riemann",
    "title": "moM: Mean of Moments Feature for Person Re-Identification",
    "authors": [
      "Mengran Gou",
      "Octavia Camps",
      "Mario Sznaier"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w21/html/Gou_moM_Mean_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w21/Gou_moM_Mean_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Person re-identification (re-id) has drawn significant attention in the recent decade. The design of view-invariant feature descriptors is one of the most crucial problems for this task. Covariance descriptors haveoften been used in person re-id because of their invariance properties. More recently, a new state-of-the-art performance wasachieved by also includingfirst-order moment andtwo-level Gaussian descriptors. However, using second-order or lower moments information might not be enough when the feature distribution is not Gaussian. In this paper, we address this limitation,by using the empirical (symmetric positive definite) moment matrix toincorporate higher order moments. Furthermore, the on-manifold mean can be applied to pool the features along horizontal strips. The new descriptor, based on the on-manifold mean of a moment matrix (moM),can be used to approximate more complex, non-Gaussian, distributions of the pixel features within a mid-sized local patch. \r"
  },
  "iccv2017_w21_clusteringpositivedefinitematricesbylearninginformationdivergences": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Manifold Learning, From Euclid to Riemann",
    "title": "Clustering Positive Definite Matrices by Learning Information Divergences",
    "authors": [
      "Panagiotis Stanitsas",
      "Anoop Cherian",
      "Vassilios Morellas",
      "Nikolaos Papanikolopoulos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w21/html/Stanitsas_Clustering_Positive_Definite_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w21/Stanitsas_Clustering_Positive_Definite_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Data representations based on Symmetric Positive Definite (SPD) matrices are gaining popularity in visual learning applications. When comparing SPD matrices, measures based on non-linear geometries often yield beneficial results. However, a manual selection process is commonly used to identify the appropriate measure for a visual learning application. In this paper, we study the problem of clustering SPD matrices while automatically learning a suitable measure. We propose a novel formulation that jointly (i) clusters the input SPD matrices in a K-Means setup and (ii) learns a suitable non-linear measure for comparing SPD matrices. For (ii), we capitalize on the recently introduced ab-logdet divergence, which generalizes a family of popular similarity measures on SPD matrices. Our formulation is cast in a Riemannian optimization framework and solved using a conjugate gradient scheme. We present experiments on five computer vision datasets and demonstrate state-of-the-art performance.\r"
  },
  "iccv2017_w21_marginbasedsemi-supervisedelasticembeddingforfaceimageanalysis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Manifold Learning, From Euclid to Riemann",
    "title": "Margin Based Semi-Supervised Elastic Embedding for Face Image Analysis",
    "authors": [
      "Fadi Dornaika",
      "Youssof El Traboulsi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w21/html/Dornaika_Margin_Based_Semi-Supervised_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w21/Dornaika_Margin_Based_Semi-Supervised_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper introduces a graph-based semi-supervised elastic embedding method as well as its kernelized version for face image embedding and classification. The proposed frameworks combines Flexible Manifold Embedding and non-linear graph based embedding for semi-supervised learning. In both proposed methods, the non-linear manifold and the mapping (linear transform for the linear method and the kernel multipliers for the kernelized method) are simultaneously estimated, which overcomes the shortcomings of a cascaded estimation. Unlikemany state-of-the art non-linear embedding approaches which suffer from the out-of-sample problem, our proposed methods have a direct out-of-sample extension to novel samples. We conductexperiments for tackling the face recognition and image-based face orientation problems on four public databases.These experimentsshow improvement over the state-of-the-art algorithms that are based on label propagation or graph-based semi-supervised embedding.\r"
  },
  "iccv2017_w21_coupledmanifoldlearningforretrievalacrossmodalities": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Manifold Learning, From Euclid to Riemann",
    "title": "Coupled Manifold Learning for Retrieval Across Modalities",
    "authors": [
      "Anees Kazi",
      "Sailesh Conjeti",
      "Amin Katouzian",
      "Nassir Navab"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w21/html/Kazi_Coupled_Manifold_Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w21/Kazi_Coupled_Manifold_Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Coupled Manifold Learning (CpML) is targeted at aligning data manifolds across two related modalities to facilitate similarity preserving cross-modal retrieval. Towards this we propose a learning paradigm which simultaneously aligns global topology while preserving local manifold structure. The global topologies are maintained by recovering underlying mapping functions in the joint manifold space by deploying partially corresponding instances. The inter- and intra-modality affinity matrices are then computed to reinforce original data skeleton using perturbed minimum spanning tree (pMST), and maximizing the affinity among similar cross-modal instances, respectively. The performance of proposed algorithm is evaluated upon two benchmark multi-modal image-text datasets (Wikipedia and PascalVOC2012 - Sentence). We exhaustively validate and compare CpML to other joint-manifold learning methods and demonstrate superior performance across datasets and tasks.\r"
  },
  "iccv2017_w21_learninginvariantriemanniangeometricrepresentationsusingdeepnets": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Manifold Learning, From Euclid to Riemann",
    "title": "Learning Invariant Riemannian Geometric Representations Using Deep Nets",
    "authors": [
      "Suhas Lohit",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w21/html/Lohit_Learning_Invariant_Riemannian_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w21/Lohit_Learning_Invariant_Riemannian_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Non-Euclidean constraints are inherent in many kinds of data in computer vision, often expressed in the language of Riemannian geometry. The central question this paper deals with is: How does one train deep neural nets whose final outputs are elements on a Riemannian manifold? To answer this, we propose a general framework for manifold-aware training of deep neural networks -- we utilize tangent spaces and exponential maps in order to convert the proposed problem into a form that allows us to bring current advances in deep learning to bear upon this problem. We describe two specific applications to demonstrate this approach: prediction of probability distributions for multi-class image classification, and prediction of illumination-invariant subspaces from a single face-image via regression on the Grassmannian. These applications show the generality of the proposed framework, and result in improved performance over baselines that ignore the geometry of the output space.\r"
  },
  "iccv2017_w22_alongshort-termmemoryconvolutionalneuralnetworkforfirst-personvisionactivityrecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "A Long Short-Term Memory Convolutional Neural Network for First-Person Vision Activity Recognition",
    "authors": [
      "Girmaw Abebe",
      "Andrea Cavallaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Abebe_A_Long_Short-Term_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Abebe_A_Long_Short-Term_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Temporal information is the main source of discriminating characteristics for the recognition of proprioceptive activities in first-person vision (FPV). In this paper, we propose a motion representation that uses stacked spectrograms. These spectrograms are generated over temporal windows from mean grid-optical-flow vectors and the displacement vectors of the intensity centroid.The stacked representation enables us to use 2D convolutions to learn and extract global motion features. Moreover, we employ a long short-term memory (LSTM) network to encode the temporal dependency among consecutive samples recursively. Experimental results show that the proposed approach achieves state-of-the-art performance in the largest public dataset for FPV activity recognition.\r"
  },
  "iccv2017_w22_behave-behavioralanalysisofvisualeventsforassistedlivingscenarios": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "BEHAVE - Behavioral Analysis of Visual Events for Assisted Living Scenarios",
    "authors": [
      "Carlos Fernando Crispim-Junior",
      "Jonas Vlasselaer",
      "Anton Dries",
      "Francois Bremond"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Crispim-Junior_BEHAVE_-_Behavioral_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Crispim-Junior_BEHAVE_-_Behavioral_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper proposes BEHAVE, a person-centered pipeline for probabilistic event recognition. The proposed pipeline firstly detects the set of people at a video frame, then it searches for correspondences between people in the current and previous frames (i.e., people tracking). Finally, event recognition is carried for each person using probabilistic logic models (PLMs, ProbLog2 language). PLMs represent interactions among people, home appliances and semantic regions. They also enable one to assess the probability of an event given noisy observations of the real world. BEHAVE was evaluated on the task of online (non-clipped videos) and open-set event recognition (e.g., target events plus none class) on video recordings of seniors carrying out daily tasks. Results have shown that BEHAVE improves event recognition accuracy by handling missed and partially satisfied logic models. Future work will investigate how to extend PLMs to represent temporal relations among events.\r"
  },
  "iccv2017_w22_recurrentassistancecross-datasettrainingoflstmsonkitchentasks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Recurrent Assistance: Cross-Dataset Training of LSTMs on Kitchen Tasks",
    "authors": [
      "Toby Perrett",
      "Dima Damen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Perrett_Recurrent_Assistance_Cross-Dataset_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Perrett_Recurrent_Assistance_Cross-Dataset_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we investigate whether it is possible to leverage information from multiple datasets when performing frame-based action recognition, which is an essential component of real-time activity monitoring systems.In particular, we investigate whether the training of an LSTM can benefit from pre-training or co-training on multiple datasets of related tasks when it uses non-transferred visual CNN features.A number of label mappings and multi-dataset training techniques are proposed and tested on three challenging kitchen activity datasets - Breakfast, 50 Salads and MPII Cooking 2. We show that transferring, by pre-training on similar datasets using label concatenation, delivers improved frame-based classification accuracy and faster training convergence than random initialisation.\r"
  },
  "iccv2017_w22_robusthumanposetrackingforrealisticservicerobotapplications": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Robust Human Pose Tracking for Realistic Service Robot Applications",
    "authors": [
      "Manolis Vasileiadis",
      "Sotiris Malassiotis",
      "Dimitrios Giakoumis",
      "Christos-Savvas Bouganis",
      "Dimitrios Tzovaras"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Vasileiadis_Robust_Human_Pose_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Vasileiadis_Robust_Human_Pose_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Robust human pose estimation and tracking plays an integral role in assistive service robot applications, as it provides information regarding the body pose and motion of the user in a scene. Even though current solutions provide high-accuracy results in controlled environments, they fail to successfully deal with problems encountered under real-life situations such as tracking initialization and failure, body part intersection, large object handling and partial-view body-part tracking. This paper presents a framework tailored for deployment under real-life situations addressing the above limitations. The framework is based on the articulated 3D-SDF data representation model, and has been extended with complementary mechanisms for addressing the above challenges.Extensive evaluation on public datasets demonstrates the framework's state-of-the-art performance, while experimental results on a challenging realistic human motion dataset exhibit its robustness in real life scenarios. \r"
  },
  "iccv2017_w22_avision-basedsystemforin-bedposturetracking": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "A Vision-Based System for In-Bed Posture Tracking",
    "authors": [
      "Shuangjun Liu",
      "Sarah Ostadabbas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Liu_A_Vision-Based_System_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Liu_A_Vision-Based_System_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Tracking human sleeping postures over time provides critical information to biomedical research including studies on sleeping behaviors and bedsore prevention. In this paper, we introduce a vision-based tracking system for pervasive yet unobtrusive long-term monitoring of in-bed postures in different environments. Once trained, our system generates an in-bed posture tracking history (iPoTH) report by applying a hierarchical inference model on the top view videos collected from any regular off-the-shelf camera. Although being based on a supervised learning structure, our model is person-independent and can be trained off-line and applied to new users without additional training. Experiments were conducted in both a simulated hospital environment and a home-like setting. In the hospital setting, posture detection accuracy using several mannequins was up to 91.0%, while the test with actual human participants in a home-like setting showed an accuracy of 93.6%.\r"
  },
  "iccv2017_w22_adaptivebinarizationforweaklysupervisedaffordancesegmentation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Adaptive Binarization for Weakly Supervised Affordance Segmentation",
    "authors": [
      "Johann Sawatzky",
      "Jurgen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Sawatzky_Adaptive_Binarization_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Sawatzky_Adaptive_Binarization_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The concept of affordance is important to understand the relevance of object parts for a certain functional interac- tion. Affordance types generalize across object categories and are not mutually exclusive. This makes the segmenta- tion of affordance regions of objects in images a difficult task. In this work, we build on an iterative approach that learns a convolutional neural network for affordance seg- mentation from sparse keypoints. During this process, the predictions of the network need to be binarized. To this end, we propose an adaptive approach for binarization and estimate the parameters for initialization by approximated cross validation. We evaluate our approach on two affor- dance datasets where our approach outperforms the state- of-the-art for weakly supervised affordance segmentation.\r"
  },
  "iccv2017_w22_inertial-visioncross-domainknowledgetransferforwearablesensors": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Inertial-Vision: Cross-Domain Knowledge Transfer for Wearable Sensors",
    "authors": [
      "Girmaw Abebe",
      "Andrea Cavallaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Abebe_Inertial-Vision_Cross-Domain_Knowledge_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Abebe_Inertial-Vision_Cross-Domain_Knowledge_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Multi-modal ego-centric data from inertial measurement units (IMU) and first-person videos (FPV) can be effectively fused to recognise proprioceptive activities. Existing IMU-based approachesmostly employ cascades of handcrafted triaxial motion features or deep frameworkstrained on limited data. FPV approaches generally encode scene dynamics with motion and pooled appearance features.In this paper, we propose a multi-modal ego-centric proprioceptive activity recognition that uses a convolutional neural network (CNN) followed by a long short-term memory (LSTM) network, transfer learning and a merit-based fusion of IMU and/or FPV streams. The CNN encodes short-term temporal dynamics of the ego-motion and the LSTM exploits the long-term temporal dependency among activities.The merit of a stream is evaluated with a sparsity measure of its initial classification output. Wevalidate the proposed framework on multiple visual and inertial datasets.\r"
  },
  "iccv2017_w22_acomputervisionbasedapproachforunderstandingemotionalinvolvementsinchildrenwithautismspectrumdisorders": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "A Computer Vision Based Approach for Understanding Emotional Involvements in Children With Autism Spectrum Disorders",
    "authors": [
      "Marco Del Coco",
      "Marco Leo",
      "Pierluigi Carcagni",
      "Paolo Spagnolo",
      "Pier Luigi Mazzeo",
      "Massimo Bernava",
      "Flavia Marino",
      "Giovanni Pioggia",
      "Cosimo Distante"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Del_Coco_A_Computer_Vision_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Del_Coco_A_Computer_Vision_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " It has been proved that Autism Spectrum Disorders (ASD) are associated with amplified emotional responses and poor emotional control. Underlying mechanisms and characteristics of these difficulties in using, sharing and responding to emotions are still not understood.Recent non-invasive technological frameworks based on computer vision can be applied to overcome this knowledge gap and this paper is right aimed at demonstrating how facial measurements from images can be exploited to compare how ASD children react to external stimuli with respect a control set of children.\r"
  },
  "iccv2017_w22_posturalassessmentindentistrybasedonmultiplemarkerstracking": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Postural Assessment in Dentistry Based on Multiple Markers Tracking",
    "authors": [
      "Marco Marcon",
      "Alberto Pispero",
      "Nicola Pignatelli",
      "Giovanni Lodi",
      "Stefano Tubaro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Marcon_Postural_Assessment_in_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Marcon_Postural_Assessment_in_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Postural assessment is a fundamental aspect to prevent long-term Musculoskeletal disorders (MSDs) due to fatiguing jobs. Operative dentistry also belongs to this category and we developed a Computer Vision approach to automatically analyze the dentist posture during operations obtaining an evaluation of MSD risk according to some well-established criteria like RULA and NERPA. In particular we analyze three different set-ups where the dentist operates with naked eyes, medical loupes or using a surgical microscope and we compared the postural effects of these three different configurations. The results present a significant improvement in posture using the microscope and validated our approach as a feasible and effective method to assess posture in fatiguing jobs. The proposed approach allows a continuous monitoring of job activity evaluating accurately posture criticalities. Furthermore the risk of MSD based on international criteria is evaluated in an objective and accurate way.\r"
  },
  "iccv2017_w22_useofthermalpointcloudforthermalcomfortmeasurementandhumanposeestimationinroboticmonitoring": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Use of Thermal Point Cloud for Thermal Comfort Measurement and Human Pose Estimation in Robotic Monitoring",
    "authors": [
      "Kaichiro Nishi",
      "Mitsuhiro Demura",
      "Jun Miura",
      "Shuji Oishi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Nishi_Use_of_Thermal_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Nishi_Use_of_Thermal_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper describes applications of thermal point cloud to lifestyle support robots. 3D information is useful for recognizing human and objects based on their shapes, while thermal information is useful for assessing the residential and the human states as well as for detecting human. Combining these two kinds of information will be beneficial to the robots which live with and support people at home or in care houses. This paper shows two applications of thermal point cloud. One is thermal comfort measurement based on predictive mean vote (PMV) which uses, as one of the factors, the amount of clothing estimated by thermal information. The other is human pose estimation only by depth images, which has an advantages in terms of privacy and insensitivity to illumination changes. We developed methods for these applications and show experimental results. \r"
  },
  "iccv2017_w22_usingtechnologydevelopedforautonomouscarstohelpnavigateblindpeople": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Using Technology Developed for Autonomous Cars to Help Navigate Blind People",
    "authors": [
      "Manuel Martinez",
      "Alina Roitberg",
      "Daniel Koester",
      "Rainer Stiefelhagen",
      "Boris Schauerte"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Martinez_Using_Technology_Developed_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Martinez_Using_Technology_Developed_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Autonomous driving is currently a very active research area with virtually all automotive manufacturers competing to bring the first fully autonomous car to the market. This race leads to billions of dollars being invested in the development of novel sensors, processing platforms, and algorithms.In this paper, we explore the synergies between the challenges in self-driving technology and development of navigation aids for blind people. We aim to leverage the recently emerged methods for self-driving cars, and use it to develop assistive technology for the visually impaired. In particular we focus on the task of perceiving the environment in real-time from cameras. We review current developments in embedded platforms for real-time computation as well as current algorithms for image processing, obstacle segmentation and classification.As a proof-of-concept, we build an obstacle avoidance system for blind people that is based on a hardware platform used in the automotive industry. \r"
  },
  "iccv2017_w22_vision-basedfallenpersondetectionfortheelderly": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Vision-Based Fallen Person Detection for the Elderly",
    "authors": [
      "Markus D. Solbach",
      "John K. Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Solbach_Vision-Based_Fallen_Person_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Solbach_Vision-Based_Fallen_Person_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Falls are serious and costly for elderly people. The Centers for Disease Control and Prevention of the US reports that millions of older people, 65 and older, fall each year at least once. Serious injuries such as; hip fractures, broken bones or head injury, are caused by 20% of the falls. The time it takes to respond and treat a fallen person is crucial. With this paper we present a new , non-invasive system for fallen people detection. Our approach uses only stereo camera data for passively sensing the environment. The key novelty is a human fall detector which uses a CNN based human pose estimator in combination with stereo data to reconstruct the human pose in 3D and estimate the ground plane in 3D. We have tested our approach in different scenarios covering most activities elderly people might encounter living at home. Based on our extensive evaluations, our systems shows high accuracy and almost no miss-classification. Our implementation is publicly available.\r"
  },
  "iccv2017_w22_mindthegapvirtualshorelinesforblindandpartiallysightedpeople": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Mind the Gap: Virtual Shorelines for Blind and Partially Sighted People",
    "authors": [
      "Daniel Koester",
      "Maximilian Awiszus",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Koester_Mind_the_Gap_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Koester_Mind_the_Gap_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Blind and partially sighted people have encountered numerous devices to improve their mobility and orientation, yet most still rely on traditional techniques, such as the white cane or a guide dog. In this paper, we consider improving the actual orientation process through the creation of routes that are better suited towards specific needs. More precisely, this work focuses on routing for blind and partially sighted people on a shoreline like level of detail, modeled after real world white cane usage. Our system is able to create such fine-grained routes through the extraction of routing features from openly available geolocation data, e.g., building facades and road crossings. More importantly, the generated routes provide a measurable safety benefit, as they reduce the number of unmarked pedestrian crossings and try to utilize much more accessible alternatives. Our evaluation shows that such a fine-grained routing can improve users' safety.\r"
  },
  "iccv2017_w22_seeingwithoutsight-anautomaticcognitionsystemdedicatedtoblindandvisuallyimpairedpeople": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Seeing Without Sight - An Automatic Cognition System Dedicated to Blind and Visually Impaired People",
    "authors": [
      "Ruxandra Tapu",
      "Bogdan Mocanu",
      "Titus Zaharia"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Tapu_Seeing_Without_Sight_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Tapu_Seeing_Without_Sight_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we present an automatic cognition system, based on computer vision algorithms and convolutional neural networks, designed to assist the visually impaired users during navigation in highly dynamic urban scenes. A first feature concerns the real-time detection of various types of objects existent in the outdoor environment relevant from the perspective of a VI person. The objects are followed between successive frames using a novel tracker, which exploits an offline trained neural-network and is able to track generic objects using motion patterns and visual attention models. The system is able to handle occlusions, sudden camera/object movements, rotation or various complex changes. Finally, an object classification module is proposed with new categories specific to assistive devices applications. The experimental evaluation, performed on the VOT 2016 dataset and on a set of videos acquired with the help of VI users, demonstrates the effectiveness of the proposed method.\r"
  },
  "iccv2017_w22_estimatingposition&velocityin3dspacefrommonocularvideosequencesusingadeepneuralnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Estimating Position & Velocity in 3D Space From Monocular Video Sequences Using a Deep Neural Network",
    "authors": [
      "Arturo Marban",
      "Vignesh Srinivasan",
      "Wojciech Samek",
      "Josep Fernandez",
      "Alicia Casals"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Marban_Estimating_Position__ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Marban_Estimating_Position__ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This work describes a regression model based on Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks for tracking objects from monocular video sequences. The target application being pursued is Vision-Based Sensor Substitution (VBSS). In particular, the tool-tip position and velocity in 3D space of a pair of surgical robotic instruments (SRI) are estimated for three surgical tasks, namely suturing, needle-passing and knot-tying. The CNN extracts features from individual video frames and the LSTM network processes these features over time and continuously outputs a 12-dimensional vector with the estimated position and velocity values. A series of analyses and experiments are carried out in the regression model to reveal the benefits and drawbacks of different design choices...\r"
  },
  "iccv2017_w22_toveerornottoveerlearningfromexpertshowtostaywithinthecrosswalk": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "To Veer or Not to Veer: Learning From Experts How to Stay Within the Crosswalk",
    "authors": [
      "Manfred Diaz",
      "Roger Girgis",
      "Thomas Fevens",
      "Jeremy Cooperstock"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Diaz_To_Veer_or_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Diaz_To_Veer_or_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " One of the many challenges faced by visually impaired (VI) individuals is the crossing of intersections while remaining within the crosswalk. We present a Learning from Demonstration (LfD) approach to tackle this problem and provide VI users with an assistive agent. Contrary to previous methods, our solution does not presume the existence of particular features in crosswalks. The application of the LfD framework helped us transfer sighted individuals' abilities to the intelligent assistive agent. Our proposed approach started from a collection of 215 demonstrative videos of intersection crossings executed by sighted individuals (\"the experts\"). We labeled the video frames to gather the experts' recommended actions, and then applied a policy derivation technique to extract the optimal behavior using state-of-the-art Convolutional Neural Networks. Finally, to assess the feasibility of such a solution, we evaluated the performance of the trained agent in predicting expert actions.\r"
  },
  "iccv2017_w22_computervisionforthevisuallyimpairedthesoundofvisionsystem": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Computer Vision for the Visually Impaired: The Sound of Vision System",
    "authors": [
      "Simona Caraiman",
      "Anca Morar",
      "Mateusz Owczarek",
      "Adrian Burlacu",
      "Dariusz Rzeszotarski",
      "Nicolae Botezatu",
      "Paul Herghelegiu",
      "Florica Moldoveanu",
      "Pawel Strumillo",
      "Alin Moldoveanu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Caraiman_Computer_Vision_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Caraiman_Computer_Vision_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents a computer vision based sensory substitution device for the visually impaired. Its main objective is to provide the users with a 3D representation of the environment around them, conveyed by means of the hearing and tactile senses. One of the biggest challenges for this system is to ensure pervasiveness, i.e., to be usable in any indoor or outdoor environments and in any illumination conditions. This work reveals both the hardware (3D acquisition system) and software (3D processing pipeline) used for developing this sensory substitution device and provides insight on its exploitation in various scenarios. Preliminary experiments with blind users revealed good usability results and provided valuable feedback for system improvement.\r"
  },
  "iccv2017_w22_asharedautonomyapproachforwheelchairnavigationbasedonlearneduserpreferences": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "A Shared Autonomy Approach for Wheelchair Navigation Based on Learned User Preferences",
    "authors": [
      "Yizhe Chang",
      "Mohammed Kutbi",
      "Nikolaos Agadakos",
      "Bo Sun",
      "Philippos Mordohai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Chang_A_Shared_Autonomy_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Chang_A_Shared_Autonomy_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Research on robotic wheelchairs covers a broad range from complete autonomy to shared autonomy to manual navigation by a joystick or other means. Shared autonomy is valuable because it allows the user and the robot to complement each other, to correct each other's mistakes and to avoid collisions. In this paper, we present an approach that can learn to replicate path selection according to the wheelchair user's individual, often subjective, criteria in order to reduce the number of times the user has to intervene during automatic navigation. This is achieved by learning to rank paths using a support vector machine trained on selections made by the user in a simulator. If the classifier's confidence in the top ranked path is high, it is executed without requesting confirmation from the user. Otherwise, the choice is deferred to the user. Simulations and laboratory experiments using two path generation strategies demonstrate the effectiveness of our approach.\r"
  },
  "iccv2017_w22_awearableassistivetechnologyforthevisuallyimpairedwithdoorknobdetectionandreal-timefeedbackforhand-to-handlemanipulation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "A Wearable Assistive Technology for the Visually Impaired With Door Knob Detection and Real-Time Feedback for Hand-To-Handle Manipulation",
    "authors": [
      "Liang Niu",
      "Cheng Qian",
      "John-Ross Rizzo",
      "Todd Hudson",
      "Zichen Li",
      "Shane Enright",
      "Eliot Sperling",
      "Kyle Conti",
      "Edward Wong",
      "Yi Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Niu_A_Wearable_Assistive_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Niu_A_Wearable_Assistive_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose an AI-driven wearable assistive technology that integrates door handle detection, user's real-time hand position in relation to this targeted object, and audio feedback for \"joy stick-like command\" for acquisition of the target and subsequent hand-to-handle manipulation. When fully envisioned, this platform will help end users locate doors and door handles and reach them with feedback, enabling them to travel safely and efficiently when navigating through environments with thresholds. Compared to the usual computer vision models, the one proposed in this paper requires significantly fewer computational resources, which allows it to pair with a stereoscopic camera running on a small graphics processing unit (GPU). This permits us to take advantage of its convenient portability. We also introduce a dataset containing different types of door handles and door knobs with bounding-box annotations, which can be used for training and testing in future research.\r"
  },
  "iccv2017_w22_aninnovativesalientobjectdetectionusingcenter-darkchannelprior": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "An Innovative Salient Object Detection Using Center-Dark Channel Prior",
    "authors": [
      "Chunbiao Zhu",
      "Ge Li",
      "Wenmin Wang",
      "Ronggang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Zhu_An_Innovative_Salient_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Zhu_An_Innovative_Salient_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Saliency detection aims to detect the most attractive objects in images, which has been widely used as a foundation for various multimedia applications. In this paper, we propose a novel salient object detection algorithm for RGB-D images using center-dark channel prior. First, we generate an initial saliency map based on a color saliency map and a depth saliency map of a given RGB-D image. Then, we generate a center-dark channel map based on a center saliency prior and a dark channel prior. Finally, we fuse the initial saliency map with the center dark channel map to generate the final saliency map. The proposed algorithm is evaluated on two public RGB-D datasets, and the experimental results show that our method outperforms the state-of-the-art methods.\r"
  },
  "iccv2017_w22_depthandmotioncueswithphosphenepatternsforprostheticvision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Depth and Motion Cues With Phosphene Patterns for Prosthetic Vision",
    "authors": [
      "Alejandro Perez-Yus",
      "Jesus Bermudez-Cameo",
      "Gonzalo Lopez-Nicolas",
      "Jose J. Guerrero"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Perez-Yus_Depth_and_Motion_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Perez-Yus_Depth_and_Motion_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recent research demonstrates that visual prostheses are able to provide visual perception to people with some kind of blindness. In visual prostheses, image information from the scene is transformed to a phosphene pattern to be sent to the implant. This is a complex problem where the main challenge is the very limited spatial and intensity resolution. Moreover, depth perception, which is relevant to perform agile navigation, is lost and codifying the semantic information to phosphene patterns remains an open problem. In this work, we consider the framework of perception for navigation where aspects such as obstacle avoidance are critical. We propose using a head-mounted RGB-D camera to detect free-space, obstacles and scene direction in front of the user. The main contribution is a new approach to represent depth information and provide motion cues by using particular phosphene patterns. The effectiveness of this approach is tested in simulation with real data from indoor environments.\r"
  },
  "iccv2017_w22_diabetes60-inferringbreadunitsfromfoodimagesusingfullyconvolutionalneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Diabetes60 - Inferring Bread Units From Food Images Using Fully Convolutional Neural Networks",
    "authors": [
      "Patrick Ferdinand Christ",
      "Sebastian Schlecht",
      "Florian Ettlinger",
      "Felix Grun",
      "Christoph Heinle",
      "Sunil Tatavatry",
      "Seyed-Ahmad Ahmadi",
      "Klaus Diepold",
      "Bjoern H. Menze"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Christ_Diabetes60_-_Inferring_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Christ_Diabetes60_-_Inferring_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we propose a challenging new computer vision task of inferring Bread Units (BUs) from food images. Assessing nutritional information and nutrient volume from a meal is an important task for diabetes patients. At the moment, diabetes patients learn the assessment of BUs on a scale of one to ten, by learning correspondence of BU and meals from textbooks. We introduce a large scale data set of around 9k different RGB-D images of 60 western dishes acquired using a Microsoft Kinect v2 sensor. We recruited 20 diabetes patients to give expert assessments of BU values to each dish based on several images. For this task, we set a challenging baseline using state-of-the-art CNNs and evaluated it against the performance of human annotators. In our work we present a CNN architecture to infer the depth from RGB-only food images to be used in BU regression such that the pipeline can operate on RGB data only and compare its performance to RGB-D input data.\r"
  },
  "iccv2017_w22_dsddepthstructuraldescriptorforedge-basedassistivenavigation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "DSD: Depth Structural Descriptor for Edge-Based Assistive Navigation",
    "authors": [
      "David Feng",
      "Shaodi You",
      "Nick Barnes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Feng_DSD_Depth_Structural_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Feng_DSD_Depth_Structural_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Structural edge detection is the task of finding edges between significant surfaces in a scene. This can underpin many computer vision tasks such as sketch recognition and 3D scene understanding, and is important for conveying scene structure for navigation with assistive vision. Identifying structural edges from a depth image can be challenging because surface structure that differentiates edges is not well represented in this format. We derive a depth input encoding, the Depth Surface Descriptor (DSD), that captures the first order properties of surfaces, allowing for improved classification of surface geometry that corresponds to structural edges. We apply the DSD feature to salient edge detection on RGB-D images using a fully convolutional neural network with deep supervision. We evaluate our method on both a new RGB-D dataset containing prosthetic vision scenarios, and the SUNRGBD dataset, and show that our approach produces improved performance compared to existing methods by 4%.\r"
  },
  "iccv2017_w22_improvedstrategiesforhpeemployinglearning-by-synthesisapproaches": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W22",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Assistive Computer Vision and Robotics",
    "title": "Improved Strategies for HPE Employing Learning-By-Synthesis Approaches",
    "authors": [
      "Andoni Larumbe",
      "Mikel Ariz",
      "Jose J. Bengoechea",
      "Ruben Segura",
      "Rafael Cabeza",
      "Arantxa Villanueva"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w22/html/Larumbe_Improved_Strategies_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w22/Larumbe_Improved_Strategies_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The first contribution of this paper is the presentation of a synthetic video database where the groundtruth of 2D facial landmarks and 3D head poses is available to be used for training and evaluating Head Pose Estimation (HPE) methods. The database is publicly available and contains videos of users performing guided and natural movements. The second and main contribution is the submission of a hybrid method for HPE based on Pose from Ortography and Scaling by Iterations (POSIT). The 2D landmark detection is performed using Random Cascaded-Regression Copse (R-CR-C). For the training stage we use, state of the art labeled databases. Learning-by-synthesis approach has been also used to augment the size of the database employing the synthetic database. HPE accuracy is tested by using two literature 3D head models. The tracking method proposed has been compared with state of the art methods using Supervised Descent Regressors (SDR) in terms of accuracy, achieving an improvement of 60%.\r"
  },
  "iccv2017_w23_improvingfaceverificationandpersonre-identificationaccuracyusinghyperplanesimilarity": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Improving Face Verification and Person Re-Identification Accuracy Using Hyperplane Similarity",
    "authors": [
      "Michael Jones",
      "Hiroko Kobori"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Jones_Improving_Face_Verification_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Jones_Improving_Face_Verification_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The standard framework for using a convolutional neural network (CNN)for face verification is to compare the feature vectors taken from the penultimate network layer of a CNN trained to classify the identity of an input face using a softmax loss over identities.Feature vectors are typically compared using the simple L2 distance.We demonstrate that the L2 distance is not the best distance to use in this scenario, and propose the hyperplane similarity as a more appropriate similarity function that is derived from the softmax loss function used to train the network.We demonstrate that hyperplane similarity improves verification results especially for low false acceptance rates which are usually the most important operating regimes for real applications.We also propose a fast algorithm for finding the separating hyperplanes needed to compute hyperplane similarity.\r"
  },
  "iccv2017_w23_fastandaccuratefacerecognitionwithimagesets": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Fast and Accurate Face Recognition With Image Sets",
    "authors": [
      "Hakan Cevikalp",
      "Hasan Serhan Yavuz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Cevikalp_Fast_and_Accurate_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Cevikalp_Fast_and_Accurate_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this study, we propose a fast and accurate method to approximate the distances from gallery images to the region spanned by the query set for large-scale facerecognition applications using image sets. To this end, we introduce a new polyhedral conic classifier that will enable us to compute those distances efficiently by using simple dot products. We also derive one-class formulation of the proposed classifier that can use query set examples only. This makes the method ideal for real-time applications since testing time approximately becomes the independent of the size of the gallery set. One-class formulation can also be used in a cascade system with more complex and time-consuming methods to return the most promising candidate gallery sets in the first stage of the cascade so that more complex methods can be run on those a few candidate sets. The proposed methods achieve the best accuracies on all tested small and moderate sized datasets.\r"
  },
  "iccv2017_w23_towarddescribinghumangaitsbyonomatopoeias": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Toward Describing Human Gaits by Onomatopoeias",
    "authors": [
      "Hirotaka Kato",
      "Takatsugu Hirayama",
      "Yasutomo Kawanishi",
      "Keisuke Doman",
      "Ichiro Ide",
      "Daisuke Deguchi",
      "Hiroshi Murase"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Kato_Toward_Describing_Human_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Kato_Toward_Describing_Human_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Native Japanese people can distinguish gaits based on their appearances and briefly express them using various onomatopoeias to express their impressions intuitively. It is said that Japanese onomatopoeias have sound-symbolism and their phoneme is strongly related to the impression of a motion. Thus, we considered that if a phonetic space based on sound-symbolism can be associated with the kinetic feature space of gaits, subtle difference of gaits could be expressed as difference in phoneme. This framework is expected to make human-computer interaction more intuitive.In this paper, we propose a method to convert the relative body-parts movements to onomatopoeias using a deep-learning based regression model. Through experiments, we confirmed the effectiveness of the proposed method, and discussed the potential of describing an arbitrary gait by not only existing onomatopoeias but also a novel one.\r"
  },
  "iccv2017_w23_smilenetregistration-freesmilingfacedetectioninthewild": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "SmileNet: Registration-Free Smiling Face Detection in the Wild",
    "authors": [
      "Youngkyoon Jang",
      "Hatice Gunes",
      "Ioannis Patras"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Jang_SmileNet_Registration-Free_Smiling_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Jang_SmileNet_Registration-Free_Smiling_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a novel smiling face detection framework called SmileNet for detecting faces and recognising smiles in the wild. SmileNet uses a Fully Convolutional Neural Network (FCNN) to detect multiple smiling faces in a given image of varying resolution. Our contributions are threefold: 1) SmileNet is the first smiling face detection network that does not require pre-processing such as face detection and registration in advance to generate a normalised (cropped and aligned) input image; 2) the proposed SmileNet is a simple and single FCNN architecture simultaneously performing face detection and smile recognition, which are conventionally treated as separate consecutive pipelines; and 3) SmileNet ensures real-time processing speed (21.15 FPS) even when detecting multiple smiling faces in a given image (300X300). Experimental results show that SmileNet can deliver state-of-the-art performance (95.76%), even under occlusions, and variances of pose, scale, and illumination.\r"
  },
  "iccv2017_w23_fromfacerecognitiontokinshipverificationanadaptationapproach": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "From Face Recognition to Kinship Verification: An Adaptation Approach",
    "authors": [
      "Qingyan Duan",
      "Lei Zhang",
      "Wangmeng Zuo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Duan_From_Face_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Duan_From_Face_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Kinship verification in the wild is a challenging yet interesting issue, which aims to determine whether two unconstrained facial images are from the same family or not. Most previous methods for kinship verification can be divided as low-level hand-crafted features based shallow methods and kin data only trained convolutional neural network (CNN) based deep methods. Worthy of affirmation, numerous work in vision get that convolutional features are discriminative, but bigger data dependent. A fact is that for a variety of data-limited vision problems, such as limited Kinship datasets, the ability of CNNs is seriously dropped because of overfitting. To this end, by inheriting the success of deep mining algorithms on face verification (e.g. LFW), in this paper, we propose a Coarse-to-Fine Transfer (CFT) based deep kinship verification framework. As the idea implied, this paper tries to answer \"is it possible to transfer a face recognition net to kinship verification?\". Therefore, a supervised coarse pre-training and domain-specific ad hoc fine re-training paradigm is exploited, with which the kin-relation specific features are effectively captured from faces. Extensive experiments on benchmark datasets demonstrate that our proposed CFT adaptation approach is comparable to the state-of-the art methods with a large margin.\r"
  },
  "iccv2017_w23_faceposenetmakingacaseforlandmark-freefacealignment": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "FacePoseNet: Making a Case for Landmark-Free Face Alignment",
    "authors": [
      "Feng-Ju Chang",
      "Anh Tuan Tran",
      "Tal Hassner",
      "Iacopo Masi",
      "Ram Nevatia",
      "Gerard Medioni"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Chang_FacePoseNet_Making_a_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Chang_FacePoseNet_Making_a_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We show how a simple convolutional neural network (CNN) can be trained to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose, directly from image intensities. We further explain how this FacePoseNet (FPN) can be used to align faces in 2D and 3D as an alternative to explicit facial landmark detection for these tasks. We claim that in many cases the standard means of measuring landmark detector accuracy can be misleading when comparing different face alignments. Instead, we compare our FPN with existing methods by evaluating how they affect face recognition accuracy on the IJB-A and IJB-B benchmarks: using the same recognition pipeline, but varying the face alignment method. Our results show that (a) better landmark detection accuracy measured on the 300W benchmark does not necessarily imply better face recognition accuracy. (b) Our FPN provides superior 2D and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces at a small fraction of the computational cost of comparably accurate landmark detectors. For many purposes, FPN is thus a far faster and far more accurate face alignment method than using facial landmark detectors.\r"
  },
  "iccv2017_w23_usingsyntheticdatatoimprovefacialexpressionanalysiswith3dconvolutionalnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Using Synthetic Data to Improve Facial Expression Analysis With 3D Convolutional Networks",
    "authors": [
      "Iman Abbasnejad",
      "Sridha Sridharan",
      "Dung Nguyen",
      "Simon Denman",
      "Clinton Fookes",
      "Simon Lucey"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Abbasnejad_Using_Synthetic_Data_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Abbasnejad_Using_Synthetic_Data_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Over the past few years, neural networks have made a huge improvement in object recognition and event analysis. However, due to a lack of available data, neural networks were not efficiently applied in expression analysis. In this paper, we tackle the problem of facial expression analysis using deep neural network by generating a realistic large scale synthetic labeled dataset. We train a deep 3-dimensional convolutional network on the generated dataset and empirically show how the presented method can efficiently classify facial expressions. Our method addresses four fundamental issues: (i) generating a large scale facial expression dataset that is realistic and accurate, (ii) a rich spatial representation of expressions, (iii) better spatiotemporal feature learning compared to recent techniques and (iv) with a simple linear classifier our learned features outperform state-of-the-art methods.\r"
  },
  "iccv2017_w23_densefacealignment": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Dense Face Alignment",
    "authors": [
      "Yaojie Liu",
      "Amin Jourabloo",
      "William Ren",
      "Xiaoming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Liu_Dense_Face_Alignment_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Liu_Dense_Face_Alignment_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Face alignment is a classic problem in the computer vision field. Previous research mostly focus on sparse alignment of the face image with a limited set offacial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim to provide more detailed dense 3D alignment for large-pose face images. To achieve this, we train a deep convolutional network to estimate the 3D shape model parameters, which not only aligns the limited number of facial landmarks, but also fits contours and SIFT feature points and utilizes them as a dense supervision. Moreover, we also address the bottleneck of training with multiple datasets, due to different landmark mark-ups such as 5, 34, 68 and even no labeling. Experimental results show that our method not only provides high-quality dense 3D face fitting, but also outperforms the state-of-the-art facial landmark detection methods on the challenging datasets, with one plain network and at real time.\r"
  },
  "iccv2017_w23_understandingandcomparingdeepneuralnetworksforageandgenderclassification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Understanding and Comparing Deep Neural Networks for Age and Gender Classification",
    "authors": [
      "Sebastian Lapuschkin",
      "Alexander Binder",
      "Klaus-Robert Muller",
      "Wojciech Samek"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Lapuschkin_Understanding_and_Comparing_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Lapuschkin_Understanding_and_Comparing_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for predictionand how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different effects.In detail, our work compares four popular neural network architectures, studies the effect of pretraining, evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model's prediction strategies in given preprocessing conditions using the recent Layer-wise Relevance Propagation (LRP) algorithm.Our evaluations on the challenging Adience benchmark show that suitable parameter initialization leads to a holistic perception of the input, compensating artefactual data representations. With a combination of simple preprocessing steps, we reach state of the art performance in gender recognition.\r"
  },
  "iccv2017_w23_earlyadaptationofdeeppriorsinagepredictionfromfaceimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Early Adaptation of Deep Priors in Age Prediction From Face Images",
    "authors": [
      "Mahdi Hajibabaei",
      "Anna Volokitin",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Hajibabaei_Early_Adaptation_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Hajibabaei_Early_Adaptation_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Age prediction from face images is a challenging task. Direct application of pre-trained models on new data leads to poor performance due to data and distribution mismatch and lack of newly annotated material.In this work, we analyze the transfer of knowledge from deep models pre-trained on massive datasets to new target datasets with (very) little information available. We investigate (i) pre-training on massive datasets with an imposed target age label distribution, (ii) pre-training on massive face datasets but without age annotations, and (iii) fine-tuning on the target train data.The experimental benchmark uses the massive IMDB-Wiki, VGG-Face and ImageNet datasets as sources and ChaLearn LAP and MORPH 2as target datasets. The deep architectures/priors are based on the VGG-16 and the recent state-of-the-art DEX and VGG-Face models. Our main findings are as follows. (i) Using deep priors (pre-trained models on similar data and/or task) boosts the performance on the target dataset. (ii) Imposing the target age label distribution on pre-trained models helps. (iii) The access to and the use of labeled target samples is critical - with as few as 12 samples used for fine-tuning a large performance gain is achieved, surpassing the impact of imposing target distribution for pre-training. Early adaptation of deep priors to new target datasets can yield sufficiently good performance at a reasonably low computational cost.\r"
  },
  "iccv2017_w23_disguisedfaceidentification(dfi)withfacialkeypointsusingspatialfusionconvolutionalnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Disguised Face Identification (DFI) With Facial KeyPoints Using Spatial Fusion Convolutional Network",
    "authors": [
      "Amarjot Singh",
      "Devendra Patil",
      "Meghana Reddy",
      "SN Omkar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Singh_Disguised_Face_Identification_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Singh_Disguised_Face_Identification_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Disguised face identification (DFI) is an extremely challenging problem due to the numerous variations that can be introduced using different disguises. This paper introduces a deep learning framework to first detect 14 facial key-points which are then utilized to perform disguised face identification. Since the training of deep learning architectures relies on large annotated datasets, two annotated facial key-points datasets are introduced. The effectiveness of the facial keypoint detection framework is presented for each keypoint. The superiority of the key-point detection framework is also demonstrated by a comparison with other deep networks. The effectiveness of classification performance is also demonstrated by comparison with the state-of-the-art face disguise classification methods. \r"
  },
  "iccv2017_w23_simpletripletlossbasedonintra/inter-classmetriclearningforfaceverification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Simple Triplet Loss Based on Intra/Inter-Class Metric Learning for Face Verification",
    "authors": [
      "Zuheng Ming",
      "Joseph Chazalon",
      "Muhammad Muzzamil Luqman",
      "Muriel Visani",
      "Jean-Christophe Burie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Ming_Simple_Triplet_Loss_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Ming_Simple_Triplet_Loss_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recently, benefiting from the advances of the deep convolution neural networks (CNNs), significant progress has been made in the field of the face verification and face recognition. Specially, the performance of the FaceNet has overpassed the human level performance in terms of the accuracy on the datasets \"Labeled Faces in the Wild (LFW)\"and \"Youtube Faces in the Wild (YTF)\". The triplet loss used in the FaceNet has proved its effectiveness for face verification. However, the number of the possible triplets is explosive when using a large scale dataset to train the model. In this paper, we propose a simple class-wise triplet loss based on the intra/inter-class distance metric learning which can largely reduce the number of the possible triplets to be learned. However the simplification of the classic triplet loss function has not degraded the performance of the proposed approach. The experimental evaluations on the most widely used benchmarks LFW and YTF show that the model with the proposed class-wise simple triplet loss can reach the state-of-the-art performance. And the visualization of the distribution of the learned features based on the MNIST dataset has also shown the effectiveness of the proposed method to better separate the classes and make the features more discriminative in comparison with the other state-of-the-art loss function.\r"
  },
  "iccv2017_w23_learningdeepconvolutionalembeddingsforfacerepresentationusingjointsample-andset-basedsupervision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Learning Deep Convolutional Embeddings for Face Representation Using Joint Sample- and Set-Based Supervision",
    "authors": [
      "Baris Gecer",
      "Vassileios Balntas",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Gecer_Learning_Deep_Convolutional_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Gecer_Learning_Deep_Convolutional_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this work, we investigate several methods and strategies to learn deep embeddings for face recognition, using joint sample- and set-based optimization. We explain our framework that expands traditional learning with set-based supervision together with the strategies used to maintain set characteristics. We, then, briefly review the related set-based loss functions, and subsequently propose a novel Max-Margin Loss which maximizes maximum possible inter-class margin with assistance of Support Vector Machines (SVMs). It implicitly pushes all the samples towards correct side of the margin with a vector perpendicular to the hyperplane and a strength inversely proportional to the distance to it. We show that the introduced loss outperform the previous sample-based and set-based ones in terms verification of faces on two commonly used benchmarks.\r"
  },
  "iccv2017_w23_detectingsmilesofyoungchildrenviadeeptransferlearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Detecting Smiles of Young Children via Deep Transfer Learning",
    "authors": [
      "Yu Xia",
      "Di Huang",
      "Yunhong Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Xia_Detecting_Smiles_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Xia_Detecting_Smiles_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Smile detection is an interesting topic in computer vision and has received increasing attention in recent years. However, the challenge caused by age variations has not been sufficiently focused on before. In this paper, we first highlight the impact of the discrepancy between infants and adults in a quantitative way on a newly collected database. We then formulate this issue as an unsupervised domain adaptation problem and present the solution of deep transfer learning, which applies the state of the art transfer learning methods, namely Deep Adaptation Networks (DAN) and Joint Adaptation Network (JAN), to two baseline deep models, i.e. AlexNet and ResNet. Thanks to DAN and JAN, the knowledge learned by deep models from adults can be transferred to infants, where very limited labeled data are available for training. Cross-dataset experiments are conducted and the results evidently demonstrate the effectiveness of the proposed approach to smile detection across such an age gap.\r"
  },
  "iccv2017_w23_deepvisagemakingfacerecognitionsimpleyetwithpowerfulgeneralizationskills": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W23",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "DeepVisage: Making Face Recognition Simple yet With Powerful Generalization Skills",
    "authors": [
      "Abul Hasnat",
      "Julien Bohne",
      "Jonathan Milgram",
      "Stephane Gentric",
      "Liming Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w23/html/Hasnat_DeepVisage_Making_Face_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w23/Hasnat_DeepVisage_Making_Face_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Face recognition (FR) methods report significant performance by adopting the convolutional neural network (CNN) based learning methods. Although CNNs are mostly trained by optimizing the softmax loss, the recent trend shows an improvement of accuracy with different strategies, such as task-specific CNN learning with different loss functions, fine-tuning on target dataset, metric learning and concatenating features from multiple CNNs. Incorporating these tasks obviously requires additional efforts. Moreover, it demotivates the discovery of efficient CNN models for FR which are trained only with identity labels. We focus on this fact and propose an easily trainable and single CNN based FR method. Our CNN model exploits the residual learning framework. Additionally, it uses normalized features to compute the loss. Our extensive experiments show excellent generalization on different datasets. We obtain very competitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and CACD datasets.\r"
  },
  "iccv2017_w24_2017iccvchallengedetectingsymmetryinthewild": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "2017 ICCV Challenge: Detecting Symmetry in the Wild",
    "authors": [
      "Christopher Funk",
      "Seungkyu Lee",
      "Martin R. Oswald",
      "Stavros Tsogkas",
      "Wei Shen",
      "Andrea Cohen",
      "Sven Dickinson",
      "Yanxi Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Funk_2017_ICCV_Challenge_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Funk_2017_ICCV_Challenge_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Motivated by various new applications of computational symmetry in computer vision and in an effort to advance machine perception of symmetry in the wild, we organize the third international symmetry detection challenge at ICCV 2017 after the CVPR 2011/2013 symmetry detection competitions. Our goal is to gauge the progress in computational symmetry with continuous benchmarking of both new algorithms and datasets, as well as more polished validation methodology. Different from previous years, this time we expand our training/testing data sets to include 3D data, and establish the most comprehensive and largest annotated datasets for symmetry detection to date; we also expand the types of symmetries to include densely-distributed and medial-axis-like symmetries; furthermore, we establish a challenge-and-paper dual track mechanism where both algorithms and articles on symmetry-related research are solicited. In this report, we provide a detailed summary of our evaluation methodology for each type of symmetry detection algorithm validated. We demonstrate and analyze quantified detection results in terms of precision-recall curves and F-measures for all algorithms evaluated. We also offer a short survey of the paper-track submissions accepted for our 2017 symmetry challenge. \r"
  },
  "iccv2017_w24_hierarchicalgroupingusinggestaltassessments": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Hierarchical Grouping Using Gestalt Assessments",
    "authors": [
      "Eckart Michaelsen",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Michaelsen_Hierarchical_Grouping_Using_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Michaelsen_Hierarchical_Grouping_Using_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Real images contain symmetric Gestalten with high probability. I.e. certain parts can be mapped on other certain parts by the usual Gestalt laws and are repeated there with high similarity. Moreover, such mapping comes in nested hierarchies - e.g. a reflection Gestalt that is made of repetition friezes, whose parts are again reflection symmetric compositions. This can be explicitly modelled by continuous assessment functions. Hard decisions on whether or not a law is fulfilled are avoided. Starting from primitive objects extracted from the input image successively aggregates are constructed: reflection pairs, rows, etc., forming a part-of-hierarchy and rising in scale. The work in this paper starts from super-pixel primitives, and the grouping ends when the Gestalten almost fill the whole image. Occasionally the results may not be in accordance with human perception. The parameters have not been adjusted specifically for the data at hand. Previous work only used the compulsory attributes location, scale, orientation and assessment for each object. A way to improve the recognition performance is utilizing additional features such as colors or eccentricity. Thus the recognition rates are a little better.\r"
  },
  "iccv2017_w24_hierarchicalgrouping-thegestaltassessmentsmethod": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Hierarchical Grouping - The Gestalt Assessments Method",
    "authors": [
      "Eckart Michaelsen",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Michaelsen_Hierarchical_Grouping_-_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Michaelsen_Hierarchical_Grouping_-_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Real images contain reflection symmetry and repetition in rows with high probability. I.e. certain parts can be mapped on other certain parts by the usual Gestalt laws and are repeated there with high similarity. Moreover, such mapping comes in nested hierarchies - e.g. a reflection Gestalt that is made of repetition friezes, whose parts are again reflection symmetric compositions. It is our intention to develop and test methods that may automatically find, parametrize, and assess such nested hierarchies. This can be explicitly modelled by continuous assessment functions. The recognition performance is raised utilizing additional features such as colors. This paper reports examples from the 2017 data set.\r"
  },
  "iccv2017_w24_symmmapestimationofthe2-dreflectionsymmetrymapanditsapplications": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "SymmMap: Estimation of the 2-D Reflection Symmetry Map and Its Applications",
    "authors": [
      "Rajendra Nagar",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Nagar_SymmMap_Estimation_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Nagar_SymmMap_Estimation_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Detecting the reflection symmetry axis present in an object has been an active research problem in computer vision and computer graphics due to its various applications such as object recognition, object detection, modelling, and symmetrization of 3D objects. However, the problem of computing the reflection symmetry map for a given image containing objects exhibiting reflection symmetry has received a very little attention. The symmetry map enables us to represent the pixels in the image using a score depending on the probability of each of them having a symmetric counterpart. In this work, we attempt to compute the 2-D reflection symmetry map. We pose the problem of generating the symmetry map as an intra-image dense symmetric pixels correspondence problem, which we solve efficiently using a randomized algorithm by observing the reflection symmetry coherency present in the image. We introduce an application of symmetry map called symmetry preserving image stylization.\r"
  },
  "iccv2017_w24_wavelet-basedreflectionsymmetrydetectionviatexturalandcolorhistograms": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Wavelet-Based Reflection Symmetry Detection via Textural and Color Histograms",
    "authors": [
      "Mohamed Elawady",
      "Christophe Ducottet",
      "Olivier Alata",
      "Cecile Barat",
      "Philippe Colantoni"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Elawady_Wavelet-Based_Reflection_Symmetry_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Elawady_Wavelet-Based_Reflection_Symmetry_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Symmetry is one of the significant visual properties inside an image plane, to identify the geometrically balanced structures through real-world objects. Existing symmetry detection methods rely on descriptors of the local image features and their neighborhood behavior, resulting incomplete symmetrical axis candidates to discover the mirror similarities on a global scale. In this paper, we propose a new reflection symmetry detection scheme, based on a reliable edge-based feature extraction using Log-Gabor filters, plus an efficient voting scheme parameterized by their corresponding textural and color neighborhood information. Experimental evaluation on four single-case and three multiple-case symmetry detection datasets validates the superior achievement of the proposed work to find global symmetries inside an image.\r"
  },
  "iccv2017_w24_wavelet-basedreflectionsymmetrydetectionviatexturalandcolorhistogramsalgorithmandresults": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Wavelet-Based Reflection Symmetry Detection via Textural and Color Histograms: Algorithm and Results",
    "authors": [
      "Mohamed Elawady",
      "Christophe Ducottet",
      "Olivier Alata",
      "Cecile Barat",
      "Philippe Colantoni"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Elawady_Wavelet-Based_Reflection_Symmetry_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Elawady_Wavelet-Based_Reflection_Symmetry_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Symmetry is one of the significant visual properties inside an image plane, to identify the geometrically balanced structures through real-world objects. Existing symmetry detection methods rely on descriptors of the local image features and their neighborhood behavior, resulting incomplete symmetrical axis candidates to discover the mirror similarities on a global scale. In this paper, we propose a new reflection symmetry detection scheme, based on a reliable edge-based feature extraction using Log-Gabor filters, plus an efficient voting scheme parameterized by their corresponding textural and color neighborhood information. Experimental evaluation on four single-case and three multiple-case symmetry detection datasets validates the superior achievement of the proposed work to find global symmetries inside an image.\r"
  },
  "iccv2017_w24_rsrnrichside-outputresidualnetworkformedialaxisdetection": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "RSRN: Rich Side-Output Residual Network for Medial Axis Detection",
    "authors": [
      "Chang Liu",
      "Wei Ke",
      "Jianbin Jiao",
      "Qixiang Ye"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Liu_RSRN_Rich_Side-Output_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Liu_RSRN_Rich_Side-Output_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a Rich Side-output Residual Network (RSRN) for medial axis detection for the ICCV 2017 workshop challenge on detecting symmetry in the wild. RSRN uses the rich features of fully convolutional network by hierarchically fusing side-outputs in a deep-to-shallow manner to decrease the residual between the detection result and the ground-truth, which refines the detection result hierarchically. Experimental results show that the proposed RSRN improve the performance compared with baseline on both SKLARGE and BMAX500 datasets.\r"
  },
  "iccv2017_w24_fusingimageandsegmentationcuesforskeletonextractioninthewild": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Fusing Image and Segmentation Cues for Skeleton Extraction in the Wild",
    "authors": [
      "Xiaolong Liu",
      "Pengyuan Lyu",
      "Xiang Bai",
      "Ming-Ming Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Liu_Fusing_Image_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Liu_Fusing_Image_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Extracting skeletons from natural images is achallenging problem, due to complex backgrounds in the scene and various scales of objects. To address this problem, we propose a two-stream fully convolutional neural network which uses the original image and its corresponding semantic segmentation probability map as inputs and predicts the skeleton map using merged multi-scale features. We find that the semantic segmentation probability map is complementary to the corresponding color image and can boost the performance of our baseline model which trained only on color images. We conduct experiments onSK-LARGE dataset and the F-measure of our method on validation set is 0.738 which outperforms current state-of-the-art significantly and demonstrates the effectiveness of our proposed approach.\r"
  },
  "iccv2017_w24_findingmirrorsymmetryviaregistrationandoptimalsymmetricpairwiseassignmentofcurves": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Finding Mirror Symmetry via Registration and Optimal Symmetric Pairwise Assignment of Curves",
    "authors": [
      "Marcelo Cicconet",
      "David G. C. Hildebrand",
      "Hunter Elliott"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Cicconet_Finding_Mirror_Symmetry_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Cicconet_Finding_Mirror_Symmetry_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We demonstrate that the problem of fitting a plane of mirror symmetry to data in any Euclidian space can be reduced to the problem of registering two datasets, and that the exactness of the solution depends entirely on the registration accuracy. This new Mirror Symmetry via Registration (MSR) framework involves (1) data reflection with respect to an arbitrary plane, (2) registration of original and reflected datasets, and (3) calculation of the eigenvector of eigenvalue -1 for the transformation matrix representing the reflection and registration mappings. To support MSR, we also introduce a novel 2D registration method based on random sample consensus of an ensemble of normalized cross-correlation matches. We further demonstrate the generality of MSR by testing it on a database of 3D shapes with an iterative closest point registration back-end.\r"
  },
  "iccv2017_w24_findingmirrorsymmetryviaregistrationandoptimalsymmetricpairwiseassignmentofcurvesalgorithmandresults": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Finding Mirror Symmetry via Registration and Optimal Symmetric Pairwise Assignment of Curves: Algorithm and Results",
    "authors": [
      "Marcelo Cicconet",
      "David G. C. Hildebrand",
      "Hunter Elliott"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Cicconet_Finding_Mirror_Symmetry_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Cicconet_Finding_Mirror_Symmetry_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We demonstrate that the problem of fitting a plane of mirror symmetry to data in any Euclidian space can be reduced to the problem of registering two datasets, and that the exactness of the solution depends entirely on the registration accuracy. This new Mirror Symmetry via Registration (MSR) framework involves (1) data reflection with respect to an arbitrary plane, (2) registration of original and reflected datasets, and (3) calculation of the eigenvector of eigenvalue -1 for the transformation matrix representing the reflection and registration mappings. To support MSR, we also introduce a novel 2D registration method based on random sample consensus of an ensemble of normalized cross-correlation matches. We further demonstrate the generality of MSR by testing it on a database of 3D shapes with an iterative closest point registration back-end.\r"
  },
  "iccv2017_w24_symmslicsymmetryawaresuperpixelsegmentation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "SymmSLIC: Symmetry Aware Superpixel Segmentation",
    "authors": [
      "Rajendra Nagar",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Nagar_SymmSLIC_Symmetry_Aware_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Nagar_SymmSLIC_Symmetry_Aware_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Over-segmentation of an image into superpixels has become an useful tool for solving various problems in computer vision. Reflection symmetry is quite prevalent in both natural and man-made objects. Existing algorithms for estimating superpixels do not preserve the reflection symmetry of an object which leads to different sizes and shapes of superpixels across the symmetry axis. In this work, we propose an algorithm to over-segment an image through the propagation of reflection symmetry evident at the pixel level to superpixel boundaries. In order to achieve this goal, we exploit the detection of a set of pairs of pixels which are mirror reflections of each other. We partition the image into superpixels while preserving this reflection symmetry information through an iterative algorithm. We compare the proposed method with state-of-the-art superpixel generation methods and show the effectiveness of the method in preserving the size and shape of superpixel boundaries across the reflection symmetry axes. We also present an application called unsupervised symmetric object segmentation to illustrate the effectiveness of the proposed approach.\r"
  },
  "iccv2017_w24_innerspectechnicalreport": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "InnerSpec: Technical Report",
    "authors": [
      "Fabrizio Guerrini",
      "Alessandro Gnutti",
      "Riccardo Leonardi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Guerrini_InnerSpec_Technical_Report_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Guerrini_InnerSpec_Technical_Report_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this report we describe \"InnerSpec\", an approach for symmetric object detection that is based both on the computation of a symmetry measure for each pixel and on gradient information analysis. The symmetry value is obtained as the energy balance of the even-odd decomposition of an oriented square patch with respect to its central axis. Such an operation is akin to the computation of a row-wise convolution in the midpoint. The candidate symmetry axes are then identified through the localization of peaks along the direction perpendicular to each considered angle. These axes are finally evaluated by computing the image gradient in their neighborhood, in particular checking whether the gradient information displays specular characteristics.\r"
  },
  "iccv2017_w24_detectingreflectionalsymmetriesin3ddatathroughsymmetricalfitting": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W24",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Detecting Symmetry in the Wild",
    "title": "Detecting Reflectional Symmetries in 3D Data Through Symmetrical Fitting",
    "authors": [
      "Aleksandrs Ecins",
      "Cornelia Fermuller",
      "Yiannis Aloimonos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w24/html/Ecins_Detecting_Reflectional_Symmetries_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w24/Ecins_Detecting_Reflectional_Symmetries_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Symmetry is ubiquitous in both natural and man-made environments. It reveals redundancies in the structure of the world around us and thus can be used in a variety of visual processing tasks. This paper presents a simple and robust approach to detecting symmetric objects and extract- ing their symmetries from three-dimensional data. Given a 3D mesh of an object, a set of candidate symmetries are proposed first and are then refined, so that they reflect the complete mesh onto itself. We show how our method can be used to detect symmetric objects in scenes consisting of syn- thetic 3D models, as well as 3D scans of real environments.\r"
  },
  "iccv2017_w25_learningrobustrepresentationsforcomputervision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Learning Robust Representations for Computer Vision",
    "authors": [
      "Peng Zheng",
      "Aleksandr Y. Aravkin",
      "Karthikeyan Natesan Ramamurthy",
      "Jayaraman Jayaraman Thiagarajan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Zheng_Learning_Robust_Representations_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Zheng_Learning_Robust_Representations_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Unsupervised learning techniques in computer vision often require learning latent representations, such as low-dimensional subspaces and distance metrics. Noise and outliers in the data can frustrate these approaches by obscuring the latent spaces. Our main goal is deeper understanding and new development of robust approaches for representation learning. We provide a new interpretation for existing robust approaches and present two specific contributions: a new robust PCA approach,which can separate foreground features from dynamic background, and a novel robust spectral clustering method, that can cluster facial images with high accuracy. Both contributions show superior performance to standard methods on real-world test sets.\r"
  },
  "iccv2017_w25_variationalrobustsubspaceclusteringwithmeanupdatealgorithm": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Variational Robust Subspace Clustering With Mean Update Algorithm",
    "authors": [
      "Sergej Dogadov",
      "Andres Masegosa",
      "Shinichi Nakajima"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Dogadov_Variational_Robust_Subspace_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Dogadov_Variational_Robust_Subspace_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose an efficient variational Bayesian (VB) solver for a robust variant oflow-rank subspace clustering (LRSC). VB learning offers automatic model selection without parameter tuning. However, it is typically performed by local search with update rules derived from conditional conjugacy, and therefore prone to local minima problem. Instead, we use an approximate global solver for LRSC with an element-wise sparse term to make it robust against spiky noise. In experiment, our method (mean update solver for robust LRSC),outperforms the original LRSC, as well as the robust LRSC with the standard VB solver.\r"
  },
  "iccv2017_w25_manifoldconstrainedlow-rankdecomposition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Manifold Constrained Low-Rank Decomposition",
    "authors": [
      "Chen Chen",
      "Baochang Zhang",
      "Alessio Del Bue",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Chen_Manifold_Constrained_Low-Rank_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Chen_Manifold_Constrained_Low-Rank_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Low rank decomposition (LRD) is a state-of-the-artmethod for visual data reconstruction and modelling. However, it is a very challenging problemwhen the data contains significant occlusion, noise, illumination variation, and misalignment from rotation and/or viewpoint changing. In this paper, we propose a new framework thatembeds manifold priors into LRD. To implementthe framework, we design a multipliers alternatingdirection method which efficiently integrates themanifold constraints during the optimization process.This is due to the assumption that we canrecast the problem as the projection over the manifoldvia an embedding method. The proposed approachis successfully used to calculate low ranksfrom faces, digits and window images, showing aconsistent increase of performance when comparedto the state of the art over a wide range of realisticmisalignments and corruptions. \r"
  },
  "iccv2017_w25_anon-convexrelaxationforfixed-rankapproximation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "A Non-Convex Relaxation for Fixed-Rank Approximation",
    "authors": [
      "Carl Olsson",
      "Marcus Carlsson",
      "Erik Bylow"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Olsson_A_Non-Convex_Relaxation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Olsson_A_Non-Convex_Relaxation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper considers the problem of finding a low rank matrix from observations of linear combinations of its elements. It is well known that if the problem fulfills a restricted isometry property (RIP), convex relaxations using the nuclear norm typically work well and come with theoretical performance guarantees. On the other hand these formulations suffer from a shrinking bias that can severely degrade the solution in the presence of noise.In this theoretical paper we study an alternative non-convex relaxation that in contrast to the nuclear norm does not penalize the leading singular values and thereby avoids this bias. We show that despite its non-convexity the proposed formulation will in many cases have a single stationary point if a RIP holds. Our numerical tests show that our approach typically converges to a better solution than nuclear norm based alternativeseven in cases when the RIP does not hold. \r"
  },
  "iccv2017_w25_robustandscalablecolumn/rowsamplingfromcorruptedbigdata": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Robust and Scalable Column/Row Sampling From Corrupted Big Data",
    "authors": [
      "Mostafa Rahmani",
      "George Atia"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Rahmani_Robust_and_Scalable_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Rahmani_Robust_and_Scalable_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Conventional sampling techniques fall short of drawing descriptive sketches of the data when the data is grossly corrupted as such corruptions break the low rank structure required for them to perform satisfactorily. In this paper, we present new sampling algorithms which can locate the informative columns in presence of severe data corruptions. In addition, we develop new scalable randomized designs of the proposed algorithms. The proposed approach is simultaneously robust to sparse corruption and outliers and substantially outperforms the state-of-the-art robust sampling algorithms as demonstrated by experiments conducted using both real and synthetic data.\r"
  },
  "iccv2017_w25_fastapproximatekarhunen-loevetransformforthree-wayarraydata": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Fast Approximate Karhunen-Loeve Transform for Three-Way Array Data",
    "authors": [
      "Hayato Itoh",
      "Atsushi Imiya",
      "Tomoya Sakai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Itoh_Fast_Approximate_Karhunen-Loeve_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Itoh_Fast_Approximate_Karhunen-Loeve_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Organs, cells and microstructures in cellsdealt with in biomedical image analysis are volumetric data.We are required to process and analyse these data as volumetric datawithout embedding into higher-dimensional vector spacefrom the viewpoints of object oriented data analysis.Sampled values of volumetric data are expressed asthree-way array data.Therefore, principal component analysis of multi-way datais an essential technique forsubspace-based pattern recognition,data retrievals and data compression of volumetric data. For one-way array (the vector form) problemthe discrete cosine transform matrixis agood relaxed solutionof the eigenmatrix for principal component analysis.This algebraic property of principal component analysis,derivesan approximate fast algorithm forPCA of three-way data arrays.\r"
  },
  "iccv2017_w25_abatch-incrementalvideobackgroundestimationmodelusingweightedlow-rankapproximationofmatrices": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "A Batch-Incremental Video Background Estimation Model Using Weighted Low-Rank Approximation of Matrices",
    "authors": [
      "Aritra Dutta",
      "Xin Li",
      "Peter Richtarik"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Dutta_A_Batch-Incremental_Video_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Dutta_A_Batch-Incremental_Video_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Principal component pursuit (PCP) is a state-of-the- art approach to background estimation problems. Due to their higher computational cost, PCP algorithms, such as robust principal component analysis (RPCA) and its variants, are not feasible in processing high definition videos. To avoid the curse of dimensionality in those algorithms, several methods have been proposed to solve the background estimation problem incrementally. We build a batch-incremental background estimation model by using a special weighted low-rank approximation of matrices. Through experiments with real and synthetic video sequences, we demonstrate that our model is superior to the existing state-of-the-art background estimation algorithms such as GRASTA, ReProCS, incPCP, and GFL.\r"
  },
  "iccv2017_w25_panningandjitterinvariantincrementalprincipalcomponentpursuitforvideobackgroundmodeling": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Panning and Jitter Invariant Incremental Principal Component Pursuit for Video Background Modeling",
    "authors": [
      "Gustavo Chau",
      "Paul Rodriguez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Chau_Panning_and_Jitter_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Chau_Panning_and_Jitter_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Video background modeling is an important preprocessing stage for various applications and principal component pursuit (PCP) is among the state-of-the-art algorithms for this task. One of the main drawbacks of PCP is its sensitivity to jitter and camera movement. This problem has only been partially solved by a few methods devised for jitter or small transformations. However, such methods cannot handle the case of moving or panning cameras. We present a novel, fully incremental PCP algorithm, named incPCP-PTI, that is able to cope with panning scenarios and jitter by continuously aligning the low-rank component to the current reference frame of the camera. To the best of our knowledge, incPCP-PTI is the first low rank plus additive incremental matrix method capable of handling these scenarios. Results on synthetic videos and CDNET2014 videos show that incPCP-PTI is able to maintain a good performance in the detection of moving objects even when panning and jitter are present in a video\r"
  },
  "iccv2017_w25_weightedlowrankapproximationforbackgroundestimationproblems": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Weighted Low Rank Approximation for Background Estimation Problems",
    "authors": [
      "Aritra Dutta",
      "Xin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Dutta_Weighted_Low_Rank_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Dutta_Weighted_Low_Rank_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Classical principal component analysis (PCA) is not robust when the data contain sparse outliers. The use of the l_1 norm in the Robust PCA (RPCA) method successfully eliminates this weakness of PCA in separating the sparse outliers. Here we propose a weighted low rank (WLR) method, where a simple weight is inserted inside the Frobenius norm. We demonstrate how this method tackles often computationally expensive algorithms that rely on the l_1 norm. As a proof of concept, we present a background estimation model based on WLR, and we compare the model with RPCA method and with other state-of-the-art algorithms used for background estimation. Our empirical validation shows that the weighted low-rank approximation we propose here can perform as well as or better than that of RPCA and other state-of-the-art algorithms.\r"
  },
  "iccv2017_w25_dynamicmodedecompositionforbackgroundmodeling": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Dynamic Mode Decomposition for Background Modeling",
    "authors": [
      "J. Nathan Kutz",
      "N. Benjamin Erichson",
      "Travis Askham",
      "Seth Pendergrass",
      "Steven L. Brunton"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Kutz_Dynamic_Mode_Decomposition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Kutz_Dynamic_Mode_Decomposition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The Dynamic Mode Decomposition (DMD) is a spatio-temporal matrix decomposition method capable of background modeling in video streams. DMD is a regression technique that integrates Fourier transforms and singular value decomposition. Innovations in compressed sensing allow for a scalable and rapid decomposition of video streams that scales with the intrinsic rank of the matrix, rather than the size of the actual video. Our results show that the quality of the resulting background model is competitive, quantified by the F-measure, recall and precision.A GPU (graphics processing unit) accelerated implementation is also possible allowing the algorithm to operate efficiently on streaming data.In addition, it is possible to leverage the native compressed format of many data streams, such as HD video and computational physics codes that are represented sparsely in the Fourier domain, to massively reduce data transfer from CPU to GPU and to enable sparse matrix multiplications.\r"
  },
  "iccv2017_w25_backgroundsubtractionviafastrobustmatrixcompletion": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Background Subtraction via Fast Robust Matrix Completion",
    "authors": [
      "Behnaz Rezaei",
      "Sarah Ostadabbas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Rezaei_Background_Subtraction_via_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Rezaei_Background_Subtraction_via_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Background subtraction is the primary task of the majority of video inspection systems. The most important part of the background subtraction which is common among different algorithms is background modeling. In this regard, our paper addresses the problem of background modeling in a computationally efficient way, which is important for current eruption of \"big data\" processing coming from high resolution multi-channel videos. Our model is based on the assumption that background in natural images lies on a low-dimensional subspace. We formulated and solved this problem in a low-rank matrix completion framework. In modeling the background, we benefited from the in-face extended Frank-Wolfe algorithm for solving a defined convex optimization problem. We evaluated our fast robust matrix completion (fRMC) method on both background models challenge (BMC) and Stuttgart artificial background subtraction (SABS) datasets. \r"
  },
  "iccv2017_w25_compressedsingularvaluedecompositionforimageandvideoprocessing": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Compressed Singular Value Decomposition for Image and Video Processing",
    "authors": [
      "N. Benjamin Erichson",
      "Steven L. Brunton",
      "J. Nathan Kutz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Erichson_Compressed_Singular_Value_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Erichson_Compressed_Singular_Value_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We demonstrate a heuristic algorithm to compute the approximate low-rank singular value decomposition. The algorithm is inspired by ideas from compressed sensing and, in particular, is suitable for image and video processing applications. Specifically, our compressed singular value decomposition (cSVD) algorithm employs aggressive random test matrices to efficiently sketch the row space of the input matrix. The resulting compressed representation of the data enables the computation of an accurate approximation of the dominant high-dimensional left and right singular vectors. We benchmark cSVD against the current state-of-the-art randomized SVD and show a performance boost while attaining near similar relative errors. The cSVD is simple to implement as well as embarrassingly parallel, i.e, ideally suited for GPU computations and mobile platforms.\r"
  },
  "iccv2017_w25_uhdvideosuper-resolutionusinglow-rankandsparsedecomposition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W25",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "UHD Video Super-Resolution Using Low-Rank and Sparse Decomposition",
    "authors": [
      "Salehe Erfanian Ebadi",
      "Valia Guerra Ones",
      "Ebroul Izquierdo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w25/html/Ebadi_UHD_Video_Super-Resolution_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w25/Ebadi_UHD_Video_Super-Resolution_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Sparse coding-based algorithms have been successfully applied to the single-image super resolution problem. Conventional multi-image SR algorithms incorporate auxiliary frames into the model by a registration process using subpixel block matching algorithms that are computationally expensive. This becomes increasingly important as super-resolving UHD video content with existing sparse-based SR approaches become less efficient. In order to fully utilize the spatio-temporal information, we propose a novel multi-frame video SR approach that is aided by a low-rank plus sparse decomposition of the video sequence. We introduce a group of pictures structure where we seek a rank-1 low-rank part that recovers the shared spatio-temporal information among the frames in the GOP. Then we super-resolve the low-rank frame and sparse frames separately. This assumption results in significant time reductions, as well as surpassing state-of-the-art performance both qualitatively and quantitatively.\r"
  },
  "iccv2017_w27_highperformancelargescalefacerecognitionwithmulti-cognitionsoftmaxandfeatureretrieval": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recognizing One Million Celebrities in the Real World",
    "title": "High Performance Large Scale Face Recognition With Multi-Cognition Softmax and Feature Retrieval",
    "authors": [
      "Yan Xu",
      "Yu Cheng",
      "Jian Zhao",
      "Zhecan Wang",
      "Lin Xiong",
      "Karlekar Jayashree",
      "Hajime Tamura",
      "Tomoyuki Kagaya",
      "Shengmei Shen",
      "Sugiri Pranata",
      "Jiashi Feng",
      "Junliang Xing"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w27/html/Xu_High_Performance_Large_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w27/Xu_High_Performance_Large_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we introduce our solution to the Challenge-1 of the MS-Celeb-1M challenges which aims to recognize one million celebrities. To solve this large scale face recognition problem, a Multi-Cognition Softmax Model is proposed to distribute training data to several cognition units by a data shuffling strategy. Here we introduce one cognition unit as a group of independent softmax models, which is designed to increase the diversity of the one softmax model to boost the performance for models ensemble. Meanwhile, a template-based Feature Retrieval module is adopted to improve the performance of MCSM by a specific voting scheme. Moreover, a one-shot learning method is applied on collected extra 600K identities due to each identity has one image only. Finally, testing images with lower score from MCSM and FR are assigned new labels with higher score by merging one-shot learning results. Our solution ranks the first place in both two settings of the final evaluation.\r"
  },
  "iccv2017_w27_howtotraintripletnetworkswith100kidentities?": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recognizing One Million Celebrities in the Real World",
    "title": "How to Train Triplet Networks With 100K Identities?",
    "authors": [
      "Chong Wang",
      "Xue Zhang",
      "Xipeng Lan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w27/html/Wang_How_to_Train_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w27/Wang_How_to_Train_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Training triplet networks with large-scale data is challenging in face recognition. Due to the number of possible triplets explodes with the number of samples, previous studies adopt the online hard negative mining(OHNM) to handle it. However, as the number of identities becomes extremely large, the training will suffer from bad local minima because effective hard triplets are difficult to be found. To solve the problem, in this paper, we propose training triplet networks with subspace learning, which splits the space of all identities into subspaces consisting of only similar identities. Combined with the batch OHNM, hard triplets can be found much easier. In addition, to deal with heavy noise and large-scale retrieval, we also make some efforts on robust noise removing and efficient image retrieval, which are used jointly with the subspace learning to obtain the state-of-the-art performance on the MS-Celeb-1M competition (without external data in Challenge1).\r"
  },
  "iccv2017_w27_doppelgangerminingforfacerepresentationlearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recognizing One Million Celebrities in the Real World",
    "title": "Doppelganger Mining for Face Representation Learning",
    "authors": [
      "Evgeny Smirnov",
      "Aleksandr Melnikov",
      "Sergey Novoselov",
      "Eugene Luckyanets",
      "Galina Lavrentyeva"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w27/html/Smirnov_Doppelganger_Mining_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w27/Smirnov_Doppelganger_Mining_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we present Doppelganger mining - a method to learn better face representations. The main idea of this method is to maintain a list with the most similar identities for each identity in the training set. This list is used to generate better mini-batches by sampling pairs of similar-looking identities (\"doppelgangers\") together. It is especially useful for methods, based on exemplar-based supervision. Usually hard example mining comes with a price of necessity to use large mini-batches or substantial extra computation and memory cost, particularly for datasets with large numbers of identities. Our method needs only a negligible extra computation and memory. In our experiments on a benchmark dataset with 21,000 persons we show that Doppelganger mining, being inserted in the face representation learning process with joint prototype-based and exemplar-based supervision, significantly improves the discriminative power of learned face representations.\r"
  },
  "iccv2017_w27_knowyouatoneglanceacompactvectorrepresentationforlow-shotlearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recognizing One Million Celebrities in the Real World",
    "title": "Know You at One Glance: A Compact Vector Representation for Low-Shot Learning",
    "authors": [
      "Yu Cheng",
      "Jian Zhao",
      "Zhecan Wang",
      "Yan Xu",
      "Karlekar Jayashree",
      "Shengmei Shen",
      "Jiashi Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w27/html/Cheng_Know_You_at_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w27/Cheng_Know_You_at_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose an enforced Softmax optimization approach which is able to improve the model's representational capacity by producing a \"compact vector representation\" for effectively solving the challenging low-shot learning face recognition problem. Compact vector representations are significantly helpful to overcome the underlying multi-modality variations and remain the primary key features as close to the mean face of the identity as possible in the high-dimensional feature space. Therefore, the gallery facial representations become more robust under various situations, leading to the overall performance improvement for low-shot learning. Comprehensive evaluations on the MNIST, LFW, and the challenging MS-Celeb-1M Low-Shot Learning Face Recognition benchmark datasets clearly demonstrate the superiority of our proposed method over state-of-the-arts.\r"
  },
  "iccv2017_w27_low-shotfacerecognitionwithhybridclassifiers": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recognizing One Million Celebrities in the Real World",
    "title": "Low-Shot Face Recognition With Hybrid Classifiers",
    "authors": [
      "Yue Wu",
      "Hongfu Liu",
      "Yun Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w27/html/Wu_Low-Shot_Face_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w27/Wu_Low-Shot_Face_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we present our solution to the MS-Celeb-1M Low-shot Face Recognition Challenge. This challenge aims to recognize 21,000 celebrities, in which 20,000 celebrities (Base Set) come with 50-100 images per person. But only one training image is provided for each person in the rest 1,000 celebrities (Novel Set). Given the dispersion in the number of training samples between Base Set and Novel Set, it is hard to build a single classifier that works well for both sets. To solve this problem, a framework with hybrid classifiers is proposed to ensemble different inferences from multiple classifiers. This decomposes a single classifier for all data into multiple classifiers that each works well for a part of data.Extensive experiments on MS-Celeb-1M Low-shot dataset demonstrate the superiority of the proposed method. Our solutionwins the challenge in the track of without external data.\r"
  },
  "iccv2017_w27_facegenerationforlow-shotlearningusinggenerativeadversarialnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recognizing One Million Celebrities in the Real World",
    "title": "Face Generation for Low-Shot Learning Using Generative Adversarial Networks",
    "authors": [
      "Junsuk Choe",
      "Song Park",
      "Kyungmin Kim",
      "Joo Hyun Park",
      "Dongseob Kim",
      "Hyunjung Shim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w27/html/Choe_Face_Generation_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w27/Choe_Face_Generation_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recently, low-shot learning has been proposed for handling the lack of training data in machine learning. Despite of the importance of this issue, relatively less efforts have been made to study this problem. In this paper, we aim to increase the size of training dataset in various ways to improve the accuracy and robustness of face recognition. In detail, we adapt a generator from the Generative Adversarial Network (GAN) to increase the size of training dataset, which includes a base set, a widely available dataset, and a novel set, a given limited dataset, while adopting transfer learning as a backend. Based on extensive experimental study, we conduct the analysis on various data augmentation methods, observing how each affects the identification accuracy. Finally, we conclude that the proposed algorithm for generating faces is effective in improving the identification accuracy and coverage at the precision of 99% using both the base and novel set.\r"
  },
  "iccv2017_w28_thevisualobjecttrackingvot2017challengeresults": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Object Tracking Challenge",
    "title": "The Visual Object Tracking VOT2017 Challenge Results",
    "authors": [
      "Matej Kristan",
      "Ales Leonardis",
      "Jiri Matas",
      "Michael Felsberg",
      "Roman Pflugfelder",
      "Luka Cehovin Zajc",
      "Tomas Vojir",
      "Gustav Hager",
      "Alan Lukezic",
      "Abdelrahman Eldesokey",
      "Gustavo Fernandez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w28/html/Kristan_The_Visual_Object_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w28/Kristan_The_Visual_Object_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The Visual Object Tracking challenge VOT2017 is the fifth annual tracker benchmarking activity organized by the VOT initiative. Results of 51 trackers are presented; many are state-of-the-art published at major computer vision conferences or journals inrecent years. The evaluation included the standard VOT and other popular methodologies and a new \"real-time\" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The VOT2017 goes beyond its predecessors by (i) improving the VOT public dataset and introducing a separate VOT2017 sequestered dataset, (ii) introducing a real-time tracking experiment and (iii) releasing a redesigned toolkit that supports complex experiments. The dataset, the evaluation kit and the results are publicly available at the challenge w ....\r"
  },
  "iccv2017_w28_uctlearningunifiedconvolutionalnetworksforreal-timevisualtracking": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Object Tracking Challenge",
    "title": "UCT: Learning Unified Convolutional Networks for Real-Time Visual Tracking",
    "authors": [
      "Zheng Zhu",
      "Guan Huang",
      "Wei Zou",
      "Dalong Du",
      "Chang Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w28/html/Zhu_UCT_Learning_Unified_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w28/Zhu_UCT_Learning_Unified_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose an end-to-end framework to learn the convolutional features and perform the tracking process simultaneously, namely, a unified convolutional tracker (UCT). Specifically, The UCT treats feature extractor and tracking process (ridge regression) both as convolution operation and trains them jointly, enabling learned CNN features are tightly coupled to tracking process. In online tracking, an efficient updating method is proposed by introducing peak-versus-noise ratio (PNR) criterion, and scale changes are handled efficiently by incorporating a scale branch into network. The proposed approach results in superior tracking performance, while maintaining real-time speed. Experiments are performed on four challenging benchmark tracking datasets: OTB2013, OTB2015, VOT2014 and VOT2015, and our method achieves state-of-the-art results on these benchmarks compared with other real-time trackers.\r"
  },
  "iccv2017_w28_thebenefitsofevaluatingtrackerperformanceusingpixel-wisesegmentations": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Object Tracking Challenge",
    "title": "The Benefits of Evaluating Tracker Performance Using Pixel-Wise Segmentations",
    "authors": [
      "Tobias Bottger",
      "Patrick Follmann"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w28/html/Bottger_The_Benefits_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w28/Bottger_The_Benefits_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " For years, the ground truth data for evaluating object trackers consists of axis-aligned or oriented boxes. This greatly reduces the workload of labeling the datasets in the common benchmarks. Nevertheless, boxes are a very coarse approximation of an object and the approximation by a box has a large degree of ambiguity. Furthermore, tracking approaches that are not restricted to boxes cannot be evaluated within thebenchmarks without adding a penalty to them. We present a simple extension to the VOT evaluation procedure that enables to include these approaches. Furthermore, we present upper bounds for trackers restricted to boxes. Moreover, we present a new measure that captures how well an approach can cope with scale changes without the need of frame-wise labels. We present a learning-based approach which helps to identify frames with heavy occlusion automatically. The framework is tested on the segmentations of the VOT2016 dataset.\r"
  },
  "iccv2017_w28_correlationfilterswithweightedconvolutionresponses": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Object Tracking Challenge",
    "title": "Correlation Filters With Weighted Convolution Responses",
    "authors": [
      "Zhiqun He",
      "Yingruo Fan",
      "Junfei Zhuang",
      "Yuan Dong",
      "HongLiang Bai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w28/html/He_Correlation_Filters_With_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w28/He_Correlation_Filters_With_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In recent years, discriminative correlation filters based trackers have shown dominant results for visual object tracking. Combining the online learning efficiency of the correlation filters with the discriminative power ofCNNfeatures has aroused great attention. In this paper, we derive a continuous convolution operator based tracker which fully exploits the discriminative power in the CNN feature representations.In our work, we normalize each individual feature extracted from different layers of the deep pretrained CNN first, and after that, the weighted convolution responses from each feature block are summed to produce the final confidence score. By this weighted sum operation, the empirical evaluations demonstrate clear improvements by our proposed tracker based on the Efficient Convolution Operators Tracker (ECO). On the other hand, we find the 10-layers design is optimal for continuous scale estimation.\r"
  },
  "iccv2017_w28_integratingboundaryandcentercorrelationfiltersforvisualtrackingwithaspectratiovariation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Object Tracking Challenge",
    "title": "Integrating Boundary and Center Correlation Filters for Visual Tracking With Aspect Ratio Variation",
    "authors": [
      "Feng Li",
      "Yingjie Yao",
      "Peihua Li",
      "David Zhang",
      "Wangmeng Zuo",
      "Ming-Hsuan Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w28/html/Li_Integrating_Boundary_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w28/Li_Integrating_Boundary_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The aspect ratio variation frequently appears in visual tracking and has a severe influence on performance. Although many correlation filter (CF)-based trackers have also been suggested for scale adaptive tracking, few studies have been given to handle the aspect ratio variation for CF trackers. In this paper, we make the first attempt to address this issue by introducing a family of 1D boundary CFs to localize the left, right, top, and bottom boundaries in videos. This allows us cope with the aspect ratio variation flexibly during tracking. Specifically, we present a novel tracking model to integrate 1D Boundary and 2D Center CFs (IBCCF) where boundary and center filters are enforced by a near-orthogonality regularization term. To optimize our IBCCF model, we develop an alternating direction method of multipliers. Experiments on several datasets show that IBCCF can effectively handle aspect ratio variation, and achieves state-of-the-art performance in terms of accuracy and robustness.\r"
  },
  "iccv2017_w28_recurrentfilterlearningforvisualtracking": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Object Tracking Challenge",
    "title": "Recurrent Filter Learning for Visual Tracking",
    "authors": [
      "Tianyu Yang",
      "Antoni B. Chan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w28/html/Yang_Recurrent_Filter_Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w28/Yang_Recurrent_Filter_Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a recurrent filter generation methods for visual tracking. We directly feed the target's image patch to a recurrent neural network (RNN) to estimate an object-specific filter for tracking. As the video sequence is a spatiotemporal data, we extend the matrix multiplications of the fully-connected layers of the RNN to a convolution operation on feature maps, which preserves the target's spatial structure and also is memory-efficient. The tracked object in the subsequent frames will be fed into the RNN to adapt the generated filters to appearance variations of the target. Note that once the off-line training process of our network is finished, there is no need to fine-tune the network for specific objects, which makes our approach more efficient than methods that use iterative fine-tuning to online learn the target. Extensive experiments conducted on widely used benchmarks, OTB and VOT, demonstrate encouraging results compared to other recent methods.\r"
  },
  "iccv2017_w29_automatedstemangledeterminationfortemporalplantphenotypinganalysis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "Automated Stem Angle Determination for Temporal Plant Phenotyping Analysis",
    "authors": [
      "Sruti Das Choudhury",
      "Saptarsi Goswami",
      "Srinidhi Bashyam",
      "Ashok Samal",
      "Tala Awada"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Choudhury_Automated_Stem_Angle_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Choudhury_Automated_Stem_Angle_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Extracting meaningful phenotypes for temporal plant phenotyping analysis by considering individual parts of a plant, e.g., leaves and stem, using computer vision techniques remains a critical bottleneck due to constantly increasing complexity in plant architecture with variations in self-occlusions and phyllotaxy. The paper introduces an algorithm to compute stem angle for use as a measure of plants' susceptibility to lodging. It involves the identification of leaf-tips and leaf-junctions based on graph theoretic analysis. The efficacy of the proposed method is demonstrated based on a public dataset called Panicoid Phenomap-1. A time-series clustering analysis is performed on stem angle values during vegetative stage life cycle of the maize plants. This analysis summarizes the temporal patterns of the stem angles into three main groups, and establishes that the temporal variation of the stem angles is likely to be regulated by genetic variation under similar environmental conditions.\r"
  },
  "iccv2017_w29_locatingcropplantcentersfromuav-basedrgbimagery": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "Locating Crop Plant Centers From UAV-Based RGB Imagery",
    "authors": [
      "Yuhao Chen",
      "Javier Ribera",
      "Christopher Boomsma",
      "Edward Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Chen_Locating_Crop_Plant_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Chen_Locating_Crop_Plant_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we propose a method to find the location of crop plants in Unmanned Aerial Vehicle (UAV) imagery. Finding the location of plants is a crucial step to derive and track phenotypic traits for each plant. We describe some initial work in estimating field crop plant locations. We approach the problem by classifying pixels as a plant center or a non plant center. We use Multiple Instance Learning (MIL) to handle the ambiguity of plant center labeling in training data. The classification results are then post-processed to estimate the exact location of the crop plant. Experimental evaluation is conducted to evaluate the method and the result achieved an overall precision and recall of 66% and 64%, respectively.\r"
  },
  "iccv2017_w29_aneasy-to-setup3dphenotypingplatformforkomatsunadataset": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "An Easy-To-Setup 3D Phenotyping Platform for KOMATSUNA Dataset",
    "authors": [
      "Hideaki Uchiyama",
      "Shunsuke Sakurai",
      "Masashi Mishima",
      "Daisaku Arita",
      "Takashi Okayasu",
      "Atsushi Shimada",
      "Rin-ichiro Taniguchi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Uchiyama_An_Easy-To-Setup_3D_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Uchiyama_An_Easy-To-Setup_3D_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a 3D phenotyping platform that measures both plant growth and environmental information in small indoor environments for plant image datasets. Our objective is to construct a compact and complete platform by using commercial devices to allow any researcher to begin plant phenotyping in their laboratory. In addition, we introduce our annotation tool to manually but effectively create leaf labels in plant images on a pixel-by-pixel basis. Finally, we show our RGB-D and multiview datasets containing images in the early growth stages of the Komatsuna with leaf annotation.\r"
  },
  "iccv2017_w29_droughtstressclassificationusing3dplantmodels": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "Drought Stress Classification Using 3D Plant Models",
    "authors": [
      "Siddharth Srivastava",
      "Swati Bhugra",
      "Brejesh Lall",
      "Santanu Chaudhury"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Srivastava_Drought_Stress_Classification_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Srivastava_Drought_Stress_Classification_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Quantification of physiological changes in plants can capture different drought mechanisms and assist in selection of tolerant varieties in a high throughput manner.In this context, an accurate 3D model of plant canopy provides a reliable representation for drought stress characterization in contrast to using 2D images. In this paper, we propose a novel end-to-end pipeline including 3D reconstruction, segmentation and feature extraction, leveraging deep neural networks at various stages, for drought stress study. To overcome the high degree of self-similarities and self-occlusions in plant canopy, prior knowledge of leaf shape based on features from deep siamese network are used to construct an accurate 3D model using Structure from motion on wheat plants. The drought stress is characterized with a deep network based feature aggregation. We compare the proposed methodology on several descriptors, and show that the network outperforms conventional methods.\r"
  },
  "iccv2017_w29_deeplearningformulti-taskplantphenotyping": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "Deep Learning for Multi-Task Plant Phenotyping",
    "authors": [
      "Michael P. Pound",
      "Jonathan A. Atkinson",
      "Darren M. Wells",
      "Tony P. Pridmore",
      "Andrew P. French"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Pound_Deep_Learning_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Pound_Deep_Learning_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " There is a particular phenotyping demand to accurately quantify images of crops, and the natural variability and structure of these plants presents unique difficulties. Recently, machine learning approaches have shown impressive results in many areas of computer vision, but these rely on large datasets that are at present not available for crops. We present a new dataset, called ACID, that provides hundreds of accurately annotated images of wheat spikes and spikelets, along with image level class annotation. We then present a deep learning approach capable of accurately localising wheat spikes and spikelets, despite the varied nature of this dataset. As well as locating features, our network offers near perfect counting accuracy for spikes (95.91%) and spikelets (99.66%). We also extend the network to perform simultaneous classification of images, demonstrating the power of multi-task deep architectures for plant phenotyping.\r"
  },
  "iccv2017_w29_arigansyntheticarabidopsisplantsusinggenerativeadversarialnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "ARIGAN: Synthetic Arabidopsis Plants Using Generative Adversarial Network",
    "authors": [
      "Mario Valerio Giuffrida",
      "Hanno Scharr",
      "Sotirios A. Tsaftaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Giuffrida_ARIGAN_Synthetic_Arabidopsis_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Giuffrida_ARIGAN_Synthetic_Arabidopsis_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Inrecent years, there has been an increasing interest in image-based plant phenotyping, applying state-of-the-art machine learning approaches. Despite the recent release of a few plant phenotyping datasets, large annotated plant image datasets are lacking. We propose an alternative solution to dataset augmentation for plant phenotyping, creating artificial images of plants using generative neural networks. We propose the Arabidopsis Rosette Image Generator (through) Adversarial Network: a deep convnet able to generate synthetic rosette plants, inspired by DCGAN. We trained the network using the CVPPP 2017 LCC dataset (only Arabidopsis). We show that our model generates realistic images of plants. We train our network conditioning on leaf count, such that it is possible to generate plants with a given number of leaves. Furthermore, we propose a new Ax dataset of artificial plants images, showing that the testing error is reduced when Ax is used as part of the training data.\r"
  },
  "iccv2017_w29_leveragingmultipledatasetsfordeepleafcounting": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "Leveraging Multiple Datasets for Deep Leaf Counting",
    "authors": [
      "Andrei Dobrescu",
      "Mario Valerio Giuffrida",
      "Sotirios A. Tsaftaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Dobrescu_Leveraging_Multiple_Datasets_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Dobrescu_Leveraging_Multiple_Datasets_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The number of leaves of a plant has is one of the key traits (phenotypes) describing its development and growth. Here, we propose an automated, deep learning based approach for counting leaves in model rosette plants. Our method treats leaf counting as a direct regression problem and thus requires as only annotation the total leaf count per plant. We argue that combining different datasets when training the deep neural network is beneficial and improves the results of the proposed approach. We evaluate our method on the CVPPP 2017 Leaf Counting Challenge dataset, which contains images of Arabidopsis and tobacco plants. Experimental results show that the proposed method significantly outperforms the winner of the previous CVPPP challenge, improving the results by a minimum of50% on each of the test datasets, and can achieve this performance without knowing the experimental origin of the data (i.e. 'in the wild' setting of the challenge).\r"
  },
  "iccv2017_w29_leafcountingwithdeepconvolutionalanddeconvolutionalnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision Problems in Plant Phenotyping",
    "title": "Leaf Counting With Deep Convolutional and Deconvolutional Networks",
    "authors": [
      "Shubhra Aich",
      "Ian Stavness"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w29/html/Aich_Leaf_Counting_With_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w29/Aich_Leaf_Counting_With_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we investigate the problem of counting rosette leaves from an RGB image, an important task in plant phenotyping. We propose a data-driven approach for this task generalized over different plant species and imaging setups. To accomplish this task, we use state-of-the-art deep learning architectures: a deconvolutional network for initial segmentation and a convolutional network for leaf counting. Evaluation is performed on the leaf counting challenge dataset at CVPPP-2017. Despite the small number of training samples in this dataset, as compared to typical deep learning image sets, we obtain satisfactory performance on segmenting leaves from the background as a whole and counting the number of leaves using simple data augmentation strategies. Comparative analysis is provided against methods evaluated on the previous competition datasets. Our framework achieves mean and standard deviation of absolute count difference of 1.62 and 2.30 averaged over all five test datasets.\r"
  },
  "iccv2017_w30_detection,estimationandavoidanceofmobileobjectsusingstereo-visionandmodelpredictivecontrol": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for UAVs",
    "title": "Detection, Estimation and Avoidance of Mobile Objects Using Stereo-Vision and Model Predictive Control",
    "authors": [
      "Helene Roggeman",
      "Julien Marzat",
      "Maxime Derome",
      "Martial Sanfourche",
      "Alexandre Eudes",
      "Guy Le Besnerais"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w30/html/Roggeman_Detection_Estimation_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w30/Roggeman_Detection_Estimation_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a complete loop (detection, estimation, avoidance) for the safe navigation of an autonomous vehicle in presence of dynamical obstacles. For detecting moving objects from stereo images and estimating their positions, two algorithms are proposed. The first one is dense and has a high computational load but is designed to fully exploit GPU processing. The second one is lighter and can run on a standard embedded processor. After a step of filtering, the estimated mobile objects are exploited in a model predictive control scheme for collision avoidance while tracking a reference trajectory. Experimental results with the complete loop are reported for a micro-air vehicle and a mobile robot in realistic situations, with everything computed on board.\r"
  },
  "iccv2017_w30_creatingroadmapsinaerialimageswithgenerativeadversarialnetworksandsmoothing-basedoptimization": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for UAVs",
    "title": "Creating Roadmaps in Aerial Images With Generative Adversarial Networks and Smoothing-Based Optimization",
    "authors": [
      "Dragos Costea",
      "Alina Marcu",
      "Emil Slusanschi",
      "Marius Leordeanu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w30/html/Costea_Creating_Roadmaps_in_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w30/Costea_Creating_Roadmaps_in_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recognizing roads and intersections in aerial images is a challenging problem in computer vision with many real world applications, such as localization and navigation for unmanned aerial vehicles (UAVs). The problem is currently gaining momentum in computer vision and is still far from being solved. While recent approaches have greatly improved due to the advances in deep learning, they provide only pixel-level semantic segmentations. In this paper, we argue that roads and intersections should be recognized at the higher semantic level of road graphs - with roads being edges that connect nodes. Towards this goal we present a method consisting of two stages. During the first stage, we detect roads and intersections with a novel, dual-hop generative adversarial network (DH-GAN) that segments images at the level of pixels. At the second stage, given the pixelwise road segmentation, we find its best covering road graph by applying a smoothing-based graph optimization procedure. Our approach is able to outperform recent published methods and baselines on a large dataset with European roads.\r"
  },
  "iccv2017_w30_embeddedreal-timeobjectdetectionforauavwarningsystem": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for UAVs",
    "title": "Embedded Real-Time Object Detection for a UAV Warning System",
    "authors": [
      "Nils Tijtgat",
      "Wiebe Van Ranst",
      "Toon Goedeme",
      "Bruno Volckaert",
      "Filip De Turck"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w30/html/Tijtgat_Embedded_Real-Time_Object_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w30/Tijtgat_Embedded_Real-Time_Object_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we demonstrate and evaluate a method to perform real-time object detection on-board a UAV using the state of the art YOLOv2 object detection algorithm running on an NVIDIA Jetson TX2, an GPU platform targeted at power constrained mobile applications that use neural networks under the hood. This, as a result of comparing several cutting edge object detection algorithms. Multiple evaluations we present provide insights that help choose the optimal object detection configuration given certain frame rate and detection accuracy requirements. We propose how this setup running on-board a UAV can be used to process a video feed during emergencies in real-time, and feed a decision support warning system using the generated detections.\r"
  },
  "iccv2017_w30_feature-basedefficientmovingobjectdetectionforlow-altitudeaerialplatforms": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for UAVs",
    "title": "Feature-Based Efficient Moving Object Detection for Low-Altitude Aerial Platforms",
    "authors": [
      "K. Berker Logoglu",
      "Hazal Lezki",
      "M. Kerim Yucel",
      "Ahu Ozturk",
      "Alper Kucukkomurler",
      "Batuhan Karagoz",
      "Erkut Erdem",
      "Aykut Erdem"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w30/html/Logoglu_Feature-Based_Efficient_Moving_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w30/Logoglu_Feature-Based_Efficient_Moving_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem's rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field.\r"
  },
  "iccv2017_w30_robustuav-basedtrackingusinghybridclassifiers": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for UAVs",
    "title": "Robust UAV-Based Tracking Using Hybrid Classifiers",
    "authors": [
      "Yong Wang",
      "Wei Shi",
      "Shandong Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w30/html/Wang_Robust_UAV-Based_Tracking_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w30/Wang_Robust_UAV-Based_Tracking_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Robust object tracking plays an important role for unmanned aerial vehicles (UAVs). In this paper, we present a robust and efficient visual object tracking algorithm with an appearance model based on the locally adaptive regression kernel (LARK). The proposed appearance model preserves the geometric structure of the object. The tracking task is formulated as two binary classifiers via two support vector machines (SVMs) with online update. The backward tracking which tracks the object in reverse of time is employed to measure the accuracy and robustness of the two trackers. The final positions are adaptively fused based on the results of the forward tracking and backward tracking validation. Several state-of-the-art trackers are evaluated on the UAV123 benchmark dataset which includes challenging situations such as illumination variation, motion blur, pose variation and heavy occlusion. \r"
  },
  "iccv2017_w30_convolutionalneuralnetwork-baseddeepurbansignatureswithapplicationtodronelocalization": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for UAVs",
    "title": "Convolutional Neural Network-Based Deep Urban Signatures With Application to Drone Localization",
    "authors": [
      "Karim Amer",
      "Mohamed Samy",
      "Reda ElHakim",
      "Mahmoud Shaker",
      "Mohamed ElHelw"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w30/html/Amer_Convolutional_Neural_Network-Based_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w30/Amer_Convolutional_Neural_Network-Based_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Most commercial Small Unmanned Aerial Vehicles (SUAVs) rely solely on Global Navigation Satellite Systems (GNSSs) - such as GPS and GLONASS - to perform localization tasks during the execution of autonomous navigation activities. Despite being fast and accurate, satellite-based navigation systems have typical vulnerabilities and pitfalls in urban settings that may prevent successful drone localization. This paper presents the novel concept of \"Deep Urban Signatures\" where a deep convolutional neural network is used to compute a unique characterization for each urban area or district based on the visual appearance of its architecture and landscape style. Such information is used to identify the district and subsequently perform localization. The paper presents the methodology to compute the signatures and discusses the experiments carried out using Google maps and Bing maps, with latter used to simulate footage captured by SUAVs at different altitudes and/or using different camera zoom levels. The results obtained demonstrate that Deep Urban Signatures can be used to successfully accomplish district-level aerial drone localization with future work comprising accurate localization within each identified district.\r"
  },
  "iccv2017_w30_distributedbundleadjustment": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for UAVs",
    "title": "Distributed Bundle Adjustment",
    "authors": [
      "Karthikeyan Natesan Ramamurthy",
      "Chung-Ching Lin",
      "Aleksandr Aravkin",
      "Sharath Pankanti",
      "Raphael Viguier"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w30/html/Ramamurthy_Distributed_Bundle_Adjustment_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w30/Ramamurthy_Distributed_Bundle_Adjustment_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Most methods for Bundle Adjustment (BA) in computer vision are either centralized or operate incrementally. This leads to poor scaling and affects the quality of solution as the number of images grows in large scale structure from motion (SfM). Furthermore, they cannot be used in scenarios where image acquisition and processing must be distributed. We address this problemwith a new distributed BA algorithm. Our distributed formulation uses alternating direction method of multipliers (ADMM), and, since each processor sees only a small portion of the data, we show that robust formulations improve performance. We analyze convergence of the proposed algorithm, andillustrate numerical performance, accuracy of the parameter estimates, and scalability of the distributed implementation in the context of synthetic 3D datasets with known camera position and orientation ground truth. The results are comparable to an alternate state-of-the-art centralized bundle adjustment algorithm on synthetic and real 3D reconstruction problems. The runtime of our implementation scales linearly with the number of observed points. \r"
  },
  "iccv2017_w31_deeplearningofconvolutionalauto-encoderforimagematchingand3dobjectreconstructionintheinfraredrange": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Deep Learning of Convolutional Auto-Encoder for Image Matching and 3D Object Reconstruction in the Infrared Range",
    "authors": [
      "Vladimir A. Knyaz",
      "Oleg Vygolov",
      "Vladimir V. Kniaz",
      "Yury Vizilter",
      "Vladimir Gorbatsevich",
      "Thomas Luhmann",
      "Niklas Conen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Knyaz_Deep_Learning_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Knyaz_Deep_Learning_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Performing image matching in thermal images is challenging due to an absence of distinctive features and presence of thermal reflections. Still, in many applications, infrared imagery is an attractive solution for 3D object reconstruction that is robust against low light conditions. We present an image patch matching method based on deep learning. For image matching in the infrared range, we use codes generated by a convolutional auto-encoder. We evaluate the method in a full 3D object reconstruction pipeline that uses infrared imagery as an input. Image matches found using the proposed method are used for estimation of the camera pose. Dense 3D object reconstruction is performed using semi-global block matching. We evaluate on a dataset with real and synthetic images to show that our method outperforms existing image matching methods on the infrared imagery. We also evaluate the geometry of generated 3D models to demonstrate the increased reconstruction accuracy.\r"
  },
  "iccv2017_w31_efficientandaccurateregistrationofpointcloudswithplanetoplanecorrespondences": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Efficient and Accurate Registration of Point Clouds With Plane to Plane Correspondences",
    "authors": [
      "Wolfgang Forstner",
      "Kourosh Khoshelham"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Forstner_Efficient_and_Accurate_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Forstner_Efficient_and_Accurate_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose and analyse methods to efficiently register point clouds based on plane correspondences. Based on a segmentation of the point clouds into planar regions and matches of planes in different point clouds, we (1) optimally estimate the relative pose(s); (2) provide three direct solutions, of which two take the uncertainty of the given planes into account; and (3) analyse the loss in accuracy of the direct solutions as compared to the optimal solution. The paper presents the different solutions, derives their uncertainty, and compares their accuracy based on simulated and real data. \r"
  },
  "iccv2017_w31_3dposeregressionusingconvolutionalneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "3D Pose Regression Using Convolutional Neural Networks",
    "authors": [
      "Siddharth Mahendran",
      "Haider Ali",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Mahendran_3D_Pose_Regression_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Mahendran_3D_Pose_Regression_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " 3D pose estimation is a key component of many important computer vision tasks such as autonomous navigation and 3D scene understanding. Most state-of-the-art approaches to 3D pose estimation solve this problem as a pose-classification problem in which the pose space is discretized into bins and a CNN classifier is used to predict a pose bin. We argue that the 3D pose space is continuous and propose to solve the pose estimation problem in a CNN regression framework with a suitable representation, data augmentation and loss function that captures the geometry of the pose space. Experiments on PASCAL3D+ show that the proposed 3D pose regression approach achieves competitive performance compared to the state-of-the-art.\r"
  },
  "iccv2017_w31_propagationoforientationuncertaintyof3drigidobjecttoitspoints": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Propagation of Orientation Uncertainty of 3D Rigid Object to Its Points",
    "authors": [
      "Marek Franaszek",
      "Geraldine S. Cheok"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Franaszek_Propagation_of_Orientation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Franaszek_Propagation_of_Orientation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " If a CAD model of a rigid object is available, the location of any point on an object can be derived from the measured 6DOF pose of the object. However, the uncertainty of the measured pose propagates to the uncertainty of the point in an anisotropic way. We investigate this propagation for a class of systems that determine an object pose by using point-based rigid body registration. For such systems, the uncertainty in the location of the points used for registration propagates to the pose uncertainty. We find that for different poses of the object, the direction corresponding to the smallest propagated uncertainty remains relatively unchanged in the object's local frame, regardless of object pose.We show that this direction may be closely approximated by the moment of inertia axis which is based on the configuration of the fiducials. We use existing theory of rigid-body registration to explain the experimental results, discuss its limitations and practical implications of results. \r"
  },
  "iccv2017_w31_mutualhypothesisverificationfor6dposeestimationofnaturalobjects": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Mutual Hypothesis Verification for 6D Pose Estimation of Natural Objects",
    "authors": [
      "Kiru Park",
      "Johann Prankl",
      "Markus Vincze"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Park_Mutual_Hypothesis_Verification_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Park_Mutual_Hypothesis_Verification_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Estimating the 6D pose of natural objects, such as vegetables and fruit, is a challenging problem due to the high variability of their shape. We propose a novel framework that consists of a local and a global hypothesis generation pipeline with a mutual verification step. The new local descriptor is proposed to find critical parts of the natural object while the global estimator calculates object pose directly. A novel hypothesis verification step, Mutual Hypothesis Verification, is proposed to determine the best pose. It interactively uses information from the local and the global pipelines. New hypotheses are generated by combining the global estimation and the local shape correspondences. The confidence of a pose candidate is calculated by comparing with estimation results from both pipelines. The evaluation with real fruit shows the potential for estimating the pose of any natural object while outperforming global feature based approaches.\r"
  },
  "iccv2017_w31_introducingmvtecitodd-adatasetfor3dobjectrecognitioninindustry": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Introducing MVTec ITODD - A Dataset for 3D Object Recognition in Industry",
    "authors": [
      "Bertram Drost",
      "Markus Ulrich",
      "Paul Bergmann",
      "Philipp Hartinger",
      "Carsten Steger"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Drost_Introducing_MVTec_ITODD_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Drost_Introducing_MVTec_ITODD_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We introduce the MVTec Industrial 3D Object Detection Dataset (MVTec ITODD), a public dataset for 3D object detection and pose estimation with a strong focus on ob- jects, settings, and requirements that are realistic for indus- trial setups. Contrary to other 3D object detection datasets that often represent scenarios from everyday life or mo- bile robotic environments, our setup models industrial bin picking and object inspection tasks that often face different challenges. Additionally, the evaluation citeria are focused on practical aspects, such as runtimes, memory consump- tion, useful correctness measurements, and accuracy. The dataset contains 28 objects with different characteristics, arranged in over 800 scenes and labeled with around 3500 rigid 3D transformations of the object instances as ground truth.\r"
  },
  "iccv2017_w31_symmetryawareevaluationof3dobjectdetectionandposeestimationinscenesofmanypartsinbulk": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in Scenes of Many Parts in Bulk",
    "authors": [
      "Romain Bregier",
      "Frederic Devernay",
      "Laetitia Leyrit",
      "James L. Crowley"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Bregier_Symmetry_Aware_Evaluation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Bregier_Symmetry_Aware_Evaluation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " While 3D object detection and pose estimation has been studied for a long time, its evaluation is not yet completely satisfactory. Indeed, existing datasets typically consist in numerous acquisitions of only a few scenes because of the tediousness of pose annotation, and existing evaluation protocols cannot handle properly objects with symmetries. This work aims at addressing those two points. We first present automatic techniques to produce fully annotated RGBD data of many object instances in arbitrary poses, with which we produce a dataset of thousands of independent scenes of bulk parts composed of both real and synthetic images. We then propose a consistent evaluation methodology suitable for any rigid object, regardless of its symmetries. We illustrate it with two reference object detection and pose estimation methods on different objects, and show that incorporating symmetry considerations into pose estimation methods themselves can lead to significant performance gains.\r"
  },
  "iccv2017_w31_combinedholisticandlocalpatchesforrecovering6dobjectpose": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Combined Holistic and Local Patches for Recovering 6D Object Pose",
    "authors": [
      "Haoruo Zhang",
      "Qixin Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Zhang_Combined_Holistic_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Zhang_Combined_Holistic_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a novel method for recovering 6D object pose in RGB-D images. By contrast with recent holistic or local patch-based method, we combine holistic patches and local patches together to fulfil this task. Our method has three stages, including holistic patch classification, local patch regression and fine 6D pose estimation. Firstly, we apply a simple Convolutional Neural Network (CNN) to classify all the sampled holistic patches from the scene image. Then, the candidate region of target object can be segmented. Secondly, a Convolutional Autoencoder (CAE) is employed to extract condensed local patch feature, and coarse 6D object pose can be estimated by the regression of feature voting. Finally, we apply Particle Swarm Optimization (PSO) to refine 6D object pose. Our method is evaluated on the LINEMOD dataset and the Occlusion dataset. Experimental results show that our method has high precision and good performance under foreground occlusion and background clutter conditions.\r"
  },
  "iccv2017_w31_multi-view6dobjectposeestimationandcameramotionplanningusingrgbdimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W31",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Recovering 6D Object Pose",
    "title": "Multi-View 6D Object Pose Estimation and Camera Motion Planning Using RGBD Images",
    "authors": [
      "Juil Sock",
      "S. Hamidreza Kasaei",
      "Luis Seabra Lopes",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w31/html/Sock_Multi-View_6D_Object_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w31/Sock_Multi-View_6D_Object_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recovering object pose in a crowd is a challenging task due to severe occlusions and clutters. In active scenario, whenever an observer fails to recover the poses of objects from the current view point, the observer is able to determine the next view position and captures a new scene from another view point to improve the knowledge of the environment, which may reduce the 6D pose estimation uncertainty. We propose a complete active multi-view framework to recognize 6DOF pose of multiple object instances in a crowded scene. We include several components in active vision setting to increase the accuracy: Hypothesis accumulation and verification combines single-shot based hypotheses estimated from previous views and extract the most likely set of hypotheses; an entropy-based Next-Best-View prediction generates next camera position to capture new data to increase the performance; camera motion planning plans the trajectory of the camera based on the view entropy and the cost of movement. \r"
  },
  "iccv2017_w32_multi-modalembeddingformainproductdetectioninfashion": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Multi-Modal Embedding for Main Product Detection in Fashion",
    "authors": [
      "Antonio Rubio",
      "LongLong Yu",
      "Edgar Simo-Serra",
      "Francesc Moreno-Noguer"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Rubio_Multi-Modal_Embedding_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Rubio_Multi-Modal_Embedding_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present an approach to detect the main product in fashion images by exploiting the textual metadata associated with each image. Our approach is based on a Convolutional Neural Network and learns a joint embedding of object proposals and textual metadata to predict the main product in the image. We additionally use several complementary classification and overlap losses in order to improve training stability and performance. Our tests on a large-scale dataset taken from eight e-commerce sites show that our approach outperforms strong baselines and is able to accurately detect the main product in a wide diversity of challenging fashion images.\r"
  },
  "iccv2017_w32_learningunifiedembeddingforapparelrecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Learning Unified Embedding for Apparel Recognition",
    "authors": [
      "Yang Song",
      "Yuan Li",
      "Bo Wu",
      "Chao-Yeh Chen",
      "Xiao Zhang",
      "Hartwig Adam"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Song_Learning_Unified_Embedding_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Song_Learning_Unified_Embedding_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In apparel recognition, deep neural network models are often trained separately for different verticals. However, using specialized models for different verticals is not scalable and expensive to deploy. This paper addresses the problem of learning one unified embedding model for multiple object verticals (e.g. all apparel classes) without sacrificing accuracy. The problem is tackled from two aspects: training data and training difficulty. On the training data aspect, we figure out that for a single model trained with triplet loss, there is an accuracy sweet spot in terms of how many verticals are trained together. To ease the training difficulty, a novel learning scheme is proposed by using the output from specialized models as learning targets so that L2 loss can be used instead of triplet loss. This new loss makes the training easier and make it possible for more efficient use of the feature space. The end result is a unified model which can achieve the same retrieval accuracy as a number of separate specialized models, while having the model complexity as one.\r"
  },
  "iccv2017_w32_whatmakesastyleexperimentalanalysisoffashionprediction": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "What Makes a Style: Experimental Analysis of Fashion Prediction",
    "authors": [
      "Moeko Takagi",
      "Edgar Simo-Serra",
      "Satoshi Iizuka",
      "Hiroshi Ishikawa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Takagi_What_Makes_a_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Takagi_What_Makes_a_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this work, we perform an experimental analysis of the differences of both how humans and machines see and distinguish fashion styles. For this purpose, we propose an expert-curated new dataset for fashion style prediction, which consists of 14 different fashion styles each with roughly 1,000 images of worn outfits. The dataset, with a total of 13,126 images, captures the diversity and complexity of modern fashion styles. We perform an extensive analysis of the dataset by benchmarking a wide variety of modern classification networks, and also perform an in-depth user study with both fashion-savvy and fashion-naive users. Our results indicate that, although classification networks are able to outperform naive users, they are still far from the performance of savvy users, for which it is important to not only consider texture and color, but subtle differences in the combination of garments.\r"
  },
  "iccv2017_w32_3dgarmentdigitisationforvirtualwardrobeusingacommoditydepthsensor": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "3D Garment Digitisation for Virtual Wardrobe Using a Commodity Depth Sensor",
    "authors": [
      "Dongjoe Shin",
      "Yu Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Shin_3D_Garment_Digitisation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Shin_3D_Garment_Digitisation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " A practical garment digitisation should be efficient and robust to minimise the cost of processing a large volume of garments manufactured in every season. In addition, the quality of a texture map needs to be high to deliver a better user experience of VR/AR applications using garment models such as digital wardrobe or virtual fitting room. To address this, we propose a novel pipeline for fast, low-cost, and robust 3D garment digitisation with minimal human involvement. The proposed system is simply configured with a commodity RGB-D sensor (e.g. Kinect) and a rotating platform where a mannequin is placed to put on a target garment. Since a conventional reconstruction pipeline such as Kinect Fusion (KF) tends to fail to track the correct camera pose under fast rotation, we modelled the camera motion and fed this as a guidance of the ICP process in KF. The proposed method is also designed to produce a high-quality texture map by stitching the best views from a single rotation, and a modified shape from silhouettes algorithm has been developed to extract a garment model from a mannequin.\r"
  },
  "iccv2017_w32_multi-labelfashionimageclassificationwithminimalhumansupervision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Multi-Label Fashion Image Classification With Minimal Human Supervision",
    "authors": [
      "Naoto Inoue",
      "Edgar Simo-Serra",
      "Toshihiko Yamasaki",
      "Hiroshi Ishikawa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Inoue_Multi-Label_Fashion_Image_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Inoue_Multi-Label_Fashion_Image_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We tackle the problem of multi-label classification of fashion images, learning from noisy data with minimal human supervision. We present a new dataset of full body poses, each with a set of 66 binary labels corresponding to the information about the garments worn in the image obtained in an automatic manner. As the automatically-collected labels contain significant noise, we manually correct the labels for a small subset of the data, and use these correct labels for further training and evaluation. We build upon a recent approach that both cleans the noisy labels and learns to classify, and introduce simple changes that can significantly improve the performance.\r"
  },
  "iccv2017_w32_leveragingweaklyannotateddataforfashionimageretrievalandlabelprediction": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction",
    "authors": [
      "Charles Corbiere",
      "Hedi Ben-Younes",
      "Alexandre Rame",
      "Charles Ollion"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Corbiere_Leveraging_Weakly_Annotated_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Corbiere_Leveraging_Weakly_Annotated_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we present a method to learn a visual representation adapted for e-commerce products. Based on weakly supervised learning, our model learns from noisy datasets crawled on e-commerce website catalogs and does not require any manual labeling. We show that our representation can be used for downward classification tasks over clothing categories with different levels of granularity. We also demonstrate that the learnt representation is suitable for image retrieval. We achieve nearly state-of-art results on the DeepFashion In-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without using the provided training set. \r"
  },
  "iccv2017_w32_recommendingoutfitsfrompersonalcloset": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Recommending Outfits From Personal Closet",
    "authors": [
      "Pongsate Tangseng",
      "Kota Yamaguchi",
      "Takayuki Okatani"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Tangseng_Recommending_Outfits_From_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Tangseng_Recommending_Outfits_From_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We consider the outfit grading problem for outfit recommendation, where we assume that users have a closet of items and we aim at producing a score for an arbitrary combination of items in the closet. The challenge in outfit grading is that the input to the system is a bag of item pictures that are unordered and vary in size. We build a deep neural network-based system that can take variable-length items and predict a score. We collect a large number of outfits from a popular fashion sharing website, Polyvore, and evaluate the performance of our grading system. We compare our model with a random-choice baseline. The performance of our model achieves 84% in both accuracy and precision, showing our model can reliably grade the quality of an outfit. We also built an outfit recommender on top of our grader to demonstrate the practical application of our model for a personal closet assistant. \r"
  },
  "iccv2017_w32_anaccuratesystemforfashionhand-drawnsketchesvectorization": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "An Accurate System for Fashion Hand-Drawn Sketches Vectorization",
    "authors": [
      "Luca Donati",
      "Simone Cesano",
      "Andrea Prati"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Donati_An_Accurate_System_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Donati_An_Accurate_System_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Automatic vectorization of fashion hand-drawn sketches is a crucial task performed by fashion industries to speed up their workflows. Performing vectorization on hand-drawn sketches is not an easy task, and it requires a first crucial step that consists in extracting precise and thin lines from sketches that are potentially very diverse (depending on the tool used and on the designer capabilities and preferences). This paper proposes a system for automatic vectorization of fashion hand-drawn sketches based on Pearson's Correlation Coefficient with multiple Gaussian kernels in order to enhance and extract curvilinear structures in a sketch. The use of correlation grants invariancy about image contrast and lighting, making the extracted lines more reliable for vectorization. Moreover, the proposed algorithm has been designed to equally extract both thin and wide lines with changing stroke hardness, which are common in fashion hand-drawn sketches. It also works for crossing lines, adjacent parallel lines and needs very few parameters (if any) to run. The efficacy of the proposal has been demonstrated on both hand-drawn sketches and images with added artificial noise, showing in both cases excellent performance w.r.t. the state of the art.\r"
  },
  "iccv2017_w32_theconditionalanalogyganswappingfashionarticlesonpeopleimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "The Conditional Analogy GAN: Swapping Fashion Articles on People Images",
    "authors": [
      "Nikolay Jetchev",
      "Urs Bergmann"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Jetchev_The_Conditional_Analogy_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Jetchev_The_Conditional_Analogy_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a novel method to solve image analogy problems: it allows to learn the relation between paired images present in training data, and then generalize and generate images that correspond to the relation, but were never seen in the training set. Therefore, we call the method Conditional Analogy Generative Adversarial Network (CAGAN), as it is based on adversarial training and employs deep convolutional neural networks. An especially interesting application of that technique is automatic swapping of clothing on fashion model photos. Our work has the following contributions. First, the definition of the end-to-end trainable CAGAN architecture, which implicitly learns segmentation masks without expensive supervised labeling data. Second, experimental results show plausible segmentation masks and often convincing swapped images, given the target article. Finally, we discuss the next steps for that technique: neural network architecture improvements and more advanced applications.\r"
  },
  "iccv2017_w32_dresslikeastarretrievingfashionproductsfromvideos": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Dress Like a Star: Retrieving Fashion Products From Videos",
    "authors": [
      "Noa Garcia",
      "George Vogiatzis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Garcia_Dress_Like_a_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Garcia_Dress_Like_a_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This work proposes a system for retrieving clothing and fashion products from video content. Although films and television are the perfect showcase for fashion brands to promote their products, spectators are not always aware of where to buy the latest trends they see on screen. Here, a framework for breaking the gap between fashion products shown on videos and users is presented. By relating clothing items and video frames in an indexed database and performing frame retrieval with temporal aggregation and fast indexing techniques, we can find fashion products from videos in a simple and non-intrusive way. Experiments in a large-scale dataset conducted here show that, by using the proposed framework, memory requirements can be reduced by 42.5X with respect to linear search, whereas accuracy is maintained at around 90%.\r"
  },
  "iccv2017_w32_pointcloudcompletionoffootshapefromasingledepthmapforfitmatchingusingdeeplearningviewsynthesis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Point Cloud Completion of Foot Shape From a Single Depth Map for Fit Matching Using Deep Learning View Synthesis",
    "authors": [
      "Nolan Lunscher",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Lunscher_Point_Cloud_Completion_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Lunscher_Point_Cloud_Completion_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In clothing and particularly in footwear, the variance in the size and shape of people and of clothing poses a problem of how to match items of clothing to a person. 3D scanning can be used to determine detailed personalized shape information, which can then be used to match against clothing shape. In current implementations however, this process is typically expensive and cumbersome. Ideally, in order to reduce the cost and complexity of scanning systems as much as possible, only a single image from a single camera would be needed. To this end, we focus on simplifying the process of scanning a person's foot for use in virtual footwear fitting. We use a deep learning approach to allow for whole foot shape reconstruction from a single input depth map view by synthesizing a view containing the remaining information about the foot not seen from the input. Our method directly adds information to the input view, and does not require any additional steps for point cloud alignment. We show that our method is capable of synthesizing the remainder of a point cloud with accuracies of 2.92+-0.72 mm.\r"
  },
  "iccv2017_w32_hierarchicalcategorydetectorforclothingrecognitionfromvisualdata": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Computer Vision for Fashion",
    "title": "Hierarchical Category Detector for Clothing Recognition From Visual Data",
    "authors": [
      "Suren Kumar",
      "Rui Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w32/html/Kumar_Hierarchical_Category_Detector_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w32/Kumar_Hierarchical_Category_Detector_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Clothing detection is an important step for retrieving similar clothing items, organizing fashion photos, artificial intelligence powered shopping assistants and automatic labeling of large catalogues. Training a deep learning based clothing detector requires pre-defined categories (dress, pants etc) and a high volume of annotated image data for each category. However, fashion evolves and new categories are constantly introduced in the marketplace. For example, consider the case of jeggings which is a combination of jeans and leggings. Detection of this new category will require adding annotated data specific to jegging class and subsequently relearning the weights for the deep network. In this paper, we propose a novel object detection method that can handle newer category without the need of obtaining new labeled data and retraining the network. Our approach learns the visual similarities between various clothing categories and predicts a tree of categories. The resulting framework significantly improves the generalization capabilities of the detector to novel clothing products.\r"
  },
  "iccv2017_w34_temporallocalizationandspatialsegmentationofjointattentioninmultiplefirst-personvideos": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Temporal Localization and Spatial Segmentation of Joint Attention in Multiple First-Person Videos",
    "authors": [
      "Yifei Huang",
      "Minjie Cai",
      "Hiroshi Kera",
      "Ryo Yonetani",
      "Keita Higuchi",
      "Yoichi Sato"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/attention_hyfiis.u-tokyo.ac.jp_cai-mjiis.u-tokyo.ac.jp_keraiis.u-tokyo.ac.jp_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/attention_hyfiis.u-tokyo.ac.jp_cai-mjiis.u-tokyo.ac.jp_keraiis.u-tokyo.ac.jp_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This work aims to develop a computer-vision technique for understanding objects jointly attended by a group of people during social interactions. As a key tool to discover such objects of joint attention, we rely on a collection of wearable eye-tracking cameras that provide a first-person video of interaction scenes and points-of-gaze data of interacting parties. Technically, we propose a hierarchical conditional random field-based model that can 1) localize events of joint attention temporally and 2) segment objects of joint attention spatially. We show that by alternating these two procedures, objects of joint attention can be discovered reliably even from cluttered scenes and noisy points-of-gaze data. Experimental results demonstrate that our approach outperforms several state-of-the-art methods for co-segmentation and joint attention discovery.\r"
  },
  "iccv2017_w34_findingtimetogetherdetectionandclassificationoffocusedinteractioninegocentricvideo": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Finding Time Together: Detection and Classification of Focused Interaction in Egocentric Video",
    "authors": [
      "Sophia Bano",
      "Stephen J. McKenna",
      "Jianguo Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/individuals_s.banodundee.ac.uk_s.j.z.mckennadundee.ac.uk_j.n.zhangdundee.ac.uk_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/individuals_s.banodundee.ac.uk_s.j.z.mckennadundee.ac.uk_j.n.zhangdundee.ac.uk_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Focused interaction occurs when co-present individuals, having mutual focus of attention, interact by establishing face-to-face engagement and direct conversation. Face-to-face engagement is often not maintained throughout the entirety of a focused interaction. In this paper, we present an online method for automatic classification of unconstrained egocentric (first-person perspective) videos into segments having no focused interaction, focused interaction when the camera wearer is stationary and focused interaction when the camera wearer is moving. We extract features from both audio and video data streams and perform temporal segmentation by using support vector machines with linear and non-linear kernels. We provide empirical evidence that fusion of visual face track scores, camera motion profile and audio voice activity scores is an effective combination for focused interaction classification. \r"
  },
  "iccv2017_w34_saltinetscan-pathpredictionon360degreeimagesusingsaliencyvolumes": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "SaltiNet: Scan-Path Prediction on 360 Degree Images Using Saliency Volumes",
    "authors": [
      "Marc Assens Reina",
      "Xavier Gir\u00f3-i-Nieto",
      "Kevin McGuinness",
      "Noel E. O'Connor"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/SaltiNet_marc.a.r95gmail.com_kevin.mcguinnessgmail.com_xavier.giroupc.edu_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/SaltiNet_marc.a.r95gmail.com_kevin.mcguinnessgmail.com_xavier.giroupc.edu_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We introduce SaltiNet, a deep neural network for scanpath prediction trained on 360-degree images. The model is based on a temporal-aware novel representation of saliency information named the saliency volume. The first part of the network consists of a model trained to generate saliency volumes, whose parameters are fit by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency volumes. Sampling strategies over these volumes are used to generate scanpaths over the 360-degree images. Our experiments show the advantages of using saliency volumes, and how they can be used for related tasks. Our source code and trained models available at https://github.com/massens/saliency-360salient-2017.\r"
  },
  "iccv2017_w34_convolutionallongshort-termmemorynetworksforrecognizingfirstpersoninteractions": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Convolutional Long Short-Term Memory Networks for Recognizing First Person Interactions",
    "authors": [
      "Swathikiran Sudhakaran",
      "Oswald Lanz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/networks_sudhakaranfbk.eu_lanzfbk.eu_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/networks_sudhakaranfbk.eu_lanzfbk.eu_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a novel deep learning approach for addressing the problem of interaction recognition from a first person perspective. The approach uses a pair of convolutional neural networks, whose parameters are shared, for extracting frame level features from successive frames of the video. The frame level features are then aggregated using a convolutional long short-term memory. The final hidden state of the convolutional long short-term memory is used for classification in to the respective categories. In our network the spatio-temporal structure of the input is preserved till the very final processing stage. Experimental results show that our method outperforms the state of the art on most recent first person interactions datasets that involve complex ego-motion. On UTKinect, it competes with methods that use depth image and skeletal joints information along with RGB images, while it surpasses previous methods that use only RGB images by more than 20% in recognition accuracy.\r"
  },
  "iccv2017_w34_batch-basedactivityrecognitionfromegocentricphoto-streams": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Batch-Based Activity Recognition From Egocentric Photo-Streams",
    "authors": [
      "Alejandro Cartas",
      "Mariella Dimiccoli",
      "Petia Radeva"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/However_alejandro.cartasub.edu_mariella.dimiccolicvc.uab.es_radevapgmail.com_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/However_alejandro.cartasub.edu_mariella.dimiccolicvc.uab.es_radevapgmail.com_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Activity recognition from unstructured egocentric photo-streams has several applications in assistive technology such as health monitoring. However, one of its main challenges is to deal with the low frame rate of wearable photo-cameras, which causes abrupt appearance changes between consecutive frames making motion estimation unfeasible. We present a batch-driven approach for training a deep learning architecture that strongly rely on Long short-term units to tackle this problem. We propose two different implementations of the same approach that process a photo-stream sequence using batches of fixed size with the goal of capturing the temporal evolution of high-level features. Experimental results over a public dataset acquired by three users demonstrate the validity of the proposed architectures to exploit the temporal evolution of convolutional features over time.\r"
  },
  "iccv2017_w34_usingcross-modelegosupervisiontolearncooperativebasketballintention": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Using Cross-Model EgoSupervision to Learn Cooperative Basketball Intention",
    "authors": [
      "Gedas Bertasius",
      "Jianbo Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/attention_gbertaseas.upenn.edu_jshiseas.upenn.edu_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/attention_gbertaseas.upenn.edu_jshiseas.upenn.edu_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a first-person method for cooperative basketball intention prediction: we predict with whom the camera wearer will cooperate in the near future from unlabeled first-person images. This is a challenging task that requires inferring the camera wearer's visual attention, and decoding the social cues of other players. Our key observation is that a first-person view provides strong cues to infer the camera wearer's intentions. We exploit this observation via a new cross-model EgoSupervision learning scheme that allows us to predict with whom the camera wearer will cooperate, without using manually labeled intention labels. Our cross-model EgoSupervision operates by transforming the outputs of a pretrained pose-estimation network, into pseudo ground truth labels, which are then used as a supervisory signal to train a new network for a cooperative intention task. We evaluate our method, and show that it achieves similar or even better accuracy than the fully supervised methods do.\r"
  },
  "iccv2017_w34_anobjectisworthsixthousandpicturestheegocentric,manual,multi-image(emmi)dataset": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "An Object Is Worth Six Thousand Pictures: The Egocentric, Manual, Multi-Image (EMMI) Dataset",
    "authors": [
      "Xiaohan Wang",
      "Fernanda M. Eliott",
      "James Ainooson",
      "Joshua H. Palmer",
      "Maithilee Kunda"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/outcomes_xiaohan.wangvanderbilt.edu_fernanda.m.eliottvanderbilt.edu_james.ainoosonvanderbilt.edu_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/outcomes_xiaohan.wangvanderbilt.edu_fernanda.m.eliottvanderbilt.edu_james.ainoosonvanderbilt.edu_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We describe a new image dataset collected to enable the study of how appearance-related and distributional properties of visual experience affect learning outcomes, called the Egocentric, Manual, Multi-Image (EMMI) dataset. Images in EMMI come from first-person, wearable camera recordingsofcommonhouseholdobjectsandtoysbeingmanually manipulated to undergo structured transformations like rotation and translation.We also present results from initial experiments, using deep convolutional neural networks, that begin to examine how different distributions of training data can affect visual object recognition, and how the representation of properties like rotation invariance can be studied in novel ways using the unique properties of EMMI.\r"
  },
  "iccv2017_w34_howshallweevaluateegocentricactionrecognition?": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "How Shall We Evaluate Egocentric Action Recognition?",
    "authors": [
      "Antonino Furnari",
      "Sebastiano Battiato",
      "Giovanni Maria Farinella"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/Consequently_furnaridmi.unict.it_battiatodmi.unict.it_gfarinelladmi.unict.it_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/Consequently_furnaridmi.unict.it_battiatodmi.unict.it_gfarinelladmi.unict.it_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Egocentric action analysis methods often assume that input videos are trimmed and hence they tend to focus on action classification rather than recognition. Consequently, adopted evaluation schemes are often unable to assess important properties of the desired action video segmentation output, which are deemed to be meaningful in real scenarios (e.g., oversegmentation and boundary localization precision). To overcome the limits of current evaluation methodologies, we propose a set of measures aimed to quantitatively and qualitatively assess the performance of egocentric action recognition methods. To improve exploitability of current action classification methods in the recognition scenario, we investigate how frame-wise predictions can be turned into action-based temporal video segmentations. Experiments on both synthetic and real data show that the proposed set of measures can help to improve evaluation and to drive the design of egocentric action recognition methods.\r"
  },
  "iccv2017_w34_fullyconvolutionalnetworkandregionproposalforinstanceidentificationwithegocentricvision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Fully Convolutional Network and Region Proposal for Instance Identification With Egocentric Vision",
    "authors": [
      "Maxime Portaz",
      "Matthias Kohl",
      "Georges Qu\u00e9not",
      "Jean-Pierre Chevallet"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/Additionally_maxime.portazgmail.com_matthias.kohletu.univ-grenoble-alpes.fr_jean-pierre.chevalletimag.fr_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/Additionally_maxime.portazgmail.com_matthias.kohletu.univ-grenoble-alpes.fr_jean-pierre.chevalletimag.fr_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents a novel approach for egocentric image retrieval and object detection. This approach uses fully convolutional network (FCN) to obtain region proposals without the need for an additional component in the network and training. It is particularly suited for small dataset with low object variability. The proposed network can be trained end-to-end and it produces an effective global descriptor as image representation. Additionally, it can be built upon any type of CNN pre-trained for classification. Through multiple experiments on two egocentric images datasets taken from museum visits, we show that the descriptor obtained using our proposed network outperforms those from previous state-of-the-art approaches. It is also just as memoryefficient, making it adapted to mobile device like augmented museum audio-guide.\r"
  },
  "iccv2017_w34_outdooroperationofstructuredlightinmobilephone": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Outdoor Operation of Structured Light in Mobile Phone",
    "authors": [
      "Byeonghoon Park",
      "Yongchan Keh",
      "Donghi Lee",
      "Yongkwan Kim",
      "Sungsoon Kim",
      "Kisuk Sung",
      "Jungkee Lee",
      "Donghoon Jang",
      "Youngkwon Yoon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/usage_bh711.parksamsung.com_y.c.kehsamsung.com_ofldhsamsung.com_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/usage_bh711.parksamsung.com_y.c.kehsamsung.com_ofldhsamsung.com_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Active Depth Camera is about to be integrated into a mobile phone and now beginning to be introduced into the market. It is expected that this technology will enable brand-new and meaningful user experiences in egocentric ecosystems. In view of practical usage, however, Active Depth Camera does not operate well especially outdoors because signal light is much weaker than ambient sunlight. To overcome this problem, Spectro-Temporal Light Filtering, adopting a light source of 940nm wavelength, has been designed. In order to check and improve outdoor depth quality, mobile phones for proof-of-concept have been implemented with structured light depth camera enclosed. We present its outdoor performance, featuring a 940nm vertical cavity surface emitting laser as a light source and an image sensor with a global shutter to reduce ambient light noise. The result makes us confident that this functionality enables mobile camera technology to step into another stunning stage.\r"
  },
  "iccv2017_w34_readingtextinthewildfromcompressedimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Egocentric Perception, Interaction and Computing",
    "title": "Reading Text in the Wild From Compressed Images",
    "authors": [
      "Leonardo Galteri",
      "Dena Bazazian",
      "Lorenzo Seidenari",
      "Marco Bertini",
      "Andrew D. Bagdanov",
      "Anguelos Nicolaou",
      "Dimosthenis Karatzas",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w34/html/degrees_leonardo.galteriunifi.it_dbazaziancvc.uab.es_lorenzo.seidenariunifi.it_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w34/degrees_leonardo.galteriunifi.it_dbazaziancvc.uab.es_lorenzo.seidenariunifi.it_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Reading text in the wild is gaining attention in the computer vision community. Images captured in the wild are almost always compressed to varying degrees, depending on application context, and this compression introduces artifacts that distort image content into the captured images. In this paper we investigate the impact these compression artifacts have on text localization and recognition in the wild. We also propose a deep Convolutional Neural Network (CNN) that can eliminate text-specific compression artifacts and which leads to an improvement in text recognition. Experimental results on the ICDAR-Challenge4 dataset demonstrate that compression artifacts have a significant impact on text localization and recognition and that our approach yields an improvement in both -- especially at high compression rates.\r"
  },
  "iccv2017_w35_edgeslamedgepointsbasedmonocularvisualslam": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "Edge SLAM: Edge Points Based Monocular Visual SLAM",
    "authors": [
      "Soumyadip Maity",
      "Arindam Saha",
      "Brojeshwar Bhowmick"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Saha_Edge_SLAM_Edge_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Saha_Edge_SLAM_Edge_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Visual SLAM shows significant progress in recent years due to high attention from vision community but still challenges remain for low textured environments. Feature based visual SLAMs break down due to insufficient features in low textured environment. This paper presents Edge SLAM, a feature based monocular visual SLAM which alleviates this problem. Our proposed method detects edge points from images and tracks those using optical flow, subsequently initialized with robust map quantification. Our method identifies the potential situations where estimating a new camera is becoming unreliable and we adopt a novel method to incorporate the new camera into existing reconstruction using a local optimization technique. We present an evaluation of our proposed system with most popular open datasets. Experimental result indicates that proposed method has comparable accuracy in featured environment and out performed in low textured environment compared to existing monocular SLAM approaches.\r"
  },
  "iccv2017_w35_probabilisticsurfelfusionfordenselidarmapping": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "Probabilistic Surfel Fusion for Dense LiDAR Mapping",
    "authors": [
      "Chanoh Park",
      "Soohwan Kim",
      "Peyman Moghadam",
      "Clinton Fookes",
      "Sridha Sridharan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Park_Probabilistic_Surfel_Fusion_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Park_Probabilistic_Surfel_Fusion_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " With the recent development of high-end LiDARs, more and more systems are able to continuously map the environment while moving and producing spatially redundant information. However, none of the previous approaches were able to effectively exploit this redundancy in a dense LiDAR mapping problem.In this paper, we present a new approach for dense LiDAR mapping using probabilistic surfel fusion. The proposed system is capable of reconstructing a high-quality dense surface element (surfel) map from spatially redundant multiple views. This is achieved by a proposed probabilistic surfel fusion along with a geometry considered data association. The proposed surfel data association method considers surficial resolution as well as high measurement uncertainty which makes the mapping system being able to control surface resolution without introducing space digitization. The proposed fusion method successfully suppresses the map noise level by considering a Bayesian filtering framework. \r"
  },
  "iccv2017_w35_computervisionmeetsgeometricmodelingmulti-viewreconstructionofsurfacepointsandnormalsusingaffinecorrespondences": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "Computer Vision Meets Geometric Modeling: Multi-View Reconstruction of Surface Points and Normals Using Affine Correspondences",
    "authors": [
      "Ivan Eichhardt",
      "Levente Hajder"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Eichhardt_Computer_Vision_Meets_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Eichhardt_Computer_Vision_Meets_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " A novel surface normal estimator is introduced using affine-invariant features extracted and tracked across multiple views. Normal estimation is robustified and integrated into our reconstruction pipeline that has increased accuracy compared to the State-of-the-Art. Parameters of the views and the obtained spatial model, including surface normals, are refined by a novel bundle adjustment-like numerical optimization. The process is an alternation with a novel robust view-dependent consistency check for surface normals, removing normals inconsistent with the multiple-view track. Our algorithms are quantitatively validated on the reverse engineering of geometrical elements such as planes, spheres, or cylinders. It is shown here that the accuracy of the estimated surface properties is appropriate for object detection. The pipeline is also tested on the reconstruction of man-made and free-form objects.\r"
  },
  "iccv2017_w35_cameraposefilteringwithlocalregressiongeodesicsontheriemannianmanifoldofdualquaternions": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "Camera Pose Filtering With Local Regression Geodesics on the Riemannian Manifold of Dual Quaternions",
    "authors": [
      "Benjamin Busam",
      "Tolga Birdal",
      "Nassir Navab"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Busam_Camera_Pose_Filtering_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Busam_Camera_Pose_Filtering_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Time-varying, smooth trajectory estimation is of great interest to the vision community for accurate 3D systems. In this paper, we propose a novel principal component local regression filter acting on the Riemannian manifold of unit dual quaternions DH_1 We use a numerically stable Lie algebra of the dual quaternions together with exp and log operators to locally linearize the 6D pose space. Unlike state of the art path smoothing methods which either operate on SE(3) of rotation matrices or the hypersphere H_1 of quaternions, we treat orientation and translation jointly on the dual quaternion quadric in 7-dimensional real projective space RP^7. We provide an outlier-robust IRLS algorithm for generic pose filtering exploiting this manifold structure. Besides our theoretical analysis, experiments on synthetic and real data show the practical advantages of the manifold aware filtering on pose tracking and smoothing.\r"
  },
  "iccv2017_w35_ause-casestudyonmulti-viewhypothesisfusionfor3dobjectclassification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "A Use-Case Study on Multi-View Hypothesis Fusion for 3D Object Classification",
    "authors": [
      "Panagiotis Papadakis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Papadakis_A_Use-Case_Study_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Papadakis_A_Use-Case_Study_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Object classification is a core element of various robot services ranging from environment mapping and object manipulation to human activity understanding. Due to limits in the robot configuration space or occlusions, a deeper understanding is needed on the potential of partial, multi-view based recognition. Towards this goal, we benchmark a number of schemes for hypothesis fusion under different environment assumptions and observation capacities, using a large-scale ground truth dataset and a baseline view-based recognition methodology. The obtained results highlight important aspects that should be taken into account when designing multi-view based recognition pipelines and converge to a hybrid scheme of enhanced performance as well as utility.\r"
  },
  "iccv2017_w35_accuratedepthmapestimationfromsmallmotions": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "Accurate Depth Map Estimation From Small Motions",
    "authors": [
      "Hossein Javidnia",
      "Peter Corcoran"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Javidnia_Accurate_Depth_Map_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Javidnia_Accurate_Depth_Map_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, a novel approach is proposed to compute a high quality dense depth map together with a semi-dense/dense 3D structure from a sequence of images captured on a narrow baseline. Computing the depth information from small motions has been a challenge for decades because of the uncertain calculation of depth values when using a small baseline - up to 12mm. The proposed method can, in fact, perform on a much wider range of baselines from 8 mm up to 400 mm while respecting the structure of the reference frame. The evaluation has been done on more than 10 sets of recorded small motion clips and for the wider baseline, on 7 sets of stereo images from Middlebury benchmark. Preliminary results indicate that the proposed method has a better performance in terms of structural accuracy in comparison with the current state of the art methods. Also, the performance of the proposed method remains stable even when only a low number of frames are available for processing.\r"
  },
  "iccv2017_w35_ontablet3dstructuredlightreconstructionandregistration": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "On Tablet 3D Structured Light Reconstruction and Registration",
    "authors": [
      "Matea Donlic",
      "Tomislav Petkovic",
      "Tomislav Pribanic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Donlic_On_Tablet_3D_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Donlic_On_Tablet_3D_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " One of the very first tablets with a built-in DLP projector has recently appeared on the market while smartphones with a built-in projector have been available around for quite a while. Interestingly, 3D reconstruction solutions on mobile devices never considered exploiting a built-in projector for the implementation of a powerful active stereo concept, structured light (SL), whose main component is a camera-projector pair. In this work we demonstrate a 3D reconstruction framework implementing SL on a tablet. In addition, we propose a 3D registration method by taking the advantage in a novel way of two commonly available sensors on mobile devices, an accelerometer and a magnetometer. The proposed solution provides robust and accurate 3D reconstruction and 3D registration results.\r"
  },
  "iccv2017_w35_multiviewabsoluteposeusing3d-2dperspectivelinecorrespondencesandverticaldirection": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "Multiview Absolute Pose Using 3D - 2D Perspective Line Correspondences and Vertical Direction",
    "authors": [
      "Nora Horanyi",
      "Zoltan Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Horanyi_Multiview_Absolute_Pose_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Horanyi_Multiview_Absolute_Pose_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we address the problem of estimating the absolute pose of a multiview calibrated perspective camera system from 3D - 2D line correspondences. We assume, that the vertical direction is known, which is often the case when the camera system is coupled with an IMU sensor, but it can also be obtained from vanishing points constructed in the images. Herein, we propose two solutions, both can be used as a minimal solver as well as a least squares solver without reformulation. The first solution consists of a single linear system of equations, while the second solution yields a polynomial equation of degree three in one variable and one systems of linear equations which can be efficiently solved in closed-form. The proposed algorithms have been evaluated on various synthetic datasets as well as on real data. Experimental results confirm state of the art performance both in terms of quality and computing time.\r"
  },
  "iccv2017_w35_combiningexemplar-basedapproachandlearning-basedapproachforlightfieldsuper-resolutionusingahybridimagingsystem": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "Combining Exemplar-Based Approach and Learning-Based Approach for Light Field Super-Resolution Using a Hybrid Imaging System",
    "authors": [
      "Haitian Zheng",
      "Minghao Guo",
      "Haoqian Wang",
      "Yebin Liu",
      "Lu Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Zheng_Combining_Exemplar-Based_Approach_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Zheng_Combining_Exemplar-Based_Approach_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a new method to super-resolve images captured by a hybrid light field system that consists of a standard light field camera and a high-resolution standard camera. The high-resolution image is taken as a reference to help with super-resolving the low-resolution light field images. Our method combines an exemplar-based algorithm with the state of-the-art single image super-resolution approach and draws on the strengths of both. Both quantitative and qualitative experiments show that our proposed method substantially outperforms existing methods on standard light field datasets in the challenging large parallax setting.\r"
  },
  "iccv2017_w35_acontent-awaremetricforstitchedpanoramicimagequalityassessment": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "A Content-Aware Metric for Stitched Panoramic Image Quality Assessment",
    "authors": [
      "Luyu Yang",
      "Zhigang Tan",
      "Zhe Huang",
      "Gene Cheung"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Yang_A_Content-Aware_Metric_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Yang_A_Content-Aware_Metric_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " One key enabling component of immersive VR visual experience is the construction of panoramic images--each stitched into one wide-angle image from multiple smaller viewpoints. To better evaluate and design stitching algorithms, a lightweight yet accurate metric is desirable. In this paper, we design a metric specifically for stitched images by fusing a perceptual geometric error metric and a local structure-guided metric into one. For the geometric error, we compute the local variance of optical flow field energy. For the structure-guided metric, we compute intensity and chrominance gradient. The two metrics are content-adaptively combined based on the amount of image structures. Extensive experiments are conducted on our stitched image quality assessment (SIQA) dataset with 408 groups of examples. Results show our proposed metric outperforms state-of-the-art metrics and the two parts of metric complement each other. Our SIQA dataset is made publicly available as part of the submission.\r"
  },
  "iccv2017_w35_kppfkeypoint-basedpoint-pair-featureforscalableautomaticglobalregistrationoflargergb-dscans": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Multiview Relationships in 3D Data",
    "title": "KPPF: Keypoint-Based Point-Pair-Feature for Scalable Automatic Global Registration of Large RGB-D Scans",
    "authors": [
      "Lucas Malleus",
      "Thomas Fisichella",
      "Diane Lingrand",
      "Frederic Precioso",
      "Nicolas Gros",
      "Yann Noutary",
      "Luc Robert",
      "Lirone Samoun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w35/html/Malleus_KPPF_Keypoint-Based_Point-Pair-Feature_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w35/Malleus_KPPF_Keypoint-Based_Point-Pair-Feature_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " One of the most important challenges in the field of 3D data processing is to be able to reconstruct a complete 3D scene with a high accuracy from several captures. In this article we propose an automatic scalable global registration method under the following constraints: markerless, very large scale data (several, potentially many millions of points per scans), little overlap between scans, for more than two or three dozens of scans, without a priori knowledge on the 6 degrees of freedom.We evaluate thoroughly our method on our own dataset of 33 real large scale scans of an indoor building. The data presents some pairs of scans with very little overlap, architectural challenges, several millions of points per scan. We will make this dataset public as part of a benchmark available for the community. We have thus evaluated the accuracy of our method, the scalability to the initial amount of points and the robustness to occlusions, little scan overlap and architectural challenges.\r"
  },
  "iccv2017_w36_the3dmenpofaciallandmarktrackingchallenge": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 300 3D Facial-Videos In-The-Wild Challenge",
    "title": "The 3D Menpo Facial Landmark Tracking Challenge",
    "authors": [
      "Stefanos Zafeiriou",
      "Grigorios G. Chrysos",
      "Anastasios Roussos",
      "Evangelos Ververas",
      "Jiankang Deng",
      "George Trigeorgis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w36/html/Zafeiriou_The_3D_Menpo_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w36/Zafeiriou_The_3D_Menpo_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recently, deformable face alignment is synonymous to the task of locating a set of 2D sparse landmarks in intensity images. Currently, discriminatively trained Deep Convolutional Neural Networks (DCNNs) are thestate-of-the-art in the task of face alignment. DCNNs exploit large amount of high quality annotations that emerged the last few years. Nevertheless, the provided 2D annotations rarely capture the 3D structure of the face (this is especially evident in the facial boundary). That is, the annotations neither provide an estimate of the depth nor correspond to the 2D projections of the 3D facial structure. This paper summarises our efforts to develop (a) a very large database suitable to be used to train 3D face alignment algorithms in images captured \"in-the-wild\" and (b) to train and evaluate new methods for 3D face landmark tracking. Finally, we report the results of the first challenge in 3D face tracking \"in-the-wild\". \r"
  },
  "iccv2017_w36_pix2facedirect3dfacemodelestimation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 300 3D Facial-Videos In-The-Wild Challenge",
    "title": "Pix2Face: Direct 3D Face Model Estimation",
    "authors": [
      "Daniel Crispell",
      "Maxim Bazik"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w36/html/Crispell_Pix2Face_Direct_3D_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w36/Crispell_Pix2Face_Direct_3D_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " An efficient, fully automatic method for 3D face shape and pose estimation in unconstrained 2D imagery is presented. The proposed method jointly estimates a dense set of 3D landmarks and facial geometry using a single pass of a modified version of the popular \"U-Net\" neural network architecture.Additionally, we propose a method for directly estimating a set of 3D Morphable Model (3DMM) parameters, using the estimated 3D landmarks and geometry as constraints in a simple linear system.Qualitative modeling results are presented, as well as quantitative evaluation of predicted 3D face landmarks in unconstrained video sequences. \r"
  },
  "iccv2017_w36_convolutionalexpertsconstrainedlocalmodelfor3dfaciallandmarkdetection": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 300 3D Facial-Videos In-The-Wild Challenge",
    "title": "Convolutional Experts Constrained Local Model for 3D Facial Landmark Detection",
    "authors": [
      "Amir Zadeh",
      "Yao Chong Lim",
      "Tadas Baltrusaitis",
      "Louis-Philippe Morency"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w36/html/Zadeh_Convolutional_Experts_Constrained_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w36/Zadeh_Convolutional_Experts_Constrained_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Constrained Local Models (CLMs) are a well-established family of methods for facial landmark detection. CE-CLM, the newest member of CLMs, brings CLMs back to state of the art performance. This is done through CE-CLMs ability to model the very complex individual landmark appearance that is affected by expression, illumination, facial hair, makeup, and accessories. A crucial component of CE-CLM is a novel local detector - Convolutional Experts Network (CEN) - that brings together the advantages of neural architectures and mixtures of experts in an end-to-end framework. In this paper we use CE-CLM to learn position of dense 84 landmark positions. To achieve best performance on the Menpo3D dense landmark detection challenge, we use two complementary networks alongside CE-CLM: a network that maps the output of CE-CLM to 84 landmarks called Adjustment Network, and a Deep Residual Network called Correction Networks that learns dataset specific corrections for CE-CLM.\r"
  },
  "iccv2017_w36_combininglocalandglobalfeaturesfor3dfacetracking": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - 300 3D Facial-Videos In-The-Wild Challenge",
    "title": "Combining Local and Global Features for 3D Face Tracking",
    "authors": [
      "Pengfei Xiong",
      "Guoqing Li",
      "Yuhang Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w36/html/Xiong_Combining_Local_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w36/Xiong_Combining_Local_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents our framework submitted to 1st 3D Face Tracking in-the-wild Competition. Different from 2d landmark tracking, 3d shapes are more fragile under face posture changes. In order to better capture the various shape and spatial relationships associated with the face, we propose a two stage shape regression method by combining the powerful local heatmap regression and global shape regression. Concretely, stacked hourglass network is adopted to generate a set of heatmaps for each 3d shape point by first. While these heatmaps are independent on each other, a hierarchical attention mechanism is applied from global to local heatmaps into the network, in order to model the correlations among neighboring regions. Extensive experiments on four challenging datasets, show that our proposed algorithm outperforms state-of-the-art baselines.\r"
  },
  "iccv2017_w37_learningtoidentifywhilefailingtodiscriminate": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "Learning to Identify While Failing to Discriminate",
    "authors": [
      "Jure Sokolic",
      "Qiang Qiu",
      "Miguel R. D. Rodrigues",
      "Guillermo Sapiro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Sokolic_Learning_to_Identify_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Sokolic_Learning_to_Identify_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Privacy and fairness are critical in computer vision applications, in particular when dealing with human identification. Achieving a universally secure, private, and fair systems is practically impossible as the exploitation of additional data can reveal private information in the original one. Faced with this challenge, we propose a new line of research, where the privacy is learned and used in a closed environment. The goal is to ensure that a given entity, trusted to infer certain information with our data, is blocked from inferring protected information from it. We design a system that learns to succeed on the positive task while simultaneously fail at the negative one, and illustrate this with challenging cases where the positive task (face verification) is harder than the negative one (gender classification). The framework opens the door to privacy and fairness in very important closed scenarios, ranging from private data accumulation companies to law-enforcement and hospitals.\r"
  },
  "iccv2017_w37_thedosanddontsforcnn-basedfaceverification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "The Do's and Don'ts for CNN-Based Face Verification",
    "authors": [
      "Ankan Bansal",
      "Carlos Castillo",
      "Rajeev Ranjan",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Bansal_The_Dos_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Bansal_The_Dos_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " While the research community appears to have developed a consensus on the methods of acquiring annotated data, design and training of CNNs, many questions still remain to be answered. In this paper, we explore the following questions that are critical to face recognition research: (i) Can we train on still images and expect the systems to work on videos? (ii) Are deeper datasets better than wider datasets? (iii) Does adding label noise lead to improvement in performance of deep networks? (iv) Is alignment needed for face recognition? We address these questions by training CNNs using CASIA-WebFace, UMDFaces, and a new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion of UMDFaces datasets. Our new data set, which will be made publicly available, has 22,075 videos and 3,735,476 human annotated frames extracted from them.\r"
  },
  "iccv2017_w37_uhdb31adatasetforbetterunderstandingfacerecognitionacrossposeandilluminationvariation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "UHDB31: A Dataset for Better Understanding Face Recognition Across Pose and Illumination Variation",
    "authors": [
      "Ha A. Le",
      "Ioannis A. Kakadiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Le_UHDB31_A_Dataset_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Le_UHDB31_A_Dataset_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The face recognition accuracy achieved on current benchmark datasets is saturated. Although multiple face datasets have been published recently, they only focus on the number of samples and lack diversity on facial appearance factors, such as pose and illumination. In addition, while 3D data have been demonstrated improved face recognition accuracy by a significant margin, only a few 3D face datasets provide high quality 2D and 3D data. In this paper, we introduce a new and challenging dataset, called \\dbname, which not only allows direct measurement of the influence of pose, illumination, and resolution on face recognition but also facilitates different experimental configurations with both 2D and 3D data. We conduct a series of experiments with various face recognition algorithms and point out how far they are from solving the face recognition problem under pose, illumination, and resolution variation. The dataset is publicly available and free for research use.\r"
  },
  "iccv2017_w37_intelligentsynthesisdrivenmodelcalibrationframeworkandfacerecognitionapplication": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "Intelligent Synthesis Driven Model Calibration: Framework and Face Recognition Application",
    "authors": [
      "Jordan Hashemi",
      "Qiang Qiu",
      "Guillermo Sapiro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Hashemi_Intelligent_Synthesis_Driven_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Hashemi_Intelligent_Synthesis_Driven_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Deep Neural Networks (DNNs) that achieve state-of-the-art results are still prone to suffer performance degradation when deployed in many real-world scenarios due to shifts between the training and deployment domains. Limited data from a given setting can be enriched through synthesis, then used to calibrate a pre-trained DNN to improve the performance in the setting. Most enrichment approaches try to generate as much data as possible; however, this `blind' approach is computationally expensive and can lead to generating redundant data. Contrary to this, we develop synthesis, here exemplified for faces, methods and propose information-driven approaches to exploit and optimally select face synthesis types both at training and testing. We show that our approaches, without re-designing a new DNN, lead to more efficient training and improved performance. We demonstrate the effectiveness of our approaches by calibrating a state-of-the-art DNN to two challenging face recognition datasets.\r"
  },
  "iccv2017_w37_fromgroupstoco-travelersetspairmatchingbasedpersonre-identificationframework": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "From Groups to Co-Traveler Sets: Pair Matching Based Person Re-Identification Framework",
    "authors": [
      "Min Cao",
      "Chen Chen",
      "Xiyuan Hu",
      "Silong Peng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Cao_From_Groups_to_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Cao_From_Groups_to_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In video surveillance, group refers to a set of people with similar velocity and close proximity. Group members can provide visual clues for person re-identification. In this paper, we discuss the essentials of group-based person re-identification and relax the group definition towards a concept of \"co-traveler set\", keeping constraints on velocity differences while loosening the distance constraint. Accordingly we propose a pair matching scheme to measure the distance between co-traveler sets, which tackles the problems caused by dynamic change of group across camera views. The final individual matching score is weighted by the obtained distance measurements between co-traveler sets. A proof of concept shows the rationality of introducing the concept of co-traveler relation into person reid. Experiments were conducted on four different datasets. Our co-traveler set based framework shows promising improvement compared with the group-based methods and the individual-based methods.\r"
  },
  "iccv2017_w37_view-invariantgaitrepresentationusingjointbayesianregularizednon-negativematrixfactorization": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "View-Invariant Gait Representation Using Joint Bayesian Regularized Non-Negative Matrix Factorization",
    "authors": [
      "Maryam Babaee",
      "Gerhard Rigoll"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Babaee_View-Invariant_Gait_Representation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Babaee_View-Invariant_Gait_Representation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Gait as a biometric feature has been investigated for human identification and biometric application. However, gait is highly dependent on the view angle. Therefore, the proposed gait features do not perform well when a person is changing his/her orientation towards camera. To tackle this problem, we propose a new method to learn lowdimensional view-invariant gait feature for person identification/verification. We model a gait observed by several different points of view as a Gaussian distribution and then utilize a function of Joint Bayesian as a regularizer coupled with the main objective function of non-negative matrix factorization to map gait features into a low-dimensional space. This process leads to an informative gait feature that can be used in a verification task. The performed experiments on a large gait dataset confirms the strength of the proposed method.\r"
  },
  "iccv2017_w37_personre-identificationbydeeplearningmulti-scalerepresentations": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "Person Re-Identification by Deep Learning Multi-Scale Representations",
    "authors": [
      "Yanbei Chen",
      "Xiatian Zhu",
      "Shaogang Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Chen_Person_Re-Identification_by_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Chen_Person_Re-Identification_by_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Existing person re-identification (re-id) methods depend mostly on single-scale appearance information. This not only ignores the potentially useful explicit information of other different scales, but also loses the chance of mining the implicit correlated complementary advantages across scales. In this work, we formulate a novel Deep Pyramid Feature Learning (DPFL) CNN architecture for multi-scale appearance feature fusion optimised simultaneously by concurrent per-scale re-id losses and interactive cross-scale consensus regularisation in a closed-loop design. Extensive comparative evaluations demonstrate the re-id advantages of the proposed DPFL model over a wide range of state-of-the-art re-id methods on three benchmarks Market-1501, CUHK03, and DukeMTMC-reID.\r"
  },
  "iccv2017_w37_unifiedframeworkforautomatedpersonre-identificationandcameranetworktopologyinferenceincameranetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W37",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Cross-Domain Human Identification",
    "title": "Unified Framework for Automated Person Re-Identification and Camera Network Topology Inference in Camera Networks",
    "authors": [
      "Yeong-Jun Cho",
      "Jae-Han Park",
      "Su-A Kim",
      "Kyuewang Lee",
      "Kuk-Jin Yoon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w37/html/Cho_Unified_Framework_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w37/Cho_Unified_Framework_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The person re-identification in large-scale multi-camera networks is a challenging task because of the spatio-temporal uncertainty and high complexity due to large numbers of cameras and people. To handle these difficulties, additional information such as camera network topology should be provided, which is also difficult to automatically estimate.In this paper, we propose a unified framework which jointly solves both person re-id and camera network topology inference problems with minimal prior knowledge about the environments. The proposed framework takes general multi-camera network environments into account. To effectively show the superiority of the proposed framework, we also provide a new person re-id dataset with full annotations, named SLP, captured in the synchronized multi-camera network. Experimental results show that the proposed methods are promising for both person re-id and camera topology inference tasks.\r"
  },
  "iccv2017_w38_curriculumlearningformulti-taskclassificationofvisualattributes": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Curriculum Learning for Multi-Task Classification of Visual Attributes",
    "authors": [
      "Nikolaos Sarafianos",
      "Theodore Giannakopoulos",
      "Christophoros Nikou",
      "Ioannis A. Kakadiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Sarafianos_Curriculum_Learning_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Sarafianos_Curriculum_Learning_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Visual attributes, from simple objects to soft-biometrics have proven to be a powerful representational approach for many applications such as image description and human identification. In this paper, we introduce a novel method to combine the advantages of both multi-task and curriculum learning in a visual attribute classification framework. Individual tasks are grouped based on their correlation so that two groups of strongly and weakly correlated tasks are formed. The two groups of tasks are learned in a curriculum learning setup by transferring the acquired knowledge from the strongly to the weakly correlated. The learning process within each group is performed in a multitask classification setup. The proposed method learns better and converges faster than learning all the tasks in a typical multi-task learning paradigm. We demonstrate the effectiveness of our approach on the publicly available, SoBiR, VIPeR and PETA datasets and report state-of-the-art results across the board. \r"
  },
  "iccv2017_w38_zero-shotlearningposedasamissingdataproblem": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Zero-Shot Learning Posed as a Missing Data Problem",
    "authors": [
      "Bo Zhao",
      "Botong Wu",
      "Tianfu Wu",
      "Yizhou Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Zhao_Zero-Shot_Learning_Posed_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Zhao_Zero-Shot_Learning_Posed_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents a method of zero-shot learning (ZSL) which poses ZSL as the missing data problem, rather than the missing label problem.Specifically, most existing ZSL methods focus on learning mapping functions from the image feature space to the label embedding space. Whereas, the proposed method explores a simple yet effective transductive framework in the reverse way -- our method estimates data distribution of unseen classes in the image feature spaceby transferring knowledge from the label embedding space. Following the transductive setting, we leverage unlabeled data to refine the initial estimation. In experiments, our method achieves the highest classification accuracies on two popular datasets, namely, 96.00% on AwA and 60.24% on CUB.\r"
  },
  "iccv2017_w38_deepmodalityinvariantadversarialnetworkforsharedrepresentationlearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Deep Modality Invariant Adversarial Network for Shared Representation Learning",
    "authors": [
      "Kuniaki Saito",
      "Yusuke Mukuta",
      "Yoshitaka Ushiku",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Saito_Deep_Modality_Invariant_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Saito_Deep_Modality_Invariant_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": "In this work, we propose a novel method to learn the mapping to the common space wherein different modalities have the same information for shared representation learning. Our goal is to correctly classify the unseen target modality with a classifier trained on source modality samples and their labels in common representations. We call these representations modality-invariant representations. Our proposed method has the major advantage of not needing any labels for the target samples in order to learn representations. For example, we obtain modality-invariant representations from pairs of images and texts. Then, we train the text classifier on the modality-invariant space. Although we do not give any explicit relationship between images and labels, we can expect that images can be classified correctly in that space. Our method draws upon the theory of domain adaptation and we propose to use adversarial training for our purpose. \r"
  },
  "iccv2017_w38_discrepancy-basednetworksforunsuperviseddomainadaptationacomparativestudy": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Discrepancy-Based Networks for Unsupervised Domain Adaptation: A Comparative Study",
    "authors": [
      "Gabriela Csurka",
      "Fabien Baradel",
      "Boris Chidlovskii",
      "Stephane Clinchant"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Domain Adaptation (DA) exploits labeled data and models from similar domains in order to alleviate theannotation burden when learning a model in a new domain. Our contribution to the field is three-fold. First, we propose a new dataset LandMarkDA, to study the adaptation between landmark place recognition models trained with different artistic image styles, such as photos, paintings and drawings.. Second, we propose an experimental study of recent shallow and deep adaptation networks, based on using Maximum Mean Discrepancy to bridge the domain gap. We study different design choices for these models by varying the network architectures and evaluate them on OFF31 and the new LandMarkDA collections. We show that shallow networks can still be competitive under an appropriate feature extraction. Finally, we also benchmark a new DA method that successfullycombines the artistic image style-transfer with deep discrepancy-based networks.\r"
  },
  "iccv2017_w38_adaptivesvm+learningwithprivilegedinformationfordomainadaptation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Adaptive SVM+: Learning With Privileged Information for Domain Adaptation",
    "authors": [
      "Nikolaos Sarafianos",
      "Michalis Vrigkas",
      "Ioannis A. Kakadiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Sarafianos_Adaptive_SVM_Learning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Sarafianos_Adaptive_SVM_Learning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Incorporating additional knowledge in the learning process can be beneficial for several computer vision tasks. Whether privileged information originates from a source domain that is adapted to a target domain, or as additional features available at training time only, utilizing such privileged information is of high importance as it improves the recognition performance and generalization. However, both primary and privileged information are rarely derived from the same distribution. In this paper, we present a novel learning paradigm that leverages privileged information in a domain adaptation setup. The proposed framework named Adaptive SVM+ combines the advantages of both the learning using privileged information paradigm and the domain adaptation framework, which are naturally embedded in the objective function of a regular SVM. We demonstrate the effectiveness of our approach on the Animals with Attributes and INTERACT datasets and report state-of-the-art results in both of them.\r"
  },
  "iccv2017_w38_deepdepthdomainadaptationacasestudy": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Deep Depth Domain Adaptation: A Case Study ",
    "authors": [
      "Novi Patricia",
      "Fabio M. Carlucci",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Patricia_Deep_Depth_Domain_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Patricia_Deep_Depth_Domain_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In the era of deep learning, many domain adaptation studies have been done on RGB images but not on depth. One of the reasons is that there are few databases available for researchers to explore domain shift on depth images. The contribution of this paper is to provide a benchmark to the community to study and evaluate deep domain adaptation methods on depth images, and compare the results with those obtained on the corresponding RGB data. We use two variants dataset that follow the settings from the first introduced RGB-D object dataset with 51 categories taken from multiple views. We also explore different colorization methods for depth images such as Colorjet and DE2CO. The experiments are conducted on several deep domain adaptation approaches on RGB and depth images. We understand that current deep DA methods can work well for RGB images but how to tackle the domain shift problem on depth images is still open questions.\r"
  },
  "iccv2017_w38_deepdomainadaptationbygeodesicdistanceminimization": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Deep Domain Adaptation by Geodesic Distance Minimization",
    "authors": [
      "Yifei Wang",
      "Wen Li",
      "Dengxin Dai",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Wang_Deep_Domain_Adaptation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Wang_Deep_Domain_Adaptation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a new approach called Deep LogCORAL for unsupervised visual domain adaptation. Our work builds on the recently proposed Deep CORAL method, which proposed to train a convolutional neural network and simultaneously minimize the Euclidean distance of convariance matrices between the source and target domains. We propose to use the Riemannian distance, approximated by Log-Euclidean distance, to replace the naive Euclidean distance in Deep CORAL. We also consider first-order information, and minimize the distance of mean vectors between two domains. We build an end-to-end model, in which we minimize both the classification loss, and the domain difference based on the first and second order information between two domains. Our experiments on the benchmark Office dataset demonstrate the improvements of our newly proposed Deep LogCORAL approach over the Deep CORAL method, as well as the further improvement when optimizing both orders of information.\r"
  },
  "iccv2017_w38_inferringhumanactivitiesusingrobustprivilegedprobabilisticlearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Inferring Human Activities Using Robust Privileged Probabilistic Learning",
    "authors": [
      "Michalis Vrigkas",
      "Evangelos Kazakos",
      "Christophoros Nikou",
      "Ioannis A. Kakadiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Vrigkas_Inferring_Human_Activities_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Vrigkas_Inferring_Human_Activities_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Classification models may often suffer from \"structure imbalance\" between training and testing data that may occur due to the deficient data collection process. This imbalance can be represented by the learning using privileged information (LUPI) paradigm. In this paper, we present a supervised probabilistic classification approach that integrates LUPI into a hidden conditional random field (HCRF) model. The proposed model is called LUPI-HCRF and is able to cope with additional information that is only available during training. Moreover, the proposed method employs Student's t-distribution to provide robustness to outliers by modeling the conditional distribution of the privileged information. Experimental results in three publicly available datasets demonstrate the effectiveness of the proposed approach and improve the state-of-the-art in the LUPI framework for recognizing human activities.\r"
  },
  "iccv2017_w38_generatingvisualrepresentationsforzero-shotclassification": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Generating Visual Representations for Zero-Shot Classification",
    "authors": [
      "Maxime Bucher",
      "Stephane Herbin",
      "Frederic Jurie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Bucher_Generating_Visual_Representations_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Bucher_Generating_Visual_Representations_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper addresses the task of learning an image clas- sifier when some categories are defined by semantic de- scriptions only (e.g. visual attributes) while the others are defined by exemplar images as well. This task is often re- ferred to as the Zero-Shot classification task (ZSC). Most of the previous methods rely on learning a common embed- ding space allowing to compare visual features of unknown categories with semantic descriptions. This paper argues that these approaches are limited as i) efficient discrimi- native classifiers can't be used ii) classification tasks with seen and unseen categories (Generalized Zero-Shot Clas- sification or GZSC) can't be addressed efficiently. In con- trast, this paper suggests to address ZSC and GZSC by i) learning a conditional generator using seen classes ii) gen- erate artificial training examples for the categories without exemplars. ZSC is then turned into a standard supervised learning problem. Experiments with 4 generative models ...\r"
  },
  "iccv2017_w38_exploitingconvolutionfilterpatternsfortransferlearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W38",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Transferring and Adapting Source Knowledge in Computer Vision",
    "title": "Exploiting Convolution Filter Patterns for Transfer Learning",
    "authors": [
      "Mehmet Aygun",
      "Yusuf Aytar",
      "Hazim Kemal Ekenel"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w38/html/Aygun_Exploiting_Convolution_Filter_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w38/Aygun_Exploiting_Convolution_Filter_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we introduce a new regularization tech- nique for transfer learning. The aim of the proposed ap- proach is to capture statistical relationships among convo- lution filters learned from a well-trained network and trans- fer this knowledge to another network. Since convolution filters of the prevalent deep Convolutional Neural Network (CNN) models share a number of similar patterns, in order to speed up the learning procedure, we capture such cor- relations by Gaussian Mixture Models (GMMs) and trans- fer them using a regularization term. The experimental results show that the feature representations have efficiently been learned and transferred through the proposed statistical regularization scheme. Moreover, our method is an architecture indepen- dent approach, which is applicable for a variety of CNN architectures.\r"
  },
  "iccv2017_w40_localdepthedgedetectioninhumansanddeepneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Local Depth Edge Detection in Humans and Deep Neural Networks",
    "authors": [
      "Krista A. Ehinger",
      "Wendy J. Adams",
      "Erich W. Graf",
      "James H. Elder"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Ehinger_Local_Depth_Edge_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Ehinger_Local_Depth_Edge_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Distinguishing edges caused by a change in depth from other types of edges is an important problem in early vision. We investigate the performance of humans and computer vision models on this task. We use spherical imagery with ground-truth LiDAR range data to build an objective ground-truth dataset for edge classification. We compare various computational models for classifying depth from non-depth edges in small images patches and achieve the best performance (86%) with a convolutional neural network. We investigate human performance on this task in a behavioral experiment and find that human performance is lower than the CNN. Although human and CNN depth responses are correlated, observers' responses are better predicted by other observers than by the CNN. The responses of CNNs and human observers also show a slightly different pattern of correlation with low-level edge cues, which suggests that CNNs and human observers may weight these features differently for classifying edges. \r"
  },
  "iccv2017_w40_canwespeedup3dscanning?acognitiveandgeometricanalysis": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Can We Speed up 3D Scanning? A Cognitive and Geometric Analysis",
    "authors": [
      "Karthikeyan Vaiapury",
      "Balamuralidhar Purushothaman",
      "Arpan Pal",
      "Swapna Agarwal",
      "Brojeshwar Bhowmick"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Vaiapury_Can_We_Speed_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Vaiapury_Can_We_Speed_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The paper propose a cognitive inspired change detection method for the detection and localization of shape variations on point clouds. A well defined pipeline is introduced by proposing a coarse to fine approach: i) shape segmentation, ii) fine segment registration using attention blocks. Shape segmentation is obtained using covariance based method and fine segment registration is carried out using gravitational registration algorithm. In particular the introduction of this partition-based approach using visual attention mechanism improves the speed of deformation detection and localization. Some results are shown on synthetic data of house and aircraft models. Experimental results shows that this simple yet effective approach designed with an eye to scalability can detect and localize the deformation in a faster manner. A real world car usecase is also presented with some preliminary promising results useful for auditing and insurance claim tasks.\r"
  },
  "iccv2017_w40_colorrepresentationincnnsparallelismswithbiologicalvision": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Color Representation in CNNs: Parallelisms With Biological Vision",
    "authors": [
      "Ivet Rafegas",
      "Maria Vanrell"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Rafegas_Color_Representation_in_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Rafegas_Color_Representation_in_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " CNNs trained for object recognition present representational capabilities approaching to primate visual systems. This provides a computational framework to explore how image features are efficiently represented. Here, we dissect a trained CNN to study how color is represented. We use amethodology used in physiology that is measuring index of selectivity of individual neurons to specific features. We use ImageNet Dataset images and synthetic versions of them to quantify color tuning properties of artificial neurons to provide a classification of the network population. We conclude three main levels of color representation showing parallelisms with biological visual systems: a decomposition in a circular hue space encoding single color regions with a wide hue sampling beyond the first layer (V2); opponent low-dimensional spaces in early stages (V1); a strong color-shape entanglement representing object-parts, object-shapes, or object-surrounds configurations in deeper layers (V4 or IT)\r"
  },
  "iccv2017_w40_whatarethevisualfeaturesunderlyinghumanversusmachinevision?": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "What Are the Visual Features Underlying Human Versus Machine Vision?",
    "authors": [
      "Drew Linsley",
      "Sven Eberhardt",
      "Tarun Sharma",
      "Pankaj Gupta",
      "Thomas Serre"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Linsley_What_Are_the_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Linsley_What_Are_the_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Although Deep Convolutional Networks (DCNs) are approaching the accuracy of human observers at object recognition, it is unknown whether they leverage similar visual representations to achieve this performance. To address this, we introduce Clicktionary, a web-based game for identifying visual features used by human observers during object recognition. Importance maps derived from the game are consistent across participants and uncorrelated with image saliency measures. These results suggest that Clicktionary identifies image regions that are meaningful and diagnostic for object recognition but different than those driving eye movements. Surprisingly, Clicktionary importance maps are only weakly correlated with relevance maps derived from DCNs trained for object recognition. Our study demonstrates that the narrowing gap between the object recognition accuracy of human observers and DCNs obscures distinct visual strategies used by each to achieve this performance.\r"
  },
  "iccv2017_w40_stnetselectivetuningofconvolutionalnetworksforobjectlocalization": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "STNet: Selective Tuning of Convolutional Networks for Object Localization",
    "authors": [
      "Mahdi Biparva",
      "John Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Biparva_STNet_Selective_Tuning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Biparva_STNet_Selective_Tuning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Visual attention modeling has recently gained momentum in developing visual hierarchies provided by Convolutional Neural Networks. Despite recent successes of feedforward processing on the abstraction of concepts form raw images, the inherent nature of feedback processing has remained computationally controversial. Inspired by the computational models of covert visual attention, we propose the Selective Tuning of Convolutional Networks (STNet). It is composed of both streams of Bottom-Up and Top-Down information processing to selectively tune the visual representation of convolutional networks. We experimentally evaluate the performance of STNet for the weakly-supervised localization task on the ImageNet benchmark dataset. We demonstrate that STNet not only successfully surpasses the state-of-the-art results but also generates attention-driven class hypothesis maps. \r"
  },
  "iccv2017_w40_spatialattentionimprovesobjectlocalizationabiologicallyplausibleneuro-computationalmodelforuseinvirtualreality": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Spatial Attention Improves Object Localization: A Biologically Plausible Neuro-Computational Model for Use in Virtual Reality",
    "authors": [
      "Amirhossein Jamalian",
      "Julia Bergelt",
      "Helge Ulo Dinkelbach",
      "Fred H. Hamker"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Jamalian_Spatial_Attention_Improves_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Jamalian_Spatial_Attention_Improves_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Visual attention is a smart mechanism performed by the brain to avoid unnecessary processing and to focus on the most relevant part of the visual scene. It can result in a remarkable reduction in the computational complexity of scene understanding. Two major kinds of top-down visual attention signals are spatial and feature-based attention. The former deals with the places in scene which are worth to attend, while the latter is more involved with the basic features of objects e.g. color, intensity, edges. In principle, there are two known sources of generating a spatial attention signal: Frontal Eye Field (FEF) in the prefrontal cortex and Lateral Intraparietal Cortex (LIP) in the parietal cortex. In this paper, first, a combined neuro-computational model of ventral and dorsal stream is introduced and then, it is shown in Virtual Reality (VR) that the spatial attention, provided by LIP, acts as a transsaccadic memory pointer which accelerates object localization.\r"
  },
  "iccv2017_w40_showandrecalllearningwhatmakesvideosmemorable": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Show and Recall: Learning What Makes Videos Memorable",
    "authors": [
      "Sumit Shekhar",
      "Dhruv Singal",
      "Harvineet Singh",
      "Manav Kedia",
      "Akhil Shetty"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Shekhar_Show_and_Recall_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Shekhar_Show_and_Recall_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " With the explosion of video content on the Internet, there is a need for research on methods for video analysis which take human cognition into account. One such cognitive measure is memorability, or the ability to recall visual content after watching it. Prior research has looked into image memorability and shown that it is intrinsic to visual content, but the problem of modeling video memorability has not been addressed sufficiently. In this work, we develop a prediction model for video memorability, including complexities of video content in it. Detailed feature analysis reveals that the proposed method correlates well with existing findings on memorability. We also describe a novel experiment of predicting video sub-shot memorability and show that our approach improves over current memorability methods in this task. Experiments on standard datasets demonstrate that the proposed metric can achieve results on par or better than the state-of-the art methods for video summarization.\r"
  },
  "iccv2017_w40_predictingthecategoryandattributesofvisualsearchtargetsusingdeepgazepooling": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Predicting the Category and Attributes of Visual Search Targets Using Deep Gaze Pooling",
    "authors": [
      "Hosnieh Sattar",
      "Andreas Bulling",
      "Mario Fritz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Sattar_Predicting_the_Category_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Sattar_Predicting_the_Category_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Predicting the target of visual search from human gaze data is a challenging problem. In contrast to previous work that focused on predicting specific instances of search targets, we propose the first approach to predict a target's category and attributes. However, state-of-the-art models for categorical recognition require large amounts of training data, which is prohibitive for gaze data. We thus propose a novelGaze Pooling Layerthat integrates gaze information and CNN-based features by an attention mechanism -- incorporating both spatial and temporal aspects of gaze behaviour. We show that our approach can leverage pre-trained CNN architectures, thus eliminating the need for expensive joint data collection of image and gaze data. We demonstrate the effectiveness of our method on a new 14 participant dataset, and indicate directions for future research in the gaze-based prediction of mental states. \r"
  },
  "iccv2017_w40_learningrgb-dsalientobjectdetectionusingbackgroundenclosure,depthcontrast,andtop-downfeatures": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Learning RGB-D Salient Object Detection Using Background Enclosure, Depth Contrast, and Top-Down Features",
    "authors": [
      "Riku Shigematsu",
      "David Feng",
      "Shaodi You",
      "Nick Barnes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Shigematsu_Learning_RGB-D_Salient_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Shigematsu_Learning_RGB-D_Salient_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In human visual saliency, top-down and bottom-up information are combined as a basis of visual attention. Recently, deep Convolutional Neural Networks (CNN) have demonstrated strong performance on RGB salient object detection, providing an effective mechanism for combining top-down semantic information with low level features. Although depth information has been shown to be important for human perception of salient objects, the use of top-down information and the exploration of CNNs for RGB-D salient object detection remains limited. Here we propose a novel deep CNN architecture for RGB-D salient object detection that utilizes both top-down and bottom-up cues. In order to produce such an architecture, we present novel depth features that capture the ideas of background enclosure, depth contrast and histogram distance in a manner that is suitable for a learned approach. We show improved results compared to state-of-the-art RGB-D salient object detection methods.\r"
  },
  "iccv2017_w40_theimportanceofphasetotexturesimilarity": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "The Importance of Phase to Texture Similarity",
    "authors": [
      "Xinghui Dong",
      "Ying Gao",
      "Junyu Dong",
      "Mike J. Chantler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Dong_The_Importance_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Dong_The_Importance_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Although the importance of the Fourier phase to image perception has been addressed, it is unknown this is the case for texture similarity or not. We first show that phase is more important to human perceptual texture similarity than magnitude. We further test the ability of 51 feature sets to use phase for texture similarity. Yet it is found that magnitude is more important to these feature sets than phase. Considering the inconsistency between the similarity data obtained using humans and those feature sets, we attribute this to the difference in the ability of humans and these feature sets to use phase. Thus, we enable the 51 feature sets to use phase by fusing the features extracted from the original and phase-only images. It is shown that the fused feature sets yield better results than those derived using the 51 feature sets. In particular, this finding can also be propagated to CNN features. These promising results should be due to the importance of phase to texture similarity.\r"
  },
  "iccv2017_w40_evaluationofdeeplearningonanabstractimageclassificationdataset": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Evaluation of Deep Learning on an Abstract Image Classification Dataset",
    "authors": [
      "Sebastian Stabinger",
      "Antonio Rodriguez-Sanchez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Stabinger_Evaluation_of_Deep_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Stabinger_Evaluation_of_Deep_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Convolutional Neural Networks have become state of the art methods for image classification over the last couple of years. By now they perform better than human subjects on many of the image classification datasets. Most of these datasets are based on the notion of concrete classes (i.e. images are classified by the type of object in the image). In this paper we present a novel image classification dataset, using abstract classes, which should be easy to solve for humans, but variations of it are challenging for CNNs. The classification performance of popular CNN architectures is evaluated on this dataset and variations of the dataset that might be interesting for further research are identified.\r"
  },
  "iccv2017_w40_exploringinter-observerdifferencesinfirst-personobjectviewsusingdeeplearningmodels": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Exploring Inter-Observer Differences in First-Person Object Views Using Deep Learning Models",
    "authors": [
      "Sven Bambach",
      "Zehua Zhang",
      "David J. Crandall",
      "Chen Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Bambach_Exploring_Inter-Observer_Differences_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Bambach_Exploring_Inter-Observer_Differences_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Recent advances in wearable camera technology have led many cognitive psychologists to study the development of the human visual system by recording the field of view of infants and toddlers. Meanwhile, the vast success of deep learning in computer vision is driving researchers in both disciplines to aim to benefit from each other's understanding. Towards this goal, we set out to explore how deep learning models could be used to gain developmentally relevant insight from such first-person data. We consider a dataset of first-person videos from different people freely interacting with a set of toy objects, and train different object-recognition models based on each subject's view. We observe large inter-observer differences and find that subjects who created more diverse images of an object result in models that learn more robust object representations.\r"
  },
  "iccv2017_w40_facialexpressionrecognitionusingvisualsaliencyanddeeplearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Facial Expression Recognition Using Visual Saliency and Deep Learning",
    "authors": [
      "Viraj Mavani",
      "Shanmuganathan Raman",
      "Krishna P. Miyapuram"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Mavani_Facial_Expression_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Mavani_Facial_Expression_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We have developed a convolutional neural network for the purpose of recognizing facial expressions in human beings. We have fine-tuned the existing convolutional neural network model trained on the visual recognition dataset used in the ILSVRC2012 to two widely used facial expression datasets - CFEE and RaFD, which when trained and tested independently yielded test accuracies of 74.79% and 95.71%, respectively. Generalization of results was evident by training on one dataset and testing on the other. Further, the image product of the cropped faces and their visual saliency maps were computed using Deep Multi-Layer Network for saliency prediction and were fed to the facial expression recognition CNN. In the most generalized experiment, we observed the top-1 accuracy in the test set to be 65.39%. General confusion trends between different facial expressions as exhibited by humans were also observed.\r"
  },
  "iccv2017_w40_deepgestaltreasoningmodelinterpretingelectrophysiologicalsignalsrelatedtocognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Deep Gestalt Reasoning Model: Interpreting Electrophysiological Signals Related to Cognition",
    "authors": [
      "Andras Lorincz",
      "Aron Fothi",
      "Bryar O. Rahman",
      "Viktor Varga"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Lorincz_Deep_Gestalt_Reasoning_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Lorincz_Deep_Gestalt_Reasoning_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We are to join deep input-output processing and Gestalt Laws driven cognition under deterministic world assumption. We consider every feedforward input-output system as a sensor: including units performing holistic recognition. A mathematical theorem is also a sensor: it senses the consequences upon receiving its conditions. Systems seeking consistencies between the outputs of sensor are cognitive units. Such units are involved in cognition. Sensor and cognitive units complement each other. We argue that the goal of learning is to turn components of the cognitive system into feedforward holistic units for gaining speed in cognition. We put forth a model for self-training of the holistic units.We connect our concepts to certain electrophysiological signals and cognitive phenomena, including evoked response potentials, working memory, and consciousness. We demonstrate the working of the two complementary systems on low level situation analysis in videos.\r"
  },
  "iccv2017_w40_cantheearlyhumanvisualsystemcompetewithdeepneuralnetworks?": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Can the Early Human Visual System Compete With Deep Neural Networks?",
    "authors": [
      "Samuel Dodge",
      "Lina Karam"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Dodge_Can_the_Early_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Dodge_Can_the_Early_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We study and compare the human visual system and state-of-the-art deep neural networks on classification of distorted images. Different from previous works, we limit the display time to 100ms to test only the early mechanisms of the human visual system, without allowing time for any eye movements or other higher level processes. Our findings show that the human visual system still outperforms modern deep neural networks under blurry and noisy images. These findings motivate future research into developing more robust deep networks.\r"
  },
  "iccv2017_w40_humandetectionandtrackingforvideosurveillanceacognitivescienceapproach": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Mutual Benefits of Cognitive and Computer Vision",
    "title": "Human Detection and Tracking for Video Surveillance: A Cognitive Science Approach",
    "authors": [
      "Vandit Gajjar",
      "Ayesha Gurnani",
      "Yash Khandhediya"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w40/html/Gajjar_Human_Detection_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w40/Gajjar_Human_Detection_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " With crimes on the rise all around the world, video surveillance is becoming more important day by day. Due to the lack of human resources to monitor this increasing number of cameras manually, new computer vision algorithms to perform lower and higher level tasks are being developed. We have developed a new method incorporating the most acclaimed Histograms of Oriented Gradients, the theory of Visual Saliency and the saliency prediction model Deep Multi-Level Network to detect human beings in video sequences. Furthermore, we implemented the k - Means algorithm to cluster the HOG feature vectors of the positively detected windows and determined the path followed by a person in the video. We achieved a detection precision of 83.11% and a recall of 41.27%. We obtained these results 76.866 times faster than classification on normal images.\r"
  },
  "iccv2017_w41_towardsautomatedrecognitionoffacialexpressionsinanimalmodels": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Towards Automated Recognition of Facial Expressions in Animal Models",
    "authors": [
      "Gaddi Blumrosen",
      "David Hawellek",
      "Bijan Pesaran"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Blumrosen_Towards_Automated_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Blumrosen_Towards_Automated_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Facial expressions play a significant role in the expression of emotional states, such as fear, surprise, and happiness in humans and other animals. The current systems for recognizing animal facial expression model in Non-human primates (NHPs) are currently limited to manual decoding of the facial muscles and observations, which is biased, time-consuming and requires a long training process and certification.The main objective of this work is to establish a computational framework for facial recognition systems for automatic recognition NHP facial expressions from standard video recordings with minimal assumptions. The suggested technology consists of: 1)a tailored facial image registration for NHPs; 2)a two-layers unsupervised clustering algorithm that forms an ordered dictionary of facial images for different facial segments; 3)extract dynamical temporal-spectral features; ,and recognize dynamic facial expressions. The feasibility of the methods was verified using video recordings of an NHP under various behavioral conditions, recognizing typical NHP facial expressions in the wild. The results were compared to three human experts, and show an agreement of more than 82%. This work is the first attempt for efficient automatic recognition of facial expressions in NHPs using minimal assumptions about the physiology of facial expressions.\r"
  },
  "iccv2017_w41_towardsautomatedvisualmonitoringofindividualgorillasinthewild": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Towards Automated Visual Monitoring of Individual Gorillas in the Wild",
    "authors": [
      "Clemens-Alexander Brust",
      "Tilo Burghardt",
      "Milou Groenenberg",
      "Christoph Kading",
      "Hjalmar S. Kuhl",
      "Marie L. Manguette",
      "Joachim Denzler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Brust_Towards_Automated_Visual_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Brust_Towards_Automated_Visual_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we report on the context and evaluation of a system for an automatic interpretation of sightings of individual western lowland gorillas (Gorilla gorilla gorilla) as captured in facial field photography in the wild. This effort aligns with a growing need for effective and integrated monitoring approaches for assessing the status of biodiversity at high spatio-temporal scales. Manual field photography and the utilisation of autonomous camera traps have already transformed the way ecological surveys are conducted. In principle, many environments can now be monitored continuously, and with a higher spatio-temporal resolution than ever before. Yet, the manual effort required to process photographic data to derive relevant information delimits any large scale application of this methodology.The described system applies existing computer vision techniques including deep convolutional neural networks to cover the tasks of detection and localisation, as well as individual identification of gorillas in a practically relevant setup. We evaluate the approach on a relatively large and challenging data corpus of 12,765 field images of 147 individual gorillas with image-level labels (i.e. missing bounding boxes) photographed at Mbeli Bai at the Nouabale-Ndoki National Park, Republic of Congo. Results indicate a facial detection rate of 90.8% AP and an individual identification accuracy for ranking within the Top 5 set of 80.3%. We conclude that, whilst keeping the human in the loop is critical, this result is practically relevant as it exemplifies model transferability and has the potential to assist manual identification efforts. We argue further that there is significant need towards integrating computer vision deeper into ecological sampling methodologies and field practice to move the discipline forward and open up new research horizons.\r"
  },
  "iccv2017_w41_integralcurvaturerepresentationandmatchingalgorithmsforidentificationofdolphinsandwhales": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Integral Curvature Representation and Matching Algorithms for Identification of Dolphins and Whales",
    "authors": [
      "Hendrik J. Weideman",
      "Zachary M. Jablons",
      "Jason Holmberg",
      "Kiirsten Flynn",
      "John Calambokidis",
      "Reny B. Tyson",
      "Jason B. Allen",
      "Randall S. Wells",
      "Krista Hupman",
      "Kim Urian",
      "Charles V. Stewart"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Weideman_Integral_Curvature_Representation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Weideman_Integral_Curvature_Representation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We address the problem of identifying individual cetaceans from images showing the trailing edge of their fins. Given the trailing edge from an unknown individual, we produce a ranking of known individuals from a database. The nicks and notches along the trailing edge define an indi- vidual's unique signature. We define a representation based on integral curvature that is robust to changes in viewpoint and pose, and captures the pattern of nicks and notches in a local neighborhood at multiple scales. We explore two ranking methods that use this representation. The first uses a dynamic programming time-warping algorithm to align two representations, and interprets the alignment cost as a measure of similarity. This algorithm also exploits learned spatial weights to downweight matches from regions of un- stable curvature. The second interprets the representation as a feature descriptor. Feature keypoints are defined at the local extrema of the representation. Descriptors for the set of known individuals are stored in a tree structure, which al- lows us to perform queries given the descriptors from an un- known trailing edge. We evaluate the top-k accuracy on two real-world datasets to demonstrate the effectiveness of the curvature representation, achieving top-1 accuracy scores of approximately 95% and 80% for bottlenose dolphins and humpback whales, respectively.\r"
  },
  "iccv2017_w41_visualtrackingofsmallanimalsinclutterednaturalenvironmentsusingafreelymovingcamera": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Visual Tracking of Small Animals in Cluttered Natural Environments Using a Freely Moving Camera",
    "authors": [
      "Benjamin Risse",
      "Michael Mangan",
      "Luca Del Pero",
      "Barbara Webb"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Risse_Visual_Tracking_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Risse_Visual_Tracking_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Image-based tracking of animals in their natural habitats can provide rich behavioural data, but is very challenging due to complex and dynamic background and target appearances. We present an effective method to recover the positions of terrestrial animals in cluttered environments from video sequences filmed using a freely moving monocular camera. The method uses residual motion cues to detect the targets and is thus robust to different lighting conditions and requires no a-priori appearance model of the animal or environment.The detection is globally optimised based on an inference problem formulation using factor graphs. This handles ambiguities such as occlusions and intersections and provides automatic initialisation. Furthermore, this formulation allows a seamless integration of occasional user input for the most difficult situations, so that the effect of a few manual position estimates are smoothly distributed over long sequences. Testing our system against a benchmark dataset featuring small targets in natural scenes, we obtain 96 accuracy for fully automated tracking. We also demonstrate reliable tracking in a new data set that includes different targets (insects, vertebrates or artificial objects) in a variety of environments (desert, jungle, meadows, urban) using different imaging devices (day / night vision cameras, smart phones) and modalities (stationary, hand-held, drone operated).We will publish our algorithm and our wildlife animal tracking ground truth database as open source resources.\r"
  },
  "iccv2017_w41_visuallocalisationandindividualidentificationofholsteinfriesiancattleviadeeplearning": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Visual Localisation and Individual Identification of Holstein Friesian Cattle via Deep Learning",
    "authors": [
      "William Andrew",
      "Colin Greatwood",
      "Tilo Burghardt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Andrew_Visual_Localisation_and_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Andrew_Visual_Localisation_and_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we demonstrate that computer vision pipelines utilising deep neural architectures are well-suited to perform automated Holstein Friesian cattle detection as well as individual identification in agriculturally relevant setups. To the best of our knowledge, this work is the first to apply deep learning to the task of automated visual bovine identification.We show that off-the-shelf networks can perform end-to-end identification of individuals in top-down still imagery acquired from fixed cameras. We then introduce a video processing pipeline composed of standard components to efficiently process dynamic herd footage filmed by Unmanned Aerial Vehicles (UAVs). We report on these setups, as well as the context, training and evaluation of their components. We publish alongside new datasets: FriesianCattle2017 of in-barn top-down imagery, and AerialCattle2017 of outdoor cattle footage filmed by a DJI Inspire MkI UAV. We show that Friesian cattle detection and localisation can be performed robustly with an accuracy of 99.3% on this data. We evaluate individual identification exploiting coat uniqueness on 940 RGB stills taken after milking in-barn (89 individuals, accuracy = 86.1%). We also evaluate identification via a video processing pipeline on 46,430 frames originating from 34 clips (approx. 20 s length each) of UAV footage taken during grazing (23 individuals, accuracy = 98.1%). These tests suggest that, particularly when videoing small herds in uncluttered environments, an application of marker-less Friesian cattle identification is not only feasible using standard deep learning components -- it appears robust enough to assist existing tagging methods.\r"
  },
  "iccv2017_w41_towardsautomaticwildanimaldetectioninlowqualitycamera-trapimagesusingtwo-channeledperceivingresidualpyramidnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Towards Automatic Wild Animal Detection in Low Quality Camera-Trap Images Using Two-Channeled Perceiving Residual Pyramid Networks",
    "authors": [
      "Chunbiao Zhu",
      "Thomas H. Li",
      "Ge Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Zhu_Towards_Automatic_Wild_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Zhu_Towards_Automatic_Wild_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Monitoring animals in the wild without disturbing them is possible using camera trapping framework, which is a technique to study wildlife using automatically triggered cameras and produces great volumes of data. However, camera trapping collects images often result in low image quality and includes a lot of false positives (images without animals), which must be detection before the post-processing step. This paper presents a two-channeled perceiving residual pyramid networks(TPRPN) for camera-trap images objection. Our TPRPN model attends to generating high-resolution and high-quality results. In order to provide enough local information, we extract depth cue from the original images and use two-channeled perceiving model as input to training our networks. Finally, the proposed three-layer residual blocks learn to merge all the information and generate full size detection results. Besides, we construct a new high-quality dataset with the help of Wildlife Thailand's Community and eMammal Organization. Experimental results on our dataset demonstrate that our method is superior to the existing object detection methods.\r"
  },
  "iccv2017_w41_deepcensusauv-basedscalloppopulationmonitoring": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Deep Census: AUV-Based Scallop Population Monitoring",
    "authors": [
      "Christopher Rasmussen",
      "Jiayi Zhao",
      "Danielle Ferraro",
      "Arthur Trembanis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Rasmussen_Deep_Census_AUV-Based_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Rasmussen_Deep_Census_AUV-Based_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We describe an integrated system for vision-based counting of wild scallops in order to measure population health, particularly pre- and post-dredging in fisheries areas.Sequential images collected by an autonomous underwater vehicle (AUV) are independently analyzed by a convolutional neural network based on the YOLOv2 architecture, which offers state-of-the-art object detection accuracy at real-time speeds.To augment the training dataset, a denoising auto-encoder network is used to automatically upgrade manually-annotated approximate object positions to full bounding boxes, increasing the detection network's performance.The system can act as a tool to improve or even replace an existing offline manual annotation workflow, and is fast enough to function \"in the loop\" for AUV control.\r"
  },
  "iccv2017_w41_coral-segmentationtrainingdenselabelingmodelswithsparsegroundtruth": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Coral-Segmentation: Training Dense Labeling Models With Sparse Ground Truth",
    "authors": [
      "Inigo Alonso",
      "Ana Cambra",
      "Adolfo Munoz",
      "Tali Treibitz",
      "Ana C. Murillo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Alonso_Coral-Segmentation_Training_Dense_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Alonso_Coral-Segmentation_Training_Dense_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Biological datasets, such as our case of study, coral segmentation, often present scarce and sparse annotated image labels. Transfer learning techniques allow us to adapt existing deep learning models to new domains, even with small amounts of training data. Therefore, one of the main challenges to train dense segmentation models is to obtain the required dense labeled training data. This work presents a novel pipeline to address this pitfall and demonstrates the advantages of applying it to coral imagery segmentation. We fine tune state-of-the-art encoder-decoder CNN models for semantic segmentation thanks to a new proposed augmented labeling strategy. Our experiments run on a recent coral dataset, proving that this augmented ground truth allows us to effectively learn coral segmentation, as well as provide a relevant score of the segmentation quality based on it.Our approach provides a segmentation of comparable or better quality than the baseline presented with the dataset and a more flexible end-to-end pipeline.\r"
  },
  "iccv2017_w41_acomputervisionframeworkfordetectingandpreventinghuman-elephantcollisions": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "A Computer Vision Framework for Detecting and Preventing Human-Elephant Collisions",
    "authors": [
      "Pushkar Shukla",
      "Isha Dua",
      "Balasubramanian Raman",
      "Ankush Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Shukla_A_Computer_Vision_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Shukla_A_Computer_Vision_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Human Elephant Collision (HEC) is a problem that is quite common across many parts of the world. There have been many incidents in the past where conflict between humans and elephants has caused serious damage and resulted in the loss of lives as well as property. The paper proposes a frame-work that relies on computer vision approaches for detecting and preventing HEC. The technique initially recognizes the areas of conflict where accidents are most likely to occur. This is followed by elephant detection system that identifies an elephant in the video frame. Two different algorithms to detect the presence of elephants having a mean average precision of 98.621% and 97.667% have been proposed in the paper. The position of the elephant once detected is tracked with respect to the area of conflict with a particle filter.A warning message is displayed as soon as the position of the elephant overlaps with the area of conflict. The results of the techniques that were applied on videos were discussed in the paper.\r"
  },
  "iccv2017_w41_activelearningfortheclassificationofspeciesinunderwaterimagesfromafixedobservatory": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Visual Wildlife Monitoring",
    "title": "Active Learning for the Classification of Species in Underwater Images From a Fixed Observatory",
    "authors": [
      "Torben Moller",
      "Ingunn Nilssen",
      "Tim W. Nattkemper"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w41/html/Moller_Active_Learning_for_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w41/Moller_Active_Learning_for_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Vision based wildlife monitoring is an important task in the field of environmental monitoring. Wildlife monitoring activities often create large collections of data needing computational approaches to (semi-) automated detection and annotation of objects in the images/video. In this work, we consider the special case of marine wildlife monitoring using camera equipped fixed observatories. In such cases where a-priori knowledge about which species to find is limited, a standard computer vision approach, employing supervised learning, will not be applicable for detecting and classifying species (or events) in the images.In a recently proposed unsupervised learning method, image patches are extracted from a time series of underwater images that feature moving species (like starfish, etc). The patches are automatically grouped into clusters with similar morphology and a so called relevance score is assigned to each of the clusters describing the likeliness that it contains patches showing unusual changes. However, due to the unsupervised fashion (i) the categories don't have labels and (ii) do not reflect the species distribution satisfactory.In this paper, we propose an active learning method that builds upon these results and can be used to assign taxonomic categories to single patches based on a set of human expert annotations making use of the cluster structure and relevance scores. The evaluation shows that compared to traditional sampling strategies our approach uses significantly less manual labels to train a classifier. We are confident that the results are relevant for non-marine contexts as well.\r"
  },
  "iccv2017_w42_ancientromancoinrecognitioninthewildusingdeeplearningbasedrecognitionofartisticallydepictedfaceprofiles": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - E-Heritage",
    "title": "Ancient Roman Coin Recognition in the Wild Using Deep Learning Based Recognition of Artistically Depicted Face Profiles",
    "authors": [
      "Imanol Schlag",
      "Ognjen Arandjelovic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w42/html/Schlag_Ancient_Roman_Coin_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w42/Schlag_Ancient_Roman_Coin_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " As an interesting application in the realm of cultural heritage, the challenging problem of computer based analysis of Roman coins is attracting an increasing amount of research. Herein we make several important contributions. Firstly, we address a key limitation of existing work characterized by the application of generic recognition techniques and the lack of use of domain knowledge. Our work approaches coin recognition in much the same way as a human expert would: by identifying the emperor on the obverse. To this end we develop a deep convolutional network, crafted for the specific instance of profile face recognition. No less importantly, we also address a major methodological flaw of previous experiments which are insufficiently systematic and mired with confounding factors. We introduce three carefully collected and annotated data sets, and demonstrate the effectiveness of the proposed approach which exceeds the performance of the state of the art by an order of magnitude.\r"
  },
  "iccv2017_w42_alearnedrepresentationofartist-specificcolourisation": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - E-Heritage",
    "title": "A Learned Representation of Artist-Specific Colourisation",
    "authors": [
      "Nanne van Noord",
      "Eric Postma"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w42/html/van_Noord_A_Learned_Representation_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w42/van_Noord_A_Learned_Representation_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The colours used in a painting are determined by artists and the pigments attheir disposal. Therefore, knowing who made the painting should help indetermining which colours to hallucinate when given a colourless version of thepainting.The main aim of this paper is to determine if we can create a colourisation model for paintings which generates artist-specificcolourisations.Building on earlier work on natural-image colourisation, we propose a model capable of producing colourisations ofpaintings by incorporating a conditional normalisation scheme, i.e., conditional instance normalisation. The results indicate that a conditional normalisation scheme is beneficial to the performance. We conclude that painting colourisation is feasible and benefits from being trained on a dataset of paintings and from applying a conditional normalisation scheme. \r"
  },
  "iccv2017_w42_learningtodetectfine-grainedchangeundervariantimagingconditions": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - E-Heritage",
    "title": "Learning to Detect Fine-Grained Change Under Variant Imaging Conditions",
    "authors": [
      "Rui Huang",
      "Wei Feng",
      "Zezheng Wang",
      "Mingyuan Fan",
      "Liang Wan",
      "Jizhou Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w42/html/Huang_Learning_to_Detect_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w42/Huang_Learning_to_Detect_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Fine-grained change detection under variant imaging conditions is an important and challenging task for high-value scene monitoring in culture heritage. In this paper, we show that after a simple coarse alignment of lighting and camera differences, fine-grained change detection can be reliably solved by a deep network model, which is specifically composed of three functional parts, i.e., camera pose correction network (PCN), fine-grained change detection network (FCDN), and detection confidence boosting. Since our model is properly pre-trained and fine-tuned on both general and specialized data, it exhibits very good generalization capability to produce high-quality minute change detection on real-world scenes under varied imaging conditions. Extensive experiments validate the superior effectiveness and reliability over state-of-the-art methods. We have achieved 67.41% relative F1-measure improvement over the best competitor on real-world benchmark dataset.\r"
  },
  "iccv2017_w42_analysisofpartialaxialsymmetryon3dsurfacesanditsapplicationintherestorationofculturalheritageobjects": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - E-Heritage",
    "title": "Analysis of Partial Axial Symmetry on 3D Surfaces and Its Application in the Restoration of Cultural Heritage Objects",
    "authors": [
      "Ivan Sipiran"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w42/html/Sipiran_Analysis_of_Partial_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w42/Sipiran_Analysis_of_Partial_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Symmetry is a ubiquitous concept that can help to understand the structure of real objects. One of the main challenging problems in the analysis of symmetry is the robustness against high partiality, i.e., when the support of the symmetry in the input geometry is small. In this paper, we address the problem of finding the partial axial symmetry of 3D objects through the analysis of surface descriptors with invariance to partiality. These descriptors are used to reduce the search space of axial symmetric correspondences, and allows us to design an effective and efficient algorithm to detect the generator axis of the symmetry. Our algorithm collects enough evidence of the presence of the axial symmetry in a consensus-based approach. Our algorithm can also identify the support of the axial symmetry. Our experiments show the robustness of our method in challenging scenarios. We show that our method is good to generate plausible restorations of damaged cultural heritage objects.\r"
  },
  "iccv2017_w42_geometrybasedfacetingof3ddigitizedarchaeologicalfragments": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - E-Heritage",
    "title": "Geometry Based Faceting of 3D Digitized Archaeological Fragments",
    "authors": [
      "Hanan ElNaghy",
      "Leo Dorst"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w42/html/ElNaghy_Geometry_Based_Faceting_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w42/ElNaghy_Geometry_Based_Faceting_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We present a robust pipeline for segmenting digital cultural heritage fragments into distinct facets, with few tunable yet archaeologically meaningful parameters. Given a terracotta broken artifact, digitally scanned in the form of irregularly sampled 3D mesh, our method first estimates the local angles of fractures by applying weighted eigenanalysis of the local neighborhoods. Using 3D fit of a quadratic polynomial, we estimate the directional derivative of the angle function along the maximum bending direction for accurate localization of the fracture lines across the mesh. Then, the salient fracture lines are detected and incidental possible gaps between them are closed in order to extract a set of closed facets. Finally, the facets are categorized into fracture and skin. The method is tested on two different datasets of the GRAVITATE project.\r"
  },
  "iccv2017_w42_aninteractivetourguideforaheritagesite": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - E-Heritage",
    "title": "An Interactive Tour Guide for a Heritage Site",
    "authors": [
      "Sahil Chelaramani",
      "Vamsidhar Muthireddy",
      "C.V. Jawahar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w42/html/Chelaramani_An_Interactive_Tour_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w42/Chelaramani_An_Interactive_Tour_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Imagine taking a guided tour of a heritage site. Generally, tour guides have canned routes and stories about the monuments. As humans, we can inform the guide about topics which we are interested in, so as to ensure that we are presented stories which match our interests. Most digital storytelling approaches fail to take into account this aspect of a storyteller. In this work, we take on the task of interactive story generation, for a casually captured video-clip of a heritage site tour. We leverage user interaction to improve the relevance of the stories presented to the user. The stories generated vary from user to user, with the stories progressively becoming more aligned with the captured interests, as the number of interactions increase. We condition the stories on visual features from the video, along with the interests of the user. We additionally present a mechanism to generate questions to be posed to a user to gain additional insights into their interests. \r"
  },
  "iccv2017_w43_litasystemandbenchmarkforlightunderstanding": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "LIT: A System and Benchmark for Light Understanding",
    "authors": [
      "Theodore Tsesmelis",
      "Irtiza Hasan",
      "Marco Cristani",
      "Alessio Del Bue",
      "Fabio Galasso"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Tsesmelis_LIT_A_System_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Tsesmelis_LIT_A_System_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " A modern lighting system should automatically calibrate itself(lightcommissioning),assess its own status (which lights are on/off and how dimmed), and allow for the creation or preservation of lighting patterns (adjustability), e.g. after the sunset. Such a system does not exist today, nor (real) data, labels, or metrics are available to compare with and foster progress. In this paper we set the baselines to such a computational system, called LIT, and its applications. Using computational imaging we try to model and benchmark the light variations of indoor scenes with different illuminations (including natural light) and luminaire setups. We show that our lighting system can be easily trained with no manual intervention; after that, the benchmark allows to test automatic calibration (LIT-EST), status awareness (LIT-ID) and relighting (RE-LIT) as application.\r"
  },
  "iccv2017_w43_depthsuper-resolutionmeetsuncalibratedphotometricstereo": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Depth Super-Resolution Meets Uncalibrated Photometric Stereo",
    "authors": [
      "Songyou Peng",
      "Bjoern Haefner",
      "Yvain Queau",
      "Daniel Cremers"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Peng_Depth_Super-Resolution_Meets_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Peng_Depth_Super-Resolution_Meets_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " A novel depth super-resolution approach for RGB-D sensors is presented. It disambiguates depth super-resolution through high-resolution photometric clues and, symmetrically, it disambiguates uncalibrated photometric stereo through low-resolution depth cues. To this end, an RGB-D sequence is acquired from the same viewing angle, while illuminating the scene from various uncalibrated directions. This sequence is handled by a variational framework which fits high-resolution shape and reflectance, as well as lighting, to both the low-resolution depth measurements and the high-resolution RGB ones. The key novelty consists in a new PDE-based photometric stereo regularizer which implicitly ensures surface regularity. This allows to carry out depth super-resolution in a purely data-driven manner, without the need for any ad-hoc prior or material calibration. Real-world experiments are carried out using an out-of-the-box RGB-D sensor and a hand-held LED light source. \r"
  },
  "iccv2017_w43_shape-from-polarisationanonlinearleastsquaresapproach": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Shape-From-Polarisation: A Nonlinear Least Squares Approach",
    "authors": [
      "Ye Yu",
      "Dizhong Zhu",
      "William A. P. Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Yu_Shape-From-Polarisation_A_Nonlinear_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Yu_Shape-From-Polarisation_A_Nonlinear_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper we present a new type of approach for estimating surface height from polarimetric data. In contrast to all previous shape-from-polarisation methods, we do not first transform the observed data into a polarisation image. Instead, we minimise the sum of squared residuals between predicted and observed intensities over all pixels and polariser angles. This is a nonlinear least squares optimisation problem in which the unknown is the surface height. The forward prediction is a series of transformations for which we provide analytical derivatives allowing the overall problem to be efficiently optimised using Gauss-Newton type methods with an analytical Jacobian matrix. We also propose a variant of the method which uses image ratios to remove dependence on illumination and albedo. We demonstrate our methods on glossy objects, including with albedo variations, and provide a comparison to a state of the art approach.\r"
  },
  "iccv2017_w43_colorconsistencycorrectionbasedonremappingoptimizationforimagestitching": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Color Consistency Correction Based on Remapping Optimization for Image Stitching",
    "authors": [
      "Menghan Xia",
      "Jian Yao",
      "Renping Xie",
      "Mi Zhang",
      "Jinsheng Xiao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Xia_Color_Consistency_Correction_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Xia_Color_Consistency_Correction_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose an effective color correction method which is feasible to optimize the color consistency across images and guarantee the imaging quality of individual image meanwhile. Our method first apply well-directed alteration detection algorithms to find coherent-content regions in inter-image overlaps where reliable color correspondences are extracted.Then, we parameterize the color remapping curve as transform model, and express the constraints of color consistency, contrast and gradient in an uniform energy function. It can be formulated as a convex quadratic programming problem which provides the global optimal solution efficiently. Our method has a good performance in color consistency and suffers no pixel saturation or tonal dimming. Experimental results of representative datasets demonstrate the superiority of our method over state-of-the-art algorithms.\r"
  },
  "iccv2017_w43_theimportanceofsmoothnessconstraintsonspectralobjectreflectanceswhenmodelingmetamermismatching": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "The Importance of Smoothness Constraints on Spectral Object Reflectances When Modeling Metamer Mismatching",
    "authors": [
      "Tarek Stiebel",
      "Dorit Merhof"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Stiebel_The_Importance_of_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Stiebel_The_Importance_of_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper analyzes the influence of multi-spectral imag- ing onto the severity of metamer mismatching, in partic- ular in the context of color-accuracy of machine vision. Camera signals associated with simulated as well as real world multi-spectral imaging systems when viewing differ- ent objects under different lighting conditions were calcu- lated. Based on the calculated camera signals, the associ- ated MMBs were computed when changing towards the CIE standard observer under illuminant D65. The results show that an increased number of channels used in multi-spectral imaging systems do not necessarily de- crease the severity of metamer mismatching. However, it is also shown that this is due to the limited capabilities of current image acquisition models which are not able to cor- rectly compute a realistic MMB as they neglect any smooth- ness constraints on spectral object reflectances.\r"
  },
  "iccv2017_w43_deepgenerativefilterformotiondeblurring": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Deep Generative Filter for Motion Deblurring",
    "authors": [
      "Sainandan Ramakrishnan",
      "Shubham Pachori",
      "Aalok Gangopadhyay",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Ramakrishnan_Deep_Generative_Filter_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Ramakrishnan_Deep_Generative_Filter_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Removing blur caused by camera shake in images has always been a challenging problem in computer vision literature due to its ill-posed nature. Motion blur caused due to the relative motion between the camera and the object in 3D space induces a spatially varying blurring effect over the entire image. In this paper, we propose a novel deep filter based on Generative Adversarial Network (GAN) architecture integrated with global skip connection and dense architecture in order to tackle this problem. Our model, while bypassing the process of blur kernel estimation, significantly reduces the test time which is necessary for practical applications. The experiments on the benchmark datasets prove the effectiveness of the proposed method which outperforms the state-of-the-art blind deblurring algorithms both quantitatively and qualitatively.\r"
  },
  "iccv2017_w43_lineardatacompressionofhyperspectralimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Linear Data Compression of Hyperspectral Images",
    "authors": [
      "Kaori Tanji",
      "Hayato Itoh",
      "Atsushi Imiya",
      "Naohiro Manago",
      "Hiroaki Kuze"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Tanji_Linear_Data_Compression_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Tanji_Linear_Data_Compression_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The aim of the paper is to analyse hyperspectral imagesusing tensor principal component analysis of multi-way data sets.Because of high-resolution sampling in the colour channels,images observed bya hyperspectral camera system are expressed bythree-mode tensors. The Tucker-3 decomposition of a three-mode tensor is usedin behaviourmetry and psychology for the extraction of relationsamong three entries as an extension of the usual principal component analysis for statistical analysis.Hyperspectral images express spectral information of two-dimensionalimages on the imaging plane.Therefore, for statistical analysis, we adopt the Tucker-3 decomposition.The Tucker-3 decomposition of hyperspectral images extracts statistically dominant information from hyperspectral images. Tensor principal component analysis allows us to extract dominantlight-channel information from hyperspectral images. \r"
  },
  "iccv2017_w43_athree-pathwaypsychobiologicalframeworkofsalientobjectdetectionusingstereoscopictechnology": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "A Three-Pathway Psychobiological Framework of Salient Object Detection Using Stereoscopic Technology",
    "authors": [
      "Chunbiao Zhu",
      "Ge Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Zhu_A_Three-Pathway_Psychobiological_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Zhu_A_Three-Pathway_Psychobiological_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Saliency detection, finding the most important parts of an image, has become increasingly popular in computer vision. Existing proposal methods are mostly based on color information, which may not be effective for cluttered backgrounds. We propose a new algorithm leveraging stereopsis to generate optical flow which can obtain addition cue (depth cue) to get the final saliency map. The proposed framework consists of three pathways. The first pathway eliminates the background based on cellular automata. The second pathway gets the optical flow and color flow saliency map. The third pathway calculates a coarse saliency map. Finally, we fuse these three pathways to generate the final saliency map. Besides, we construct a new high-quality dataset with the complex scene to make computer challenge human vision. Experimental results on our dataset and another three popular datasets demonstrate that our method is superior to the existing methods in terms of robustness.\r"
  },
  "iccv2017_w43_anewlow-lightimageenhancementalgorithmusingcameraresponsemodel": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "A New Low-Light Image Enhancement Algorithm Using Camera Response Model",
    "authors": [
      "Zhenqiang Ying",
      "Ge Li",
      "Yurui Ren",
      "Ronggang Wang",
      "Wenmin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Ying_A_New_Low-Light_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Ying_A_New_Low-Light_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. To solve this problem, many image enhancement techniques have been proposed. However, existing techniques inevitably introduce color and lightness distortion when increasing visibility. To lower the distortion, we propose a novel enhancement method using the response characteristics of cameras. First, we investigate the relationship between two images with different exposures to obtain an accurate camera response model. Then we borrow the illumination estimation techniques to estimate the exposure ratio map. Finally, we use our camera response model to adjust each pixel to its desired exposure according to the estimated exposure ratio map. Experiments show that our method can obtain enhancement results with less color and lightness distortion compared to several state-of-the-art methods.\r"
  },
  "iccv2017_w43_globalandlocalcontrastadaptiveenhancementfornon-uniformilluminationcolorimages": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Global and Local Contrast Adaptive Enhancement for Non-Uniform Illumination Color Images",
    "authors": [
      "Qi-Chong Tian",
      "Laurent D. Cohen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Tian_Global_and_Local_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Tian_Global_and_Local_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Color images captured by digital devices may contain some non-uniform illuminations. Many enhancement methods produce undesirable results in the aspect of contrast improvement or naturalness preservation. A global and local contrast enhancement method is proposed for adaptively enhancing the non-uniform illumination images. Firstly, a novel global contrast adaptive enhancement algorithm obtains the global enhancement image. Secondly, a hue-preserving local contrast adaptive enhancement algorithm produces the local enhancement image. Finally, a contrast-brightness-based fusion algorithm obtains the final result, which represents a trade-off between global contrast and local contrast. This method improves the visual quality and preserves the image naturalness. Experiments are conducted on a dataset including different kinds of non-uniform illumination images. Results demonstrate the proposed method outperforms the compared enhancement algorithms both qualitatively and quantitatively.\r"
  },
  "iccv2017_w43_image-basedrelightingwith5-dincidentlightfields": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Image-Based Relighting With 5-D Incident Light Fields",
    "authors": [
      "Shinnosuke Oya",
      "Takahiro Okabe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/Oya_Image-Based_Relighting_With_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/Oya_Image-Based_Relighting_With_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a method for image-based relighting with 5-D incident light fields: 4 DoF of the position and direction and 1 DoF of the color of an incident ray. Specifically, we illuminate a scene with various rays by using a two-layer 5 DoF lighting system consisting of a rear-projection display and a transmissive LC panel, and synthesize images under desired 5-D incident light fields by combining the images captured under those rays. Our proposed method efficiently acquires the required images by using coded illumination; it reduces the number of captured images and the measurement time, and enhances their SNRs. In addition, we propose a method for removing the effects of the black offsets due to the projector and the LC panel in the two-layer setup. The experimental results using the prototype system show that our method enables us to synthesize photo-realistic images of scenes where wavelength-dependent phenomena such as fluorescence are observed.\r"
  },
  "iccv2017_w43_colorimageprocessingusingreducedbiquaternionswithapplicationtofacerecognitioninapcaframework": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W43",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Color and Photometry in Computer Vision",
    "title": "Color Image Processing Using Reduced Biquaternions With Application to Face Recognition in a PCA Framework",
    "authors": [
      "Moumen T. El-Melegy",
      "Aliaa T. Kamal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w43/html/El-Melegy_Color_Image_Processing_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w43/El-Melegy_Color_Image_Processing_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we present the theory of reduced biquaternion algebra to represent color images and to develop efficient vector processing methods. We apply this theory to the field of face recognition in a principal component analysis (PCA) framework. We develop a novel PCA method based on reduced biquaternion to make full use of the face color cues. Moreover, we derive new mathematical results on the computation of the eigenvalues/eigenvectors of the data scatter matrix. We also extend this method to two-dimensional color PCA to combine the face spatial and color information. Experiments on several public-domain color face benchmark datasets demonstrate the higher performance of the proposed methods compared to regular PCA and like methods. \r"
  },
  "iccv2017_w44_multimodalgesturerecognitionbasedontheresc3dnetwork": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Multimodal Gesture Recognition Based on the ResC3D Network",
    "authors": [
      "Qiguang Miao",
      "Yunan Li",
      "Wanli Ouyang",
      "Zhenxin Ma",
      "Xin Xu",
      "Weikang Shi",
      "Xiaochun Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Miao_Multimodal_Gesture_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Miao_Multimodal_Gesture_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Gesture recognition is an important issue in computer vision. Recognizing gestures with videos remains a challenging task due to the barriers of gesture-irrelevant factors. In this paper, we propose a multimodal gesture recognition method based on a ResC3D network. One key idea is to find a compact and effective representation of video sequences. Therefore, the video enhancement techniques, such as Retinex and median filter are applied to eliminate the illumination variation and noise in the input video, and a weighted frame unification strategy is utilized to sample key frames. Upon these representations, a ResC3D network, which leverages the advantages of both residual and C3D model, is developed to extract features, together with a canonical correlation analysis based fusion scheme for blending features. The performance of our method is evaluated in the Chalearn LAP isolated gesture recognition challenge. It reaches 67.71% accuracy and ranks the 1st place in this challenge.\r"
  },
  "iccv2017_w44_continuousgesturerecognitionwithhand-orientedspatiotemporalfeature": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Continuous Gesture Recognition With Hand-Oriented Spatiotemporal Feature",
    "authors": [
      "Zhipeng Liu",
      "Xiujuan Chai",
      "Zhuang Liu",
      "Xilin Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Liu_Continuous_Gesture_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Liu_Continuous_Gesture_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, an efficient spotting-recognition framework is proposed to tackle the large scale continuous gesture recognition problem with the RGB-D data input. Concretely, continuous gestures are firstly segmented into isolated gestures based on the hand positions obtained by our proposed two streams Faster R-CNN. In the subsequent recognition stage, firstly, a specific hand-oriented spatiotemporal feature is extracted by 3D convolutional network. In this feature, only the hand regions and face location are considered, which can effectively block the negative influence of the distractors. Next, the extracted features from RGB and depth are fused to boost the representative power and the classification is achieved by using the linear SVM. Extensive experiments are conducted to validate the effectiveness of the proposed method. Our method achieves the mean Jaccard Index of 0.6103 and outperforms other results in the ChaLearn LAP Large-scale Continuous Gesture Recognition Challenge.\r"
  },
  "iccv2017_w44_discriminationbetweengenuineversusfakeemotionusinglong-shorttermmemorywithparametricbiasandfaciallandmarks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Discrimination Between Genuine Versus Fake Emotion Using Long-Short Term Memory With Parametric Bias and Facial Landmarks",
    "authors": [
      "Xuan-Phung Huynh",
      "Yong-Guk Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Huynh_Discrimination_Between_Genuine_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Huynh_Discrimination_Between_Genuine_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Discriminating between genuine and fake emotion is a new challenge because it is in contrast to the typical facial expression recognition that aims to classify the emotional state of a given facial stimulus. Fake emotion detection could be useful in telling how good an actor is in the movie or in judging a suspect tells the truth or not. To tackle this issue, we propose a new model by combining a mirror neuron modeling and deep recurrent networks, called long-short term memory (LSTM) with parametric bias (PB), by which features are extracted in the spatial-temporal domain from the facial landmarks, and then boil down to two PB vectors: one for genuine and other for fake one. Additionally, a binary classifier based on a gradient boosting is used to enhance discrimination capability between two PB vectors. The highest score from our system was 66.7 % in accuracy, suggesting that this approach could have a potential for useful applications.\r"
  },
  "iccv2017_w44_realvs.fakeemotionchallengelearningtorankauthenticityfromfacialactivitydescriptors": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Real vs. Fake Emotion Challenge: Learning to Rank Authenticity From Facial Activity Descriptors",
    "authors": [
      "Frerk Saxen",
      "Philipp Werner",
      "Ayoub Al-Hamadi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Saxen_Real_vs._Fake_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Saxen_Real_vs._Fake_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Distinguishing real from fake expressions is an emergent research topic. We propose a new method to rank authenticity of multiple videos from facial activity descriptors, which won the ChaLearn real vs. fake emotion challenge. Two studies with 22 human observers show that our method outperforms humans by a large margin. Further, it shows that our proposed ranking method is superior to direct classification. However, when humans are asked to compare two videos from the same subject and emotion before deciding which is fake or real there is no significant increase in performance compared to classifying each video individually. This suggests that our computer vision model is able to exploit facial attributes that are invisible for humans. The code is available at https://github.com/fsaxen/NIT-ICCV17Challenge.\r"
  },
  "iccv2017_w44_particlefilterbasedprobabilisticforcedalignmentforcontinuousgesturerecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Particle Filter Based Probabilistic Forced Alignment for Continuous Gesture Recognition",
    "authors": [
      "Necati Cihan Camgoz",
      "Simon Hadfield",
      "Richard Bowden"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Camgoz_Particle_Filter_Based_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Camgoz_Particle_Filter_Based_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we propose a novel particle filter based probabilistic forced alignment approach for training deep neural networks using weak border level annotations. The proposed method jointly learns to localize and recognize isolated instances in continuous streams. This is done by drawing training volumes from a prior distribution of likely regions and training a discriminative 3D-CNN from this data. The classifier is then used to calculate the posterior distribution by scoring the training examples and using this as the prior for the next sampling stage. We apply the proposed approach to the challenging task of continuous gesture recognition. We evaluate the performance on the popular ChaLearn 2016 ConGD dataset. Our method surpasses state-of-the-art results by obtaining 0.3646 and 0.3744 Mean Jaccard Index Score on the validation and test sets of ConGD, respectively. Furthermore, we participated in the ChaLearn 2017 Continuous Gesture Recognition Challenge and was ranked 3rd.\r"
  },
  "iccv2017_w44_gestureandsignlanguagerecognitionwithtemporalresidualnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Gesture and Sign Language Recognition With Temporal Residual Networks",
    "authors": [
      "Lionel Pigou",
      "Mieke Van Herreweghe",
      "Joni Dambre"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Pigou_Gesture_and_Sign_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Pigou_Gesture_and_Sign_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Gesture and sign language recognition in a continuous video stream is a challenging task, especially with a large vocabulary. In this work, we approach this as a framewise classification problem. We tackle it using temporal convolutions and recent advances in the deep learning field like residual networks, batch normalization and exponential linear units (ELUs). The models are evaluated on three different datasets: the Dutch Sign Language Corpus (Corpus NGT), the Flemish Sign Language Corpus (Corpus VGT) and the ChaLearn LAP RGB-D Continuous Gesture Dataset (ConGD). We achieve a 73.5% top-10 accuracy for 100 signs with the Corpus NGT, 56.4% with the Corpus VGT and a mean Jaccard index of 0.316 with the ChaLearn LAP ConGD without the usage of depth maps.\r"
  },
  "iccv2017_w44_relaxedspatio-temporaldeepfeatureaggregationforreal-fakeexpressionprediction": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Relaxed Spatio-Temporal Deep Feature Aggregation for Real-Fake Expression Prediction",
    "authors": [
      "Savas Ozkan",
      "Gozde Bozdagi Akar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Ozkan_Relaxed_Spatio-Temporal_Deep_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Ozkan_Relaxed_Spatio-Temporal_Deep_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Frame-level visual features are generally aggregated in time with the techniques such as LSTM, Fisher Vectors, NetVLAD etc. to produce a robust video-level representation. We here introduce a learnable aggregation technique whose primary objective is to retain short-time temporal structure between frame-level features and their spatial interdependencies in the representation. Also, it can be easily adapted to the cases where there have very scarce training samples. We evaluate the method on a real-fake expression prediction dataset to demonstrate its superiority. Our method obtains 65% score on the test dataset in the official MAP evaluation and there is only one misclassified decision with the best reported result in the Chalearn Challenge (i.e. 66:7%) . Lastly, we believe that this method can be extended to different problems such as action/event recognition in future.\r"
  },
  "iccv2017_w44_visualizingapparentpersonalityanalysiswithdeepresidualnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Visualizing Apparent Personality Analysis With Deep Residual Networks",
    "authors": [
      "Yagmur Gucluturk",
      "Umut Guclu",
      "Marc Perez",
      "Hugo Jair Escalante",
      "Xavier Baro",
      "Isabelle Guyon",
      "Carlos Andujar",
      "Julio Jacques Junior",
      "Meysam Madadi",
      "Sergio Escalera",
      "Marcel A. J. van Gerven",
      "Rob van Lier"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Gucluturk_Visualizing_Apparent_Personality_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Gucluturk_Visualizing_Apparent_Personality_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " The real world application scenarios for automatic prediction of apparent personality traits are vast and fall within a wide range of domains such as entertainment, health, human computer interaction, recruitment and security. These predictions can be critical for individuals in many scenarios (e.g., hiring an applicant). However, these predictions in and of themselves might be deemed to be untrustworthy without further supportive evidence in such scenarios. Through a series of experiments on a recently released benchmark dataset for automatic apparent personality trait prediction, this paper characterizes the audio and visual information that is used by a state-of-the-art model while making its predictions so as to provide such supportive evidence by explaining these predictions. Additionally, it describes a new web application, which gives feedback on apparent personality traits of its users by combining model predictions with their explanations.\r"
  },
  "iccv2017_w44_two-streamflow-guidedconvolutionalattentionnetworksforactionrecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Two-Stream Flow-Guided Convolutional Attention Networks for Action Recognition",
    "authors": [
      "An Tran",
      "Loong-Fah Cheong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Tran_Two-Stream_Flow-Guided_Convolutional_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Tran_Two-Stream_Flow-Guided_Convolutional_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper proposes a two-stream flow-guided convolutional attention networks for action recognition in videos. The central idea is that optical flows, when properly compensated for the camera motion, can be used to guide attention to the human foreground. We thus develop cross-link layers from the temporal network (trained on flows) to the spatial network (trained on RGB frames). These cross-link layers guide the spatial-stream to pay more attention to the human foreground areas and be less affected by background clutter. We obtain promising performances with our approach on the UCF101 and HMDB51 datasets.\r"
  },
  "iccv2017_w44_learningspatiotemporalfeaturesusing3dcnnandconvolutionallstmforgesturerecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Learning Spatiotemporal Features Using 3DCNN and Convolutional LSTM for Gesture Recognition",
    "authors": [
      "Liang Zhang",
      "Guangming Zhu",
      "Peiyi Shen",
      "Juan Song",
      "Syed Afaq Shah",
      "Mohammed Bennamoun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Zhang_Learning_Spatiotemporal_Features_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Zhang_Learning_Spatiotemporal_Features_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Gesture recognition aims at understanding the ongoing human gestures. In this paper, we present a deep architecture to learn spatiotemporal features for gesture recognition. The deep architecture first learns 2D spatiotemporal feature maps using 3D convolutional neural networks (3DCNN) and bidirectional convolutional long-short-term-memory networks (ConvLSTM). The learnt 2D feature maps can encode the global temporal information and local spatial information simultaneously. Then, 2DCNN is utilized further to learn the higher-level spatiotemporal features from the 2D feature maps for the final gesture recognition. The spatiotemporal correlation information is kept through the whole process of feature learning. This makes the deep architecture an effective spatiotemporal feature learner. Experiments on the ChaLearn LAP large-scale isolated gesture dataset (IsoGD) and the Sheffield Kinect Gesture (SKIG) dataset demonstrate the superiority of the proposed deep architecture.\r"
  },
  "iccv2017_w44_large-scalemultimodalgesturerecognitionusingheterogeneousnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Large-Scale Multimodal Gesture Recognition Using Heterogeneous Networks",
    "authors": [
      "Huogen Wang",
      "Pichao Wang",
      "Zhanjie Song",
      "Wanqing Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/altWang_Large-Scale_Multimodal_Gesture_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/altWang_Large-Scale_Multimodal_Gesture_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents the method designed for the 2017 ChaLearn LAP Large-scale Gesture Recognition Challenge. The proposed method converts a video sequence into multiple body level dynamic images and hand level dynamic images as the inputs to Convolutional Neural Networks (ConvNets) respectively through bidirectional rank pooling and adopts Convolutional LSTM Networks (ConvLSTM) to learn long-term spatiotemporal features from short-term spatiotemporal features extracted using a 3D convolutional neural network (3DCNN) at body and hand level. Such a heterogeneous network system learns effectively different levels of spatiotemporal features that are complementary to each other to improve the recognition accuracy largely. The method has been evaluated on the 2017 isolated and continuous ChaLearn LAP Large-scale Gesture Recognition Challenge datasets and the results are ranked among the top performances.\r"
  },
  "iccv2017_w44_large-scalemultimodalgesturesegmentationandrecognitionbasedonconvolutionalneuralnetworks": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Large-Scale Multimodal Gesture Segmentation and Recognition Based on Convolutional Neural Networks",
    "authors": [
      "Huogen Wang",
      "Pichao Wang",
      "Zhanjie Song",
      "Wanqing Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/alt2Wang_Large-Scale_Multimodal_Gesture_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/alt2Wang_Large-Scale_Multimodal_Gesture_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " This paper presents an effective method for continuous gesture recognition. The method consists of two modules: segmentation and recognition. In the segmentation module, a continuous gesture sequence is segmented into isolated gesture sequences by classifying the frames into gesture frames andtransitional frames using two stream convolutional neural networks. In the recognition module, our method exploits the spatiotemporal information embedded in RGB and depth sequences. For the depth modality, our method converts a sequence into Dynamic Images and Motion Dynamic Images through rank pooling and input them to Convolutional Neural Networks respectively. For the RGB modality, our method adopts Convolutional LSTM Networks to learn long-term spatiotemporal features from short-term spatiotemporal features obtained by a 3D convolutional neural network. Our method has been evaluated on ChaLearn LAP Large-scale Continuous Gesture Dataset and achieved the state-of-the-art performance. \r"
  },
  "iccv2017_w44_combiningsequentialgeometryandtexturefeaturesfordistinguishinggenuineanddeceptiveemotions": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Combining Sequential Geometry and Texture Features for Distinguishing Genuine and Deceptive Emotions",
    "authors": [
      "Liandong Li",
      "Tadas Baltrusaitis",
      "Bo Sun",
      "Louis-Philippe Morency"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Li_Combining_Sequential_Geometry_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Li_Combining_Sequential_Geometry_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this paper, we explore a new type of automatic emotion recognition task - distinguishing genuine and deceptiveemotions from video clips. For this task, it is not enough only using static images clipped from the video data, as there's only subtle differences between two types of emotions, which makes it even harder for automatic analysis. To utilize the temporal information, we introduce temporal attention gated model for this emotion recognition task. Compared to texture features which describe the whole face area, the facial landmark sequences may also indicate the temporal changes of the face, thus we utilize them by encoding feature sequence unsupervisedly. \r"
  },
  "iccv2017_w44_learningspatio-temporalfeatureswith3dresidualnetworksforactionrecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Learning Spatio-Temporal Features With 3D Residual Networks for Action Recognition",
    "authors": [
      "Kensho Hara",
      "Hirokatsu Kataoka",
      "Yutaka Satoh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Convolutional neural networks with spatio-temporal 3D kernels (3D CNNs) have an ability to directly extract spatio-temporal features from videos for action recognition. Although the 3D kernels tend to overfit because of a large number of their parameters, the 3D CNNs are greatly improved by using recent huge video databases. However, the architecture of 3D CNNs is relatively shallow against to the success of very deep neural networks in 2D-based CNNs, such as residual networks (ResNets). In this paper, we propose a 3D CNNs based on ResNets toward a better action representation. We describe the training procedure of our 3D ResNets in details. We experimentally evaluate the 3D ResNets on the ActivityNet and Kinetics datasets. The 3D ResNets trained on the Kinetics did not suffer from overfitting despite the large number of parameters of the model, and achieved better performance than relatively shallow networks, such as C3D. Our code and pretrained models are publicly available.\r"
  },
  "iccv2017_w44_facialexpressionrecognitionviajointdeeplearningofrgb-depthmaplatentrepresentations": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Facial Expression Recognition via Joint Deep Learning of RGB-Depth Map Latent Representations",
    "authors": [
      "Oyebade K. Oyedotun",
      "Girum Demisse",
      "Abd El Rahman Shabayek",
      "Djamila Aouada",
      "Bjorn Ottersten"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Oyedotun_Facial_Expression_Recognition_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Oyedotun_Facial_Expression_Recognition_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " Humans use facial expressions successfully for conveying their emotional states. However, replicating such success in the human-computer interaction domain is an active research problem. In this paper, we propose deep convolutional neural network (DCNN) for joint learning of robust facial expression features from fused RGB and depth map latent representations. We posit that learning jointly from both modalities result in a more robust classifier for facial expression recognition (FER) as opposed to learning from either of the modalities independently. Particularly, we construct a learning pipeline that allows us to learn several hierarchical levels of feature representations and then perform the fusion of RGB and depth map latent representations for joint learning of facial expressions. Our experimental results on the BU-3DFE dataset validate the proposed fusion approach, as a model learned from the joint modalities outperforms models learned from either of the modalities. \r"
  },
  "iccv2017_w44_darwintreesforactionrecognition": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Darwintrees for Action Recognition",
    "authors": [
      "Albert Clapes",
      "Tinne Tuytelaars",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Clapes_Darwintrees_for_Action_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Clapes_Darwintrees_for_Action_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We propose a novel mid-level representation for action recognition on RGB videos. We model the evolution of improved dense trajectory features not only for the entire video sequence, but also on subparts of the video. Subparts are obtained using a spectral divisive clustering that yields an unordered tree decomposing the entire cloud of trajectories of a sequence. We compute videodarwin on video subparts, exploiting more finegrained temporal information and reducing the sensitivity of the standard time varying mean of videodarwin. Next, we model the evolution of features through both frames of subparts and paths in tree branches. We refer to these mid-level representations as node-darwintree and branch-darwintree respectively. For classification, we construct a kernel for both mid-level and holistic videodarwin. Our approach achieves better performance than standard videodarwin and defines the current state-of-the-art on UCF-Sports and Highfive action recognition datasets.\r"
  },
  "iccv2017_w44_actionrecognitionfromrgb-ddatacomparisonandfusionofspatio-temporalhandcraftedfeaturesanddeepstrategies": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Action Recognition From RGB-D Data: Comparison and Fusion of Spatio-Temporal Handcrafted Features and Deep Strategies",
    "authors": [
      "Maryam Asadi-Aghbolaghi",
      "Hugo Bertiche",
      "Vicent Roig",
      "Shohreh Kasaei",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Asadi-Aghbolaghi_Action_Recognition_From_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Asadi-Aghbolaghi_Action_Recognition_From_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " In this work, multimodal fusion of RGB-D data are analyzed for action recognition by using scene flow as early fusion and integrating the results of all modalities in a late fusion fashion. Recently, there is a migration from traditional handcrafting to deep learning. However, handcrafted features are still widely used owing to their high performance and low computational complexity. In this research, Multimodal dense trajectories (MMDT) is proposed to describe RGB-D videos. Dense trajectories are pruned based on scene flow data. Besides, 2DCNN is extended to multimodal (MM2DCNN) by adding one more stream (scene flow) as input and then fusing the output of all models. We evaluate and compare the results from each modality and their fusion on two action datasets. The experimental result shows that the new representation improves the accuracy. Furthermore, the fusion of handcrafted and learning-based features shows a boost in the final performance, achieving state of the art results.\r"
  },
  "iccv2017_w44_resultsandanalysisofchalearnlapmulti-modalisolatedandcontinuousgesturerecognition,andrealversusfakeexpressedemotionschallenges": {
    "conf_id": "ICCV2017",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "ICCV2017_workshops - Action, Gesture, and Emotion Recognition Competitions: Large Scale Multimodal Gesture Recognition and Real Versus Fake Expressed Emotions",
    "title": "Results and Analysis of ChaLearn LAP Multi-Modal Isolated and Continuous Gesture Recognition, and Real Versus Fake Expressed Emotions Challenges",
    "authors": [
      "Jun Wan",
      "Sergio Escalera",
      "Gholamreza Anbarjafari",
      "Hugo Jair Escalante",
      "Xavier Baro",
      "Isabelle Guyon",
      "Meysam Madadi",
      "Juri Allik",
      "Jelena Gorbova",
      "Chi Lin",
      "Yiliang Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/w44/html/Wan_Results_and_Analysis_ICCV_2017_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2017_workshops/../content_ICCV_2017_workshops/papers/w44/Wan_Results_and_Analysis_ICCV_2017_paper.pdf",
    "published": "2017-10",
    "summary": " We analyze the results of the 2017 ChaLearn Looking at People Challenge at ICCV. The challenge comprised three tracks: (1) large-scale isolated (2) continuous gesture recognition, and (3) real versus fake expressed emotions tracks. It is the second round for both gesture recognition challenges, which were held first in the context of the ICPR 2016 workshop on \"multimedia challenges beyond visual analysis\". In this second round, more participants joined the competitions, and the performances considerably improved compared to the first round. The third track is the first challenge on real versus fake expressed emotion classification, including six emotion categories, for which a novel database was introduced. The first place was shared between two teams who achieved 67.7 averaged recognition rate on the test set. The data of the three tracks, the participants' code and method descriptions are publicly available to allow researchers to keep making progress in the field.\r"
  }
}