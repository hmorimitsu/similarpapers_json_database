{
  "aaai2021_main_theundergraduategamescorpusadatasetformachineperceptionofinteractivemedia": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "The Undergraduate Games Corpus: A Dataset for Machine Perception of Interactive Media",
    "authors": [
      "Barrett R. Anderson",
      "Adam M. Smith"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16071",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16071/15878",
    "published": "2021-02",
    "summary": "Machine perception research primarily focuses on processing static inputs (e.g. images and texts). We are interested in machine perception of interactive media (such as games, apps, and complex web applications) where interactive audience choices have long-term implications for the audience experience. While there is ample research on AI methods for the task of playing games (often just one game at a time), this work is difficult to apply to new and in-development games or to use for non-playing tasks such as similarity-based retrieval or authoring assistance. In response, we contribute a corpus of 755 games and structured metadata, spread across several platforms (Twine, Bitsy, Construct, and Godot), with full source and assets available and appropriately licensed for use and redistribution in research. Because these games were sourced from student projects in an undergraduate game development program, they reference timely themes in their content and represent a variety of levels of design polish rather than only representing past commercial successes. This corpus could accelerate research in understanding interactive media while anchoring that work in freshly-developed games intended as legitimate human experiences (rather than lab-created AI testbeds). We validate the utility of this corpus by setting up the novel task of predicting tags relevant to the player experience from the game source code, showing that representations that better exploit the structure of the media outperform a text-only baseline."
  },
  "aaai2021_main_efficientpovertymappingfromhighresolutionremotesensingimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Efficient Poverty Mapping from High Resolution Remote Sensing Images",
    "authors": [
      "Kumar Ayush",
      "Burak Uzkent",
      "Kumar Tanmay",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16072",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16072/15879",
    "published": "2021-02",
    "summary": "The combination of high-resolution satellite imagery and machine learning have proven useful in many sustainability-related tasks, including poverty prediction, infrastructure measurement, and forest monitoring. However, the accuracy afforded by high-resolution imagery comes at a cost, as such imagery is extremely expensive to purchase at scale. This creates a substantial hurdle to the efficient scaling and widespread adoption of high-resolution-based approaches.To reduce acquisition costs while maintaining accuracy, we propose a reinforcement learning approach in which free low-resolution imagery is used to dynamically identify where to acquire costly high-resolution images, prior to performing a deep learning task on the high-resolution images. We apply this approach to the task of poverty prediction in Uganda, building on an earlier approach that used object detection to count objects and use these counts to predict poverty. Our approach exceeds previous performance benchmarks on this task while using 80% fewer high-resolution images, and could be useful in many domains that require high-resolution imagery."
  },
  "aaai2021_main_optimalkidneyexchangewithimmunosuppressants": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Optimal Kidney Exchange with Immunosuppressants",
    "authors": [
      "Haris Aziz",
      "\u00c1gnes Cseh",
      "John P. Dickerson",
      "Duncan C. McElfresh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16073",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16073/15880",
    "published": "2021-02",
    "summary": "Algorithms for exchange of kidneys is one of the key successful applications in market design, artificial intelligence, and operations research. Potent immunosuppressant drugs suppress the body's ability to reject a transplanted organ up to the point that a transplant across blood- or tissue-type incompatibility becomes possible. In contrast to the standardkidney exchange problem, we consider a setting that also involves the decision about which recipients receive from the limited supply of immunosuppressants that make them compatible with originally incompatible kidneys. We firstly present a general computational framework to model this problem. Our main contribution is a range of efficient algorithms that provide flexibility in terms of meeting meaningful objectives. Motivated by the current reality of kidney exchanges using sophisticated mathematical-programming-based clearing algorithms, we then present a general but scalable approach to optimal clearing with immunosuppression; we validate our approach on realistic data from a large fielded exchange."
  },
  "aaai2021_main_treecapstree-basedcapsulenetworksforsourcecodeprocessing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "TreeCaps: Tree-Based Capsule Networks for Source Code Processing",
    "authors": [
      "Nghi D. Q. Bui",
      "Yijun Yu",
      "Lingxiao Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16074",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16074/15881",
    "published": "2021-02",
    "summary": "Recently program learning techniques have been proposed to process source code based on syntactical structures (e.g., abstract syntax trees) and/or semantic information (e.g., dependency graphs). While graphs may be better than trees at capturing code semantics, constructing the graphs from code inputs through the semantic analysis of multiple viewpoints can lead to inaccurate noises for a specific software engineering task. Compared to graphs, syntax trees are more precisely defined on the grammar and easier to parse; unfortunately, previous tree-based learning techniques have not been able to learn semantic information from trees to achieve better accuracy than graph-based techniques. We have proposed a new learning technique, named TreeCaps, by fusing together capsule networks with tree-based convolutional neural networks to achieve a learning accuracy higher than some existing graph-based techniques while it is based only on trees. TreeCaps introduces novel variable-to-static routing algorithms into the capsule networks to compensate for the loss of previous routing algorithms. Aside from accuracy, we also find that TreeCaps is the most robust to withstand those semantic-preserving program transformations that change code syntax without modifying the semantics. Evaluated on a large number of Java and C/C++ programs, TreeCaps models outperform prior deep learning models of program source code, in terms of both accuracy and robustness for program comprehension tasks such as code functionality classification and function name prediction. Our implementation is publicly available at: https://github.com/bdqnghi/treecaps."
  },
  "aaai2021_main_abottom-updagstructureextractionmodelformathwordproblems": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Bottom-Up DAG Structure Extraction Model for Math Word Problems",
    "authors": [
      "Yixuan Cao",
      "Feng Hong",
      "Hongwei Li",
      "Ping Luo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16075",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16075/15882",
    "published": "2021-02",
    "summary": "Research on automatically solving mathematical word problems (MWP) has a long history. Most recent works adopt Seq2Seq approach to predict the result equations as a sequence of quantities and operators. Although result equations can be written as a sequence, it is essentially a structure. More precisely, it is a Direct Acyclic Graph (DAG) whose leaf nodes are the quantities, and internal and root nodes are arithmetic or comparison operators. In this paper, we propose a novel Seq2DAG approach to extract the equation set directly as a DAG structure. It is extracted in a bottom-up fashion by aggregating quantities and sub-expressions layer by layer iteratively. The advantages of our approach approach are three-fold: it is intrinsically suitable to solve multivariate problems, it always outputs valid structure, and its computation satisfies commutative law for +, x and =. Experimental results on Math23K and DRAW1K demonstrate that our model outperforms state-of-the-art deep learning methods. We also conduct detailed analysis on the results to show the strengths and limitations of our approach."
  },
  "aaai2021_main_diagnoselikeapathologistweakly-supervisedpathologist-treenetworkforslide-levelimmunohistochemicalscoring": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Diagnose Like A Pathologist: Weakly-Supervised Pathologist-Tree Network for Slide-Level Immunohistochemical Scoring",
    "authors": [
      "Zhen Chen",
      "Jun Zhang",
      "Shuanlong Che",
      "Junzhou Huang",
      "Xiao Han",
      "Yixuan Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16076",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16076/15883",
    "published": "2021-02",
    "summary": "The immunohistochemistry (IHC) test of biopsy tissue is crucial to develop targeted treatment and evaluate prognosis for cancer patients. The IHC staining slide is usually digitized into the whole-slide image (WSI) with gigapixels for quantitative image analysis. To perform a whole image prediction (e.g., IHC scoring, survival prediction, and cancer grading) from this kind of high-dimensional image, algorithms are often developed based on multi-instance learning (MIL) framework. However, the multi-scale information of WSI and the associations among instances are not well explored in existing MIL based studies. Inspired by the fact that pathologists jointly analyze visual fields at multiple powers of objective for diagnostic predictions, we propose a Pathologist-Tree Network (PTree-Net) to sparsely model the WSI efficiently in multi-scale manner. Specifically, we propose a Focal-Aware Module (FAM) that can approximately estimate diagnosis-related regions with an extractor trained using the thumbnail of WSI. With the initial diagnosis-related regions, we hierarchically model the multi-scale patches in a tree structure, where both the global and local information can be captured. To explore this tree structure in an end-to-end network, we propose a patch Relevance-enhanced Graph Convolutional Network (RGCN) to explicitly model the correlations of adjacent parent-child nodes, accompanied by patch relevance to exploit the implicit contextual information among distant nodes. In addition, tree-based self-supervision is devised to improve representation learning and suppress irrelevant instances adaptively. Extensive experiments are performed on a large-scale IHC HER2 dataset. The ablation study confirms the effectiveness of our design, and our approach outperforms state-of-the-art by a large margin."
  },
  "aaai2021_main_modelingthemomentumspillovereffectforstockpredictionviaattribute-drivengraphattentionnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Modeling the Momentum Spillover Effect for Stock Prediction via Attribute-Driven Graph Attention Networks",
    "authors": [
      "Rui Cheng",
      "Qing Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16077",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16077/15884",
    "published": "2021-02",
    "summary": "In finance, the momentum spillovers of listed firms is well acknowledged. Only few studies predicted the trend of one firm in terms of its relevant firms. A common strategy of the pilot work is to adopt graph convolution networks (GCNs) with some predefined firm relations. However, momentum spillovers are propagated via a variety of firm relations, of which the bridging importance varies with time. Restricting to several predefined relations inevitably makes noise and thus misleads stock predictions. In addition, traditional GCNs transfer and aggregate the peer influences without considering the states of both connected firms once a connection is built. Such non-attribute sensibility makes traditional GCNs inappropriate to deal with the attribute-sensitive momentum spillovers of listed firms wherein the abnormal price drop of one firm may not spill over if the trade volume of this decreasing price is small or the prices of the linked firms are undervalued. In this study, we propose an attribute-driven graph attention network (AD-GAT) to address both problems in modeling momentum spillovers. This is achieved by element-wisely multiplying the nonlinear transformation of the attributes of the connected firms with the attributes of the source firm to consider its attribute-sensitive momentum spillovers, and applying the unmasked attention mechanism to infer the general dynamic firm relation from observed market signals fused by a novel tensor-based feature extractor. Experiments on the three-year data of the S&P 500 demonstrate the superiority of the proposed framework over stateof-the-art algorithms, including GCN, eLSTM, and TGC."
  },
  "aaai2021_main_differentiallyprivatelinkpredictionwithprotectedconnections": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Differentially Private Link Prediction with Protected Connections",
    "authors": [
      "Abir De",
      "Soumen Chakrabarti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16078",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16078/15885",
    "published": "2021-02",
    "summary": "Link prediction (LP) algorithms propose to each node a ranked list of nodes that are currently non-neighbors, as the most likely candidates for future linkage. Owing to increasing concerns about privacy, users (nodes) may prefer to keep some of their connections protected or private. Motivated by this observation, our goal is to design a differentially private LPalgorithm, which trades off between privacy of the protected node-pairs and the link prediction accuracy. More specifically, we first propose a form of differential privacy on graphs, which models the privacy loss only of those node-pairs which are marked as protected. Next, we develop DPLP, a learning to rank algorithm, which applies a monotone transform to base scores from a non-private LP system, and then adds noise.DPLP is trained with a privacy induced ranking loss, which optimizes the ranking utility for a given maximum allowed level of privacy leakage of the protected node-pairs. Under a recently introduced latent node embedding model, we present a formal trade-off between privacy and LP utility. Extensive experiments with several real-life graphs and several LP heuristics show that DPLP can trade off between privacy and predictive performance more effectively than several alternatives."
  },
  "aaai2021_main_graphneuralnetworktodiluteoutliersforrefactoringmonolithapplication": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Graph Neural Network to Dilute Outliers for Refactoring Monolith Application",
    "authors": [
      "Utkarsh Desai",
      "Sambaran Bandyopadhyay",
      "Srikanth Tamilselvam"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16079",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16079/15886",
    "published": "2021-02",
    "summary": "Microservices are becoming the defacto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. Therefore, enterprises as part of their journey to cloud, are increasingly looking to refactor their monolith application into one or more candidate microservices; wherein each service contains a group of software entities (e.g., classes) that are responsible for a common functionality. Graphs are a natural choice to represent a software system. Each software entity can be represented as nodes and its dependencies with other entities as links. Therefore, this problem of refactoring can be viewed as a graph based clustering task. In this work, we propose a novel method to adapt the recent advancements in graph neural networks in the context of code to better understand the software and apply them in the clustering task. In that process, we also identify the outliers in the graph which can be directly mapped to top refactor candidates in the software. Our solution is able to improve state-of-the-art performance compared to works from both software engineering and existing graph representation based techniques."
  },
  "aaai2021_main_kanknowledge-awareattentionnetworkforfakenewsdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "KAN: Knowledge-aware Attention Network for Fake News Detection",
    "authors": [
      "Yaqian Dun",
      "Kefei Tu",
      "Chen Chen",
      "Chunyan Hou",
      "Xiaojie Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16080",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16080/15887",
    "published": "2021-02",
    "summary": "The explosive growth of fake news on social media has drawn great concern both from industrial and academic communities. There has been an increasing demand for fake news detection due to its detrimental effects. Generally, news content is condensed and full of knowledge entities. However, existing methods usually focus on the textual contents and social context, and ignore the knowledge-level relationships among news entities. To address this limitation, in this paper, we propose a novel Knowledge-aware Attention Network (KAN) that incorporates external knowledge from knowledge graph for fake news detection. Firstly, we identify entity mentions in news contents and align them with the entities in knowledge graph. Then, the entities and their contexts are used as external knowledge to provide complementary information. Finally, we design News towards Entities (N-E) attention and News towards Entities and Entity Contexts (N-E^2C) attention to measure the importances of knowledge. Thus, our proposed model can incorporate both semantic-level and knowledge-level representations of news to detect fake news. Experimental results on three public datasets show that our model outperforms the state-of-the-art methods, and also validate the effectiveness of knowledge attention."
  },
  "aaai2021_main_whenhashingmetmatchingefficientspatio-temporalsearchforridesharing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "When Hashing Met Matching: Efficient Spatio-Temporal Search for Ridesharing",
    "authors": [
      "Chinmoy Dutta"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16081",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16081/15888",
    "published": "2021-02",
    "summary": "Shared on-demand mobility holds immense potential for urban transportation. However, finding ride matches in real-time at urban scale is a very difficult combinatorial optimization problem and mostly heuristic approaches are applied. In this work, we introduce a principled approach to this combinatorial problem. Our approach proceeds by constructing suitable representations for rides and driver routes capturing their essential spatio-temporal aspects in an appropriate vector space, and defining a similarity metric in this space that expresses matching utility. This then lets us mathematically model the problem of finding ride matches as that of Near Neighbor Search (NNS). Exploiting this modeling, we devise a novel spatio-temporal search algorithm for finding ride matches based on the theory of Locality Sensitive Hashing (LSH). Apart from being highly efficient, our algorithm enjoys several practically useful properties and extension possibilities. Experiments with large real-world datasets show that our algorithm consistently outperforms state-of-the-art heuristic methods thereby proving its practical applicability."
  },
  "aaai2021_main_generegulatorynetworkinferenceusing3dconvolutionalneuralnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Gene Regulatory Network Inference using 3D Convolutional Neural Network",
    "authors": [
      "Yue Fan",
      "Xiuli Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16082",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16082/15889",
    "published": "2021-02",
    "summary": "Gene regulatory networks (GRNs) consist of gene regulations between transcription factors (TFs) and their target genes. Single-cell RNA sequencing (scRNA-seq) brings both opportunities and challenges to the inference of GRNs. On the one hand, scRNA-seq data reveals statistic information of gene expressions at the single-cell resolution, which is conducive to the construction of GRNs; on the other hand, noises and dropouts pose great difficulties on the analysis of scRNA-seq data, causing low prediction accuracy by traditional methods. In this paper, we propose 3D Co-Expression Matrix Analysis (3DCEMA), which predicts regulatory relationships by classifying 3D co-expression matrices of gene triples using a 3D convolutional neural network. We found that by introducing a third gene as a comparison factor, our method can avoid the disturbance of noises and dropouts, and significantly increase the prediction accuracy of regulations between gene pairs. Compared with other existing GRN inference algorithms on both in-silico datasets and scRNA-Seq datasets, our algorithm based on deep learning shows higher stability and accuracy in the task of GRN inference."
  },
  "aaai2021_main_universaltradingfororderexecutionwithoraclepolicydistillation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Universal Trading for Order Execution with Oracle Policy Distillation",
    "authors": [
      "Yuchen Fang",
      "Kan Ren",
      "Weiqing Liu",
      "Dong Zhou",
      "Weinan Zhang",
      "Jiang Bian",
      "Yong Yu",
      "Tie-Yan Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16083",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16083/15890",
    "published": "2021-02",
    "summary": "As a fundamental problem in algorithmic trading, order execution aims at fulfilling a specific trading order, either liquidation or acquirement, for a given instrument. Towards effective execution strategy, recent years have witnessed the shift from the analytical view with model-based market assumptions to model-free perspective, i.e., reinforcement learning, due to its nature of sequential decision optimization. However, the noisy and yet imperfect market information that can be leveraged by the policy has made it quite challenging to build up sample efficient reinforcement learning methods to achieve effective order execution. In this paper, we propose a novel universal trading policy optimization framework to bridge the gap between the noisy yet imperfect market states and the optimal action sequences for order execution. Particularly, this framework leverages a policy distillation method that can better guide the learning of the common policy towards practically optimal execution by an oracle teacher with perfect information to approximate the optimal trading strategy. The extensive experiments have shown significant improvements of our method over various strong baselines, with reasonable trading actions."
  },
  "aaai2021_main_dual-octaveconvolutionforacceleratedparallelmrimagereconstruction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dual-Octave Convolution for Accelerated Parallel MR Image Reconstruction",
    "authors": [
      "Chun-Mei Feng",
      "Zhanyuan Yang",
      "Geng Chen",
      "Yong Xu",
      "Ling Shao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16084",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16084/15891",
    "published": "2021-02",
    "summary": "Magnetic resonance (MR) image acquisition is an inherently prolonged process, whose acceleration by obtaining multiple undersampled images simultaneously through parallel imaging has always been the subject of research. In this paper, we propose the Dual-Octave Convolution (Dual-OctConv), which is capable of learning multi-scale spatial-frequency features from both real and imaginary components, for fast parallel MR image reconstruction. By reformulating the complex operations using octave convolutions, our model shows a strong ability to capture richer representations of MR images, while at the same time greatly reducing the spatial redundancy. More specifically, the input feature maps and convolutional kernels are first split into two components (i.e., real and imaginary), which are then divided into four groups according to their spatial frequencies. Then, our Dual-OctConv conducts intra-group information updating and inter-group information exchange to aggregate the contextual information across different groups. Our framework provides two appealing benefits: (i) it encourages interactions between real and imaginary components at various spatial frequencies to achieve richer representational capacity, and (ii) it enlarges the receptive field by learning multiple spatial-frequency features of both the real and imaginary components. We evaluate the performance of the proposed model on the acceleration of multi-coil MR image reconstruction. Extensive experiments are conducted on an {in vivo} knee dataset under different undersampling patterns and acceleration factors. The experimental results demonstrate the superiority of our model in accelerated parallel MR image reconstruction. Our code is available at: github.com/chunmeifeng/Dual-OctConv."
  },
  "aaai2021_main_mimosamulti-constraintmoleculesamplingformoleculeoptimization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "MIMOSA: Multi-constraint Molecule Sampling for Molecule Optimization",
    "authors": [
      "Tianfan Fu",
      "Cao Xiao",
      "Xinhao Li",
      "Lucas M. Glass",
      "Jimeng Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16085",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16085/15892",
    "published": "2021-02",
    "summary": "Molecule optimization is a fundamental task for accelerating drug discovery, with the goal of generating new valid molecules that maximize multiple drug properties while maintaining similarity to the input molecule. Existing generative models and reinforcement learning approaches made initial success, but still face difficulties in simultaneously optimizing multiple drug properties. To address such challenges, we propose the MultI-constraint MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule as an initial guess and sample molecules from the target distribution. MIMOSA first pretrains two property agnostic graph neural networks (GNNs) for molecule topology and substructure-type prediction, where a substructure can be either atom or single ring. For each iteration, MIMOSA uses the GNNs\u2019 prediction and employs three basic substructure operations (add, replace, delete) to generate new molecules and associated weights. The weights can encode multiple constraints including similarity and drug property constraints, upon which we select promising molecules for next iteration. MIMOSA enables flexible encoding of multiple property- and similarity-constraints and can efficiently generate new molecules that satisfy various property constraints and achieved up to 49.1% relative improvement over the best baseline in terms of success rate."
  },
  "aaai2021_main_ecgode-ganlearningordinarydifferentialequationsofecgdynamicsviagenerativeadversariallearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "ECG ODE-GAN: Learning Ordinary Differential Equations of ECG Dynamics via Generative Adversarial Learning",
    "authors": [
      "Tomer Golany",
      "Daniel Freedman",
      "Kira Radinsky"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16086",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16086/15893",
    "published": "2021-02",
    "summary": "Understanding the dynamics of complex biological and physiological systems has been exploredfor many years in the form of physically-based mathematical simulators. The behavior of a physical system is often described via ordinary differential equations (ODE), referred to as the dynamics. In the standard case, the dynamics are derived from purely physical considerations. By contrast, in this work we study how the dynamics can be learned by a generative adversarial network which combines both physical and data considerations. As a use case, we focus on the dynamics of the heart signal electrocardiogram (ECG). We begin by introducing a new GAN framework, dubbed ODE-GAN, in which the generator learns the dynamics of a physical system in the form of an ordinary differential equation. Specifically, the generator network receives as input a value at a specific time step, and produces the derivative of the system at that time step. Thus, the ODE-GAN learns purely data-driven dynamics. We then show how to incorporate physical considerations into ODE-GAN. We achieve this through the introduction of an additional input to the ODE-GAN generator: physical parameters, which partially characterize the signal of interest. As we focus on ECG signals, we refer to this new framework as ECG-ODE-GAN. We perform an empirical evaluation and show that generating ECG heartbeats from our learned dynamics improves ECG heartbeat classification."
  },
  "aaai2021_main_toweredactorcriticforhandlingmultipleactiontypesinreinforcementlearningfordrugdiscovery": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Towered Actor Critic For Handling Multiple Action Types In Reinforcement Learning For Drug Discovery",
    "authors": [
      "Sai Krishna Gottipati",
      "Yashaswi Pathak",
      "Boris Sattarov",
      "Sahir",
      "Rohan\n      Nuttall",
      "Mohammad Amini",
      "Matthew E. Taylor",
      "Sarath Chandar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16087",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16087/15894",
    "published": "2021-02",
    "summary": "Reinforcement learning (RL) has made significant progress in both abstract and real-world domains, but the majority of state-of-the-art algorithms deal only with monotonic actions. However, some applications require agents to reason over different types of actions. Our application simulates reaction-based molecule generation, used as part of the drug discovery pipeline, and includes both uni-molecular and bi-molecular reactions. This paper introducesa novel framework, towered actor critic (TAC), to handle multiple action types. The TAC framework is general in that it is designed to be combined with any existing RL algorithms for continuous action space. We combine it with TD3 to empirically obtain significantly better results than existing methods in the drug discovery setting. TAC is also applied to RL benchmarks in OpenAI Gym and results show that our framework can improve, or at least does not hurt, performance relative to standard TD3."
  },
  "aaai2021_main_hierarchicalgraphconvolutionnetworkfortrafficforecasting": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Hierarchical Graph Convolution Network for Traffic Forecasting",
    "authors": [
      "Kan Guo",
      "Yongli Hu",
      "Yanfeng Sun",
      "Sean Qian",
      "Junbin Gao",
      "Baocai Yin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16088",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16088/15895",
    "published": "2021-02",
    "summary": "Traffic forecasting is attracting considerable interest due to its widespread application in intelligent transportation systems. Given the complex and dynamic traffic data, many methods focus on how to establish a spatial-temporal model to express the non-stationary traffic patterns. Recently, the latest Graph Convolution Network (GCN) has been introduced to learn spatial features while the time neural networks are used to learn temporal features. These GCN based methods obtain state-of-the-art performance. However, the current GCN based methods ignore the natural hierarchical structure of traffic systems which is composed of the micro layers of road networks and the macro layers of region networks, in which the nodes are obtained through pooling method and could include some hot traffic regions such as downtown and CBD etc., while the current GCN is only applied on the micro graph of road networks. In this paper, we propose a novel Hierarchical Graph Convolution Networks (HGCN) for traffic forecasting by operating on both the micro and macro traffic graphs. The proposed method is evaluated on two complex city traffic speed datasets. Compared to the latest GCN based methods like Graph WaveNet, the proposed HGCN gets higher traffic forecasting precision with lower computational cost.The website of the code is https://github.com/guokan987/HGCN.git."
  },
  "aaai2021_main_automatedlaylanguagesummarizationofbiomedicalscientificreviews": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Automated Lay Language Summarization of Biomedical Scientific Reviews",
    "authors": [
      "Yue Guo",
      "Wei Qiu",
      "Yizhong Wang",
      "Trevor Cohen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16089",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16089/15896",
    "published": "2021-02",
    "summary": "Health literacy has emerged as a crucial factor in making appropriate health decisions and ensuring treatment outcomes. However, medical jargon and the complex structure of professional language in this domain make health information especially hard to interpret. Thus, there is an urgent unmet need for automated methods to enhance the accessibility of the biomedical literature to the general population. This problem can be framed as a type of translation problem between the language of healthcare professionals, and that of the general public.In this paper,we introduce the novel task of automated generation of lay language summaries of biomedical scientific reviews, and construct a dataset to support the development and evaluation of automated methods through which to enhance the accessibility of the biomedical literature.We conduct analyses of the various challenges in performing this task, including not only summarization of the key points but also explanation of background knowledge and simplification of professional language. We experiment with state-of-the-art summarization models as well as several data augmentation techniques, and evaluate their performance using both automated metrics and human assessment. Results indicate that automatically generated summaries produced using contemporary neural architectures can achieve promising quality and readability as compared with reference summaries developed for the lay public by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score of 13.30). We also discuss the limitations of the current effort, providing insights and directions for future work."
  },
  "aaai2021_main_sub-seasonalclimateforecastingviamachinelearningchallenges,analysis,andadvances": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Sub-Seasonal Climate Forecasting via Machine Learning: Challenges, Analysis, and Advances",
    "authors": [
      "Sijie He",
      "Xinyan Li",
      "Timothy DelSole",
      "Pradeep Ravikumar",
      "Arindam Banerjee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16090",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16090/15897",
    "published": "2021-02",
    "summary": "Sub-seasonal forecasting (SSF) focuses on predicting key variables such as temperature and precipitation on the 2-week to 2-month time scale. Skillful SSF would have immense societal value in such areas as agricultural productivity, water resource management, and emergency planning for extreme weather events. However, SSF is considered more challenging than either weather prediction or even seasonal prediction, and is still a largely understudied problem. In this paper, we carefully investigate 10 Machine Learning (ML) approaches to sub-seasonal temperature forecasting over the contiguous U.S. on the SSF dataset we collect, including a variety of climate variables from the atmosphere, ocean, and land. Because of the complicated atmosphere-land-ocean couplings and the limited amount of good quality observational data, SSF imposes a great challenge for ML despite the recent advances in various domains. Our results indicate that suitable ML models, e.g., XGBoost, to some extent, capture the predictability on sub-seasonal time scales and can outperform the climatological baselines, while Deep Learning (DL) models barely manage to match the best results with carefully designed architecture. Besides, our analysis and exploration provide insights on important aspects to improve the quality of sub-seasonal forecasts, e.g., feature representation and model architecture. The SSF dataset and code are released with this paper for use by the broader research community."
  },
  "aaai2021_main_compoundwordtransformerlearningtocomposefull-songmusicoverdynamicdirectedhypergraphs": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs",
    "authors": [
      "Wen-Yi Hsiao",
      "Jen-Yu Liu",
      "Yin-Cheng Yeh",
      "Yi-Hsuan Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16091",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16091/15898",
    "published": "2021-02",
    "summary": "To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note\u2019s pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5 to 10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music"
  },
  "aaai2021_main_modelingthecompatibilityofstemtrackstogeneratemusicmashups": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Modeling the Compatibility of Stem Tracks to Generate Music Mashups",
    "authors": [
      "Jiawen Huang",
      "Ju-Chiang Wang",
      "Jordan B. L. Smith",
      "Xuchen Song",
      "Yuxuan Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16092",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16092/15899",
    "published": "2021-02",
    "summary": "A music mashup combines audio elements from two or more songs to create a new work. To reduce the time and effort required to make them, researchers have developed algorithms that predict the compatibility of audio elements. Prior work has focused on mixing unaltered excerpts, but advances in source separation enable the creation of mashups from isolated stems (e.g., vocals, drums, bass, etc.). In this work, we take advantage of separated stems not just for creating mashups, but for training a model that predicts the mutual compatibility of groups of excerpts, using self-supervised and semi-supervised methods. Specifically, we first produce a random mashup creation pipeline that combines stem tracks obtained via source separation, with key and tempo automatically adjusted to match, since these are prerequisites for high-quality mashups. To train a model to predict compatibility, we use stem tracks obtained from the same song as positive examples, and random combinations of stems with key and/or tempo unadjusted as negative examples. To improve the model and use more data, we also train on \"average\" examples: random combinations with matching key and tempo, where we treat them as unlabeled data as their true compatibility is unknown. To determine whether the combined signal or the set of stem signals is more indicative of the quality of the result, we experiment on two model architectures and train them using semi-supervised learning technique. Finally, we conduct objective and subjective evaluations of the system, comparing them to a standard rule-based system."
  },
  "aaai2021_main_sdgnnlearningnoderepresentationforsigneddirectednetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SDGNN: Learning Node Representation for Signed Directed Networks",
    "authors": [
      "Junjie Huang",
      "Huawei Shen",
      "Liang Hou",
      "Xueqi Cheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16093",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16093/15900",
    "published": "2021-02",
    "summary": "Network embedding is aimed at mapping nodes in a network into low-dimensional vector representations. Graph Neural Networks (GNNs) have received widespread attention and lead to state-of-the-art performance in learning node representations. However, most GNNs only work in unsigned networks, where only positive links exist. It is not trivial to transfer these models to signed directed networks, which are widely observed in the real world yet less studied. In this paper, we first review two fundamental sociological theories (i.e., status theory and balance theory) and conduct empirical studies on real-world datasets to analyze the social mechanism in signed directed networks. Guided by related socio- logical theories, we propose a novel Signed Directed Graph Neural Networks model named SDGNN to learn node embeddings for signed directed networks. The proposed model simultaneously reconstructs link signs, link directions, and signed directed triangles. We validate our model\u2019s effectiveness on five real-world datasets, which are commonly used as the benchmark for signed network embeddings. Experiments demonstrate the proposed model outperforms existing models, including feature-based methods, network embedding methods, and several GNN methods."
  },
  "aaai2021_main_thecausallearningofretaildelinquency": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "The Causal Learning of Retail Delinquency",
    "authors": [
      "Yiyan Huang",
      "Cheuk Hang Leung",
      "Xing Yan",
      "Qi Wu",
      "Nanbo Peng",
      "Dongdong Wang",
      "Zhixiang Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16094",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16094/15901",
    "published": "2021-02",
    "summary": "This paper focuses on the expected difference in borrower's repayment when there is a change in the lender's credit decisions. Classical estimators overlook the confounding effects and hence the estimation error can be magnificent. As such, we propose another approach to construct the estimators such that the error can be greatly reduced. The proposed estimators are shown to be unbiased, consistent, and robust through a combination of theoretical analysis and numerical testing. Moreover, we compare the power of estimating the causal quantities between the classical estimators and the proposed estimators. The comparison is tested across a wide range of models, including linear regression models, tree-based models, and neural network-based models, under different simulated datasets that exhibit different levels of causality, different degrees of nonlinearity, and different distributional properties. Most importantly, we apply our approaches to a large observational dataset provided by a global technology firm that operates in both the e-commerce and the lending business. We find that the relative reduction of estimation error is strikingly substantial if the causal effects are accounted for correctly."
  },
  "aaai2021_main_deepportfoliooptimizationviadistributionalpredictionofresidualfactors": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Portfolio Optimization via Distributional Prediction of Residual Factors",
    "authors": [
      "Kentaro Imajo",
      "Kentaro Minami",
      "Katsuya Ito",
      "Kei Nakagawa"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16095",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16095/15902",
    "published": "2021-02",
    "summary": "Recent developments in deep learning techniques have motivated intensive research in machine learning-aided stock trading strategies. However, since the financial market has a highly non-stationary nature hindering the application of typical data-hungry machine learning methods, leveraging financial inductive biases is important to ensure better sample efficiency and robustness. In this study, we propose a novel method of constructing a portfolio based on predicting the distribution of a financial quantity called residual factors, which is known to be generally useful for hedging the risk exposure to common market factors. The key technical ingredients are twofold. First, we introduce a computationally efficient extraction method for the residual information, which can be easily combined with various prediction algorithms. Second, we propose a novel neural network architecture that allows us to incorporate widely acknowledged financial inductive biases such as amplitude invariance and time-scale invariance. We demonstrate the efficacy of our method on U.S. and Japanese stock market data. Through ablation experiments, we also verify that each individual technique contributes to improving the performance of trading strategies. We anticipate our techniques may have wide applications in various financial problems."
  },
  "aaai2021_main_complexcoordinate-basedmeta-analysiswithprobabilisticprogramming": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Complex Coordinate-Based Meta-Analysis with Probabilistic Programming",
    "authors": [
      "Valentin Iovene",
      "Gaston E Zanitti",
      "Demian Wassermann"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16096",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16096/15903",
    "published": "2021-02",
    "summary": "With the growing number of published functional magnetic resonance imaging (fMRI) studies, meta-analysis databases and models have become an integral part of brain mapping research. Coordinate-based meta-analysis (CBMA) databases are built by extracting both coordinates of reported peak activations and term associations using natural language processing techniques from neuroimaging studies. Solving term-based queries on these databases makes it possible to obtain statistical maps of the brain related to specific cognitive processes. However, existing tools for analysing CBMA data are limited in their expressivity to propositional logic, restricting the variety of their queries. Moreover, with tools like Neurosynth, term-based queries on multiple terms often lead to power failure, because too few studies from the database contribute to the statistical estimations. We design a probabilistic domain-specific language (DSL) standing on Datalog and one of its probabilistic extensions, CP-Logic, for expressing and solving complex logic-based queries. We show how CBMA databases can be encoded as probabilistic programs. Using the joint distribution of their Bayesian network translation, we show that solutions of queries on these programs compute the right probability distributions of voxel activations. We explain how recent lifted query processing algorithms make it possible to scale to the size of large neuroimaging data, where knowledge compilation techniques fail to solve queries fast enough for practical applications. Finally, we introduce a method for relating studies to terms probabilistically, leading to better solutions for two-term conjunctive queries (CQs) on smaller databases. We demonstrate results for two-term CQs, both on simulated meta-analysis databases and on the widely used Neurosynth database."
  },
  "aaai2021_main_whoyouwouldliketosharewith?astudyofsharerecommendationinsociale-commerce": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Who You Would Like to Share With? A Study of Share Recommendation in Social E-commerce",
    "authors": [
      "Houye Ji",
      "Junxiong Zhu",
      "Xiao Wang",
      "Chuan Shi",
      "Bai Wang",
      "Xiaoye Tan",
      "Yanghua Li",
      "Shaojian He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16097",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16097/15904",
    "published": "2021-02",
    "summary": "The prosperous development of social e-commerce has spawned diverse recommendation demands, and accompanied a new recommendation paradigm, share recommendation. Signi\ufb01cantly different from traditional binary recommendations (e.g., item recommendation and friend recommendation), share recommendation models ternary interactions among \u3008 User, Item, Friend \u3009 , which aims to recommend a most likely friend to a user who would like to share a speci\ufb01c item, progressively becoming an indispensable service in social e-commerce. Seamlessly integrating the social relations and purchase behaviours, share recommendation improves user stickiness and monetizes the user in\ufb02uence, meanwhile encountering three unique challenges: rich heterogeneous information, complex ternary interaction, and asymmetric share action. In this paper, we \ufb01rst study the share recommendation problem and propose a heterogeneous graph neural network based share recommendation model, called HGSRec. Speci\ufb01cally, HGSRec delicately designs a tripartite heterogeneous GNNs to describe the multifold characteristics of users and items, and then dynamically fuses them via capturing potential ternary dependency with a dual co-attention mechanism, followed by a transitive triplet representation to depict the asymmetry of share action and predict whether share action happens. Of\ufb02ine experiments demonstrate the superiority of the proposed HGSRec with signi\ufb01cant improvements (11.7%-14.5%) over the state-of-the-arts, and online A/B testing on Taobao platform further demonstrates the high industrial practicability and stability of HGSRec."
  },
  "aaai2021_main_estimatingcalibratedindividualizedsurvivalcurveswithdeeplearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Estimating Calibrated Individualized Survival Curves with Deep Learning",
    "authors": [
      "Fahad Kamran",
      "Jenna Wiens"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16098",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16098/15905",
    "published": "2021-02",
    "summary": "In survival analysis, deep learning approaches have been proposed for estimating an individual's probability of survival over some time horizon. Such approaches can capture complex non-linear relationships, without relying on restrictive assumptions regarding the relationship between an individual's characteristics and their underlying survival process. To date, however, these methods have focused primarily on optimizing discriminative performance and have ignored model calibration. Well-calibrated survival curves present realistic and meaningful probabilistic estimates of the true underlying survival process for an individual. However, due to the lack of ground-truth regarding the underlying stochastic process of survival for an individual, optimizing and measuring calibration in survival analysis is an inherently difficult task. In this work, we i) highlight the shortcomings of existing approaches in terms of calibration and ii) propose a new training scheme for optimizing deep survival analysis models that maximizes discriminative performance, subject to good calibration. Compared to state-of-the-art approaches across two publicly available datasets, our proposed training scheme leads to significant improvements in calibration, while maintaining good discriminative performance."
  },
  "aaai2021_main_deepcontextualclinicalpredictionwithreversedistillation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Contextual Clinical Prediction with Reverse Distillation",
    "authors": [
      "Rohan Kodialam",
      "Rebecca Boiarsky",
      "Justin Lim",
      "Aditya Sai",
      "Neil Dixit",
      "David Sontag"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16099",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16099/15906",
    "published": "2021-02",
    "summary": "Healthcare providers are increasingly using machine learning to predict patient outcomes to make meaningful interventions. However, despite innovations in this area, deep learning models often struggle to match performance of shallow linear models in predicting these outcomes, making it difficult to leverage such techniques in practice. In this work, motivated by the task of clinical prediction from insurance claims, we present a new technique called reverse distillation which pretrains deep models by using high-performing linear models for initialization. We make use of the longitudinal structure of insurance claims datasets to develop Self Attention with Reverse Distillation, or SARD, an architecture that utilizes a combination of contextual embedding, temporal embedding and self-attention mechanisms and most critically is trained via reverse distillation. SARD outperforms state-of-the-art methods on multiple clinical prediction outcomes, with ablation studies revealing that reverse distillation is a primary driver of these improvements. Code is available at https://github.com/clinicalml/omop-learn."
  },
  "aaai2021_main_learningtostopdynamicsimulationmonte-carlotreesearch": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning to Stop: Dynamic Simulation Monte-Carlo Tree Search",
    "authors": [
      "Li-Cheng Lan",
      "Ti-Rong Wu",
      "I-Chen Wu",
      "Cho-Jui Hsieh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16100",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16100/15907",
    "published": "2021-02",
    "summary": "Monte Carlo tree search (MCTS) has achieved state-of-the-art results in many domains such as Go and Atari games when combining with deep neural networks (DNNs). When more simulations are executed, MCTS can achieve higher performance but also requires enormous amounts of CPU and GPU resources. However, not all states require a long searching time to identify the best action that the agent can find. For example, in 19x19 Go and NoGo, we found that for more than half of the states, the best action predicted by DNN remains unchanged even after searching 2 minutes. This implies that a significant amount of resources can be saved if we are able to stop the searching earlier when we are confident with the current searching result. In this paper, we propose to achieve this goal by predicting the uncertainty of the current searching status and use the result to decide whether we should stop searching. With our algorithm, called Dynamic Simulation MCTS (DS-MCTS), we can speed up a NoGo agent trained by AlphaZero 2.5 times faster while maintaining a similar winning rate, which is critical for training and conducting experiments. Also, under the same average simulation count, our method can achieve a 61\\% winning rate against the original program."
  },
  "aaai2021_main_predictinglivelihoodindicatorsfromcommunity-generatedstreet-levelimagery": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Predicting Livelihood Indicators from Community-Generated Street-Level Imagery",
    "authors": [
      "Jihyeon Lee",
      "Dylan Grosz",
      "Burak Uzkent",
      "Sicheng Zeng",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16101",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16101/15908",
    "published": "2021-02",
    "summary": "Major decisions from governments and other large organizations rely on measurements of the populace's well-being, but making such measurements at a broad scale is expensive and thus infrequent in much of the developing world. We propose an inexpensive, scalable, and interpretable approach to predict key livelihood indicators from public crowd-sourced street-level imagery. Such imagery can be cheaply collected and more frequently updated compared to traditional surveying methods, while containing plausibly relevant information for a range of livelihood indicators. We propose two approaches to learn from the street-level imagery: (1) a method that creates multi-household cluster representations by detecting informative objects and (2) a graph-based approach that captures the relationships between images. By visualizing what features are important to a model and how they are used, we can help end-user organizations understand the models and offer an alternate approach for index estimation that uses cheaply obtained roadway features. By comparing our results against ground data collected in nationally-representative household surveys, we demonstrate the performance of our approach in accurately predicting indicators of poverty, population, and health and its scalability by testing in two different countries, India and Kenya.Our code is available at https://github.com/sustainlab-group/mapillarygcn."
  },
  "aaai2021_main_deepconservationalatent-dynamicsmodelforexactsatisfactionofphysicalconservationlaws": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Conservation: A Latent-Dynamics Model for Exact Satisfaction of Physical Conservation Laws",
    "authors": [
      "Kookjin Lee",
      "Kevin T. Carlberg"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16102",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16102/15909",
    "published": "2021-02",
    "summary": "This work proposes an approach for latent-dynamics learning that exactly enforces physical conservation laws. The method comprises two steps. First, the method computes a low-dimensional embedding of the high-dimensional dynamical-system state using deep convolutional autoencoders. This defines a low-dimensional nonlinear manifold on which the state is subsequently enforced to evolve. Second, the method defines a latent-dynamics model that associates with the solution to a constrained optimization problem. Here, the objective function is defined as the sum of squares of conservation-law violations over control volumes within a finite-volume discretization of the problem; nonlinear equality constraints explicitly enforce conservation over prescribed subdomains of the problem. Under modest conditions, the resulting dynamics model guarantees that the time-evolution of the latent state exactly satisfies conservation laws over the prescribed subdomains."
  },
  "aaai2021_main_two-streamconvolutionaugmentedtransformerforhumanactivityrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Two-Stream Convolution Augmented Transformer for Human Activity Recognition",
    "authors": [
      "Bing Li",
      "Wei Cui",
      "Wei Wang",
      "Le Zhang",
      "Zhenghua Chen",
      "Min Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16103",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16103/15910",
    "published": "2021-02",
    "summary": "Recognition of human activities is an important task due to its far-reaching applications such as healthcare system, context-aware applications, and security monitoring. Recently, WiFi based human activity recognition (HAR) is becoming ubiquitous due to its non-invasiveness. Existing WiFi-based HAR methods regard WiFi signals as a temporal sequence of channel state information (CSI), and employ deep sequential models (e.g., RNN, LSTM) to automatically capture channel-over-time features. Although being remarkably effective, they suffer from two major drawbacks. Firstly, the granularity of a single temporal point is blindly elementary for representing meaningful CSI patterns.Secondly, the time-over-channel features are also important, and could be a natural data augmentation. To address the drawbacks, we propose a novel Two-stream Convolution Augmented Human Activity Transformer (THAT) model. Our model proposes to utilize a two-stream structure to capture both time-over-channel and channel-over-time features, and use the multi-scale convolution augmented transformer to capture range-based patterns. Extensive experiments on four real experiment datasets demonstrate that our model outperforms state-of-the-art models in terms of both effectiveness and efficiency."
  },
  "aaai2021_main_trafficflowpredictionwithvehicletrajectories": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Traffic Flow Prediction with Vehicle Trajectories",
    "authors": [
      "Mingqian Li",
      "Panrong Tong",
      "Mo Li",
      "Zhongming Jin",
      "Jianqiang Huang",
      "Xian-Sheng Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16104",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16104/15911",
    "published": "2021-02",
    "summary": "This paper proposes a spatiotemporal deep learning framework, Trajectory-based Graph Neural Network (TrGNN), that mines the underlying causality of flows from historical vehicle trajectories and incorporates that into road traffic prediction. The vehicle trajectory transition patterns are studied to explicitly model the spatial traffic demand via graph propagation along the road network; an attention mechanism is designed to learn the temporal dependencies based on neighborhood traffic status; and finally, a fusion of multi-step prediction is integrated into the graph neural network design. The proposed approach is evaluated with a real-world trajectory dataset. Experiment results show that the proposed TrGNN model achieves over 5% error reduction when compared with the state-of-the-art approaches across all metrics for normal traffic, and up to 14% for atypical traffic during peak hours or abnormal events. The advantage of trajectory transitions especially manifest itself in inferring high fluctuation of flows as well as non-recurrent flow patterns."
  },
  "aaai2021_main_revmanrevenue-awaremulti-taskonlineinsurancerecommendation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "RevMan: Revenue-aware Multi-task Online Insurance Recommendation",
    "authors": [
      "Yu Li",
      "Yi Zhang",
      "Lu Gan",
      "Gengwei Hong",
      "Zimu Zhou",
      "Qiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16105",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16105/15912",
    "published": "2021-02",
    "summary": "Online insurance is a new type of e-commerce with exponential growth. An effective recommendation model that maximizes the total revenue of insurance products listed in multiple customized sales scenarios is crucial for the success of online insurance business. Prior recommendation models are ineffective because they fail to characterize the complex relatedness of insurance products in multiple sales scenarios and maximize the overall conversion rate rather than the total revenue. Even worse, it is impractical to collect training data online for total revenue maximization due to the business logic of online insurance. We propose RevMan, a Revenue-aware Multi-task Network for online insurance recommendation. RevMan adopts an adaptive attention mechanism to allow effective feature sharing among complex insurance products and sales scenarios. It also designs an efficient offline learning mechanism to learn the rank that maximizes the expected total revenue, by reusing training data and model for conversion rate maximization. Extensive offline and online evaluations show that RevMan outperforms the state-of-the-art recommendation systems for e-commerce."
  },
  "aaai2021_main_meingamecreateagamecharacterfacefromasingleportrait": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "MeInGame: Create a Game Character Face from a Single Portrait",
    "authors": [
      "Jiangke Lin",
      "Yi Yuan",
      "Zhengxia Zou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16106",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16106/15913",
    "published": "2021-02",
    "summary": "Many deep learning based 3D face reconstruction methods have been proposed recently, however, few of them have applications in games. Current game character customization systems either require players to manually adjust considerable face attributes to obtain the desired face, or have limited freedom of facial shape and texture. In this paper, we propose an automatic character face creation method that predicts both facial shape and texture from a single portrait, and it can be integrated into most existing 3D games. Although 3D Morphable Face Model (3DMM) based methods can restore accurate 3D faces from single images, the topology of 3DMM mesh is different from the meshes used in most games. To acquire fidelity texture, existing methods require a large amount of face texture data for training, while building such datasets is time-consuming and laborious. Besides, such a dataset collected under laboratory conditions may not generalized well to in-the-wild situations. To tackle these problems, we propose 1) a low-cost facial texture acquisition method, 2) a shape transfer algorithm that can transform the shape of a 3DMM mesh to games, and 3) a new pipeline for training 3D game face reconstruction networks. The proposed method not only can produce detailed and vivid game characters similar to the input portrait, but can also eliminate the influence of lighting and occlusions. Experiments show that our method outperforms state-of-the-art methods used in games. Code and dataset are available at https://github.com/FuxiCV/MeInGame."
  },
  "aaai2021_main_community-awaremulti-tasktransportationdemandprediction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Community-Aware Multi-Task Transportation Demand Prediction",
    "authors": [
      "Hao Liu",
      "Qiyu Wu",
      "Fuzhen Zhuang",
      "Xinjiang Lu",
      "Dejing Dou",
      "Hui Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16107",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16107/15914",
    "published": "2021-02",
    "summary": "Transportation demand prediction is of great importance to urban governance and has become an essential function in many online applications. While many efforts have been made for regional transportation demand prediction, predicting the diversified transportation demand for different communities (e.g., the aged, the juveniles) remains an unexplored problem. However, this task is challenging because of the joint influence of spatio-temporal correlation among regions and implicit correlation among different communities. To this end, in this paper, we propose the Multi-task Spatio-Temporal Network with Mutually-supervised Adaptive task grouping (Ada-MSTNet) for community-aware transportation demand prediction.Specifically, we first construct a sequence of multi-view graphs from both spatial and community perspectives, and devise a spatio-temporal neural network to simultaneously capture the sophisticated correlations between regions and communities, respectively. Then, we propose an adaptively clustered multi-task learning module,where the prediction of each region-community specific transportation demand is regarded as distinct task. Moreover, a mutually supervised adaptive task grouping strategy is introduced to softly cluster each task into different task groups, by leveraging the supervision signal from one another graph view. In such a way, Ada-MSTNet is not only able to share common knowledge among highly related communities and regions, but also shield the noise from unrelated tasks in an end-to-end fashion. Finally, extensive experiments on two real-world datasets demonstrate the effectiveness of our approach compared with seven baselines."
  },
  "aaai2021_main_asynchronousstochasticgradientdescentforextreme-scalerecommendersystems": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Asynchronous Stochastic Gradient Descent for Extreme-Scale Recommender Systems",
    "authors": [
      "Lewis Liu",
      "Kun Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16108",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16108/15915",
    "published": "2021-02",
    "summary": "Recommender systems are influential for many internet applications. As the size of the dataset provided for a recommendation model grows rapidly, how to utilize such amount of data effectively matters a lot. For a typical Click-Through-Rate(CTR) prediction model, the amount of daily samples can probably be up to hundreds of terabytes, which reaches dozens of petabytes at an extreme-scale when we take several days into consideration. Such data makes it essential to train the model parallelly and continuously. Traditional asynchronous stochastic gradient descent (ASGD) and its variants are proved efficient but often suffer from stale gradients. Hence, the model convergence tends to be worse as more workers are used. Moreover, the existing adaptive optimizers, which are friendly to sparse data, stagger in long-term training due to the significant imbalance between new and accumulated gradients.To address the challenges posed by extreme-scale data, we propose: 1) Staleness normalization and data normalization to eliminate the turbulence of stale gradients when training asynchronously in hundreds and thousands of workers; 2) SWAP, a novel framework for adaptive optimizers to balance the new and historical gradients by taking sampling period into consideration. We implement these approaches in TensorFlow and apply them to CTR tasks in real-world e- commerce scenarios. Experiments show that the number of workers in asynchronous training can be extended to 3000 with guaranteed convergence, and the final AUC is improved by more than 5 percentage."
  },
  "aaai2021_main_in-gameresidentialhomeplanningviavisualcontext-awareglobalrelationlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "In-game Residential Home Planning via Visual Context-aware Global Relation Learning",
    "authors": [
      "Lijuan Liu",
      "Yin Yang",
      "Yi Yuan",
      "Tianjia Shao",
      "He Wang",
      "Kun Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16109",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16109/15916",
    "published": "2021-02",
    "summary": "In this paper, we propose an effective global relation learning algorithm to recommend an appropriate location of a building unit for in-game customization of residential home complex. Given a construction layout, we propose a visual context-aware graph generation network that learns the implicit global relations among the scene components and infers the location of a new building unit. The proposed network takes as input the scene graph and the corresponding top-view depth image. It provides the location recommendations for a newly added building units by learning an auto-regressive edge distribution conditioned on existing scenes. We also introduce a global graph-image matching loss to enhance the awareness of essential geometry semantics of the site. Qualitative and quantitative experiments demonstrate that the recommended location well reflects the implicit spatial rules of components in the residential estates, and it is instructive and practical to locate the building units in the 3D scene of the complex construction."
  },
  "aaai2021_main_relationalclassificationofbiologicalcellsinmicroscopyimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Relational Classification of Biological Cells in Microscopy Images",
    "authors": [
      "Ping Liu",
      "Mustafa Bilgic"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16110",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16110/15917",
    "published": "2021-02",
    "summary": "We investigate the relational classification of biological cells in 2D microscopy images. Rather than treating each cell image independently, we investigate whether and how the neighborhood information of a cell can be informative for its prediction.We propose a Relational Long Short-Term Memory (R-LSTM) algorithm, coupled with auto-encoders and convolutional neural networks, that can learn from both annotated and unlabeled microscopy images and that can utilize both the local and neighborhood information to perform an improved classification of biological cells. Experimental results on both synthetic and real datasets show that R-LSTM performs comparable to or better than six baselines."
  },
  "aaai2021_main_deepstyletransferforlinedrawings": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Style Transfer for Line Drawings",
    "authors": [
      "Xueting Liu",
      "Wenliang Wu",
      "Huisi Wu",
      "Zhenkun Wen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16111",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16111/15918",
    "published": "2021-02",
    "summary": "Line drawings are frequently used to illustrate ideas and concepts in digital documents and presentations. To compose a line drawing, it is common for users to retrieve multiple line drawings from the Internet and combine them as one image. However, different line drawings may have different line styles and are visually inconsistent when put together. In order that the line drawings can have consistent looks, in this paper, we make the first attempt to perform style transfer for line drawings. The key of our design lies in the fact that centerline plays a very important role in preserving line topology and extracting style features. With this finding, we propose to formulate the style transfer problem as a centerline stylization problem and solve it via a novel style-guided image-to-image translation network. Results and statistics show that our method significantly outperforms the existing methods both visually and quantitatively."
  },
  "aaai2021_main_rnasecondarystructurerepresentationnetworkforrna-proteinsbindingprediction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "RNA Secondary Structure Representation Network for RNA-proteins Binding Prediction",
    "authors": [
      "Ziyi Liu",
      "Fulin Luo",
      "Bo Du"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16112",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16112/15919",
    "published": "2021-02",
    "summary": "RNA-binding proteins (RBPs) play a significant part in several biological processes in the living cell, such as gene regulation and mRNA localization. Several deep learning methods, especially the model based on convolutional neural network(CNN), have been used to predict the binding sites. However, previous methods fail to represent RNA secondary structure features. The traditional deep learning methods generally transform the RNA secondary structure to a regular matrix that cannot reveal the topological structure information of RNA. To effectively extract the structure features of RNA, we propose an RNA secondary structure representation network (RNASSR-Net) based on graph convolutional neural network (GCN) and convolution neural network (CNN) for RBP binding prediction. RNASSR-Net constructs the graph model derived from the RNA secondary structure to learn the topological properties of RNA. Then, it obtains the spatial importance of each base in RNA with CNN to guide the representation of the RNA secondary structure. Finally, RNASSR-Net combines the structure and sequence features to predict the binding sites. Experimental results demonstrate the proposed method outperforms a few state-of-the-art methods on the benchmark datasets and gets a higher improvement on the small-size data. Besides, the proposed RNASSR-Net is also used to detect the accurate motifs compared with the experimentally verified motifs, which reveals the binding region location and RNA structure interpretation for some biological guidance in the future."
  },
  "aaai2021_main_pantherpathwayaugmentednonnegativetensorfactorizationforhigher-orderfeaturelearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PANTHER: Pathway Augmented Nonnegative Tensor Factorization for HighER-order Feature Learning",
    "authors": [
      "Yuan Luo",
      "Chengsheng Mao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16113",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16113/15920",
    "published": "2021-02",
    "summary": "Genetic pathways usually encode molecular mechanisms that can inform targeted interventions. It is often challenging for existing machine learning approaches to jointly model genetic pathways (higher-order features) and variants (atomic features), and present to clinicians interpretable models. In order to build more accurate and better interpretable machine learning models for genetic medicine, we introduce Pathway Augmented Nonnegative Tensor factorization for HighER-order feature learning (PANTHER). PANTHER selects informative genetic pathways that directly encode molecular mechanisms. We apply genetically motivated constrained tensor factorization to group pathways in a way that reflects molecular mechanism interactions. We then train a softmax classifier for disease types using the identified pathway groups. We evaluated PANTHER against multiple state-of-the-art constrained tensor/matrix factorization models, as well as group guided and Bayesian hierarchical models. PANTHER outperforms all state-of-the-art comparison models significantly (p<0.05). Our experiments on large scale Next Generation Sequencing (NGS) and whole-genome genotyping datasets also demonstrated wide applicability of PANTHER. We performed feature analysis in predicting disease types, which suggested insights and benefits of the identified pathway groups."
  },
  "aaai2021_main_programmaticstrategiesforreal-timestrategygames": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Programmatic Strategies for Real-Time Strategy Games",
    "authors": [
      "Julian R. H. Mari\u00f1o",
      "Rubens O. Moraes",
      "Tassiana C. Oliveira",
      "Claudio\n      Toledo",
      "Levi H. S. Lelis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16114",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16114/15921",
    "published": "2021-02",
    "summary": "Search-based systems have shown to be effective for planning in zero-sum games. However, search-based approaches have important disadvantages. First, the decisions of search algorithms are mostly non-interpretable, which is problematic in domains where predictability and trust are desired such as commercial games. Second, the computational complexity of search-based algorithms might limit their applicability, especially in contexts where resources are shared among other tasks such as graphic rendering. In this work we introduce a system for synthesizing programmatic strategies for a real-time strategy (RTS) game. In contrast with search algorithms, programmatic strategies are more amenable to explanations and tend to be efficient, once the program is synthesized. Our system uses a novel algorithm for simplifying domain-specific languages (DSLs) and a local search algorithm that synthesizes programs with self play. We performed a user study where we enlisted four professional programmers to develop programmatic strategies for mRTS, a minimalist RTS game. Our results show that the programs synthesized by our approach can outperform search algorithms and be competitive with programs written by the programmers."
  },
  "aaai2021_main_capturinguncertaintyinunsupervisedgpstrajectorysegmentationusingbayesiandeeplearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Capturing Uncertainty in Unsupervised GPS Trajectory Segmentation Using Bayesian Deep Learning",
    "authors": [
      "Christos Markos",
      "James J. Q. Yu",
      "Richard Yi Da Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16115",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16115/15922",
    "published": "2021-02",
    "summary": "Intelligent transportation management requires not only statistical information on users' mobility patterns, but also knowledge of their corresponding transportation modes. While GPS trajectories can be readily obtained from GPS sensors found in modern smartphones and vehicles, these massive geospatial data are neither automatically annotated nor segmented by transportation mode, subsequently complicating transportation mode identification. In addition, predictive uncertainty caused by the learned model parameters or variable noise in GPS sensor readings typically remains unaccounted for. To jointly address the above issues, we propose a Bayesian deep learning framework for unsupervised GPS trajectory segmentation. After unlabeled GPS trajectories are preprocessed into sequences of motion features, they are used in unsupervised training of a channel-calibrated temporal convolutional neural network for timestep-level transportation mode identification. At test time, we approximate variational inference via Monte Carlo dropout sampling, leveraging the mean and variance of the predicted distributions to classify each input timestep and estimate its predictive uncertainty, respectively. The proposed approach outperforms both its non-Bayesian variant and established GPS trajectory segmentation baselines on Microsoft's Geolife dataset without using any labels."
  },
  "aaai2021_main_low-rankregistrationbasedmanifoldsforconvection-dominatedpdes": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Low-Rank Registration Based Manifolds for Convection-Dominated PDEs",
    "authors": [
      "Rambod Mojgani",
      "Maciej Balajewicz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16116",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16116/15923",
    "published": "2021-02",
    "summary": "We develop an auto-encoder-type nonlinear dimensionality reduction algorithm to enable the construction of reduced order models of systems governed by convection-dominated nonlinear partial differential equations (PDEs), i.e. snapshots of solutions with large Kolmogorov n-width. Although several existing nonlinear manifold learning methods, such as LLE, ISOMAP, MDS, etc., appear as compelling candidates to reduce the dimensionality of such data, most are not applicable to reduced order modeling of PDEs, because: (i) they typically lack a straightforward mapping from the latent space to the high-dimensional physical space, and (ii) the identified latent variables are often difficult to interpret. In our proposed method, these limitations are overcome by training a low-rank diffeomorphic spatio-temporal grid that registers the output sequence of the PDEs on a non-uniform parameter/time-varying grid, such that the Kolmogorov n-width of the mapped data on the learned grid is minimized. We demonstrate the efficacy and interpretability of our proposed approach on several challenging manufactured computer vision-inspired tasks and physical systems."
  },
  "aaai2021_main_symbolicmusicgenerationwithtransformer-gans": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Symbolic Music Generation with Transformer-GANs",
    "authors": [
      "Aashiq Muhamed",
      "Liang Li",
      "Xingjian Shi",
      "Suri Yaddanapudi",
      "Wayne Chi",
      "Dylan\n      Jackson",
      "Rahul Suresh",
      "Zachary C. Lipton",
      "Alex J. Smola"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16117",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16117/15924",
    "published": "2021-02",
    "summary": "Autoregressive models using Transformers have emerged as thedominantapproachformusicgenerationwiththegoal of synthesizing minute-long compositions that exhibit large-scale musical structure. These models are commonly trained by minimizing the negative log-likelihood (NLL) of the observed sequence in an autoregressive manner. Unfortunately, thequalityofsamplesfromthesemodelstendstodegrade significantly for long sequences, a phenomenon attributed to exposure bias. Fortunately, we are able to detect these failures with classifiers trained to distinguish between real and sampled sequences, an observation that motivates our exploration of adversarial losses to complement the NLL objective. We use a pre-trained Span-BERT model for the discriminator of the GAN, which in our experiments helped with training stability. We use the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. This makes discrete sequences amenable to optimization in GANs. In addition, we break the sequences into smaller chunks to ensure that we stay within a given memory budget. We demonstrate via human evaluations and a new discriminative metric that the music generated by our approach outperforms a baseline trained with likelihood maximization, the state-of-the-art Music Transformer, and other GANs used for sequence generation. 57% of people prefer music generated via our approach while 43% prefer Music Transformer."
  },
  "aaai2021_main_bringingumapclosertothespeedoflightwithgpuacceleration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Bringing UMAP Closer to the Speed of Light with GPU Acceleration",
    "authors": [
      "Corey J. Nolet",
      "Victor Lafargue",
      "Edward Raff",
      "Thejaswi Nanditale",
      "Tim\n      Oates",
      "John Zedlewski",
      "Joshua Patterson"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16118",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16118/15925",
    "published": "2021-02",
    "summary": "The Uniform Manifold Approximation and Projection (UMAP) algorithm has become widely popular for its ease of use, quality of results, and support for exploratory, unsupervised, supervised, and semi-supervised learning. While many algorithms can be ported to a GPU in a simple and direct fashion, such efforts have resulted in inefficent and inaccurate versions of UMAP. We show a number of techniques that can be used to make a faster and more faithful GPU version of UMAP, and obtain speedups of up to 100x in practice. Many of these design choices/lessons are general purpose and may inform the conversion of other graph and manifold learning algorithms to use GPUs.Our implementation has been made publicly available as part of the open source RAPIDS cuML library (https://github.com/rapidsai/cuml)."
  },
  "aaai2021_main_deepjust-in-timeinconsistencydetectionbetweencommentsandsourcecode": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Just-In-Time Inconsistency Detection Between Comments and Source Code",
    "authors": [
      "Sheena Panthaplackel",
      "Junyi Jessy Li",
      "Milos Gligoric",
      "Raymond J. Mooney"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16119",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16119/15926",
    "published": "2021-02",
    "summary": "Natural language comments convey key aspects of source code such as implementation, usage, and pre- and post-conditions. Failure to update comments accordingly when the corresponding code is modified introduces inconsistencies, which is known to lead to confusion and software bugs. In this paper, we aim to detect whether a comment becomes inconsistent as a result of changes to the corresponding body of code, in order to catch potential inconsistencies just-in-time, i.e., before they are committed to a code base. To achieve this, we develop a deep-learning approach that learns to correlate a comment with code changes. By evaluating on a large corpus of comment/code pairs spanning various comment types, we show that our model outperforms multiple baselines by significant margins. For extrinsic evaluation, we show the usefulness of our approach by combining it with a comment update model to build a more comprehensive automatic comment maintenance system which can both detect and resolve inconsistent comments based on code changes."
  },
  "aaai2021_main_xraysynrealisticviewsynthesisfromasingleradiographthroughctpriors": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "XraySyn: Realistic View Synthesis From a Single Radiograph Through CT Priors",
    "authors": [
      "Cheng Peng",
      "Haofu Liao",
      "Gina Wong",
      "Jiebo Luo",
      "S. Kevin Zhou",
      "Rama\n      Chellappa"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16120",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16120/15927",
    "published": "2021-02",
    "summary": "A radiograph visualizes the internal anatomy of a patient through the use of X-ray, which projects 3D information onto a 2D plane. Hence, radiograph analysis naturally requires physicians to relate their prior knowledge about 3D human anatomy to 2D radiographs. Synthesizing novel radiographic views in a small range can assist physicians in interpreting anatomy more reliably; however, radiograph view synthesis is heavily ill-posed, lacking in paired data, and lacking in differentiable operations to leverage learning-based approaches. To address these problems, we use Computed Tomography (CT) for radiograph simulation and design a differentiable projection algorithm, which enables us to achieve geometrically consistent transformations between the radiography and CT domains. Our method, XraySyn, can synthesize novel views on real radiographs through a combination of realistic simulation and finetuning on real radiographs. To the best of our knowledge, this is the first work on radiograph view synthesis. We show that by gaining an understanding of radiography in 3D space, our method can be applied to radiograph bone extraction and suppression without requiring groundtruth bone labels."
  },
  "aaai2021_main_pragmaticcodeautocomplete": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Pragmatic Code Autocomplete",
    "authors": [
      "Gabriel Poesia",
      "Noah Goodman"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16121",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16121/15928",
    "published": "2021-02",
    "summary": "Human language is ambiguous, with intended meanings recovered via pragmatic reasoning in context. Such reliance on context is essential for the efficiency of human communication. Programming languages, in stark contrast, are defined by unambiguous grammars. In this work, we aim to make programming languages more concise by allowing programmers to utilize a controlled level of ambiguity. Specifically, we allow single-character abbreviations for common keywords and identifiers. Our system first proposes a set of strings that can be abbreviated by the user. Using only 100 abbreviations, we observe that a large dataset of Python code can be compressed by 15%, a number that can be improved even further by specializing the abbreviations to a particular code base. We then use a contextualized sequence-to-sequence model to rank potential expansions of inputs that include abbreviations. In an offline reconstruction task our model achieves accuracies ranging from 93% to 99%, depending on the programming language and user settings. The model is small enough to run on a commodity CPU in real-time. We evaluate the usability of our system in a user study, integrating it in Microsoft VSCode, a popular code text editor. We observe that our system performs well and is complementary to traditional autocomplete features."
  },
  "aaai2021_main_rareberttransformerarchitectureforrarediseasepatientidentificationusingadministrativeclaims": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "RareBERT: Transformer Architecture for Rare Disease Patient Identification using Administrative Claims",
    "authors": [
      "PKS Prakash",
      "Srinivas Chilukuri",
      "Nikhil Ranade",
      "Shankar Viswanathan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16122",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16122/15929",
    "published": "2021-02",
    "summary": "A rare disease is any disease that affects a very small percentage (1 in 1,500) of population. It is estimated that there are nearly 7,000 rare disease affecting 30 million patients in the U. S. alone. Most of the patients suffering from rare diseases experience multiple misdiagnoses and may never be diagnosed correctly. This is largely driven by the low prevalence of the disease that results in a lack of awareness among healthcare providers. There have been efforts from machine learning researchers to develop predictive models to help diagnose patients using healthcare datasets such as electronic health records and administrative claims. Most recently, transformer models have been applied to predict diseases BEHRT, G-BERT and Med-BERT. However, these have been developed specifically for electronic health records (EHR) and have not been designed to address rare disease challenges such as class imbalance, partial longitudinal data capture, and noisy labels. As a result, they deliver poor performance in predicting rare diseases compared with baselines. Besides, EHR datasets are generally confined to the hospital systems using them and do not capture a wider sample of patients thus limiting the availability of sufficient rare dis-ease patients in the dataset. To address these challenges, we introduced an extension of the BERT model tailored for rare disease diagnosis called RareBERT which has been trained on administrative claims datasets. RareBERT extends Med-BERT by including context embedding and temporal reference embedding. Moreover, we introduced a novel adaptive loss function to handle the class imbal-ance. In this paper, we show our experiments on diagnosing X-Linked Hypophosphatemia (XLH), a genetic rare disease. While RareBERT performs significantly better than the baseline models (79.9% AUPRC versus 30% AUPRC for Med-BERT), owing to the transformer architecture, it also shows its robustness in partial longitudinal data capture caused by poor capture of claims with a drop in performance of only 1.35% AUPRC, compared with 12% for Med-BERT and 33.0% for LSTM and 67.4% for boosting trees based baseline."
  },
  "aaai2021_main_queue-learningareinforcementlearningapproachforprovidingqualityofservice": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Queue-Learning: A Reinforcement Learning Approach for Providing Quality of Service",
    "authors": [
      "Majid Raeis",
      "Ali Tizghadam",
      "Alberto Leon-Garcia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16123",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16123/15930",
    "published": "2021-02",
    "summary": "End-to-end delay is a critical attribute of quality of service (QoS) in application domains such as cloud computing and computer networks. This metric is particularly important in tandem service systems, where the end-to-end service is provided through a chain of services. Service-rate control is a common mechanism for providing QoS guarantees in service systems. In this paper, we introduce a reinforcement learning-based (RL-based) service-rate controller that provides probabilistic upper-bounds on the end-to-end delay of the system, while preventing the overuse of service resources. In order to have a general framework, we use queueing theory to model the service systems. However, we adopt an RL-based approach to avoid the limitations of queueing-theoretic methods. In particular, we use Deep Deterministic Policy Gradient (DDPG) to learn the service rates (action) as a function of the queue lengths (state) in tandem service systems. In contrast to existing RL-based methods that quantify their performance by the achieved overall reward, which could be hard to interpret or even misleading, our proposed controller provides explicit probabilistic guarantees on the end-to-end delay of the system. The evaluations are presented for a tandem queueing system with non-exponential inter-arrival and service times, the results of which validate our controller's capability in meeting QoS constraints."
  },
  "aaai2021_main_researchreproducibilityasasurvivalanalysis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Research Reproducibility as a Survival Analysis",
    "authors": [
      "Edward Raff"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16124",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16124/15931",
    "published": "2021-02",
    "summary": "There has been increasing concern within the machine learning community that we are in a reproducibility crisis. As many have begun to work on this problem, all work we are aware of treat the issue of reproducibility as an intrinsic binary property: a paper is or is not reproducible. Instead, we consider modeling the reproducibility of a paper as a survival analysis problem. We argue that this perspective represents a more accurate model of the underlying meta-science question of reproducible research, and we show how a survival analysis allows us to draw new insights that better explain prior longitudinal data. The data and code can be found at https://github.com/EdwardRaff/Research-Reproducibility-Survival-Analysis"
  },
  "aaai2021_main_deeppseudopseudovaluebaseddeeplearningmodelsforcompetingriskanalysis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DeepPseudo: Pseudo Value Based Deep Learning Models for Competing Risk Analysis",
    "authors": [
      "Md Mahmudur Rahman",
      "Koji Matsuo",
      "Shinya Matsuzaki",
      "Sanjay Purushotham"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16125",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16125/15932",
    "published": "2021-02",
    "summary": "Competing Risk Analysis (CRA) aims at the correct estimation of the marginal probability of occurrence of an event in the presence of competing events. Many of the statistical approaches developed for CRA are limited by strong assumptions about the underlying stochastic processes. To overcome these issues and to handle censoring, machine learning approaches for CRA have designed specialized cost functions. However, these approaches are not generalizable, and are computationally expensive. This paper formulates CRA as a cause-specific regression problem and proposes DeepPseudo models, which use simple and effective feed-forward deep neural networks, to predict the cumulative incidence function (CIF) using Aalen-Johansen estimator-based pseudo values. DeepPseudo models capture the time-varying covariate effect on CIF while handling the censored observations. We show how DeepPseudo models can address co-variate dependent censoring by using modified pseudo values. Experiments on real and synthetic datasets demonstrate that our proposed models obtain promising and statistically significant results compared to the state-of-the-art CRA approaches. Furthermore, we show that explainable methods such as Layer-wise Relevance Propagation can be used to interpret the predictions of our DeepPseudo models."
  },
  "aaai2021_main_cardioganattentivegenerativeadversarialnetworkwithdualdiscriminatorsforsynthesisofecgfromppg": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG",
    "authors": [
      "Pritam Sarkar",
      "Ali Etemad"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16126",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16126/15933",
    "published": "2021-02",
    "summary": "Electrocardiogram (ECG) is the electrical measurement of cardiac activity, whereas Photoplethysmogram (PPG) is the optical measurement of volumetric changes in blood circulation. While both signals are used for heart rate monitoring, from a medical perspective, ECG is more useful as it carries additional cardiac information. Despite many attempts toward incorporating ECG sensing in smartwatches or similar wearable devices for continuous and reliable cardiac monitoring, PPG sensors are the main feasible sensing solution available. In order to tackle this problem, we propose CardioGAN, an adversarial model which takes PPG as input and generates ECG as output. The proposed network utilizes an attention-based generator to learn local salient features, as well as dual discriminators to preserve the integrity of generated data in both time and frequency domains. Our experiments show that the ECG generated by CardioGAN provides more reliable heart rate measurements compared to the original input PPG, reducing the error from 9.74 beats per minute (measured from the PPG) to 2.89 (measured from the generated ECG)."
  },
  "aaai2021_main_stockselectionviaspatiotemporalhypergraphattentionnetworkalearningtorankapproach": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Stock Selection via Spatiotemporal Hypergraph Attention Network: A Learning to Rank Approach",
    "authors": [
      "Ramit Sawhney",
      "Shivam Agarwal",
      "Arnav Wadhwa",
      "Tyler Derr",
      "Rajiv Ratn Shah"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16127",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16127/15934",
    "published": "2021-02",
    "summary": "Quantitative trading and investment decision making are intricate financial tasks that rely on accurate stock selection. Despite advances in deep learning that have made significant progress in the complex and highly stochastic stock prediction problem, modern solutions face two significant limitations.They do not directly optimize the target of investment in terms of profit, and treat each stock as independent from the others, ignoring the rich signals between related stocks' temporal price movements. Building on these limitations, we reformulate stock prediction as a learning to rank problem and propose STHAN-SR, a neural hypergraph architecture for stock selection. The key novelty of our work is the proposal of modeling the complex relations between stocks through a hypergraph and a temporal Hawkes attention mechanism to tailor a new spatiotemporal attention hypergraph network architecture to rank stocks based on profit by jointly modeling stock interdependence and the temporal evolution of their prices. Through experiments on three markets spanning over six years of data, we show that STHAN-SR significantly outperforms state-of-the-art neural stock forecasting methods. We validate our design choices through ablative and exploratory analyses over STHAN-SR's spatial and temporal components and demonstrate its practical applicability."
  },
  "aaai2021_main_contentmaskedlosshuman-likebrushstrokeplanninginareinforcementlearningpaintingagent": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Content Masked Loss: Human-Like Brush Stroke Planning in a Reinforcement Learning Painting Agent",
    "authors": [
      "Peter Schaldenbrand",
      "Jean Oh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16128",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16128/15935",
    "published": "2021-02",
    "summary": "The objective of most Reinforcement Learning painting agents is to minimize the loss between a target image and the paint canvas.Human painter artistry emphasizes important features of the target image rather than simply reproducing it.Using adversarial or L2 losses in the RL painting models, although its final output is generally a work of finesse, produces a stroke sequence that is vastly different from that which a human would produce since the model does not have knowledge about the abstract features in the target image.In order to increase the human-like planning of the model without the use of expensive human data, we introduce a new loss function for use with the model's reward function: Content Masked Loss. In the context of robot painting, Content Masked Loss employs an object detection model to extract features which are used to assign higher weight to regions of the canvas that a human would find important for recognizing content. The results, based on 332 human evaluators, show that the digital paintings produced by our Content Masked model show detectable subject matter earlier in the stroke sequence than existing methods without compromising on the quality of the final painting. Our code is available at https://github.com/pschaldenbrand/ContentMaskedLoss."
  },
  "aaai2021_main_stateconetstatisticalecologyneuralnetworksforspeciesdistributionmodeling": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "StatEcoNet: Statistical Ecology Neural Networks for Species Distribution Modeling",
    "authors": [
      "Eugene Seo",
      "Rebecca A. Hutchinson",
      "Xiao Fu",
      "Chelsea Li",
      "Tyler A. Hallman",
      "John Kilbride",
      "W. Douglas Robinson"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16129",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16129/15936",
    "published": "2021-02",
    "summary": "This paper focuses on a core task in computational sustainability and statistical ecology: species distribution modeling (SDM). In SDM, the occurrence pattern of a species on a landscape is predicted by environmental features based on observations at a set of locations. At first, SDM may appear to be a binary classification problem, and one might be inclined to employ classic tools (e.g., logistic regression, support vector machines, neural networks) to tackle it. However, wildlife surveys introduce structured noise (especially under-counting) in the species observations. If unaccounted for, these observation errors systematically bias SDMs. To address the unique challenges of SDM, this paper proposes a framework called StatEcoNet. Specifically, this work employs a graphical generative model in statistical ecology to serve as the skeleton of the proposed computational framework and carefully integrates neural networks under the framework. The advantages of StatEcoNet over related approaches are demonstrated on simulated datasets as well as bird species data. Since SDMs are critical tools for ecological science and natural resource management, StatEcoNet may offer boosted computational and analytical powers to a wide range of applications that have significant social impacts, e.g., the study and conservation of threatened species."
  },
  "aaai2021_main_integratingstaticanddynamicdataforimprovedpredictionofcognitivedeclinesusingaugmentedgenotype-phenotyperepresentations": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Integrating Static and Dynamic Data for Improved Prediction of Cognitive Declines Using Augmented Genotype-Phenotype Representations",
    "authors": [
      "Hoon Seo",
      "Lodewijk Brand",
      "Hua Wang",
      "Feiping Nie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16130",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16130/15937",
    "published": "2021-02",
    "summary": "Alzheimer\u2019s Disease (AD) is a chronic neurodegenerative disease that causes severe problems in patients\u2019 thinking, memory, and behavior. An early diagnosis is crucial to prevent AD progression; to this end, many algorithmic approaches have recently been proposed to predict cognitive decline. However, these predictive models often fail to integrate heterogeneous genetic and neuroimaging biomarkers and struggle to handle missing data. In this work we propose a novel objective function and an associated optimization algorithm to identify cognitive decline related to AD. Our approach is designed to incorporate dynamic neuroimaging data by way of a participant-specific augmentation combined with multimodal data integration aligned via a regression task. Our approach, in order to incorporate additional side-information, utilizes structured regularization techniques popularized in recent AD literature. Armed with the fixed-length vector representation learned from the multimodal dynamic and static modalities, conventional machine learning methods can be used to predict the clinical outcomes associated with AD.Our experimental results show that the proposed augmentation model improves the prediction performance on cognitive assessment scores for a collection of popular machine learning algorithms. The results of our approach are interpreted to validate existing genetic and neuroimaging biomarkers that have been shown to be predictive of cognitive decline."
  },
  "aaai2021_main_gtagraphtruncatedattentionforretrosynthesis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "GTA: Graph Truncated Attention for Retrosynthesis",
    "authors": [
      "Seung-Woo Seo",
      "You Young Song",
      "June Yong Yang",
      "Seohui Bae",
      "Hankook Lee",
      "Jinwoo Shin",
      "Sung Ju Hwang",
      "Eunho Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16131",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16131/15938",
    "published": "2021-02",
    "summary": "Retrosynthesis is the task of predicting reactant molecules from a given product molecule and is, important in organic chemistry because the identification of a synthetic path is as demanding as the discovery of new chemical compounds. Recently, the retrosynthesis task has been solved automatically without human expertise using powerful deep learning models. Recent deep models are primarily based on seq2seq or graph neural networks depending on the function of molecular representation, sequence, or graph. Current state-of-the-art models represent a molecule as a graph, but they require joint training with auxiliary prediction tasks, such as the most probable reaction template or reaction center prediction. Furthermore, they require additional labels by experienced chemists, thereby incurring additional cost. Herein, we propose a novel template-free model, i.e., Graph Truncated Attention (GTA), which leverages both sequence and graph representations by inserting graphical information into a seq2seq model. The proposed GTA model masks the self-attention layer using the adjacency matrix of product molecule in the encoder and applies a new loss using atom mapping acquired from an automated algorithm to the cross-attention layer in the decoder. Our model achieves new state-of-the-art records, i.e., exact match top-1 and top-10 accuracies of 51.1% and 81.6% on the USPTO-50k benchmark dataset, respectively, and 46.0% and 70.0% on the USPTO-full dataset, respectively, both without any reaction class information. The GTA model surpasses prior graph-based template-free models by 2% and 7% in terms of the top-1 and top-10 accuracies on the USPTO-50k dataset, respectively, and by over 6% for both the top-1 and top-10 accuracies on the USPTO-full dataset."
  },
  "aaai2021_main_physics-informeddeeplearningfortrafficstateestimationahybridparadigminformedbysecond-ordertrafficmodels": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Physics-Informed Deep Learning for Traffic State Estimation: A Hybrid Paradigm Informed By Second-Order Traffic Models",
    "authors": [
      "Rongye Shi",
      "Zhaobin Mo",
      "Xuan Di"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16132",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16132/15939",
    "published": "2021-02",
    "summary": "Traffic state estimation (TSE) reconstructs the traffic variables (e.g., density or average velocity) on road segments using partially observed data, which is important for traffic managements. Traditional TSE approaches mainly bifurcate into two categories: model-driven and data-driven, and each of them has shortcomings. To mitigate these limitations, hybrid TSE methods, which combine both model-driven and data-driven, are becoming a promising solution. This paper introduces a hybrid framework, physics-informed deep learning (PIDL), to combine second-order traffic flow models and neural networks to solve the TSE problem. PIDL can encode traffic flow models into deep neural networks to regularize the learning process to achieve improved data efficiency and estimation accuracy. We focus on highway TSE with observed data from loop detectors and probe vehicles, using both density and average velocity as the traffic variables. With numerical examples, we show the use of PIDL to solve a popular second-order traffic flow model, i.e., a Greenshields-based Aw-Rascle-Zhang (ARZ) model, and discover the model parameters. We then evaluate the PIDL-based TSE method using the Next Generation SIMulation (NGSIM) dataset. Experimental results demonstrate the proposed PIDL-based approach to outperform advanced baseline methods in terms of data efficiency and estimation accuracy."
  },
  "aaai2021_main_thelobrecreationmodelpredictingthelimitorderbookfromtaqhistoryusinganordinarydifferentialequationrecurrentneuralnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "The LOB Recreation Model: Predicting the Limit Order Book from TAQ History Using an Ordinary Differential Equation Recurrent Neural Network",
    "authors": [
      "Zijian Shi",
      "Yu Chen",
      "John Cartlidge"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16133",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16133/15940",
    "published": "2021-02",
    "summary": "In an order-driven financial market, the price of a financial asset is discovered through the interaction of orders - requests to buy or sell at a particular price - that are posted to the public limit order book (LOB). Therefore, LOB data is extremely valuable for modelling market dynamics. However, LOB data is not freely accessible, which poses a challenge to market participants and researchers wishing to exploit this information. Fortunately, trades and quotes (TAQ) data - orders arriving at the top of the LOB, and trades executing in the market - are more readily available. In this paper, we present the LOB recreation model, a first attempt from a deep learning perspective to recreate the top five price levels of the LOB for small-tick stocks using only TAQ data. Volumes of orders sitting deep in the LOB are predicted by combining outputs from: (1) a history compiler that uses a Gated Recurrent Unit (GRU) module to selectively compile prediction relevant quote history; (2) a market events simulator, which uses an Ordinary Differential Equation Recurrent Neural Network (ODE-RNN) to simulate the accumulation of net order arrivals; and (3) a weighting scheme to adaptively combine the predictions generated by (1) and (2). By the paradigm of transfer learning, the core encoder trained on one stock can be fine-tuned to enable application to other financial assets of the same class with much lower demand on additional data. Comprehensive experiments conducted on two real world intraday LOB datasets demonstrate that the proposed model can efficiently recreate the LOB with high accuracy using only TAQ data as input."
  },
  "aaai2021_main_embracingdomaindifferencesinfakenewscross-domainfakenewsdetectionusingmulti-modaldata": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Embracing Domain Differences in Fake News: Cross-domain Fake News Detection using Multi-modal Data",
    "authors": [
      "Amila Silva",
      "Ling Luo",
      "Shanika Karunasekera",
      "Christopher Leckie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16134",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16134/15941",
    "published": "2021-02",
    "summary": "With the rapid evolution of social media, fake news has become a significant social problem, which cannot be addressed in a timely manner using manual investigation. This has motivated numerous studies on automating fake news detection. Most studies explore supervised training models with different modalities (e.g., text, images, and propagation networks) of news records to identify fake news. However, the performance of such techniques generally drops if news records are coming from different domains (e.g., politics, entertainment), especially for domains that are unseen or rarely-seen during training. As motivation, we empirically show that news records from different domains have significantly different word usage and propagation patterns. Furthermore, due to the sheer volume of unlabelled news records, it is challenging to select news records for manual labelling so that the domain-coverage of the labelled dataset is maximised. Hence, this work: (1) proposes a novel framework that jointly preserves domain-specific and cross-domain knowledge in news records to detect fake news from different domains; and (2) introduces an unsupervised technique to select a set of unlabelled informative news records for manual labelling, which can be ultimately used to train a fake news detection model that performs well for many domains while minimizing the labelling cost. Our experiments show that the integration of the proposed fake news model and the selective annotation approach achieves state-of-the-art performance for cross-domain news datasets, while yielding notable improvements for rarely-appearing domains in news datasets."
  },
  "aaai2021_main_oral-3dreconstructingthe3dstructureoforalcavityfrompanoramicx-ray": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Oral-3D: Reconstructing the 3D Structure of Oral Cavity from Panoramic X-ray",
    "authors": [
      "Weinan Song",
      "Yuan Liang",
      "Jiawei Yang",
      "Kun Wang",
      "Lei He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16135",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16135/15942",
    "published": "2021-02",
    "summary": "Panoramic X-ray (PX) provides a 2D picture of the patient's mouth in a panoramic view to help dentists observe the invisible disease inside the gum. However, it provides limited 2D information compared with cone-beam computed tomography (CBCT), another dental imaging method that generates a 3D picture of the oral cavity but with more radiation dose and a higher price. Consequently, it is of great interest to reconstruct the 3D structure from a 2D X-ray image, which can greatly explore the application of X-ray imaging in dental surgeries. In this paper, we propose a framework, named Oral-3D, to reconstruct the 3D oral cavity from a single PX image and prior information of the dental arch. Specifically, we first train a generative model to learn the cross-dimension transformation from 2D to 3D. Then we restore the shape of the oral cavity with a deformation module with the dental arch curve, which can be obtained simply by taking a photo of the patient's mouth. To be noted, Oral-3D can restore both the density of bony tissues and the curved mandible surface. Experimental results show that Oral-3D can efficiently and effectively reconstruct the 3D oral structure and show critical information in clinical applications, e.g., tooth pulling and dental implants. To the best of our knowledge, we are the first to explore this domain transformation problem between these two imaging methods."
  },
  "aaai2021_main_trafficshapingine-commercialsearchenginemulti-objectiveonlinewelfaremaximization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Traffic Shaping in E-Commercial Search Engine: Multi-Objective Online Welfare Maximization",
    "authors": [
      "Liucheng Sun",
      "Chenwei Weng",
      "Chengfu Huo",
      "Weijun Ren",
      "Guochuan Zhang",
      "Xin\n      Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16136",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16136/15943",
    "published": "2021-02",
    "summary": "The e-commercial search engine is the primary gateway for customers to find desired products and engage in online shopping. Besides displaying items to optimize for a single objective (i.e., relevance), ranking items needs to satisfy some other business requirements in practice. Recently, traffic shaping was introduced to incorporate multiple objectives in a constrained optimization framework. However, many practical business requirements can not explicitly represented by linear constraints as in the existing work, and this may limit the scalablity of their framework. This paper presents a unified framework from the aspect of multi-objective welfare maximization where we regard all business requirements as objectives to optimize. Our framework can naturally incorporate a wide range of application-driven requirements. In addition to formulating the problem, we design an online traffic splitting algorithm that allows us to flexibly adjust the priorities of different objectives, and it has rigorous theoretical guarantees over the adversarial scenario. We also run experiments on both synthetic and real-world datasets to validate our algorithms."
  },
  "aaai2021_main_fullyexploitingcascadegraphsforreal-timeforwardingprediction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Fully Exploiting Cascade Graphs for Real-time Forwarding Prediction",
    "authors": [
      "Xiangyun Tang",
      "Dongliang Liao",
      "Weijie Huang",
      "Jin Xu",
      "Liehuang Zhu",
      "Meng\n      Shen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16137",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16137/15944",
    "published": "2021-02",
    "summary": "Real-time forwarding prediction for predicting online contents' popularity is beneficial to various social applications for enhancing interactive social behaviors. Cascade graphs, formed by online contents' propagation, play a vital role in real-time forwarding prediction. Existing cascade graph modeling methods are inadequate to embed cascade graphs that have hub structures and deep cascade paths, or they fail to handle the short-term outbreak of forwarding amount. To this end, we propose a novel real-time forwarding prediction method that includes an effective approach for cascade graph embedding and a short-term variation sensitive method for time-series modeling, making the best of cascade graph features. Using two real world datasets, we demonstrate the significant superiority of the proposed method compared with the state-of-the-art. Our experiments also reveal interesting implications hidden in the performance differences between cascade graph embedding and time-series modeling."
  },
  "aaai2021_main_ahierarchicalapproachtomulti-eventsurvivalanalysis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Hierarchical Approach to Multi-Event Survival Analysis",
    "authors": [
      "Donna Tjandra",
      "Yifei He",
      "Jenna Wiens"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16138",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16138/15945",
    "published": "2021-02",
    "summary": "In multi-event survival analysis, one aims to predict the probability of multiple different events occurring over some time horizon. One typically assumes that the timing of events is drawn from some distribution conditioned on an individual's covariates. However, during training, one does not have access to this distribution, and the natural variation in the observed event times makes the task of survival prediction challenging, on top of the potential interdependence among events. To address this issue, we introduce a novel approach for multi-event survival analysis that models the probability of event occurrence hierarchically at different time scales, using coarse predictions (e.g., monthly predictions) to iteratively guide predictions at finer and finer grained time scales (e.g., daily predictions). We evaluate the proposed approach across several publicly available datasets in terms of both intra-event, inter-individual (global) and intra-individual, inter-event (local) consistency. We show that the proposed method consistently outperforms well-accepted and commonly used approaches to multi-event survival analysis. When estimating survival curves for Alzheimer's disease and mortality, our approach achieves a C-index of 0.91 (95% CI 0.88-0.93) and a local consistency score of 0.97 (95% CI 0.94-0.98) compared to a C-index of 0.75 (95% CI 0.70-0.80) and a local consistency score of 0.94 (95% CI 0.91-0.97) when modeling each event separately. Overall, our approach improves the accuracy of survival predictions by iteratively reducing the original task to a set of nested, simpler subtasks."
  },
  "aaai2021_main_deepwritesynon-linehandwritingsynthesisviadeepshort-termrepresentations": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DeepWriteSYN: On-Line Handwriting Synthesis via Deep Short-Term Representations",
    "authors": [
      "Ruben Tolosana",
      "Paula Delgado-Santos",
      "Andres Perez-Uribe",
      "Ruben\n      Vera-Rodriguez",
      "Julian Fierrez",
      "Aythami Morales"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16139",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16139/15946",
    "published": "2021-02",
    "summary": "This study proposes DeepWriteSYN, a novel on-line handwriting synthesis approach via deep short-term representations. It comprises two modules: i) an optional and interchangeable temporal segmentation, which divides the handwriting into short-time segments consisting of individual or multiple concatenated strokes; and ii) the on-line synthesis of those short-time handwriting segments, which is based on a sequence-to-sequence Variational Autoencoder (VAE). The main advantages of the proposed approach are that the synthesis is carried out in short-time segments (that can run from a character fraction to full characters) and that the VAE can be trained on a configurable handwriting dataset. These two properties give a lot of flexibility to our synthesiser, e.g., as shown in our experiments, DeepWriteSYN can generate realistic handwriting variations of a given handwritten structure corresponding to the natural variation within a given population or a given subject. These two cases are developed experimentally for individual digits and handwriting signatures, respectively, achieving in both cases remarkable results.Also, we provide experimental results for the task of on-line signature verification showing the high potential of DeepWriteSYN to improve significantly one-shot learning scenarios. To the best of our knowledge, this is the first synthesis approach capable of generating realistic on-line handwriting in the short term (including handwritten signatures) via deep learning. This can be very useful as a module toward long-term realistic handwriting generation either completely synthetic or as natural variation of given handwriting samples."
  },
  "aaai2021_main_sketchgenerationwithdrawingprocessguidedbyvectorflowandgrayscale": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale",
    "authors": [
      "Zhengyan Tong",
      "Xuanhong Chen",
      "Bingbing Ni",
      "Xiaohang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16140",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16140/15947",
    "published": "2021-02",
    "summary": "We propose a novel image-to-pencil translation method that could not only generate high-quality pencil sketches but also offer the drawing process. Existing pencil sketch algorithms are based on texture rendering rather than the direct imitation of strokes, making them unable to show the drawing process but only a final result. To address this challenge, we first establish a pencil stroke imitation mechanism. Next, we develop a framework with three branches to guide stroke drawing: the first branch guides the direction of the strokes, the second branch determines the shade of the strokes, and the third branch enhances the details further. Under this framework's guidance, we can produce a pencil sketch by drawing one stroke every time. Our method is fully interpretable. Comparison with existing pencil drawing algorithms shows that our method is superior to others in terms of texture quality, style, and user evaluation. Our code and supplementary material are now available at: https://github.com/TZYSJTU/Sketch-Generation-withDrawing-Process-Guided-by-Vector-Flow-and-Grayscale"
  },
  "aaai2021_main_pssm-distilproteinsecondarystructureprediction(pssp)onlow-qualitypssmbyknowledgedistillationwithcontrastivelearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PSSM-Distil: Protein Secondary Structure Prediction (PSSP) on Low-Quality PSSM by Knowledge Distillation with Contrastive Learning",
    "authors": [
      "Qin Wang",
      "Boyuan Wang",
      "Zhenlei Xu",
      "Jiaxiang Wu",
      "Peilin Zhao",
      "Zhen Li",
      "Sheng Wang",
      "Junzhou Huang",
      "Shuguang Cui"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16141",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16141/15948",
    "published": "2021-02",
    "summary": "Protein secondary structure prediction (PSSP) is an essential task in computational biology. To achieve the accurate PSSP, the general and vital feature engineering is to use multiple sequence alignment (MSA) for Position-Specific Scoring Matrix (PSSM) extraction. However, when only low-quality PSSM can be obtained due to poor sequence homology, previous PSSP accuracy (merely around 65%) is far from practical usage for subsequent tasks. In this paper, we propose a novel PSSM-Distil framework for PSSP on low-quality PSSM, which not only enhances the PSSM feature at a lower level but also aligns the feature distribution at a higher level. In practice, the PSSM-Distil first exploits the proteins with high-quality PSSM to achieve a teacher network for PSSP in a full-supervised way. Under the guidance of the teacher network, the low-quality PSSM and corresponding student network with low discriminating capacity are effectively resolved by feature enhancement through EnhanceNet and distribution alignment through knowledge distillation with contrastive learning. Further, our PSSM-Distil supports the input from a pre-trained protein sequence language BERT model to provide auxiliary information, which is designed to address the extremely low-quality PSSM cases, i.e., no homologous sequence. Extensive experiments demonstrate the proposed PSSM-Distil outperforms state-of-the-art models on PSSP by 6% on average and nearly 8% in extremely low-quality cases on public benchmarks, BC40 and CB513."
  },
  "aaai2021_main_commissionfeeisnotenoughahierarchicalreinforcedframeworkforportfoliomanagement": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Commission Fee is not Enough: A Hierarchical Reinforced Framework for Portfolio Management",
    "authors": [
      "Rundong Wang",
      "Hongxin Wei",
      "Bo An",
      "Zhouyan Feng",
      "Jun Yao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16142",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16142/15949",
    "published": "2021-02",
    "summary": "Portfolio management via reinforcement learning is at the forefront of fintech research, which explores how to optimally reallocate a fund into different financial assets over the long term by trial-and-error. Existing methods are impractical since they usually assume each reallocation can be finished immediately and thus ignoring the price slippage as part of the trading cost. To address these issues, we propose a hierarchical reinforced stock trading system for portfolio management (HRPM). Concretely, we decompose the trading process into a hierarchy of portfolio management over trade execution and train the corresponding policies. The high-level policy gives portfolio weights at a lower frequency to maximize the long-term profit and invokes the low-level policy to sell or buy the corresponding shares within a short time window at a higher frequency to minimize the trading cost. We train two levels of policies via a pre-training scheme and an iterative training scheme for data efficiency. Extensive experimental results in the U.S. market and the China market demonstrate that HRPM achieves significant improvement against many state-of-the-art approaches."
  },
  "aaai2021_main_alternativebaselinesforlow-shot3dmedicalimagesegmentation---anatlasperspective": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Alternative Baselines for Low-Shot 3D Medical Image Segmentation---An Atlas Perspective",
    "authors": [
      "Shuxin Wang",
      "Shilei Cao",
      "Dong Wei",
      "Cong Xie",
      "Kai Ma",
      "Liansheng Wang",
      "Deyu\n      Meng",
      "Yefeng Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16143",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16143/15950",
    "published": "2021-02",
    "summary": "Low-shot (one/few-shot) segmentation has attracted increasing attention as it works well with limited annotation. State-of-the-art low-shot segmentation methods on natural images usually focus on implicit representation learning for each novel class, such as learning prototypes, deriving guidance features via masked average pooling, and segmenting using cosine similarity in feature space. We argue that low-shot segmentation on medical images should step further to explicitly learn dense correspondences between images to utilize the anatomical similarity. The core ideas are inspired by the classical practice of multi-atlas segmentation, where the indispensable parts of atlas-based segmentation, i.e., registration, label propagation, and label fusion are unified into a single framework in our work. Specifically, we propose two alternative baselines, i.e., the Siamese-Baseline and Individual-Difference-Aware Baseline, where the former is targeted at anatomically stable structures (such as brain tissues), and the latter possesses a strong generalization ability to organs suffering large morphological variations (such as abdominal organs). In summary, this work sets up a benchmark for low-shot 3D medical image segmentation and sheds light on further understanding of atlas-based few-shot segmentation."
  },
  "aaai2021_main_deeptraderadeepreinforcementlearningapproachforrisk-returnbalancedportfoliomanagementwithmarketconditionsembedding": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DeepTrader: A Deep Reinforcement Learning Approach for Risk-Return Balanced Portfolio Management with Market Conditions Embedding",
    "authors": [
      "Zhicheng Wang",
      "Biwei Huang",
      "Shikui Tu",
      "Kun Zhang",
      "Lei Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16144",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16144/15951",
    "published": "2021-02",
    "summary": "Most existing reinforcement learning (RL)-based portfolio management models do not take into account the market conditions, which limits their performance in risk-return balancing. In this paper, we propose DeepTrader, a deep RL method to optimize the investment policy. In particular, to tackle the risk-return balancing problem, our model embeds macro market conditions as an indicator to dynamically adjust the proportion between long and short funds, to lower the risk of market fluctuations, with the negative maximum drawdown as the reward function. Additionally, the model involves a unit to evaluate individual assets, which learns dynamic patterns from historical data with the price rising rate as the reward function. Both temporal and spatial dependencies between assets are captured hierarchically by a specific type of graph structure. Particularly, we find that the estimated causal structure best captures the interrelationships between assets, compared to industry classification and correlation. The two units are complementary and integrated to generate a suitable portfolio which fits the market trend well and strikes a balance between return and risk effectively. Experiments on three well-known stock indexes demonstrate the superiority of DeepTrader in terms of risk-gain criteria."
  },
  "aaai2021_main_dynamicgaussianmixturebaseddeepgenerativemodelforrobustforecastingonsparsemultivariatetimeseries": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dynamic Gaussian Mixture based Deep Generative Model For Robust Forecasting on Sparse Multivariate Time Series",
    "authors": [
      "Yinjun Wu",
      "Jingchao Ni",
      "Wei Cheng",
      "Bo Zong",
      "Dongjin Song",
      "Zhengzhang Chen",
      "Yanchi Liu",
      "Xuchao Zhang",
      "Haifeng Chen",
      "Susan B Davidson"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16145",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16145/15952",
    "published": "2021-02",
    "summary": "Forecasting on sparse multivariate time series (MTS) aims to model the predictors of future values of time series given their incomplete past, which is important for many emerging applications. However, most existing methods process MTS\u2019s individually, and do not leverage the dynamic distributions underlying the MTS\u2019s, leading to sub-optimal results when the sparsity is high. To address this challenge, we propose a novel generative model, which tracks the transition of latent clusters, instead of isolated feature representations, to achieve robust modeling. It is characterized by a newly designed dynamic Gaussian mixture distribution, which captures the dynamics of clustering structures, and is used for emitting time series. The generative model is parameterized by neural networks. A structured inference network is also designed for enabling inductive analysis.A gating mechanism is further introduced to dynamically tune the Gaussian mixture distributions. Extensive experimental results on a variety of real-life datasets demonstrate the effectiveness of our method."
  },
  "aaai2021_main_automatedsymboliclawdiscoveryacomputervisionapproach": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Automated Symbolic Law Discovery: A Computer Vision Approach",
    "authors": [
      "Hengrui Xing",
      "Ansaf Salleb-Aouissi",
      "Nakul Verma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16146",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16146/15953",
    "published": "2021-02",
    "summary": "One of the most exciting applications of modern artificial intelligence is to automatically discover scientific laws from experimental data. This is not a trivial problem as it involves searching for a complex mathematical relationship over a large set of explanatory variables and operators that can be combined in an infinite number of ways.Inspired by the incredible success of deep learning in computer vision, we tackle this problem by adapting various successful network architectures into the symbolic law discovery pipeline. The novelty of our approach is in (1) encoding the input data as an image with super-resolution, (2) developing an appropriate deep network pipeline, and (3) predicting the importance of each mathematical operator from the relationship image. This allows us to prior the exponentially large search with the predicted importance of the symbolic operators, which can significantly accelerate the discovery process.We apply our model to a variety of plausible relationships---both simulated and from physics and mathematics domains---involving different dimensions and constituents. We show that our model is able to identify the underlying operators from data, achieving a high accuracy and AUC (91% and 0.96 on average resp.) for systems with as many as ten independent variables. Our method significantly outperforms the current state of the art in terms of data fitting (R^2), discovery rate (recovering the true relationship), and succinctness (output formula complexity). The discovered equations can be seen as first drafts of scientific laws that can be helpful to the scientists for (1) hypothesis building, and (2) understanding the complex underlying structure of the studied phenomena. Our approach holds a real promise to help speed up the rate of scientific discovery."
  },
  "aaai2021_main_hierarchicallyandcooperativelylearningtrafficsignalcontrol": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Hierarchically and Cooperatively Learning Traffic Signal Control",
    "authors": [
      "Bingyu Xu",
      "Yaowei Wang",
      "Zhaozhi Wang",
      "Huizhu Jia",
      "Zongqing Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16147",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16147/15954",
    "published": "2021-02",
    "summary": "Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated superior performance to conventional control methods. However, there are still several challenges we have to address before fully applying deep RL to traffic signal control. Firstly, the objective of traffic signal control is to optimize average travel time, which is a delayed reward in a long time horizon in the context of RL. However, existing work simplifies the optimization by using queue length, waiting time, delay, etc., as immediate reward and presumes these short-term targets are always aligned with the objective. Nevertheless, these targets may deviate from the objective in different road networks with various traffic patterns. Secondly, it remains unsolved how to cooperatively control traffic signals to directly optimize average travel time. To address these challenges, we propose a hierarchical and cooperative reinforcement learning method-HiLight. HiLight enables each agent to learn a high-level policy that optimizes the objective locally by selecting among the sub-policies that respectively optimize short-term targets. Moreover, the high-level policy additionally considers the objective in the neighborhood with adaptive weighting to encourage agents to cooperate on the objective in the road network. Empirically, we demonstrate that HiLight outperforms state-of-the-art RL methods for traffic signal control in real road networks with real traffic."
  },
  "aaai2021_main_deeppartialrankaggregationforpersonalizedattributes": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Partial Rank Aggregation for Personalized Attributes",
    "authors": [
      "Qianqian Xu",
      "Zhiyong Yang",
      "Zuyao Chen",
      "Yangbangyan Jiang",
      "Xiaochun Cao",
      "Yuan Yao",
      "Qingming Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16148",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16148/15955",
    "published": "2021-02",
    "summary": "In this paper, we study the problem of how to aggregate pairwise personalized attributes (PA) annotations (e.g., Shoes A is more comfortable than B) from different annotators on the crowdsourcing platforms, which is an emerging topic gaining increasing attention in recent years. Given the crowdsourced annotations, the majority of the traditional literature assumes that all the pairs in the collected dataset are distinguishable. However, this assumption is incompatible with how humans perceive attributes since indistinguishable pairs are ubiquitous for the annotators due to the limitation of human perception. To attack this problem, we propose a novel deep prediction model that could simultaneously detect the indistinguishable pairs and aggregate ranking results for distinguishable pairs. First of all, we represent the pairwise annotations as a multi-graph. Based on such data structure, we propose an end-to-end partial ranking model which consists of a deep backbone architecture and a probabilistic model that captures the generative process of the partial rank annotations. Specifically, to recognize the indistinguishable pairs, the probabilistic model we proposed is equipped with an adaptive perception threshold, where indistinguishable pairs could be automatically detected when the absolute value of the score difference is below the learned threshold. In our empirical studies, we perform a series of experiments on three real-world datasets: LFW-10, Shoes, and Sun. The corresponding results consistently show the superiority of our proposed model."
  },
  "aaai2021_main_towardsefficientselectionofactivitytrajectoriesbasedondiversityandcoverage": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Towards Efficient Selection of Activity Trajectories based on Diversity and Coverage",
    "authors": [
      "Chengcheng Yang",
      "Lisi Chen",
      "Hao Wang",
      "Shuo Shang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16149",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16149/15956",
    "published": "2021-02",
    "summary": "With the prevalence of location based services, activity trajectories are being generated at a rapid pace. The activity trajectory data enriches traditional trajectory data with semantic activities of users, which not only shows where the users have been, but also the preference of users. However, the large volume of data is expensive for people to explore. To address this issue, we study the problem of Diversity-aware Activity Trajectory Selection (DaATS). Given a region of interest for a user, it finds a small number of representative activity trajectories that can provide the user with a broad coverage of different aspects of the region. The problem is challenging in both the efficiency of trajectory similarity computation and subset selection. To tackle the two challenges, we propose a novel solution by: (1) exploiting a deep metric learning method to speedup the similarity computation; and (2) proving that DaATS is an NP-hard problem, and developing an efficient approximation algorithm with performance guarantees. Experiments on two real-world datasets show that our proposal significantly outperforms state-of-the-art baselines."
  },
  "aaai2021_main_minimizinglabelingcostfornucleiinstancesegmentationandclassificationwithcross-domainimagesandweaklabels": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Minimizing Labeling Cost for Nuclei Instance Segmentation and Classification with Cross-domain Images and Weak Labels",
    "authors": [
      "Siqi Yang",
      "Jun Zhang",
      "Junzhou Huang",
      "Brian C. Lovell",
      "Xiao Han"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16150",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16150/15957",
    "published": "2021-02",
    "summary": "Nucleus instance segmentation and classification in histopathological images is an essential prerequisite in pathology diagnosis/prognosis. However, nucleus annotations (e.g., segmentation and labeling) require domain experts, and annotating nuclei at pixel-level is time-consuming and labor-intensive. Moreover, nuclei from different cancer types vary in shapes and appearances. These inter-cancer variations require careful annotations for specific cancer types. Therefore, to minimize the labeling cost, we propose a novel application that considers each cancer type as an individual domain and apply domain adaptation techniques to improve the segmentation/classification performance among different cancer types. Unlike the previous studies that focus on unsupervised or weakly-supervised domain adaptation independently, we would like to discover what kinds of labeling can achieve the most cost-effective domain adaptation performance in nucleus instance segmentation and classification. Specifically, we propose a unified framework that is applicable to different level annotations: no annotations, image-level, and point-level annotations. Cyclic adaptation with pseudo labels and adversarial discriminator are utilized for unsupervised domain alignment. Image-level or point-level annotations are additionally adopted to supervise the nucleus classification and refine the pseudo labels. Experiments demonstrate the effectiveness and efficacy of the proposed framework (jointly using unsupervised and weakly supervised learning) on adapting the segmentation and classification model from one cancer type to 18 other cancer types."
  },
  "aaai2021_main_bigramandunigrambasedtextattackviaadaptivemonotonicheuristicsearch": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Bigram and Unigram Based Text Attack via Adaptive Monotonic Heuristic Search",
    "authors": [
      "Xinghao Yang",
      "Weifeng Liu",
      "James Bailey",
      "Dacheng Tao",
      "Wei Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16151",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16151/15958",
    "published": "2021-02",
    "summary": "Deep neural networks (DNNs) are known to be vulnerable to adversarial images, while their robustness in text classification are rarely studied. Several lines of text attack methods have been proposed in the literature, such as character-level, word-level, and sentence-level attacks. However, it is still a challenge to minimize the number of word distortions necessary to induce misclassification, while simultaneously ensuring the lexical correctness, syntactic correctness, and semantic similarity. In this paper, we propose the Bigram and Unigram based Monotonic Heuristic Search (BU-MHS) method to examine the vulnerability of deep models. Our method has three major merits. Firstly, we propose to attack text documents not only at the unigram word level but also at the bigram level to avoid producing meaningless outputs. Secondly, we propose a hybrid method to replace the input words with both their synonyms and sememe candidates, which greatly enriches potential substitutions compared to only using synonyms. Lastly, we design a search algorithm, i.e., Monotonic Heuristic Search (MHS), to determine the priority of word replacements, aiming to reduce the modification cost in an adversarial attack. We evaluate the effectiveness of BU-MHS on IMDB, AG's News, and Yahoo! Answers text datasets by attacking four state-of-the-art DNNs models. Experimental results show that our BU-MHS achieves the highest attack success rate by changing the smallest number of words compared with other existing models."
  },
  "aaai2021_main_graspgenericframeworkforhealthstatusrepresentationlearningbasedonincorporatingknowledgefromsimilarpatients": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "GRASP: Generic Framework for Health Status Representation Learning Based on Incorporating Knowledge from Similar Patients",
    "authors": [
      "Chaohe Zhang",
      "Xin Gao",
      "Liantao Ma",
      "Yasha Wang",
      "Jiangtao Wang",
      "Wen Tang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16152",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16152/15959",
    "published": "2021-02",
    "summary": "Deep learning models have been applied to many healthcare tasks based on electronic medical records (EMR) data and shown substantial performance. Existing methods commonly embed the records of a single patient into a representation for medical tasks. Such methods learn inadequate representations and lead to inferior performance, especially when the patient\u2019s data is sparse or low-quality. Aiming at the above problem, we propose GRASP, a generic framework for healthcare models. For a given patient, GRASP first finds patients in the dataset who have similar conditions and similar results (i.e., the similar patients), and then enhances the representation learning and prognosis of the given patient by leveraging knowledge extracted from these similar patients. GRASP defines similarities with different meanings between patients for different clinical tasks, and finds similar patients with useful information accordingly, and then learns cohort representation to extract valuable knowledge contained in the similar patients. The cohort information is fused with the current patient\u2019s representation to conduct final clinical tasks. Experimental evaluations on two real-world datasets show that GRASP can be seamlessly integrated into state-of-the-art models with consistent performance improvements. Besides, under the guidance of medical experts, we verified the findings extracted by GRASP, and the findings are consistent with the existing medical knowledge, indicating that GRASP can generate useful insights for relevant predictions."
  },
  "aaai2021_main_windowlossforbonefracturedetectionandlocalizationinx-rayimageswithpoint-basedannotation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Window Loss for Bone Fracture Detection and Localization in X-ray Images with Point-based Annotation",
    "authors": [
      "Xinyu Zhang",
      "Yirui Wang",
      "Chi-Tung Cheng",
      "Le Lu",
      "Adam P. Harrison",
      "Jing\n      Xiao",
      "Chien-Hung Liao",
      "Shun Miao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16153",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16153/15960",
    "published": "2021-02",
    "summary": "Object detection methods are widely adopted for computer-aided diagnosis using medical images. Anomalous findings are usually treated as objects that are described by bounding boxes. Yet, many pathological findings, e.g., bone fractures, cannot be clearly defined by bounding boxes, owing to considerable instance, shape and boundary ambiguities. This makes bounding box annotations, and their associated losses, highly ill-suited. In this work, we propose a new bone fracture detection method for X-ray images, based on a labor effective and flexible annotation scheme suitable for abnormal findings with no clear object-level spatial extents or boundaries. Our method employs a simple, intuitive, and informative point-based annotation protocol to mark localized pathology information. To address the uncertainty in the fracture scales annotated via point(s), we convert the annotations into pixel-wise supervision that uses lower and upper bounds with positive, negative, and uncertain regions. A novel Window Loss is subsequently proposed to only penalize the predictions outside of the uncertain regions. Our method has been extensively evaluated on 4410 pelvic X-ray images of unique patients. Experiments demonstrate that our method outperforms previous state-of-the-art image classification and object detection baselines by healthy margins, with an AUROC of 0.983 and FROC score of 89.6%."
  },
  "aaai2021_main_aspatialregulatedpatch-wiseapproachforcervicaldysplasiadiagnosis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Spatial Regulated Patch-Wise Approach for Cervical Dysplasia Diagnosis",
    "authors": [
      "Ying Zhang",
      "Yifang Yin",
      "Zhenguang Liu",
      "Roger Zimmermann"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16154",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16154/15961",
    "published": "2021-02",
    "summary": "Cervical dysplasia diagnosis via visual investigation is a challenging problem. Recent approaches use deep learning techniques to extract features and require the downsampling of high-resolution cervical screening images to smaller sizes for training. Such a reduction may result in the loss of visual details that appear weakly and locally within a cervical image. To overcome this challenge, our work divides an image into patches and then represents it from patch features. We aggregate patch patterns into an image feature in a weighted manner by considering the patch--image label relation. The weights are visualized as a heatmap to explain where the diagnosis results come from. We further introduce a spatial regulator to guide the classifier to focus on the cervix region and to adjust the weight distribution, without requiring any manual annotations of the cervix region. A novel iterative algorithm is designed to refine the regulator, which is able to capture the variations in cervix center locations and shapes. Experiments on an 18-year real-world dataset indicate a minimal of 3.47%, 4.59%, 8.54% improvements over the state-of-the-art in accuracy, F1, and recall measures, respectively."
  },
  "aaai2021_main_online3dbinpackingwithconstraineddeepreinforcementlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Online 3D Bin Packing with Constrained Deep Reinforcement Learning",
    "authors": [
      "Hang Zhao",
      "Qijin She",
      "Chenyang Zhu",
      "Yin Yang",
      "Kai Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16155",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16155/15962",
    "published": "2021-02",
    "summary": "We solve a challenging yet practically useful variant of 3D Bin Packing Problem (3D-BPP). In our problem, the agent has limited information about the items to be packed into a single bin, and an item must be packed immediately after its arrival without buffering or readjusting. The item's placement also subjects to the constraints of order dependence and physical stability. We formulate this online 3D-BPP as a constrained Markov decision process (CMDP). To solve the problem, we propose an effective and easy-to-implement constrained deep reinforcement learning (DRL) method under the actor-critic framework. In particular, we introduce a prediction-and-projection scheme: The agent first predicts a feasibility mask for the placement actions as an auxiliary task and then uses the mask to modulate the action probabilities output by the actor during training. Such supervision and projection facilitate the agent to learn feasible policies very efficiently. Our method can be easily extended to handle lookahead items, multi-bin packing, and item re-orienting. We have conducted extensive evaluation showing that the learned policy significantly outperforms the state-of-the-art methods. A preliminary user study even suggests that our method might attain a human-level performance."
  },
  "aaai2021_main_deardeepreinforcementlearningforonlineadvertisingimpressioninrecommendersystems": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DEAR: Deep Reinforcement Learning for Online Advertising Impression in Recommender Systems",
    "authors": [
      "Xiangyu Zhao",
      "Changsheng Gu",
      "Haoshenglun Zhang",
      "Xiwang Yang",
      "Xiaobing Liu",
      "Jiliang Tang",
      "Hui Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16156",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16156/15963",
    "published": "2021-02",
    "summary": "With the recent prevalence of Reinforcement Learning (RL), there have been tremendous interests in utilizing RL for online advertising in recommendation platforms (e.g., e-commerce and news feed sites). However, most RL-based advertising algorithms focus on optimizing ads' revenue while ignoring the possible negative influence of ads on user experience of recommended items (products, articles and videos). Developing an optimal advertising algorithm in recommendations faces immense challenges because interpolating ads improperly or too frequently may decrease user experience, while interpolating fewer ads will reduce the advertising revenue. Thus, in this paper, we propose a novel advertising strategy for the rec/ads trade-off. To be specific, we develop an RL-based framework that can continuously update its advertising strategies and maximize reward in the long run. Given a recommendation list, we design a novel Deep Q-network architecture that can determine three internally related tasks jointly, i.e., (i) whether to interpolate an ad or not in the recommendation list, and if yes, (ii) the optimal ad and (iii) the optimal location to interpolate. The experimental results based on real-world data demonstrate the effectiveness of the proposed framework."
  },
  "aaai2021_main_towardsbalanceddefectpredictionwithbetterinformationpropagation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Towards Balanced Defect Prediction with Better Information Propagation",
    "authors": [
      "Xianda Zheng",
      "Yuan-Fang Li",
      "Huan Gao",
      "Yuncheng Hua",
      "Guilin Qi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16157",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16157/15964",
    "published": "2021-02",
    "summary": "Defect prediction, the task of predicting the presence of defects in source code artifacts, has broad application in software development. Defect prediction faces two major challenges, label scarcity, where only a small percentage of code artifacts are labeled, and data imbalance, where the majority of labeled artifacts are non-defective. Moreover, current defect prediction methods ignore the impact of information propagation among code artifacts and this negligence leads to performance degradation. In this paper, we propose DPCAG, a novel model to address the above three issues. We treat code artifacts as nodes in a graph, and learn to propagate influence among neighboring nodes iteratively in an EM framework. DPCAG dynamically adjusts the contributions of each node and selects high-confidence nodes for data augmentation. Experimental results on real-world benchmark datasets show that DPCAG improves performance compare to the state-of-the-art models. In particular, DPCAG achieves substantial performance superiority when measured by Matthews Correlation Coefficient (MCC), a metric that is widely acknowledged to be the most suitable for imbalanced data."
  },
  "aaai2021_main_many-to-onedistributionlearningandk-nearestneighborsmoothingforthoracicdiseaseidentification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Many-to-One Distribution Learning and K-Nearest Neighbor Smoothing for Thoracic Disease Identification",
    "authors": [
      "Yi Zhou",
      "Lei Huang",
      "Tianfei Zhou",
      "Ling Shao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16158",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16158/15965",
    "published": "2021-02",
    "summary": "Chest X-rays are an important and accessible clinical imaging tool for the detection of many thoracic diseases. Over the past decade, deep learning, with a focus on the convolutional neural network (CNN), has become the most powerful computer-aided diagnosis technology for improving disease identification performance. However, training an effective and robust deep CNN usually requires a large amount of data with high annotation quality. For chest X-ray imaging, annotating large-scale data requires professional domain knowledge and is time-consuming. Thus, existing public chest X-ray datasets usually adopt language pattern based methods to automatically mine labels from reports. However, this results in label uncertainty and inconsistency. In this paper, we propose many-to-one distribution learning (MODL) and K-nearest neighbor smoothing (KNNS) methods from two perspectives to improve a single model's disease identification performance, rather than focusing on an ensemble of models. MODL integrates multiple models to obtain a soft label distribution for optimizing the single target model, which can reduce the effects of original label uncertainty. Moreover, KNNS aims to enhance the robustness of the target model to provide consistent predictions on images with similar medical findings. Extensive experiments on the public NIH Chest X-ray and CheXpert datasets show that our model achieves consistent improvements over the state-of-the-art methods."
  },
  "aaai2021_main_probabilisticprogrammingbotsinintuitivephysicsgameplay": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Probabilistic Programming Bots in Intuitive Physics Game Play",
    "authors": [
      "Fahad Alhasoun",
      "Sarah Alneghiemish"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16159",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16159/15966",
    "published": "2021-02",
    "summary": "Recent findings suggest that humans deploy cognitive mechanism of physics simulation engines to simulate the physics of objects. We propose a framework for bots to deploy probabilistic programming tools for interacting with intuitive physics environments. The framework employs a physics simulation in a probabilistic way to infer about moves performed by an agent in a setting governed by Newtonian laws of motion. However, methods of probabilistic programs can be slow in such setting due to their need to generate many samples. We complement the model with a model-free approach to aid the sampling procedures in becoming more efficient through learning from experience during game playing. We present an approach where combining model-free approaches (a convolutional neural network in our model) and model-based approaches (probabilistic physics simulation) is able to achieve what neither could alone. This way the model outperforms an all model-free or all model-based approach. We discuss a case study showing empirical results of the performance of the model on the game of Flappy Bird."
  },
  "aaai2021_main_model-agnosticfitsforunderstandinginformationseekingpatternsinhumans": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Model-Agnostic Fits for Understanding Information Seeking Patterns in Humans",
    "authors": [
      "Soumya Chatterjee",
      "Pradeep Shenoy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16160",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16160/15967",
    "published": "2021-02",
    "summary": "In decision making tasks under uncertainty, humans display characteristic biases in seeking, integrating, and acting upon information relevant to the task. Here, we reexamine data from previous carefully designed experiments, collected at scale, that measured and catalogued these biases in aggregate form. We design deep learning models that replicate these biases in aggregate, while also capturing individual variation in behavior. A key finding of our work is that paucity of data collected from each individual subject can be overcome by sampling large numbers of subjects from the population, while still capturing individual differences. We predict human behavior with high accuracy without making any assumptions about task goals, reward structure, or individual biases, thus providing a model-agnostic fit to human behavior in the task. Such an approach can sidestep potential limitations in modeler-specified inductive biases, and has implications for computational modeling of human cognitive function in general, and of human-AI interfaces in particular."
  },
  "aaai2021_main_apparentlyirrationalchoiceasoptimalsequentialdecisionmaking": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Apparently Irrational Choice as Optimal Sequential Decision Making",
    "authors": [
      "Haiyang Chen",
      "Hyung Jin Chang",
      "Andrew Howes"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16161",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16161/15968",
    "published": "2021-02",
    "summary": "In this paper, we propose a normative approach to modeling apparently human irrational decision making (cognitive biases) that makes use of inherently rational computational mechanisms. We view preferential choice tasks as sequential decision making problems and formulate them as Partially Observable Markov Decision Processes (POMDPs). The resulting sequential decision model learns what information to gather about which options, whether to calculate option values or make comparisons between options and when to make a choice. We apply the model to choice problems where context is known to influence human choice, an effect that has been taken as evidence that human cognition is irrational. Our results show that the new model approximates a bounded optimal cognitive policy and makes quantitative predictions that correspond well to evidence about human choice. Furthermore, the model uses context to help infer which option has a maximum expected value while taking into account computational cost and cognitive limits. In addition, it predicts when, and explains why, people stop evidence accumulation and make a decision. We argue that the model provides evidence that apparent human irrationalities are emergent consequences of processes that prefer higher value (rational) policies."
  },
  "aaai2021_main_visualrelationdetectionusinghybridanalogicallearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Visual Relation Detection using Hybrid Analogical Learning",
    "authors": [
      "Kezhen Chen",
      "Ken Forbus"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16162",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16162/15969",
    "published": "2021-02",
    "summary": "Visual Relation Detection is currently one of the most popular problems for visual understanding. Many deep-learning models are designed for relation detection on images and have achieved impressive results. However, deep-learning models have several serious problems, including poor training-efficiency and lack of understandability. Psychologists have ample evidence that analogy is central in human learning and reasoning, including visual reasoning. This paper introduces a new hybrid system for visual relation detection combining deep-learning models and analogical generalization. Object bounding boxes and masks are detected using deep-learning models and analogical generalization over qualitative representations is used for visual relation detection between object pairs. Experiments on the Visual Relation Detection dataset indicates that our hybrid system gets comparable results on the task and is more training-efficient and explainable than pure deep-learning models."
  },
  "aaai2021_main_neuralanalogicalmatching": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Neural Analogical Matching",
    "authors": [
      "Maxwell Crouse",
      "Constantine Nakos",
      "Ibrahim Abdelaziz",
      "Ken Forbus"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16163",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16163/15970",
    "published": "2021-02",
    "summary": "Analogy is core to human cognition. It allows us to solve problems based on prior experience, it governs the way we conceptualize new information, and it even influences our visual perception. The importance of analogy to humans has made it an active area of research in the broader field of artificial intelligence, resulting in data-efficient models that learn and reason in human-like ways.While cognitive perspectives of analogy and deep learning have generally been studied independently of one another, the integration of the two lines of research is a promising step towards more robust and efficient learning techniques. As part of a growing body of research on such an integration, we introduce the Analogical Matching Network: a neural architecture that learns to produce analogies between structured, symbolic representations that are largely consistent with the principles of Structure-Mapping Theory."
  },
  "aaai2021_main_interpretableself-supervisedfacialmicro-expressionlearningtopredictcognitivestateandneurologicaldisorders": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Interpretable Self-Supervised Facial Micro-Expression Learning to Predict Cognitive State and Neurological Disorders",
    "authors": [
      "Arun Das",
      "Jeffrey Mock",
      "Yufei Huang",
      "Edward Golob",
      "Peyman Najafirad"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16164",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16164/15971",
    "published": "2021-02",
    "summary": "Human behavior is the confluence of output from voluntary and involuntary motor systems. The neural activities that mediate behavior, from individual cells to distributed networks, are in a state of constant flux. Artificial intelligence (AI) research over the past decade shows that behavior, in the form of facial muscle activity, can reveal information about fleeting voluntary and involuntary motor system activity related to emotion, pain, and deception. However, the AI algorithms often lack an explanation for their decisions, and learning meaningful representations requires large datasets labeled by a subject-matter expert. Motivated by the success of using facial muscle movements to classify brain states and the importance of learning from small amounts of data, we propose an explainable self-supervised representation-learning paradigm that learns meaningful temporal facial muscle movement patterns from limited samples. We validate our methodology by carrying out comprehensive empirical study to predict future speech behavior in a real-world dataset of adults who stutter (AWS). Our explainability study found facial muscle movements around the eyes (p<0.001) and lips (p<0.001) differ significantly before producing fluent vs. disfluent speech. Evaluations using the AWS dataset demonstrates that the proposed self-supervised approach achieves a minimum of 2.51% accuracy improvement over fully-supervised approaches."
  },
  "aaai2021_main_quantumcognitivelymotivateddecisionfusionforvideosentimentanalysis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Quantum Cognitively Motivated Decision Fusion for Video Sentiment Analysis",
    "authors": [
      "Dimitris Gkoumas",
      "Qiuchi Li",
      "Shahram Dehdashti",
      "Massimo Melucci",
      "Yijun Yu",
      "Dawei Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16165",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16165/15972",
    "published": "2021-02",
    "summary": "Video sentiment analysis as a decision-making process is inherently complex, involving the fusion of decisions from multiple modalities and the so-caused cognitive biases. Inspired by recent advances in quantum cognition, we show that the sentiment judgment from one modality could be incompatible with the judgment from another, i.e., the order matters and they cannot be jointly measured to produce a final decision. Thus the cognitive process exhibits ``quantum-like'' biases that cannot be captured by classical probability theories. Accordingly, we propose a fundamentally new, quantum cognitively motivated fusion strategy for predicting sentiment judgments. In particular, we formulate utterances as quantum superposition states of positive and negative sentiment judgments, and uni-modal classifiers as mutually incompatible observables, on a complex-valued Hilbert space with positive-operator valued measures. Experiments on two benchmarking datasets illustrate that our model significantly outperforms various existing decision level and a range of state-of-the-art content-level fusion approaches. The results also show that the concept of incompatibility allows effective handling of all combination patterns, including those extreme cases that are wrongly predicted by all uni-modal classifiers."
  },
  "aaai2021_main_towardsabetterunderstandingofvrsicknessphysicalsymptompredictionforvrcontents": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Towards a Better Understanding of VR Sickness: Physical Symptom Prediction for VR Contents",
    "authors": [
      "Hak Gu Kim",
      "Sangmin Lee",
      "Seongyeop Kim",
      "Heoun-taek Lim",
      "Yong Man Ro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16166",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16166/15973",
    "published": "2021-02",
    "summary": "We address the black-box issue of VR sickness assessment (VRSA) by evaluating the level of physical symptoms of VR sickness. For the VR contents inducing the similar VR sickness level, the physical symptoms can vary depending on the characteristics of the contents. Most of existing VRSA methods focused on assessing the overall VR sickness score. To make better understanding of VR sickness, it is required to predict and provide the level of major symptoms of VR sickness rather than overall degree of VR sickness. In this paper, we predict the degrees of main physical symptoms affecting the overall degree of VR sickness, which are disorientation, nausea, and oculomotor. In addition, we introduce a new large-scale dataset for VRSA including 360 videos with various frame rates, physiological signals, and subjective scores. On VRSA benchmark and our newly collected dataset, our approach shows a potential to not only achieve the highest correlation with subjective scores, but also to better understand which symptoms are the main causes of VR sickness."
  },
  "aaai2021_main_phasephysically-groundedabstractsocialeventsformachinesocialperception": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception",
    "authors": [
      "Aviv Netanyahu",
      "Tianmin Shu",
      "Boris Katz",
      "Andrei Barbu",
      "Joshua B. Tenenbaum"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16167",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16167/15974",
    "published": "2021-02",
    "summary": "The ability to perceive and reason about social interactions in the context of physical environments is core to human social intelligence and human-machine cooperation. However, no prior dataset or benchmark has systematically evaluated physically grounded perception of complex social interactions that go beyond short actions, such as high-fiving, or simple group activities, such as gathering. In this work, we create a dataset of physically-grounded abstract social events, PHASE, that resemble a wide range of real-life social interactions by including social concepts such as helping another agent. PHASE consists of 2D animations of pairs of agents moving in a continuous space generated procedurally using a physics engine and a hierarchical planner. Agents have a limited field of view, and can interact with multiple objects, in an environment that has multiple landmarks and obstacles. Using PHASE, we design a social recognition task and a social prediction task. PHASE is validated with human experiments demonstrating that humans perceive rich interactions in the social events, and that the simulated agents behave similarly to humans. As a baseline model, we introduce a Bayesian inverse planning approach, SIMPLE (SIMulation, Planning and Local Estimation), which outperforms state-of-the-art feed-forward neural networks. We hope that PHASE can serve as a difficult new challenge for developing new models that can recognize complex social interactions."
  },
  "aaai2021_main_riemannianembeddingbanksforcommonspatialpatternswitheeg-basedspdneuralnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Riemannian Embedding Banks for Common Spatial Patterns with EEG-based SPD Neural Networks",
    "authors": [
      "Yoon-Je Suh",
      "Byung Hyung Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16168",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16168/15975",
    "published": "2021-02",
    "summary": "Modeling non-linear data as symmetric positive definite (SPD) matrices on Riemannian manifolds has attracted much attention for various classification tasks. In the context of deep learning, SPD matrix-based Riemannian networks have been shown to be a promising solution for classifying electroencephalogram (EEG) signals, capturing the Riemannian geometry within their structured 2D feature representation. However, existing approaches usually learn spatial-temporal structures in an embedding space for all available EEG signals, and their optimization procedures rely on computationally expensive iterations. Furthermore, these approaches often struggle to encode all of the various types of relationships into a single distance metric, resulting in a loss of generality. To address the above limitations, we propose a Riemannian Embedding Banks method, which divides the problem of common spatial patterns learning in an entire embedding space into K-subproblems and builds one model for each subproblem, to be combined with SPD neural networks. By leveraging the concept of the \"separate to learn\" technology on a Riemannian manifold, REB divides the data and the embedding space into K non-overlapping subsets and learns K separate distance metrics in a Riemannian geometric space instead of the vector space. Then, the learned K non-overlapping subsets are grouped into neurons in the SPD neural network's embedding layer. Experimental results on public EEG datasets demonstrate the superiority of the proposed approach for learning common spatial patterns of EEG signals despite their non-stationary nature, increasing the convergence speed while maintaining generalization."
  },
  "aaai2021_main_plug-and-playdomainadaptationforcross-subjecteeg-basedemotionrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Plug-and-Play Domain Adaptation for Cross-Subject EEG-based Emotion Recognition",
    "authors": [
      "Li-Ming Zhao",
      "Xu Yan",
      "Bao-Liang Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16169",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16169/15976",
    "published": "2021-02",
    "summary": "Human emotion decoding in affective brain-computer interfaces suffers a major setback due to the inter-subject variability of electroencephalography (EEG) signals. Existing approaches usually require amassing extensive EEG data of each new subject, which is prohibitively time-consuming along with poor user experience. To tackle this issue, we divide EEG representations into private components specific to each subject and shared emotional components that are universal to all subjects. According to this representation partition, we propose a plug-and-play domain adaptation method for dealing with the inter-subject variability. In the training phase, subject-invariant emotional representations and private components of source subjects are separately captured by a shared encoder and private encoders. Furthermore, we build one emotion classifier on the shared partition and subjects' individual classifiers on the combination of these two partitions. In the calibration phase, the model only requires few unlabeled EEG data from incoming target subjects to model their private components. Therefore, besides the shared emotion classifier, we have another pipeline to use the knowledge of source subjects through the similarity of private components. In the test phase, we integrate predictions of the shared emotion classifier with those of individual classifiers ensemble after modulation by similarity weights. Experimental results on the SEED dataset show that our model greatly shortens the calibration time within a minute while maintaining the recognition accuracy, all of which make emotion decoding more generalizable and practicable."
  },
  "aaai2021_main_localizationinthecrowdwithtopologicalconstraints": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Localization in the Crowd with Topological Constraints",
    "authors": [
      "Shahira Abousamra",
      "Minh Hoai",
      "Dimitris Samaras",
      "Chao Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16170",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16170/15977",
    "published": "2021-02",
    "summary": "We address the problem of crowd localization, i.e., the prediction of dots corresponding to people in a crowded scene. Due to various challenges, a localization method is prone to spatial semantic errors, i.e., predicting multiple dots within a same person or collapsing multiple dots in a cluttered region. We propose a topological approach targeting these semantic errors. We introduce a topological constraint that teaches the model to reason about the spatial arrangement of dots. To enforce this constraint, we define a persistence loss based on the theory of persistent homology. The loss compares the topographic landscape of the likelihood map and the topology of the ground truth. Topological reasoning improves the quality of the localization algorithm especially near cluttered regions. On multiple public benchmarks, our method outperforms previous localization methods. Additionally, we demonstrate the potential of our method in improving the performance in the crowd counting task."
  },
  "aaai2021_main_deepeventstereoleveragedbyevent-to-imagetranslation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Event Stereo Leveraged by Event-to-Image Translation",
    "authors": [
      "Soikat Hasan Ahmed",
      "Hae Woong Jang",
      "S M Nadim Uddin",
      "Yong Ju Jung"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16171",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16171/15978",
    "published": "2021-02",
    "summary": "Depth estimation in real-world applications requires precise responses to fast motion and challenging lighting conditions. Event cameras use bio-inspired event-driven sensors that provide instantaneous and asynchronous information of pixel-level log intensity changes, which makes them suitable for depth estimation in such challenging conditions. However, as the event cameras primarily provide asynchronous and spatially sparse event data, it is hard to provide accurate dense disparity map in stereo event camera setups - especially in estimating disparities on local structures or edges. In this study, we develop a novel deep event stereo network that reconstructs spatial intensity image features from embedded event streams and leverages the event features using the reconstructed image features to compute dense disparity maps. To this end, we propose a novel event-to-image translation network with a cross-semantic attention mechanism that calculates the global semantic context of the event features for the intensity image reconstruction. In addition, a feature aggregation module is developed for accurate disparity estimation, which modulates the event features with the reconstructed image features by a stacked dilated spatially-adaptive denormalization mechanism. Experimental results reveal that our method can outperform the state-of-the-art methods by significant margins both in quantitative and qualitative measures."
  },
  "aaai2021_main_opticalflowestimationfromasinglemotion-blurredimage": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Optical Flow Estimation from a Single Motion-blurred Image",
    "authors": [
      "Dawit Mureja Argaw",
      "Junsik Kim",
      "Francois Rameau",
      "Jae Won Cho",
      "In So Kweon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16172",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16172/15979",
    "published": "2021-02",
    "summary": "In most of computer vision applications, motion blur is regarded as an undesirable artifact. However, it has been shown that motion blur in an image may have practical interests in fundamental computer vision problems. In this work, we propose a novel framework to estimate optical flow from a single motion-blurred image in an end-to-end manner. We design our network with transformer networks to learn globally and locally varying motions from encoded features of a motion-blurred input, and decode left and right frame features without explicit frame supervision. A flow estimator network is then used to estimate optical flow from the decoded features in a coarse-to-fine manner. We qualitatively and quantitatively evaluate our model through a large set of experiments on synthetic and real motion-blur datasets. We also provide in-depth analysis of our model in connection with related approaches to highlight the effectiveness and favorability of our approach. Furthermore, we showcase the applicability of the flow estimated by our method on deblurring and moving object segmentation tasks."
  },
  "aaai2021_main_motion-blurredvideointerpolationandextrapolation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Motion-blurred Video Interpolation and Extrapolation",
    "authors": [
      "Dawit Mureja Argaw",
      "Junsik Kim",
      "Francois Rameau",
      "In So Kweon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16173",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16173/15980",
    "published": "2021-02",
    "summary": "Abrupt motion of camera or objects in a scene result in a blurry video, and therefore recovering high quality video requires two types of enhancements: visual enhancement and temporal upsampling. A broad range of research attempted to recover clean frames from blurred image sequences or temporally upsample frames by interpolation, yet there are very limited studies handling both problems jointly. In this work, we present a novel framework for deblurring, interpolating and extrapolating sharp frames from a motion-blurred video in an end-to-end manner. We design our framework by first learning the pixel-level motion that caused the blur from the given inputs via optical flow estimation and then predict multiple clean frames by warping the decoded features with the estimated flows. To ensure temporal coherence across predicted frames and address potential temporal ambiguity, we propose a simple, yet effective flow-based rule. The effectiveness and favorability of our approach are highlighted through extensive qualitative and quantitative evaluations on motion-blurred datasets from high speed videos."
  },
  "aaai2021_main_disentangledmulti-relationalgraphconvolutionalnetworkforpedestriantrajectoryprediction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction",
    "authors": [
      "Inhwan Bae",
      "Hae-Gon Jeon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16174",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16174/15981",
    "published": "2021-02",
    "summary": "Pedestrian trajectory prediction is one of the important tasks required for autonomous navigation and social robots in human environments. Previous studies focused on estimating social forces among individual pedestrians. However, they did not consider the social forces of groups on pedestrians, which results in over-collision avoidance problems. To address this problem, we present a Disentangled Multi-Relational Graph Convolutional Network (DMRGCN) for socially entangled pedestrian trajectory prediction. We first introduce a novel disentangled multi-scale aggregation to better represent social interactions, among pedestrians on a weighted graph. For the aggregation, we construct the multi-relational weighted graphs based on distances and relative displacements among pedestrians. In the prediction step, we propose a global temporal aggregation to alleviate accumulated errors for pedestrians changing their directions. Finally, we apply DropEdge into our DMRGCN to avoid the over-fitting issue on relatively small pedestrian trajectory datasets. Through the effective incorporation of the three parts within an end-to-end framework, DMRGCN achieves state-of-the-art performances on a variety of challenging trajectory prediction benchmarks."
  },
  "aaai2021_main_denseeventsgroundinginvideo": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dense Events Grounding in Video",
    "authors": [
      "Peijun Bao",
      "Qian Zheng",
      "Yadong Mu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16175",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16175/15982",
    "published": "2021-02",
    "summary": "This paper explores a novel setting of temporal sentence grounding for the first time, dubbed as dense events grounding. Given an untrimmed video and a paragraph description, dense events grounding aims to jointly localize temporal moments of multiple events described in the paragraph. Our main motivating fact is that multiple events to be grounded in a video are often semantically related and temporally coordinated according to their order appearing in the paragraph. This fact sheds light on devising more accurate visual grounding model. In this work, we propose Dense Events Propagation Network (DepNet) for this novel task. DepNet first adaptively aggregates temporal and semantic information of dense events into a compact set through a second-order attention pooling, then selectively propagates the aggregated information to each single event with soft attention. Based on such aggregation-and-propagation mechanism, DepNet can effectively exploit both the temporal order and semantic relations of dense events. We conduct comprehensive experiments on large-scale datasets ActivityNet Captions and TACoS. For fair comparisons, our evaluations include both state-of-art single-event grounding methods and their natural extensions to the dense-events grounding setting implemented by us. All experiments clearly shows the performance superiority of the proposed DepNet by significant margins."
  },
  "aaai2021_main_context-awareattentionalpooling(cap)forfine-grainedvisualclassification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Context-aware Attentional Pooling (CAP) for Fine-grained Visual Classification",
    "authors": [
      "Ardhendu Behera",
      "Zachary Wharton",
      "Pradeep R P G Hewage",
      "Asish Bera"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16176",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16176/15983",
    "published": "2021-02",
    "summary": "Deep convolutional neural networks (CNNs) have shown a strong ability in mining discriminative object pose and parts information for image recognition. For fine-grained recognition, context-aware rich feature representation of object/scene plays a key role since it exhibits a significant variance in the same subcategory and subtle variance among different subcategories. Finding the subtle variance that fully characterizes the object/scene is not straightforward. To address this, we propose a novel context-aware attentional pooling (CAP) that effectively captures subtle changes via sub-pixel gradients, and learns to attend informative integral regions and their importance in discriminating different subcategories without requiring the bounding-box and/or distinguishable part annotations. We also introduce a novel feature encoding by considering the intrinsic consistency between the informativeness of the integral regions and their spatial structures to capture the semantic correlation among them. Our approach is simple yet extremely effective and can be easily applied on top of a standard classification backbone network. We evaluate our approach using six state-of-the-art (SotA) backbone networks and eight benchmark datasets. Our method significantly outperforms the SotA approaches on six datasets and is very competitive with the remaining two."
  },
  "aaai2021_main_appearance-motionmemoryconsistencynetworkforvideoanomalydetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Appearance-Motion Memory Consistency Network for Video Anomaly Detection",
    "authors": [
      "Ruichu Cai",
      "Hao Zhang",
      "Wen Liu",
      "Shenghua Gao",
      "Zhifeng Hao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16177",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16177/15984",
    "published": "2021-02",
    "summary": "Abnormal event detection in the surveillance video is an essential but challenging task, and many methods have been proposed to deal with this problem. The previous methods either only consider the appearance information or directly integrate the results of appearance and motion information without considering their endogenous consistency semantics explicitly. Inspired by the rule humans identify the abnormal frames from multi-modality signals, we propose an Appearance-Motion Memory Consistency Network (AMMC-Net). Our method first makes full use of the prior knowledge of appearance and motion signals to explicitly capture the correspondence between them in the high-level feature space. Then, it combines the multi-view features to obtain a more essential and robust feature representation of regular events, which can significantly increase the gap between an abnormal and a regular event. In the anomaly detection phase, we further introduce a commit error in the latent space joint with the prediction error in pixel space to enhance the detection accuracy. Solid experimental results on various standard datasets validate the effectiveness of our approach."
  },
  "aaai2021_main_rethinkingobjectdetectioninretailstores": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Rethinking Object Detection in Retail Stores",
    "authors": [
      "Yuanqiang Cai",
      "Longyin Wen",
      "Libo Zhang",
      "Dawei Du",
      "Weiqiang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16178",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16178/15985",
    "published": "2021-02",
    "summary": "The conventional standard for object detection uses a bounding box to represent each individual object instance. However, it is not practical in the industry-relevant applications in the context of warehouses due to severe occlusions among groups of instances of the same categories. In this paper, we propose a new task, i.e., simultaneously object localization and counting, abbreviated as Locount, which requires algorithms to localize groups of objects of interest with the number of instances. However, there does not exist a dataset or benchmark designed for such a task. To this end, we collect a large-scale object localization and counting dataset with rich annotations in retail stores, which consists of 50,394 images with more than 1.9 million object instances in 140 categories. Together with this dataset, we provide a new evaluation protocol and divide the training and testing subsets to fairly evaluate the performance of algorithms for Locount, developing a new benchmark for the Locount task. Moreover, we present a cascaded localization and counting network as a strong baseline, which gradually classifies and regresses the bounding boxes of objects with the predicted numbers of instances enclosed in the bounding boxes, trained in an end-to-end manner. Extensive experiments are conducted on the proposed dataset to demonstrate its significance and the analysis is provided to indicate future directions. Dataset is available at https://isrc.iscas.ac.cn/gitlab/research/locount-dataset."
  },
  "aaai2021_main_yolobilereal-timeobjectdetectiononmobiledevicesviacompression-compilationco-design": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "YOLObile: Real-Time Object Detection on Mobile Devices via Compression-Compilation Co-Design",
    "authors": [
      "Yuxuan Cai",
      "Hongjia Li",
      "Geng Yuan",
      "Wei Niu",
      "Yanyu Li",
      "Xulong Tang",
      "Bin\n      Ren",
      "Yanzhi Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16179",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16179/15986",
    "published": "2021-02",
    "summary": "The rapid development and wide utilization of object detection techniques have aroused attention on both accuracy and speed of object detectors. However, the current state-of-the-art object detection works are either accuracy-oriented using a large model but leading to high latency or speed-oriented using a lightweight model but sacrificing accuracy. In this work, we propose YOLObile framework, a real-time object detection on mobile devices via compression-compilation co-design. A novel block-punched pruning scheme is proposed for any kernel size. To improve computational efficiency on mobile devices, a GPU-CPU collaborativescheme is adopted along with advanced compiler-assisted optimizations. Experimental results indicate that our pruning scheme achieves 14x compression rate of YOLOv4 with 49.0 mAP. Under our YOLObile framework, we achieve 17 FPS inference speed using GPU on Samsung Galaxy S20. By incorporating our proposed GPU-CPU collaborative scheme, the inference speed is increased to 19.1 FPS, and outperforms the original YOLOv4 by 5x speedup. Source code is at: https://github.com/nightsnack/YOLObile."
  },
  "aaai2021_main_semanticmapnetbuildingallocentricsemanticmapsandrepresentationsfromegocentricviews": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Semantic MapNet: Building Allocentric Semantic Maps and Representations from Egocentric Views",
    "authors": [
      "Vincent Cartillier",
      "Zhile Ren",
      "Neha Jain",
      "Stefan Lee",
      "Irfan Essa",
      "Dhruv\n      Batra"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16180",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16180/15987",
    "published": "2021-02",
    "summary": "We study the task of semantic mapping \u2013 specifically, an embodied agent (a robot or an egocentric AI assistant) is given a tour of a new environment and asked to build an allocentric top-down semantic map (\u2018what is where?\u2019) from egocentric observations of an RGB-D camera with known pose (via localization sensors). Importantly, our goal is to build neural episodic memories and spatio-semantic representations of 3D spaces that enable the agent to easily learn subsequent tasks in the same space \u2013 navigating to objects seen during the tour (\u2018Find chair\u2019) or answering questions about the space (\u2018How many chairs did you see in the house?\u2019). Towards this goal, we present Semantic MapNet (SMNet), which consists of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame, (2) a Feature Projector that projects egocentric features to appropriate locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan length\u00d7width\u00d7feature-dims that learns to accumulate projected egocentric features, and (4) a Map Decoder that uses the memory tensor to produce semantic top-down maps. SMNet combines the strengths of (known) projective camera geometry and neural representation learning. On the task of semantic mapping in theMatterport3D dataset, SMNet significantly outperforms competitive baselines by 4.01\u221216.81% (absolute) on mean-IoU and 3.81\u221219.69% (absolute) on Boundary-F1 metrics. Moreover, we show how to use the spatio-semantic allocentric representations build by SMNet for the task of ObjectNav and Embodied Question Answering. Project page: https://vincentcartillier.github.io/smnet.html."
  },
  "aaai2021_main_understandingdeformablealignmentinvideosuper-resolution": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Understanding Deformable Alignment in Video Super-Resolution",
    "authors": [
      "Kelvin C.K. Chan",
      "Xintao Wang",
      "Ke Yu",
      "Chao Dong",
      "Chen Change Loy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16181",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16181/15988",
    "published": "2021-02",
    "summary": "Deformable convolution, originally proposed for the adaptation to geometric variations of objects, has recently shown compelling performance in aligning multiple frames and is increasingly adopted for video super-resolution. Despite its remarkable performance, its underlying mechanism for alignment remains unclear. In this study, we carefully investigate the relation between deformable alignment and the classic flow-based alignment. We show that deformable convolution can be decomposed into a combination of spatial warping and convolution. This decomposition reveals the commonality of deformable alignment and flow-based alignment in formulation, but with a key difference in their offset diversity. We further demonstrate through experiments that the increased diversity in deformable alignment yields better-aligned features, and hence significantly improves the quality of video super-resolution output. Based on our observations, we propose an offset-fidelity loss that guides the offset learning with optical flow. Experiments show that our loss successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Aside from the contributions to deformable alignment, our formulation inspires a more flexible approach to introduce offset diversity to flow-based alignment, improving its performance."
  },
  "aaai2021_main_deepmetriclearningwithgraphconsistency": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Metric Learning with Graph Consistency",
    "authors": [
      "Binghui Chen",
      "Pengyu Li",
      "Zhaoyi Yan",
      "Biao Wang",
      "Lei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16182",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16182/15989",
    "published": "2021-02",
    "summary": "Deep Metric Learning (DML) has been more attractive and widely applied in many computer vision tasks, in which a discriminative embedding is requested such that the image features belonging to the same class are gathered together and the ones belonging to different classes are pushed apart. Most existing works insist to learn this discriminative embedding by either devising powerful pair-based loss functions or hard-sample mining strategies. However, in this paper, we start from an another perspective and propose Deep Consistent Graph Metric Learning (CGML) framework to enhance the discrimination of the learned embedding. It is mainly achieved by rethinking the conventional distance constraints as a graph regularization and then introducing a Graph Consistency regularization term, which intends to optimize the feature distribution from a global graph perspective. Inspired by the characteristic of our defined \u2019Discriminative Graph\u2019, which regards DML from another novel perspective, the Graph Consistency regularization term encourages the sub-graphs randomly sampled from the training set to be consistent. We show that our CGML indeed serves as an efficient technique for learning towards discriminative embedding and is applicable to various popular metric objectives, e.g. Triplet, N-Pair and Binomial losses. This paper empirically and experimentally demonstrates the effectiveness of our graph regularization idea, achieving competitive results on the popular CUB, CARS, Stanford Online Products and In-Shop datasets."
  },
  "aaai2021_main_cnnprofileronpolarcoordinateimagesfortropicalcyclonestructureanalysis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "CNN Profiler on Polar Coordinate Images for Tropical Cyclone Structure Analysis",
    "authors": [
      "Boyo Chen",
      "Buo-Fu Chen",
      "Chun Min Hsiao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16183",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16183/15990",
    "published": "2021-02",
    "summary": "Convolutional neural networks (CNN) have achieved great success in analyzing tropical cyclones (TC) with satellite images in several tasks, such as TC intensity estimation. In contrast, TC structure, which is conventionally described by a few parameters estimated subjectively by meteorology specialists, is still hard to be profiled objectively and routinely. This study applies CNN on satellite images to create the entire TC structure profiles, covering all the structural parameters. By utilizing the meteorological domain knowledge to construct TC wind profiles based on historical structure parameters, we provide valuable labels for training in our newly released benchmark dataset. With such a dataset, we hope to attract more attention to this crucial issue among data scientists. Meanwhile, a baseline is established based on a specialized convolutional model operating on polar-coordinates. We discovered that it is more feasible and physically reasonable to extract structural information on polar-coordinates, instead of Cartesian coordinates, according to a TC\u2019s rotational and spiral natures. Experimental results on the released benchmark dataset verified the robustness of the proposed model and demonstrated the potential for applying deep learning techniques for this barely developed yet important topic. For codes and implementation details, please visit https://github.com/BoyoChen/TCSA-CNN-profiler."
  },
  "aaai2021_main_commonsenseknowledgeawareconceptselectionfordiverseandinformativevisualstorytelling": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Commonsense Knowledge Aware Concept Selection For Diverse and Informative Visual Storytelling",
    "authors": [
      "Hong Chen",
      "Yifei Huang",
      "Hiroya Takamura",
      "Hideki Nakayama"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16184",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16184/15991",
    "published": "2021-02",
    "summary": "Visual storytelling is a task of generating relevant and interesting stories for given image sequences. In this work we aim at increasing the diversity of the generated stories while preserving the informative content from the images. We propose to foster the diversity and informativeness of a generated story by using a concept selection module that suggests a set of concept candidates. Then, we utilize a large scale pre-trained model to convert concepts and images into full stories. To enrich the candidate concepts, a commonsense knowledge graph is created for each image sequence from which the concept candidates are proposed. To obtain appropriate concepts from the graph, we propose two novel modules that consider the correlation among candidate concepts and the image-concept correlation. Extensive automatic and human evaluation results demonstrate that our model can produce reasonable concepts. This enables our model to outperform the previous models by a large margin on the diversity and informativeness of the story, while retaining the relevance of the story to the image sequence."
  },
  "aaai2021_main_attention-basedmulti-levelfusionnetworkforlightfielddepthestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Attention-based Multi-Level Fusion Network for Light Field Depth Estimation",
    "authors": [
      "Jiaxin Chen",
      "Shuo Zhang",
      "Youfang Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16185",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16185/15992",
    "published": "2021-02",
    "summary": "Depth estimation from Light Field (LF) images is a crucial basis for LF related applications. Since multiple views with abundant information are available, how to effectively fuse features of these views is a key point for accurate LF depth estimation. In this paper, we propose a novel attention-based multi-level fusion network. Combined with the four-branch structure, we design intra-branch fusion strategy and inter-branch fusion strategy to hierarchically fuse effective features from different views. By introducing the attention mechanism, features of views with less occlusions and richer textures are selected inside and between these branches to provide more effective information for depth estimation. The depth maps are finally estimated after further aggregation. Experimental results shows the proposed method achieves state-of-the-art performance in both quantitative and qualitative evaluation, which also ranks first in the commonly used HCI 4D Light Field Benchmark."
  },
  "aaai2021_main_jointdemosaickinganddenoisinginthewildthecaseoftrainingundergroundtruthuncertainty": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Joint Demosaicking and Denoising in the Wild: The Case of Training Under Ground Truth Uncertainty",
    "authors": [
      "Jierun Chen",
      "Song Wen",
      "S.-H. Gary Chan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16186",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16186/15993",
    "published": "2021-02",
    "summary": "Image demosaicking and denoising are the two key fundamental steps in digital camera pipelines, aiming to reconstruct clean color images from noisy luminance readings. In this paper, we propose and study Wild-JDD, a novel learning framework for joint demosaicking and denoising in the wild. In contrast to previous works which generally assume the ground truth of training data is a perfect reflection of the reality, we consider here the more common imperfect case of ground truth uncertainty in the wild. We first illustrate its manifestation as various kinds of artifacts including zipper effect, color moire and residual noise. Then we formulate a two-stage data degradation process to capture such ground truth uncertainty, where a conjugate prior distribution is imposed upon a base distribution. After that, we derive an evidence lower bound (ELBO) loss to train a neural network that approximates the parameters of the conjugate prior distribution conditioned on the degraded input. Finally, to further enhance the performance for out-of-distribution input, we design a simple but effective fine-tuning strategy by taking the input as a weakly informative prior. Taking into account ground truth uncertainty, Wild-JDD enjoys good interpretability during optimization. Extensive experiments validate that it outperforms state-of-the-art schemes on joint demosaicking and denoising tasks on both synthetic and realistic raw datasets."
  },
  "aaai2021_main_spatial-temporalcausalinferenceforpartialimage-to-videoadaptation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Spatial-temporal Causal Inference for Partial Image-to-video Adaptation",
    "authors": [
      "Jin Chen",
      "Xinxiao Wu",
      "Yao Hu",
      "Jiebo Luo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16187",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16187/15994",
    "published": "2021-02",
    "summary": "Image-to-video adaptation leverages off-the-shelf learned models in labeled images to help classification in unlabeled videos, thus alleviating the high computation overhead of training a video classifier from scratch. This task is very challenging since there exist two types of domain shifts between images and videos: 1) spatial domain shift caused by static appearance variance between images and video frames, and 2) temporal domain shift caused by the absence of dynamic motion in images. Moreover, for different video classes, these two domain shifts have different effects on the domain gap and should not be treated equally during adaptation. In this paper, we propose a spatial-temporal causal inference framework for image-to-video adaptation. We first construct a spatial-temporal causal graph to infer the effects of the spatial and temporal domain shifts by performing counterfactual causality. We then learn causality-guided bidirectional heterogeneous mappings between images and videos to adaptively reduce the two domain shifts. Moreover, to relax the assumption that the label spaces of the image and video domains are the same by the existing methods, we incorporate class-wise alignment into the learning of image-video mappings to perform partial image-to-video adaptation where the image label space subsumes the video label space. Extensive experiments on several video datasets have validated the effectiveness of our proposed method."
  },
  "aaai2021_main_ref-nmsbreakingproposalbottlenecksintwo-stagereferringexpressiongrounding": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding",
    "authors": [
      "Long Chen",
      "Wenbo Ma",
      "Jun Xiao",
      "Hanwang Zhang",
      "Shih-Fu Chang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16188",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16188/15995",
    "published": "2021-02",
    "summary": "The prevailing framework for solving referring expression grounding is based on a two-stage process: 1) detecting proposals with an object detector and 2) grounding the referent to one of the proposals. Existing two-stage solutions mostly focus on the grounding step, which aims to align the expressions with the proposals. In this paper, we argue that these methods overlook an obvious mismatch between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (i.e., expression-agnostic), hoping that the proposals contain all right instances in the expression (i.e., expression-aware). Due to this mismatch, current two-stage methods suffer from a severe performance drop between detected and ground-truth proposals. To this end, we propose Ref-NMS, which is the first method to yield expression-aware proposals at the first stage. Ref-NMS regards all nouns in the expression as critical objects, and introduces a lightweight module to predict a score for aligning each box with a critical object. These scores can guide the NMS operation to filter out the boxes irrelevant to the expression, increasing the recall of critical objects, resulting in a significantly improved grounding performance. Since Ref- NMS is agnostic to the grounding step, it can be easily integrated into any state-of-the-art two-stage method. Extensive ablation studies on several backbones, benchmarks, and tasks consistently demonstrate the superiority of Ref-NMS. Codes are available at: https://github.com/ChopinSharp/ref-nms."
  },
  "aaai2021_main_rspnetrelativespeedperceptionforunsupervisedvideorepresentationlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning",
    "authors": [
      "Peihao Chen",
      "Deng Huang",
      "Dongliang He",
      "Xiang Long",
      "Runhao Zeng",
      "Shilei\n      Wen",
      "Mingkui Tan",
      "Chuang Gan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16189",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16189/15996",
    "published": "2021-02",
    "summary": "We study unsupervised video representation learning that seeks to learn both motion and appearance features from unlabeled video only, which can be reused for downstream tasks such as action recognition. This task, however, is extremely challenging due to 1) the highly complex spatial-temporal information in videos and 2) the lack of labeled data for training. Unlike representation learning for static images, it is difficult to construct a suitable self-supervised task to effectively model both motion and appearance features. More recently, several attempts have been made to learn video representation through video playback speed prediction. However, it is non-trivial to obtain precise speed labels for the videos. More critically, the learned models may tend to focus on motion patterns and thus may not learn appearance features well. In this paper, we observe that the relative playback speed is more consistent with motion patterns and thus provides more effective and stable supervision for representation learning. Therefore, we propose a new way to perceive the playback speed and exploit the relative speed between two video clips as labels. In this way, we are able to effectively perceive speed and learn better motion features. Moreover, to ensure the learning of appearance features, we further propose an appearance-focused task, where we enforce the model to perceive the appearance difference between two video clips. We show that jointly optimizing the two tasks consistently improves the performance on two downstream tasks (namely, action recognition and video retrieval) w.r.t the increasing pre-training epochs. Remarkably, for action recognition on the UCF101 dataset, we achieve 93.7% accuracy without the use of labeled data for pre-training, which outperforms the ImageNet supervised pre-trained model. Our code, pre-trained models, and supplementary materials can be found at https://github.com/PeihaoChen/RSPNet."
  },
  "aaai2021_main_dualdistributionalignmentnetworkforgeneralizablepersonre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dual Distribution Alignment Network for Generalizable Person Re-Identification",
    "authors": [
      "Peixian Chen",
      "Pingyang Dai",
      "Jianzhuang Liu",
      "Feng Zheng",
      "Mingliang Xu",
      "Qi\n      Tian",
      "Rongrong Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16190",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16190/15997",
    "published": "2021-02",
    "summary": "Domain generalization (DG) offers a preferable real-world setting for Person Re-Identification (Re-ID), which trains a model using multiple source domain datasets and expects it to perform well in an unseen target domain without any model updating. Unfortunately, most DG approaches are designed explicitly for classification tasks, which fundamentally differs from the retrieval task Re-ID. Moreover, existing applications of DG in Re-ID cannot correctly handle the massive variation among Re-ID datasets. In this paper, we identify two fundamental challenges in DG for Person Re-ID: domain-wise variations and identity-wise similarities. To this end, we propose an end-to-end Dual Distribution Alignment Network (DDAN) to learn domain-invariant features with dual-level constraints: the domain-wise adversarial feature learning and the identity-wise similarity enhancement. These constraints effectively reduce the domain-shift among multiple source domains further while agreeing to real-world scenarios. We evaluate our method in a large-scale DG Re-ID benchmark and compare it with various cutting-edge DG approaches. Quantitative results show that DDAN achieves state-of-the-art performance."
  },
  "aaai2021_main_rgb-dsalientobjectdetectionvia3dconvolutionalneuralnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "RGB-D Salient Object Detection via 3D Convolutional Neural Networks",
    "authors": [
      "Qian Chen",
      "Ze Liu",
      "Yi Zhang",
      "Keren Fu",
      "Qijun Zhao",
      "Hongwei Du"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16191",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16191/15998",
    "published": "2021-02",
    "summary": "RGB-D salient object detection (SOD) recently has attracted increasing research interest and many deep learning methods based on encoder-decoder architectures have emerged. However, most existing RGB-D SOD models conduct feature fusion either in the single encoder or the decoder stage, which hardly guarantees sufficient cross-modal fusion ability. In this paper, we make the first attempt in addressing RGB-D SOD through 3D convolutional neural networks. The proposed model, named RD3D, aims at pre-fusion in the encoder stage and in-depth fusion in the decoder stage to effectively promote the full integration of RGB and depth streams. Specifically, RD3D first conducts pre-fusion across RGB and depth modalities through an inflated 3D encoder, and later provides in-depth feature fusion by designing a 3D decoder equipped with rich back-projection paths (RBPP) for leveraging the extensive aggregation ability of 3D convolutions. With such a progressive fusion strategy involving both the encoder and decoder, effective and thorough interaction between the two modalities can be exploited and boost the detection accuracy. Extensive experiments on six widely used benchmark datasets demonstrate that RD3D performs favorably against 14 state-of-the-art RGB-D SOD approaches in terms of four key evaluation metrics. Our code will be made publicly available: https://github.com/PPOLYpubki/RD3D."
  },
  "aaai2021_main_mind-the-gap!unsuperviseddomainadaptationfortext-videoretrieval": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Mind-the-Gap! Unsupervised Domain Adaptation for Text-Video Retrieval",
    "authors": [
      "Qingchao Chen",
      "Yang Liu",
      "Samuel Albanie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16192",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16192/15999",
    "published": "2021-02",
    "summary": "When can we expect a text-video retrieval system to work effectively on datasets that differ from its training domain?In this work, we investigate this question through the lens of unsupervised domain adaptation in which the objective is to match natural language queries and video content in the presence of domain shift at query-time.Such systems have significant practical applications since they are capable generalising to new data sources without requiring corresponding text annotations. We make the following contributions: (1) We propose theUDAVR (Unsupervised Domain Adaptation for Video Retrieval) benchmark and employ it to study the performance of text-video retrieval in the presence of domain shift. (2) We propose Concept-Aware-Pseudo-Query (CAPQ), a method for learning discriminative and transferable features that bridge these cross-domain discrepancies to enable effective target domain retrieval using source domain supervision. (3) We show that CAPQ outperforms alternative domain adaptation strategies on UDAVR."
  },
  "aaai2021_main_localrelationlearningforfaceforgerydetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Local Relation Learning for Face Forgery Detection",
    "authors": [
      "Shen Chen",
      "Taiping Yao",
      "Yang Chen",
      "Shouhong Ding",
      "Jilin Li",
      "Rongrong Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16193",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16193/16000",
    "published": "2021-02",
    "summary": "With the rapid development of facial manipulation techniques, face forgery has received considerable attention in digital media forensics due to security concerns. Most existing methods formulate face forgery detection as a classification problem and utilize binary labels or manipulated region masks as supervision. However, without considering the correlation between local regions, these global supervisions are insufficient to learn a generalized feature and prone to overfitting. To address this issue, we propose a novel perspective of face forgery detection via local relation learning. Specifically, we propose a Multi-scale Patch Similarity Module (MPSM), which measures the similarity between features of local regions and forms a robust and generalized similarity pattern. Moreover, we propose an RGB-Frequency Attention Module (RFAM) to fuse information in both RGB and frequency domains for more comprehensive local feature representation, which further improves the reliability of the similarity pattern. Extensive experiments show that the proposed method consistently outperforms the state-of-the-arts on widely-used benchmarks. Furthermore, detailed visualization shows the robustness and interpretability of our method."
  },
  "aaai2021_main_deductivelearningforweakly-supervised3dhumanposeestimationviauncalibratedcameras": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deductive Learning for Weakly-Supervised 3D Human Pose Estimation via Uncalibrated Cameras",
    "authors": [
      "Xipeng Chen",
      "Pengxu Wei",
      "Liang Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16194",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16194/16001",
    "published": "2021-02",
    "summary": "Without prohibitive and laborious 3D annotations, weakly-supervised 3D human pose methods mainly employ the model regularization with geometric projection consistency or geometry estimation from multi-view images. Nevertheless, those approaches explicitly need known parameters of calibrated cameras, exhibiting a limited model generalization in various realistic scenarios. To mitigate this issue, in this paper, we propose a Deductive Weakly-Supervised Learning (DWSL) for 3D human pose machine. Our DWSL firstly learns latent representations on depth and camera pose for 3D pose reconstruction. Since weak supervision usually causes ill-conditioned learning or inferior estimation, our DWSL introduces deductive reasoning to make an inference for the human pose from a view to another and develops a reconstruction loss to demonstrate what the model learns and infers is reliable.This learning by deduction strategy employs the view-transform demonstration and structural rules derived from depth, geometry and angle constraints, which improves the reliability of the model training with weak supervision. On three 3D human pose benchmarks, we conduct extensive experiments to evaluate our proposed method, which achieves superior performance in comparison with state-of-the-art weak-supervised methods. Particularly, our model shows an appealing potential for learning from 2D data captured in dynamic outdoor scenes, which demonstrates promising robustness and generalization in realistic scenarios. Our code is publicly available at https://github.com/Xipeng-Chen/DWSL-3D-pose."
  },
  "aaai2021_main_aunifiedmulti-scenarioattackingnetworkforvisualobjecttracking": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Unified Multi-Scenario Attacking Network for Visual Object Tracking",
    "authors": [
      "Xuesong Chen",
      "Canmiao Fu",
      "Feng Zheng",
      "Yong Zhao",
      "Hongsheng Li",
      "Ping Luo",
      "Guo-Jun Qi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16195",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16195/16002",
    "published": "2021-02",
    "summary": "Existing methods of adversarial attacks successfully generate adversarial examples to confuse Deep Neural Networks (DNNs) of image classification and object detection, resulting in wrong predictions. However, these methods are difficult to attack models of video object tracking, because the tracking algorithms could handle sequential information across video frames and the categories of targets tracked are normally unknown in advance. In this paper, we propose a Unified and Effective Network, named UEN, to attack visual object tracking models.There are several appealing characteristics of UEN: (1) UEN could produce various invisible adversarialperturbations according to different attack settings by using only one simple end-to-end network with three ingenious loss function; (2) UEN could generate general visible adversarial patch patterns to attack the advanced trackers in the real-world; (3) Extensive experiments show that UEN is able to attack many state-of-the-art trackers effectively (e.g. SiamRPN-based networks and DiMP) on popular tracking datasets including OTB100, UAV123, and GOT10K, making online real-time attacks possible. The attack results outperform the introduced baseline in terms of attacking ability and attacking efficiency."
  },
  "aaai2021_main_ssd-ganmeasuringtherealnessinthespatialandspectraldomains": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SSD-GAN: Measuring the Realness in the Spatial and Spectral Domains",
    "authors": [
      "Yuanqi Chen",
      "Ge Li",
      "Cece Jin",
      "Shan Liu",
      "Thomas Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16196",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16196/16003",
    "published": "2021-02",
    "summary": "This paper observes that there is an issue of high frequencies missing in the discriminator of standard GAN, and we reveal it stems from downsampling layers employed in the network architecture. This issue makes the generator lack the incentive from the discriminator to learn high-frequency content of data, resulting in a significant spectrum discrepancy between generated images and real images. Since the Fourier transform is a bijective mapping, we argue that reducing this spectrum discrepancy would boost the performance of GANs. To this end, we introduce SSD-GAN, an enhancement of GANs to alleviate the spectral information loss in the discriminator. Specifically, we propose to embed a frequency-aware classifier into the discriminator to measure the realness of the input in both the spatial and spectral domains. With the enhanced discriminator, the generator of SSD-GAN is encouraged to learn high-frequency content of real data and generate exact details. The proposed method is general and can be easily integrated into most existing GANs framework without excessive cost. The effectiveness of SSD-GAN is validated on various network architectures, objective functions, and datasets. Code is available at https://github.com/cyq373/SSD-GAN."
  },
  "aaai2021_main_multi-scalespatialtemporalgraphconvolutionalnetworkforskeleton-basedactionrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Multi-Scale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition",
    "authors": [
      "Zhan Chen",
      "Sicheng Li",
      "Bing Yang",
      "Qinghan Li",
      "Hong Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16197",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16197/16004",
    "published": "2021-02",
    "summary": "Graph convolutional networks have been widely used for skeleton-based action recognition due to their excellent modeling ability of non-Euclidean data. As the graph convolution is a local operation, it can only utilize the short-range joint dependencies and short-term trajectory but fails to directly model the distant joints relations and long-range temporal information that are vital to distinguishing various actions. To solve this problem, we present a multi-scale spatial graph convolution (MS-GC) module and a multi-scale temporal graph convolution (MT-GC) module to enrich the receptive field of the model in spatial and temporal dimensions. \tConcretely, the MS-GC and MT-GC modules decompose the corresponding local graph convolution into a set of sub-graph convolution, forming a hierarchical residual architecture. Without introducing additional parameters, the features will be processed with a series of sub-graph convolutions, and each node could complete multiple spatial and temporal aggregations with its neighborhoods. The final equivalent receptive field is accordingly enlarged, which is capable of capturing both short- and long-range dependencies in spatial and temporal domains. By coupling these two modules as a basic block, we further propose a multi-scale spatial temporal graph convolutional network (MST-GCN), which stacks multiple blocks to learn effective motion representations for action recognition. The proposed MST-GCN achieves remarkable performance on three challenging benchmark datasets, NTU RGB+D, NTU-120 RGB+D and Kinetics-Skeleton, for skeleton-based action recognition."
  },
  "aaai2021_main_cascadenetworkwithguidedlossandhybridattentionforfindinggoodcorrespondences": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Cascade Network with Guided Loss and Hybrid Attention for Finding Good Correspondences",
    "authors": [
      "Zhi Chen",
      "Fan Yang",
      "Wenbing Tao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16198",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16198/16005",
    "published": "2021-02",
    "summary": "Finding good correspondences is a critical prerequisite in many feature based tasks. Given a putative correspondence set of an image pair, we propose a neural network which finds correct correspondences by a binary-class classifier and estimates relative pose through classified correspondences. First, we analyze that due to the imbalance in the number of correct and wrong correspondences, the loss function has a great impact on the classification results. Thus, we propose a new Guided Loss that can directly use evaluation criterion (Fn-measure) as guidance to dynamically adjust the objective function during training. We theoretically prove that the perfect negative correlation between the Guided Loss and Fn-measure, so that the network is always trained towards the direction of increasing Fn-measure to maximize it. We then propose a hybrid attention block to extract feature, which integrates the Bayesian attentive context normalization (BACN) and channel-wise attention (CA). BACN can mine the prior information to better exploit global context and CA can capture complex channel context to enhance the channel awareness of the network. Finally, based on our Guided Loss and hybrid attention block, a cascade network is designed to gradually optimize the result for more superior performance. Experiments have shown that our network achieves the state-of-the-art performance on benchmark datasets. Our code will be available in https://github.com/wenbingtao/GLHA."
  },
  "aaai2021_main_generalizablerepresentationlearningformixturedomainfaceanti-spoofing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing",
    "authors": [
      "Zhihong Chen",
      "Taiping Yao",
      "Kekai Sheng",
      "Shouhong Ding",
      "Ying Tai",
      "Jilin Li",
      "Feiyue Huang",
      "Xinyu Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16199",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16199/16006",
    "published": "2021-02",
    "summary": "Face anti-spoofing approach based on domain generalization (DG) has drawn growing attention due to its robustness for unseen scenarios. Existing DG methods assume that the domain label is known. However, in real-world applications, the collected dataset always contains mixture domains, where the domain label is unknown. In this case, most of existing methods may not work. Further, even if we can obtain the domain label as existing methods, we think this is just a sub-optimal partition. To overcome the limitation, we propose domain dynamic adjustment meta-learning (D$^2$AM) without using domain labels, which iteratively divides mixture domains via discriminative domain representation and trains a generalizable face anti-spoofing with meta-learning. Specifically, we design a domain feature based on Instance Normalization (IN)and propose a domain representation learning module (DRLM) to extract discriminative domain features for clustering. Moreover, to reduce the side effect of outliers on clustering performance, we additionally utilize maximum mean discrepancy (MMD) to align the distribution ofsample features to a prior distribution, which improves the reliability of clustering. Extensive experiments show that the proposed method outperforms conventional DG-based face anti-spoofing methods, including those utilizing domain labels. Furthermore, we enhance the interpretability through visualization."
  },
  "aaai2021_main_sspc-netsemi-supervisedsemantic3dpointcloudsegmentationnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SSPC-Net: Semi-supervised Semantic 3D Point Cloud Segmentation Network",
    "authors": [
      "Mingmei Cheng",
      "Le Hui",
      "Jin Xie",
      "Jian Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16200",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16200/16007",
    "published": "2021-02",
    "summary": "Point cloud semantic segmentation is a crucial task in 3D scene understanding. Existing methods mainly focus on employing a large number of annotated labels for supervised semantic segmentation. Nonetheless, manually labeling such large point clouds for the supervised segmentation task is time-consuming. In order to reduce the number of annotated labels, we propose a semi-supervised semantic point cloud segmentation network, named SSPC-Net, where we train the semantic segmentation network by inferring the labels of unlabeled points from the few annotated 3D points. In our method, we first partition the whole point cloud into superpoints and build superpoint graphs to mine the long-range dependencies in point clouds. Based on the constructed superpoint graph, we then develop a dynamic label propagation method to generate the pseudo labels for the unsupervised superpoints. Particularly, we adopt a superpoint dropout strategy to dynamically select the generated pseudo labels. In order to fully exploit the generated pseudo labels of the unsupervised superpoints, we furthermore propose a coupled attention mechanism for superpoint feature embedding. Finally, we employ the cross-entropy loss to train the semantic segmentation network with the labels of the supervised superpoints and the pseudo labels of the unsupervised superpoints. Experiments on various datasets demonstrate that our semisupervised segmentation method can achieve better performance than the current semi-supervised segmentation method with fewer annotated 3D points."
  },
  "aaai2021_main_deepfeaturespacetrojanattackofneuralnetworksbycontrolleddetoxification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification",
    "authors": [
      "Siyuan Cheng",
      "Yingqi Liu",
      "Shiqing Ma",
      "Xiangyu Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16201",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16201/16008",
    "published": "2021-02",
    "summary": "Trojan (backdoor) attack is a form of adversarial attack on deep neural networks where the attacker provides victims with a model trained/retrained on malicious data. The backdoor can be activated when a normal input is stamped with a certain pattern called trigger, causing misclassification. Many existing trojan attacks have their triggers being input space patches/objects (e.g., a polygon with solid color) or simple input transformations such as Instagram filters. These simple triggers are susceptible to recent backdoor detection algorithms. We propose a novel deep feature space trojan attack with five characteristics: effectiveness, stealthiness, controllability, robustness and reliance on deep features. We conduct extensive experiments on 9 image classifiers on various datasets including ImageNet to demonstrate these properties and show that our attack can evade state-of-the-art defense."
  },
  "aaai2021_main_graphandtemporalconvolutionalnetworksfor3dmulti-personposeestimationinmonocularvideos": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Graph and Temporal Convolutional Networks for 3D Multi-person Pose Estimation in Monocular Videos",
    "authors": [
      "Yu Cheng",
      "Bo Wang",
      "Bo Yang",
      "Robby T. Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16202",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16202/16009",
    "published": "2021-02",
    "summary": "Despite the recent progress, 3D multi-person pose estimation from monocular videos is still challenging due to the commonly encountered problem of missing information caused by occlusion, partially out-of-frame target persons, and inaccurate person detection. To tackle this problem, we propose a novel framework integrating graph convolutional networks (GCNs) and temporal convolutional networks (TCNs) to robustly estimate camera-centric multi-person 3D poses that does not require camera parameters. In particular, we introduce a human-joint GCN, which unlike the existing GCN, is based on a directed graph that employs the 2D pose estimator's confidence scores to improve the pose estimation results. We also introduce a human-bone GCN, which models the bone connections and provides more information beyond human joints. The two GCNs work together to estimate the spatial frame-wise 3D poses and can make use of both visible joint and bone information in the target frame to estimate the occluded or missing human-part information. To further refine the 3D pose estimation, we use our temporal convolutional networks (TCNs) to enforce the temporal and human-dynamics constraints. We use a joint-TCN to estimate person-centric 3D poses across frames, and propose a velocity-TCN to estimate the speed of 3D joints to ensure the consistency of the 3D pose estimation in consecutive frames. Finally, to estimate the 3D human poses for multiple persons, we propose a root-TCN that estimates camera-centric 3D poses without requiring camera parameters. Quantitative and qualitative evaluations demonstrate the effectiveness of the proposed method."
  },
  "aaai2021_main_dramaqacharacter-centeredvideostoryunderstandingwithhierarchicalqa": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DramaQA: Character-Centered Video Story Understanding with Hierarchical QA",
    "authors": [
      "Seongho Choi",
      "Kyoung-Woon On",
      "Yu-Jung Heo",
      "Ahjeong Seo",
      "Youwon Jang",
      "Minsu\n      Lee",
      "Byoung-Tak Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16203",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16203/16010",
    "published": "2021-02",
    "summary": "Despite recent progress on computer vision and natural language processing, developing a machine that can understand video story is still hard to achieve due to the intrinsic difficulty of video story. Moreover, researches on how to evaluate the degree of video understanding based on human cognitive process have not progressed as yet. In this paper, we propose a novel video question answering (Video QA) task, DramaQA, for a comprehensive understanding of the video story. The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an evaluation metric based on the cognitive developmental stages of human intelligence. 2) Character-centered video annotations to model local coherence of the story. Our dataset is built upon the TV drama \"Another Miss Oh\" and it contains 17,983 QA pairs from 23,928 various length video clips, with each QA pair belonging to one of four difficulty levels. We provide 217,308 annotated images with rich character-centered annotations, including visual bounding boxes, behaviors and emotions of main characters, and coreference resolved scripts. Additionally, we suggest Multi-level Context Matching model which hierarchically understands character-centered representations of video to answer questions. We release our dataset and model publicly for research purposes, and we expect our work to provide a new perspective on video story understanding research."
  },
  "aaai2021_main_deepcollaborationcollaborativegenerativeanddiscriminativemodelsforclassincrementallearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DeepCollaboration: Collaborative Generative and Discriminative Models for Class Incremental Learning",
    "authors": [
      "Bo Cui",
      "Guyue Hu",
      "Shan Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16204",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16204/16011",
    "published": "2021-02",
    "summary": "An important challenge for neural networks is to learn incrementally, i.e., learn new classes without catastrophic forgetting. To overcome this problem, generative replay technique has been suggested, which can generate samples belonging to learned classes while learning new ones. However, such generative models usually suffer from increased distribution mismatch between the generated and original samples along the learning process. In this work, we propose DeepCollaboration (D-Collab), a collaborative framework of deep generative and discriminative models to solve this problem effectively. We develop a discriminative learning model to incrementally update the latent feature space for continual classification. At the same time, a generative model is introduced to achieve conditional generation using the latent feature distribution produced by the discriminative model. Importantly, the generative and discriminative models are connected through bidirectional training to enforce cycle-consistency of mappings between feature and image domains. Furthermore, a domain alignment module is used to eliminate the divergence between the feature distributions of generated images and real ones. This module together with the discriminative model can perform effective sample mining to facilitate incremental learning. Extensive experiments on several visual recognition datasets show that our system can achieve state-of-the-art performance."
  },
  "aaai2021_main_splitthenrefinestackedattention-guidedresunetsforblindsingleimagevisiblewatermarkremoval": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Split then Refine: Stacked Attention-guided ResUNets for Blind Single Image Visible Watermark Removal",
    "authors": [
      "Xiaodong Cun",
      "Chi-Man Pun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16205",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16205/16012",
    "published": "2021-02",
    "summary": "Digital watermark is a commonly used technique to protect the copyright of medias. Simultaneously, to increase the robustness of watermark, attacking technique, such as watermark removal, also gets the attention from the community. Previous watermark removal methods require to gain the watermark location from users or train a multi-task network to recover the background indiscriminately. However, when jointly learning, the network performs better on watermark detection than recovering the texture. Inspired by this observation and to erase the visible watermarks blindly, we propose a novel two-stage framework with a stacked attention-guided ResUNets to simulate the process of detection, removal and refinement.In the first stage, we design a multi-task network called SplitNet. It learns the basis features for three sub-tasks altogether while the task-specific features separately use multiple channel attentions. Then, with the predicted mask and coarser restored image, we design RefineNet to smooth the watermarked region with a mask-guided spatial attention. Besides network structure, the proposed algorithm also combines multiple perceptual losses for better quality both visually and numerically.We extensively evaluate our algorithm over four different datasets under various settings and the experiments show that our approach outperforms other state-of-the-art methods by a large margin."
  },
  "aaai2021_main_rsgnetrelationbasedskeletongraphnetworkforcrowdedscenesposeestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "RSGNet: Relation based Skeleton Graph Network for Crowded Scenes Pose Estimation",
    "authors": [
      "Yan Dai",
      "Xuanhan Wang",
      "Lianli Gao",
      "Jingkuan Song",
      "Heng Tao Shen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16206",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16206/16013",
    "published": "2021-02",
    "summary": "Despite of the recent great progress on multi-person pose estimation, existing solutions still remain challenging under the condition of\"crowded scenes'', where RGB images capture complex real-world scenes with highly-overlapped people, severe occlusions and diverse postures. In this work, we focus on two main problems: 1) how to design an effective pipeline for crowded scenes pose estimation; and 2) how to equip this pipeline with the ability of relation modeling for interference resolving. To tackle these problems, we propose a new pipeline named Relation based Skeleton Graph Network (RSGNet). Unlike existing works that directly predict joints-of-target by labeling joints-of-interference as false positive, we first encourage all joints to be predicted. And then, a Target-aware Relation Parser (TRP) is designed to model the relation over all predicted joints, resulting in a target-aware encoding. This new pipeline will largely relieve the confusion of the joints estimation model when seeing identical joints with totally distinct labels (e.g., the identical hand exists in two bounding boxes). Furthermore, we introduce a Skeleton Graph Machine (SGM) to model the skeleton-based commonsense knowledge, aiming to estimate the target pose with the constraint of human body structure. Such skeleton-based constraint can help to deal with the challenges in crowded scenes from a reasoning perspective. Solid experiments on pose estimation benchmarks demonstrate that our method outperforms existing state-of-the-art methods."
  },
  "aaai2021_main_voxelr-cnntowardshighperformancevoxel-based3dobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection",
    "authors": [
      "Jiajun Deng",
      "Shaoshuai Shi",
      "Peiwei Li",
      "Wengang Zhou",
      "Yanyong Zhang",
      "Houqiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16207",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16207/16014",
    "published": "2021-02",
    "summary": "Recent advances on 3D object detection heavily rely on how the 3D data are represented, i.e., voxel-based or point-based representation. Many existing high performance 3D detectors are point-based because this structure can better retain precise point positions. Nevertheless, point-level features lead to high computation overheads due to unordered storage. In contrast, the voxel-based structure is better suited for feature extraction but often yields lower accuracy because the input data are divided into grids. In this paper, we take a slightly different viewpoint --- we find that precise positioning of raw points is not essential for high performance 3D object detection and that the coarse voxel granularity can also offer sufficient detection accuracy. Bearing this view in mind, we devise a simple but effective voxel-based framework, named Voxel R-CNN. By taking full advantage of voxel features in a two-stage approach, our method achieves comparable detection accuracy with state-of-the-art point-based models, but at a fraction of the computation cost.Voxel R-CNN consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network, and a detect head. A voxel RoI pooling is devised to extract RoI features directly from voxel features for further refinement. Extensive experiments are conducted on the widely used KITTI Dataset and the more recent Waymo Open Dataset.Our results show that compared to existing voxel-based methods, Voxel R-CNN delivers a higher detection accuracy while maintaining a real-time frame processing rate,i.e., at a speed of 25 FPS on an NVIDIA RTX 2080 Ti GPU.The code is available at https://github.com/djiajunustc/Voxel-R-CNN."
  },
  "aaai2021_main_arbitraryvideostyletransferviamulti-channelcorrelation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Arbitrary Video Style Transfer via Multi-Channel Correlation",
    "authors": [
      "Yingying Deng",
      "Fan Tang",
      "Weiming Dong",
      "Haibin Huang",
      "Chongyang Ma",
      "Changsheng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16208",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16208/16015",
    "published": "2021-02",
    "summary": "Video style transfer is attracting increasing attention from the artificial intelligence community because of its numerous applications, such as augmented reality and animation production. Relative to traditional image style transfer, video style transfer presents new challenges, including how to effectively generate satisfactory stylized results for any specified style while maintaining temporal coherence across frames. Towards this end, we propose a Multi-Channel Correlation network (MCCNet), which can be trained to fuse exemplar style features and input content features for efficient style transfer while naturally maintaining the coherence of input videos to output videos. Specifically, MCCNet works directly on the feature space of style and content domain where it learns to rearrange and fuse style features on the basis of their similarity to content features. The outputs generated by MCC are features containing the desired style patterns that can further be decoded into images with vivid style textures.Moreover, MCCNet is also designed to explicitly align the features to input and thereby ensure that the outputs maintain the content structures and the temporal continuity. To further improve the performance of MCCNet under complex light conditions, we also introduce illumination loss during training. Qualitative and quantitative evaluations demonstrate that MCCNet performs well in arbitrary video and image style transfer tasks. Code is available at https://github.com/diyiiyiii/MCCNet."
  },
  "aaai2021_main_similarityreasoningandfiltrationforimage-textmatching": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Similarity Reasoning and Filtration for Image-Text Matching",
    "authors": [
      "Haiwen Diao",
      "Ying Zhang",
      "Lin Ma",
      "Huchuan Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16209",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16209/16016",
    "published": "2021-02",
    "summary": "Image-text matching plays a critical role in bridging the vision and language, and great progress has been made by exploiting the global alignment between image and sentence, or local alignments between regions and words. However, how to make the most of these alignments to infer more accurate matching scores is still underexplored. In this paper, we propose a novel Similarity Graph Reasoning and Attention Filtration (SGRAF) network for image-text matching. Specifically, the vector-based similarity representations are firstly learned to characterize the local and global alignments in a more comprehensive manner, and then the Similarity Graph Reasoning (SGR) module relying on one graph convolutional neural network is introduced to infer relation-aware similarities with both the local and global alignments. The Similarity Attention Filtration (SAF) module is further developed to integrate these alignments effectively by selectively attending on the significant and representative alignments and meanwhile casting aside the interferences of non-meaningful alignments. We demonstrate the superiority of the proposed method with achieving state-of-the-art performances on the Flickr30K and MSCOCO datasets, and the good interpretability of SGR and SAF with extensive qualitative experiments and analyses."
  },
  "aaai2021_main_spatio-temporaldifferencedescriptorforskeleton-basedactionrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Spatio-Temporal Difference Descriptor for Skeleton-Based Action Recognition",
    "authors": [
      "Chongyang Ding",
      "Kai Liu",
      "Jari Korhonen",
      "Evgeny Belyaev"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16210",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16210/16017",
    "published": "2021-02",
    "summary": "In skeletal representation, intra-frame differences between body joints, as well as inter-frame dynamics between body skeletons contain discriminative information for action recognition. Conventional methods for modeling human skeleton sequences generally depend on motion trajectory and body joint dependency information, thus lacking the ability to identify the inherent differences of human skeletons. In this paper, we propose a spatio-temporal difference descriptor based on a directional convolution architecture that enables us to learn the spatio-temporal differences and contextual dependencies between different body joints simultaneously. The overall model is built on a deep symmetric positive definite (SPD) metric learning architecture designed to learn discriminative manifold features with the well-designed non-linear mapping operation. Experiments on several action datasets show that our proposed method achieves up to 3% accuracy improvement over state-of-the-art methods."
  },
  "aaai2021_main_towardsuniversalphysicalattacksonsingleobjecttracking": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Towards Universal Physical Attacks on Single Object Tracking",
    "authors": [
      "Li Ding",
      "Yongwei Wang",
      "Kaiwen Yuan",
      "Minyang Jiang",
      "Ping Wang",
      "Hua Huang",
      "Z. Jane Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16211",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16211/16018",
    "published": "2021-02",
    "summary": "Recent studies show that small perturbations in video frames could misguide single object trackers. However, such attacks have been mainly designed for digital-domain videos (i.e., perturbation on full images), which makes them practically infeasible to evaluate the adversarial vulnerability of trackers in real-world scenarios. Here we made the first step towards physically feasible adversarial attacks against visual tracking in real scenes with a universal patch to camouflage single object trackers. Fundamentally different from physical object detection, the essence of single object tracking lies in the feature matching between the search image and templates, and we therefore specially design the maximum textural discrepancy (MTD), a resolution-invariant and target location-independent feature de-matching loss. The MTD distills global textural information of the template and search images at hierarchical feature scales prior to performing feature attacks. Moreover, we evaluate two shape attacks, the regression dilation and shrinking, to generate stronger and more controllable attacks. Further, we employ a set of transformations to simulate diverse visual tracking scenes in the wild. Experimental results show the effectiveness of the physically feasible attacks on SiamMask and SiamRPN++ visual trackers both in digital and physical scenes."
  },
  "aaai2021_main_modelingtheprobabilisticdistributionofunlabeleddataforone-shotmedicalimagesegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Modeling the Probabilistic Distribution of Unlabeled Data for One-shot Medical Image Segmentation",
    "authors": [
      "Yuhang Ding",
      "Xin Yu",
      "Yi Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16212",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16212/16019",
    "published": "2021-02",
    "summary": "Existing image segmentation networks mainly leverage large-scale labeled datasets to attain high accuracy. However, labeling medical images is very expensive since it requires sophisticated expert knowledge. Thus, it is more desirable to employ only a few labeled data in pursuing high segmentation performance. In this paper, we develop a data augmentation method for one-shot brain magnetic resonance imaging (MRI) image segmentation which exploits only one labeled MRI image (named atlas) and a few unlabeled images. In particular, we propose to learn the probability distributions of deformations (including shapes and intensities) of different unlabeled MRI images with respect to the atlas via 3D variational autoencoders (VAEs). In this manner, our method is able to exploit the learned distributions of image deformations to generate new authentic brain MRI images, and the number of generated samples will be sufficient to train a deep segmentation network. Furthermore, we introduce a new standard segmentation benchmark to evaluate the generalization performance of a segmentation network through a cross-dataset setting (collected from different sources). Extensive experiments demonstrate that our method outperforms the state-of-the-art one-shot medical segmentation methods. Our code has been released at https://github.com/dyh127/Modeling-the-Probabilistic-Distribution-of-Unlabeled-Data."
  },
  "aaai2021_main_few-shotclass-incrementallearningviarelationknowledgedistillation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Few-Shot Class-Incremental Learning via Relation Knowledge Distillation",
    "authors": [
      "Songlin Dong",
      "Xiaopeng Hong",
      "Xiaoyu Tao",
      "Xinyuan Chang",
      "Xing Wei",
      "Yihong\n      Gong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16213",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16213/16020",
    "published": "2021-02",
    "summary": "In this paper, we focus on the challenging few-shot class incremental learning (FSCIL) problem, which requires to transfer knowledge from old tasks to new ones and solves catastrophic forgetting. We propose the exemplar relation distillation incremental learning framework to balance the tasks of old-knowledge preserving and new-knowledge adaptation. First, we construct an exemplar relation graph to represent the knowledge learned by the original network and update gradually for new tasks learning. Then an exemplar relation loss function for discovering the relation knowledge between different classes is introduced to learn and transfer the structural information in relation graph. A large number of experiments demonstrate that relation knowledge does exist in the exemplars and our approach outperforms other state-of-the-art class-incremental learning methods on the CIFAR100, miniImageNet, and CUB200 datasets."
  },
  "aaai2021_main_miehdrcnnmainimageenhancementbasedghost-freehighdynamicrangeimagingusingdual-lenssystems": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "MIEHDR CNN: Main Image Enhancement based Ghost-Free High Dynamic Range Imaging using Dual-Lens Systems",
    "authors": [
      "Xuan Dong",
      "Xiaoyan Hu",
      "Weixin Li",
      "Xiaojie Wang",
      "Yunhong Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16214",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16214/16021",
    "published": "2021-02",
    "summary": "We study the High Dynamic Range (HDR) imaging problem using two Low Dynamic Range (LDR) images that are shot from dual-lens systems in a single shot time with different exposures. In most of the related HDR imaging methods, the problem is usually solved by Multiple Images Merging, i.e. the final HDR image is fused from pixels of all the input LDR images. However, ghost artifacts can be hardly avoided using this strategy. Instead of directly merging the multiple LDR inputs, we use an indirect way which enhances the main image, i.e. the short exposure image IS, using the long exposure image IL serving as guidance. In detail, we propose a new model, named MIEHDR CNN model, which consists of three subnets, i.e. Soft Warp CNN, 3D Guided Denoising CNN and Fusion CNN. The Soft Warp CNN aligns IL to get the aligned result ILA using the soft exposed result of IS as reference. The 3D Guided Denoising CNN denoises the soft exposed result of IS using ILA as guidance, whose result are fed into the Fusion CNN with IS to get the HDR result. The MIEHDR CNN model is implemented by MindSpore and experimental results show that we can outperform related methods largely and avoid ghost artifacts."
  },
  "aaai2021_main_boostingimage-basedmutualgazedetectionusingpseudo3dgaze": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Boosting Image-based Mutual Gaze Detection using Pseudo 3D Gaze",
    "authors": [
      "Bardia Doosti",
      "Ching-Hui Chen",
      "Raviteja Vemulapalli",
      "Xuhui Jia",
      "Yukun Zhu",
      "Bradley Green"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16215",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16215/16022",
    "published": "2021-02",
    "summary": "Mutual gaze detection, i.e., predicting whether or not two people are looking at each other, plays an important role in understanding human interactions. In this work, we focus on the task of image-based mutual gaze detection, and propose a simple and effective approach to boost the performance by using an auxiliary 3D gaze estimation task during the training phase. We achieve the performance boost without additional labeling cost by training the 3D gaze estimation branch using pseudo 3D gaze labels deduced from mutual gaze labels. By sharing the head image encoder between the 3D gaze estimation and the mutual gaze detection branches, we achieve better head features than learned by training the mutual gaze detection branch alone. Experimental results on three image datasets show that the proposed approach improves the detection performance significantly without additional annotations. This work also introduces a new image dataset that consists of 33.1K pairs of humans annotated with mutual gaze labels in 29.2K images."
  },
  "aaai2021_main_howtosaveyourannotationcostforpanopticsegmentation?": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "How to Save your Annotation Cost for Panoptic Segmentation?",
    "authors": [
      "Xuefeng Du",
      "ChenHan Jiang",
      "Hang Xu",
      "Gengwei Zhang",
      "Zhenguo Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16216",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16216/16023",
    "published": "2021-02",
    "summary": "How to properly reduce the annotation cost for panoptic segmentation? How to leverage and optimize the cost-quality trade-off for training data and model? These questions are key challenges towards a label-efficient and scalable panoptic segmentation system due to its expensive instance/semantic pixel-level annotation requirements. By closely examining different kinds of cheaper labels, we introduce a novel multi-objective framework to automatically determine the allocation of different annotations, so as to reach a better segmentation quality with a lower annotation cost. Specifically, we design a Cost-Quality Balanced Network (CQB-Net) to generate the panoptic segmentation map, which distills the crucial relations between various supervisions including panoptic labels, image-level classification labels, bounding boxes, and the semantic coherence information between the foreground and background. Instead of ad-hoc allocation during training, we formulate the optimization of cost-quality trade-off as a Multi-Objective Optimization Problem (MOOP). We model the marginal quality improvement of each annotation and approximate the Pareto-front to enable a label-efficient allocation ratio. Extensive experiments on COCO benchmark show the superiority of our method, e.g. achieving a segmentation quality of 43.4% compared to 43.0% of OCFusion while saving 2.4x annotation cost."
  },
  "aaai2021_main_dirvdenseinteractionregionvotingforend-to-endhuman-objectinteractiondetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DIRV: Dense Interaction Region Voting for End-to-End Human-Object Interaction Detection",
    "authors": [
      "Hao-Shu Fang",
      "Yichen Xie",
      "Dian Shao",
      "Cewu Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16217",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16217/16024",
    "published": "2021-02",
    "summary": "Recent years, human-object interaction (HOI) detection has achieved impressive advances. However, conventional two-stage methods are usually slow in inference. On the other hand, existing one-stage methods mainly focus on the union regions of interactions, which introduce unnecessary visual information as disturbances to HOI detection. To tackle the problems above, we propose a novel one-stage HOI detection approach DIRV in this paper, based on a new concept called interaction region for the HOI problem. Unlike previous methods, our approach concentrates on the densely sampled interaction regions across different scales for each human-object pair, so as to capture the subtle visual features that is most essential to the interaction. Moreover, in order to compensate for the detection flaws of a single interaction region, we introduce a novel voting strategy that makes full use of those overlapped interaction regions in place of conventional Non-Maximal Suppression (NMS). Extensive experiments on two popular benchmarks: V-COCO and HICO-DET show that our approach outperforms existing state-of-the-arts by a large margin with the highest inference speed and lightest network architecture. Our codeis publicly available at www.github.com/MVIG-SJTU/DIRV."
  },
  "aaai2021_main_decaugaugmentinghoidetectionviadecomposition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DecAug: Augmenting HOI Detection via Decomposition",
    "authors": [
      "Hao-Shu Fang",
      "Yichen Xie",
      "Dian Shao",
      "Yong-Lu Li",
      "Cewu Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16218",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16218/16025",
    "published": "2021-02",
    "summary": "Human-object interaction (HOI) detection requires a large amount of annotated data. Current algorithms suffer from insufficient training samples and category imbalance within datasets. To increase data efficiency, in this paper, we propose an efficient and effective data augmentation method called DecAug for HOI detection. Based on our proposed object state similarity metric, object patterns across different HOIs are shared to augment local object appearance features without changing their states. Further, we shift spatial correlation between humans and objects to other feasible configurations with the aid of a pose-guided Gaussian Mixture Model while preserving their interactions. Experiments show that our method brings up to 3.3 mAP and 1.6 mAP improvements on V-COCO and HICO-DET dataset for two advanced models. Specifically, interactions with fewer samples enjoy more notable improvement. Our method can be easily integrated into various HOI detection models with negligible extra computational consumption."
  },
  "aaai2021_main_partiallynon-autoregressiveimagecaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Partially Non-Autoregressive Image Captioning",
    "authors": [
      "Zhengcong Fei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16219",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16219/16026",
    "published": "2021-02",
    "summary": "Current state-of-the-art image captioning systems usually generated descriptions autoregressively, i.e., every forward step conditions on the given image and previously produced words. The sequential attribution causes a unavoidable decoding latency. Non-autoregressive image captioning, on the other hand, predicts the entire sentence simultaneously and accelerates the inference process significantly. However, it removes the dependence in a caption and commonly suffers from repetition or missing issues. To make a better trade-off between speed and quality, we introduce a partially non-autoregressive model, named PNAIC, which considers a caption as a series of concatenated word groups. The groups are generated parallelly in global while each word in group is predicted from left to right, and thus the captioner can create multiple discontinuous words concurrently at each time step. More importantly, by incorporating curriculum learning-based training tasks of group length prediction and invalid group deletion, our model is capable of generating accurate captions as well as preventing common incoherent errors. Extensive experiments on MS COCO benchmark demonstrate that our proposed method achieves more than 3.5\u00d7 speedup while maintaining competitive performance."
  },
  "aaai2021_main_memory-augmentedimagecaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Memory-Augmented Image Captioning",
    "authors": [
      "Zhengcong Fei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16220",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16220/16027",
    "published": "2021-02",
    "summary": "Current deep learning-based image captioning systems have been proven to store practical knowledge with their parameters and achieve competitive performances in the public datasets. Nevertheless, their ability to access and precisely manipulate the mastered knowledge is still limited. Besides, providing evidence for decisions and updating memory information are also important yet under explored. Towards this goal, we introduce a memory-augmented method, which extends an existing image caption model by incorporating extra explicit knowledge from a memory bank. Adequate knowledge is recalled according to the similarity distance in the embedding space of history context, and the memory bank can be constructed conveniently from any matched image-text set, e.g., the previous training data. Incorporating such non-parametric memory-augmented method to various captioning baselines, the performance of resulting captioners imporves consistently on the evaluation benchmark. More encouragingly, extensive experiments demonstrate that our approach holds the capability for efficiently adapting to larger training datasets, by simply transferring the memory bank without any additional training."
  },
  "aaai2021_main_edge-competingpathologicallivervesselsegmentationwithlimitedlabels": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Edge-competing Pathological Liver Vessel Segmentation with Limited Labels",
    "authors": [
      "Zunlei Feng",
      "Zhonghua Wang",
      "Xinchao Wang",
      "Xiuming Zhang",
      "Lechao Cheng",
      "Jie\n      Lei",
      "Yuexuan Wang",
      "Mingli Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16221",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16221/16028",
    "published": "2021-02",
    "summary": "The microvascular invasion (MVI) is a major prognostic factor in hepatocellular carcinoma, which is one of the malignant tumors with the highest mortality rate. The diagnosis of MVI needs discovering the vessels that contain hepatocellular carcinoma cells and counting their number in each vessel, which depends heavily on experiences of the doctor, is largely subjective andtime-consuming. However, there is no algorithm as yet tailored for the MVI detection frompathological images. This paper collects the first pathological liver image dataset containing $522$ whole slide images with labels of vessels, MVI, and hepatocellular carcinoma grades. The first and essential step for the automatic diagnosis of MVI is the accurate segmentation of vessels. The unique characteristics of pathological liver images, such as super-large size, multi-scale vessel, and blurred vessel edges, make the accurate vessel segmentation challenging. Based on the collected dataset, we propose an Edge-competing Vessel Segmentation Network (EVS-Net), which contains a segmentation network and two edge segmentation discriminators. The segmentation network, combined with an edge-aware self-supervision mechanism, is devised to conduct vessel segmentation with limited labeled patches. Meanwhile, two discriminators are introduced to distinguish whether the segmented vessel and background contain residual features in an adversarial manner. In the training stage, two discriminators are devised to compete for the predictedposition of edges. Exhaustive experiments demonstrate that, with only limited labeled patches, EVS-Net achieves a close performance of fully supervised methods, which provides a convenient tool for the pathological liver vessel segmentation. Code is publicly available at https://github.com/wang97zh/EVS-Net."
  },
  "aaai2021_main_visualboundaryknowledgetranslationforforegroundsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Visual Boundary Knowledge Translation for Foreground Segmentation",
    "authors": [
      "Zunlei Feng",
      "Lechao Cheng",
      "Xinchao Wang",
      "Xiang Wang",
      "Ya Jie Liu",
      "Xiangtong\n      Du",
      "Mingli Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16222",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16222/16029",
    "published": "2021-02",
    "summary": "When confronted with objects of unknown types in an image, humans can effortlessly and precisely tell their visual boundaries. This recognition mechanism and underlying generalization capability seem to contrast to state-of-the-art image segmentation networks that rely on large-scale category-aware annotated training samples. In this paper, we make an attempt towards building models that explicitly account for visual boundary knowledge, in hope to reduce the training effort on segmenting unseen categories. Specifically, we investigate a new tasktermed as Boundary Knowledge Translation (BKT). Given a set of fully labeled categories, BKT aims to translate the visual boundary knowledge learned from the labeled categories, to a set of novel categories, each of which is provided only a few labeled samples. To this end, we propose a Translation Segmentation Network (Trans-Net), which comprises a segmentation network and two boundary discriminators. The segmentation network, combined with a boundary-aware self-supervised mechanism, is devised to conduct foreground segmentation, while the two discriminators work together in an adversarial manner to ensure an accurate segmentation of the novel categories under light supervision. Exhaustive experiments demonstrate that, with only tens of labeled samples as guidance, Trans-Net achieves close results on par with fully supervised methods."
  },
  "aaai2021_main_learningcomplex3dhumanself-contact": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Complex 3D Human Self-Contact",
    "authors": [
      "Mihai Fieraru",
      "Mihai Zanfir",
      "Elisabeta Oneata",
      "Alin-Ionut Popa",
      "Vlad\n      Olaru",
      "Cristian Sminchisescu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16223",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16223/16030",
    "published": "2021-02",
    "summary": "Monocular estimation of three dimensional human self-contact is fundamental for detailed scene analysis including body language understanding and behaviour modeling.Existing 3d reconstruction methods do not focus on body regions in self-contact and consequently recover configurations that are either far from each other or self-intersecting, when they should just touch. This leads to perceptually incorrect estimates and limits impact in those very fine-grained analysis domains where detailed 3d models are expected to play an important role. To address such challenges we detect self-contact and design 3d losses to explicitly enforce it. Specifically, we develop a model for Self-Contact Prediction (SCP), that estimates the body surface signature of self-contact, leveraging the localization of self-contact in the image, during both training and inference. We collect two large datasets to support learning and evaluation: (1) HumanSC3D, an accurate 3d motion capture repository containing 1,032 sequences with 5,058 contact events and 1,246,487 ground truth 3d poses synchronized with images collected from multiple views, and (2) FlickrSC3D, a repository of 3,969 images, containing 25,297 surface-to-surface correspondences with annotated image spatial support. We also illustrate how more expressive 3d reconstructions can be recovered under self-contact signature constraints and present monocular detection of face-touch as one of the multiple applications made possible by more accurate self-contact models."
  },
  "aaai2021_main_rainstreakremovalviadualgraphconvolutionalnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Rain Streak Removal via Dual Graph Convolutional Network",
    "authors": [
      "Xueyang Fu",
      "Qi Qi",
      "Zheng-Jun Zha",
      "Yurui Zhu",
      "Xinghao Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16224",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16224/16031",
    "published": "2021-02",
    "summary": "Deep convolutional neural networks (CNNs) have become dominant in the single image de-raining area. However, most deep CNNs-based de-raining methods are designed by stacking vanilla convolutional layers, which can only be used to model local relations. Therefore, long-range contextual information is rarely considered for this specific task. To address the above problem, we propose a simple yet effective dual graph convolutional network (GCN) for single image rain removal. Specifically, we design two graphs to perform global relational modeling and reasoning. The first GCN is used to explore global spatial relations among pixels in feature maps, while the second GCN models the global relations across the channels. Compared to standard convolutional operations, the proposed two graphs enable the network to extract representations from new dimensions. To achieve the image rain removal, we further embed these two graphs and multi-scale dilated convolution into a symmetrically skip-connected network architecture. Therefore, our dual graph convolutional network is able to well handle complex and spatially long rain streaks by exploring multiple representations, e.g., multi-scale local feature, global spatial coherence and cross-channel correlation. Meanwhile, our model is easy to implement, end-to-end trainable and computationally efficient. Extensive experiments on synthetic and real data demonstrate that our method achieves significant improvements over the recent state-of-the-art methods."
  },
  "aaai2021_main_compfeatcomprehensivefeatureaggregationforvideoinstancesegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "CompFeat: Comprehensive Feature Aggregation for Video Instance Segmentation",
    "authors": [
      "Yang Fu",
      "Linjie Yang",
      "Ding Liu",
      "Thomas S. Huang",
      "Humphrey Shi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16225",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16225/16032",
    "published": "2021-02",
    "summary": "Video instance segmentation is a complex task in which we need to detect, segment, and track each object for any given video. Previous approaches only utilize single-frame features for the detection, segmentation, and tracking of objects and they suffer in the video scenario due to several distinct challenges such as motion blur and drastic appearance change. To eliminate ambiguities introduced by only using single-frame features, we propose a novel comprehensive feature aggregation approach (CompFeat) to refine features atboth frame-level and object-level with temporal and spatial context information. The aggregation process is carefully designed with a new attention mechanism which significantly increases the discriminative power of the learned features. We further improve the tracking capability of our model through a siamese design by incorporating both feature similarities and spatial similarities. Experiments conducted on the YouTube-VIS dataset validate the effectiveness of proposed CompFeat."
  },
  "aaai2021_main_deepmetriclearningwithself-supervisedranking": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Metric Learning with Self-Supervised Ranking",
    "authors": [
      "Zheren Fu",
      "Yan Li",
      "Zhendong Mao",
      "Quan Wang",
      "Yongdong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16226",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16226/16033",
    "published": "2021-02",
    "summary": "Deep metric learning aims to learn a deep embedding space, where similar objects are pushed towards together and different objects are repelled against. Existing approaches typically use inter-class characteristics, e.g. class-level information or instance-level similarity, to obtain semantic relevance of data points and get a large margin between different classes in the embedding space. However, the intra-class characteristics, e.g. local manifold structure or relative relationship within the same class, are usually overlooked in the learning process. Hence the data structure cannot be fully exploited and the output embeddings have limitation in retrieval. More importantly, retrieval results lack in a good ranking. This paper presents a novel self-supervised ranking auxiliary framework, which captures intra-class characteristics as well as inter-class characteristics for better metric learning. Our method defines specific transform functions to simulates the local structure change of intra-class in the initial image domain, and formulates a self-supervised learning procedure to fully exploit this property and preserve it in the embedding space. Extensive experiments on three standard benchmarks show that our method significantly improves and outperforms the state-of-the-art methods on the performances of both retrieval and ranking by 2%-4%."
  },
  "aaai2021_main_asystematicevaluationofobjectdetectionnetworksforscientificplots": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Systematic Evaluation of Object Detection Networks for Scientific Plots",
    "authors": [
      "Pritha Ganguly",
      "Nitesh S Methani",
      "Mitesh M. Khapra",
      "Pratyush Kumar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16227",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16227/16034",
    "published": "2021-02",
    "summary": "Are existing object detection methods adequate for detecting text and visual elements in scientific plots which are arguably different than the objects found in natural images? To answer this question, we train and compare the accuracy of Fast/Faster R-CNN, SSD, YOLO and RetinaNet on the PlotQA dataset with over 220,000 scientific plots. At the standard IOU setting of 0.5, most networks perform well with mAP scores greater than 80% in detecting the relatively simple objects in plots. However, the performance drops drastically when evaluated at a stricter IOU of 0.9 with the best model giving a mAP of 35.70%. Note that such a stricter evaluation is essential when dealing with scientific plots where even minor localisation errors can lead to large errors in downstream numerical inferences. Given this poor performance, we propose minor modifications to existing models by combining ideas from different object detection networks. While this significantly improves the performance, there are still two main issues: (i) performance on text objects which are essential for reasoning is very poor, and (ii) inference time is unacceptably large considering the simplicity of plots. To solve this open problem, we make a series of contributions: (a) an efficient region proposal method based on Laplacian edge detectors, (b) a feature representation of region proposals that includes neighbouring information, (c) a linking component to join multiple region proposals for detecting longer textual objects, and (d) a custom loss function that combines a smooth L1-loss with an IOU-based loss. Combining these ideas, our final model is very accurate at extreme IOU values achieving a mAP of 93.44%@0.9 IOU. Simultaneously, our model is very efficient with an inference time 16x lesser than the current models, including one-stage detectors. Our model also achieves a high accuracy on an extrinsic plot-to-table conversion task with an F1 score of 0.77. With these contributions, we make a definitive progress in object detection for plots and enable further exploration on automated reasoning of plots."
  },
  "aaai2021_main_thecomplexityofobjectassociationinmultipleobjecttracking": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "The Complexity of Object Association in Multiple Object Tracking",
    "authors": [
      "Robert Ganian",
      "Thekla Hamm",
      "Sebastian Ordyniak"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16228",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16228/16035",
    "published": "2021-02",
    "summary": "Object association, i.e., the identification of which observations correspond to the same object, is a central task for the area of multiple object tracking. Two prominent models capturing this task have been introduced in the literature: the Lifted Multicut model and the more recent Lifted Paths model. Here, we carry out a detailed complexity-theoretic study of the problems arising from these two models that is aimed at complementing previous empirical work on object association. We obtain a comprehensive complexity map for both models that takes into account natural restrictions to instances such as possible bounds on the number of frames, number of tracked objects and branching degree, as well as less explicit structural restrictions such as having bounded treewidth. Our results include new fixed-parameter and XP algorithms for the problems as well as hardness proofs which altogether indicate that the Lifted Paths problem exhibits a more favorable complexity behavior than Lifted Multicut."
  },
  "aaai2021_main_learninglocalneighboringstructureforrobust3dshaperepresentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Local Neighboring Structure for Robust 3D Shape Representation",
    "authors": [
      "Zhongpai Gao",
      "Junchi Yan",
      "Guangtao Zhai",
      "Juyong Zhang",
      "Yiyan Yang",
      "Xiaokang Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16229",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16229/16036",
    "published": "2021-02",
    "summary": "Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods."
  },
  "aaai2021_main_semantic-guidedreinforcedregionembeddingforgeneralizedzero-shotlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Semantic-guided Reinforced Region Embedding for Generalized Zero-Shot Learning",
    "authors": [
      "Jiannan Ge",
      "Hongtao Xie",
      "Shaobo Min",
      "Yongdong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16230",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16230/16037",
    "published": "2021-02",
    "summary": "Generalized zero-shot Learning (GZSL) aims to recognize images from either seen or unseen domain, mainly by learning a joint embedding space to associate image features with the corresponding category descriptions. Recent methods have proved that localizing important object regions can effectively bridge the semantic-visual gap. However, these are all based on one-off visual localizers, lacking of interpretability and flexibility. In this paper, we propose a novel Semantic-guided Reinforced Region Embedding (SR2E) network that can localize important objects in the long-term interests to construct semantic-visual embedding space. SR2E consists of Reinforced Region Module (R2M) and Semantic Alignment Module (SAM). First, without the annotated bounding box as supervision, R2M encodes the semantic category guidance into the reward and punishment criteria to teach the localizer serialized region searching. Besides, R2M explores different action spaces during the serialized searching path to avoid local optimal localization, which thereby generates discriminative visual features with less redundancy. Second, SAM preserves the semantic relationship into visual features via semantic-visual alignment and designs a domain detector to alleviate the domain confusion. Experiments on four public benchmarks demonstrate that the proposed SR2E is an effective GZSL method with reinforced embedding space, which obtains averaged 6.1% improvements."
  },
  "aaai2021_main_dynamicgraphrepresentationlearningforvideodialogviamulti-modalshuffledtransformers": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers",
    "authors": [
      "Shijie Geng",
      "Peng Gao",
      "Moitreya Chatterjee",
      "Chiori Hori",
      "Jonathan Le Roux",
      "Yongfeng Zhang",
      "Hongsheng Li",
      "Anoop Cherian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16231",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16231/16038",
    "published": "2021-02",
    "summary": "Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog (AVSD) task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a semantics-controlled multi-modal shuffled Transformer reasoning framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an intra-frame reasoning layer producing spatio-semantic graph representations for every frame, and an inter-frame aggregation module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics."
  },
  "aaai2021_main_boundary-awaregeometricencodingforsemanticsegmentationofpointclouds": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds",
    "authors": [
      "Jingyu Gong",
      "Jiachen Xu",
      "Xin Tan",
      "Jie Zhou",
      "Yanyun Qu",
      "Yuan Xie",
      "Lizhuang\n      Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16232",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16232/16039",
    "published": "2021-02",
    "summary": "Boundary information plays a significant role in 2D image segmentation, while usually being ignored in 3D point cloud segmentation where ambiguous features might be generated in feature extraction, leading to misclassification in the transition area between two objects. In this paper, firstly, we propose a Boundary Prediction Module (BPM) to predict boundary points. Based on the predicted boundary, a boundary-aware Geometric Encoding Module (GEM) is designed to encode geometric information and aggregate features with discrimination in a neighborhood, so that the local features belonging to different categories will not be polluted by each other. To provide extra geometric information for boundary-aware GEM, we also propose a light-weight Geometric Convolution Operation (GCO), making the extracted features more distinguishing. Built upon the boundary-aware GEM, we build our network and test it on benchmarks like ScanNet v2, S3DIS. Results show our methods can significantly improve the baseline and achieve state-of-the-art performance."
  },
  "aaai2021_main_analogicalimagetranslationforfoggeneration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Analogical Image Translation for Fog Generation",
    "authors": [
      "Rui Gong",
      "Dengxin Dai",
      "Yuhua Chen",
      "Wen Li",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16233",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16233/16040",
    "published": "2021-02",
    "summary": "Image-to-image translation is to map images from a given style to another given style. While exceptionally successful, current methods assume the availability of training images in both source and target domains, which does not always hold in practice. Inspired by humans' reasoning capability of analogy, we propose analogical image translation (AIT)that exploit the concept of gist, for the first time. Given images of two styles in the source domain: A and A', along with images B of the first style in the target domain, learn a model to translate B to B' in the target domain, such that A:A' :: B:B'.AIT is especially useful for translation scenarios in which training data of one style is hard to obtain but training data of the same two styles in another domain is available. For instance, in the case from normal conditions to extreme, rare conditions, obtaining real training images for the latter case is challenging. However, obtaining synthetic data for both cases is relatively easy. In this work, we aim at adding adverse weather effects, more specifically fog, to images taken in clear weather. To circumvent the challenge of collecting real foggy images, AIT learns the gist of translating synthetic clear-weather to foggy images, followed by adding fog effects onto real clear-weather images, without ever seeing any real foggy image. AIT achieves zero-shot image translation capability, whoseeffectiveness and benefit are demonstratedbythe downstream task of semantic foggy scene understanding."
  },
  "aaai2021_main_temporalroialignforvideoobjectrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Temporal ROI Align for Video Object Recognition",
    "authors": [
      "Tao Gong",
      "Kai Chen",
      "Xinjiang Wang",
      "Qi Chu",
      "Feng Zhu",
      "Dahua Lin",
      "Nenghai\n      Yu",
      "Huamin Feng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16234",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16234/16041",
    "published": "2021-02",
    "summary": "Video object detection is challenging in the presence of appearance deterioration in certain video frames. Therefore, it is a natural choice to aggregate temporal information from other frames of the same video into the current frame. However, ROI Align, as one of the most core procedures of video detectors, still remains extracting features from a single-frame feature map for proposals, making the extracted ROI features lack temporal information from videos. In this work, considering the features of the same object instance are highly similar among frames in a video, a novel Temporal ROI Align operator is proposed to extract features from other frames feature maps for current frame proposals by utilizing feature similarity. The proposed Temporal ROI Align operator can extract temporal information from the entire video for proposals. We integrate it into single-frame video detectors and other state-of-the-art video detectors, and conduct quantitative experiments to demonstrate that the proposed Temporal ROI Align operator can consistently and significantly boost the performance. Besides, the proposed Temporal ROI Align can also be applied into video instance segmentation."
  },
  "aaai2021_main_smartframeselectionforactionrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SMART Frame Selection for Action Recognition",
    "authors": [
      "Shreyank N Gowda",
      "Marcus Rohrbach",
      "Laura Sevilla-Lara"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16235",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16235/16042",
    "published": "2021-02",
    "summary": "Video classification is computationally expensive.In this paper, we address theproblem of frame selection to reduce the computational cost of video classification.Recent work has successfully leveraged frame selection for long, untrimmed videos,where much of the content is not relevant, and easy to discard. In this work, however,we focus on the more standard short, trimmed video classification problem. Weargue that good frame selection can not only reduce the computational cost of videoclassification but also increase the accuracy by getting rid of frames that are hard toclassify. In contrast to previous work, we propose a method that instead of selectingframes by considering one at a time, considers them jointly. This results in a moreefficient selection, where \u201cgood\" frames are more effectively distributed over thevideo, like snapshots that tell a story. We call the proposed frame selection SMARTand we test it in combination with different backbone architectures and on multiplebenchmarks (Kinetics [5], Something-something [14], UCF101 [31]). We showthat the SMART frame selection consistently improves the accuracy compared toother frame selection strategies while reducing the computational cost by a factorof 4 to 10 times. Additionally, we show that when the primary goal is recognitionperformance, our selection strategy can improve over recent state-of-the-art modelsand frame selection strategies on various benchmarks (UCF101, HMDB51 [21],FCVID [17], and ActivityNet [4])."
  },
  "aaai2021_main_proxysynthesislearningwithsyntheticclassesfordeepmetriclearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Proxy Synthesis: Learning with Synthetic Classes for Deep Metric Learning",
    "authors": [
      "Geonmo Gu",
      "Byungsoo Ko",
      "Han-Gyu Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16236",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16236/16043",
    "published": "2021-02",
    "summary": "One of the main purposes of deep metric learning is to construct an embedding space that has well-generalized embeddings on both seen (training) classes and unseen (test) classes. Most existing works have tried to achieve this using different types of metric objectives and hard sample mining strategies with given training data. However, learning with only the training data can be overfitted to the seen classes, leading to the lack of generalization capability on unseen classes. To address this problem, we propose a simple regularizer called Proxy Synthesis that exploits synthetic classes for stronger generalization in deep metric learning. The proposed method generates synthetic embeddings and proxies that work as synthetic classes, and they mimic unseen classes when computing proxy-based losses. Proxy Synthesis derives an embedding space considering class relations and smooth decision boundaries for robustness on unseen classes. Our method is applicable to any proxy-based losses, including softmax and its variants. Extensive experiments on four famous benchmarks in image retrieval tasks demonstrate that Proxy Synthesis significantly boosts the performance of proxy-based losses and achieves state-of-the-art performance. Our implementation is available at github.com/navervision/proxy-synthesis."
  },
  "aaai2021_main_interpretablegraphcapsulenetworksforobjectrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Interpretable Graph Capsule Networks for Object Recognition",
    "authors": [
      "Jindong Gu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16237",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16237/16044",
    "published": "2021-02",
    "summary": "Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness."
  },
  "aaai2021_main_class-incrementalinstancesegmentationviamulti-teachernetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Class-Incremental Instance Segmentation via Multi-Teacher Networks",
    "authors": [
      "Yanan Gu",
      "Cheng Deng",
      "Kun Wei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16238",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16238/16045",
    "published": "2021-02",
    "summary": "Although deep neural networks have achieved amazing results on instance segmentation, they are still ill-equipped when they are required to learn new tasks incrementally. Concretely, they suffer from \u201ccatastrophic forgetting\u201d, an abrupt degradation of performance on old classes with the initial training data missing. Moreover, they are subjected to a negative transfer problem on new classes, which renders the model unable to update its knowledge while preserving the previous knowledge. To address these problems, we propose an incremental instance segmentation method that consists of three networks: Former Teacher Network (FTN), Current Student Network (CSN) and Current Teacher Network (CTN). Specifically, FTN supervises CSN to preserve the previous knowledge, and CTN supervises CSN to adapt to new classes. The supervision of two teacher networks is achieved by a distillation loss function for instances, bounding boxes, and classes. In addition, we adjust the supervision weights of different teacher networks to balance between the knowledge preservation for former classes and the adaption to new classes. Extensive experimental results on PASCAL 2012 SBD and COCO datasets show the effectiveness of the proposed method."
  },
  "aaai2021_main_efficientderainlearningpixel-wisedilationfilteringforhigh-efficiencysingle-imagederaining": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "EfficientDeRain: Learning Pixel-wise Dilation Filtering for High-Efficiency Single-Image Deraining",
    "authors": [
      "Qing Guo",
      "Jingyang Sun",
      "Felix Juefei-Xu",
      "Lei Ma",
      "Xiaofei Xie",
      "Wei Feng",
      "Yang Liu",
      "Jianjun Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16239",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16239/16046",
    "published": "2021-02",
    "summary": "Single-image deraining is rather challenging due to the unknown rain model. Existing methods often make specific assumptions of the rain model, which can hardly cover many diverse circumstances in the real world, compelling them to employ complex optimization or progressive refinement. This, however, significantly affects these methods' efficiency and effectiveness for many efficiency-critical applications. To fill this gap, in this paper, we regard the single-image deraining as a general image-enhancing problem and originally propose a model-free deraining method, i.e., EfficientDeRain, which is able to process a rainy image within 10 ms (i.e., around 6 ms on average), over 80 times faster than the state-of-the-art method (i.e., RCDNet), while achieving similar de-rain effects. We first propose novel pixel-wise dilation filtering. In particular, a rainy image is filtered with the pixel-wise kernels estimated from a kernel prediction network, by which suitable multi-scale kernels for each pixel can be efficiently predicted. Then, to eliminate the gap between synthetic and real data, we further propose an effective data augmentation method (i.e., RainMix) that helps to train the network for handling real rainy images. We perform a comprehensive evaluation on both synthetic and real-world rainy datasets to demonstrate the effectiveness and efficiency of our method. We release the model and code in https://github.com/tsingqguo/efficientderain.git."
  },
  "aaai2021_main_orderregularizationonordinallossforheadpose,ageandgazeestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Order Regularization on Ordinal Loss for Head Pose, Age and Gaze Estimation",
    "authors": [
      "Tianchu Guo",
      "Hui Zhang",
      "ByungIn Yoo",
      "Yongchao Liu",
      "Youngjun Kwak",
      "Jae-Joon\n      Han"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16240",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16240/16047",
    "published": "2021-02",
    "summary": "Ordinal loss is widely used in solving regression problems with deep learning technologies. Its basic idea is to convert regression to classification while preserving the natural order. However, the order constraint is enforced only by ordinal label implicitly, leading to the real output values not strictly in order. It causes the network to learn separable feature rather than discriminative feature, and possibly overfit on training set. In this paper, we propose order regularization on ordinal loss, which makes the outputs in order by explicitly constraining the ordinal classifiers in order. The proposed method contains two parts, i.e. similar-weights constraint, which reduces the ineffective space between classifiers, and differential-bias constraint, which enforces the decision planes in order and enhances the discrimination power of the classifiers. Experimental results show that our proposed method boosts the performance of original ordinal loss on various regression problems such as head pose, age, and gaze estimation, with significant error reduction of around 5%. Furthermore, our method outperforms the state of the art on all these tasks, with the performance gain of 14.4%, 2.2% and 6.5% on head pose, age and gaze estimation respectively."
  },
  "aaai2021_main_decoupledandmemory-reinforcednetworkstowardseffectivefeaturelearningforone-steppersonsearch": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Decoupled and Memory-Reinforced Networks: Towards Effective Feature Learning for One-Step Person Search",
    "authors": [
      "Chuchu Han",
      "Zhedong Zheng",
      "Changxin Gao",
      "Nong Sang",
      "Yi Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16241",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16241/16048",
    "published": "2021-02",
    "summary": "The goal of person search is to localize and match query persons from scene images. For high efficiency, one-step methods have been developed to jointly handle the pedestrian detection and identification sub-tasks using a single network. There are two major challenges in the current one-step approaches. One is the mutual interference between the optimization objectives of multiple sub-tasks. The other is the sub-optimal identification feature learning caused by small batch size when end-to-end training. To overcome these problems, we propose a decoupled and memory-reinforced network (DMRNet). Specifically, to reconcile the conflicts of multiple objectives, we simplify the standard tightly coupled pipelines and establish a deeply decoupled multi-task learning framework. Further, we build a memory-reinforced mechanism to boost the identification feature learning. By queuing the identification features of recently accessed instances into a memory bank, the mechanism augments the similarity pair construction for pairwise metric learning. For better encoding consistency of the stored features, a slow-moving average of the network is applied for extracting these features. In this way, the dual networks reinforce each other and converge to robust solution states. Experimentally, the proposed method obtains 93.2% and 46.9% mAP on CUHK-SYSU and PRW datasets, which exceeds all the existing one-step methods."
  },
  "aaai2021_main_sphericalimagegenerationfromasingleimagebyconsideringscenesymmetry": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Spherical Image Generation from a Single Image by Considering Scene Symmetry",
    "authors": [
      "Takayuki Hara",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16242",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16242/16049",
    "published": "2021-02",
    "summary": "Spherical images taken in all directions (360 degrees by 180 degrees) allow the full surroundings of a subject to be represented, providing an immersive experience to viewers. Generating a spherical image from a single normal-field-of-view (NFOV) image is convenient and expands the usage scenarios considerably without relying on a specific panoramic camera or images taken from multiple directions; however, achieving such images remains a challenging and unresolved problem. The primary challenge is controlling the high degree of freedom involved in generating a wide area that includes all directions of the desired spherical image. We focus on scene symmetry, which is a basic property of the global structure of spherical images, such as rotational symmetry, plane symmetry, and asymmetry. We propose a method for generating a spherical image from a single NFOV image and controlling the degree of freedom of the generated regions using the scene symmetry. To estimate and control the scene symmetry using both a circular shift and flip of the latent image features, we incorporate the intensity of the symmetry as a latent variable into conditional variational autoencoders. Our experiments show that the proposed method can generate various plausible spherical images controlled from symmetric to asymmetric, and can reduce the reconstruction errors of the generated images based on the estimated symmetry."
  },
  "aaai2021_main_progressiveone-shothumanparsing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Progressive One-shot Human Parsing",
    "authors": [
      "Haoyu He",
      "Jing Zhang",
      "Bhavani Thuraisingham",
      "Dacheng Tao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16243",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16243/16050",
    "published": "2021-02",
    "summary": "Prior human parsing models are limited to parsing humans into classes pre-defined in the training data, which is not flexible to generalize to unseen classes, e.g., new clothing in fashion analysis. In this paper, we propose a new problem named one-shot human parsing (OSHP) that requires to parse human into an open set of reference classes defined by any single reference example. During training, only base classes defined in the training set are exposed, which can overlap with part of reference classes. In this paper, we devise a novel Progressive One-shot Parsing network (POPNet) to address two critical challenges , i.e., testing bias and small sizes. POPNet consists of two collaborative metric learning modules named Attention Guidance Module and Nearest Centroid Module, which can learn representative prototypes for base classes and quickly transfer the ability to unseen classes during testing, thereby reducing testing bias. Moreover, POPNet adopts a progressive human parsing framework that can incorporate the learned knowledge of parent classes at the coarse granularity to help recognize the descendant classes at the fine granularity, thereby handling the small sizes issue. Experiments on the ATR-OS benchmark tailored for OSHP demonstrate POPNet outperforms other representative one-shot segmentation models by large margins and establishes a strong baseline. Source code can be found at https://github.com/Charleshhy/One-shot-Human-Parsing."
  },
  "aaai2021_main_consistent-separablefeaturerepresentationforsemanticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Consistent-Separable Feature Representation for Semantic Segmentation",
    "authors": [
      "Xingjian He",
      "Jing Liu",
      "Jun Fu",
      "Xinxin Zhu",
      "Jinqiao Wang",
      "Hanqing Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16244",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16244/16051",
    "published": "2021-02",
    "summary": "Cross-entropy loss combined with softmax is one of the most commonly used supervision components in most existing segmentation methods. The softmax loss is typically good at optimizing the inter-class difference, but not good at reducing the intra-class variation, which can be suboptimal for semantic segmentation task. In this paper, we propose a Consistent-Separable Feature Representation Network to model the Consistent-Separable (C-S) features, which are intra-class consistent and inter-class separable, improving the discriminative power of the deep features. Specifically, we develop a Consistent-Separable Feature Learning Module to obtain C-S features through a new loss, called Class-Aware Consistency loss. This loss function is proposed to force the deep features to be consistent among the same class and apart between different classes. Moreover, we design an Adaptive feature Aggregation Module to fuse the C-S features and original features from backbone for the better semantic prediction. We show that compared with various baselines, the proposed method brings consistent performance improvement. Our proposed approach achieves state-of-the-art performance on Cityscapes (82.6% mIoU in test set), ADE20K (46.65% mIoU in validation set), COCO Stuff (41.3% mIoU in validation set) and PASCAL Context (55.9% mIoU in test set)."
  },
  "aaai2021_main_error-awaredensityisomorphismreconstructionforunsupervisedcross-domaincrowdcounting": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Error-Aware Density Isomorphism Reconstruction for Unsupervised Cross-Domain Crowd Counting",
    "authors": [
      "Yuhang He",
      "Zhiheng Ma",
      "Xing Wei",
      "Xiaopeng Hong",
      "Wei Ke",
      "Yihong Gong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16245",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16245/16052",
    "published": "2021-02",
    "summary": "This paper focuses on the unsupervised domain adaptation problem for video-based crowd counting, in which we use labeled data as source domain and unlabelled video data as target domain. It is challenging as there is a huge gap between the source and the target domain and no annotations of samples are available in the target domain. The key issue is how to utilize unlabelled videos in the target domain for knowledge learning and transferring from the source domain. To tackle this problem, we propose a novel Error-aware Density Isomorphism REConstruction Network (EDIREC-Net) for cross-domain crowd counting. EDIREC-Net jointly transfers a pre-trained counting model to target domains using a density isomorphism reconstruction objective and models the reconstruction erroneousness by error reasoning. Specifically, as crowd flows in videos are consecutive, the density maps in adjacent frames turn out to be isomorphic. On this basis, we regard the density isomorphism reconstruction error as a self-supervised signal to transfer the pre-trained counting models to different target domains. Moreover, we leverage an estimation-reconstruction consistency to monitor the density reconstruction erroneousness and suppress unreliable density reconstructions during training. Experimental results on four benchmark datasets demonstrate the superiority of the proposed method and ablation studies investigate the efficiency and robustness. The source code is available at https://github.com/GehenHe/EDIREC-Net."
  },
  "aaai2021_main_droplossforlong-tailinstancesegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DropLoss for Long-Tail Instance Segmentation",
    "authors": [
      "Ting-I Hsieh",
      "Esther Robb",
      "Hwann-Tzong Chen",
      "Jia-Bin Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16246",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16246/16053",
    "published": "2021-02",
    "summary": "Long-tailed class distributions are prevalent among the practical applications of object detection and instance segmentation. Prior work in long-tail instance segmentation addresses the imbalance of losses between rare and frequent categories by reducing the penalty for a model incorrectly predicting a rare class label. We demonstrate that the rare categories are heavily suppressed by correct background predictions, which reduces the probability for all foreground categories with equal weight. Due to the relative infrequency of rare categories, this leads to an imbalance that biases towards predicting more frequent categories. Based on this insight, we develop DropLoss -- a novel adaptive loss to compensate for this imbalance without a trade-off between rare and frequent categories. With this loss, we show state-of-the-art mAP across rare, common, and frequent categories on the LVIS dataset. Codes are available at https://github.com/timy90022/DropLoss."
  },
  "aaai2021_main_hand-model-awaresignlanguagerecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Hand-Model-Aware Sign Language Recognition",
    "authors": [
      "Hezhen Hu",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16247",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16247/16054",
    "published": "2021-02",
    "summary": "Hand gestures play a dominant role in the expression of sign language. Current deep-learning based video sign language recognition (SLR) methods usually follow a data-driven paradigm under the supervision of the category label. However, those methods suffer limited interpretability and may encounter the overfitting issue due to limited sign data sources. In this paper, we introduce the hand prior and propose a new hand-model-aware framework for isolated SLR with the modeling hand as the intermediate representation. We first transform the cropped hand sequence into the latent semantic feature. Then the hand model introduces the hand prior and provides a mapping from the semantic feature to the compact hand pose representation. Finally, the inference module enhances the spatio-temporal pose representation and performs the final recognition. Due to the lack of annotation on the hand pose under current sign language datasets, we further guide its learning by utilizing multiple weakly-supervised losses to constrain its spatial and temporal consistency. To validate the effectiveness of our method, we perform extensive experiments on four benchmark datasets, including NMFs-CSL, SLR500, MSASL and WLASL. Experimental results demonstrate that our method achieves state-of-the-art performance on all four popular benchmarks with a notable margin."
  },
  "aaai2021_main_stratifiedrule-awarenetworkforabstractvisualreasoning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Stratified Rule-Aware Network for Abstract Visual Reasoning",
    "authors": [
      "Sheng Hu",
      "Yuqing Ma",
      "Xianglong Liu",
      "Yanlu Wei",
      "Shihao Bai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16248",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16248/16055",
    "published": "2021-02",
    "summary": "Abstract reasoning refers to the ability to analyze information, discover rules at an intangible level, and solve problems in innovative ways. Raven's Progressive Matrices (RPM) test is typically used to examine the capability of abstract reasoning. The subject is asked to identify the correct choice from the answer set to fill the missing panel at the bottom right of RPM (e.g., a 3\u00d73 matrix), following the underlying rules inside the matrix. Recent studies, taking advantage of Convolutional Neural Networks (CNNs), have achieved encouraging progress to accomplish the RPM test. However, they partly ignore necessary inductive biases of RPM solver, such as order sensitivity within each row/column and incremental rule induction. To address this problem, in this paper we propose a Stratified Rule-Aware Network (SRAN) to generate the rule embeddings for two input sequences. Our SRAN learns multiple granularity rule embeddings at different levels, and incrementally integrates the stratified embedding flows through a gated fusion module. With the help of embeddings, a rule similarity metric is applied to guarantee that SRAN can not only be trained using a tuplet loss but also infer the best answer efficiently. We further point out the severe defects existing in the popular RAVEN dataset for RPM test, which prevent from the fair evaluation of the abstract reasoning ability. To fix the defects, we propose an answer set generation algorithm called Attribute Bisection Tree (ABT), forming an improved dataset named Impartial-RAVEN (I-RAVEN for short). Extensive experiments are conducted on both PGM and I-RAVEN datasets, showing that our SRAN outperforms the state-of-the-art models by a considerable margin."
  },
  "aaai2021_main_vivovisualvocabularypre-trainingfornovelobjectcaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning",
    "authors": [
      "Xiaowei Hu",
      "Xi Yin",
      "Kevin Lin",
      "Lei Zhang",
      "Jianfeng Gao",
      "Lijuan Wang",
      "Zicheng Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16249",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16249/16056",
    "published": "2021-02",
    "summary": "It is highly desirable yet challenging to generate image captions that can describe novel objects which are unseen in caption-labeled training data, a capability that is evaluated in the novel object captioning challenge (nocaps). In this challenge, no additional image-caption training data, other than COCO Captions, is allowed for model training. Thus, conventional Vision-Language Pre-training (VLP) methods cannot be applied. This paper presents VIsual VOcabulary pre-training (VIVO) that performs pre-training in the absence of caption annotations. By breaking the dependency of paired image-caption training data in VLP, VIVO can leverage large amounts of paired image-tag data to learn a visual vocabulary. This is done by pre-training a multi-layer Transformer model that learns to align image-level tags with their corresponding image region features. To address the unordered nature of image tags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct pre-training.We validate the effectiveness of VIVO by fine-tuning the pre-trained model for image captioning. In addition, we perform an analysis of the visual-text alignment inferred by our model. The results show that our model can not only generate fluent image captions that describe novel objects, but also identify the locations of these objects. Our single model has achieved new state-of-the-art results on nocaps and surpassed the human CIDEr score."
  },
  "aaai2021_main_exploitingrelationshipforcomplex-sceneimagegeneration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Exploiting Relationship for Complex-scene Image Generation",
    "authors": [
      "Tianyu Hua",
      "Hongdong Zheng",
      "Yalong Bai",
      "Wei Zhang",
      "Xiao-Ping Zhang",
      "Tao\n      Mei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16250",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16250/16057",
    "published": "2021-02",
    "summary": "The significant progress on Generative Adversarial Networks (GANs) has facilitated realistic single-object image generation based on language input. However, complex-scene generation (with various interactions among multiple objects) still suffers from messy layouts and object distortions, due to diverse configurations in layouts and appearances. Prior methods are mostly object-driven and ignore their inter-relations that play a significant role in complex-scene images. This work explores relationship-aware complex-scene image generation, where multiple objects are inter-related as a scene graph. With the help of relationships, we propose three major updates in the generation framework. First, reasonable spatial layouts are inferred by jointly considering the semantics and relationships among objects. Compared to standard location regression, we show relative scales and distances serve a more reliable target. Second, since the relations between objects have significantly influenced an object's appearance, we design a relation-guided generator to generate objects reflecting their relationships. Third, a novel scene graph discriminator is proposed to guarantee the consistency between the generated image and the input scene graph. Our method tends to synthesize plausible layouts and objects, respecting the interplay of multiple objects in an image. Experimental results on Visual Genome and HICO-DET datasets show that our proposed method significantly outperforms prior arts in terms of IS and FID metrics. Based on our user study and visual inspection, our method is more effective in generating logical layout and appearance for complex-scenes."
  },
  "aaai2021_main_modelingdeeplearningbasedprivacyattacksonphysicalmail": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Modeling Deep Learning Based Privacy Attacks on Physical Mail",
    "authors": [
      "Bingyao Huang",
      "Ruyi Lian",
      "Dimitris Samaras",
      "Haibin Ling"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16251",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16251/16058",
    "published": "2021-02",
    "summary": "Mail privacy protection aims to prevent unauthorized access to hidden content within an envelope since normal paper envelopes are not as safe as we think. In this paper, for the first time, we show that with a well designed deep learning model, the hidden content may be largely recovered without opening the envelope. We start by modeling deep learning-based privacy attacks on physical mail content as learning the mapping from the camera-captured envelope front face image to the hidden content, then we explicitly model the mapping as a combination of perspective transformation, image dehazing and denoising using a deep convolutional neural network, named Neural-STE (See-Through-Envelope). We show experimentally that hidden content details, such as texture and image structure, can be clearly recovered. Finally, our formulation and model allow us to design envelopes that can counter deep learning-based privacy attacks on physical mail."
  },
  "aaai2021_main_ptnapoissontransfernetworkforsemi-supervisedfew-shotlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PTN: A Poisson Transfer Network for Semi-supervised Few-shot Learning",
    "authors": [
      "Huaxi Huang",
      "Junjie Zhang",
      "Jian Zhang",
      "Qiang Wu",
      "Chang Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16252",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16252/16059",
    "published": "2021-02",
    "summary": "The predicament in semi-supervised few-shot learning (SSFSL) is to maximize the value of the extra unlabeled data to boost the few-shot learner. In this paper, we propose a Poisson Transfer Network (PTN) to mine the unlabeled information for SSFSL from two aspects. First, the Poisson Merriman\u2013Bence\u2013Osher (MBO) model builds a bridge for the communications between labeled and unlabeled examples. This model serves as a more stable and informative classifier than traditional graph-based SSFSL methods in the message-passing process of the labels. Second, the extra unlabeled samples are employed to transfer the knowledge from base classes to novel classes through contrastive learning. Specifically, we force the augmented positive pairs close while push the negative ones distant. Our contrastive transfer scheme implicitly learns the novel-class embeddings to alleviate the over-fitting problem on the few labeled data. Thus, we can mitigate the degeneration of embedding generality in novel classes. Extensive experiments indicate that PTN outperforms the state-of-the-art few-shot and SSFSL models on miniImageNet and tieredImageNet benchmark datasets."
  },
  "aaai2021_main_text-guidedgraphneuralnetworksforreferring3dinstancesegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation",
    "authors": [
      "Pin-Hao Huang",
      "Han-Hung Lee",
      "Hwann-Tzong Chen",
      "Tyng-Luh Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16253",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16253/16060",
    "published": "2021-02",
    "summary": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object's features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to \"guide\" the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively."
  },
  "aaai2021_main_initiativedefenseagainstfacialmanipulation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Initiative Defense against Facial Manipulation",
    "authors": [
      "Qidong Huang",
      "Jie Zhang",
      "Wenbo Zhou",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16254",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16254/16061",
    "published": "2021-02",
    "summary": "Benefiting from the development of generative adversarial networks (GAN), facial manipulation has achieved significant progress in both academia and industry recently. It inspires an increasing number of entertainment applications but also incurs severe threats to individual privacy and even political security meanwhile. To mitigate such risks, many countermeasures have been proposed. However, the great majority methods are designed in a passive manner, which is to detect whether the facial images or videos are tampered after their wide propagation. These detection-based methods have a fatal limitation, that is, they only work for ex-post forensics but can not prevent the engendering of malicious behavior.To address the limitation, in this paper, we propose a novel framework of initiative defense to degrade the performance of facial manipulation models controlled by malicious users. The basic idea is to actively inject imperceptible venom into target facial data before manipulation. To this end, we first imitate the target manipulation model with a surrogate model, and then devise a poison perturbation generator to obtain the desired venom. An alternating training strategy are further leveraged to train both the surrogate model and the perturbation generator. Two typical facial manipulation tasks: face attribute editing and face reenactment, are considered in our initiative defense framework. Extensive experiments demonstrate the effectiveness and robustness of our framework in different settings. Finally, we hope this work can shed some light on initiative countermeasures against more adversarial scenarios."
  },
  "aaai2021_main_snapmixsemanticallyproportionalmixingforaugmentingfine-graineddata": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data",
    "authors": [
      "Shaoli Huang",
      "Xinchao Wang",
      "Dacheng Tao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16255",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16255/16062",
    "published": "2021-02",
    "summary": "Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly according to the mixture proportion of image pixels. Due to the major discriminative information of a fine-grained image usually resides in subtle regions, these methods tend to introduce heavy label noise in fine-grained recognition. We propose Semantically Proportional Mixing (SnapMix) that exploits class activation map (CAM) to lessen the label noise in augmenting fine-grained data. SnapMix generates the target label for a mixed image by estimating its intrinsic semantic composition. This strategy can adapt to asymmetric mixing operations and ensure semantic correspondence between synthetic images and target labels. Experiments show that our method consistently outperforms existing mixed-based approaches regardless of different datasets or network depths. Further, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a strong baseline for fine-grained recognition."
  },
  "aaai2021_main_ahybridattentionmechanismforweakly-supervisedtemporalactionlocalization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization",
    "authors": [
      "Ashraful Islam",
      "Chengjiang Long",
      "Richard Radke"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16256",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16256/16063",
    "published": "2021-02",
    "summary": "Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methodscannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an ``action-ness'' score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset."
  },
  "aaai2021_main_context-awaregraphconvolutionnetworkfortargetre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Context-Aware Graph Convolution Network for Target Re-identification",
    "authors": [
      "Deyi Ji",
      "Haoran Wang",
      "Hanzhe Hu",
      "Weihao Gan",
      "Wei Wu",
      "Junjie Yan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16257",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16257/16064",
    "published": "2021-02",
    "summary": "Most existing re-identification methods focus on learning robust and discriminative features with deep convolution networks. However, many of them consider content similarity separately and fail to utilize the context information of the query and gallery sets, e.g. probe-gallery and gallery-gallery relations, thus hard samples may not be well solved due to the limited or even misleading information. In this paper, we present a novel Context-Aware Graph Convolution Network (CAGCN), where the probe-gallery relations are encoded into the graph nodes and the graph edge connections are well controlled by the gallery-gallery relations. In this way, hard samples can be addressed with the context information flows among other easy samples during the graph reasoning. Specifically, we adopt an effective hard gallery sampler to obtain high recall for positive samples while keeping a reasonable graph size, which can also weaken the imbalanced problem in training process with low computation complexity. Experiments show that the proposed method achieves state-of-the-art performance on both person and vehicle re-identification datasets in a plug and play fashion with limited overhead."
  },
  "aaai2021_main_improvingimagecaptioningbyleveragingintra-andinter-layerglobalrepresentationintransformernetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network",
    "authors": [
      "Jiayi Ji",
      "Yunpeng Luo",
      "Xiaoshuai Sun",
      "Fuhai Chen",
      "Gen Luo",
      "Yongjian Wu",
      "Yue Gao",
      "Rongrong Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16258",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16258/16065",
    "published": "2021-02",
    "summary": "Transformer-based architectures have shown great success in image captioning, where object regions are encoded and then attended into the vectorial representations to guide the caption decoding. However, such vectorial representations only contain region-level information without considering the global information reflecting the entire image, which fails to expand the capability of complex multi-modal reasoning in image captioning. In this paper, we introduce a Global Enhanced Transformer (termed GET) to enable the extraction of a more comprehensive global representation, and then adaptively guide the decoder to generate high-quality captions.In GET, a Global Enhanced Encoderis designed for the embedding of the global feature,and a Global Adaptive Decoder are designed for the guidance of the caption generation. The former models intra- and inter-layer global representation by taking advantage of the proposed Global Enhanced Attention and a layer-wise fusion module. The lattercontains a Global Adaptive Controller that can adaptively fuse the global information into the decoder to guide the caption generation.Extensive experiments on MS COCO dataset demonstrate the superiority of our GET over many state-of-the-arts."
  },
  "aaai2021_main_frequencyconsistentadaptationforrealworldsuperresolution": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Frequency Consistent Adaptation for Real World Super Resolution",
    "authors": [
      "Xiaozhong Ji",
      "Guangpin Tao",
      "Yun Cao",
      "Ying Tai",
      "Tong Lu",
      "Chengjie Wang",
      "Jilin Li",
      "Feiyue Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16259",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16259/16066",
    "published": "2021-02",
    "summary": "Recent deep-learning based Super-Resolution (SR) methods have achieved remarkable performance on images with known degradation. However, these methods always fail in real-world scene, since the Low-Resolution (LR) images after the ideal degradation (e.g., bicubic down-sampling) deviate from real source domain. The domain gap between the LR images and the real-world images can be observed clearly on frequency density, which inspires us to explicitly narrow the undesired gap caused by incorrect degradation. From this point of view, we design a novel Frequency Consistent Adaptation (FCA) that ensures the frequency domain consistency when applying existing SR methods to the real scene. We estimate degradation kernels from unsupervised images and generate the corresponding LR images. To provide useful gradient information for kernel estimation, we propose Frequency Density Comparator (FDC) by distinguishing the frequency density of images on different scales. Based on the domain-consistent LR-HR pairs, we train easy-implemented Convolutional Neural Network (CNN) SR models. Extensive experiments show that the proposed FCA improves the performance of the SR model under real-world setting achieving state-of-the-art results with high fidelity and plausible perception, thus providing a novel effective framework for real-world SR application."
  },
  "aaai2021_main_matchingonsetsconqueroccludedpersonre-identificationwithoutalignment": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Matching on Sets: Conquer Occluded Person Re-identification Without Alignment",
    "authors": [
      "Mengxi Jia",
      "Xinhua Cheng",
      "Yunpeng Zhai",
      "Shijian Lu",
      "Siwei Ma",
      "Yonghong\n      Tian",
      "Jian Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16260",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16260/16067",
    "published": "2021-02",
    "summary": "Occluded person re-identification (re-ID) is a challenging task as different human parts may become invisible in cluttered scenes, making it hard to match person images of different identities. Most existing methods address this challenge by aligning spatial features of body parts according to semantic information (e.g. human poses) or feature similarities but this approach is complicated and sensitive to noises. This paper presents Matching on Sets (MoS), a novel method that positions occluded person re-ID as a set matching task without requiring spatial alignment. MoS encodes a person image by a pattern set as represented by a `global vector\u2019 with each element capturing one specific visual pattern, and it introduces Jaccard distance as a metric to compute the distance between pattern sets and measure image similarity. To enable Jaccard distance over continuous real numbers, we employ minimization and maximization to approximate the operations of intersection and union, respectively. In addition, we design a Jaccard triplet loss that enhances the pattern discrimination and allows to embed set matching into deep neural networks for end-to-end training. In the inference stage, we introduce a conflict penalty mechanism that detects mutually exclusive patterns in the pattern union of image pairs and decreases their similarities accordingly. Extensive experiments over three widely used datasets (Market1501, DukeMTMC and Occluded-DukeMTMC) show that MoS achieves superior re-ID performance. Additionally, it is tolerant of occlusions and outperforms the state-of-the-art by large margins for Occluded-DukeMTMC."
  },
  "aaai2021_main_gradingnettowardsprovidingreliablesupervisionsforweaklysupervisedobjectdetectionbygradingtheboxcandidates": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "GradingNet: Towards Providing Reliable Supervisions for Weakly Supervised Object Detection by Grading the Box Candidates",
    "authors": [
      "Qifei Jia",
      "Shikui Wei",
      "Tao Ruan",
      "Yufeng Zhao",
      "Yao Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16261",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16261/16068",
    "published": "2021-02",
    "summary": "Weakly-Supervised Object Detection (WSOD) aims at training a model with limited and coarse annotations for precisely locating the regions of objects. Existing works solve the WSOD problem by using a two-stage framework, i.e., generating candidate bounding boxes with weak supervision information and then refining them by directly employing supervised object detection models. However, most of such works mainly focus on the performance boosting of the first stage, while ignoring the better usage of generated candidate bounding boxes. To address this issue, we propose a new two-stage framework for WSOD, named GradingNet, which can make good use of the generated candidate bounding boxes. Specifically, the proposed GradingNet consists of two modules: Boxes Grading Module (BGM) and Informative Boosting Module (IBM). BGM generates proposals of the bounding boxes by using standard one-stage weakly-supervised methods, then utilizes Inclusion Principle to pick out highly-reliable boxes and evaluate the grade of each box. With the above boxes and their grade information, an effective anchor generator and a grade-aware loss are carefully designed to train the IBM. Taking the advantages of the grade information, our GradingNet achieves state-of-the-art performance on COCO, VOC 2007 and VOC 2012 benchmarks."
  },
  "aaai2021_main_ssn3dself-separatednetworktoalignpartsfor3dconvolutioninvideopersonre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SSN3D: Self-Separated Network to Align Parts for 3D Convolution in Video Person Re-Identification",
    "authors": [
      "Xiaoke Jiang",
      "Yu Qiao",
      "Junjie Yan",
      "Qichen Li",
      "Wanrong Zheng",
      "Dapeng Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16262",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16262/16069",
    "published": "2021-02",
    "summary": "Temporal appearance misalignment is a crucial problem in video person re-identification. The same part of person (e.g. head or hand) appearing on different locations in video sequence weakens its discriminative ability, especially when we apply standard temporal aggregation such as 3D convolution or LSTM. To address this issue, we propose Self-Separated network (SSN)to seek out the same parts in different images.As the name implies, SSN, if trained in an unsupervised strategy, guarantees the selected parts distinct. With a few samples of labeled parts to guide SSN training, this semi-supervised trained SSN seeks out the parts that are human-understandable within a frame and stable across a video snippet. Given the distinct and stable person parts, rather than performing aggregation on features, we then apply 3D convolution across different frames for person re-identification.This SSN + 3D pipeline, dubbed SSN3D, is proved to be efficient through extensive experiments on both synthetic and real data."
  },
  "aaai2021_main_trainingbinaryneuralnetworkwithoutbatchnormalizationforimagesuper-resolution": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Training Binary Neural Network without Batch Normalization for Image Super-Resolution",
    "authors": [
      "Xinrui Jiang",
      "Nannan Wang",
      "Jingwei Xin",
      "Keyu Li",
      "Xi Yang",
      "Xinbo Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16263",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16263/16070",
    "published": "2021-02",
    "summary": "Recently, binary neural network (BNN) based super-resolution (SR) methods have enjoyed initial success in the SR field.However, there is a noticeableperformance gap between the binarized model and the full-precision one. Furthermore, the batch normalization (BN) in binary SR networks introduces floating-point calculations, which is unfriendly to low-precision hardwares. Therefore, there is still room for improvement in terms of model performance and efficiency. Focusing on this issue, in this paper,we first explore a novel binary training mechanism based on the feature distribution, allowing us to replace all BN layers with a simple training method.Then, we construct a strong baseline by combining the highlights of recent binarization methods, whichalreadysurpasses the state-of-the-arts. Next, to train highly accurate binarized SR model, we also develop a lightweight network architecture and a multi-stage knowledge distillation strategy toenhance the model representation ability. Extensive experiments demonstrate that the proposed method not only presents advantages of lower computation as compared to conventional floating-point networksbut outperforms the state-of-the-art binary methods on the standard SR networks."
  },
  "aaai2021_main_whattoselectpursuingconsistentmotionsegmentationfrommultiplegeometricmodels": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "What to Select: Pursuing Consistent Motion Segmentation from Multiple Geometric Models",
    "authors": [
      "Yangbangyan Jiang",
      "Qianqian Xu",
      "Ke Ma",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16264",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16264/16071",
    "published": "2021-02",
    "summary": "Motion segmentation aims at separating motions of different moving objects in a video sequence. Facing the complicated real-world scenes, recent studies reveal that combining multiple geometric models would be a more effective way than just employing a single one. This motivates a new wave of model-fusion based motion segmentation methods. However, the vast majority of models of this kind merely seek consensus in spectral embeddings. We argue that a simple consensus might be insufficient to filter out the harmful information which is either unreliable or semantically unrelated to the segmentation task. Therefore, how to automatically select valuable patterns across multiple models should be regarded as a key challenge here. In this paper, we present a novel geometric-model-fusion framework for motion segmentation, which targets at constructing a consistent affinity matrix across all the geometric models. Specifically, it incorporates the structural information shared by affinity matrices to select those semantically consistent entries. Meanwhile, a multiplicative decomposition scheme is adopted to ensure structural consistency among multiple affinities. To solve this problem, an alternative optimization scheme is proposed, together with a proof of its global convergence. Experiments on four real-world benchmarks show the superiority of the proposed method."
  },
  "aaai2021_main_asynchronousteacherguidedbit-wisehardminingforonlinehashing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Asynchronous Teacher Guided Bit-wise Hard Mining for Online Hashing",
    "authors": [
      "Sheng Jin",
      "Qin Zhou",
      "Hongxun Yao",
      "Yao Liu",
      "Xian-Sheng Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16265",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16265/16072",
    "published": "2021-02",
    "summary": "Online hashing for streaming data has attracted increasing attention recently. However, most existing algorithms focus on batch inputs and instance-balanced optimization, which is limited in the single datum input case and does not match the dynamic training in online hashing. Furthermore, constantly updating the online model with new-coming samples will inevitably lead to the catastrophic forgetting problem. In this paper, we propose a novel online hashing method to handle the above-mentioned issues jointly, termed Asynchronus Teacher-Guided Bit-wise Hard Mining for Online Hashing. Firstly, to meet the needs of datum-wise online hashing, we design a novel binary codebook that is discriminative to separate different classes. Secondly, we propose a novel semantic loss (termed bit-wise attention loss) to dynamically focus on hard samples of each bit during training. Last but not least, we design a asynchronous knowledge distillation scheme to alleviate the catastrophic forgetting problem, where the teacher model is delaying updated to maintain the old knowledge, guiding the student model learning. Extensive experiments conducted on two public benchmarks demonstrate the favorable performance of our method over the state-of-the-arts."
  },
  "aaai2021_main_deeplow-contrastimageenhancementusingstructuretensorrepresentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Low-Contrast Image Enhancement using Structure Tensor Representation",
    "authors": [
      "Hyungjoo Jung",
      "Hyunsung Jang",
      "Namkoo Ha",
      "Kwanghoon Sohn"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16266",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16266/16073",
    "published": "2021-02",
    "summary": "We present a new deep learning framework for low-contrast image enhancement, which trains the network using the multi-exposure sequences rather than explicit ground-truth images. The purpose of our method is to enhance a low-contrast image so as to contain abundant details in various exposure levels. To realize this, we propose to design the loss function using the structure tensor representation, which has been widely used as high-dimensional image contrast. Our loss function penalizes the difference of the structure tensor between the network output and the multi-exposure images in a multi-scale manner. Eventually, the network trained by the loss function produces a high-quality image approximating the overall contrast of the sequence. We provide in-depth analysis on our method and comparison with conventional loss functions. Quantitative and qualitative evaluations demonstrate that the proposed method outperforms the existing state-of-the-art approaches in various benchmarks."
  },
  "aaai2021_main_spectraldistributionawareimagegeneration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Spectral Distribution Aware Image Generation",
    "authors": [
      "Steffen Jung",
      "Margret Keuper"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16267",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16267/16074",
    "published": "2021-02",
    "summary": "Recent advances in deep generative models for photo-realistic images have led to high quality visual results. Such models learn to generate data from a given training distribution such that generated images can not be easily distinguished from real images by the human eye. Yet, recent work on the detection of such fake images pointed out that they are actually easily distinguishable by artifacts in their frequency spectra. In this paper, we propose to generate images according to the frequency distribution of the real data by employing a spectral discriminator. The proposed discriminator is lightweight, modular and works stably with different commonly used GAN losses. We show that the resulting models can better generate images with realistic frequency spectra, which are thus harder to detect by this cue."
  },
  "aaai2021_main_starnettowardsweaklysupervisedfew-shotobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "StarNet: towards Weakly Supervised Few-Shot Object Detection",
    "authors": [
      "Leonid Karlinsky",
      "Joseph Shtok",
      "Amit Alfassy",
      "Moshe Lichtenstein",
      "Sivan\n      Harary",
      "Eli Schwartz",
      "Sivan Doveh",
      "Prasanna Sattigeri",
      "Rogerio Feris",
      "Alex\n      Bronstein",
      "Raja Giryes"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16268",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16268/16075",
    "published": "2021-02",
    "summary": "Few-shot detection and classification have advanced significantly in recent years. Yet, detection approaches require strong annotation (bounding boxes) both for pre-training and for adaptation to novel classes, and classification approaches rarely provide localization of objects in the scene. In this paper, we introduce StarNet - a few-shot model featuring an end-to-end differentiable non-parametric star-model detection and classification head. Through this head, the backbone is meta-trained using only image-level labels to produce good features for jointly localizing and classifying previously unseen categories of few-shot test tasks using a star-model that geometrically matches between the query and support images (to find corresponding object instances). Being a few-shot detector, StarNet does not require any bounding box annotations, neither during pre-training nor for novel classes adaptation. It can thus be applied to the previously unexplored and challenging task of Weakly Supervised Few-Shot Object Detection (WS-FSOD), where it attains significant improvements over the baselines. In addition, StarNet shows significant gains on few-shot classification benchmarks that are less cropped around the objects (where object localization is key)."
  },
  "aaai2021_main_discriminativeregionsuppressionforweakly-supervisedsemanticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation",
    "authors": [
      "Beomyoung Kim",
      "Sangeun Han",
      "Junmo Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16269",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16269/16076",
    "published": "2021-02",
    "summary": "Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate high-quality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach."
  },
  "aaai2021_main_visualcomfortaware-reinforcementlearningfordepthadjustmentofstereoscopic3dimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Visual Comfort Aware-Reinforcement Learning for Depth Adjustment of Stereoscopic 3D Images",
    "authors": [
      "Hak Gu Kim",
      "Minho Park",
      "Sangmin Lee",
      "Seongyeop Kim",
      "Yong Man Ro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16270",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16270/16077",
    "published": "2021-02",
    "summary": "Depth adjustment aims to enhance the visual experience of stereoscopic 3D (S3D) images, which accompanied with improving visual comfort and depth perception. For a human expert, the depth adjustment procedure is a sequence of iterative decision making. The human expert iteratively adjusted the depth until he is satisfied with the both levels of visual comfort and the perceived depth. In this work, we present a novel deep reinforcement learning (DRL)-based approach for depth adjustment named VCA-RL (Visual Comfort Aware Reinforcement Learning) to explicitly model human sequential decision making in depth editing operations. We formulate the depth adjustment process as a Markov decision process where actions are defined as camera movement operations to control the distance between the left and right cameras. Our agent is trained based on the guidance of an objective visual comfort assessment metric to learn the optimal sequence of camera movement actions in terms of perceptual aspects in stereoscopic viewing. With extensive experiments and user studies, we show the effectiveness of our VCA-RL model on three different S3D databases."
  },
  "aaai2021_main_dualcompositionallearningininteractiveimageretrieval": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dual Compositional Learning in Interactive Image Retrieval",
    "authors": [
      "Jongseok Kim",
      "Youngjae Yu",
      "Hoeseong Kim",
      "Gunhee Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16271",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16271/16078",
    "published": "2021-02",
    "summary": "We present an approach named Dual Composition Network (DCNet) for interactive image retrieval that searches for the best target image for a natural language query and a reference image. To accomplish this task, existing methods have focused on learning a composite representation of the reference image and the text query to be as close to the embedding of the target image as possible. We refer this approach as Composition Network. In this work, we propose to close the loop with Correction Network that models the difference between the reference and target image in the embedding space and matches it with the embedding of the text query. That is, we consider two cyclic directional mappings for triplets of (reference image, text query, target image) by using both Composition Network and Correction Network. We also propose a joint training loss that can further improve the robustness of multimodal representation learning. We evaluate the proposed model on three benchmark datasets for multimodal retrieval: Fashion-IQ, Shoes, and Fashion200K. Our experiments show that our DCNet achieves new state-of-the-art performance on all three datasets, and the addition of Correction Network consistently improves multiple existing methods that are solely based on Composition Network. Moreover, an ensemble of our model won the first place in Fashion-IQ 2020 challenge held in a CVPR 2020 workshop."
  },
  "aaai2021_main_end-to-enddifferentiablelearningtohdrimagesynthesisformulti-exposureimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "End-to-End Differentiable Learning to HDR Image Synthesis for Multi-exposure Images",
    "authors": [
      "Junghee Kim",
      "Siyeong Lee",
      "Suk-Ju Kang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16272",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16272/16079",
    "published": "2021-02",
    "summary": "Recently, high dynamic range (HDR) image reconstruction based on the multiple exposure stack from a given single exposure utilizes a deep learning framework to generate high-quality HDR images. These conventional networks focus on the exposure transfer task to reconstruct the multi-exposure stack. Therefore, they often fail to fuse the multi-exposure stack into a perceptually pleasant HDR image as the inversion artifacts occur. We tackle the problem in stack reconstruction-based methods by proposing a novel framework with a fully differentiable high dynamic range imaging (HDRI) process. By explicitly using the loss, which compares the network's output with the ground truth HDR image, our framework enables a neural network that generates the multiple exposure stack for HDRI to train stably. In other words, our differentiable HDR synthesis layer helps the deep neural network to train to create multi-exposure stacks while reflecting the precise correlations between multi-exposure images in the HDRI process. In addition, our network uses the image decomposition and the recursive process to facilitate the exposure transfer task and to adaptively respond to recursion frequency. The experimental results show that the proposed network outperforms the state-of-the-art quantitative and qualitative results in terms of both the exposure transfer tasks and the whole HDRI process."
  },
  "aaai2021_main_structuredco-referencegraphattentionforvideo-groundeddialogue": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Structured Co-reference Graph Attention for Video-grounded Dialogue",
    "authors": [
      "Junyeong Kim",
      "Sunjae Yoon",
      "Dahyun Kim",
      "Chang D. Yoo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16273",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16273/16080",
    "published": "2021-02",
    "summary": "A video-grounded dialogue system referred to as the Structured Co-reference Graph Attention (SCGA) is presented for decoding the answer sequence to a question regarding a given video while keeping track of the dialogue context. Although recent efforts have made great strides in improving the quality of the response, performance is still far from satisfactory. The two main challenging issues are as follows: (1) how to deduce co-reference among multiple modalities and (2) how to reason on the rich underlying semantic structure of video with complex spatial and temporal dynamics. To this end, SCGA is based on (1) Structured Co-reference Resolver that performs dereferencing via building a structured graph over multiple modalities, (2) Spatio-temporal Video Reasoner that captures local-to-global dynamics of video via gradually neighboring graph attention. SCGA makes use of pointer network to dynamically replicate parts of the question for decoding the answer sequence. The validity of the proposed SCGA is demonstrated on AVSD@DSTC7 and AVSD@DSTC8 datasets, a challenging video-grounded dialogue benchmarks, and TVQA dataset, a large-scale videoQA benchmark. Our empirical results show that SCGA outperforms other state-of-the-art dialogue systems on both benchmarks, while extensive ablation study and qualitative analysis reveal performance gain and improved interpretability."
  },
  "aaai2021_main_cross-domaingroupingandalignmentfordomainadaptivesemanticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Cross-Domain Grouping and Alignment for Domain Adaptive Semantic Segmentation",
    "authors": [
      "Minsu Kim",
      "Sunghun Joung",
      "Seungryong Kim",
      "JungIn Park",
      "Ig-Jae Kim",
      "Kwanghoon Sohn"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16274",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16274/16081",
    "published": "2021-02",
    "summary": "Existing techniques to adapt semantic segmentation networks across source and target domains within deep convolutional neural networks (CNNs) deal with all the samples from the two domains in a global or category-aware manner. They do not consider an inter-class variation within the target domain itself or estimated category, providing the limitation to encode the domains having a multi-modal data distribution. To overcome this limitation, we introduce a learnable clustering module, and a novel domain adaptation framework, called cross-domain grouping and alignment. To cluster the samples across domains with an aim to maximize the domain alignment without forgetting precise segmentation ability on the source domain, we present two loss functions, in particular, for encouraging semantic consistency and orthogonality among the clusters. We also present a loss so as to solve a class imbalance problem, which is the other limitation of the previous methods. Our experiments show that our method consistently boosts the adaptation performance in semantic segmentation, outperforming the state-of-the-arts on various domain adaptation settings."
  },
  "aaai2021_main_bidirectionalrnn-basedfewshotlearningfor3dmedicalimagesegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation",
    "authors": [
      "Soopil Kim",
      "Sion An",
      "Philip Chikontwe",
      "Sang Hyun Park"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16275",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16275/16082",
    "published": "2021-02",
    "summary": "Segmentation of organs of interest in 3D medical images is necessary for accurate diagnosis and longitudinal studies. Though recent advances using deep learning have shown success for many segmentation tasks, large datasets are required for high performance and the annotation process is both time consuming and labor intensive. In this paper, we propose a 3D few shot segmentation framework for accurate organ segmentation using limited training samples of the target organ annotation. To achieve this, a U-Net like network is designed to predict segmentation by learning the relationship between 2D slices of support data and a query image, including a bidirectional gated recurrent unit (GRU) that learns consistency of encoded features between adjacent slices. Also, we introduce a transfer learning method to adapt the characteristics of the target image and organ by updating the model before testing with arbitrary support and query data sampled from the support data. We evaluate our proposed model using three 3D CT datasets with annotations of different organs. Our model yielded significantly improved performance over state-of-the-art few shot segmentation models and was comparable to a fully supervised model trained with more target training data."
  },
  "aaai2021_main_daszldynamicactionsignaturesforzero-shotlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DASZL: Dynamic Action Signatures for Zero-shot Learning",
    "authors": [
      "Tae Soo Kim",
      "Jonathan Jones",
      "Michael Peven",
      "Zihao Xiao",
      "Jin Bai",
      "Yi Zhang",
      "Weichao Qiu",
      "Alan Yuille",
      "Gregory D. Hager"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16276",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16276/16083",
    "published": "2021-02",
    "summary": "There are many realistic applications of activity recognition where the set of potential activity descriptions is combinatorially large. This makes end-to-end supervised training of a recognition system impractical as no training set is practically able to encompass the entire label set. In this paper, we present an approach to fine-grained recognition that models activities as compositions of dynamic action signatures.This compositional approach allows us to reframe fine-grained recognition as zero-shot activity recognition,where a detector is composed \"on the fly\" from simple first-principles state machines supported by deep-learned components.We evaluate our method on the Olympic Sports and UCF101 datasets, where our model establishes a new state of the art under multiple experimental paradigms. We also extend this method to form a unique framework for zero-shot joint segmentation and classification of activities in video and demonstrate the first results in zero- shot decoding of complex action sequences on a widely-used surgical dataset. Lastly, we show that we can useoff-the-shelf object detectors to recognize activities in completely de-novo settings with no additional training."
  },
  "aaai2021_main_multi-leveldistanceregularizationfordeepmetriclearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Multi-level Distance Regularization for Deep Metric Learning",
    "authors": [
      "Yonghyun Kim",
      "Wonpyo Park"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16277",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16277/16084",
    "published": "2021-02",
    "summary": "We propose a novel distance-based regularization method for deep metric learning called Multi-level Distance Regularization (MDR).MDR explicitly disturbs a learning procedure by regularizing pairwise distances between embedding vectors into multiple levels that represents a degree of similarity between a pair.In the training stage, the model is trained with both MDR and an existing loss function of deep metric learning, simultaneously; the two losses interfere with the objective of each other, and it makes the learning process difficult.Moreover, MDR prevents some examples from being ignored or overly influenced in the learning process. These allow the parameters of the embedding network to be settle on a local optima with better generalization. Without bells and whistles, MDR with simple Triplet loss achieves the-state-of-the-art performance in various benchmark datasets: CUB-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval. We extensively perform ablation studies on its behaviors to show the effectiveness of MDR. By easily adopting our MDR, the previous approaches can be improved in performance and generalization ability."
  },
  "aaai2021_main_dynamictostaticlidarscanreconstructionusingadversariallytrainedautoencoder": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dynamic to Static Lidar Scan Reconstruction Using Adversarially Trained Auto Encoder",
    "authors": [
      "Prashant Kumar",
      "Sabyasachi Sahoo",
      "Vanshil Shah",
      "Vineetha Kondameedi",
      "Abhinav Jain",
      "Akshaj Verma",
      "Chiranjib Bhattacharyya",
      "Vinay Vishwanath"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16278",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16278/16085",
    "published": "2021-02",
    "summary": "Accurate reconstruction of static environments from LiDAR scans of scenes containing dynamic objects, which we refer to as Dynamic to Static Translation (DST), is an important area of research in Autonomous Navigation. This problem has been recently explored for visual SLAM, but to the best of our knowledge no work has been attempted to address DST for LiDAR scans. The problem is of critical importance due to wide-spread adoption of LiDAR in Autonomous Vehicles. We show that state-of the art methods developed for the visual domain when adapted for LiDAR scans perform poorly. We develop DSLR, a deep generative model which learns a mapping between dynamic scan to its static counterpart through an adversarially trained autoencoder. Our model yields the first solution for DST on LiDAR that generates static scans without using explicit segmentation labels. DSLR cannot always be applied to real world data due to lack of paired dynamic-static scans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer to real world data and experimentally show that this performs well in real world settings. Additionally, if segmentation information is available, we extend DSLR to DSLR-Seg to further improve the reconstruction quality. DSLR gives the state of the art performance on simulated and real-world datasets and also shows at least 4\u00d7 improvement. We show that DSLR, unlike the existing baselines, is a practically viable model with its reconstruction quality within the tolerable limits for tasks pertaining to autonomous navigation like SLAM in dynamic environments."
  },
  "aaai2021_main_regularizingattentionnetworksforanomalydetectioninvisualquestionanswering": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Regularizing Attention Networks for Anomaly Detection in Visual Question Answering",
    "authors": [
      "Doyup Lee",
      "Yeongjae Cheon",
      "Wook-Shin Han"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16279",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16279/16086",
    "published": "2021-02",
    "summary": "For stability and reliability of real-world applications, the robustness of DNNs in unimodal tasks has been evaluated. However, few studies consider abnormal situations that a visual question answering (VQA) model might encounter at test time after deployment in the real-world. In this study, we evaluate the robustness of state-of-the-art VQA models to five different anomalies, including worst-case scenarios, the most frequent scenarios, and the current limitation of VQA models. Different from the results in unimodal tasks, the maximum confidence of answers in VQA models cannot detect anomalous inputs, and post-training of the outputs, such as outlier exposure, is ineffective for VQA models. Thus, we propose an attention-based method, which uses confidence of reasoning between input images and questions and shows much more promising results than the previous methods in unimodal tasks. In addition, we show that a maximum entropy regularization of attention networks can significantly improve the attention-based anomaly detection of the VQA models. Thanks to the simplicity, attention-based anomaly detection and the regularization are model-agnostic methods, which can be used for various cross-modal attentions in the state-of-the-art VQA models. The results imply that cross-modal attention in VQA is important to improve not only VQA accuracy, but also the robustness to various anomalies."
  },
  "aaai2021_main_weakly-supervisedtemporalactionlocalizationbyuncertaintymodeling": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Weakly-supervised Temporal Action Localization by Uncertainty Modeling",
    "authors": [
      "Pilhyeon Lee",
      "Jinglu Wang",
      "Yan Lu",
      "Hyeran Byun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16280",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16280/16087",
    "published": "2021-02",
    "summary": "Weakly-supervised temporal action localization aims to learn detecting temporal intervals of action classes with only video-level labels. To this end, it is crucial to separate frames of action classes from the background frames (i.e., frames not belonging to any action classes). In this paper, we present a new perspective on background frames where they are modeled as out-of-distribution samples regarding their inconsistency. Then, background frames can be detected by estimating the probability of each frame being out-of-distribution, known as uncertainty, but it is infeasible to directly learn uncertainty without frame-level labels. To realize the uncertainty learning in the weakly-supervised setting, we leverage the multiple instance learning formulation. Moreover, we further introduce a background entropy loss to better discriminate background frames by encouraging their in-distribution (action) probabilities to be uniformly distributed over all action classes. Experimental results show that our uncertainty modeling is effective at alleviating the interference of background frames and brings a large performance gain without bells and whistles. We demonstrate that our model significantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14 and ActivityNet (1.2 & 1.3). Our code is available at https://github.com/Pilhyeon/WTAL-Uncertainty-Modeling."
  },
  "aaai2021_main_learningmonoculardepthindynamicscenesviainstance-awareprojectionconsistency": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency",
    "authors": [
      "Seokju Lee",
      "Sunghoon Im",
      "Stephen Lin",
      "In So Kweon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16281",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16281/16088",
    "published": "2021-02",
    "summary": "We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are publicly available."
  },
  "aaai2021_main_patch-wiseattentionnetworkformonoculardepthestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Patch-Wise Attention Network for Monocular Depth Estimation",
    "authors": [
      "Sihaeng Lee",
      "Janghyeon Lee",
      "Byungju Kim",
      "Eojindl Yi",
      "Junmo Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16282",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16282/16089",
    "published": "2021-02",
    "summary": "In computer vision, monocular depth estimation is the problem of obtaining a high-quality depth map from a two-dimensional image. This map provides information on three-dimensional scene geometry, which is necessary for various applications in academia and industry, such as robotics and autonomous driving. Recent studies based on convolutional neural networks achieved impressive results for this task. However, most previous studies did not consider the relationships between the neighboring pixels in a local area of the scene. To overcome the drawbacks of existing methods, we propose a patch-wise attention method for focusing on each local area. After extracting patches from an input feature map, our module generates attention maps for each local patch, using two attention modules for each patch along the channel and spatial dimensions. Subsequently, the attention maps return to their initial positions and merge into one attention feature. Our method is straightforward but effective. The experimental results on two challenging datasets, KITTI and NYU Depth V2, demonstrate that the proposed method achieves significant performance. Furthermore, our method outperforms other state-of-the-art methods on the KITTI depth estimation benchmark."
  },
  "aaai2021_main_semi-supervisedlearningformulti-tasksceneunderstandingbyneuralgraphconsensus": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Semi-Supervised Learning for Multi-Task Scene Understanding by Neural Graph Consensus",
    "authors": [
      "Marius Leordeanu",
      "Mihai Cristian P\u00eervu",
      "Dragos Costea",
      "Alina E Marcu",
      "Emil\n      Slusanschi",
      "Rahul Sukthankar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16283",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16283/16090",
    "published": "2021-02",
    "summary": "We address the challenging problem of semi-supervised learning in the context of multiple visual interpretations of the world by finding consensus in a graph of neural networks. Each graph node is a scene interpretation layer, while each edge is a deep net that transforms one layer at one node into another from a different node. During the supervised phase edge networks are trained independently. During the next unsupervised stage edge nets are trained on the pseudo-ground truth provided by consensus among multiple paths that reach the nets' start and end nodes. These paths act as ensemble teachers for any given edge and strong consensus is used for high-confidence supervisory signal. The unsupervised learning process is repeated over several generations, in which each edge becomes a \"student\" and also part of different ensemble \"teachers\" for training other students. By optimizing such consensus between different paths, the graph reaches consistency and robustness over multiple interpretations and generations, in the face of unknown labels. We give theoretical justifications of the proposed idea and validate it on a large dataset. We show how prediction of different representations such as depth, semantic segmentation, surface normals and pose from RGB input could be effectively learned through self-supervised consensus in our graph. We also compare to state-of-the-art methods for multi-task and semi-supervised learning and show superior performance."
  },
  "aaai2021_main_static-dynamicinteractionnetworksforofflinesignatureverification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Static-Dynamic Interaction Networks for Offline Signature Verification",
    "authors": [
      "Huan Li",
      "Ping Wei",
      "Ping Hu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16284",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16284/16091",
    "published": "2021-02",
    "summary": "Offline signature verification is a challenging issue that is widely used in various fields. Previous approaches model this task as a static feature matching or distance metric problem of two images. In this paper, we propose a novel Static-Dynamic Interaction Network (SDINet) model which introduces sequential representation into static signature images. A static signature image is converted to sequences by assuming pseudo dynamic processes in the static image. A static representation extracting deep features from signature images describes the global information of signatures. A dynamic representation extracting sequential features with LSTM networks characterizes the local information of signatures. A dynamic-to-static attention is learned from the sequences to refine the static features. Through the static-to-dynamic conversion and the dynamic-to-static attention, the static representation and dynamic representation are unified into a compact framework. The proposed method was evaluated on four popular datasets of different languages. The extensive experimental results manifest the strength of our model."
  },
  "aaai2021_main_proposal-freevideogroundingwithcontextualpyramidnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Proposal-Free Video Grounding with Contextual Pyramid Network",
    "authors": [
      "Kun Li",
      "Dan Guo",
      "Meng Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16285",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16285/16092",
    "published": "2021-02",
    "summary": "The challenge of video grounding - localizing activities in an untrimmed video via a natural language query - is to tackle the semantics of vision and language consistently along the temporal dimension. Most existing proposal-based methods are trapped by computational cost with extensive candidate proposals. In this paper, we propose a novel proposal-free framework named Contextual Pyramid Network (CPNet) to investigate multi-scale temporal correlation in the video. Specifically, we propose a pyramid network to extract 2D contextual correlation maps at different temporal scales (T*T, T/2*T/2, T/4*T/4), where the 2D correlation map (past to current & future to current) is designed to model all the relations of any two moments in the video. In other words, CPNet progressively replenishes the temporal contexts and refines the location of queried activity by enlarging the temporal receptive fields. Finally, we implement a temporal self-attentive regression (i.e., proposal-free regression) to predict the activity boundary from the above hierarchical context-aware 2D correlation maps. Extensive experiments on ActivityNet Captions, Charades-STA, and TACoS datasets demonstrate that our approach outperforms state-of-the-art methods."
  },
  "aaai2021_main_write-a-speakertext-basedemotionalandrhythmictalking-headgeneration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation",
    "authors": [
      "Lincheng Li",
      "Suzhen Wang",
      "Zhimeng Zhang",
      "Yu Ding",
      "Yixing Zheng",
      "Xin Yu",
      "Changjie Fan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16286",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16286/16093",
    "published": "2021-02",
    "summary": "In this paper, we propose a novel text-based talking-head video generation framework that synthesizes high-fidelity facial expressions and head motions in accordance with contextual sentiments as well as speech rhythm and pauses. To be specific, our framework consists of a speaker-independent stage and a speaker-specific stage. In the speaker-independent stage, we design three parallel networks to generate animation parameters of the mouth, upper face, and head from texts, separately. In the speaker-specific stage, we present a 3D face model guided attention network to synthesize videos tailored for different individuals. It takes the animation parameters as input and exploits an attention mask to manipulate facial expression changes for the input individuals. Furthermore, to better establish authentic correspondences between visual motions (i.e., facial expression changes and head movements) and audios, we leverage a high-accuracy motion capture dataset instead of relying on long videos of specific individuals. After attaining the visual and audio correspondences, we can effectively train our network in an end-to-end fashion. Extensive experiments on qualitative and quantitative results demonstrate that our algorithm achieves high-quality photo-realistic talking-head videos including various facial expressions and head motions according to speech rhythms and outperforms the state-of-the-art."
  },
  "aaai2021_main_exploitinglearnablejointgroupsforhandposeestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Exploiting Learnable Joint Groups for Hand Pose Estimation",
    "authors": [
      "Moran Li",
      "Yuan Gao",
      "Nong Sang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16287",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16287/16094",
    "published": "2021-02",
    "summary": "In this paper, we propose to estimate 3D hand pose by recovering the 3D coordinates of joints in a group-wise manner, where less-related joints are automatically categorized into different groups and exhibit different features. This is different from the previous methods where all the joints are considered holistically and share the same feature. The benefits of our method are illustrated by the principle of multi-task learning (MTL), i.e., by separating less-related joints into different groups (as different tasks), our method learns different features for each of them, therefore efficiently avoids the negative transfer (among less related tasks/groups of joints). The key of our method is a novel binary selector that automatically selects related joints into the same group. We implement such a selector with binary values stochastically sampled from a Concretedistribution, which is constructed using Gumbel softmax on trainable parameters. This enables us to preserve the differentiable property of the whole network. We further exploit features from those less-related groups by carrying out an additional feature fusing scheme among them, to learn more discriminative features. This is realized by implementing multiple 1x1 convolutions on the concatenated features, where each joint group contains a unique 1x1convolutionfor feature fusion.The detailed ablation analysis and the extensive experiments on several benchmark datasets demonstrate the promising performance of the proposed method over the state-of-the-art (SOTA) methods. Besides, our method achieves top-1 among all the methods that do not exploit the dense 3D shape labels on the most recently released FreiHAND competition at the submission date. The source code and models are available at https://github.com/moranli-aca/LearnableGroups-Hand."
  },
  "aaai2021_main_rts3dreal-timestereo3ddetectionfrom4dfeature-consistencyembeddingspaceforautonomousdriving": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "RTS3D: Real-time Stereo 3D Detection from 4D Feature-Consistency Embedding Space for Autonomous Driving",
    "authors": [
      "Peixuan Li",
      "Shun Su",
      "Huaici Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16288",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16288/16095",
    "published": "2021-02",
    "summary": "Although the recent image-based 3D object detection methods using Pseudo-LiDAR representation have shown great capabilities, a notable gap in efficiency and accuracy still exist compared with LiDAR-based methods. Besides, over-reliance on the stand-alone depth estimator, requiring a large number of pixel-wise annotations in the training stage and more computation in the inferencing stage, limits the scaling application in the real world. In this paper, we propose an efficient and accurate 3D object detection method from stereo images, named RTS3D. Different from the 3D occupancy space in the Pseudo-LiDAR similar methods, we design a novel 4D feature-consistent embedding (FCE) space as the intermediate representation of the 3D scene without depth supervision. The FCE space encodes the object's structural and semantic information by exploring the multi-scale feature consistency warped from stereo pair. Furthermore, a semantic-guided RBF (Radial Basis Function) and a structure-aware attention module are devised to reduce the influence of FCE space noise without instance mask supervision. Experiments on KITTI benchmark show that RTS3D is the first true real-time system (FPS>24) for stereo image 3D detection meanwhile achieves 10% improvement in average precision comparing with the previous state-of-the-art method."
  },
  "aaai2021_main_adversarialposeregressionnetworkforpose-invariantfacerecognitions": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Adversarial Pose Regression Network for Pose-Invariant Face Recognitions",
    "authors": [
      "Pengyu Li",
      "Biao Wang",
      "Lei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16289",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16289/16096",
    "published": "2021-02",
    "summary": "Face recognition has achieved significant progress in recent years. However, the large pose variation between face images remains a challenge in face recognition. We observe that the pose variation in the hidden feature maps is one of the most critical factors to hinder the representations from being pose-invariant. Based on the observation, we propose an Adversarial Pose Regression Network (APRN) to extract pose-invariant identity representations by disentangling their pose variation in hidden feature maps. To model the pose discriminator in APRN as a regression task in its 3D space, we also propose an Adversarial Regression Loss Function and extend the adversarial learning from classification problems to regression problems in this paper. Our APRN is a plug-and-play structure that can be embedded in other state-of-the-art face recognition algorithms to improve their performance additionally. The experiments show that the proposed APRN consistently and significantly boosts the performance of baseline networks without extra computational costs in the inference phase. APRN achieves comparable or even superior to the state-of-the-art on CFP, Multi-PIE, IJB-A and MegaFace datasets. The code will be released, hoping to nourish our proposals to other computer vision fields"
  },
  "aaai2021_main_categorydictionaryguidedunsuperviseddomainadaptationforobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Category Dictionary Guided Unsupervised Domain Adaptation for Object Detection",
    "authors": [
      "Shuai Li",
      "Jianqiang Huang",
      "Xian-Sheng Hua",
      "Lei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16290",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16290/16097",
    "published": "2021-02",
    "summary": "Unsupervised domain adaption (UDA) is a promising solution to enhance the generalization ability of a model from a source domain to a target domain without manually annotating labels for target data. Recent works in cross-domain object detection mostly resort to adversarial feature adaptation to match the marginal distributions of two domains. However, perfect feature alignment is hard to achieve and is likely to cause negative transfer due to the high complexity of object detection. In this paper, we propose a category dictionary guided (CDG) UDA model for cross-domain object detection, which learns category-specific dictionaries from the source domain to represent the candidate boxes in target domain. The representation residual can be used for not only pseudo label assignment but also quality (e.g., IoU) estimation of the candidate box. A residual weighted self-training paradigm is then developed to implicitly align source and target domains for detection model training. Compared with decision boundary based classifiers such as softmax, the proposed CDG scheme can select more informative and reliable pseudo-boxes. Experimental results on benchmark datasets show that the proposed CDG significantly exceeds the state-of-the-arts in cross-domain object detection."
  },
  "aaai2021_main_jointsemantic-geometriclearningforpolygonalbuildingsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Joint Semantic-geometric Learning for Polygonal Building Segmentation",
    "authors": [
      "Weijia Li",
      "Wenqian Zhao",
      "Huaping Zhong",
      "Conghui He",
      "Dahua Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16291",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16291/16098",
    "published": "2021-02",
    "summary": "Building extraction from aerial or satellite images has been an important research issue in remote sensing and computer vision domains for decades. Compared with pixel-wise semantic segmentation models that output raster building segmentation map, polygonal building segmentation approaches produce more realistic building polygons that are in the desirable vector format for practical applications. Despite the substantial efforts over recent years, state-of-the-art polygonal building segmentation methods still suffer from several limitations, e.g., (1) relying on a perfect segmentation map to guarantee the vectorization quality; (2) requiring a complex post-processing procedure; (3) generating inaccurate vertices with a fixed quantity, a wrong sequential order, self-intersections, etc. To tackle the above issues, in this paper, we propose a polygonal building segmentation approach and make the following contributions: (1) We design a multi-task segmentation network for joint semantic and geometric learning via three tasks, i.e., pixel-wise building segmentation, multi-class corner prediction, and edge orientation prediction. (2) We propose a simple but effective vertex generation module for transforming the segmentation contour into high-quality polygon vertices. (3) We further propose a polygon refinement network that automatically moves the polygon vertices into more accurate locations. Results on two popular building segmentation datasets demonstrate that our approach achieves significant improvements for both building instance segmentation (with 2% F1-score gain) and polygon vertex prediction (with 6% F1-score gain) compared with current state-of-the-art methods."
  },
  "aaai2021_main_generalizedzero-shotlearningviadisentangledrepresentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Generalized Zero-Shot Learning via Disentangled Representation",
    "authors": [
      "Xiangyu Li",
      "Zhe Xu",
      "Kun Wei",
      "Cheng Deng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16292",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16292/16099",
    "published": "2021-02",
    "summary": "Zero-Shot Learning (ZSL) aims to recognize images belonging to unseen classes that are unavailable in the training process, while Generalized Zero-Shot Learning (GZSL) is a more realistic variant that both seen and unseen classes appear during testing. Most GZSL approaches achieve knowledge transfer based on the features of samples that inevitably contain information irrelevant to recognition, bringing negative influence for the performance. In this work, we propose a novel method, dubbed Disentangled-VAE, which aims to disentangle category-distilling factors and category-dispersing factors from visual as well as semantic features, respectively. In addition, a batch re-combining strategy on latent features is introduced to guide the disentanglement, encouraging the distilling latent features to be more discriminative for recognition. Extensive experiments demonstrate that our method outperforms the state-of-the-art approaches on four challenging benchmark datasets"
  },
  "aaai2021_main_learningomni-frequencyregion-adaptiverepresentationsforrealimagesuper-resolution": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Omni-Frequency Region-adaptive Representations for Real Image Super-Resolution",
    "authors": [
      "Xin Li",
      "Xin Jin",
      "Tao Yu",
      "Simeng Sun",
      "Yingxue Pang",
      "Zhizheng Zhang",
      "Zhibo\n      Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16293",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16293/16100",
    "published": "2021-02",
    "summary": "Traditional single image super-resolution (SISR) methods that focus on solving single and uniform degradation (i.e., bicubic down-sampling), typically suffer from poor performance when applied into real-world low-resolution (LR) images due to the complicated realistic degradations. The key to solving this more challenging real image super-resolution (RealSR) problem lies in learning feature representations that are both informative and content-aware. In this paper, we propose a Omni-frequency Region-adaptive Network (OR-Net) to address both challenges, here we call features of all low, middle and high frequencies omni-frequency features. Specifically, we start from the frequency perspective and design a Frequency Decomposition (FD) module to separate different frequency components to comprehensively compensate the information lost for real LR image. Then, considering the different regions of real LR image have different frequency information lost, we further design a Region-adaptive Frequency Aggregation (RFA) module by leveraging dynamic convolution and spatial attention to adaptively restore frequency components for different regions. The extensive experiments endorse the high-efficient, effective, and scenario-agnostic nature of our OR-Net for RealSR."
  },
  "aaai2021_main_group-wisesemanticminingforweaklysupervisedsemanticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation",
    "authors": [
      "Xueyi Li",
      "Tianfei Zhou",
      "Jianwu Li",
      "Yi Zhou",
      "Zhaoxiang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16294",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16294/16101",
    "published": "2021-02",
    "summary": "Acquiring sufficient ground-truth supervision to train deep vi- sual models has been a bottleneck over the years due to the data-hungry nature of deep learning. This is exacerbated in some structured prediction tasks, such as semantic segmen- tation, which requires pixel-level annotations. This work ad- dresses weakly supervised semantic segmentation (WSSS), with the goal of bridging the gap between image-level anno- tations and pixel-level segmentation. We formulate WSSS as a novel group-wise learning task that explicitly models se- mantic dependencies in a group of images to estimate more reliable pseudo ground-truths, which can be used for training more accurate segmentation models. In particular, we devise a graph neural network (GNN) for group-wise semantic min- ing, wherein input images are represented as graph nodes, and the underlying relations between a pair of images are char- acterized by an efficient co-attention mechanism. Moreover, in order to prevent the model from paying excessive atten- tion to common semantics only, we further propose a graph dropout layer, encouraging the model to learn more accurate and complete object responses. The whole network is end-to- end trainable by iterative message passing, which propagates interaction cues over the images to progressively improve the performance. We conduct experiments on the popular PAS- CAL VOC 2012 and COCO benchmarks, and our model yields state-of-the-art performance. Our code is available at: https://github.com/Lixy1997/Group-WSSS."
  },
  "aaai2021_main_inferencefusionwithassociativesemanticsforunseenobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Inference Fusion with Associative Semantics for Unseen Object Detection",
    "authors": [
      "Yanan Li",
      "Pengyang Li",
      "Han Cui",
      "Donghui Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16295",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16295/16102",
    "published": "2021-02",
    "summary": "We study the problem of object detection when training and test objects are disjoint, i.e. no training examples of the target classes are available. Existing unseen object detection approaches usually combine generic detection frameworks with a single-path unseen classifier, by aligning object regions with semantic class embeddings. In this paper, inspired from human cognitive experience, we propose a simple but effective dual-path detection model that further explores associative semantics to supplement the basic visual-semantic knowledge transfer. We use a novel target-centric multiple-association strategy to establish concept associations, to ensure that the predictor generalized to unseen domain can be learned during training. In this way, through a reasonable inference fusion mechanism, those two parallel reasoning paths can strengthen the correlation between seen and unseen objects, thus improving detection performance. Experiments show that our inductive method can significantly boost the performance by 7.42% over inductive models, and even 5.25% over transductive models on MSCOCO dataset."
  },
  "aaai2021_main_deepunsupervisedimagehashingbymaximizingbitentropy": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Unsupervised Image Hashing by Maximizing Bit Entropy",
    "authors": [
      "Yunqiang Li",
      "Jan van Gemert"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16296",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16296/16103",
    "published": "2021-02",
    "summary": "Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-Half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy, we do not add a term to the loss function as this is difficult to optimize and tune. Instead, we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets FLICKR25K, NUS-WIDE, CIFAR-10, MS COCO, MNIST and the video datasets UCF-101 and HMDB-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art."
  },
  "aaai2021_main_sequentialend-to-endnetworkforefficientpersonsearch": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Sequential End-to-end Network for Efficient Person Search",
    "authors": [
      "Zhengjia Li",
      "Duoqian Miao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16297",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16297/16104",
    "published": "2021-02",
    "summary": "Person search aims at jointly solving Person Detection and Person Re-identification (re-ID). Existing works have designed end-to-end networks based on Faster R-CNN. However, due to the parallel structure of Faster R-CNN, the extracted features come from the low-quality proposals generated by the Region Proposal Network, rather than the detected high-quality bounding boxes. Person search is a fine-grained task and such inferior features will significantly reduce re-ID performance. To address this issue, we propose a Sequential End-to-end Network (SeqNet) to extract superior features. In SeqNet, detection and re-ID are considered as a progressive process and tackled with two sub-networks sequentially. In addition, we design a robust Context Bipartite Graph Matching (CBGM) algorithm to effectively employ context information as an important complementary cue for person matching. Extensive experiments on two widely used person search benchmarks, CUHK-SYSU and PRW, have shown that our method achieves state-of-the-art results. Also, our model runs at 11.5 fps on a single GPU and can be integrated into the existing end-to-end framework easily."
  },
  "aaai2021_main_sd-posesemanticdecompositionforcross-domain6dobjectposeestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SD-Pose: Semantic Decomposition for Cross-Domain 6D Object Pose Estimation",
    "authors": [
      "Zhigang Li",
      "Yinlin Hu",
      "Mathieu Salzmann",
      "Xiangyang Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16298",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16298/16105",
    "published": "2021-02",
    "summary": "The current leading 6D object pose estimation methods rely heavily on annotated real data, which is highly costly to acquire. To overcome this, many works have proposed to introduce computer-generated synthetic data. However, bridging the gap between the synthetic and real data remains a severe problem. Images depicting different levels of realism/semantics usually have different transferability between the synthetic and real domains. Inspired by this observation, we introduce an approach, SD-Pose, that explicitly decomposes the input image into multi-level semantic representations and then combines the merits of each representation to bridge the domain gap. Our comprehensive analyses and experiments show that our semantic decomposition strategy can fully utilize the different domain similarities of different representations, thus allowing us to outperform the state of the art on modern 6D object pose datasets without accessing any real data during training."
  },
  "aaai2021_main_temporalpyramidnetworkforpedestriantrajectorypredictionwithmulti-supervision": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Temporal Pyramid Network for Pedestrian Trajectory Prediction with Multi-Supervision",
    "authors": [
      "Rongqin Liang",
      "Yuanman Li",
      "Xia Li",
      "Yi Tang",
      "Jiantao Zhou",
      "Wenbin Zou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16299",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16299/16106",
    "published": "2021-02",
    "summary": "Predicting human motion behavior in a crowd is important for many applications, ranging from the natural navigation of autonomous vehicles to intelligent security systems of video surveillance. All the previous works model and predict the trajectory with a single resolution, which is relatively ineffective and difficult to simultaneously exploit the long-range information (e.g., the destination of the trajectory), and the short-range information (e.g., the walking direction and speed at a certain time) of the motion behavior. In this paper, we propose a temporal pyramid network for pedestrian trajectory prediction through a squeeze modulation and a dilation modulation. Our hierarchical framework builds a feature pyramid with increasingly richer temporal information from top to bottom, which can better capture the motion behavior at various tempos. Furthermore, we propose a coarse-to-fine fusion strategy with multi-supervision. By progressively merging the top coarse features of global context to the bottom fine features of rich local context, our method can fully exploit both the long-range and short-range information of the trajectory.Experimental results on two benchmarks demonstrate the superiority of our method.Our code and models will be available upon acceptance."
  },
  "aaai2021_main_query-memoryre-aggregationforweakly-supervisedvideoobjectsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Query-Memory Re-Aggregation for Weakly-supervised Video Object Segmentation",
    "authors": [
      "Fanchao Lin",
      "Hongtao Xie",
      "Yan Li",
      "Yongdong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16300",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16300/16107",
    "published": "2021-02",
    "summary": "Weakly-supervised video object segmentation (WVOS) is an emerging video task that can track and segment the target given a simple bounding box label. However, existing WVOS methods are still unsatisfied in either speed or accuracy, since they only use the exemplar frame to guide the prediction while they neglect the reference from other frames. To solve the problem, we propose a novel Re-Aggregation based framework, which uses feature matching to efficiently find the target and capture the temporal dependencies from multiple frames to guide the segmentation. Based on a two-stage structure, our framework builds an information-symmetric matching process to achieve robust aggregation. In each stage, we design a Query-Memory Aggregation (QMA) module to gather features from the past frames and make bidirectional aggregation to adaptively weight the aggregated features, which relieves the latent misguidance in unidirectional aggregation. To further exploit the information from different aggregation stages, we propose a novel coarse-fine constraint by using the Cascaded Refinement Module (CRM) to combine the predictions from different stages and further boosts the performance. Experimental results on three benchmarks show that our method achieves the state-of-the-art performance in WVOS (e.g., an overall score of 84.7% on the DAVIS 2016 validation set)."
  },
  "aaai2021_main_augmentedpartialmutuallearningwithframemaskingforvideocaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Augmented Partial Mutual Learning with Frame Masking for Video Captioning",
    "authors": [
      "Ke Lin",
      "Zhuoxin Gan",
      "Liwei Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16301",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16301/16108",
    "published": "2021-02",
    "summary": "Recent video captioning work improves greatly due to the invention of various elaborate model architectures. If multiple captioning models are combined into a unified framework not only by simple more ensemble, and each model can benefit from each other, the final captioning might be boosted further. Jointly training of multiple model have not been explored in previous works. In this paper, we propose a novel Augmented Partial Mutual Learning (APML) training method where multiple decoders are trained jointly with mimicry losses between different decoders and different input variations. Another problem of training captioning model is the \"one-to-many\" mapping problem which means that one identical video input is mapped to multiple caption annotations.To address this problem, we propose an annotation-wise frame masking approach to convert the \"one-to-many\" mapping to \"one-to-one\" mapping. The experiments performed on MSR-VTT and MSVD datasets demonstrate our proposed algorithm achieves the state-of-the-art performance."
  },
  "aaai2021_main_exploitingaudio-visualconsistencywithpartialsupervisionforspatialaudiogeneration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Exploiting Audio-Visual Consistency with Partial Supervision for Spatial Audio Generation",
    "authors": [
      "Yan-Bo Lin",
      "Yu-Chiang Frank Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16302",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16302/16109",
    "published": "2021-02",
    "summary": "Human perceives rich auditory experience with distinct sound heard by ears. Videos recorded with binaural audio particular simulate how human receives ambient sound. However, a large number of videos are with monaural audio only, which would degrade the user experience due to the lack of ambient information. To address this issue, we propose an audio spatialization framework to convert a monaural video into a binaural one exploiting the relationship across audio and visual components. By preserving the left-right consistency in both audio and visual modalities, our learning strategy can be viewed as a self-supervised learning technique, and alleviates the dependency on a large amount of video data with ground truth binaural audio data during training. Experiments on benchmark datasets confirm the effectiveness of our proposed framework in both semi-supervised and fully supervised scenarios, with ablation studies and visualization further support the use of our model for audio spatialization."
  },
  "aaai2021_main_singleviewpointcloudgenerationviaunified3dprototype": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Single View Point Cloud Generation via Unified 3D Prototype",
    "authors": [
      "Yu Lin",
      "Yigong Wang",
      "Yi-Fan Li",
      "Zhuoyi Wang",
      "Yang Gao",
      "Latifur Khan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16303",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16303/16110",
    "published": "2021-02",
    "summary": "As 3D point clouds become the representation of choice for multiple vision and graphics applications, such as autonomous driving, robotics, etc., the generation of them by deep neural networks has attracted increasing attention in the research community. Despite the recent success of deep learning models in classification and segmentation, synthesizing point clouds remains challenging, especially from a single image. State-of-the-art (SOTA) approaches can generate a point cloud from a hidden vector, however, they treat 2D and 3D features equally and disregard the rich shape information within the 3D data. In this paper, we address this problem by integrating image features with 3D prototype features. Specifically, we propose to learn a set of 3D prototype features from a real point cloud dataset and dynamically adjust them through the training. These prototypes are then integrated with incoming image features to guide the point cloud generation process. Experimental results show that our proposed method outperforms SOTA methods on single image based 3D reconstruction tasks."
  },
  "aaai2021_main_self-supervisedsketch-to-imagesynthesis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Self-Supervised Sketch-to-Image Synthesis",
    "authors": [
      "Bingchen Liu",
      "Yizhe Zhu",
      "Kunpeng Song",
      "Ahmed Elgammal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16304",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16304/16111",
    "published": "2021-02",
    "summary": "Imagining a colored realistic image from an arbitrary-drawn sketch is one of human capabilities that we eager machines to mimic.Unlike previous methods that either require the sketch-image pairs or utilize low-quantity detected edges as sketches, we study the exemplar-based sketch-to-image (s2i) synthesis task in a self-supervised learning manner, eliminating the necessity of the paired sketch data. To this end,we first propose an unsupervised method to efficiently synthesize line-sketches for general RGB-only datasets. With the synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to decouple the content/style features from sketches and RGB-images, and synthesize images both content-faithful to the sketches and style-consistent to the RGB-images. While prior works employ either the cycle-consistence loss or dedicated attentional modules to enforce the content/style fidelity, we show AE's superior performance with pure self-supervisions.To further improve the synthesis quality in high resolution, we also leverage an adversarial network to refine the details of synthetic images. Extensive experiments on $1024^2$ resolution demonstrate a new state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art datasets. Moreover, with the proposed sketch generator, the model shows a promising performance on style mixing and style transfer, which the synthesized images are not only style-consistent but also semantically meaningful."
  },
  "aaai2021_main_timetextandimagemutual-translationadversarialnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "TIME: Text and Image Mutual-Translation Adversarial Networks",
    "authors": [
      "Bingchen Liu",
      "Kunpeng Song",
      "Yizhe Zhu",
      "Gerard de Melo",
      "Ahmed Elgammal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16305",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16305/16112",
    "published": "2021-02",
    "summary": "Focusing on text-to-image (T2I) generation, we propose Text and Image Mutual-Translation Adversarial Networks (TIME), a lightweight but effective model that jointly learns a T2I generator G and an image captioning discriminator D under the Generative Adversarial Network framework. While previous methods tackle the T2I problem as a uni-directional task and use pre-trained language models to enforce the image--text consistency, TIME requires neither extra modules nor pre-training. We show that the performance of G can be boosted substantially by training it jointly with D as a language model. Specifically, we adopt Transformers to model the cross-modal connections between the image features and word embeddings, and design an annealing conditional hinge loss that dynamically balances the adversarial learning. In our experiments, TIME achieves state-of-the-art (SOTA) performance on the CUB dataset (Inception Score of 4.91 and Fr\u00e9chet Inception Distance of 14.3 on CUB), and shows promising performance on MS-COCO dataset on image captioning and downstream vision-language tasks."
  },
  "aaai2021_main_sa-bnnstate-awarebinaryneuralnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SA-BNN: State-Aware Binary Neural Network",
    "authors": [
      "Chunlei Liu",
      "Peng Chen",
      "Bohan Zhuang",
      "Chunhua Shen",
      "Baochang Zhang",
      "Wenrui\n      Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16306",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16306/16113",
    "published": "2021-02",
    "summary": "Binary Neural Networks (BNNs) have received significant attention due to the memory and computation efficiency recently. However, the considerable accuracy gap between BNNs and their full-precision counterparts hinders BNNs to be deployed to resource-constrained platforms. One of the main reasons for the performance gap can be attributed to the frequent weight flip, which is caused by the misleading weight update in BNNs. To address this issue, we propose a state-aware binary neural network (SA-BNN) equipped with the well designed state-aware gradient. Our SA-BNN is inspired by the observation that the frequent weight flip is more likely to occur, when the gradient magnitude for all quantization states {-1,1} is identical. Accordingly, we propose to employ independent gradient coefficients for different states when updating the weights. Furthermore, we also analyze the effectiveness of the state-aware gradient on suppressing the frequent weight flip problem. Experiments on ImageNet show that the proposed SA-BNN outperforms the current state-of-the-arts (e.g., Bi-Real Net) by more than 3% when using a ResNet architecture. Specifically, we achieve 61.7%, 65.5% and 68.7% Top-1 accuracy with ResNet-18, ResNet-34 and ResNet-50 on ImageNet, respectively."
  },
  "aaai2021_main_spatiotemporalgraphneuralnetworkbasedmaskreconstructionforvideoobjectsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation",
    "authors": [
      "Daizong Liu",
      "Shuangjie Xu",
      "Xiao-Yang Liu",
      "Zichuan Xu",
      "Wei Wei",
      "Pan Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16307",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16307/16114",
    "published": "2021-02",
    "summary": "This paper addresses the task of segmenting class-agnostic objects in semi-supervised setting. Although previous detection based methods achieve relatively good performance, these approaches extract the best proposal by a greedy strategy, which may lose the local patch details outside the chosen candidate. In this paper, we propose a novel spatiotemporal graph neural network (STG-Net) to reconstruct more accurate masks for video object segmentation, which captures the local contexts by utilizing all proposals. In the spatial graph, we treat object proposals of a frame as nodes and represent their correlations with an edge weight strategy for mask context aggregation. To capture temporal information from previous frames, we use a memory network to refine the mask of current frame by retrieving historic masks in a temporal graph. The joint use of both local patch details and temporal relationships allow us to better address the challenges such as object occlusions and missing. Without online learning and fine-tuning, our STG-Net achieves state-of-the-art performance on four large benchmarks, demonstrating the effectiveness of the proposed approach."
  },
  "aaai2021_main_f2netlearningtofocusontheforegroundforunsupervisedvideoobjectsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "F2Net: Learning to Focus on the Foreground for Unsupervised Video Object Segmentation",
    "authors": [
      "Daizong Liu",
      "Dongdong Yu",
      "Changhu Wang",
      "Pan Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16308",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16308/16115",
    "published": "2021-02",
    "summary": "Although deep learning based methods have achieved great progress in unsupervised video object segmentation, difficult scenarios (e.g., visual similarity, occlusions, and appearance changing) are still no well-handled. To alleviate these issues, we propose a novel Focus on Foreground Network(F2Net), which delves into the intra-inter frame details for the foreground objects and thus effectively improve the segmentation performance. Specifically, our proposed network consists of three main parts: Siamese Encoder Module, Center Guiding Appearance Diffusion Module, and Dynamic Information Fusion Module. Firstly, we take a siamese encoder to extract the feature representations of paired frames (reference frame and current frame). Then, a Center Guiding Appearance Diffusion Module is designed to capture the inter-frame feature (dense correspondences between reference frame and current frame), intra-frame feature (dense correspondences in current frame), and original semantic feature of current frame. Different from the Anchor Diffusion Network, we establish a Center Prediction Branch to predict the center location of the foreground object in current frame and leverage the center point information as spatial guidance prior to enhance the inter-frame and intra-frame feature extraction, and thus the feature representation considerably focus on the foreground objects. Finally, we propose a Dynamic Information Fusion Module to automatically select relatively important features through three aforementioned different level features. Extensive experiments on DAVIS, Youtube-object, and FBMS datasets show that our proposed F2Net achieves the state-of-the-art performance with significant improvement."
  },
  "aaai2021_main_towardrealisticvirtualtry-onthroughlandmarkguidedshapematching": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Toward Realistic Virtual Try-on Through Landmark Guided Shape Matching",
    "authors": [
      "Guoqiang Liu",
      "Dan Song",
      "Ruofeng Tong",
      "Min Tang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16309",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16309/16116",
    "published": "2021-02",
    "summary": "Image-based virtual try-on aims to synthesize the customer image with an in-shop clothes image to acquire seamless and natural try-on results, which have attracted increasing attentions. The main procedures of image-based virtual try-on usually consist of clothes image generation and try-on image synthesis, whereas prior arts cannot guarantee satisfying clothes results when facing large geometric changes and complex clothes patterns, which further deteriorates the afterwards try-on results. To address this issue, we propose a novel virtual try-on network based on landmark-guided shape matching (LM-VTON). Specifically, the clothes image generation progressively learns the warped clothes and refined clothes in an end-to-end manner, where we introduce a landmark-based constraint in Thin-Plate Spline (TPS) warping to inject finer deformation constraints around the clothes. The try-on process synthesizes the warped clothes with personal characteristics via a semantic indicator. Qualitative and quantitative experiments on two public datasets validate the superiority of the proposed method, especially for challenging cases such as large geometric changes and complex clothes patterns. Code will be available at https://github.com/lgqfhwy/LM-VTON."
  },
  "aaai2021_main_largemotionvideosuper-resolutionwithdualsubnetandmulti-stagecommunicatedupsampling": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage Communicated Upsampling",
    "authors": [
      "Hongying Liu",
      "Peng Zhao",
      "Zhubo Ruan",
      "Fanhua Shang",
      "Yuanyuan Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16310",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16310/16117",
    "published": "2021-02",
    "summary": "Video super-resolution (VSR) aims at restoring a video in low-resolution (LR) and improving it to higher-resolution (HR). Due to the characteristics of video tasks, it is very important that motion information among frames should be well concerned, summarized and utilized for guidance in a VSR algorithm. Especially, when a video contains large motion, conventional methods easily bring incoherent results or artifacts. In this paper, we propose a novel deep neural network with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for super-resolution of videos with large motion. We design a new module named U-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit motion estimation and motion compensation (MEMC) as well as coarse spatial feature extraction. And we present a new Multi-Stage Communicated Upsampling (MSCU) module to make full use of the intermediate results of upsampling for guiding the VSR. Moreover, a novel dual subnet is devised to aid the training of our DSMC, whose dual loss helps to reduce the solution space as well as enhance the generalization ability. Our experimental results confirm that our method achieves superior performance on videos with large motion compared to state-of-the-art methods."
  },
  "aaai2021_main_fcfr-netfeaturefusionbasedcoarse-to-fineresiduallearningfordepthcompletion": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "FCFR-Net: Feature Fusion based Coarse-to-Fine Residual Learning for Depth Completion",
    "authors": [
      "Lina Liu",
      "Xibin Song",
      "Xiaoyang Lyu",
      "Junwei Diao",
      "Mengmeng Wang",
      "Yong Liu",
      "Liangjun Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16311",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16311/16118",
    "published": "2021-02",
    "summary": "Depth completion aims to recover a dense depth map from a sparse depth map with the corresponding color image as input. Recent approaches mainly formulate the depth completion as a one-stage end-to-end learning task, which outputs dense depth maps directly. However, the feature extraction and supervision in one-stage frameworks are insufficient, limiting the performance of these approaches. To address this problem, we propose a novel end-to-end residual learning framework, which formulates the depth completion as a two-stage learning task, i.e., a sparse-to-coarse stage and a coarse-to-fine stage. First, a coarse dense depth map is obtained by a simple CNN framework. Then, a refined depth map is further obtained using a residual learning strategy in the coarse-to-fine stage with coarse depth map and color image as input. Specially, in the coarse-to-fine stage, a channel shuffle extraction operation is utilized to extract more representative features from color image and coarse depth map, and an energy based fusion operation is exploited to effectively fuse these features obtained by channel shuffle operation, thus leading to more accurate and refined depth maps. We achieve SoTA performance in RMSE on KITTI benchmark. Extensive experiments on other datasets future demonstrate the superiority of our approach over current state-of-the-art depth completion approaches."
  },
  "aaai2021_main_activityimage-to-videoretrievalbydisentanglingappearanceandmotion": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Activity Image-to-Video Retrieval by Disentangling Appearance and Motion",
    "authors": [
      "Liu Liu",
      "Jiangtong Li",
      "Li Niu",
      "Ruicong Xu",
      "Liqing Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16312",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16312/16119",
    "published": "2021-02",
    "summary": "With the rapid emergence of video data, image-to-video retrieval has attracted much attention. There are two types of image-to-video retrieval: instance-based and activity-based. The former task aims to retrieve videos containing the same main objects as the query image, while the latter focuses on finding the similar activity. Since dynamic information plays a significant role in the video, we pay attention to the latter task to explore the motion relation between images and videos. In this paper, we propose a Motion-assisted Activity Proposal-based Image-to-Video Retrieval (MAP-IVR) approach to disentangle the video features into motion features and appearance features and obtain appearance features from the images. Then, we perform image-to-video translation to improve the disentanglement quality. The retrieval is performed in both appearance and video feature spaces. Extensive experiments demonstrate that our MAP-IVR approach remarkably outperforms the state-of-the-art approaches on two benchmark activity-based video datasets."
  },
  "aaai2021_main_adaptivepattern-parametermatchingforrobustpedestriandetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection",
    "authors": [
      "Mengyin Liu",
      "Chao Zhu",
      "Jun Wang",
      "Xu-Cheng Yin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16313",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16313/16120",
    "published": "2021-02",
    "summary": "Pedestrians with challenging patterns, e.g. small scale or heavy occlusion, appear frequently in practical applications like autonomous driving, which remains tremendous obstacle to higher robustness of detectors. Although plenty of previous works have been dedicated to these problems, properly matching patterns of pedestrian and parameters of detector, i.e., constructing a detector with proper parameter sizes for certain pedestrian patterns of different complexity, has been seldom investigated intensively. Pedestrian instances are usually handled equally with the same amount of parameters, which in our opinion is inadequate for those with more difficult patterns and leads to unsatisfactory performance. Thus, we propose in this paper a novel detection approach via adaptive pattern-parameter matching. The input pedestrian patterns, especially the complex ones, are first disentangled into simpler patterns for detection head by Pattern Disentangling Module (PDM) with various receptive fields. Then, Gating Feature Filtering Module (GFFM) dynamically decides the spatial positions where the patterns are still not simple enough and need further disentanglement by the next-level PDM. Cooperating with these two key components, our approach can adaptively select the best matched parameter size for the input patterns according to their complexity. Moreover, to further explore the relationship between parameter sizes and their performance on the corresponding patterns, two parameter selection policies are designed: 1) extending parameter size to maximum, aiming at more difficult patterns for different occlusion types; 2) specializing parameter size by group division, aiming at complex patterns for scale variations. Extensive experiments on two popular benchmarks, Caltech and CityPersons, show that our proposed method achieves superior performance compared with other state-of-the-art methods on subsets of different scales and occlusion types."
  },
  "aaai2021_main_temporalsegmentationoffine-gainedsemanticactionamotion-centeredfigureskatingdataset": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Temporal Segmentation of Fine-gained Semantic Action: A Motion-Centered Figure Skating Dataset",
    "authors": [
      "Shenglan Liu",
      "Aibin Zhang",
      "Yunheng Li",
      "Jian Zhou",
      "Li Xu",
      "Zhuben Dong",
      "Renhao Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16314",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16314/16121",
    "published": "2021-02",
    "summary": "Temporal Action Segmentation (TAS) has achieved great success in many fields such as exercise rehabilitation, movie editing, etc. Currently, task-driven TAS is a central topic in human action analysis. However, motion-centered TAS, as an important topic, is little researched due to unavailable datasets. In order to explore more models and practical applications of motion-centered TAS, we introduce a Motion-Centered Figure Skating (MCFS) dataset in this paper. Compared with existing temporal action segmentation datasets, the MCFS dataset is fine-grained semantic, specialized and motion-centered. Besides, RGB-based and Skeleton-based features are provided in the MCFS dataset. Experimental results show that existing state-of-the-art methods are difficult to achieve excellent segmentation results (including accuracy, edit and F1 score) in the MCFS dataset. This indicates that MCFS is a challenging dataset for motion-centered TAS. The latest dataset can be downloaded at https://shenglanliu.github.io/mcfs-dataset/."
  },
  "aaai2021_main_learninghybridrelationshipsforpersonre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Hybrid Relationships for Person Re-identification",
    "authors": [
      "Shuang Liu",
      "Wenmin Huang",
      "Zhong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16315",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16315/16122",
    "published": "2021-02",
    "summary": "Recently, the relationship among individual pedestrian images and the relationship among pairwise pedestrian images have become attractive for person re-identification (re-ID) as they effectively improve the ability of feature representation. In this paper, we propose a novel method named Hybrid Relationship Network (HRNet) to learn the two types of relationships in a unified framework that makes use of their own advantages. Specifically, for the relationship among individual pedestrian images, we take the features of pedestrian images as the nodes to construct a locally-connected graph, so as to improve the discriminative ability of nodes. Meanwhile, we propose the consistent node constraint to inject the identity information into the graph learning process and guide the information to propagate accurately. As for the relationship among pairwise pedestrian images, we treat the feature differences of pedestrian images as the nodes to construct a fully-connected graph so as to estimate robust similarity of nodes. Furthermore, we propose the inter-graph propagation to alleviate the information loss for the fully-connected graph. Extensive experiments on Market-1501, DukeMTMCreID, CUHK03 and MSMT17 demonstrate that the proposed HRNet outperforms the state-of-the-art methods."
  },
  "aaai2021_main_translatethefacialregionsyoulikeusingself-adaptiveregiontranslation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Translate the Facial Regions You Like Using Self-Adaptive Region Translation",
    "authors": [
      "Wenshuang Liu",
      "Wenting Chen",
      "Zhanjia Yang",
      "Linlin Shen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16316",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16316/16123",
    "published": "2021-02",
    "summary": "With the progression of Generative Adversarial Networks (GANs), image translation methods has achieved increasingly remarkable performance. However, most available methods can only achieve image level translation, which is unable to precisely control the regions to be translated. In this paper, we propose a novel self-adaptive region translation network (SART) for region-level translation, which uses region-adaptive instance normalization (RIN) and a region matching loss (RML) for this task. We first encode the style and content image for each region with style and content encoder. To translate both shape and texture of the target region, we inject region-adaptive style features into the decoder by RIN. To ensure independent translation among different regions, RML is proposed to measure the similarity between the non-translated/translated regions of content and translated images. Extensive experiments on three publicly available datasets, i.e. Morph, RaFD and CelebAMask-HQ, suggest that our approach demonstrate obvious improvement over state-of-the-art methods like StarGAN, SEAN and FUNIT. Our approach has further advantages in precise control of the regions to be translated. As a result, region level expression changes and step-by-step make-up can be achieved. The video demo is available at (https://youtu.be/DvIdmcR2LEc)."
  },
  "aaai2021_main_subtype-awareunsuperviseddomainadaptationformedicaldiagnosis": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Subtype-aware Unsupervised Domain Adaptation for Medical Diagnosis",
    "authors": [
      "Xiaofeng Liu",
      "Xiongchang Liu",
      "Bo Hu",
      "Wenxuan Ji",
      "Fangxu Xing",
      "Jun Lu",
      "Jane\n      You",
      "C.-C. Jay Kuo",
      "Georges El Fakhri",
      "Jonghye Woo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16317",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16317/16124",
    "published": "2021-02",
    "summary": "Recent advances in unsupervised domain adaptation (UDA) show that transferable prototypical learning presents a powerful means for class conditional alignment, which encourages the closeness of cross-domain class centroids. However, the cross-domain inner-class compactness and the underlying fine-grained subtype structure remained largely underexplored. In this work, we propose to adaptively carry out the fine-grained subtype-aware alignment by explicitly enforcing the class-wise separation and subtype-wise compactness with intermediate pseudo labels. Our key insight is that the unlabeled subtypes of a class can be divergent to one another with different conditional and label shifts, while inheriting the local proximity within a subtype. The cases with or without the prior information on subtype numbers are investigated to discover the underlying subtype structure in an online fashion. The proposed subtype-aware dynamic UDA achieves promising results on a medical diagnosis task."
  },
  "aaai2021_main_fontrlchinesefontsynthesisviadeepreinforcementlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "FontRL: Chinese Font Synthesis via Deep Reinforcement Learning",
    "authors": [
      "Yitian Liu",
      "Zhouhui Lian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16318",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16318/16125",
    "published": "2021-02",
    "summary": "Automatic generation of Chinese fonts is a valuable but challenging task in areas of AI and Computer Graphics, mainly due to the huge amount of Chinese characters and their complex glyph structures. In this paper, we propose FontRL, a novel method for Chinese font synthesis by using deep reinforcement learning. Specifically, we first train a deep reinforcement learning model to obtain the Thin-Plate Spline (TPS) transformation that is able to modify the reference stroke skeleton in a mean font style into the skeleton of a required style for each stroke of every unseen Chinese character. Afterwards, we utilize a CNN model to predict the location and scale information of these strokes, and then assemble them to get the skeleton of the corresponding character. Finally, we convert each synthesized character skeleton into the glyph image via an image-to-image translation model. Both quantitative and qualitative experimental results demonstrate the superiority of the proposed FontRL compared to the state of the art. Our code is available at https://github.com/lsflyt-pku/FontRL."
  },
  "aaai2021_main_hierarchicalinformationpassingbasednoise-toleranthybridlearningforsemi-supervisedhumanparsing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Hierarchical Information Passing Based Noise-Tolerant Hybrid Learning for Semi-Supervised Human Parsing",
    "authors": [
      "Yunan Liu",
      "Shanshan Zhang",
      "Jian Yang",
      "PongChi Yuen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16319",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16319/16126",
    "published": "2021-02",
    "summary": "Deep learning based human parsing methods usually require a large amount of training data to reach high performance. However, it is costly and time-consuming to obtain manually annotated high quality labels for a large scale dataset. To alleviate annotation efforts, we propose a new semi-supervised human parsing method for which we only need a small number of labels for training.First, we generate high quality pseudo labels on unlabeled images using a hierarchical information passing network (HIPN), which reasons human part segmentation in a coarse to fine manner.Furthermore, we develop a noise-tolerant hybrid learning method, which takes advantage of positive and negative learning to better handle noisy pseudo labels.When evaluated on standard human parsing benchmarks, our HIPN achieves a new state-of-the-artperformance. Moreover, our noise-tolerant hybrid learning method further improves the performance and outperforms the state-of-the-art semi-supervised method (i.e. GRN) by 4.47 points w.r.t mIoU on the LIP dataset."
  },
  "aaai2021_main_delvingintovariancetransmissionandnormalizationshiftofaveragegradientmakesthenetworkcollapse": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Delving into Variance Transmission and Normalization: Shift of Average Gradient Makes the Network Collapse",
    "authors": [
      "Yuxiang Liu",
      "Jidong Ge",
      "Chuanyi Li",
      "Jie Gui"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16320",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16320/16127",
    "published": "2021-02",
    "summary": "Normalization operations are essential for state-of-the-art neural networks and enable us to train a network from scratch with a large learning rate (LR). We attempt to explain the real effect of Batch Normalization (BN) from the perspective of variance transmission by investigating the relationship between BN and Weights Normalization (WN). In this work, we demonstrate that the problem of the shift of the average gradient will amplify the variance of every convolutional (conv) layer. We propose Parametric Weights Standardization (PWS), a fast and robust to mini-batch size module used for conv filters, to solve the shift of the average gradient. PWS can provide the speed-up of BN. Besides, it has less computation and does not change the output of a conv layer. PWS enables the network to converge fast without normalizing the outputs. This result enhances the persuasiveness of the shift of the average gradient and explains why BN works from the perspective of variance transmission. The code and appendix will be made available on https://github.com/lyxzzz/PWSConv."
  },
  "aaai2021_main_aggregatedmulti-gansforcontrolled3dhumanmotionprediction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Aggregated Multi-GANs for Controlled 3D Human Motion Prediction",
    "authors": [
      "Zhenguang Liu",
      "Kedi Lyu",
      "Shuang Wu",
      "Haipeng Chen",
      "Yanbin Hao",
      "Shouling Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16321",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16321/16128",
    "published": "2021-02",
    "summary": "Human motion prediction from historical pose sequence is at the core of many applications in machine intelligence. However, in current state-of-the-art methods, the predicted future motion is confined within the same activity. One can neither generate predictions that differ from the current activity, nor manipulate the body parts to explore various future possibilities. Undoubtedly, this greatly limits the usefulness and applicability of motion prediction. In this paper, we propose a generalization of the human motion prediction task in which control parameters can be readily incorporated to adjust the forecasted motion. Our method is compelling in that it enables manipulable motion prediction across activity types and allows customization of the human movement in a variety of fine-grained ways. To this aim, a simple yet effective composite GAN structure, consisting of local GANs for different body parts and aggregated via a global GAN is presented. The local GANs game in lower dimensions, while the global GAN adjusts in high dimensional space to avoid mode collapse. Extensive experiments show that our method outperforms state-of-the-art. The codes are available at https://github.com/herolvkd/AM-GAN."
  },
  "aaai2021_main_acsnetaction-contextseparationnetworkforweaklysupervisedtemporalactionlocalization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "ACSNet: Action-Context Separation Network for Weakly Supervised Temporal Action Localization",
    "authors": [
      "Ziyi Liu",
      "Le Wang",
      "Qilin Zhang",
      "Wei Tang",
      "Junsong Yuan",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16322",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16322/16129",
    "published": "2021-02",
    "summary": "The object of Weakly-supervised Temporal Action Localization (WS-TAL) is to localize all action instances in an untrimmed video with only video-level supervision. Due to the lack of frame-level annotations during training, current WS-TAL methods rely on attention mechanisms to localize the foreground snippets or frames that contribute to the video-level classification task. This strategy frequently confuse context with the actual action, in the localization result. Separating action and context is a core problem for precise WS-TAL, but it is very challenging and has been largely ignored in the literature. In this paper, we introduce an Action-Context Separation Network (ACSNet) that explicitly takes into account context for accurate action localization. It consists of two branches (i.e., the Foreground-Background branch and the Action-Context branch). The Foreground-Background branch first distinguishes foreground from background within the entire video while the Action-Context branch further separates the foreground as action and context. We associate video snippets with two latent components (i.e., a positive component and a negative component), and their different combinations can effectively characterize foreground, action and context. Furthermore, we introduce extended labels with auxiliary context categories to facilitate the learning of action-context separation. Experiments on THUMOS14 and ActivityNet v1.2/v1.3 datasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL methods by a large margin."
  },
  "aaai2021_main_weaklysupervisedtemporalactionlocalizationthroughlearningexplicitsubspacesforactionandcontext": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Weakly Supervised Temporal Action Localization Through Learning Explicit Subspaces for Action and Context",
    "authors": [
      "Ziyi Liu",
      "Le Wang",
      "Wei Tang",
      "Junsong Yuan",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16323",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16323/16130",
    "published": "2021-02",
    "summary": "Weakly-supervised Temporal Action Localization (WS-TAL) methods learn to localize temporal starts and ends of action instances in a video under only video-level supervision. Existing WS-TAL methods rely on deep features learned for action recognition. However, due to the mismatch between classification and localization, these features cannot distinguish the frequently co-occurring contextual background, i.e., the context, and the actual action instances. We term this challenge action-context confusion, and it will adversely affect the action localization accuracy. To address this challenge, we introduce a framework that learns two feature subspaces respectively for actions and their context. By explicitly accounting for action visual elements, the action instances can be localized more precisely without the distraction from the context. To facilitate the learning of these two feature subspaces with only video-level categorical labels, we leverage the predictions from both spatial and temporal streams for snippets grouping. In addition, an unsupervised learning task is introduced to make the proposed module focus on mining temporal information. The proposed approach outperforms state-of-the-art WS-TAL methods on three benchmarks, i.e., THUMOS14, ActivityNet v1.2 and v1.3 datasets."
  },
  "aaai2021_main_pointinetpointcloudframeinterpolationnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PointINet: Point Cloud Frame Interpolation Network",
    "authors": [
      "Fan Lu",
      "Guang Chen",
      "Sanqing Qu",
      "Zhijun Li",
      "Yinlong Liu",
      "Alois Knoll"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16324",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16324/16131",
    "published": "2021-02",
    "summary": "LiDAR point cloud streams are usually sparse in time dimension, which is limited by hardware performance. Generally, the frame rates ofmechanical LiDAR sensors are 10 to 20 Hz, which is much lower than other commonly used sensors like cameras. To overcome the temporal limitations of LiDAR sensors, a novel task named Point Cloud Frame Interpolation is studied in this paper. Given two consecutive point cloud frames, Point Cloud Frame Interpolation aims to generate intermediate frame(s) between them. To achieve that, we propose a novel framework, namely Point Cloud Frame Interpolation Network (PointINet). Based on the proposed method, the low frame rate point cloud streams can be upsampled to higher frame rates. We start by estimating bi-directional 3D scene flow between the two point clouds and then warp them to the given time step based on the 3D scene flow. To fuse the two warped frames and generate intermediate point cloud(s), we propose a novel learning-based points fusion module, which simultaneously takes two warped point clouds into consideration. We design both quantitative and qualitative experiments to evaluate the performance of the point cloud frame interpolation method and extensive experiments on two large scale outdoor LiDAR datasets demonstrate the effectiveness of the proposed PointINet. Our code is available at https://github.com/ispc-lab/PointINet.git."
  },
  "aaai2021_main_aglobalocclusion-awareapproachtoself-supervisedmonocularvisualodometry": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Global Occlusion-Aware Approach to Self-Supervised Monocular Visual Odometry",
    "authors": [
      "Yao Lu",
      "Xiaoli Xu",
      "Mingyu Ding",
      "Zhiwu Lu",
      "Tao Xiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16325",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16325/16132",
    "published": "2021-02",
    "summary": "Self-Supervised monocular visual odometry (VO) is often cast into a view synthesis problem based on depth and camera pose estimation. One of the key challenges is to accurately and robustly estimate depth with occlusions and moving objects in the scene. Existing methods simply detect and mask out regions of occlusions locally by several convolutional layers, and then perform only partial view synthesis in the rest of the image. However, occlusion and moving object detection is an unsolved problem itself which requires global layout information. Inaccurate detection inevitably results in incorrect depth as well as pose estimation. In this work, instead of locally detecting and masking out occlusions and moving objects, we propose to alleviate their negative effects on monocular VO implicitly but more effectively from two global perspectives. First, a multi-scale non-local attention module, consisting of both intra-stage augmented attention and cascaded across-stage attention, is proposed for robust depth estimation given occlusions, alleviating the impacts of occlusions via global attention modeling. Second, adversarial learning is introduced in view synthesis for monocular VO. Unlike existing methods that use pixel-level losses on the quality of synthesized views, we enforce the synthetic view to be indistinguishable from the real one at the scene-level. Such a global constraint again helps cope with occluded and moving regions. Extensive experiments on the KITTI dataset show that our approach achieves new state-of-the-art in both pose estimation and depth recovery."
  },
  "aaai2021_main_pc-hmrposecalibrationfor3dhumanmeshrecoveryfrom2dimages/videos": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos",
    "authors": [
      "Tianyu Luan",
      "Yali Wang",
      "Junhao Zhang",
      "Zhe Wang",
      "Zhipeng Zhou",
      "Yu Qiao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16326",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16326/16133",
    "published": "2021-02",
    "summary": "The end-to-end Human Mesh Recovery (HMR) approach has been successfully used for 3D body reconstruction. However, most HMR-based frameworks reconstruct human body by directly learning mesh parameters from images or videos, while lacking explicit guidance of 3D human pose in visual data. As a result, the generated mesh often exhibits incorrect pose for complex activities. To tackle this problem, we propose to exploit 3D pose to calibrate human mesh. Specifically, we develop two novel Pose Calibration frameworks, i.e., Serial PC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in a serial or parallel manner, these two frameworks can effectively correct human mesh with guidance of a concise pose calibration module. Furthermore, since the calibration module is designed via non-rigid pose transformation, our PC-HMR frameworks can flexibly tackle bone length variations to alleviate misplacement in the calibrated mesh. Finally, our frameworks are based on generic and complementary integration of data-driven learning and geometrical modeling. Via plug-and-play modules, they can be efficiently adapted for both image/video-based human mesh recovery. Additionally, they have no requirement of extra 3D pose annotations in the testing phase, which releases inference difficulties in practice. We perform extensive experiments on the popular benchmarks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks achieve the SOTA results."
  },
  "aaai2021_main_deepdtlearninggeometryfromdelaunaytriangulationforsurfacereconstruction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DeepDT: Learning Geometry From Delaunay Triangulation for Surface Reconstruction",
    "authors": [
      "Yiming Luo",
      "Zhenxing Mi",
      "Wenbing Tao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16327",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16327/16134",
    "published": "2021-02",
    "summary": "In this paper, a novel learning-based network, named DeepDT, is proposed to reconstruct the surface from Delaunay triangulation of point cloud. DeepDT learns to predict inside/outside labels of Delaunay tetrahedrons directly from a point cloud and corresponding Delaunay triangulation. The local geometry features are first extracted from the input point cloud and aggregated into a graph deriving from the Delaunay triangulation. Then a graph filtering is applied on the aggregated features in order to add structural regularization to the label prediction of tetrahedrons. Due to the complicated spatial relations between tetrahedrons and the triangles, it is impossible to directly generate ground truth labels of tetrahedrons from ground truth surface. Therefore, we propose a multi-label supervision strategy which votes for the label of a tetrahedron with labels of sampling locations inside it. The proposed DeepDT can maintain abundant geometry details without generating overly complex surfaces, especially for inner surfaces of open scenes. Meanwhile, the generalization ability and time consumption of the proposed method is acceptable and competitive compared with the state-of-the-art methods. Experiments demonstrate the superior performance of the proposed DeepDT."
  },
  "aaai2021_main_dual-levelcollaborativetransformerforimagecaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dual-level Collaborative Transformer for Image Captioning",
    "authors": [
      "Yunpeng Luo",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Liujuan Cao",
      "Yongjian Wu",
      "Feiyue\n      Huang",
      "Chia-Wen Lin",
      "Rongrong Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16328",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16328/16135",
    "published": "2021-02",
    "summary": "Descriptive region features extracted by object detection networks have played an important role in the recent advancements of image captioning. However, they are still criticized for the lack of contextual information and fine-grained details, which in contrast are the merits of traditional grid features. In this paper, we introduce a novel Dual-Level Collaborative Transformer (DLCT) network to realize the complementary advantages of the two features. Concretely, in DLCT, these two features are first processed by a novel Dual-way Self Attenion (DWSA) to mine their intrinsic properties, where a Comprehensive Relation Attention component is also introduced to embed the geometric information. In addition, we propose a Locality-Constrained Cross Attention module to address the semantic noises caused by the direct fusion of these two features, where a geometric alignment graph is constructed to accurately align and reinforce region and grid features. To validate our model, we conduct extensive experiments on the highly competitive MS-COCO dataset, and achieve new state-of-the-art performance on both local and online test sets, i.e., 133.8% CIDEr on Karpathy split and 135.4% CIDEr on the official split."
  },
  "aaai2021_main_hr-depthhighresolutionself-supervisedmonoculardepthestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "HR-Depth: High Resolution Self-Supervised Monocular Depth Estimation",
    "authors": [
      "Xiaoyang Lyu",
      "Liang Liu",
      "Mengmeng Wang",
      "Xin Kong",
      "Lina Liu",
      "Yong Liu",
      "Xinxin Chen",
      "Yi Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16329",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16329/16136",
    "published": "2021-02",
    "summary": "Self-supervised learning shows great potential in monocular depth estimation, using image sequences as the only source of supervision. Although people try to use the high-resolution imagefordepthestimation,theaccuracyofpredictionhas notbeensignificantlyimproved.Inthiswork,wefindthe corereasoncomesfromtheinaccuratedepthestimationin large gradient regions, making the bilinear interpolation error gradually disappear as the resolution increases. To obtain more accurate depth estimation in large gradient regions, it isnecessarytoobtainhigh-resolutionfeatureswithspatial and semantic information. Therefore, we present an improved DepthNet,HR-Depth,withtwoeffectivestrategies:(1)re-designtheskip-connectioninDepthNettogetbetterhigh-resolution features and (2) propose feature fusion Squeeze-and-Excitation(fSE) module to fuse feature more efficiently. Using Resnet-18 as the encoder, HR-Depth surpasses all previousstate-of-the-art(SoTA)methodswiththeleastparametersatbothhighandlowresolution.Moreover,previous SoTA methods are based on fairly complex and deep networks with a mass of parameters which limits their real applications. Thus we also construct a lightweight network which uses MobileNetV3 as encoder. Experiments show that the lightweight network can perform on par with many large models like Monodepth2 at high-resolution with only20%parameters. All codes and models will be available at https://github.com/shawLyu/HR-Depth."
  },
  "aaai2021_main_smilmultimodallearningwithseverelymissingmodality": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SMIL: Multimodal Learning with Severely Missing Modality",
    "authors": [
      "Mengmeng Ma",
      "Jian Ren",
      "Long Zhao",
      "Sergey Tulyakov",
      "Cathy Wu",
      "Xi Peng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16330",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16330/16137",
    "published": "2021-02",
    "summary": "A common assumption in multimodal learning is the completeness of training data, i.e., full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., ninety percent of training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks."
  },
  "aaai2021_main_pyramidalfeatureshrinkingforsalientobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Pyramidal Feature Shrinking for Salient Object Detection",
    "authors": [
      "Mingcan Ma",
      "Changqun Xia",
      "Jia Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16331",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16331/16138",
    "published": "2021-02",
    "summary": "Recently, we have witnessed the great progress of salient object detection (SOD),which benefits from the effectiveness of various feature aggregation strategies. However, existing methods usually aggregate the low-level features containing details and the high-level features containing semantics over a large span, which introduces noise into the aggregated features and generate inaccurate saliency map. To address this issue, we propose pyramidal feature shrinking network (PFSNet), which aims to aggregate adjacent feature nodes in pairs with layer-by-layer shrinkage, so that the aggregated features fuse effective details and semantics together and discard interference information. Specifically, pyramidal shrinking decoder (PSD) is proposed to aggregate adjacent features hierarchically in an asymptotic manner. Unlike other methods that aggregate features with significantly different information, this method only focuses on adjacent feature nodes in each layer and shrinks them to a final unique feature node. Besides, we propose adjacent fusion module (AFM) to perform mutual spatial enhancement between the adjacent features so as to dynamically weight the features and adaptively fuse the appropriate information. In addition, scale-aware enrichment module (SEM) based on the features extracted from backbone is utilized to obtain rich scale information and generate diverse initial features with dilated convolutions. Extensive quantitative and qualitative experiments demonstrate that the proposed intuitive framework outperforms 14 state-of-the-art approaches on 5 public datasets."
  },
  "aaai2021_main_learningtocountviaunbalancedoptimaltransport": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning to Count via Unbalanced Optimal Transport",
    "authors": [
      "Zhiheng Ma",
      "Xing Wei",
      "Xiaopeng Hong",
      "Hui Lin",
      "Yunfeng Qiu",
      "Yihong Gong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16332",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16332/16139",
    "published": "2021-02",
    "summary": "Counting dense crowds through computer vision technology has attracted widespread attention. Most crowd counting datasets use point annotations. In this paper, we formulate crowd counting as a measure regression problem to minimize the distance between two measures with different supports and unequal total mass. Specifically, we adopt the unbalanced optimal transport distance, which remains stable under spatial perturbations, to quantify the discrepancy between predicted density maps and point annotations. An efficient optimization algorithm based on the regularized semi-dual formulation of UOT is introduced, which alternatively learns the optimal transportation and optimizes the density regressor. The quantitative and qualitative results illustrate that our method achieves state-of-the-art counting and localization performance."
  },
  "aaai2021_main_scenegraphembeddingsusingrelativesimilaritysupervision": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Scene Graph Embeddings Using Relative Similarity Supervision",
    "authors": [
      "Paridhi Maheshwari",
      "Ritwick Chaudhry",
      "Vishwa Vinay"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16333",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16333/16140",
    "published": "2021-02",
    "summary": "Scene graphs are a powerful structured representation of the underlying content of images, and embeddings derived from them have been shown to be useful in multiple downstream tasks. In this work, we employ a graph convolutional network to exploit structure in scene graphs and produce image embeddings useful for semantic image retrieval. Different from classification-centric supervision traditionally available for learning image representations, we address the task of learning from relative similarity labels in a ranking context. Rooted within the contrastive learning paradigm, we propose a novel loss function that operates on pairs of similar and dissimilar images and imposes relative ordering between them in embedding space. We demonstrate that this Ranking loss, coupled with an intuitive triple sampling strategy, leads to robust representations that outperform well-known contrastive losses on the retrieval task. In addition, we provide qualitative evidence of how retrieved results that utilize structured scene information capture the global context of the scene, different from visual similarity search."
  },
  "aaai2021_main_few-shotlifelonglearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Few-Shot Lifelong Learning",
    "authors": [
      "Pratik Mazumder",
      "Pravendra Singh",
      "Piyush Rai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16334",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16334/16141",
    "published": "2021-02",
    "summary": "Many real-world classification problems often have classes with very few labeled training samples. Moreover, all possible classes may not be initially available for training, and may be given incrementally. Deep learning models need to deal with this two-fold problem in order to perform well in real-life situations. In this paper, we propose a novel Few-Shot Lifelong Learning (FSLL) method that enables deep learning models to perform lifelong/continual learning on few-shot data. Our method selects very few parameters from the model for training every new set of classes instead of training the full model. This helps in preventing overfitting. We choose the few parameters from the model in such a way that only the currently unimportant parameters get selected. By keeping the important parameters in the model intact, our approach minimizes catastrophic forgetting. Furthermore, we minimize the cosine similarity between the new and the old class prototypes in order to maximize their separation, thereby improving the classification performance. We also show that integrating our method with self-supervision improves the model performance significantly. We experimentally show that our method significantly outperforms existing methods on the miniImageNet, CIFAR-100, and CUB-200 datasets. Specifically, we outperform the state-of-the-art method by an absolute margin of 19.27% for the CUB dataset."
  },
  "aaai2021_main_carpeposterumaconvolutionalapproachforreal-timepedestrianpathprediction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "CARPe Posterum: A Convolutional Approach for Real-Time Pedestrian Path Prediction",
    "authors": [
      "Matias Mendieta",
      "Hamed Tabkhi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16335",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16335/16142",
    "published": "2021-02",
    "summary": "Pedestrian path prediction is an essential topic in computer vision and video understanding. Having insight into the movement of pedestrians is crucial for ensuring safe operation in a variety of applications including autonomous vehicles, social robots, and environmental monitoring. Current works in this area utilize complex generative or recurrent methods to capture many possible futures. However, despite the inherent real-time nature of predicting future paths, little work has been done to explore accurate and computationally efficient approaches for this task. To this end, we propose a convolutional approach for real-time pedestrian path prediction, CARPe. It utilizes a variation of Graph Isomorphism Networks in combination with an agile convolutional neural network design to form a fast and accurate path prediction approach. Notable results in both inference speed and prediction accuracy are achieved, improving FPS considerably in comparison to current state-of-the-art methods while delivering competitive accuracy on well-known path prediction datasets."
  },
  "aaai2021_main_dynamicanchorlearningforarbitrary-orientedobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dynamic Anchor Learning for Arbitrary-Oriented Object Detection",
    "authors": [
      "Qi Ming",
      "Zhiqiang Zhou",
      "Lingjuan Miao",
      "Hongwei Zhang",
      "Linhao Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16336",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16336/16143",
    "published": "2021-02",
    "summary": "Arbitrary-oriented objects widely appear in natural scenes, aerial photographs, remote sensing images, etc., and thus arbitrary-oriented object detection has received considerable attention. Many current rotation detectors use plenty of anchors with different orientations to achieve spatial alignment with ground truth boxes. Intersection-over-Union (IoU) is then applied to sample the positive and negative candidates for training. However, we observe that the selected positive anchors cannot always ensure accurate detections after regression, while some negative samples can achieve accurate localization. It indicates that the quality assessment of anchors through IoU is not appropriate, and this further leads to inconsistency between classification confidence and localization accuracy. In this paper, we propose a dynamic anchor learning (DAL) method, which utilizes the newly defined matching degree to comprehensively evaluate the localization potential of the anchors and carries out a more efficient label assignment process. In this way, the detector can dynamically select high-quality anchors to achieve accurate object detection, and the divergence between classification and regression will be alleviated. With the newly introduced DAL, we can achieve superior detection performance for arbitrary-oriented objects with only a few horizontal preset anchors. Experimental results on three remote sensing datasets HRSC2016, DOTA, UCAS-AOD as well as a scene text dataset ICDAR 2015 show that our method achieves substantial improvement compared with the baseline model. Besides, our approach is also universal for object detection using horizontal bound box. The code and models are available at https://github.com/ming71/DAL."
  },
  "aaai2021_main_terrace-basedfoodcountingandsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Terrace-based Food Counting and Segmentation",
    "authors": [
      "Huu-Thanh Nguyen",
      "Chong-Wah Ngo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16337",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16337/16144",
    "published": "2021-02",
    "summary": "This paper represents object instance as a terrace, where the height of terrace corresponds to object attention while the evolution of layers from peak to sea level represents the complexity in drawing the finer boundary of an object. A multitask neural network is presented to learn the terrace representation. The attention of terrace is leveraged for instance counting, and the layers provide prior for easy-to-hard pathway of progressive instance segmentation. We study the model for counting and segmentation for a variety of food instances, ranging from Chinese, Japanese to Western food. This paper presents how the terrace model deals with arbitrary shape, size, obscure boundary and occlusion of instances, where other techniques are currently short of."
  },
  "aaai2021_main_embodiedvisualactivelearningforsemanticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Embodied Visual Active Learning for Semantic Segmentation",
    "authors": [
      "David Nilsson",
      "Aleksis Pirinen",
      "Erik G\u00e4rtner",
      "Cristian Sminchisescu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16338",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16338/16145",
    "published": "2021-02",
    "summary": "We study the task of embodied visual active learning, where an agent is set to explore a 3d environment with the goal to acquire visual scene understanding by actively selecting views for which to request annotation. While accurate on some benchmarks, today's deep visual recognition pipelines tend to not generalize well in certain real-world scenarios, or for unusual viewpoints. Robotic perception, in turn, requires the capability to refine the recognition capabilities for the conditions where the mobile system operates, including cluttered indoor environments or poor illumination. This motivates the proposed task, where an agent is placed in a novel environment with the objective of improving its visual recognition capability. To study embodied visual active learning, we develop a battery of agents - both learnt and pre-specified - and with different levels of knowledge of the environment. The agents are equipped with a semantic segmentation network and seek to acquire informative views, move and explore in order to propagate annotations in the neighbourhood of those views, then refine the underlying segmentation network by online retraining. The trainable method uses deep reinforcement learning with a reward function that balances two competing objectives: task performance, represented as visual recognition accuracy, which requires exploring the environment, and the necessary amount of annotated data requested during active exploration. We extensively evaluate the proposed models using the photorealistic Matterport3D simulator and show that a fully learnt method outperforms comparable pre-specified counterparts, even when requesting fewer annotations."
  },
  "aaai2021_main_tdaftop-downattentionframeworkforvisiontasks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "TDAF: Top-Down Attention Framework for Vision Tasks",
    "authors": [
      "Bo Pang",
      "Yizhuo Li",
      "Jiefeng Li",
      "Muchen Li",
      "Hanwen Cao",
      "Cewu Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16339",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16339/16146",
    "published": "2021-02",
    "summary": "Human attention mechanisms often work in a top-down manner, yet it is not well explored in vision research. Here, we propose the Top-Down Attention Framework (TDAF) to capture top-down attentions, which can be easily adopted in most existing models. The designed Recursive Dual-Directional Nested Structure in it forms two sets of orthogonal paths, recursive and structural ones, where bottom-up spatial features and top-down attention features are extracted respectively. Such spatial and attention features are nested deeply, therefore, the proposed framework works in a mixed top-down and bottom-up manner. Empirical evidence shows that our TDAF can capture effective stratified attention information and boost performance. ResNet with TDAF achieves 2.0% improvements on ImageNet. For object detection, the performance is improved by 2.7% AP over FCOS. For pose estimation, TDAF improves the baseline by 1.6%. And for action recognition, the 3D-ResNet adopting TDAF achieves improvements of 1.7% accuracy."
  },
  "aaai2021_main_few-shotfontgenerationwithlocalizedstylerepresentationsandfactorization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Few-shot Font Generation with Localized Style Representations and Factorization",
    "authors": [
      "Song Park",
      "Sanghyuk Chun",
      "Junbum Cha",
      "Bado Lee",
      "Hyunjung Shim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16340",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16340/16147",
    "published": "2021-02",
    "summary": "Automatic few-shot font generation is a practical and widely studied problem because manual designs are expensive and sensitive to the expertise of designers. Existing few-shot font generation methods aim to learn to disentangle the style and content element from a few reference glyphs, and mainly focus on a universal style representation for each font style. However, such approach limits the model in representing diverse local styles, and thus makes it unsuitable to the most complicated letter system, e.g., Chinese, whose characters consist of a varying number of components (often called ``radical'') with a highly complex structure. In this paper, we propose a novel font generation method by learning localized styles, namely component-wise style representations, instead of universal styles. The proposed style representations enable us to synthesize complex local details in text designs. However, learning component-wise styles solely from reference glyphs is infeasible in the few-shot font generation scenario, when a target script has a large number of components, e.g., over 200 for Chinese. To reduce the number of reference glyphs, we simplify component-wise styles by a product of component factor and style factor, inspired by low-rank matrix factorization. Thanks to the combination of strong representation and a compact factorization strategy, our method shows remarkably better few-shot font generation results (with only 8 reference glyph images) than other state-of-the-arts, without utilizing strong locality supervision, e.g., location of each component, skeleton, or strokes. The source code is available at https://github.com/clovaai/lffont."
  },
  "aaai2021_main_learningdisentangledrepresentationforfairfacialattributeclassificationviafairness-awareinformationalignment": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Disentangled Representation for Fair Facial Attribute Classification via Fairness-aware Information Alignment",
    "authors": [
      "Sungho Park",
      "Sunhee Hwang",
      "Dohyung Kim",
      "Hyeran Byun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16341",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16341/16148",
    "published": "2021-02",
    "summary": "Although AI systems archive a great success in various societal fields, there still exists a challengeable issue of outputting discriminatory results with respect to protected attributes (e.g., gender and age). The popular approach to solving the issue is to remove protected attribute information in the decision process. However, this approach has a limitation that beneficial information for target tasks may also be eliminated. To overcome the limitation, we propose Fairness-aware Disentangling Variational Auto-Encoder (FD-VAE) that disentangles data representation into three subspaces: 1) Target Attribute Latent (TAL), 2) Protected Attribute Latent (PAL), 3) Mutual Attribute Latent (MAL). On top of that, we propose a decorrelation loss that aligns the overall information into each subspace, instead of removing the protected attribute information. After learning the representation, we re-encode MAL to include only target information and combine it with TAL to perform downstream tasks. In our experiments on CelebA and UTK Face datasets, we show that the proposed method mitigates unfairness in facial attribute classification tasks with respect to gender and age. Ours outperforms previous methods by large margins on two standard fairness metrics, equal opportunity and equalized odds."
  },
  "aaai2021_main_vid-odecontinuous-timevideogenerationwithneuralordinarydifferentialequation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Vid-ODE: Continuous-Time Video Generation with Neural Ordinary Differential Equation",
    "authors": [
      "Sunghyun Park",
      "Kangyeol Kim",
      "Junsoo Lee",
      "Jaegul Choo",
      "Joonseok Lee",
      "Sookyung Kim",
      "Edward Choi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16342",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16342/16149",
    "published": "2021-02",
    "summary": "Video generation models often operate under the assumption of fixed frame rates, which leads to suboptimal performance when it comes to handling flexible frame rates (e.g., increasing the frame rate of the more dynamic portion of the video as well as handling missing video frames). To resolve the restricted nature of existing video generation models' ability to handle arbitrary timesteps, we propose continuous-time video generation by combining neural ODE (Vid-ODE) with pixel-level video processing techniques. Using ODE-ConvGRU as an encoder, a convolutional version of the recently proposed neural ODE, which enables us to learn continuous-time dynamics, Vid-ODE can learn the spatio-temporal dynamics of input videos of flexible frame rates. The decoder integrates the learned dynamics function to synthesize video frames at any given timesteps, where the pixel-level composition technique is used to maintain the sharpness of individual frames. With extensive experiments on four real-world video datasets, we verify that the proposed Vid-ODE outperforms state-of-the-art approaches under various video generation settings, both within the trained time range (interpolation) and beyond the range (extrapolation). To the best of our knowledge, Vid-ODE is the first work successfully performing continuous-time video generation using real-world videos."
  },
  "aaai2021_main_chefcross-modalhierarchicalembeddingsforfooddomainretrieval": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "CHEF: Cross-modal Hierarchical Embeddings for Food Domain Retrieval",
    "authors": [
      "Hai X. Pham",
      "Ricardo Guerrero",
      "Vladimir Pavlovic",
      "Jiatong Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16343",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16343/16150",
    "published": "2021-02",
    "summary": "Despite the abundance of multi-modal data, such as image-text pairs, there has been little effort in understanding the individual entities and their different roles in the construction of these data instances. In this work, we endeavour to discover the entities and their corresponding importance in cooking recipes automatically as a visual-linguistic association problem. More specifically, we introduce a novel cross-modal learning framework to jointly model the latent representations of images and text in the food image-recipe association and retrieval tasks. This model allows one to discover complex functional and hierarchical relationships between images and text, and among textual parts of a recipe including title, ingredients and cooking instructions. Our experiments show that by making use of efficient tree-structured Long Short-Term Memory as the text encoder in our computational cross-modal retrieval framework,we are not only able to identify the main ingredients and cooking actions in the recipe descriptions without explicit supervision, but we can also learn more meaningful feature representations of food recipes, appropriate for challenging cross-modal retrieval and recipe adaption tasks."
  },
  "aaai2021_main_explainablemodelswithconsistentinterpretations": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Explainable Models with Consistent Interpretations",
    "authors": [
      "Vipin Pillai",
      "Hamed Pirsiavash"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16344",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16344/16151",
    "published": "2021-02",
    "summary": "Given the widespread deployment of black box deep neural networks in computer vision applications, the interpretability aspect of these black box systems has recently gained traction. Various methods have been proposed to explain the results of such deep neural networks. However, some recent works have shown that such explanation methods are biased and do not produce consistent interpretations. Hence, rather than introducing a novel explanation method, we learn models that are encouraged to be interpretable given an explanation method. We use Grad-CAM as the explanation algorithm and encourage the network to learn consistent interpretations along with maximizing the log-likelihood of the correct class. We show that our method outperforms the baseline on the pointing game evaluation on ImageNet and MS-COCO datasets respectively. We also introduce new evaluation metrics that penalize the saliency map if it lies outside the ground truth bounding box or segmentation mask, and show that our method outperforms the baseline on these metrics as well. Moreover, our model trained with interpretation consistency generalizes to other explanation algorithms on all the evaluation metrics. The code and models are publicly available."
  },
  "aaai2021_main_dualadversarialgraphneuralnetworksformulti-labelcross-modalretrieval": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval",
    "authors": [
      "Shengsheng Qian",
      "Dizhan Xue",
      "Huaiwen Zhang",
      "Quan Fang",
      "Changsheng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16345",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16345/16152",
    "published": "2021-02",
    "summary": "Cross-modal retrieval has become an active study field with the expanding scale of multimodal data. To date, most existing methods transform multimodal data into a common representation space where semantic similarities between items can be directly measured across different modalities. However, these methods typically suffer from following limitations: 1) They usually attempt to bridge the modality gap by designing losses in the common representation space which may not be sufficient to eliminate potential heterogeneity of different modalities in the common space. 2) They typically treat labels as independent individuals and ignore label relationships which are important for constructing semantic links between multimodal data. In this work, we propose a novel Dual Adversarial Graph Neural Networks (DAGNN) composed of the dual generative adversarial networks and the multi-hop graph neural networks, which learn modality-invariant and discriminative common representations for cross-modal retrieval. Firstly, we construct the dual generative adversarial networks to project multimodal data into a common representation space. Secondly, we leverage the multi-hop graph neural networks, in which a layer aggregation mechanism is proposed to exploit multi-hop propagation information, to capture the label correlation dependency and learn inter-dependent classifiers. Comprehensive experiments conducted on two cross-modal retrieval benchmark datasets, NUS-WIDE and MIRFlickr, indicate the superiority of DAGNN."
  },
  "aaai2021_main_kgdetkeypoint-guidedfashiondetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "KGDet: Keypoint-Guided Fashion Detection",
    "authors": [
      "Shenhan Qian",
      "Dongze Lian",
      "Binqiang Zhao",
      "Tong Liu",
      "Bohui Zhu",
      "Hai Li",
      "Shenghua Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16346",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16346/16153",
    "published": "2021-02",
    "summary": "Locating and classifying clothes, usually referred to as clothing detection, is a fundamental task in fashion analysis. Motivated by the strong structural characteristics of clothes, we pursue a detection method enhanced by clothing keypoints, which is a compact and effective representation of structures. To incorporate the keypoint cues into clothing detection, we design a simple yet effective Keypoint-Guided clothing Detector, named KGDet. Such a detector can fully utilize information provided by keypoints with the following two aspects: i) integrating local features around keypoints to benefit both classification and regression; ii) generating accurate bounding boxes from keypoints. To effectively incorporate local features , two alternative modules are proposed. One is a multi-column keypoint-encoding-based feature aggregation module; the other is a keypoint-selection-based feature aggregation module. With either of the above modules as a bridge, a cascade strategy is introduced to refine detection performance progressively. Thanks to the keypoints, our KGDet obtains superior performance on the DeepFashion2 dataset and the FLD dataset with high efficiency."
  },
  "aaai2021_main_learningmodulatedlossforrotatedobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Modulated Loss for Rotated Object Detection",
    "authors": [
      "Wen Qian",
      "Xue Yang",
      "Silong Peng",
      "Junchi Yan",
      "Yue Guo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16347",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16347/16154",
    "published": "2021-02",
    "summary": "Popular rotated detection methods usually use five parameters (coordinates of the central point, width, height, and rotation angle) or eight parameters (coordinates of four vertices) to describe the rotated bounding box and l1 loss as the loss function. In this paper, we argue that the aforementioned integration can cause training instability and performance degeneration. The main reason is the discontinuity of loss which is caused by the contradiction between the definition of the rotated bounding box and the loss function. We refer to the above issues as rotation sensitivity error (RSE) and propose a modulated rotation loss to dismiss the discontinuity of loss. The modulated rotation loss can achieve consistent improvement on the five parameter methods and the eight parameter methods. Experimental results using one stage and two stages detectors demonstrate the effectiveness of our loss. The integrated network achieves competitive performances on several benchmarks including DOTA and UCAS AOD. The code is available at https://github.com/yangxue0827/RotationDetection."
  },
  "aaai2021_main_mangoamaskattentionguidedone-stagescenetextspotter": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "MANGO: A Mask Attention Guided One-Stage Scene Text Spotter",
    "authors": [
      "Liang Qiao",
      "Ying Chen",
      "Zhanzhan Cheng",
      "Yunlu Xu",
      "Yi Niu",
      "Shiliang Pu",
      "Fei\n      Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16348",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16348/16155",
    "published": "2021-02",
    "summary": "Recently end-to-end scene text spotting has become a popular research topic due to its advantages of global optimization and high maintainability in real applications. Most methods attempt to develop various region of interest (RoI) operations to concatenate the detection part and the sequence recognition part into a two-stage text spotting framework. However, in such framework, the recognition part is highly sensitive to the detected results (e.g., the compactness of text contours). To address this problem, in this paper, we propose a novel Mask AttentioN Guided One-stage text spotting framework named MANGO, in which character sequences can be directly recognized without RoI operation. Concretely, a position-aware mask attention module is developed to generate attention weights on each text instance and its characters. It allows different text instances in an image to be allocated on different feature map channels which are further grouped as a batch of instance features. Finally, a lightweight sequence decoder is applied to generate the character sequences. It is worth noting that MANGO inherently adapts to arbitrary-shaped text spotting and can be trained end-to-end with only coarse position information (e.g., rectangular bounding box) and text annotations. Experimental results show that the proposed method achieves competitive and even new state-of-the-art performance on both regular and irregular text spotting benchmarks, i.e., ICDAR 2013, ICDAR 2015, Total-Text, and SCUT-CTW1500."
  },
  "aaai2021_main_refinepredictionfusionnetworkforpanopticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "REFINE: Prediction Fusion Network for Panoptic Segmentation",
    "authors": [
      "Jiawei Ren",
      "Cunjun Yu",
      "Zhongang Cai",
      "Mingyuan Zhang",
      "Chongsong Chen",
      "Haiyu\n      Zhao",
      "Shuai Yi",
      "Hongsheng Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16349",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16349/16156",
    "published": "2021-02",
    "summary": "Panoptic segmentation aims at generating pixel-wise class and instance predictions for each pixel in the input image, which is a challenging task and far more complicated than naively fusing the semantic and instance segmentation results. Prediction fusion is therefore important to achieve accurate panoptic segmentation. In this paper, we present REFINE, pREdiction FusIon NEtwork for panoptic segmentation, to achieve high-quality panoptic segmentation by improving cross-task prediction fusion, and within-task prediction fusion. Our single-model ResNeXt-101 with DCN achieves PQ=51.5 on the COCO dataset, surpassing state-of-the-art performance by a convincing margin and is comparable with ensembled models. Our smaller model with a ResNet-50 backbone achieves PQ=44.9, which is comparable with state-of-the-art methods with larger backbones."
  },
  "aaai2021_main_autolrlayer-wisepruningandauto-tuningoflearningratesinfine-tuningofdeepnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "AutoLR: Layer-wise Pruning and Auto-tuning of Learning Rates in Fine-tuning of Deep Networks",
    "authors": [
      "Youngmin Ro",
      "Jin Young Choi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16350",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16350/16157",
    "published": "2021-02",
    "summary": "Existing fine-tuning methods use a single learning rate over all layers. In this paper, first, we discuss that trends of layer-wise weight variations by fine-tuning using a single learning rate do not match the well-known notion that lower-level layers extract general features and higher-level layers extract specific features. Based on our discussion, we propose an algorithm that improves fine-tuning performance and reduces network complexity through layer-wise pruning and auto-tuning of layer-wise learning rates. The proposed algorithm has verified the effectiveness by achieving state-of-the-art performance on the image retrieval benchmark datasets (CUB-200, Cars-196, Stanford online product, and Inshop). Code is available at https://github.com/youngminPIL/AutoLR."
  },
  "aaai2021_main_dpfpsdynamicandprogressivefilterpruningforcompressingconvolutionalneuralnetworksfromscratch": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "DPFPS: Dynamic and Progressive Filter Pruning for Compressing Convolutional Neural Networks from Scratch",
    "authors": [
      "Xiaofeng Ruan",
      "Yufan Liu",
      "Bing Li",
      "Chunfeng Yuan",
      "Weiming Hu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16351",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16351/16158",
    "published": "2021-02",
    "summary": "Filter pruning is a commonly used method for compressing Convolutional Neural Networks (ConvNets), due to its friendly hardware supporting and flexibility. However, existing methods mostly need a cumbersome procedure, which brings many extra hyper-parameters and training epochs. This is because only using sparsity and pruning stages cannot obtain a satisfying performance. Besides, many works do not consider the difference of pruning ratio across different layers. To overcome these limitations, we propose a novel dynamic and progressive filter pruning (DPFPS) scheme that directly learns a structured sparsity network from Scratch. In particular, DPFPS imposes a new structured sparsity-inducing regularization specifically upon the expected pruning parameters in a dynamic sparsity manner. The dynamic sparsity scheme determines sparsity allocation ratios of different layers and a Taylor series based channel sensitivity criteria is presented to identify the expected pruning parameters. Moreover, we increase the structured sparsity-inducing penalty in a progressive manner. This helps the model to be sparse gradually instead of forcing the model to be sparse at the beginning. Our method solves the pruning ratio based optimization problem by an iterative soft-thresholding algorithm (ISTA) with dynamic sparsity. At the end of the training, we only need to remove the redundant parameters without other stages, such as fine-tuning. Extensive experimental results show that the proposed method is competitive with 11 state-of-the-art methods on both small-scale and large-scale datasets (i.e., CIFAR and ImageNet). Specifically, on ImageNet, we achieve a 44.97% pruning ratio of FLOPs by compressing ResNet-101, even with an increase of 0.12% Top-5 accuracy. Our pruned models and codes are released at https://github.com/taoxvzi/DPFPS."
  },
  "aaai2021_main_efficientcertificationofspatialrobustness": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Efficient Certification of Spatial Robustness",
    "authors": [
      "Anian Ruoss",
      "Maximilian Baader",
      "Mislav Balunovi\u0107",
      "Martin Vechev"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16352",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16352/16159",
    "published": "2021-02",
    "summary": "Recent work has exposed the vulnerability of computer vision models to vector field attacks. Due to the widespread usage of such models in safety-critical applications, it is crucial to quantify their robustness against such spatial transformations. However, existing work only provides empirical robustness quantification against vector field deformations via adversarial attacks, which lack provable guarantees. In this work, we propose novel convex relaxations, enabling us, for the first time, to provide a certificate of robustness against vector field transformations. Our relaxations are model-agnostic and can be leveraged by a wide range of neural network verifiers. Experiments on various network architectures and different datasets demonstrate the effectiveness and scalability of our method."
  },
  "aaai2021_main_semanticgroupingnetworkforvideocaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Semantic Grouping Network for Video Captioning",
    "authors": [
      "Hobin Ryu",
      "Sunghun Kang",
      "Haeyong Kang",
      "Chang D. Yoo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16353",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16353/16160",
    "published": "2021-02",
    "summary": "This paper considers a video caption generating network referred to as Semantic Grouping Network (SGN) that attempts (1) to group video frames with discriminating word phrases of partially decoded caption and then (2) to decode those semantically aligned groups in predicting the next word. As consecutive frames are not likely to provide unique information, prior methods have focused on discarding or merging repetitive information based only on the input video. The SGN learns an algorithm to capture the most discriminating word phrases of the partially decoded caption and a mapping that associates each phrase to the relevant video frames - establishing this mapping allows semantically related frames to be clustered, which reduces redundancy. In contrast to the prior methods, the continuous feedback from decoded words enables the SGN to dynamically update the video representation that adapts to the partially decoded caption. Furthermore, a contrastive attention loss is proposed to facilitate accurate alignment between a word phrase and video frames without manual annotations. The SGN achieves state-of-the-art performances by outperforming runner-up methods by a margin of 2.1%p and 2.4%p in a CIDEr-D score on MSVD and MSR-VTT datasets, respectively. Extensive experiments demonstrate the effectiveness and interpretability of the SGN."
  },
  "aaai2021_main_audio-visuallocalizationbysyntheticacousticimagegeneration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Audio-Visual Localization by Synthetic Acoustic Image Generation",
    "authors": [
      "Valentina Sanguineti",
      "Pietro Morerio",
      "Alessio Del Bue",
      "Vittorio Murino"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16354",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16354/16161",
    "published": "2021-02",
    "summary": "Acoustic images constitute an emergent data modality for multimodal scene understanding. Such images have the peculiarity to distinguish the spectral signature of sounds coming from different directions in space, thus providing richer information than the one derived from mono and binaural microphones. However, acoustic images are typically generated by cumbersome microphone arrays, which are not as widespread as ordinary microphones mounted on optical cameras. To exploit this empowered modality while using standard microphones and cameras we propose to leverage the generation of synthetic acoustic images from common audio-video data for the task of audio-visual localization. The generation of synthetic acoustic images is obtained by a novel deep architecture, based on Variational Autoencoder and U-Net models, which is trained to reconstruct the ground truth spatialized audio data collected by a microphone array, from the associated video and its corresponding monaural audio signal. Namely, the model learns how to mimic what an array of microphones can produce in the same conditions.We assess the quality of the generated synthetic acoustic images on the task of unsupervised sound source localization in a qualitative and quantitative manner, while also considering standard generation metrics. Our model is evaluated by considering both multimodal datasets containing acoustic images, used for the training, and unseen datasets containing just monaural audio signals and RGB frames, showing to reach more accurate localization results as compared to the state of the art."
  },
  "aaai2021_main_enhancedregularizersforattributionalrobustness": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Enhanced Regularizers for Attributional Robustness",
    "authors": [
      "Anindya Sarkar",
      "Anirban Sarkar",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16355",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16355/16162",
    "published": "2021-02",
    "summary": "Deep neural networks are the default choice of learning models for computer vision tasks. Extensive work has been carried out in recent years on explaining deep models for vision tasks such as classification. However, recent work has shown that it is possible for these models to produce substantially different attribution maps even when two very similar images are given to the network, raising serious questions about trustworthiness. To address this issue, we propose a robust attribution training strategy to improve attributional robustness of deep neural networks. Our method carefully analyzes the requirements for attributional robustness and introduces two new regularizers that preserve a model's attribution map during attacks. Our method surpasses state-of-the-art attributional robustness methods by a margin of approximately 3% to 9% in terms of attribution robustness measures on several datasets including MNIST, FMNIST, Flower and GTSRB."
  },
  "aaai2021_main_progressivenetworkgraftingforfew-shotknowledgedistillation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Progressive Network Grafting for Few-Shot Knowledge Distillation",
    "authors": [
      "Chengchao Shen",
      "Xinchao Wang",
      "Youtan Yin",
      "Jie Song",
      "Sihui Luo",
      "Mingli Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16356",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16356/16163",
    "published": "2021-02",
    "summary": "Knowledge distillation has demonstrated encouraging performances in deep model compression. Most existing approaches, however, require massive labeled data to accomplish the knowledge transfer,making the model compression a cumbersome and costly process. In this paper, we investigate the practical few-shot knowledge distillation scenario, where we assume only a few samples without human annotations are available for each category. To this end, we introduce a principled dual-stage distillation scheme tailored for few-shot data. In the first step, we graft the student blocks one by one onto the teacher, and learn the parameters of the grafted block intertwined with those of the other teacher blocks. In the second step, the trained student blocks are progressively connected and then together grafted onto the teacher network, allowing the learned student blocks to adapt themselves to each other and eventually replace the teacher network. Experiments demonstrate that our approach, with only a few unlabeled samples, achieves gratifying results on CIFAR10,CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances are even on par with those of knowledge distillation schemes that utilize the full datasets. The source code is available at https://github.com/zju-vipa/NetGraft."
  },
  "aaai2021_main_social-dpfsociallyacceptabledistributionpredictionoffutures": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Social-DPF: Socially Acceptable Distribution Prediction of Futures",
    "authors": [
      "Xiaodan Shi",
      "Xiaowei Shao",
      "Guangming Wu",
      "Haoran Zhang",
      "Zhiling Guo",
      "Renhe\n      Jiang",
      "Ryosuke Shibasaki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16357",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16357/16164",
    "published": "2021-02",
    "summary": "We consider long-term path forecasting problems in crowds, where future sequence trajectories are generated given a short observation. Recent methods for this problem have focused on modeling social interactions and predicting multi-modal futures. However, it is not easy for machines to successfully consider social interactions, such as avoiding collisions while considering the uncertainty of futures under a highly interactive and dynamic scenario. In this paper, we propose a model that incorporates multiple interacting motion sequences jointly and predicts multi-modal socially acceptable distributions of futures. Specifically, we introduce a new aggregation mechanism for social interactions, which selectively models long-term inter-related dynamics between movements in a shared environment through a message passing mechanism. Moreover, we propose a loss function that not only accesses how accurate the estimated distributions of the futures are but also considers collision avoidance. We further utilize mixture density functions to describe the trajectories and learn the multi-modality of future paths.Extensive experiments over several trajectory prediction benchmarks demonstrate that our method is able to forecast socially acceptable distributions in complex scenarios."
  },
  "aaai2021_main_robustknowledgetransferviahybridforwardontheteacher-studentmodel": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Robust Knowledge Transfer via Hybrid Forward on the Teacher-Student Model",
    "authors": [
      "Liangchen Song",
      "Jialian Wu",
      "Ming Yang",
      "Qian Zhang",
      "Yuan Li",
      "Junsong Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16358",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16358/16165",
    "published": "2021-02",
    "summary": "When adopting deep neural networks for a new vision task, a common practice is to start with fine-tuning some off-the-shelf well-trained network models from the community. Since a new task may require training a different network architecture with new domain data, taking advantage of off-the-shelf models is not trivial and generally requires considerable try-and-error and parameter tuning. In this paper, we denote a well-trained model as a teacher network and a model for the new task as a student network. We aim to ease the efforts of transferring knowledge from the teacher to the student network, robust to the gaps between their network architectures, domain data, and task definitions. Specifically, we propose a hybrid forward scheme in training the teacher-student models, alternately updating layer weights of the student model. The key merit of our hybrid forward scheme is on the dynamical balance between the knowledge transfer loss and task specific loss in training. We demonstrate the effectiveness of our method on a variety of tasks, e.g., model compression, segmentation, and detection, under a variety of knowledge transfer settings."
  },
  "aaai2021_main_attanetattention-augmentednetworkforfastandaccuratesceneparsing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "AttaNet: Attention-Augmented Network for Fast and Accurate Scene Parsing",
    "authors": [
      "Qi Song",
      "Kangfu Mei",
      "Rui Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16359",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16359/16166",
    "published": "2021-02",
    "summary": "Two factors have proven to be very important to the performance of semantic segmentation models: global context and multi-level semantics. However, generating features that capture both factors always leads to high computational complexity, which is problematic in real-time scenarios. In this paper, we propose a new model, called Attention-Augmented Network (AttaNet), to capture both global context and multi-level semantics while keeping the efficiency high. AttaNet consists of two primary modules: Strip Attention Module (SAM) and Attention Fusion Module (AFM). Viewing that in challenging images with low segmentation accuracy, there are a significantly larger amount of vertical strip areas than horizontal ones, SAM utilizes a striping operation to reduce the complexity of encoding global context in the vertical direction drastically while keeping most of contextual information, compared to the non-local approaches. Moreover, AFM follows a cross-level aggregation strategy to limit the computation, and adopts an attention strategy to weight the importance of different levels of features at each pixel when fusing them, obtaining an efficient multi-level representation. We have conducted extensive experiments on two semantic segmentation benchmarks, and our network achieves different levels of speed/accuracy trade-offs on Cityscapes, e.g., 71 FPS/79.9% mIoU, 130 FPS/78.5% mIoU, and 180 FPS/70.1% mIoU, and leading performance on ADE20K as well."
  },
  "aaai2021_main_tochooseortofuse?scaleselectionforcrowdcounting": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "To Choose or to Fuse? Scale Selection for Crowd Counting",
    "authors": [
      "Qingyu Song",
      "Changan Wang",
      "Yabiao Wang",
      "Ying Tai",
      "Chengjie Wang",
      "Jilin Li",
      "Jian Wu",
      "Jiayi Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16360",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16360/16167",
    "published": "2021-02",
    "summary": "In this paper, we address the large scale variation problem in crowd counting by taking full advantage of the multi-scale feature representations in a multi-level network. We implement such an idea by keeping the counting error of a patch as small as possible with a proper feature level selection strategy, since a specific feature level tends to perform better for a certain range of scales. However, without scale annotations, it is sub-optimal and error-prone to manually assign the predictions for heads of different scales to specific feature levels. Therefore, we propose a Scale-Adaptive Selection Network (SASNet), which automatically learns the internal correspondence between the scales and the feature levels. Instead of directly using the predictions from the most appropriate feature level as the final estimation, our SASNet also considers the predictions from other feature levels via weighted average, which helps to mitigate the gap between discrete feature levels and continuous scale variation. Since the heads in a local patch share roughly a same scale, we conduct the adaptive selection strategy in a patch-wise style. However, pixels within a patch contribute different counting errors due to the various difficulty degrees of learning. Thus, we further propose a Pyramid Region Awareness Loss (PRA Loss) to recursively select the most hard sub-regions within a patch until reaching the pixel level. With awareness of whether the parent patch is over-estimated or under-estimated, the fine-grained optimization with the PRA Loss for these region-aware hard pixels helps to alleviate the inconsistency problem between training target and evaluation metric. The state-of-the-art results on four datasets demonstrate the superiority of our approach.The code will be available at: https://github.com/TencentYoutuResearch/CrowdCounting-SASNet."
  },
  "aaai2021_main_imagecaptioningwithcontext-awareauxiliaryguidance": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Image Captioning with Context-Aware Auxiliary Guidance",
    "authors": [
      "Zeliang Song",
      "Xiaofei Zhou",
      "Zhendong Mao",
      "Jianlong Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16361",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16361/16168",
    "published": "2021-02",
    "summary": "Image captioning is a challenging computer vision task, which aims to generate a natural language description of an image. Most recent researches follow the encoder-decoder framework which depends heavily on the previous generated words for the current prediction. Such methods can not effectively take advantage of the future predicted information to learn complete semantics. In this paper, we proposeContext-Aware Auxiliary Guidance (CAAG) mechanism that can guide the captioning model to perceive global contexts. Upon the captioning model, CAAG performs semantic attention that selectively concentrates on useful information of the global predictions to reproduce the current generation. To validate the adaptability of the method, we apply CAAG to three popular captioners and our proposal achieves competitive performance on the challenging Microsoft COCO image captioning benchmark, e.g. 132.2 CIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official online evaluation server."
  },
  "aaai2021_main_unsupervisedmodeladaptationforcontinualsemanticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Unsupervised Model Adaptation for Continual Semantic Segmentation",
    "authors": [
      "Serban Stan",
      "Mohammad Rostami"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16362",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16362/16169",
    "published": "2021-02",
    "summary": "We develop an algorithm for adapting a semantic segmentation model that is trained using a labeled source domain to generalize well in an unlabeled target domain. A similar problem has been studied extensively in the unsupervised domain adaptation (UDA) literature, but existing UDA algorithms require access to both the source domain labeled data and the target domain unlabeled data for training a domain agnostic semantic segmentation model.Relaxing this constraint enables a user to adaptpretrained models to generalize in a target domain, without requiring access to source data. To this end, we learn a prototypical distribution for the source domain in an intermediate embedding space. This distribution encodes the abstract knowledge that is learned from the source domain. We then use this distribution for aligning the target domain distribution with the source domain distribution in the embedding space. We provide theoretical analysis and explain conditions under which our algorithm is effective. Experiments onbenchmark adaptation tasks demonstrate our method achieves competitive performance even compared with joint UDA approaches."
  },
  "aaai2021_main_bsn++complementaryboundaryregressorwithscale-balancedrelationmodelingfortemporalactionproposalgeneration": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation",
    "authors": [
      "Haisheng Su",
      "Weihao Gan",
      "Wei Wu",
      "Yu Qiao",
      "Junjie Yan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16363",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16363/16170",
    "published": "2021-02",
    "summary": "Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task."
  },
  "aaai2021_main_mangaganunpairedphoto-to-mangatranslationbasedonthemethodologyofmangadrawing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "MangaGAN: Unpaired Photo-to-Manga Translation Based on The Methodology of Manga Drawing",
    "authors": [
      "Hao Su",
      "Jianwei Niu",
      "Xuefeng Liu",
      "Qingfeng Li",
      "Jiahe Cui",
      "Ji Wan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16364",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16364/16171",
    "published": "2021-02",
    "summary": "Manga is a world popular comic form originated in Japan, which typically employs black-and-white stroke lines and geometric exaggeration to describe humans' appearances, poses, and actions. In this paper, we propose MangaGAN, the first method based on Generative Adversarial Network (GAN) for unpaired photo-to-manga translation. Inspired by the drawing process of experienced manga artists, MangaGAN generates geometric features and converts each facial region into the manga domain with a tailored multi-GANs architecture. For training MangaGAN, we collect a new data-set from a popular manga work with extensive features. To produce high-quality manga faces, we propose a structural smoothing loss to smooth stroke-lines and avoid noisy pixels, and a similarity preserving module to improve the similarity between domains of photo and manga. Extensive experiments show that MangaGAN can produce high-quality manga faces preserving both the facial similarity and manga style, and outperforms other reference methods."
  },
  "aaai2021_main_mambamulti-levelaggregationviamemorybankforvideoobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "MAMBA: Multi-level Aggregation via Memory Bank for Video Object Detection",
    "authors": [
      "Guanxiong Sun",
      "Yang Hua",
      "Guosheng Hu",
      "Neil Robertson"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16365",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16365/16172",
    "published": "2021-02",
    "summary": "State-of-the-art video object detection methods maintain a memory structure, either a sliding window or a memory queue, to enhance the current frame using attention mechanisms. However, we argue that these memory structures are not efficient or sufficient because of two implied operations: (1) concatenating all features in memory for enhancement, leading to a heavy computational cost; (2) frame-wise memory updating, preventing the memory from capturing more temporal information. In this paper, we propose a multi-level aggregation architecture via memory bank called MAMBA. Specifically, our memory bank employs two novel operations to eliminate disadvantages of existing methods: (1) light-weight key-set construction which can significantly reduce the computational cost; (2) fine-grained feature-wise updating strategy which enables our method to utilize knowledge from the whole video. To better enhance features from complementary levels, i.e., feature maps and proposals, we further propose a generalized enhancement operation (GEO) to aggregate multi-level features in a unified manner. We conduct extensive evaluations on the challenging ImageNetVID dataset. Compared with existing state-of-the-art methods, our method achieves superior performance in terms of both speed and accuracy. More remarkably, MAMBA achieves mAP of 83.7%/84.6% at 12.6/9.1 FPS with ResNet-101."
  },
  "aaai2021_main_deepprobabilisticimaginguncertaintyquantificationandmulti-modalsolutioncharacterizationforcomputationalimaging": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging",
    "authors": [
      "He Sun",
      "Katherine L. Bouman"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16366",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16366/16173",
    "published": "2021-02",
    "summary": "Computational image reconstruction algorithms generally produce a single image without any measure of uncertainty or confidence. Regularized Maximum Likelihood (RML) and feed-forward deep learning approaches for inverse problems typically focus on recovering a point estimate. This is a serious limitation when working with under-determined imaging systems, where it is conceivable that multiple image modes would be consistent with the measured data. Characterizing the space of probable images that explain the observational data is therefore crucial. In this paper, we propose a variational deep probabilistic imaging approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging (DPI) employs an untrained deep generative model to estimate a posterior distribution of an unobserved image. This approach does not require any training data; instead, it optimizes the weights of a neural network to generate image samples that fit a particular measurement dataset. Once the network weights have been learned, the posterior distribution can be efficiently sampled. We demonstrate this approach in the context of interferometric radio imaging, which is used for black hole imaging with the Event Horizon Telescope, and compressed sensing Magnetic Resonance Imaging (MRI)."
  },
  "aaai2021_main_domaingeneralfaceforgerydetectionbylearningtoweight": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Domain General Face Forgery Detection by Learning to Weight",
    "authors": [
      "Ke Sun",
      "Hong Liu",
      "Qixiang Ye",
      "Yue Gao",
      "Jianzhuang Liu",
      "Ling Shao",
      "Rongrong\n      Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16367",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16367/16174",
    "published": "2021-02",
    "summary": "In this paper, we propose a domain-general model, termed learning-to-weight (LTW), that guarantees face detection performance across multiple domains, particularly the target domains that are never seen before. However, various face forgery methods cause complex and biased data distributions, making it challenging to detect fake faces in unseen domains. We argue that different faces contribute differently to a detection model trained on multiple domains, making the model likely to fit domain-specific biases. As such, we propose the LTW approach based on the meta-weight learning algorithm, which configures different weights for face images from different domains. The LTW network can balance the model's generalizability across multiple domains. Then, the meta-optimization calibrates the source domain's gradient enabling more discriminative features to be learned. The detection ability of the network is further improved by introducing an intra-class compact loss. Extensive experiments on several commonly used deepfake datasets to demonstrate the effectiveness of our method in detecting synthetic faces. Code and supplemental material are available at https://github.com/skJack/LTW."
  },
  "aaai2021_main_object-centricimagegenerationfromlayouts": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Object-Centric Image Generation from Layouts",
    "authors": [
      "Tristan Sylvain",
      "Pengchuan Zhang",
      "Yoshua Bengio",
      "R Devon Hjelm",
      "Shikhar\n      Sharma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16368",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16368/16175",
    "published": "2021-02",
    "summary": "We begin with the hypothesis that a model must be able to understand individual objects and relationships between objects in order to generate complex scenes with multiple objects well. Our layout-to-image-generation method, which we call Object-Centric Generative Adversarial Network (or OC-GAN), relies on a novel Scene-Graph Similarity Module (SGSM). The SGSM learns representations of the spatial relationships between objects in the scene, which lead to our model's improved layout-fidelity. We also propose changes to the conditioning mechanism of the generator that enhance its object instance-awareness. Apart from improving image quality, our contributions mitigate two failure modes in previous approaches: (1) spurious objects being generated without corresponding bounding boxes in the layout, and (2) overlapping bounding boxes in the layout leading to merged objects in images. Extensive quantitative evaluation and ablation studies demonstrate the impact of our contributions, with our model outperforming previous state-of-the-art approaches on both the COCO-Stuff and Visual Genome datasets. Finally, we address an important limitation of evaluation metrics used in previous works by introducing SceneFID -- an object-centric adaptation of the popular Fr\u00e9chet Inception Distance metric, that is better suited for multi-object images."
  },
  "aaai2021_main_structure-awarepersonimagegenerationwithposedecompositionandsemanticcorrelation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Structure-aware Person Image Generation with Pose Decomposition and Semantic Correlation",
    "authors": [
      "Jilin Tang",
      "Yi Yuan",
      "Tianjia Shao",
      "Yong Liu",
      "Mengmeng Wang",
      "Kun Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16369",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16369/16176",
    "published": "2021-02",
    "summary": "In this paper we tackle the problem of pose guided person image generation, which aims to transfer a person image from the source pose to a novel target pose while maintaining the source appearance. Given the inefficiency of standard CNNs in handling large spatial transformation, we propose a structure-aware flow based method for high-quality person image generation. Specifically, instead of learning the complex overall pose changes of human body, we decompose the human body into different semantic parts (e.g., head, torso, and legs) and apply different networks to predict the flow fields for these parts separately. Moreover, we carefully design the network modules to effectively capture the local and global semantic correlations of features within and among the human parts respectively. Extensive experimental results show that our method can generate high-quality results under large pose discrepancy and outperforms state-of-the-art methods in both qualitative and quantitative comparisons."
  },
  "aaai2021_main_gradientregularizedcontrastivelearningforcontinualdomainadaptation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Gradient Regularized Contrastive Learning for Continual Domain Adaptation",
    "authors": [
      "Shixiang Tang",
      "Peng Su",
      "Dapeng Chen",
      "Wanli Ouyang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16370",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16370/16177",
    "published": "2021-02",
    "summary": "Human beings can quickly adapt to environmental changes by leveraging learning experience.However, adapting deep neural networks to dynamic environments by machine learning algorithms remains a challenge. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labelled source domain and a sequence of unlabelled target domains. The obstacles in this problem are both domain shift and catastrophic forgetting. We propose Gradient Regularized Contrastive Learning (GRCL) to solve the obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforcing the gradient not to harm the discriminative ability of source features which can, in turn, benefit the adaptation ability of the model to target domains; (2) constraining the gradient not to increase the classification loss on old target domains, which enables the model to preserve the performance on old target domains when adapting to an in-coming target domain. Experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the state-of-the-art."
  },
  "aaai2021_main_adversarialtrainingreducesinformationandimprovestransferability": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Adversarial Training Reduces Information and Improves Transferability",
    "authors": [
      "Matteo Terzi",
      "Alessandro Achille",
      "Marco Maggipinto",
      "Gian Antonio Susto"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16371",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16371/16178",
    "published": "2021-02",
    "summary": "Recent results show that features of adversarially trained networks for classification, in addition to being robust, enable desirable properties such as invertibility.The latter property may seem counter-intuitive as it is widely accepted by the community that classification models should only capture the minimal information (features) required for the task.Motivated by this discrepancy, we investigate the dual relationship between Adversarial Training and Information Theory. We show that the Adversarial Training can improve linear transferability to new tasks, from which arises a new trade-off between transferability of representations and accuracy on the source task. We validate our results employing robust networks trained on CIFAR-10, CIFAR-100 and ImageNet on several datasets.Moreover, we show that Adversarial Training reduces Fisher information of representations about the input and of the weights about the task, and we provide a theoretical argument which explains the invertibility of deterministic networks without violating the principle of minimality. Finally, we leverage our theoretical insights to remarkably improve the quality of reconstructed images through inversion."
  },
  "aaai2021_main_adversarialturingpatternsfromcellularautomata": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Adversarial Turing Patterns from Cellular Automata",
    "authors": [
      "Nurislam Tursynbek",
      "Ilya Vilkoviskiy",
      "Maria Sindeeva",
      "Ivan Oseledets"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16372",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16372/16179",
    "published": "2021-02",
    "summary": "State-of-the-art deep classifiers are intriguingly vulnerable to universal adversarial perturbations: single disturbances of small magnitude that lead to misclassification of most inputs. This phenomena may potentially result in a serious security problem. Despite the extensive research in this area, there is a lack of theoretical understanding of the structure of these perturbations. In image domain, there is a certain visual similarity between patterns, that represent these perturbations, and classical Turing patterns, which appear as a solution of non-linear partial differential equations and are underlying concept of many processes in nature. In this paper, we provide a theoretical bridge between these two different theories, by mapping a simplified algorithm for crafting universal perturbations to (inhomogeneous) cellular automata, the latter is known to generate Turing patterns. Furthermore, we propose to use Turing patterns, generated by cellular automata, as universal perturbations, and experimentally show that they significantly degrade the performance of deep learning models. We found this method to be a fast and efficient way to create a data-agnostic quasi-imperceptible perturbation in the black-box scenario. The source code is available at https://github.com/NurislamT/advTuring."
  },
  "aaai2021_main_artificialdummiesforurbandatasetaugmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Artificial Dummies for Urban Dataset Augmentation",
    "authors": [
      "Anton\u00edn Vobeck\u00fd",
      "David Hurych",
      "Michal U\u0159i\u010d\u00e1\u0159",
      "Patrick P\u00e9rez",
      "Josef Sivic"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16373",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16373/16180",
    "published": "2021-02",
    "summary": "Existing datasets for training pedestrian detectors in images suffer from limited appearance and pose variation. The most challenging scenarios are rarely included because they are too difficult to capture due to safety reasons, or they are very unlikely to happen. The strict safety requirements in assisted and autonomous driving applications call for an extra high detection accuracy also in these rare situations. Having the ability to generate people images in arbitrary poses, with arbitrary appearances and embedded in different background scenes with varying illumination and weather conditions, is a crucial component for the development and testing of such applications. The contributions of this paper are three-fold. First, we describe an augmentation method for the controlled synthesis of urban scenes containing people, thus producing rare or never-seen situations. This is achieved with a data generator (called DummyNet) with disentangled control of the pose, the appearance, and the target background scene. Second, the proposed generator relies on novel network architecture and associated loss that takes into account the segmentation of the foreground person and its composition into the background scene. Finally, we demonstrate that the data generated by our DummyNet improve the performance of several existing person detectors across various datasets as well as in challenging situations, such as night-time conditions, where only a limited amount of training data is available.In the setup with only day-time data available, we improve the night-time detector by 17% log-average miss rate over the detector trained with the day-time data only."
  },
  "aaai2021_main_scnettraininginferencesampleconsistencyforinstancesegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SCNet: Training Inference Sample Consistency for Instance Segmentation",
    "authors": [
      "Thang Vu",
      "Haeyong Kang",
      "Chang D. Yoo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16374",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16374/16181",
    "published": "2021-02",
    "summary": "Cascaded architectures have brought significant performance improvement in object detection and instance segmentation. However, there are lingering issues regarding the disparity in the Intersection-over-Union (IoU) distribution of the samples between training and inference. This disparity can potentially exacerbate detection accuracy. This paper proposes an architecture referred to as Sample Consistency Network (SCNet) to ensure that the IoU distribution of the samples at training time is close to that at inference time. Furthermore, SCNet incorporates feature relay and utilizes global contextual information to further reinforce the reciprocal relationships among classifying, detecting, and segmenting sub-tasks. Extensive experiments on the standard COCO dataset reveal the effectiveness of the proposed method over multiple evaluation metrics, including box AP, mask AP, and inference speed. In particular, while running 38\\% faster, the proposed SCNet improves the AP of the box and mask predictions by respectively 1.3 and 2.3 points compared to the strong Cascade Mask R-CNN baseline. Code is available at https://github.com/thangvubk/SCNet."
  },
  "aaai2021_main_task-independentknowledgemakesfortransferablerepresentationsforgeneralizedzero-shotlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Task-Independent Knowledge Makes for Transferable Representations for Generalized Zero-Shot Learning",
    "authors": [
      "Chaoqun Wang",
      "Xuejin Chen",
      "Shaobo Min",
      "Xiaoyan Sun",
      "Houqiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16375",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16375/16182",
    "published": "2021-02",
    "summary": "Generalized Zero-Shot Learning (GZSL) targets recognizing new categories by learning transferable image representations. Existing methods find that, by aligning image representations with corresponding semantic labels, the semantic-aligned representations can be transferred to unseen categories. However, supervised by only seen category labels, the learned semantic knowledge is highly task-specific, which makes image representations biased towards seen categories. In this paper, we propose a novel Dual-Contrastive Embedding Network (DCEN) that simultaneously learns task-specific and task-independent knowledge via semantic alignment and instance discrimination. First, DCEN leverages task labels to cluster representations of the same semantic category by cross-modal contrastive learning and exploring semantic-visual complementarity. Besides task-specific knowledge, DCEN then introduces task-independent knowledge by attracting representations of different views of the same image and repelling representations of different images. Compared to high-level seen category supervision, this instance discrimination supervision encourages DCEN to capture low-level visual knowledge, which is less biased toward seen categories and alleviates the representation bias. Consequently, the task-specific and task-independent knowledge jointly make for transferable representations of DCEN, which obtains averaged 4.1% improvement on four public benchmarks."
  },
  "aaai2021_main_efficientobject-levelvisualcontextmodelingformultimodalmachinetranslationmaskingirrelevantobjectshelpsgrounding": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding",
    "authors": [
      "Dexin Wang",
      "Deyi Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16376",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16376/16183",
    "published": "2021-02",
    "summary": "Visual context provides grounding information for multimodal machine translation (MMT). However, previous MMT models and probing studies on visual features suggest that visual information is less explored in MMT as it is often redundant to textual information. In this paper, we propose an Object-level Visual Context modeling framework (OVC) to efficiently capture and explore visual information for multimodal machine translation. With detected objects, the proposed OVC encourages MMT to ground translation on desirable visual objects by masking irrelevant objects in the visual modality. We equip the proposed with an additional object-masking loss to achieve this goal. The object-masking loss is estimated according to the similarity between masked objects and the source texts so as to encourage masking source-irrelevant objects. Additionally, in order to generate vision-consistent target words, we further propose a vision-weighted translation loss for OVC. Experiments on MMT datasets demonstrate that the proposed OVC model outperforms state-of-the-art MMT models and analyses show that masking irrelevant objects helps grounding in MMT."
  },
  "aaai2021_main_temporalrelationalmodelingwithself-supervisionforactionsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Temporal Relational Modeling with Self-Supervision for Action Segmentation",
    "authors": [
      "Dong Wang",
      "Di Hu",
      "Xingjian Li",
      "Dejing Dou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16377",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16377/16184",
    "published": "2021-02",
    "summary": "Temporal relational modeling in video is essential for human action understanding, such as action recognition and action segmentation. Although Graph Convolution Networks (GCNs) have shown promising advantages in relation reasoning on many tasks, it is still a challenge to apply graph convolution networks on long video sequences effectively. The main reason is that large number of nodes (i.e., video frames) makes GCNs hard to capture and model temporal relations in videos. To tackle this problem, in this paper, we introduce an effective GCN module, Dilated Temporal Graph Reasoning Module (DTGRM), designed to model temporal relations and dependencies between video frames at various time spans. In particular, we capture and model temporal relations via constructing multi-level dilated temporal graphs where the nodes represent frames from different moments in video. Moreover, to enhance temporal reasoning ability of the proposed model, an auxiliary self-supervised task is proposed to encourage the dilated temporal graph reasoning module to find and correct wrong temporal relations in videos. Our DTGRM model outperforms state-of-the-art action segmentation models on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset. The code is available at https://github.com/redwang/DTGRM."
  },
  "aaai2021_main_towardsrobustvisualinformationextractioninrealworldnewdatasetandnovelsolution": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution",
    "authors": [
      "Jiapeng Wang",
      "Chongyu Liu",
      "Lianwen Jin",
      "Guozhi Tang",
      "Jiaxin Zhang",
      "Shuaitao Zhang",
      "Qianying Wang",
      "Yaqiang Wu",
      "Mingxiang Cai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16378",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16378/16185",
    "published": "2021-02",
    "summary": "Visual Information Extraction (VIE) has attracted considerable attention recently owing to its various advanced applications such as document understanding, automatic marking and intelligent education. Most existing works decoupled this problem into several independent sub-tasks of text spotting (text detection and recognition) and information extraction, which completely ignored the high correlation among them during optimization. In this paper, we propose a robust Visual Information Extraction System (VIES) towards real-world scenarios, which is an unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information. Specifically, the information extraction branch collects abundant visual and semantic representations from text spotting for multimodal feature fusion and conversely, provides higher-level semantic clues to contribute to the optimization of text spotting. Moreover, regarding the shortage of public benchmarks, we construct a fully-annotated dataset called EPHOIE (https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. Compared with the state-of-the-art methods, our VIES shows significant superior performance on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used SROIE dataset under the end-to-end scenario."
  },
  "aaai2021_main_self-domainadaptationforfaceanti-spoofing": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Self-Domain Adaptation for Face Anti-Spoofing",
    "authors": [
      "Jingjing Wang",
      "Jingyi Zhang",
      "Ying Bian",
      "Youyi Cai",
      "Chunmao Wang",
      "Shiliang\n      Pu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16379",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16379/16186",
    "published": "2021-02",
    "summary": "Although current face anti-spoofing methods achieve promising results under intra-dataset testing, they suffer from poor generalization to unseen attacks. Most existing works adopt domain adaptation (DA) or domain generalization (DG) techniques to address this problem. However, the target domain is often unknown during training which limits the utilization of DA methods. DG methods can conquer this by learning domain invariant features without seeing any target data. However, they fail in utilizing the information of target data. In this paper, we propose a self-domain adaptation framework to leverage the unlabeled test domain data at inference. Specifically, a domain adaptor is designed to adapt the model for test domain. In order to learn a better adaptor, a meta-learning based adaptor learning algorithm is proposed using the data of multiple source domains at the training step. At test time, the adaptor is updated using only the test domain data according to the proposed unsupervised adaptor loss to further improve the performance. Extensive experiments on four public datasets validate the effectiveness of the proposed method."
  },
  "aaai2021_main_weaklysuperviseddeephypersphericalquantizationforimageretrieval": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval",
    "authors": [
      "Jinpeng Wang",
      "Bin Chen",
      "Qiang Zhang",
      "Zaiqiao Meng",
      "Shangsong Liang",
      "Shutao\n      Xia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16380",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16380/16187",
    "published": "2021-02",
    "summary": "Deep quantization methods have shown high efficiency on large-scale image retrieval. However, current models heavily rely on ground-truth information, hindering the application of quantization in label-hungry scenarios. A more realistic demand is to learn from inexhaustible uploaded images that are associated with informal tags provided by amateur users. Though such sketchy tags do not obviously reveal the labels, they actually contain useful semantic information for supervising deep quantization. To this end, we propose Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first work to learn deep quantization from weakly tagged images. Specifically, 1) we use word embeddings to represent the tags and enhance their semantic information based on a tag correlation graph. 2) To better preserve semantic information in quantization codes and reduce quantization error, we jointly learn semantics-preserving embeddings and supervised quantizer on hypersphere by employing a well-designed fusion layer and tailor-made loss functions. Extensive experiments show that WSDHQ can achieve state-of-art performance in weakly-supervised compact coding."
  },
  "aaai2021_main_camera-awareproxiesforunsupervisedpersonre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Camera-Aware Proxies for Unsupervised Person Re-Identification",
    "authors": [
      "Menglin Wang",
      "Baisheng Lai",
      "Jianqiang Huang",
      "Xiaojin Gong",
      "Xian-Sheng Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16381",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16381/16188",
    "published": "2021-02",
    "summary": "This paper tackles the purely unsupervised person re-identification (Re-ID) problem that requires no annotations. Some previous methods adopt clustering techniques to generate pseudo labels and use the produced labels to train Re-ID models progressively. These methods are relatively simple but effective. However, most clustering-based methods take each cluster as a pseudo identity class, neglecting the large intra-ID variance caused mainly by the change of camera views. To address this issue, we propose to split each single cluster into multiple proxies and each proxy represents the instances coming from the same camera. These camera-aware proxies enable us to deal with large intra-ID variance and generate more reliable pseudo labels for learning. Based on the camera-aware proxies, we design both intra and inter-camera contrastive learning components for our Re-ID model to effectively learn the ID discrimination ability within and across cameras. Meanwhile, a proxy-balanced sampling strategy is also designed, which facilitates our learning further. Extensive experiments on three large-scale Re-ID datasets show that our proposed approach outperforms most unsupervised methods by a significant margin. Especially, on the challenging MSMT17 dataset, we gain 14.3 percent Rank-1 and 10.2 percent mAP improvements when compared to the second place. Code is available at: https://github.com/Terminator8758/CAP-master."
  },
  "aaai2021_main_unsupervised3dlearningforshapeanalysisviamultiresolutioninstancediscrimination": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination",
    "authors": [
      "Peng-Shuai Wang",
      "Yu-Qi Yang",
      "Qian-Fang Zou",
      "Zhirong Wu",
      "Yang Liu",
      "Xin Tong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16382",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16382/16189",
    "published": "2021-02",
    "summary": "We propose an unsupervised method for learning a generic and efficient shape encoding network for different shape analysis tasks. Our key idea is to jointly encode and learn shape and point features from unlabeled 3D point clouds. For this purpose, we adapt HRNet to octree-based convolutional neural networks for jointly encoding shape and point features with fused multiresolution subnetworks and design a simple-yet-efficient Multiresolution Instance Discrimination (MID) loss for jointly learning the shape and point features. Our network takes a 3D point cloud as input and output both shape and point features. After training, Our network is concatenated with simple task-specific back-ends and fine-tuned for different shape analysis tasks. We evaluate the efficacy and generality of our method with a set of shape analysis tasks, including shape classification, semantic shape segmentation, as well as shape registration tasks. With simple back-ends, our network demonstrates the best performance among all unsupervised methods and achieves competitive performance to supervised methods. For fine-grained shape segmentation on the PartNet dataset, our method even surpasses existing supervised methods by a large margin."
  },
  "aaai2021_main_pgnetreal-timearbitrarily-shapedtextspottingwithpointgatheringnetwork": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Network",
    "authors": [
      "Pengfei Wang",
      "Chengquan Zhang",
      "Fei Qi",
      "Shanshan Liu",
      "Xiaoqiang Zhang",
      "Pengyuan Lyu",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui Ding",
      "Guangming Shi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16383",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16383/16190",
    "published": "2021-02",
    "summary": "The reading of arbitrarily-shaped text has received increasing research attention. However, existing text spotters are mostly built on two-stage frameworks or character-based methods, which suffer from either Non-Maximum Suppression (NMS), Region-of-Interest (RoI) operations, or character-level annotations. In this paper, to address the above problems, we propose a novel fully convolutional Point Gathering Network (PGNet) for reading arbitrarily-shaped text in real-time. The PGNet is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance. Experiments prove that the proposed method achieves competitive accuracy, meanwhile significantly improving the running speed. In particular, in Total-Text, it runs at 46.7 FPS, surpassing the previous spotters with a large margin."
  },
  "aaai2021_main_dynamicposition-awarenetworkforfine-grainedimagerecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Dynamic Position-aware Network for Fine-grained Image Recognition",
    "authors": [
      "Shijie Wang",
      "Haojie Li",
      "Zhihui Wang",
      "Wanli Ouyang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16384",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16384/16191",
    "published": "2021-02",
    "summary": "Most weakly supervised fine-grained image recognition (WFGIR) approaches predominantly focus on learning the discriminative details which contain the visual variances and position clues. The position clues can be indirectly learnt by utilizing context information of discriminative visual content. However, this will cause the selected discriminative regions containing some non-discriminative information introduced by the position clues. These analysis motivate us to directly introduce position clues into visual content to only focus on the visual variances, achieving more precise discriminative region localization. Though important, position modelling usually requires significant pixel/region annotations and therefore is labor-intensive. To address this issue, we propose an end-to-end Dynamic Position-aware Network (DP-Net) to directly incorporate the position clues into visual content and dynamically align them without extra annotations, which eliminates the effect of position information for visual variances of subcategories. In particular, the DP-Net consists of: 1) Position Encoding Module, which learns a set of position-aware parts by directly adding the learnable position information into the horizontal/vertical visual content of images; 2) Position-vision Aligning Module, which dynamically aligns both visual content and learnable position information via performing graph convolution on position-aware parts; 3) Position-vision Reorganization Module, which projects the aligned position clues and visual content into the Euclidean space to construct a position-aware feature maps. Finally, the position-aware feature maps are used which is implicitly applied the aligned visual content and position clues for more accurate discriminative regions localization. Extensive experiments verify that DP-Net yields the best performance under the same settings with most competitive approaches, on CUB Bird, Stanford-Cars, and FGVC Aircraft datasets."
  },
  "aaai2021_main_co-miningself-supervisedlearningforsparselyannotatedobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Co-mining: Self-Supervised Learning for Sparsely Annotated Object Detection",
    "authors": [
      "Tiancai Wang",
      "Tong Yang",
      "Jiale Cao",
      "Xiangyu Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16385",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16385/16192",
    "published": "2021-02",
    "summary": "Object detectors usually achieve promising results with the supervision of complete instance annotations. However, their performance is far from satisfactory with sparse instance annotations. Most existing methods for sparsely annotated object detection either re-weight the loss of hard negative samples or convert the unlabeled instances into ignored regions to reduce the interference of false negatives. We argue that these strategies are insufficient since they can at most alleviate the negative effect caused by missing annotations. In thispaper, we propose a simple but effective mechanism, called Co-mining, for sparsely annotated object detection. In our Co-mining, two branches of a siamese network predict the pseudo-label sets for each other. To enhance multi-view learning and better mine unlabeled instances, the original image and corresponding augmented image are used as the inputs of two branches of the siamese network, respectively. Co-mining can serve as a general training mechanism applied to most of modern object detectors. Experiments are performed on MS COCO dataset with three different sparsely annotated settings using two typical frameworks: anchor-based detector RetinaNet and anchor-free detector FCOS. Experimental results show that our Co-mining with RetinaNet achieves 1.4%\u223c2.1% improvements compared with different baselines and surpasses existing methods under the same sparsely annotated setting."
  },
  "aaai2021_main_veryimportantpersonlocalizationinunconstrainedconditionsanewbenchmark": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Very Important Person Localization in Unconstrained Conditions: A New Benchmark",
    "authors": [
      "Xiao Wang",
      "Zheng Wang",
      "Toshihiko Yamasaki",
      "Wenjun Zeng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16386",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16386/16193",
    "published": "2021-02",
    "summary": "This paper presents a new high-quality dataset for Very Important Person Localization (VIPLoc), named Unconstrained-7k. Generally, current datasets: 1) are limited in scale; 2) built under simple and constrained conditions, where the number of disturbing non-VIPs is not large, the scene is relatively simple, and the face of VIP is always in frontal view and salient. To tackle these problems, the proposed Unconstrained-7k dataset is featured in two aspects. First, it contains over 7,000 annotated images, making it the largest VIPLoc datasetunder unconstrained conditions to date. Second, our dataset is collected freely on the Internet, including multiple scenes, where images are in unconstrained conditions. VIPs in the new dataset are in different settings, e.g., large view variation, varying sizes, occluded, and complex scenes. Meanwhile, each image has more persons (> 20), making the dataset more challenging.As a minor contribution, motivated by the observation that VIPs are highly related to not only neighbors but also iconic objects, this paper proposes a Joint Social Relation and Individual Interaction Graph Neural Networks (JSRII-GNN) for VIPLoc. Experiments show that the JSRII-GNN yields competitive accuracy on NCAA (National Collegiate Athletic Association), MS (Multi-scene), and Unconstrained-7k datasets. https://github.com/xiaowang1516/VIPLoc."
  },
  "aaai2021_main_teacherguidedneuralarchitecturesearchforfacerecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Teacher Guided Neural Architecture Search for Face Recognition",
    "authors": [
      "Xiaobo Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16387",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16387/16194",
    "published": "2021-02",
    "summary": "Knowledge distillation is an effective tool to compress large pre-trained convolutional neural networks (CNNs) or their ensembles into models applicable to mobile and embedded devices. However, with expected flops or latency, existing methods are hand-crafted heuristics. They propose to pre-define the target student network for knowledge distillation, which may be sub-optimal because it requires much effort to explore a powerful student from the large design space. In this paper, we develop a novel teacher guided neural architecture search method to directly search for a student network with flexible channel and layer sizes. Specifically, we define the search space as the number of the channels/layers, which is sampled based on the probability distribution and is learned by minimizing the search objective of the student network. The maximum probability for the size in each distribution serves as the final searched width and depth of the target student network. Extensive experiments on a variety of face recognition benchmarks have demonstrated the superiority of our method over the state-of-the-art alternatives."
  },
  "aaai2021_main_deepmulti-tasklearningfordiabeticretinopathygradinginfundusimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Deep Multi-Task Learning for Diabetic Retinopathy Grading in Fundus Images",
    "authors": [
      "Xiaofei Wang",
      "Mai Xu",
      "Jicong Zhang",
      "Lai Jiang",
      "Liu Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16388",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16388/16195",
    "published": "2021-02",
    "summary": "Recent years have witnessed the growing interest in disease severity grading, especially for ocular diseases based on fundus images. The existing grading methods are usually trained with high resolution (HR) images. However, the grading performance decreases a lot given low resolution (LR) images, which are common in practice. In this paper, we mainly focus on diabetic retinopathy (DR) grading with LR fundus images. According to our analysis on the DR task, we find that: 1) image super-resolution (ISR) can boost the performance of DR grading and lesion segmentation; 2) the lesion segmentation regions of fundus images are highly consistent with pathological regions for DR grading. Thus, we propose a deep multi-task learning based DR grading (DeepMT-DR) method for LR fundus images, which simultaneously handles the auxiliary tasks of ISR and lesion segmentation. Specifically, based on our findings, we propose a hierarchical deep learning structure that simultaneously processes the low-level task of ISR, the mid-level task of lesion segmentation and the high-level task of DR grading. Moreover, a novel task-aware loss is developed to encourage ISR to focus on the pathological regions for its subsequent tasks: lesion segmentation and DR grading. Extensive experimental results show that our DeepMT-DR method significantly outperforms other state-of-the-art methods for DR grading over two public datasets. In addition, our method achieves comparable performance in two auxiliary tasks of ISR and lesion segmentation."
  },
  "aaai2021_main_confidence-awarenon-repetitivemultimodaltransformersfortextcaps": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Confidence-aware Non-repetitive Multimodal Transformers for TextCaps",
    "authors": [
      "Zhaokai Wang",
      "Renda Bao",
      "Qi Wu",
      "Si Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16389",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16389/16196",
    "published": "2021-02",
    "summary": "When describing an image, reading text in the visual scene is crucial to understand the key information. Recent work explores the TextCaps task, i.e. image captioning with reading Optical Character Recognition (OCR) tokens, which requires models to read text and cover them in generated captions. Existing approaches fail to generate accurate descriptions because of their (1) poor reading ability; (2) inability to choose the crucial words among all extracted OCR tokens; (3) repetition of words in predicted captions. To this end, we propose a Confidence-aware Non-repetitive Multimodal Transformers (CNMT) to tackle the above challenges. Our CNMT consists of a reading, a reasoning and a generation modules, in which Reading Module employs better OCR systems to enhance text reading ability and a confidence embedding to select the most noteworthy tokens. To address the issue of word redundancy in captions, our Generation Module includes a repetition mask to avoid predicting repeated word in captions. Our model outperforms state-of-the-art models on TextCaps dataset, improving from 81.0 to 93.0 in CIDEr. Our source code is publicly available."
  },
  "aaai2021_main_geodesic-hof3dreconstructionwithoutcuttingcorners": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Geodesic-HOF: 3D Reconstruction Without Cutting Corners",
    "authors": [
      "Ziyun Wang",
      "Eric A. Mitchell",
      "Volkan Isler",
      "Daniel D. Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16390",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16390/16197",
    "published": "2021-02",
    "summary": "Single-view 3D object reconstruction is a challenging fundamental problem in machine perception, largely due to the morphological diversity of objects in the natural world. In particular, high curvature regions are not always represented accurately by methods trained with common set-based loss functions such as Chamfer Distance, resulting in reconstructions short-circuiting the surface or \"cutting corners.\" To address this issue, we propose an approach to 3D reconstruction that embeds points on the surface of an object into a higher-dimensional space that captures both the original 3D surface as well as geodesic distances between points on the surface of the object. The precise specification of these additional \"lifted\" coordinates ultimately yields useful surface information without requiring excessive additional computation during either training or testing, in comparison with existing approaches. Our experiments show that taking advantage of these learned lifted coordinates yields better performance for estimating surface normals and generating surfaces than using point cloud reconstructions alone. Further, we find that this learned geodesic embedding space provides useful information for applications such as unsupervised object decomposition."
  },
  "aaai2021_main_c2f-fwncoarse-to-fineflowwarpingnetworkforspatial-temporalconsistentmotiontransfer": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "C2F-FWN: Coarse-to-Fine Flow Warping Network for Spatial-Temporal Consistent Motion Transfer",
    "authors": [
      "Dongxu Wei",
      "Xiaowei Xu",
      "Haibin Shen",
      "Kejie Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16391",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16391/16198",
    "published": "2021-02",
    "summary": "Human video motion transfer (HVMT) aims to synthesize videos that one person imitates other persons' actions. Although existing GAN-based HVMT methods have achieved great success, they either fail to preserve appearance details due to the loss of spatial consistency between synthesized and exemplary images, or generate incoherent video results due to the lack of temporal consistency among video frames. In this paper, we propose Coarse-to-Fine Flow Warping Network (C2F-FWN) for spatial-temporal consistent HVMT. Particularly, C2F-FWN utilizes coarse-to-fine flow warping and Layout-Constrained Deformable Convolution (LC-DConv) to improve spatial consistency, and employs Flow Temporal Consistency (FTC) Loss to enhance temporal consistency. In addition, provided with multi-source appearance inputs, C2F-FWN can support appearance attribute editing with great flexibility and efficiency. Besides public datasets, we also collected a large-scale HVMT dataset named SoloDance for evaluation. Extensive experiments conducted on our SoloDance dataset and the iPER dataset show that our approach outperforms state-of-art HVMT methods in terms of both spatial and temporal consistency. Source code and the SoloDance dataset are available at https://github.com/wswdx/C2F-FWN."
  },
  "aaai2021_main_semanticconsistencynetworksfor3dobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Semantic Consistency Networks for 3D Object Detection",
    "authors": [
      "Wenwen Wei",
      "Ping Wei",
      "Nanning Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16392",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16392/16199",
    "published": "2021-02",
    "summary": "Detecting 3D objects from point clouds is a significant yet challenging issue in many applications. While most existing approaches seek to leverage geometric information of point clouds, few studies accommodate the inherent semantic characteristics of each point and the consistency between the geometric and semantic cues. In this work, we propose a novel semantic consistency network (SCNet) driven by a natural principle: the class of a predicted 3D bounding box should be consistent with the classes of all the points inside this box. Specifically, our SCNet consists of a feature extraction structure, a detection decision structure, and a semantic segmentation structure. In inference, the feature extraction and the detection decision structures are used to detect 3D objects. In training, the semantic segmentation structure is jointly trained with the other two structures to produce more robust and applicative model parameters. A novel semantic consistency loss is proposed to regulate the output 3D object boxes and the segmented points to boost the performance. Our model is evaluated on two challenging datasets and achieves comparable results to the state-of-the-art methods."
  },
  "aaai2021_main_holisticmulti-viewbuildinganalysisinthewildwithprojectionpooling": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Holistic Multi-View Building Analysis in the Wild with Projection Pooling",
    "authors": [
      "Zbigniew Wojna",
      "Krzysztof Maziarz",
      "\u0141ukasz Jocz",
      "Robert Pa\u0142uba",
      "Robert\n      Kozikowski",
      "Iason Kokkinos"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16393",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16393/16200",
    "published": "2021-02",
    "summary": "We address six different classification tasks related to fine-grained building attributes: construction type, number of floors, pitch and geometry of the roof, facade material, and occupancy class. Tackling such a remote building analysis problem became possible only recently due to growing large-scale datasets of urban scenes. To this end, we introduce a new benchmarking dataset, consisting of 49426 images (top-view and street-view) of 9674 buildings. These photos are further assembled, together with the geometric metadata. The dataset showcases various real-world challenges, such as occlusions, blur, partially visible objects, and a broad spectrum of buildings. We propose a new \\emph{projection pooling layer}, creating a unified, top-view representation of the top-view and the side views in a high-dimensional space. It allows us to utilize the building and imagery metadata seamlessly. Introducing this layer improves classification accuracy -- compared to highly tuned baseline models -- indicating its suitability for building analysis."
  },
  "aaai2021_main_stereopagnosiafoolingstereonetworkswithadversarialperturbations": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Stereopagnosia: Fooling Stereo Networks with Adversarial Perturbations",
    "authors": [
      "Alex Wong",
      "Mukund Mundhra",
      "Stefano Soatto"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16394",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16394/16201",
    "published": "2021-02",
    "summary": "We study the effect of adversarial perturbations of images on the estimates of disparity by deep learning models trained for stereo. We show that imperceptible additive perturbations can significantly alter the disparity map, and correspondingly the perceived geometry of the scene. These perturbations not only affect the specific model they are crafted for, but transfer to models with different architecture, trained with different loss functions. We show that, when used for adversarial data augmentation, our perturbations result in trained models that are more robust, without sacrificing overall accuracy of the model. This is unlike what has been observed in image classification, where adding the perturbed images to the training set makes the model less vulnerable to adversarial perturbations, but to the detriment of overall accuracy. We test our method using the most recent stereo networks and evaluate their performance on public benchmark datasets."
  },
  "aaai2021_main_generalisingwithoutforgettingforlifelongpersonre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Generalising without Forgetting for Lifelong Person Re-Identification",
    "authors": [
      "Guile Wu",
      "Shaogang Gong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16395",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16395/16202",
    "published": "2021-02",
    "summary": "Existing person re-identification (Re-ID) methods mostly prepare all training data in advance, while real-world Re-ID data are inherently captured over time or from different locations, which requires a model to be incrementally generalised from sequential learning of piecemeal new data without forgetting what is already learned. In this work, we call this lifelong person Re-ID, characterised by solving a problem of unseen class identification subject to continuous new domain generalisation and adaptation with class imbalanced learning. We formulate a new Generalising without Forgetting method (GwFReID) for lifelong Re-ID and design a comprehensive learning objective that accounts for classification coherence, distribution coherence and representation coherence in a unified framework. This design helps to simultaneously learn new information, distil old knowledge and solve class imbalance, which enables GwFReID to incrementally improve model generalisation without catastrophic forgetting of what is already learned. Extensive experiments on eight Re-ID benchmarks, CIFAR-100 and ImageNet show the superiority of GwFReID over the state-of-the-art methods."
  },
  "aaai2021_main_decentralisedlearningfromindependentmulti-domainlabelsforpersonre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification",
    "authors": [
      "Guile Wu",
      "Shaogang Gong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16396",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16396/16203",
    "published": "2021-02",
    "summary": "Deep learning has been successful for many computer vision tasks due to the availability of shared and centralised large-scale training data. However, increasing awareness of privacy concerns poses new challenges to deep learning, especially for human subject related recognition such as person re-identification (Re-ID). In this work, we solve the Re-ID problem by decentralised learning from non-shared private training data distributed at multiple user sites of independent multi-domain label spaces. We propose a novel paradigm called Federated Person Re-Identification (FedReID) to construct a generalisable global model (a central server) by simultaneously learning with multiple privacy-preserved local models (local clients). Specifically, each local client receives global model updates from the server and trains a local model using its local data independent from all the other clients. Then, the central server aggregates transferrable local model updates to construct a generalisable global feature embedding model without accessing local data so to preserve local privacy. This client-server collaborative learning process is iteratively performed under privacy control, enabling FedReID to realise decentralised learning without sharing distributed data nor collecting any centralised data. Extensive experiments on ten Re-ID benchmarks show that FedReID achieves compelling generalisation performance beyond any locally trained models without using shared training data, whilst inherently protects the privacy of each local client. This is uniquely advantageous over contemporary Re-ID methods."
  },
  "aaai2021_main_region-awareglobalcontextmodelingforautomaticnervesegmentationfromultrasoundimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Region-aware Global Context Modeling for Automatic Nerve Segmentation from Ultrasound Images",
    "authors": [
      "Huisi Wu",
      "Jiasheng Liu",
      "Wei Wang",
      "Zhenkun Wen",
      "Jing Qin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16397",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16397/16204",
    "published": "2021-02",
    "summary": "We present a novel deep learning model equipped with a new region-aware global context modeling technique for automatic nerve segmentation from ultrasound images, which is a challenging task due to (1) the large variation and blurred boundaries of targets, (2) the large amount of speckle noise in ultrasound images, and (3) the inherent real-time requirement of this task. It is essential to efficiently capture long-range dependencies by global context modeling for a segmentation network to overcome these challenges. Traditional global context modeling techniques usually explore pixel-aware correlations to establish long-range dependencies, which are usually computation-intensive and greatly degrade time performance. In addition, in this application, pixel-aware modeling may inevitably introduce much speckle noise in the computation and potentially degrade segmentation performance. In this paper, we propose a novel region-aware modeling technique to establish long-range dependencies based on different regions to improve segmentation accuracy while maintaining real-time performance; we call it region-aware pyramid aggregation (RPA) module. In order to adaptively divide the feature maps into a set of semantic-independent regions, we develop an attention mechanism and integrate it into the spatial pyramid network to evaluate the semantic similarity of different regions. We further develop an adaptive pyramid fusion (APF) module to dynamically fuse the multi-level features generated from the decoder to refining the segmentation results. We conducted extensive experiments on a famous public ultrasound nerve image segmentation dataset. Experimental results demonstrate that our method consistently outperforms our rivals in terms of segmentation accuracy. The code is available at https://github.com/jsonliu-szu/RAGCM."
  },
  "aaai2021_main_preciseyetefficientsemanticcalibrationandrefinementinconvnetsforreal-timepolypsegmentationfromcolonoscopyvideos": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Precise Yet Efficient Semantic Calibration and Refinement in ConvNets for Real-time Polyp Segmentation from Colonoscopy Videos",
    "authors": [
      "Huisi Wu",
      "Jiafu Zhong",
      "Wei Wang",
      "Zhenkun Wen",
      "Jing Qin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16398",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16398/16205",
    "published": "2021-02",
    "summary": "We propose a novel convolutional neural network (ConvNet) equipped with two new semantic calibration and refinement approaches for automatic polyp segmentation from colonoscopy videos. While ConvNets set state-of-the-are performance for this task, it is still difficult to achieve satisfactory results in a real-time manner, which is a necessity in clinical practice. The main obstacle is the huge semantic gap between high-level features and low-level features, making it difficult to take full advantage of complementary semantic information contained in these hierarchical features. Compared with existing solutions, which either directly aggregate these features without considering the semantic gap or employ sophisticated non-local modeling techniques to refine semantic information by introduce many extra computational costs, the proposed ConvNet is able to more precisely yet efficiently calibrate and refine semantic information for better segmentation performance without increasing model complexity; we call the proposed ConvNet as SCR-Net, which has two key modules. We first propose a semantic calibration module (SCM) to effectively transmit the semantic information from high-level layers to low-level layers by learning the semantic-spatial relations during the training procedure. We then propose a semantic refinement module (SRM) to, based on the features calibrated by SCM, enhance the discrimination capability of the features for targeting objects. Extensive experiments on the Kvasir-SEG dataset demonstrate that the proposed SCR-Net is capable of achieving better segmentation accuracy than state-of-the-art approaches with a faster speed. The proposed techniques are general enough to be applied to similar applications where precise and efficient multi-level feature fusion is critical. The code is available at https://github.com/jiafuz/SCR-Net."
  },
  "aaai2021_main_graph-to-graphtowardsaccurateandinterpretableonlinehandwrittenmathematicalexpressionrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Graph-to-Graph: Towards Accurate and Interpretable Online Handwritten Mathematical Expression Recognition",
    "authors": [
      "Jin-Wen Wu",
      "Fei Yin",
      "Yan-Ming Zhang",
      "Xu-Yao Zhang",
      "Cheng-Lin Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16399",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16399/16206",
    "published": "2021-02",
    "summary": "Recent handwritten mathematical expression recognition (HMER) approaches treat the problem as an image-to-markup generation task where the handwritten formula is translated into a sequence (e.g. LaTeX). The encoder-decoder framework is widely used to solve this image-to-sequence problem. However, (i) for structured mathematical formula, the hierarchical structure neither in the formula nor in the markup has been explored adequately. In addition, (ii) existing image-to-markup methods could not explicitly segment mathematical symbols in the formula corresponding to each target markup token. In this paper, we address the above issues by formulating the HMER as a graph-to-graph (G2G) learning problem. Graph is more flexible and general for structure representation and learning compared with image or sequence. At the core of our method lies the embedding of input formula and output markup into graphs on primitives, with Graph Neural Networks (GNN) to explore the structural information, and a novel sub-graph attention mechanism to match primitives in the input and output graphs. We conduct extensive experiments on CROHME datasets to demonstrate the benefits of the proposed G2G model. Our method yields significant improvements over previous SOTA image-to-markup systems. Moreover, it explicitly resolves the symbol segmentation problem while still being trained end-to-end, making the whole system much more accurate and interpretable."
  },
  "aaai2021_main_learningcomprehensivemotionrepresentationforactionrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Comprehensive Motion Representation for Action Recognition",
    "authors": [
      "Mingyu Wu",
      "Boyuan Jiang",
      "Donghao Luo",
      "Junchi Yan",
      "Yabiao Wang",
      "Ying Tai",
      "Chengjie Wang",
      "Jilin Li",
      "Feiyue Huang",
      "Xiaokang Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16400",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16400/16207",
    "published": "2021-02",
    "summary": "For action recognition learning, 2D CNN-based methods are efficient but may yield redundant features due to applying the same 2D convolution kernel to each frame. Recent efforts attempt to capture motion information by establishing inter-frame connections while still suffering the limited temporal receptive field or high latency. Moreover, the feature enhancement is often only performed by channel or space dimension in action recognition. To address these issues, we first devise a Channel-wise Motion Enhancement (CME) module to adaptively emphasize the channels related to dynamic information with a channel-wise gate vector. The channel gates generated by CME incorporate the information from all the other frames in the video. We further propose a Spatial-wise Motion Enhancement (SME) module to focus on the regions with the critical target in motion, according to the point-to-point similarity between adjacent feature maps. The intuition is that the change of background is typically slower than the motion area. Both CME and SME have clear physical meaning in capturing action clues. By integrating the two modules into the off-the-shelf 2D network, we finally obtain a Comprehensive Motion Representation (CMR) learning method for action recognition, which achieves competitive performance onSomething-Something V1 & V2 and Kinetics-400. On the temporal reasoning datasets Something-Something V1 and V2, our method outperforms the current state-of-the-art by 2.3% and 1.9% when using 16 frames as input, respectively."
  },
  "aaai2021_main_mvfnetmulti-viewfusionnetworkforefficientvideorecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "MVFNet: Multi-View Fusion Network for Efficient Video Recognition",
    "authors": [
      "Wenhao Wu",
      "Dongliang He",
      "Tianwei Lin",
      "Fu Li",
      "Chuang Gan",
      "Errui Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16401",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16401/16208",
    "published": "2021-02",
    "summary": "Conventionally, spatiotemporal modeling network and its complexity are the two most concentrated research topics in video action recognition. Existing state-of-the-art methods have achieved excellent accuracy regardless of the complexity meanwhile efficient spatiotemporal modeling solutions are slightly inferior in performance. In this paper, we attempt to acquire both efficiency and effectiveness simultaneously. First of all, besides traditionally treating H x W x T video frames as space-time signal (viewing from the Height-Width spatial plane), we propose to also model video from the other two Height-Time and Width-Time planes, to capture the dynamics of video thoroughly. Secondly, our model is designed based on 2D CNN backbones and model complexity is well kept in mind by design. Specifically, we introduce a novel multi-view fusion (MVF) module to exploit video dynamics using separable convolution for efficiency. It is a plug-and-play module and can be inserted into off-the-shelf 2D CNNs to form a simple yet effective model called MVFNet. Moreover, MVFNet can be thought of as a generalized video modeling framework and it can specialize to be existing methods such as C2D, SlowOnly, and TSM under different settings. Extensive experiments are conducted on popular benchmarks (i.e., Something-Something V1 & V2, Kinetics, UCF-101, and HMDB-51) to show its superiority. The proposed MVFNet can achieve state-of-the-art performance with 2D CNN's complexity."
  },
  "aaai2021_main_anticipatingfuturerelationsviagraphgrowingforactionprediction": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Anticipating Future Relations via Graph Growing for Action Prediction",
    "authors": [
      "Xinxiao Wu",
      "Jianwei Zhao",
      "Ruiqi Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16402",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16402/16209",
    "published": "2021-02",
    "summary": "Predicting actions from partially observed videos is challenging as the partial videos containing incomplete action executions have insufficient discriminative information for classification. Recent progress has been made through enriching the features of the observed video part or generating the features for the unobserved video part, but without explicitly modeling the fine-grained evolution of visual object relations over both space and time. In this paper, we investigate how the interaction and correlation between visual objects evolve and propose a graph growing method to anticipate future object relations from limited video observations for reliable action prediction. There are two tasks in our method. First, we work with spatial-temporal graph neural networks to reason object relations in the observed video part. Then, we synthesize the spatial-temporal relation representation for the unobserved video part via graph node generation and aggregation. These two tasks are jointly learned to enable the anticipated future relation representation informative to action prediction. Experimental results on two action video datasets demonstrate the effectiveness of our method."
  },
  "aaai2021_main_binauralaudio-visuallocalization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Binaural Audio-Visual Localization",
    "authors": [
      "Xinyi Wu",
      "Zhenyao Wu",
      "Lili Ju",
      "Song Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16403",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16403/16210",
    "published": "2021-02",
    "summary": "Localizing sound sources in a visual scene has many important applications and quite a few traditional or learning-based methods have been proposed for this task. Humans have the ability to roughly localize sound sources within or beyond the range of the vision using their binaural system. However most existing methods use monaural audio, instead ofbinaural audio, as a modality to help the localization. In addition, prior works usually localize sound sources in the form of object-level bounding boxes in images or videos and evaluate the localization accuracy by examining the overlap between the ground-truth and predicted bounding boxes. This is too rough since a real sound source is often only a part of an object. In this paper, we propose a deep learning method for pixel-level sound source localization by leveraging both binaural recordings and the corresponding videos. Specifically, we design a novel Binaural Audio-Visual Network (BAVNet), which concurrently extracts and integrates features from binaural recordings and videos. We also propose a point-annotation strategy to construct pixel-level ground truth for network training and performance evaluation. Experimental results on Fair-Play and YT-Music datasets demonstratethe effectiveness of the proposed method and show that binaural audio can greatly improve the performance of localizing the sound sources, especially when the quality of the visual information is limited."
  },
  "aaai2021_main_beatingattackersattheirowngamesadversarialexampledetectionusingadversarialgradientdirections": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Beating Attackers At Their Own Games: Adversarial Example Detection Using Adversarial Gradient Directions",
    "authors": [
      "Yuhang Wu",
      "Sunpreet S Arora",
      "Yanhong Wu",
      "Hao Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16404",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16404/16211",
    "published": "2021-02",
    "summary": "Adversarial examples are input examples that are specifically crafted to deceive machine learning classifiers. State-of-the-art adversarial example detection methods characterize an input example as adversarial either by quantifying the magnitude of feature variations under multiple perturbations or by measuring its distance from estimated benign example distribution. Instead of using such metrics, the proposed method is based on the observation that the directions of adversarial gradients when crafting (new) adversarial examples play a key role in characterizing the adversarial space. Compared to detection methods that use multiple perturbations, the proposed method is efficient as it only applies a single random perturbation on the input example. Experiments conducted on two different databases, CIFAR-10 and ImageNet, show that the proposed detection method achieves, respectively, 97.9% and 98.6% AUC-ROC (on average) on five different adversarial attacks, and outperforms multiple state-of-the-art detection methods. Results demonstrate the effectiveness of using adversarial gradient directions for adversarial example detection."
  },
  "aaai2021_main_shape-poseambiguityinlearning3dreconstructionfromimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Shape-Pose Ambiguity in Learning 3D Reconstruction from Images",
    "authors": [
      "Yunjie Wu",
      "Zhengxing Sun",
      "Youcheng Song",
      "Yunhan Sun",
      "YiJie Zhong",
      "Jinlong\n      Shi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16405",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16405/16212",
    "published": "2021-02",
    "summary": "Learning single-image 3D reconstruction with only 2D images supervision is a promising research topic. The main challenge in image-supervised 3D reconstruction is the shape-pose ambiguity, which means a 2D supervision can be explained by an erroneous 3D shape from an erroneous pose. It will introduce high uncertainty and mislead the learning process. Existed works rely on multi-view images or pose-aware annotations to resolve the ambiguity. In this paper, we propose to resolve the ambiguity without extra pose-aware labels or annotations. Our training data is single-view images from the same object category. To overcome the shape-pose ambiguity, we introduce a pose-independent GAN to learn the category-specific shape manifold from the image collections. With the learned shape space, we resolve the shape-pose ambiguity in original images by training a pseudo pose regressor. Finally, we learn a reconstruction network with both the common re-projection loss and a pose-independent discrimination loss, making the results plausible from all views. Through experiments on synthetic and real image datasets, we demonstrate that our method can perform comparably to existing methods while not requiring any extra pose-aware annotations, making it more applicable and adaptable."
  },
  "aaai2021_main_boundaryproposalnetworkfortwo-stagenaturallanguagevideolocalization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Boundary Proposal Network for Two-stage Natural Language Video Localization",
    "authors": [
      "Shaoning Xiao",
      "Long Chen",
      "Songyang Zhang",
      "Wei Ji",
      "Jian Shao",
      "Lu Ye",
      "Jun\n      Xiao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16406",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16406/16213",
    "published": "2021-02",
    "summary": "We aim to address the problem of Natural Language Video Localization (NLVL) \u2014 localizing the video segment corresponding to a natural language description in a long and untrimmed video. State-of-the-art NLVL methods are almost in one-stage fashion, which can be typically grouped into two categories: 1) anchor-based approach: it first pre-defines a series of video segment candidates (e.g., by sliding window), and then does classification for each candidate; 2) anchor-free approach: it directly predicts the probabilities for each video frame as a boundary or intermediate frame inside the positive segment. However, both kinds of one-stage approaches have inherent drawbacks: the anchor-based approach is susceptible to the heuristic rules, further limiting the capability of handling videos with variant length. While the anchor-free approach fails to exploit the segment-level interaction thus achieving inferior results. In this paper, we propose a novel Boundary Proposal Network (BPNet), a universal two-stage framework that gets rid of the issues mentioned above. Specifically, in the first stage, BPNet utilizes an anchor-free model to generate a group of high-quality candidate video segments with their boundaries. In the second stage, a visual-language fusion layer is proposed to jointly model the multi-modal interaction between the candidate and the language query, followed by a matching score rating layer that outputs the alignment score for each candidate. We evaluate our BPNet on three challenging NLVL benchmarks (i.e., Charades-STA, TACoS and ActivityNet-Captions). Extensive experiments and ablative studies on these datasets demonstrate that the BPNet outperforms the state-of-the-art methods."
  },
  "aaai2021_main_amodalsegmentationbasedonvisibleregionsegmentationandshapeprior": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Amodal Segmentation Based on Visible Region Segmentation and Shape Prior",
    "authors": [
      "Yuting Xiao",
      "Yanyu Xu",
      "Ziming Zhong",
      "Weixin Luo",
      "Jiawei Li",
      "Shenghua Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16407",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16407/16214",
    "published": "2021-02",
    "summary": "Almost all existing amodal segmentation methods make the inferences of occluded regions by using features corresponding to the whole image. This is against the human's amodal perception, where human uses the visible part and the shape prior knowledge of the target to infer the occluded region. To mimic the behavior of human and solve the ambiguity in the learning, we propose a framework, it firstly estimates a coarse visible mask and a coarse amodal mask. Then based on the coarse prediction, our model infers the amodal mask by concentrating on the visible region and utilizing the shape prior in the memory. In this way, features corresponding to background and occlusion can be suppressed for amodal mask estimation. Consequently, the amodal mask would not be affected by what the occlusion is given the same visible regions. The leverage of shape prior makes the amodal mask estimation more robust and reasonable. Our proposed model is evaluated on three datasets. Experiments show that our proposed model outperforms existing state-of-the-art methods. The visualization of shape prior indicates that the category-specific feature in the codebook has certain interpretability. The code is available at https://github.com/YutingXiao/Amodal-Segmentation-Based-on-Visible-Region-Segmentation-and-Shape-Prior."
  },
  "aaai2021_main_locateglobally,segmentlocallyaprogressivearchitecturewithknowledgereviewnetworkforsalientobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Locate Globally, Segment Locally: A Progressive Architecture With Knowledge Review Network for Salient Object Detection",
    "authors": [
      "Binwei Xu",
      "Haoran Liang",
      "Ronghua Liang",
      "Peng Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16408",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16408/16215",
    "published": "2021-02",
    "summary": "Salient object location and segmentation are two different tasks in salient object detection (SOD). The former aims to globally find the most attractive objects in an image, whereas the latter can be achieved only using local regions that contain salient objects. However, previous methods mainly accomplish the two tasks simultaneously in a simple end-to-end manner, which leads to the ignorance of the differences between them. We assume that the human vision system orderly locates and segments objects, so we propose a novel progressive architecture with knowledge review network (PA-KRN) for SOD. It consists of three parts. (1) A coarse locating module (CLM) that uses body-attention label locates rough areas containing salient objects without boundary details. (2) An attention-based sampler highlights salient object regions with high resolution based on body-attention maps. (3) A fine segmenting module (FSM) finely segments salient objects. The networks applied in CLM and FSM are mainly based on our proposed knowledge review network (KRN) that utilizes the finest feature maps to reintegrate all previous layers, which can make up for the important information that is continuously diluted in the top-down path. Experiments on five benchmarks demonstrate that our single KRN can outperform state-of-the-art methods. Furthermore, our PA-KRN performs better and substantially surpasses the aforementioned methods."
  },
  "aaai2021_main_invariantteacherandequivariantstudentforunsupervised3dhumanposeestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Invariant Teacher and Equivariant Student for Unsupervised 3D Human Pose Estimation",
    "authors": [
      "Chenxin Xu",
      "Siheng Chen",
      "Maosen Li",
      "Ya Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16409",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16409/16216",
    "published": "2021-02",
    "summary": "We propose a novel method based on teacher-student learning framework for 3D human pose estimation without any 3D annotation or side information. To solve this unsupervised-learning problem, the teacher network adopts pose-dictionary-based modeling for regularization to estimate a physically plausible 3D pose. To handle the decomposition ambiguity in the teacher network, we propose a cycle-consistent architecture promoting a 3D rotation-invariant property to train the teacher network. To further improve the estimation accuracy, the student network adopts a novel graph convolution network for flexibility to directly estimate the 3D coordinates. Another cycle-consistent architecture promoting 3D rotation-equivariant property is adopted to exploit geometry consistency, together with knowledge distillation from the teacher network to improve the pose estimation performance. We conduct extensive experiments on Human3.6M and MPI-INF-3DHP. Our method reduces the 3D joint prediction error by 11.4% compared to state-of-the-art unsupervised methods and also outperforms many weakly-supervised methods that use side information on Human3.6M.Code will be available at https://github.com/sjtuxcx/ITES."
  },
  "aaai2021_main_imagine,reasonandwritevisualstorytellingwithgraphknowledgeandrelationalreasoning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Imagine, Reason and Write: Visual Storytelling with Graph Knowledge and Relational Reasoning",
    "authors": [
      "Chunpu Xu",
      "Min Yang",
      "Chengming Li",
      "Ying Shen",
      "Xiang Ao",
      "Ruifeng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16410",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16410/16217",
    "published": "2021-02",
    "summary": "Visual storytelling is a task of creating a short story based on photo streams. Different from visual captions, stories contain not only factual descriptions, but also imaginary concepts that do not appear in the images. In this paper, we propose a novel imagine-reason-write generation framework (IRW) for visual storytelling, inspired by the logic of humans when they write the story. First, an imagine module is leveraged to learn the imaginative storyline explicitly, improving the coherence and reasonability of the generated story. Second, we employ a reason module to fully exploit the external knowledge (commonsense knowledge base) and task-specific knowledge (scene graph and event graph) with relational reasoning method based on the storyline. In this way, we can effectively capture the most informative commonsense and visual relationships among objects in images, which enhances the diversity and informativeness of the generated story. Finally, we integrate the imaginary concepts and relational knowledge to generate human-like story based on the original semantics of images. Extensive experiments on a benchmark dataset (i.e., VIST) demonstrate that the proposed IRW framework significantly outperforms the state-of-the-art methods across multiple evaluation metrics."
  },
  "aaai2021_main_self-supervisedmulti-viewstereoviaeffectiveco-segmentationanddata-augmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Self-supervised Multi-view Stereo via Effective Co-Segmentation and Data-Augmentation",
    "authors": [
      "Hongbin Xu",
      "Zhipeng Zhou",
      "Yu Qiao",
      "Wenxiong Kang",
      "Qiuxia Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16411",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16411/16218",
    "published": "2021-02",
    "summary": "Recent studies have witnessed that self-supervised methods based on view synthesis obtain clear progress on multi-view stereo (MVS). However, existing methods rely on the assumption that the corresponding points among different views share the same color, which may not always be true in practice. This may lead to unreliable self-supervised signal and harm the final reconstruction performance. To address the issue, we propose a framework integrated with more reliable supervision guided by semantic co-segmentation and data-augmentation. Specially, we excavate mutual semantic from multi-view images to guide the semantic consistency. And we devise effective data-augmentation mechanism which ensures the transformation robustness by treating the prediction of regular samples as pseudo ground truth to regularize the prediction of augmented samples. Experimental results on DTU dataset show that our proposed methods achieve the state-of-the-art performance among unsupervised methods, and even compete on par with supervised methods. Furthermore, extensive experiments on Tanks&Temples dataset demonstrate the effective generalization ability of the proposed method."
  },
  "aaai2021_main_efficientdeepimagedenoisingviaclassspecificconvolution": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Efficient Deep Image Denoising via Class Specific Convolution",
    "authors": [
      "Lu Xu",
      "Jiawei Zhang",
      "Xuanye Cheng",
      "Feng Zhang",
      "Xing Wei",
      "Jimmy Ren"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16412",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16412/16219",
    "published": "2021-02",
    "summary": "Deep neural networks have been widely used in image denoising during the past few years. Even though they achieve great success on this problem, they are computationally inefficient which makes them inappropriate to be implemented in mobile devices. In this paper, we propose an efficient deep neural network for image denoising based on pixel-wise classification. Despite using a computationally efficient network cannot effectively remove the noises from any content, it is still capable to denoise from a specific type of pattern or texture. The proposed method follows such a divide and conquer scheme. We first use an efficient U-net to pixel-wisely classify pixels in the noisy image based on the local gradient statistics.Then we replace part of the convolution layers in existing denoising networks by the proposed Class Specific Convolution layers (CSConv) which use different weights for different classes of pixels. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method can reduce the computational costs without sacrificing the performance compared to state-of-the-art algorithms."
  },
  "aaai2021_main_investigateindistinguishablepointsinsemanticsegmentationof3dpointcloud": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Investigate Indistinguishable Points in Semantic Segmentation of 3D Point Cloud",
    "authors": [
      "Mingye Xu",
      "Zhipeng Zhou",
      "Junhao Zhang",
      "Yu Qiao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16413",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16413/16220",
    "published": "2021-02",
    "summary": "This paper investigates the indistinguishable points (difficult to predict label) in semantic segmentation for large-scale 3D point clouds. The indistinguishable points consist of those located in complex boundary, points with similar local textures but different categories, and points in isolate small hard areas, which largely harm the performance of 3D semantic segmentation. To address this challenge, we propose a novel Indistinguishable Area Focalization Network (IAF-Net), which select indistinguishable points adaptively by utilizing the hierarchical semantic features and enhance fine-grained features for points especially those indistinguishable points. We also introduce multi-stage loss to improve the feature representation in a progressive way. Moreover, in order to analyze the segmentation performances of indistinguishable areas, we propose a new evaluation metric called Indistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the state-of-the-art performance on several popular 3D point datasets e.g. S3DIS and ScanNet, and clearly outperform other methods on IPBM. Our code will be available athttps://github.com/MingyeXu/IAF-Net."
  },
  "aaai2021_main_learninggeometry-disentangledrepresentationforcomplementaryunderstandingof3dobjectpointcloud": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Geometry-Disentangled Representation for Complementary Understanding of 3D Object Point Cloud",
    "authors": [
      "Mutian Xu",
      "Junhao Zhang",
      "Zhipeng Zhou",
      "Mingye Xu",
      "Xiaojuan Qi",
      "Yu Qiao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16414",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16414/16221",
    "published": "2021-02",
    "summary": "In 2D image processing, some attempts decompose images into high and low frequency components for describing edge and smooth parts respectively. Similarly, the contour and flat area of 3D objects, such as the boundary and seat area of a chair, describe different but also complementary geometries. However, such investigation is lost in previous deep networks that understand point clouds by directly treating all points or local patches equally. To solve this problem, we propose Geometry-Disentangled Attention Network (GDANet). GDANet introduces Geometry-Disentangle Module to dynamically disentangle point clouds into the contour and flat part of 3D objects, respectively denoted by sharp and gentle variation components. Then GDANet exploits Sharp-Gentle Complementary Attention Module that regards the features from sharp and gentle variation components as two holistic representations, and pays different attentions to them while fusing them respectively with original point cloud features. In this way, our method captures and refines the holistic and complementary 3D geometric semantics from two distinct disentangled components to supplement the local information. Extensive experiments on 3D object classification and segmentation benchmarks demonstrate that GDANet achieves the state-of-the-arts with fewer parameters."
  },
  "aaai2021_main_searchingforalignmentinfacerecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Searching for Alignment in Face Recognition",
    "authors": [
      "Xiaqing Xu",
      "Qiang Meng",
      "Yunxiao Qin",
      "Jianzhu Guo",
      "Chenxu Zhao",
      "Feng Zhou",
      "Zhen Lei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16415",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16415/16222",
    "published": "2021-02",
    "summary": "A standard pipeline of current face recognition frameworks consists of four individual steps: locating a face with a rough bounding box and several fiducial landmarks, aligning the face image using a pre-defined template, extracting representations and comparing. Among them, face detection, landmark detection and representation learning have long been studied and a lot of works have been proposed. As an important step with a big impact on recognition performance, the alignment step has attracted little attention. In this paper, we first explore and highlight the effects of different alignment templates on face recognition. Then, for the first time, we try to automatically search for the optimal template. We construct a well-defined searching space by decomposing the template searching into the crop size and vertical shift, and propose an efficient method Face Alignment Policy Search (FAPS). Besides, a well-designed benchmark is proposed to evaluate the searched policy. Experiments on our proposed benchmark validate the effectiveness of our method to improve the face recognition performance."
  },
  "aaai2021_main_gifthumbnailsattractmoreclickstoyourvideos": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "GIF Thumbnails: Attract More Clicks to Your Videos",
    "authors": [
      "Yi Xu",
      "Fan Bai",
      "Yingxuan Shi",
      "Qiuyu Chen",
      "Longwen Gao",
      "Kai Tian",
      "Shuigeng\n      Zhou",
      "Huyang Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16416",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16416/16223",
    "published": "2021-02",
    "summary": "With the rapid increase of mobile devices and online media, more and more people prefer posting/viewing videos online. Generally, these videos are presented on video streaming sites with image thumbnails and text titles. While facing huge amounts of videos, a viewer clicks through a certain video with high probability because of its eye-catching thumbnail. However, current video thumbnails are created manually, which is time-consuming and quality-unguaranteed. And static image thumbnails contain very limited information of the corresponding videos, which prevents users from successfully clicking what they really want to view. \t\t In this paper, we address a novel problem, namely GIF thumbnail generation, which aims to automatically generate GIF thumbnails for videos and consequently boost their Click-Through-Rate (CTR). Here, a GIF thumbnail is an animated GIF file consisting of multiple segments from the video, containing more information of the target video than a static image thumbnail. To support this study, we build the first GIF thumbnails benchmark dataset that consists of 1070 videos covering a total duration of 69.1 hours, and 5394 corresponding manually-annotated GIFs. To solve this problem, we propose a learning-based automatic GIF thumbnail generation model, which is called Generative Variational Dual-Encoder (GEVADEN).As not relying on any user interaction information (e.g. time-sync comments and real-time view counts), this model is applicable to newly-uploaded/rarely-viewed videos. Experiments on our built dataset show that GEVADEN significantly outperforms several baselines, including video-summarization and highlight-detection based ones. Furthermore, we develop a pilot application of the proposed model on an online video platform with 9814 videos covering 1231 hours, which shows that our model achieves a 37.5% CTR improvement over traditional image thumbnails. This further validates the effectiveness of the proposed model and the promising application prospect of GIF thumbnails."
  },
  "aaai2021_main_facecontrollercontrollableattributeeditingforfaceinthewild": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "FaceController: Controllable Attribute Editing for Face in the Wild",
    "authors": [
      "Zhiliang Xu",
      "Xiyu Yu",
      "Zhibin Hong",
      "Zhen Zhu",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui\n      Ding",
      "Xiang Bai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16417",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16417/16224",
    "published": "2021-02",
    "summary": "Face attribute editing aims to generate faces with one or multiple desired face attributes manipulated while other details are preserved. Unlike prior works such as GAN inversion which has an expensive reverse mapping process, we propose a simple feed-forward network to generate high-fidelity manipulated faces. By simply employing some existing and easy-obtainable prior information, our method can control, transfer, and edit diverse attributes of faces in the wild. The proposed method can consequently be applied to various applications such as face swapping, face relighting, and makeup transfer. In our method, we decouple identity, expression, pose, and illumination by using 3D priors; separate texture and colors by using region-wise style codes. All the information is embedded into adversarial learning by our identity-style normalization module. Disentanglement losses are proposed to enhance the generator to extract information independently from each attribute. Comprehensive quantitative and qualitative evaluations have been conducted. In a single framework, our method achieves the best or competitive scores on a variety of face applications."
  },
  "aaai2021_main_anchorfaceananchor-basedfaciallandmarkdetectoracrosslargeposes": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "AnchorFace: An Anchor-based Facial Landmark Detector Across Large Poses",
    "authors": [
      "Zixuan Xu",
      "Banghuai Li",
      "Ye Yuan",
      "Miao Geng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16418",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16418/16225",
    "published": "2021-02",
    "summary": "Facial landmark localization aims to detect the predefined points of human faces, and the topic has been rapidly improved with the recent development of neural network based methods. However, it remains a challenging task when dealing with faces in unconstrained scenarios, especially with large pose variations. In this paper, we target the problem of facial landmark localization across large poses and address this task based on a split-and-aggregate strategy. To split the search space, we propose a set of anchor templates as references for regression, which well addresses the large variations of face poses. Based on the prediction of each anchor template, we propose to aggregate the results, which can reduce the landmark uncertainty due to the large poses. Overall, our proposed approach, named AnchorFace, obtains state-of-the-art results with extremely efficient inference speed on four challenging benchmarks, i.e. AFLW, 300W, Menpo, and WFLW dataset. Code will be available soon."
  },
  "aaai2021_main_sparsesinglesweeplidarpointcloudsegmentationvialearningcontextualshapepriorsfromscenecompletion": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion",
    "authors": [
      "Xu Yan",
      "Jiantao Gao",
      "Jie Li",
      "Ruimao Zhang",
      "Zhen Li",
      "Rui Huang",
      "Shuguang\n      Cui"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16419",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16419/16226",
    "published": "2021-02",
    "summary": "LiDAR point cloud analysis is a core task for 3D computer vision, especially for autonomous driving. However, due to the severe sparsity and noise interference in the single sweep LiDAR point cloud, the accurate semantic segmentation is non-trivial to achieve. In this paper, we propose a novel sparse LiDAR point cloud semantic segmentation framework assisted by learned contextual shape priors. In practice, an initial semantic segmentation (SS) of a single sweep point cloud can be achieved by any appealing network and then flows into the semantic scene completion (SSC) module as the input. By merging multiple frames in the LiDAR sequence as supervision, the optimized SSC module has learned the contextual shape priors from sequential LiDAR data, completing the sparse single sweep point cloud to the dense one. Thus, it inherently improves SS optimization through fully end-to-end training. Besides, a Point-Voxel Interaction (PVI) module is proposed to further enhance the knowledge fusion between SS and SSC tasks, i.e., promoting the interaction of incomplete local geometry of point cloud and complete voxel-wise global structure. Furthermore, the auxiliary SSC and PVI modules can be discarded during inference without extra burden for SS. Extensive experiments confirm that our JS3C-Net achieves superior performance on both SemanticKITTI and SemanticPOSS benchmarks, i.e., 4% and 3% improvement correspondingly."
  },
  "aaai2021_main_learningsemanticcontextfromnormalsamplesforunsupervisedanomalydetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Semantic Context from Normal Samples for Unsupervised Anomaly Detection",
    "authors": [
      "Xudong Yan",
      "Huaidong Zhang",
      "Xuemiao Xu",
      "Xiaowei Hu",
      "Pheng-Ann Heng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16420",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16420/16227",
    "published": "2021-02",
    "summary": "Unsupervised anomaly detection aims to identify data samples that have low probability density from a set of input samples, and only the normal samples are provided for model training. The inference of abnormal regions on the input image requires an understanding of the surrounding semantic context. This work presents a Semantic Context based Anomaly Detection Network, SCADN, for unsupervised anomaly detection by learning the semantic context from the normal samples. To achieve this, we first generate multi-scale striped masks to remove a part of regionsfrom the normal samples, and then train a generative adversarial network to reconstruct the unseen regions. Note that the masks are designed in multiple scales and stripe directions, and various training examples are generated to obtain the rich semantic context . In testing, we obtain an error map by computing the difference between the reconstructed image and the input image for all samples, and infer the abnormal samples based on the error maps. Finally, we perform various experiments on three public benchmark datasets and a new dataset LaceAD collected by us, and show that our method clearly outperforms the current state-of-the-art methods."
  },
  "aaai2021_main_non-autoregressivecoarse-to-finevideocaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Non-Autoregressive Coarse-to-Fine Video Captioning",
    "authors": [
      "Bang Yang",
      "Yuexian Zou",
      "Fenglin Liu",
      "Can Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16421",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16421/16228",
    "published": "2021-02",
    "summary": "It is encouraged to see that progress has been made to bridge videos and natural language. However, mainstream video captioning methods suffer from slow inference speed due to the sequential manner of autoregressive decoding, and prefer generating generic descriptions due to the insufficient training of visual words (e.g., nouns and verbs) and inadequate decoding paradigm. In this paper, we propose a non-autoregressive decoding based model with a coarse-to-fine captioning procedure to alleviate these defects. In implementations, we employ a bi-directional self-attention based network as our language model for achieving inference speedup, based on which we decompose the captioning procedure into two stages, where the model has different focuses. Specifically, given that visual words determine the semantic correctness of captions, we design a mechanism of generating visual words to not only promote the training of scene-related words but also capture relevant details from videos to construct a coarse-grained sentence ``template''. Thereafter, we devise dedicated decoding algorithms that fill in the ``template'' with suitable words and modify inappropriate phrasing via iterative refinement to obtain a fine-grained description. Extensive experiments on two mainstream video captioning benchmarks, i.e., MSVD and MSR-VTT, demonstrate that our approach achieves state-of-the-art performance, generates diverse descriptions, and obtains high inference efficiency."
  },
  "aaai2021_main_learningtoattackreal-worldmodelsforpersonre-identificationviavirtual-guidedmeta-learning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning to Attack Real-World Models for Person Re-identification via Virtual-Guided Meta-Learning",
    "authors": [
      "Fengxiang Yang",
      "Zhun Zhong",
      "Hong Liu",
      "Zheng Wang",
      "Zhiming Luo",
      "Shaozi Li",
      "Nicu Sebe",
      "Shin'ichi Satoh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16422",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16422/16229",
    "published": "2021-02",
    "summary": "Recent advances in person re-identification (re-ID) have led to impressive retrieval accuracy. However, existing re-ID models are challenged by the adversarial examples crafted by adding quasi-imperceptible perturbations. Moreover, re-ID systems face the domain shift issue that training and testing domains are not consistent. In this study, we argue that learning powerful attackers with high universality that works well on unseen domains is an important step in promoting the robustness of re-ID systems. Therefore, we introduce a novel universal attack algorithm called ``MetaAttack'' for person re-ID. MetaAttack can mislead re-ID models on unseen domains by a universal adversarial perturbation. Specifically, to capture common patterns across different domains, we propose a meta-learning scheme to seek the universal perturbation via the gradient interaction between meta-train and meta-test formed by two datasets. We also take advantage of a virtual dataset (PersonX), instead of real ones, to conduct meta-test. This scheme not only enables us to learn with more comprehensive variation factors but also mitigates the negative effects caused by biased factors of real datasets. Experiments on three large-scale re-ID datasets demonstrate the effectiveness of our method in attacking re-ID models on unseen domains. Our final visualization results reveal some new properties of existing re-ID systems, which can guide us in designing a more robust re-ID model. Code and supplemental material are available at \\url{https://github.com/FlyingRoastDuck/MetaAttack_AAAI21}."
  },
  "aaai2021_main_objectrelationattentionforimageparagraphcaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Object Relation Attention for Image Paragraph Captioning",
    "authors": [
      "Li-Chuan Yang",
      "Chih-Yuan Yang",
      "Jane Yung-jen Hsu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16423",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16423/16230",
    "published": "2021-02",
    "summary": "Image paragraph captioning aims to automatically generate a paragraph from a given image. It is an extension of image captioning in terms of generating multiple sentences instead of a single one, and it is more challenging because paragraphs are longer, more informative, and more linguistically complicated.Because a paragraph consists of several sentences, an effective image paragraph captioning method should generate consistent sentences rather than contradictory ones. It is still an open question how to achieve this goal, and for it we propose a method to incorporate objects' spatial coherence into a language-generating model.For every two overlapping objects, the proposed method concatenates their raw visual features to create two directional pair features and learns weights optimizing those pair features as relation-aware object features for a language-generating model. Experimental results show that the proposed network extracts effective object features for image paragraph captioning and achieves promising performance against existing methods."
  },
  "aaai2021_main_adversarialrobustnessthroughdisentangledrepresentations": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Adversarial Robustness through Disentangled Representations",
    "authors": [
      "Shuo Yang",
      "Tianyu Guo",
      "Yunhe Wang",
      "Chang Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16424",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16424/16231",
    "published": "2021-02",
    "summary": "Despite the remarkable empirical performance of deep learning models, their vulnerability to adversarial examples has been revealed in many studies. They are prone to make a susceptible prediction to the input with imperceptible adversarial perturbation. Although recent works have remarkably improved the model's robustness under the adversarial training strategy, an evident gap between the natural accuracy and adversarial robustness inevitably exists. In order to mitigate this problem, in this paper, we assume that the robust and non-robust representations are two basic ingredients entangled in the integral representation. For achieving adversarial robustness, the robust representations of natural and adversarial examples should be disentangled from the non-robust part and the alignment of the robust representations can bridge the gap between accuracy and robustness. Inspired by this motivation, we propose a novel defense method called Deep Robust Representation Disentanglement Network (DRRDN). Specifically, DRRDN employs a disentangler to extract and align the robust representations from both adversarial and natural examples. Theoretical analysis guarantees the mitigation of the trade-off between robustness and accuracy with good disentanglement and alignment performance. Experimental results on benchmark datasets finally demonstrate the empirical superiority of our method."
  },
  "aaai2021_main_cpcganacontrollable3dpointcloudgenerativeadversarialnetworkwithsemanticlabelgenerating": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "CPCGAN: A Controllable 3D Point Cloud Generative Adversarial Network with Semantic Label Generating",
    "authors": [
      "Ximing Yang",
      "Yuan Wu",
      "Kaiyi Zhang",
      "Cheng Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16425",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16425/16232",
    "published": "2021-02",
    "summary": "Generative Adversarial Networks (GAN) are good at generating variant samples of complex data distributions. Generating a sample with certain properties is one of the major tasks in the real-world application of GANs. In this paper, we propose a novel generative adversarial network to generate 3D point clouds from random latent codes, named Controllable Point Cloud Generative Adversarial Network(CPCGAN). A two-stage GAN framework is utilized in CPCGAN and a sparse point cloud containing major structural information is extracted as the middle-level information between the two stages. With their help, CPCGAN has the ability to control the generated structure and generate 3D point clouds with semantic labels for points. Experimental results demonstrate that the proposed CPCGAN outperforms state-of-the-art point cloud GANs."
  },
  "aaai2021_main_r3detrefinedsingle-stagedetectorwithfeaturerefinementforrotatingobject": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object",
    "authors": [
      "Xue Yang",
      "Junchi Yan",
      "Ziming Feng",
      "Tao He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16426",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16426/16233",
    "published": "2021-02",
    "summary": "Rotation detection is a challenging task due to the difficulties of locating the multi-angle objects and separating them effectively from the background. Though considerable progress has been made, for practical settings, there still exist challenges for rotating objects with large aspect ratio, dense distribution and category extremely imbalance. In this paper, we propose an end-to-end refined single-stage rotation detector for fast and accurate object detection by using a progressive regression approach from coarse to fine granularity. Considering the shortcoming of feature misalignment in existing refined single-stage detector, we design a feature refinement module to improve detection performance by getting more accurate features. The key idea of feature refinement module is to re-encode the position information of the current refined bounding box to the corresponding feature points through pixel-wise feature interpolation to realize feature reconstruction and alignment. For more accurate rotation estimation, an approximate SkewIoU loss is proposed to solve the problem that the calculation of SkewIoU is not derivable. Experiments on three popular remote sensing public datasets DOTA, HRSC2016, UCAS-AOD as well as one scene text dataset ICDAR2015 show the effectiveness of our approach. The source code is available at https://github.com/Thinklab-SJTU/R3Det_Tensorflow and is also integrated in our open source rotation detection benchmark: https://github.com/yangxue0827/RotationDetection."
  },
  "aaai2021_main_one-shotfacereenactmentusingappearanceadaptivenormalization": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "One-shot Face Reenactment Using Appearance Adaptive Normalization",
    "authors": [
      "Guangming Yao",
      "Yi Yuan",
      "Tianjia Shao",
      "Shuang Li",
      "Shanqi Liu",
      "Yong Liu",
      "Mengmeng Wang",
      "Kun Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16427",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16427/16234",
    "published": "2021-02",
    "summary": "The paper proposes a novel generative adversarial network for one-shot face reenactment, which can animate a single face image to a different pose-and-expression (provided by a driving image) while keeping its original appearance. The core of our network is a novel mechanism called appearance adaptive normalization, which can effectively integrate the appearance information from the input image into our face generator by modulating the feature maps of the generator using the learned adaptive parameters. Furthermore, we specially design a local net to reenact the local facial components (i.e., eyes, nose and mouth) first, which is a much easier task for the network to learn and can in turn provide explicit anchors to guide our face generator to learn the global appearance and pose-and-expression. Extensive quantitative and qualitative experiments demonstrate the significant efficacy of our model compared with prior one-shot methods."
  },
  "aaai2021_main_acasestudyoftheshortcuteffectsinvisualcommonsensereasoning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Case Study of the Shortcut Effects in Visual Commonsense Reasoning",
    "authors": [
      "Keren Ye",
      "Adriana Kovashka"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16428",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16428/16235",
    "published": "2021-02",
    "summary": "Visual reasoning and question-answering have gathered attention in recent years. Many datasets and evaluation protocols have been proposed; some have been shown to contain bias that allows models to ``cheat'' without performing true, generalizable reasoning. A well-known bias is dependence on language priors (frequency of answers) resulting in the model not looking at the image. We discover a new type of bias in the Visual Commonsense Reasoning (VCR) dataset. In particular we show that most state-of-the-art models exploit co-occurring text between input (question) and output (answer options), and rely on only a few pieces of information in the candidate options, to make a decision. Unfortunately, relying on such superficial evidence causes models to be very fragile. To measure fragility, we propose two ways to modify the validation data, in which a few words in the answer choices are modified without significant changes in meaning. We find such insignificant changes cause models' performance to degrade significantly. To resolve the issue, we propose a curriculum-based masking approach, as a mechanism to perform more robust training. Our method improves the baseline by requiring it to pay attention to the answers as a whole, and is more effective than prior masking strategies."
  },
  "aaai2021_main_instanceminingwithclassfeaturebanksforweaklysupervisedobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Instance Mining with Class Feature Banks for Weakly Supervised Object Detection",
    "authors": [
      "Yufei Yin",
      "Jiajun Deng",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16429",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16429/16236",
    "published": "2021-02",
    "summary": "Recent progress on weakly supervised object detection (WSOD) is characterized by formulating WSOD as a Multiple Instance Learning (MIL) problem and taking online refinement with the selected region proposals from MIL. However, MIL inclines to select the most discriminative part rather than the entire instance as the top-scoring region proposals, which leads to weak localization capability for weakly supervised object detectors. We attribute this problem to the limited intra-class diversity within a single image. Specifically, due to the lack of annotated bounding boxes, the network tends to focus on the most common parts of each class and neglect the diverse parts of objects. To solve the problem, we introduce a novel Instance Mining with Class Feature Banks (IM-CFB) framework, which includes a Class Feature Banks (CFB) module and a Feature Guided Instance Mining (FGIM) algorithm. Concretely, Class Feature Banks (CFB) consist of sub-banks for each class, which are utilized to collect diversity information from a broader view. At the training stage, the RoI features of reliable region proposals are recorded and updated in the CFB. Then, FGIM leverages the features recorded in the CFB to ameliorate the region proposal selection of the MIL branch. Extensive experiments conducted on two publicly available datasets, Pascal VOC 2007 and 2012, demonstrate the effectiveness of our method. More remarkably, our method achieves 54.3% on mAP and 70.7% on CorLoc on Pascal VOC 2007. When further re-trained by a Fast-RCNN detector, we obtain to-date the best reported mAP and CorLoc of 55.8% and 72.2%, respectively."
  },
  "aaai2021_main_multimodalfusionviateacher-studentnetworkforindooractionrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Multimodal Fusion via Teacher-Student Network for Indoor Action Recognition",
    "authors": [
      "Bruce X.B. Yu",
      "Yan Liu",
      "Keith C.C. Chan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16430",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16430/16237",
    "published": "2021-02",
    "summary": "Indoor action recognition plays an important role in modern society, such as intelligent healthcare in large mobile cabin hospitals. With the wide usage of depth sensors like Kinect, multimodal information including skeleton and RGB modalities brings a promising way to improve the performance. However, existing methods are either focusing on a single data modality or failed to take the advantage of multiple data modalities. In this paper, we propose a Teacher-Student Multimodal Fusion (TSMF) model that fuses the skeleton and RGB modalities at the model level for indoor action recognition. In our TSMF, we utilize a teacher network to transfer the structural knowledge of the skeleton modality to a student network for the RGB modality. With extensive experiments on two benchmarking datasets: NTU RGB+D and PKU-MMD, results show that the proposed TSMF consistently performs better than state-of-the-art single modal and multimodal methods. It also indicates that our TSMF could not only improve the accuracy of the student network but also significantly improve the ensemble accuracy."
  },
  "aaai2021_main_ernie-vilknowledgeenhancedvision-languagerepresentationsthroughscenegraphs": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs",
    "authors": [
      "Fei Yu",
      "Jiji Tang",
      "Weichong Yin",
      "Yu Sun",
      "Hao Tian",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16431",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16431/16238",
    "published": "2021-02",
    "summary": "We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. ERNIE-ViL tries to build the detailed semantic connections (objects, attributes of objects and relationships between objects) across vision and language, which are essential to vision-language cross-modal tasks. Utilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and Relationship Prediction tasks in the pre-training phase. Specifically, these prediction tasks are implemented by predicting nodes of different types in the scene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. After pre-training on large scale image-text aligned datasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal downstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these tasks and ranks the first place on the VCR leaderboard with an absolute improvement of 3.7%."
  },
  "aaai2021_main_high-resolutiondeepimagematting": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "High-Resolution Deep Image Matting",
    "authors": [
      "Haichao Yu",
      "Ning Xu",
      "Zilong Huang",
      "Yuqian Zhou",
      "Humphrey Shi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16432",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16432/16239",
    "published": "2021-02",
    "summary": "Image matting is a key technique for image and video editing and composition. Conventionally, deep learning approaches take the whole input image and an associated trimap to infer the alpha matte using convolutional neural networks. Such approaches set state-of-the-arts in image matting; however, they may fail in real-world matting applications due to hardware limitations, since real-world input images for matting are mostly of very high resolution. In this paper, we propose HDMatt, a first deep learning based image matting approach for high-resolution inputs. More concretely, HDMatt runs matting in a patch-based crop-and-stitch manner for high-resolution inputs with a novel module design to address the contextual dependency and consistency issues between different patches. Compared with vanilla patch-based inference which computes each patch independently, we explicitly model the cross-patch contextual dependency with a newly-proposed Cross-Patch Contextual module (CPC) guided by the given trimap. Extensive experiments demonstrate the effectiveness of the proposed method and its necessity for high-resolution inputs. Our HDMatt approach also sets new state-of-the-art performance on Adobe Image Matting and AlphaMatting benchmarks and produce impressive visual results on more real-world high-resolution images."
  },
  "aaai2021_main_cakeschannel-wiseautomatickernelshrinkingforefficient3dnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks",
    "authors": [
      "Qihang Yu",
      "Yingwei Li",
      "Jieru Mei",
      "Yuyin Zhou",
      "Alan Yuille"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16433",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16433/16240",
    "published": "2021-02",
    "summary": "3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene understanding, such as video analysis and volumetric image recognition. However, 3D networks can easily lead to over-parameterization which incurs expensive computation cost. In this paper, we propose Channel-wise Automatic KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard 3D convolutions into a set of economic operations (e.g., 1D, 2D convolutions). Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which enjoys the following benefits: 1) enabling operations deployed in every layer to be heterogeneous, so that they can extract diverse and complementary information to benefit the learning process; and 2) allowing for an efficient and flexible replacement design, which can be generalized to both spatial-temporal and volumetric data. Further, we propose a new search space based on CAKES, so that the configuration can be determined automatically for simplifying 3D networks. CAKES shows superior performance to other methods with similar model size, and it also achieves comparable performance to state-of-the-art methods with much fewer parameters and computational costs on tasks including 3D medical imaging segmentation and video action recognition. Codes and models are available at https://github.com/yucornetto/CAKES"
  },
  "aaai2021_main_structure-consistentweaklysupervisedsalientobjectdetectionwithlocalsaliencycoherence": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Structure-Consistent Weakly Supervised Salient Object Detection with Local Saliency Coherence",
    "authors": [
      "Siyue Yu",
      "Bingfeng Zhang",
      "Jimin Xiao",
      "Eng Gee Lim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16434",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16434/16241",
    "published": "2021-02",
    "summary": "Sparse labels have been attracting much attention in recent years. However, the performance gap between weakly supervised and fully supervised salient object detection methods is huge, and most previous weakly supervised works adopt complex training methods with many bells and whistles. In this work, we propose a one-round end-to-end training approach for weakly supervised salient object detection via scribble annotations without pre/post-processing operations or extra supervision data. Since scribble labels fail to offer detailed salient regions, we propose a local coherence loss to propagate the labels to unlabeled regions based on image features and pixel distance, so as to predict integral salient regions with complete object structures. We design a saliency structure consistency loss as self-consistent mechanism to ensure consistent saliency maps are predicted with different scales of the same image as input, which could be viewed as a regularization technique to enhance the model generalization ability. Additionally, we design an aggregation module (AGGM) to better integrate high-level features, low-level features and global context information for the decoder to aggregate various information. Extensive experiments show that our method achieves a new state-of-the-art performance on six benchmarks (e.g. for the ECSSD dataset: F\u03b2 = 0.8995, E\u03be = 0.9079 and MAE = 0.0489), with an average gain of 4.60% for F-measure, 2.05% for E-measure and 1.88% for MAE over the previous best performing method on this task. Source code is available at http://github.com/siyueyu/SCWSSOD."
  },
  "aaai2021_main_fastandcompactbilinearpoolingbyshiftedrandommaclaurin": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Fast and Compact Bilinear Pooling by Shifted Random Maclaurin",
    "authors": [
      "Tan Yu",
      "Xiaoyun Li",
      "Ping Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16435",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16435/16242",
    "published": "2021-02",
    "summary": "Bilinear pooling has achieved an excellent performance in many computer vision tasks such as fine-grained classification, scene recognition and texture recognition. However, the high-dimension features from bilinear pooling can sometimes be inefficient and prone to over-fitting. Random Maclaurin (RM) is a widely used GPU-friendly approximation method to reduce the dimensionality of bilinear features. However, to achieve good performance, large projection matrices are usually required in practice, making it costly in computation and memory. In this paper, we propose a Shifted Random Maclaurin (SRM) strategy forfast and compact bilinear pooling. With merely negligible extra computational cost, the proposed SRM provides an estimator with a provably smaller variance than RM, which benefits accurate kernel approximation and thus the learning performance. Using a small projection matrix, the proposed SRM achieves a comparable estimation performance as RM based on a large projection matrix, and thus boosts the efficiency.Furthermore, we upgrade the proposed SRM to SRM+ to further improve the efficiency and make the compact bilinear pooling compatible with fast matrix normalization.Fast and Compact Bilinear Network (FCBN) built upon the proposed SRM+ is devised, achieving an end-to-end training. Systematic experimentsconducted on four public datasets demonstrate the effectiveness and efficiency of the proposed FCBN."
  },
  "aaai2021_main_simpleandeffectivestochasticneuralnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Simple and Effective Stochastic Neural Networks",
    "authors": [
      "Tianyuan Yu",
      "Yongxin Yang",
      "Da Li",
      "Timothy Hospedales",
      "Tao Xiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16436",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16436/16243",
    "published": "2021-02",
    "summary": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, robustness against adversarial attack and label noise, and model calibration. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,adversarial defense, learning with label noise, and model calibration."
  },
  "aaai2021_main_learningvisualcontextforgroupactivityrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Visual Context for Group Activity Recognition",
    "authors": [
      "Hangjie Yuan",
      "Dong Ni"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16437",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16437/16244",
    "published": "2021-02",
    "summary": "Group activity recognition aims to recognize an overall activity in a multi-person scene. Previous methods strive to reason on individual features. However, they under-explore the person-specific contextual information, which is significant and informative in computer vision tasks. In this paper, we propose a new reasoning paradigm to incorporate global contextual information. Specifically, we propose two modules to bridge the gap between group activity and visual context. The first is Transformer based Context Encoding (TCE) module, which enhances individual representation by encoding global contextual information to individual features and refining the aggregated information. The second is Spatial-Temporal Bilinear Pooling (STBiP) module. It firstly further explores pairwise relationships for the context encoded individual representation, then generates semantic representations via gated message passing on a constructed spatial-temporal graph. On their basis, we further design a two-branch model that integrates the designed modules into a pipeline. Systematic experiments demonstrate each module's effectiveness on either branch. Visualizations indicate that visual contextual cues can be aggregated globally by TCE. Moreover, our method achieves state-of-the-art results on two widely used benchmarks using only RGB images as input and 2D backbones."
  },
  "aaai2021_main_strokeganreducingmodecollapseinchinesefontgenerationviastrokeencoding": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "StrokeGAN: Reducing Mode Collapse in Chinese Font Generation via Stroke Encoding",
    "authors": [
      "Jinshan Zeng",
      "Qi Chen",
      "Yunxin Liu",
      "Mingwen Wang",
      "Yuan Yao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16438",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16438/16245",
    "published": "2021-02",
    "summary": "The generation of stylish Chinese fonts is an important problem involved in many applications. Most of existing generation methods are based on the deep generative models, particularly, the generative adversarial networks (GAN) based models. However, these deep generative models may suffer from the mode collapse issue, which significantly degrades the diversity and quality of generated results. In this paper, we introduce a one-bit stroke encoding to capture the key mode information of Chinese characters and then incorporate it into CycleGAN, a popular deep generative model for Chinese font generation. As a result we propose an efficient method called StrokeGAN, mainly motivated by the observation that the stroke encoding contains amount of mode information of Chinese characters. In order to reconstruct the one-bit stroke encoding of the associated generated characters, we introduce a stroke-encoding reconstruction loss imposed on the discriminator. Equipped with such one-bit stroke encoding and stroke-encoding reconstruction loss, the mode collapse issue of CycleGAN can be significantly alleviated, with an improved preservation of strokes and diversity of generated characters. The effectiveness of StrokeGAN is demonstrated by a series of generation tasks over nine datasets with different fonts. The numerical results demonstrate that StrokeGAN generally outperforms the state-of-the-art methods in terms of content and recognition accuracies, as well as certain stroke error, and also generates more realistic characters."
  },
  "aaai2021_main_demodalizingfacerecognitionwithsyntheticsamples": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Demodalizing Face Recognition with Synthetic Samples",
    "authors": [
      "Zhonghua Zhai",
      "Pengju Yang",
      "Xiaofeng Zhang",
      "Maji Huang",
      "Haijing Cheng",
      "Xuejun Yan",
      "Chunmao Wang",
      "Shiliang Pu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16439",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16439/16246",
    "published": "2021-02",
    "summary": "Using data generated by generative adversarial networks or three-dimensional (3D) technology for face recognition training is a theoretically reasonable solution to the problems of unbalanced data distributions and data scarcity. However, due to the modal difference between synthetic data and real data, the direct use of data for training often leads to a decrease in the recognition performance, and the effect of synthetic data on recognition remains ambiguous. In this paper, after observing in experiments that modality information has a fixed form, we propose a demodalizing face recognition training architecture for the first time and provide a feasible method for recognition training using synthetic samples. Specifically, three different demodalizing training methods, from implicit to explicit, are proposed. These methods gradually reveal a generated modality that is difficult to quantify or describe. By removing the modalities of the synthetic data, the performance degradation is greatly alleviated. We validate the effectiveness of our approach on various benchmarks of large-scale face recognition and outperform the previous methods, especially in the low FAR range."
  },
  "aaai2021_main_emlightlightingestimationviasphericaldistributionapproximation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "EMLight: Lighting Estimation via Spherical Distribution Approximation",
    "authors": [
      "Fangneng Zhan",
      "Changgong Zhang",
      "Yingchen Yu",
      "Yuan Chang",
      "Shijian Lu",
      "Feiying Ma",
      "Xuansong Xie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16440",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16440/16247",
    "published": "2021-02",
    "summary": "Illumination estimation from a single image is critical in 3D rendering and it has been investigated extensively in the computer vision and computer graphic research community. On the other hand, existing works estimate illumination by either regressing light parameters or generating illumination maps that are often hard to optimize or tend to produce inaccurate predictions. We propose Earth Mover\u2019s Light (EMLight), an illumination estimation framework that leverages a regression network and a neural projector for accurate illumination estimation. We decompose the illumination map into spherical light distribution, light intensity and the ambient term, and define the illumination estimation as a parameter regression task for the three illumination components. Motivated by the Earth Mover's distance, we design a novel spherical mover's loss that guides to regress light distribution parameters accurately by taking advantage of the subtleties of spherical distribution. Under the guidance of the predicted spherical distribution, light intensity and ambient term, the neural projector synthesizes panoramic illumination maps with realistic light frequency. Extensive experiments show that EMLight achieves accurate illumination estimation and the generated relighting in 3D object embedding exhibits superior plausibility and fidelity as compared with state-of-the-art methods."
  },
  "aaai2021_main_universaladversarialperturbationsthroughthelensofdeepsteganographytowardsafourierperspective": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Universal Adversarial Perturbations Through the Lens of Deep Steganography: Towards a Fourier Perspective",
    "authors": [
      "Chaoning Zhang",
      "Philipp Benz",
      "Adil Karjauv",
      "In So Kweon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16441",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16441/16248",
    "published": "2021-02",
    "summary": "The booming interest in adversarial attacks stems from a misalignment between human vision and a deep neural network (DNN), \\ie~a human imperceptible perturbation fools the DNN. Moreover, a single perturbation, often called universal adversarial perturbation (UAP), can be generated to fool the DNN for most images. A similar misalignment phenomenon has also been observed in the deep steganography task, where a decoder network can retrieve a secret image back from a slightly perturbed cover image. We attempt explaining the success of both in a unified manner from the Fourier perspective. We perform task-specific and joint analysis and reveal that (a) frequency is a key factor that influences their performance based on the proposed entropy metric for quantifying the frequency distribution; (b) their success can be attributed to a DNN being highly sensitive to high-frequency content. We also perform feature layer analysis for providing deep insight on model generalization and robustness. Additionally, we propose two new variants of universal perturbations: (1) high-pass UAP (HP-UAP) being less visible to the human eye;(2) Universal Secret Adversarial Perturbation (USAP) that simultaneously achieves attack and hiding."
  },
  "aaai2021_main_spinstructure-preservinginneroffsetnetworkforscenetextrecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition",
    "authors": [
      "Chengwei Zhang",
      "Yunlu Xu",
      "Zhanzhan Cheng",
      "Shiliang Pu",
      "Yi Niu",
      "Fei Wu",
      "Futai Zou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16442",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16442/16249",
    "published": "2021-02",
    "summary": "Arbitrary text appearance poses a great challenge in scene text recognition tasks. Existing works mostly handle with the problem in consideration of the shape distortion, including perspective distortions, line curvature or other style variations. Rectification (i.e., spatial transformers) as the preprocessing stage is one popular approach and extensively studied. However, chromatic difficulties in complex scenes have not been paid much attention on. In this work, we introduce a new learnable geometric-unrelated rectification, Structure-Preserving Inner Offset Network (SPIN), which allows the color manipulation of source data within the network. This differentiable module can be inserted before any recognition architecture to ease the downstream tasks, giving neural networks the ability to actively transform input intensity rather than only the spatial rectification. It can also serve as a complementary module to known spatial transformations and work in both independent and collaborative ways with them. Extensive experiments show the proposed transformation outperforms existing rectification networks and has comparable performance among the state-of-the-arts."
  },
  "aaai2021_main_visualtrackingviahierarchicaldeepreinforcementlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Visual Tracking via Hierarchical Deep Reinforcement Learning",
    "authors": [
      "Dawei Zhang",
      "Zhonglong Zheng",
      "Riheng Jia",
      "Minglu Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16443",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16443/16250",
    "published": "2021-02",
    "summary": "Visual tracking has achieved great progress due to numerous different algorithms. However, deep trackers based on classification or Siamese network still have their specific limitations. In this work, we show how to teach machines to track a generic object in videos like humans, who can use a few search steps to perform tracking. By constructing a Markov decision process in Deep Reinforcement Learning (DRL), our agents can learn to determine hierarchical decisions on tracking mode and motion estimation. To be specific, our Hierarchical DRL framework is composed of a Siamese-based observation network which models the motion information of an arbitrary target, a policy network for mode switch and an actor-critic network for box regression. This tracking strategy is more in line with human behavior paradigm, and is effective and efficient to cope with fast motion, background clutter and large deformations. Extensive experiments on the GOT-10k, OTB-100, UAV-123, VOT and LaSOT tracking benchmarks, demonstrate that the proposed tracker achieves state-of-the-art performance while running in real-time."
  },
  "aaai2021_main_oneformoreselectinggeneralizablesamplesforgeneralizablereidmodel": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "One for More: Selecting Generalizable Samples for Generalizable ReID Model",
    "authors": [
      "Enwei Zhang",
      "Xinyang Jiang",
      "Hao Cheng",
      "Ancong Wu",
      "Fufu Yu",
      "Ke Li",
      "Xiaowei\n      Guo",
      "Feng Zheng",
      "Weishi Zheng",
      "Xing Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16444",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16444/16251",
    "published": "2021-02",
    "summary": "Current training objectives of existing person Re-IDentification (ReID) models only ensure that the loss of the model decreases on selected training batch, with no regards to the performance on samples outside the batch. It will inevitably cause the model to over-fit the data in the dominant position (e.g., head data in imbalanced class, easy samples or noisy samples). The latest resampling methods address the issue by designing specific criterion to select specific samples that trains the model generalize more on certain type of data (e.g., hard samples, tail data),which is not adaptive to the inconsistent real world ReID data distributions.Therefore, instead of simply presuming on what samples are generalizable, this paper proposes a one-for-more training objective that directly takes the generalization ability of selected samples as a loss function and learn a sampler to automatically select generalizable samples. More importantly, our proposed one-for-more based sampler can be seamlessly integrated into the ReID training framework which is able to simultaneously train ReID models and the sampler in an end-to-end fashion. The experimental results show that our method can effectively improve the ReID model training and boost the performance of ReID models."
  },
  "aaai2021_main_ada-segmentautomatedmulti-lossadaptationforpanopticsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Ada-Segment: Automated Multi-loss Adaptation for Panoptic Segmentation",
    "authors": [
      "Gengwei Zhang",
      "Yiming Gao",
      "Hang Xu",
      "Hao Zhang",
      "Zhenguo Li",
      "Xiaodan Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16445",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16445/16252",
    "published": "2021-02",
    "summary": "Panoptic segmentation that unifies instance segmentation and semantic segmentation has recently attracted increasing attention. While most existing methods focus on designing novel architectures, we steer toward a different perspective: performing automated multi-loss adaptation (named Ada-Segment) on the fly to flexibly adjust multiple training losses over the course of training using a controller trained to capture the learning dynamics. This offers a few advantages: it bypasses manual tuning of the sensitive loss combination, a decisive factor for panoptic segmentation; allows to explicitly model the learning dynamics, and reconcile the learning of multiple objectives (up to ten in our experiments); with an end-to-end architecture, it generalizes to different datasets without the need of re-tuning hyperparameters or re-adjusting the training process laboriously. Our Ada-Segment brings 2.7% panoptic quality (PQ) improvement on COCO val split from the vanilla baseline, achieving the state-of-the-art 48.5% PQ on COCO test-dev split and 32.9% PQ on ADE20K dataset. The extensive ablation studies reveal the ever-changing dynamics throughout the training process, necessitating the incorporation of an automated and adaptive learning strategy as presented in this paper."
  },
  "aaai2021_main_simplesingle-networkwithmimickingandpointlearningforbottom-uphumanposeestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "SIMPLE: SIngle-network with Mimicking and Point Learning for Bottom-up Human Pose Estimation",
    "authors": [
      "Jiabin Zhang",
      "Zheng Zhu",
      "Jiwen Lu",
      "Junjie Huang",
      "Guan Huang",
      "Jie Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16446",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16446/16253",
    "published": "2021-02",
    "summary": "The practical application requests both accuracy and efficiency on multi-person pose estimation algorithms. But the high accuracy and fast inference speed are dominated by top-down methods and bottom-up methods respectively. To make a better trade-off between accuracy and efficiency, we propose a novel multi-person pose estimation framework, SIngle-network with Mimicking and Point Learning for Bottom-up Human Pose Estimation (SIMPLE). Specifically, in the training process, we enable SIMPLE to mimic the pose knowledge from the high-performance top-down pipeline, which significantly promotes SIMPLE's accuracy while maintaining its high efficiency during inference. Besides, SIMPLE formulates human detection and pose estimation as a unified point learning framework to complement each other in single-network.This is quite different from previous works where the two tasks may interfere with each other. To the best of our knowledge, both mimicking strategy between different method types and unified point learning are firstly proposed in pose estimation. In experiments, our approach achieves the new state-of-the-art performance among bottom-up methods on the COCO, MPII and PoseTrack datasets. Compared with the top-down approaches, SIMPLE has comparable accuracy and faster inference speed."
  },
  "aaai2021_main_enhancingaudio-visualassociationwithself-supervisedcurriculumlearning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Enhancing Audio-Visual Association with Self-Supervised Curriculum Learning",
    "authors": [
      "Jingran Zhang",
      "Xing Xu",
      "Fumin Shen",
      "Huimin Lu",
      "Xin Liu",
      "Heng Tao Shen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16447",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16447/16254",
    "published": "2021-02",
    "summary": "The recent success of audio-visual representations learning can be largely attributed to their pervasive concurrency property, which can be used as a self-supervision signal and extract correlation information. While most recent works focus on capturing the shared associations between the audio and visual modalities, they rarely consider multiple audio and video pairs at once and pay little attention to exploiting the valuable information within each modality. To tackle this problem,we propose a novel audio-visual representation learning method dubbed self-supervised curriculum learning (SSCL) under the teacher-student learning manner. Specifically, taking advantage of contrastive learning, a two-stage scheme is exploited, which transfers the cross-modal information between teacher and student model as a phased process. The proposed SSCL approach regards the pervasive property of audiovisual concurrency as latent supervision and mutually distills the structure knowledge of visual to audio data. Notably, the SSCL method can learn discriminative audio and visual representations for various downstream applications. Extensive experiments conducted on both action video recognition and audio sound recognition tasks show the remarkably improved performance of the SSCL method compared with the state-of-the-art self-supervised audio-visual representation learning methods."
  },
  "aaai2021_main_unsuperviseddomainadaptationforpersonre-identificationviaheterogeneousgraphalignment": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Unsupervised Domain Adaptation for Person Re-identification via Heterogeneous Graph Alignment",
    "authors": [
      "Minying Zhang",
      "Kai Liu",
      "Yidong Li",
      "Shihui Guo",
      "Hongtao Duan",
      "Yimin Long",
      "Yi Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16448",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16448/16255",
    "published": "2021-02",
    "summary": "Unsupervised person re-identification (re-ID) is becoming increasingly popular due to its power in real-world systems such as public security and intelligent transportation systems. However, the person re-ID task is challenged by the problems of data distribution discrepancy across cameras and lack of label information. In this paper, we propose a coarse-to-fine heterogeneous graph alignment (HGA) method to find cross-camera person matches by characterizing the unlabeled data as a heterogeneous graph for each camera. In the coarse-alignment stage, we assign a projection for each camera and utilize an adversarial learning based method to align coarse-grained node groups from different cameras into a shared space, which consequently alleviates the distribution discrepancy between cameras. In the fine-alignment stage, we exploit potential fine-grained node groups in the shared space and introduce conservative alignment loss functions to constrain the graph aligning process, resulting in reliable pseudo labels as learning guidance. The proposed domain adaptation framework not only improves model generalization on target domain, but also facilitates mining and integrating the potential discriminative information across different cameras. Extensive experiments on benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-arts."
  },
  "aaai2021_main_proactiveprivacy-preservinglearningforretrieval": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Proactive Privacy-preserving Learning for Retrieval",
    "authors": [
      "Peng-Fei Zhang",
      "Zi Huang",
      "Xin-Shun Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16449",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16449/16256",
    "published": "2021-02",
    "summary": "Deep Neural Networks (DNNs) have recently achieved remarkable performance in image retrieval, yet posing great threats to data privacy. On the one hand, one may misuse a deployed DNNs based system to look up data without consent. On the other hand, organizations or individuals would legally or illegally collect data to train high-performance models outside the scope of legitimate purposes. Unfortunately, less effort has been made to safeguard data privacy against malicious uses of DNNs. In this paper, we propose a data-centric Proactive Privacy-preserving Learning (PPL) algorithm for hashing based retrieval, which achieves the protection purpose by employing a generator to transfer the original data into the adversarial data with quasi-imperceptible perturbations before releasing them. When the data source is infiltrated, the adversarial data can confuse menacing retrieval models to make erroneous predictions. Given that the prior knowledge of malicious models is not available, a surrogate retrieval model is instead introduced acting as a fooling target. The framework is trained by a two-player game conducted between the generator and the surrogate model. More specifically, the generator is updated to enlarge the gap between the adversarial data and the original data, aiming to lower the search accuracy of the surrogate model. On the contrary, the surrogate model is trained with the opposing objective that is to maintain the search performance. As a result, an effective and robust adversarial generator is encouraged. Furthermore, to facilitate an effective optimization, a Gradient Reversal Layer (GRL) module is inserted to connect two models, enabling the two-player game in a one-step learning. Extensive experiments on three widely-used realistic datasets prove the effectiveness of the proposed method."
  },
  "aaai2021_main_anovelvisualinterpretabilityfordeepneuralnetworksbyoptimizingactivationmapswithperturbation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "A Novel Visual Interpretability for Deep Neural Networks by Optimizing Activation Maps with Perturbation",
    "authors": [
      "Qinglong Zhang",
      "Lu Rao",
      "Yubin Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16450",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16450/16257",
    "published": "2021-02",
    "summary": "Interpretability has been regarded as an essential component for deploying deep neural networks, in which the saliency-based method is one of the most prevailing interpretable approaches since it can generate individually intuitive heatmaps that highlight parts of the input image that are most important to the decision of the deep networks on a particular classification target. However, heatmaps generated by existing methods either contain little information to represent objects (perturbation-based methods) or cannot effectively locate multi-class objects (activation-based approaches). To address this issue, a two-stage framework for visualizing the interpretability of deep neural networks, called Activation Optimized with Perturbation (AOP), is designed to optimize activation maps generated by general activation-based methods with the help of perturbation-based methods. Finally, in order to obtain better explanations for different types of images, we further present an instance of the AOP framework, Smooth Integrated Gradient-based Class Activation Map (SIGCAM), which proposes a weighted GradCAM by applying the feature map as weight coefficients and employs I-GOS to optimize the base-mask generated by weighted GradCAM. Experimental results on common-used benchmarks, including deletion and insertion tests on ImageNet-1k, and pointing game tests on COCO2017, show that the proposed AOP and SIGCAM outperform the current state-of-the-art methods significantly by generating higher quality image-based saliency maps."
  },
  "aaai2021_main_pointcloudsemanticscenecompletionfromrgb-dimages": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Point Cloud Semantic Scene Completion from RGB-D Images",
    "authors": [
      "Shoulong Zhang",
      "Shuai Li",
      "Aimin Hao",
      "Hong Qin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16451",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16451/16258",
    "published": "2021-02",
    "summary": "In this paper, we devise a novel semantic completion network, called point cloud semantic scene completion network (PCSSC-Net), for indoor scenes solely based on point clouds. Existing point cloud completion networks still suffer from their inability of fully recovering complex structures and contents from global geometric descriptions neglecting semantic hints. To extract and infer comprehensive information from partial input, we design a patch-based contextual encoder to hierarchically learn point-level, patch-level, and scene-level geometric and contextual semantic information with a divide-and-conquer strategy. Consider that the scene semantics afford a high-level clue of constituting geometry for an indoor scene environment, we articulate a semantics-guided completion decoder where semantics could help cluster isolated points in the latent space and infer complicated scene geometry. Given the fact that real-world scans tend to be incomplete as ground truth, we choose to synthesize scene dataset with RGB-D images and annotate complete point clouds as ground truth for the supervised training purpose. Extensive experiments validate that our new method achieves the state-of-the-art performance, in contrast with the current methods applied to our dataset."
  },
  "aaai2021_main_consensusgraphrepresentationlearningforbettergroundedimagecaptioning": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Consensus Graph Representation Learning for Better Grounded Image Captioning",
    "authors": [
      "Wenqiao Zhang",
      "Haochen Shi",
      "Siliang Tang",
      "Jun Xiao",
      "Qiang Yu",
      "Yueting\n      Zhuang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16452",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16452/16259",
    "published": "2021-02",
    "summary": "The contemporary visual captioning models frequently hallucinate objects that are not actually in a scene, due to the visual misclassification or over-reliance on priors that resulting in the semantic inconsistency between the visual information and the target lexical words. The most common way is to encourage the captioning model to dynamically link generated object words or phrases to appropriate regions of the image, i.e., the grounded image captioning (GIC). However, GIC utilizes an auxiliary task (grounding objects) that has not solved the key issue of object hallucination, i.e., the semantic inconsistency. In this paper, we take a novel perspective on the issue above: exploiting the semantic coherency between the visual and language modalities. Specifically, we propose the Consensus Rraph Representation Learning framework (CGRL) for GIC that incorporates a consensus representation into the grounded captioning pipeline. The consensus is learned by aligning the visual graph (e.g., scene graph) to the language graph that consider both the nodes and edges in a graph. With the aligned consensus, the captioning model can capture both the correct linguistic characteristics and visual relevance, and then grounding appropriate image regions further. We validate the effectiveness of our model, with a significant decline in object hallucination (-9% CHAIRi) on the Flickr30k Entities dataset. Besides, our CGRL also evaluated by several automatic metrics and human evaluation, the results indicate that the proposed approach can simultaneously improve the performance of image captioning (+2.9 Cider) andgrounding (+2.3 F1LOC})."
  },
  "aaai2021_main_bowpoolingaplug-and-playunitforfeatureaggregationofpointclouds": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "BoW Pooling: A Plug-and-Play Unit for Feature Aggregation of Point Clouds",
    "authors": [
      "Xiang Zhang",
      "Xiao Sun",
      "Zhouhui Lian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16453",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16453/16260",
    "published": "2021-02",
    "summary": "Point cloud provides a compact and flexible representation for 3D shapes and recently attracts more and more attention due to the increasing demands in practical applications. The major challenge of handling such irregular data is how to achieve the permutation invariance of points in the input. Most of existing methods extract local descriptors that encode the geometry of local structure, followed by a symmetric function to form a global representation. The max pooling usually serves as the symmetric function and shows slight superiority compared to the average pooling. We argue that some discrimination information is inevitably missing when applying the max pooling across all local descriptors. In this paper, we propose the BoW pooling, a plug-and-play unit to substitute the max pooling. Our BoW pooling analyzes the set of local descriptors statistically and generates a histogram that reflects how the primitives in the dictionary constitute the overall geometry. Extensive experiments demonstrate that the proposed Bow pooling is efficient to improve the performance in point cloud classification, shape retrieval and segmentation tasks and outperforms other existing symmetric functions."
  },
  "aaai2021_main_diverseknowledgedistillationforend-to-endpersonsearch": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Diverse Knowledge Distillation for End-to-End Person Search",
    "authors": [
      "Xinyu Zhang",
      "Xinlong Wang",
      "Jia-Wang Bian",
      "Chunhua Shen",
      "Mingyu You"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16454",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16454/16261",
    "published": "2021-02",
    "summary": "Person search aims to localize and identify a specific person from a gallery of images. Recent methods can be categorized into two groups, i.e., two-step and end-to-end approaches. The former views person search as two independent tasks and achieves dominant results using separately trained person detection and re-identification (Re-ID) models. The latter performs person search in an end-to-end fashion. Although the end-to-end approaches yield higher inference efficiency, they largely lag behind those two-step counterparts in terms of accuracy. In this paper, we argue that the gap between the two kinds of methods is mainly caused by the Re-ID sub-networks of end-to-end methods. To this end, we propose a simple yet strong end-to-end network with diverse knowledge distillation to break the bottleneck. We also design a spatial-invariant augmentation to assist model to be invariant to inaccurate detection results. Experimental results on the CUHK-SYSU and PRW datasets demonstrate the superiority of our method against existing approaches -- it achieves on par accuracy with state-of-the-art two-step methods while maintaining high efficiency due to the single joint model. Code is available at: https://git.io/DKD-PersonSearch."
  },
  "aaai2021_main_weaklysupervisedsemanticsegmentationforlarge-scalepointcloud": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Weakly Supervised Semantic Segmentation for Large-Scale Point Cloud",
    "authors": [
      "Yachao Zhang",
      "Zonghao Li",
      "Yuan Xie",
      "Yanyun Qu",
      "Cuihua Li",
      "Tao Mei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16455",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16455/16262",
    "published": "2021-02",
    "summary": "Existing methods for large-scale point cloud semantic segmentation require expensive, tedious and error-prone manual point-wise annotation. Intuitively, weakly supervised training is a direct solution to reduce the labeling costs. However, for weakly supervised large-scale point cloud semantic segmentation, too few annotations will inevitably lead to ineffective learning of network. We propose an effective weakly supervised method containing two components to solve the above problem. Firstly, we construct a pretext task, \\textit{i.e.,} point cloud colorization, with a self-supervised training manner to transfer the learned prior knowledge from a large amount of unlabeled point cloud to a weakly supervised network. In this way, the representation capability of the weakly supervised network can be improved by knowledge from a heterogeneous task. Besides, to generative pseudo label for unlabeled data, a sparse label propagation mechanism is proposed with the help of generated class prototypes, which is used to measure the classification confidence of unlabeled point. Our method is evaluated on large-scale point cloud datasets with different scenarios including indoor and outdoor. The experimental results show the large gain against existing weakly supervised methods and comparable results to fully supervised methods."
  },
  "aaai2021_main_pc-rgnnpointcloudcompletionandgraphneuralnetworkfor3dobjectdetection": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "PC-RGNN: Point Cloud Completion and Graph Neural Network for 3D Object Detection",
    "authors": [
      "Yanan Zhang",
      "Di Huang",
      "Yunhong Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16456",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16456/16263",
    "published": "2021-02",
    "summary": "LiDAR-based 3D object detection is an important task for autonomous driving and current approaches suffer from sparse and partial point clouds caused by distant and occluded objects. In this paper, we propose a novel two-stage framework, namely PC-RGNN, which deals with these challenges by two specific solutions. On the one hand, we introduce a point cloud completion module to recover high-quality proposals of dense points and entire view with original structures preserved. On the other hand, a graph neural network module, is designed, which comprehensively captures relations among points by the local-global attention mechanism as well as the multi-scale graph based context aggregation and substantially strengthens encoded features. Extensive experiments on the KITTI benchmark show that the proposed approach outperforms the previous state-of-the-art baselines by remarkable margins, highlighting its effectiveness."
  },
  "aaai2021_main_efficientlicenseplaterecognitionviaholisticpositionattention": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Efficient License Plate Recognition via Holistic Position Attention",
    "authors": [
      "Yesheng Zhang",
      "Zilei Wang",
      "Jiafan Zhuang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16457",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16457/16264",
    "published": "2021-02",
    "summary": "License plate recognition (LPR) is a fundamental component of various intelligent transportation systems, and is always expected to be accurate and efficient enough in real-world applications. Nowadays, recognition of single character has been sophisticated benefiting from the power of deep learning, and extracting position information for forming a character sequence becomes the main bottleneck of LPR. To tackle this issue, we propose a novel holistic position attention (HPA) in this paper that consists of position network and shared classifier. Specifically, the position network explicitly encodes the character position into the maps of HPA, and then the shared classifier performs the character recognition in a unified and parallel way. Here the extracted features are modulated by the attention maps before feeding into the classifier to yield the final recognition results. Note that our proposed method is end-to-end trainable, character recognition can be concurrently performed, and no post-processing is needed. Thus our LPR system can achieve good effectiveness and efficiency simultaneously. The experimental results on four public datasets, including AOLP, Media Lab, CCPD, and CLPD, well demonstrate the superiority of our method to previous state-of-the-art methods in both accuracy and speed."
  },
  "aaai2021_main_bagoftricksforlong-tailedvisualrecognitionwithdeepconvolutionalneuralnetworks": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks",
    "authors": [
      "Yongshun Zhang",
      "Xiu-Shen Wei",
      "Boyan Zhou",
      "Jianxin Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16458",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16458/16265",
    "published": "2021-02",
    "summary": "In recent years, visual recognition on challenging long-tailed distributions, where classes often exhibit extremely imbalanced frequencies, has made great progress mostly based on various complex paradigms (e.g., meta learning). Apart from these complex methods, simple refinements on training procedures also make contributions. These refinements, also called tricks, are minor but effective, such as adjustments in the data distribution or loss functions. However, different tricks might conflict with each other. If users apply these long-tail related tricks inappropriately, it could cause worse recognition accuracy than expected. Unfortunately, there has not been a scientific guideline of these tricks in the literature. In this paper, we first collect existing tricks in long-tailed visual recognition and then perform extensive and systematic experiments, in order to give a detailed experimental guideline and obtain an effective combination of these tricks. Furthermore, we also propose a novel data augmentation approach based on class activation maps for long-tailed recognition, which can be friendly combined with re-sampling methods and shows excellent results. By assembling these tricks scientifically, we can outperform state-of-the-art methods on four long-tailed benchmark datasets, including ImageNet-LT and iNaturalist 2018. Our code is open-source and available at https://github.com/zhangyongshun/BagofTricks-LT."
  },
  "aaai2021_main_depthprivilegedobjectdetectioninindoorscenesviadeformationhallucination": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Depth Privileged Object Detection in Indoor Scenes via Deformation Hallucination",
    "authors": [
      "Zhijie Zhang",
      "Yan Liu",
      "Junjie Chen",
      "Li Niu",
      "Liqing Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16459",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16459/16266",
    "published": "2021-02",
    "summary": "RGB-D object detection has achieved significant advance, because depth provides complementary geometric information to RGB images. Considering depth images are unavailable in some scenarios, we focus on depth privileged object detection in indoor scenes, where the depth images are only available in the training phase. Under this setting, one prevalent research line is modality hallucination, in which depth image and depth feature are the common choices for hallucinating. In contrast, we choose to hallucinate depth deformation, which is explicit geometric information and efficient to hallucinate. Specifically, we employ the deformable convolution layer with augmented offsets as our deformation module and regard the offsets as geometric deformation, because the offsets enable flexibly sampling over the object and transforming to a canonical shape for ease of detection. In addition, we design a quality-based mechanism to avoid negative transfer of depth deformation. Experimental results and analyses on NYUDv2 and SUN RGB-D demonstrate the effectiveness of our method against the state-of-the-art methods for depth privileged object detection."
  },
  "aaai2021_main_learningflexiblydistributionalrepresentationforlow-quality3dfacerecognition": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Learning Flexibly Distributional Representation for Low-quality 3D Face Recognition",
    "authors": [
      "Zihui Zhang",
      "Cuican Yu",
      "Shuang Xu",
      "Huibin Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16460",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16460/16267",
    "published": "2021-02",
    "summary": "Due to the superiority of using geometric information, 3D Face Recognition (FR) has achieved great successes. Existing methods focus on high-quality 3D FR which is unpractical in real scenarios. Low-quality 3D FR is a more realistic scenario but the low-quality data are born with heavy noises. Therefore, exploring noise-robust low-quality 3D FR methods becomes an urgent and challenging problem. To solve this issue, in this paper, we propose to learn flexibly distributional representation for low-quality 3D FR. Firstly, we introduce the distributional representation for low-quality 3D faces due to that it can weaken the impact of noises. Generally, the distributional representation of a given 3D face is restricted to a specific distribution such as Gaussian distribution. However, the specific distribution may be not up to describing the complex low-quality face. Therefore, we propose to transform this specific distribution to a flexible one via Continuous Normalizing Flow (CNF), which can get rid of the form limitation. This kind of flexible distribution can approximate the latent distribution of the given noisy face more accurately, which further improves accuracy of low-quality 3D FR. Comprehensive experiments show that our proposed method improves both low-quality and cross-quality 3D FR performances on low-quality benchmarks. Furthermore, the improvements are more remarkable on low-quality 3D faces when the intensity of noise increases which indicate the robustness"
  },
  "aaai2021_main_ia-gmadeepbidirectionallearningmethodforgraphmatching": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "IA-GM: A Deep Bidirectional Learning Method for Graph Matching",
    "authors": [
      "Kaixuan Zhao",
      "Shikui Tu",
      "Lei Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16461",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16461/16268",
    "published": "2021-02",
    "summary": "Existingdeeplearningmethodsforgraphmatching(GM)problemsusuallyconsideredaffinitylearningtoassistcombinatorialoptimizationina feedforward pipeline, and parameter learning is executed by back-propagating the gradients of the matching loss. Such a pipeline pays little attention to the possible complementary benefit from the optimization layer to the learning component. In this paper, we overcome the above limitation under a deep bidirectional learning framework.Our method circulates the output of the GM optimization layer to fusewith the input for affinity learning. Such direct feedback enhances the input by afeature enrichment and fusion technique, which exploits andintegrates the global matching patterns from the deviation of the similarity permuted by the current matching estimate. As a result, the circulation enables the learning component to benefit from the optimization process, taking advantage of both global feature andthe embedding result which is calculated by local propagationthrough node-neighbors. Moreover, circulation consistency induces an unsupervised loss that can be implemented individually or jointly to regularize the supervised loss. Experiments on challenging datasets demonstrate the effectiveness of our methods for both supervised learning and unsupervised learning."
  },
  "aaai2021_main_distributionadaptiveint8quantizationfortrainingcnns": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Distribution Adaptive INT8 Quantization for Training CNNs",
    "authors": [
      "Kang Zhao",
      "Sida Huang",
      "Pan Pan",
      "Yinghan Li",
      "Yingya Zhang",
      "Zhenyu Gu",
      "Yinghui Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16462",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16462/16269",
    "published": "2021-02",
    "summary": "Researches have demonstrated that low bit-width (e.g., INT8) quantization can be employed to accelerate the inference process. It makes the gradient quantization very promising since the backward propagation requires approximately twice more computation than forward one. Due to the variability and uncertainty of gradient distribution, a lot of methods have been proposed to attain training stability. However, most of them ignore the channel-wise gradient distributions and the impact of gradients with different magnitudes, resulting in the degradation of final accuracy. In this paper, we propose a novel INT8 quantization training framework for convolutional neural network to address the above issues. Specifically, we adopt Gradient Vectorized Quantization to quantize the gradient, based on the observation that layer-wise gradients contain multiple distributions along the channel dimension. Then, Magnitude-aware Clipping Strategy is introduced by taking the magnitudes of gradients into consideration when minimizing the quantization error, and we present a theoretical derivation to solve the quantization parameters of different distributions. Experimental results on broad range of computer vision tasks, such as image classification, object detection and video classification, demonstrate that the proposed Distribution Adaptive INT8 Quantization training method has achieved almost lossless training accuracy for different backbones, including ResNet, MobileNetV2, InceptionV3, VGG and AlexNet, which is superior to the state-of-the-art techniques. Moreover, we further implement the INT8 kernel that can accelerate the training iteration more than 200% under the latest Turing architecture, i.e., our method excels on both training accuracy and speed."
  },
  "aaai2021_main_context-guidedadaptivenetworkforefficienthumanposeestimation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Context-Guided Adaptive Network for Efficient Human Pose Estimation",
    "authors": [
      "Lei Zhao",
      "Jun Wen",
      "Pengfei Wang",
      "Nenggan Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16463",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16463/16270",
    "published": "2021-02",
    "summary": "Although recent work has achieved great progress in human pose estimation (HPE), most methods show limitations in either inference speed or accuracy. In this paper, we propose a fast and accurate end-to-end HPE method, which is specifically designed to overcome the commonly encountered jitter box, defective box and ambiguous box problems of box-based methods, e.g. Mask R-CNN. Concretely, 1) we propose the ROIGuider to aggregate box instance features from all feature levels under the guidance of global context instance information. Further, 2) the proposed Center Line Branch is equipped with a Dichotomy Extended Area algorithm to adaptively expand each instance box area, and Ambiguity Alleviation strategy to eliminate duplicated keypoints. Finally, 3) to achieve efficient multi-scale feature fusion and real-time inference, we design a novel Trapezoidal Network (TNet) backbone. Experimenting on the COCO dataset, our method achieves 68.1 AP at 25.4 fps, and outperforms Mask-RCNN by 8.9 AP at a similar speed. The competitive performance on the HPE and person instance segmentation tasks over the state-of-the-art models show the promise of the proposed method. The source code will be made available at https://github.com/zlcnup/CGANet."
  },
  "aaai2021_main_epointdaanend-to-endsimulation-to-realdomainadaptationframeworkforlidarpointcloudsegmentation": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "ePointDA: An End-to-End Simulation-to-Real Domain Adaptation Framework for LiDAR Point Cloud Segmentation",
    "authors": [
      "Sicheng Zhao",
      "Yezhen Wang",
      "Bo Li",
      "Bichen Wu",
      "Yang Gao",
      "Pengfei Xu",
      "Trevor\n      Darrell",
      "Kurt Keutzer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16464",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16464/16271",
    "published": "2021-02",
    "summary": "Due to its robust and precise distance measurements, LiDAR plays an important role in scene understanding for autonomous driving. Training deep neural networks (DNNs) on LiDAR data requires large-scale point-wise annotations, which are time-consuming and expensive to obtain. Instead, simulation-to-real domain adaptation (SRDA) trains a DNN using unlimited synthetic data with automatically generated labels and transfers the learned model to real scenarios. Existing SRDA methods for LiDAR point cloud segmentation mainly employ a multi-stage pipeline and focus on feature-level alignment. They require prior knowledge of real-world statistics and ignore the pixel-level dropout noise gap and the spatial feature gap between different domains. In this paper, we propose a novel end-to-end framework, named ePointDA, to address the above issues. Specifically, ePointDA consists of three modules: self-supervised dropout noise rendering, statistics-invariant and spatially-adaptive feature alignment, and transferable segmentation learning. The joint optimization enables ePointDA to bridge the domain shift at the pixel-level by explicitly rendering dropout noise for synthetic LiDAR and at the feature-level by spatially aligning the features between different domains, without requiring the real-world statistics. Extensive experiments adapting from synthetic GTA-LiDAR to real KITTI and SemanticKITTI demonstrate the superiority of ePointDA for LiDAR point cloud segmentation."
  },
  "aaai2021_main_robustlightweightfacialexpressionrecognitionnetworkwithlabeldistributiontraining": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Robust Lightweight Facial Expression Recognition Network with Label Distribution Training",
    "authors": [
      "Zengqun Zhao",
      "Qingshan Liu",
      "Feng Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16465",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16465/16272",
    "published": "2021-02",
    "summary": "This paper presents an efficiently robust facial expression recognition (FER) network, named EfficientFace, which holds much fewer parameters but more robust to the FER in the wild. Firstly, to improve the robustness of the lightweight network, a local-feature extractor and a channel-spatial modulator are designed, in which the depthwise convolution is employed. As a result, the network is aware of local and global-salient facial features. Then, considering the fact that most emotions occur as combinations, mixtures, or compounds of the basic emotions, we introduce a simple but efficient label distribution learning (LDL) method as a novel training strategy. Experiments conducted on realistic occlusion and pose variation datasets demonstrate that the proposed EfficientFace is robust under occlusion and pose variation conditions. Moreover, the proposed method achieves state-of-the-art results on RAF-DB, CAER-S, and AffectNet-7 datasets with accuracies of 88.36%, 85.87%, and 63.70%, respectively, and a comparable result on the AffectNet-8 dataset with an accuracy of 59.89%. The code is public available at https://github.com/zengqunzhao/EfficientFace."
  },
  "aaai2021_main_jointcolor-irrelevantconsistencylearningandidentity-awaremodalityadaptationforvisible-infraredcrossmodalitypersonre-identification": {
    "conf_id": "AAAI2021",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2021",
    "title": "Joint Color-irrelevant Consistency Learning and Identity-aware Modality Adaptation for Visible-infrared Cross Modality Person Re-identification",
    "authors": [
      "Zhiwei Zhao",
      "Bin Liu",
      "Qi Chu",
      "Yan Lu",
      "Nenghai Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16466",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16466/16273",
    "published": "2021-02",
    "summary": "Visible-infrared cross modality person re-identification (VI-ReID) is a core but challenging technology in the 24-hours intelligent surveillance system. How to eliminate the large modality gap lies in the heart of VI-ReID. Conventional methods mainly focus on directly aligning the heterogeneous modalities into the same space. However, due to the unbalanced color information between the visible and infrared images, the features of visible images tend to overfit the clothing color information, which would be harmful to the modality alignment. Besides, these methods mainly align the heterogeneous feature distributions in dataset-level while ignoring the valuable identity information, which may cause the feature misalignment of some identities and weaken the discrimination of features. To tackle above problems, we propose a novel approach for VI-ReID. It learns the color-irrelevant features through the color-irrelevant consistency learning (CICL) and aligns the identity-level feature distributions by the identity-aware modality adaptation (IAMA). The CICL and IAMA are integrated into a joint learning framework and can promote each other. Extensive experiments on two popular datasets SYSU-MM01 and RegDB demonstrate the superiority and effectiveness of our approach against the state-of-the-art methods."
  }
}