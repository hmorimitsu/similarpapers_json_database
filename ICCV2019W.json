{
  "iccv2019_visdrone_howtofullyexploittheabilitiesofaerialimagedetectors": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "How to Fully Exploit The Abilities of Aerial Image Detectors",
    "authors": [
      "Junyi Zhang",
      "Junying Huang",
      "Xuankun Chen",
      "Dongyu Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Zhang_How_to_Fully_Exploit_The_Abilities_of_Aerial_Image_Detectors_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Zhang_How_to_Fully_Exploit_The_Abilities_of_Aerial_Image_Detectors_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Detecting objects in aerial images usually faces two major challenges: (1) detecting difficult targets (e.g., small objects, objects that are interfered by the background, or various orientation of the objects, etc.); (2) the imbalance problem inherent in object detection (e.g., imbalanced quantity in different categories, imbalanced sampling method, or imbalanced loss between classification and localization, etc.). Due to these challenges, detectors are often unable to perform the most effective training and testing. In this paper, we propose a simple but effective framework to address these concerns. First, we propose an adaptive cropping method based on a Difficult Region Estimation Network (DREN) to enhance the detection of the difficult targets, which allows the detector to fully exploit its performance during the testing phase. Second, we use the well-trained DREN to generate more diverse and representative training images, which is effective in enhancing the training set. Besides, in order to alleviate the impact of imbalance during training, we add a balance module in which the IoU balanced sampling method and balanced L1 loss are adopted. Finally, we evaluate our method on two aerial image datasets. Without bells and whistles, our framework achieves 8.0 points and 3.3 points higher Average Precision (AP) than the corresponding baselines on VisDrone and UAVDT, respectively.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_few-shotstructureddomainadaptationforvirtual-to-realsceneparsing": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Few-Shot Structured Domain Adaptation for Virtual-to-Real Scene Parsing",
    "authors": [
      "Junyi Zhang",
      "Ziliang Chen",
      "Junying Huang",
      "Liang Lin",
      "Dongyu Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Zhang_Few-Shot_Structured_Domain_Adaptation_for_Virtual-to-Real_Scene_Parsing_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Zhang_Few-Shot_Structured_Domain_Adaptation_for_Virtual-to-Real_Scene_Parsing_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A structured domain adaptation (SDA) model for virtual-to-real scene parsing, learning to predict visual structure labels in real-world target scenes via mitigating the statistical discrepancy between large scale labeled virtual source and unlabeled real-world target images. But different from the source images drawn from urban simulation platforms, the target images could be expansive and difficult to collect at scale in real-world scenes. Besides, the trend of urbanization constantly changes the visual appearances of target scenes, which encourages SDA models to quickly adapt to new target scenes by merely given very few target images for training. To address the concerns, we attempt to achieve the virtual-to-real scene parsing from a new perspective inspired by few-shot learning. Instead of using a large amount of unlabeled target data used in existing SDA models, our few-shot SDA model takes a few of target real images with semantic labels in each scene, which collaborates with virtual source domain to train a virtual-to-real scene parser. Specifically, our framework is a two-stage adversarial network which contains a scene parser and two discriminators. Based on the data pairing method, our framework can handle the problem of scarce target data well and make full use of the limited semantic labels. We evaluate our method on two suites of virtual-to-real scene parsing setups. The experimental results show that our method exceeds the state-of-the-art SDA model by 7.1% in mIoU on SYNTHIA-to-CITYSCAPES and 4.03% in mIoU on GTA5-to-CITYSCAPES in the case of 1-shot.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_crowdcountingonimageswithscalevariationandisolatedclusters": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Crowd Counting on Images with Scale Variation and Isolated Clusters",
    "authors": [
      "Haoyue Bai",
      "Song Wen",
      "S.-H. Gary Chan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Bai_Crowd_Counting_on_Images_with_Scale_Variation_and_Isolated_Clusters_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Bai_Crowd_Counting_on_Images_with_Scale_Variation_and_Isolated_Clusters_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Crowd counting is to estimate the number of objects (e.g., people or vehicles) in an image of unconstrained congested scenes. Designing a general crowd counting algorithm applicable to a wide range of crowd images is challenging, mainly due to the possibly large variation in object scales and the presence of many isolated small clusters. Previous approaches based on convolution operations with multi-branch architecture are effective for only some narrow bands of scales, and have not captured the long-range contextual relationship due to isolated clustering. To address that, we propose SACANet, a novel scale-adaptive long-range context-aware network for crowd counting. SACANet consists of three major modules: the pyramid contextual module which extracts long-range contextual information and enlarges the receptive field, a scale-adaptive self-attention multi-branch module to attain high scale sensitivity and detection accuracy of isolated clusters, and a hierarchical fusion module to fuse multi-level self-attention features. With group normalization, SACANet achieves better optimality in the training process. We have conducted extensive experiments using the VisDrone2019 People dataset, the VisDrone2019 Vehicle dataset, and some other challenging benchmarks. As compared with the state-of-the-art methods, SACANet is shown to be effective, especially for extremely crowded conditions with diverse scales and scattered clusters, and achieves much lower MAE as compared with baselines.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_learningcascadedcontext-awareframeworkforrobustvisualtracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Learning Cascaded Context-Aware Framework for Robust Visual Tracking",
    "authors": [
      "Ding Ma",
      "Xiangqian Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Ma_Learning_Cascaded_Context-Aware_Framework_for_Robust_Visual_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Ma_Learning_Cascaded_Context-Aware_Framework_for_Robust_Visual_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Context information on each corner of the whole image is useful for visual tracking. However, some trackers may not be able to model such information, this will result in suboptimal performance. To directly model fully context information is intractable since first the region of the foreground is relatively small, the structure of foreground is lost for some part by straightforwardly aware. Second, the target may share a similar structure of the surrounding distractors. To this end, we propose a cascaded context-aware framework based on two networks that progressively model the foreground and background of the various targets over time. The first network pays attention to the most discriminative information within the whole context and coarser structure of the target, the second network focuses on the self-structure information of the target. Depending on the output of these two networks-the final context-aware map, we can generate the bounding box of the target flexibly. Extensive experiments on 3 popular benchmarks demonstrate the robustness of the proposed CAT tracker.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_slimyolov3narrower,fasterandbetterforreal-timeuavapplications": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "SlimYOLOv3: Narrower, Faster and Better for Real-Time UAV Applications",
    "authors": [
      "Pengyi Zhang",
      "Yunxin Zhong",
      "Xiaoqiong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Zhang_SlimYOLOv3_Narrower_Faster_and_Better_for_Real-Time_UAV_Applications_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Zhang_SlimYOLOv3_Narrower_Faster_and_Better_for_Real-Time_UAV_Applications_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Drones or general Unmanned Aerial Vehicles (UAVs), endowed with computer vision function by on-board cameras and embedded systems, have become popular in a wide range of applications. However, real-time scene parsing through object detection running on a UAV platform is very challenging, due to limited memory and computing power of embedded devices. To deal with these challenges, in this paper we propose to learn efficient deep object detectors through channel pruning of convolutional layers. To this end, we enforce channel-level sparsity of convolutional layers by imposing L1 regularization on channel scaling factors and prune less informative feature channels to obtain \"slim\" object detectors. Based on such approach, we present SlimYOLOv3 with fewer trainable parameters and floating point operations (FLOPs) in comparison of original YOLOv3 as a promising solution for real-time object detection on UAVs. We evaluate SlimYOLOv3 on VisDrone2018-Det benchmark dataset; compelling results are achieved by SlimYOLOv3 in comparison of unpruned counterpart, including90.8% decrease of FLOPs,92.0% decline of parameter size, running2 times faster and comparable detection accuracy as YOLOv3. Experimental results with different pruning ratios consistently verify that proposed SlimYOLOv3 with narrower structure are more efficient, faster and better than YOLOv3, and thus are more suitable for real-time object detection on UAVs. Our codes are made publicly available at https://github.com/PengyiZhang/SlimYOLOv3.\r",
    "code_link": "https://github.com/PengyiZhang/SlimYOLOv3"
  },
  "iccv2019_visdrone_multitargettrackingfromdronesbylearningfromgeneralizedgraphdifferences": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Multi Target Tracking from Drones by Learning from Generalized Graph Differences",
    "authors": [
      "Hakan Ardo",
      "Mikael Nilsson"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Ardo_Multi_Target_Tracking_from_Drones_by_Learning_from_Generalized_Graph_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Ardo_Multi_Target_Tracking_from_Drones_by_Learning_from_Generalized_Graph_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Formulating the multi object tracking problem as a network flow optimization problem is a popular choice. The weights of such network flow problem can be learnt efficiently from training data using a recently introduced concept called Generalized Graph Differences (GGD). This allows a general tracker implementation to be specialized to drone videos by training it on the VisDrone dataset. Two modifications to the original GGD is introduced in this paper and a result with an average precision of 23.09 on the test set of VisDrone 2019 was achieved.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_i-siamimprovingsiamesetrackerwithdistractorssuppressionandlong-termstrategies": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "i-Siam: Improving Siamese Tracker with Distractors Suppression and Long-Term Strategies",
    "authors": [
      "Wei Ren Tan",
      "Shang-Hong Lai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Tan_i-Siam_Improving_Siamese_Tracker_with_Distractors_Suppression_and_Long-Term_Strategies_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Tan_i-Siam_Improving_Siamese_Tracker_with_Distractors_Suppression_and_Long-Term_Strategies_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently, Siamese Network (SiamFC) has attracted much attention due to its fast tracking capability. Despite many improvements introduced, its accuracy is still far from human performance. In this work, we show that there are several design flaws in SiamFC tracker. In particular, the negative signals produced by SiamFC lead to noisy response map. In addition, background noises prevent SiamFC from extracting clean features from the template. To suppress these distractions, first we propose a negative signal suppression approach such that irrelevant features are deactivated. Secondly, we demonstrate that image-level suppression is also important in maximizing the tracking accuracy in addition to the existing feature-level suppression. With better detection sensitivity, we further propose a Diverse Multi-Template approach for appearance adaptation while reducing the risk of template drifting during long-term tracking. In our experiments, we conduct extensive ablation studies to demonstrate the effectiveness of the proposed components. Our tracker named improved Siamese (i-Siam) Tracker is able to achieve state-of-the-art results on UAV123, OTB-100, OxUvA, and TLP datasets compared to the existing trackers. Nonetheless, our tracker runs in real time, which is around 43 FPS.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_spatialattentionformulti-scalefeaturerefinementforobjectdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Spatial Attention for Multi-Scale Feature Refinement for Object Detection",
    "authors": [
      "Haoran Wang",
      "Zexin Wang",
      "Meixia Jia",
      "Aijin Li",
      "Tuo Feng",
      "Wenhua Zhang",
      "Licheng Jiao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Wang_Spatial_Attention_for_Multi-Scale_Feature_Refinement_for_Object_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Wang_Spatial_Attention_for_Multi-Scale_Feature_Refinement_for_Object_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Scale variation is one of the primary challenges in the object detection, existing in both inter-class and intra-class instances, especially on the drone platform. The latest methods focus on feature pyramid for detecting objects at different scales. In this work, we propose two techniques to refine multi-scale features for detecting various-scale instances in FPN-based Network. A Receptive Field Expansion Block (RFEB) is designed to increase the receptive field size for high-level semantic features, then the generated features are passed through a Spatial-Refinement Module (SRM) to repair the spatial details of multi-scale objects in images before summation by the lateral connection. To evaluate its effectiveness, we conduct experiments on VisDrone2019 benchmark dataset and achieve impressive improvement. Meanwhile, results on PASCAL VOC and MS COCO datasets show that our model is able to reach the competitive performance.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_real-timeaerialsuspiciousanalysis(asana)systemfortheidentificationandre-identificationofsuspiciousindividualsusingthebayesianscatternethybrid(bsh)network": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Real-Time Aerial Suspicious Analysis (ASANA) System for the Identification and Re-Identification of Suspicious Individuals using the Bayesian ScatterNet Hybrid (BSH) Network",
    "authors": [
      "Amarjot Singh",
      "Kranthi Kiran GV",
      "Onkar Harsh",
      "Rishav Kumar",
      "Koushlendra Singh Rajput",
      "Chandra S S Vamsi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Singh_Real-Time_Aerial_Suspicious_Analysis_ASANA_System_for_the_Identification_and_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Singh_Real-Time_Aerial_Suspicious_Analysis_ASANA_System_for_the_Identification_and_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video monitoring and safety systems have been used to keep track of hostiles, conduct border control operations as well as to monitor the suspicious entities in public spaces. However, these systems are inadequate for the monitoring of large crowds due to the limited field of view of cameras. This paper introduces the Aerial Suspicious Analysis (ASANA) System for the Identification and Re-Identification of suspicious Individuals in large public areas using the Bayesian ScatterNet Hybrid (BSH) Network. The BSH network first estimates the human pose in each frame. Next, a batch of frames is used by the Bayesian 3D ResNext to identify individuals with suspicious postures. The system can also re-identify the identified suspicious individuals as they tend to move after committing the suspicious event. The proposed architecture is advantageous as it can learn meaningful representations quickly using the ScatterNet with fewer labelled examples. This is of great importance as real-life annotated training samples are hard to collect, especially for these applications. The pose estimation, suspicious individual identification, and re-identification performance of the proposed framework is compared with the state-of-the-art techniques. The proposed dataset is also made public which may encourage other researchers who are interested in using the deep learning technique for aerial visual crowd monitoring.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_anindoorcrowddetectionnetworkframeworkbasedonfeatureaggregationmoduleandhybridattentionselectionmodule": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "An Indoor Crowd Detection Network Framework Based on Feature Aggregation Module and Hybrid Attention Selection Module",
    "authors": [
      "Wenxiang Shen",
      "Pinle Qin",
      "Jianchao Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Shen_An_Indoor_Crowd_Detection_Network_Framework_Based_on_Feature_Aggregation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Shen_An_Indoor_Crowd_Detection_Network_Framework_Based_on_Feature_Aggregation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we present an indoor crowd detection network framework based on feature aggregation module and hybrid attention selection module (HSFA2Net). In order to better provide the details needed for small scale pupulation detection, we propose a novel feature aggregation module (FAM), which uses the idea of fusion and decomposition to aggregate contextual feature information. Since the indoor population feature and background feature overlap and the classification boundaries are not obvious, the proposed improved hybrid attention selection module (HASM) combines the selection mechanism with the previously proposed mixed attention module. Ultimately, we implement an indoor crowd detection network framework and achieve a recall rate of 0.92 and an F1 score of 0.92 on a public dataset SCUT-HEAD.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_deepadaptivefusionnetworkforhighperformancergbttracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Deep Adaptive Fusion Network for High Performance RGBT Tracking",
    "authors": [
      "Yuan Gao",
      "Chenglong Li",
      "Yabin Zhu",
      "Jin Tang",
      "Tao He",
      "Futian Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Gao_Deep_Adaptive_Fusion_Network_for_High_Performance_RGBT_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Gao_Deep_Adaptive_Fusion_Network_for_High_Performance_RGBT_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Due to the complementarity of RGB and thermal data, RGBT tracking has received more and more attention in recent years because it can effectively solve the degradation of tracking performance in dark environments and bad weather conditions. How to effectively fuse the information from RGB and thermal modality is the key to give full play to their complementarities for effective RGBT tracking. In this paper, we propose a high performance RGBT tracking framework based on a novel deep adaptive fusion network, named DAFNet. Our DAFNet consists of a recursive fusion chain that could adaptively integrate all layer features in an end-to-end manner. Due to simple yet effective operations in DAFNet, our tracker is able to reach the near-real-time speed. Comparing with the state-of-the-art trackers on two public datasets, our DAFNet tracker achieves the outstanding performance and yields a new state-of-the-art in RGBT tracking.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_rrnetahybriddetectorforobjectdetectionindrone-capturedimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "RRNet: A Hybrid Detector for Object Detection in Drone-Captured Images",
    "authors": [
      "Changrui Chen",
      "Yu Zhang",
      "Qingxuan Lv",
      "Shuo Wei",
      "Xiaorui Wang",
      "Xin Sun",
      "Junyu Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Chen_RRNet_A_Hybrid_Detector_for_Object_Detection_in_Drone-Captured_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Chen_RRNet_A_Hybrid_Detector_for_Object_Detection_in_Drone-Captured_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Objects captured by UAVs and drones in city scenes usually come in various sizes and are extremely dense. Therefore, we propose a hybrid detector, called RRNet, for object detection in such challenging tasks. We mix up the anchor-free detectors with a re-regression module to construct the detector. The discard of prior anchors released our model from the difficult task on bounding-box size regression so that we achieved a better performance in multi-scale object detection in the dense scene. The anchor-free based detector firstly generates the coarse boxes. A re-regression module is then applied on the coarse predictions to produce accurate bounding boxes. In addition, we introduce an adaptive resampling augmentation strategy to logically augment the data. Our experiments demonstrate that RRNet significantly outperforms all the state-of-the-art detectors on VisDrone2018 dataset. We are runner-up to the ICCV VisDrone2019 Object Detection in Images Challenge, and we achieve the best AP50, AR10, and AR100. Source code will be published on our official website in due course.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_accuracyandlong-termtrackingviaoverlapmaximizationintegratedwithmotioncontinuity": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Accuracy and Long-Term Tracking via Overlap Maximization Integrated with Motion Continuity",
    "authors": [
      "Wenhua Zhang",
      "Haoran Wang",
      "Zhongjian Huang",
      "Yuxuan Li",
      "Jinliu Zhou",
      "Licheng Jiao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Zhang_Accuracy_and_Long-Term_Tracking_via_Overlap_Maximization_Integrated_with_Motion_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Zhang_Accuracy_and_Long-Term_Tracking_via_Overlap_Maximization_Integrated_with_Motion_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The baseline is ATOM which aims at solving the problem of accurate target state estimation by proposing a novel tracking architecture. The architecture consists of dedicated target estimation and classification components. Classification component is trained online to guarantee high discriminative power in the presence of distractors. Target estimation is performed by the IoU-predictor network inspired by the IoU-Net which was recently proposed for object detection as an alternative to typical anchor-based bounding box regression techniques. In this work, we further enhance the performance of ATOM by embedding Squeeze-and-Excitation (SE) blocks into IoU-Net in ATOM to recalibrate useful features and suppress useless features and obtain ATOMFR. To solve the abnormal changes in the target box in ATOMFR, we add the Relocation Module on ATOMFR and get ATOMFR (RL). To solve the occlusion problem, we introduce the Inference Module into ATOMFR (RL) and obtain ATOMFR (RL + InF). Experimental results on VisDrone2019-SOT test set demonstrate the state-of-the-art performance of ATOMFR (RL + InF) compared with several existed trackers and it ranks the second place among all competitors.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_denseandsmallobjectdetectioninuavvisionbasedoncascadenetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Dense and Small Object Detection in UAV Vision Based on Cascade Network",
    "authors": [
      "Xindi Zhang",
      "Ebroul Izquierdo",
      "Krishna Chandramouli"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Zhang_Dense_and_Small_Object_Detection_in_UAV_Vision_Based_on_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Zhang_Dense_and_Small_Object_Detection_in_UAV_Vision_Based_on_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the development of Unmanned Aerial Vehicles, drones are being deployed in a number of commercial and civil government applications ranging from remote surveillance and infrastructure maintenance among others. However, processing the videos captured by drones for the extracting meaningful information is hindered by multitude of challenges that include, the appearance of small objects, changes in viewpoint of these objects, illumination changes, large-scale resolution of the captured video, occlusion and truncation. Addressing these challenges, there is a critical need to develop algorithms that is able to efficiently process the videos that can result in robust detection and recognition of small objects. In this paper, we propose a novel processing pipeline, that brings together several key contributions including (i) the introduction of DeForm convolution layers within backbone; (ii) use of the interleaved cascade architecture; (iii) data augmentation process based on crop functionality and (iv) multi-model fusion of sub-category detection networks. The proposed approach has been exhaustively benchmarked against VisDrone-DET object detection dataset, which includes 10,209 images for training, validation and testing. The evaluation of the proposed approach has resulted in 22.61 average precision on the test-challenge set in VisDrone-DET 2019.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_patch-levelaugmentationforobjectdetectioninaerialimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Patch-Level Augmentation for Object Detection in Aerial Images",
    "authors": [
      "Sungeun Hong",
      "Sungil Kang",
      "Donghyeon Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Hong_Patch-Level_Augmentation_for_Object_Detection_in_Aerial_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Hong_Patch-Level_Augmentation_for_Object_Detection_in_Aerial_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Object detection in specific views (e.g., top view, road view, and aerial view) suffers from a lack of dataset, which causes class imbalance and difficulties of covering hard examples. In order to handle these issues, we propose a hard chip mining method that makes the ratio of each class balanced and generates hard examples that are efficient for model training. First, we generate multi-scale chips to train object detector. Next, we extract object patches from the dataset to construct an object pool; then those patches are used to augment the dataset. By this augmentation, we can overcome the class imbalance problem. After that, we perform inference with the trained detector on augmented images, then generate hard chips from misclassified regions. Finally, we train the final detector by both normal and hard chips. The proposed method achieves superior results on VisDrone dataset both qualitatively and quantitatively. Also, our model is ranked 3rd in VisDrone-DET2019 challenge (http://aiskyeye.com/).\r",
    "code_link": ""
  },
  "iccv2019_visdrone_multi-objecttrackinghierarchicallyinvisualdatatakenfromdrones": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Multi-Object Tracking Hierarchically in Visual Data Taken From Drones",
    "authors": [
      "Siyang Pan",
      "Zhihang Tong",
      "Yanyun Zhao",
      "Zhicheng Zhao",
      "Fei Su",
      "Bojin Zhuang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Pan_Multi-Object_Tracking_Hierarchically_in_Visual_Data_Taken_From_Drones_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Pan_Multi-Object_Tracking_Hierarchically_in_Visual_Data_Taken_From_Drones_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Visual understanding tasks on the drone platform have gained considerable attention recently due to the rapid development of drones. In this paper, we present a hierarchical multi-target tracker (HMTT) for visual data taken from drones. Our approach is specifically directed against sequences shot from drone's view with several stages hierarchically performed. The detector detects objects taken from different viewing angles and the detections are filtered to ensure the correctness. Moreover, we propose a method to locate the frames in the case of camera's fast move by two-norm of the homography matrix. Based on that, performance on Multi-Object Tracking is improved with the involvement of Single Object Tracking and a re-identification subnet. Our method participated in the Multi-Object Tracking Challenge (Task 4) of VisDrone2019 benchmark and achieved state-of-the-art performance.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_real-timeuavtrackingbasedonpsrstability": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Real-Time UAV Tracking Based on PSR Stability",
    "authors": [
      "Yong Wang",
      "Lu Ding",
      "Robert Laganiere"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Wang_Real-Time_UAV_Tracking_Based_on_PSR_Stability_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Wang_Real-Time_UAV_Tracking_Based_on_PSR_Stability_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, a real-time unmanned aerial vehicle (UAV) tracking method is proposed. This approach builds a target representation from histograms of oriented gradient (HOG) and ColorNames features. Correlation filters have been utilized in tracking recently because of their high efficiency. To better fuse the tracking results from different features, peak-to-sidelobe ratio (PSR) is employed to evaluate robustness of our trackers. A stability measure is proposed, based on the PSR values computed over a short period of time which is also used to predict object position. Additionally, we show that the proposed PSR stability enables our tracking method to be robust to various appearance variations. The method is carried out on five UAV tracking datasets and achieves appealing results comparable to state-of-the-art trackers but at a lower computational complexity.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_vision-basedonlinelocalizationandtrajectorysmoothingforfixed-winguavtrackingamovingtarget": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Vision-Based Online Localization and Trajectory Smoothing for Fixed-Wing UAV Tracking a Moving Target",
    "authors": [
      "Yong Zhou",
      "Dengqing Tang",
      "Han Zhou",
      "Xiaojia Xiang",
      "Tianjia Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Zhou_Vision-Based_Online_Localization_and_Trajectory_Smoothing_for_Fixed-Wing_UAV_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Zhou_Vision-Based_Online_Localization_and_Trajectory_Smoothing_for_Fixed-Wing_UAV_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Using unmanned aerial vehicle (UAV) to estimate and predict the position and motion of the ground target has been widely focused on in many computer vision tasks. This work aims to address the development of a vision-based ground target localization and estimation for a fixed-wing UAV. Limited by the lightweight onboard processor, it is conflicting with the need for online onboard operation and the computing resource limitation of the platform. In this paper, we develop a practical approach to recover dynamic targets based on extended Kalman filter (EKF) for localization and locally weighted regression for trajectory smoothing. Our methods run online in real time with the only data up to the current timestep. The flight experiment results show the effective tracking and localization of the ground moving target.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_multipleobjecttrackingwithmotionandappearancecues": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Multiple Object Tracking with Motion and Appearance Cues",
    "authors": [
      "Weiqiang Li",
      "Jiatong Mu",
      "Guizhong Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Li_Multiple_Object_Tracking_with_Motion_and_Appearance_Cues_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Li_Multiple_Object_Tracking_with_Motion_and_Appearance_Cues_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Due to better video quality and higher frame rate, the performance of multiple object tracking issues has been greatly improved in recent years. However, in real application scenarios, camera motion and noisy per frame detection results degrade the performance of trackers significantly. High-speed and high-quality multiple object trackers are still in urgent demand. In this paper, we propose a new multiple object tracker following the popular tracking-by-detection scheme. We tackle the camera motion problem with an optical flow network and utilize an auxiliary tracker to deal with the missing detection problem. Besides, we use both the appearance and motion information to improve the matching quality. The experi-mental results on the VisDrone-MOT dataset show that our approach can improve the performance of multiple object tracking significantly while achieving a high efficiency.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_flowguidedshort-termtrackerswithcascadedetectionforlong-termtracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "Flow Guided Short-Term Trackers with Cascade Detection for Long-Term Tracking",
    "authors": [
      "Han Wu",
      "Xueyuan Yang",
      "Yong Yang",
      "Guizhong Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Wu_Flow_Guided_Short-Term_Trackers_with_Cascade_Detection_for_Long-Term_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Wu_Flow_Guided_Short-Term_Trackers_with_Cascade_Detection_for_Long-Term_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Object tracking has been studied for decades, but most of the existing works are focused on the short-term tracking. For a long sequence, the object is often fully occluded or out of view for a long time, and existing short-term object tracking algorithms often lose the target, and it is difficult to re-catch the target even if it reappears again. In this paper a novel long-term object tracking algorithm flow_MDNet_RPN is proposed, in which a tracking result judgement module and a detection module are added to the short-term object tracking algorithm. Experiments show that the proposed long-term tracking algorithm is effective to the problem of target disappearance.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_anovelspatialandtemporalcontext-awareapproachfordrone-basedvideoobjectdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "A Novel Spatial and Temporal Context-Aware Approach for Drone-Based Video Object Detection",
    "authors": [
      "Zhaoliang Pi",
      "Yanchao Lian",
      "Xier Chen",
      "Yinan Wu",
      "Yingping Li",
      "Licheng Jiao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Pi_A_Novel_Spatial_and_Temporal_Context-Aware_Approach_for_Drone-Based_Video_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Pi_A_Novel_Spatial_and_Temporal_Context-Aware_Approach_for_Drone-Based_Video_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Nowadays, with the advent of Unmanned Aerial Vehicles (UAV), drones equipped with cameras have been fast deployed to a wide range of applications. Consequently, automatic and effective object detection plays an important role in understanding and analysis of visual data collected from the drones, which could be further applied to civilian and military fields. However, various challenges still exist in object detection of drone-based videos, such as defocus, motion blur, occlusion and various variations (e.g., illumination, view and size), leaving too weak visual clues for successful detections. In this paper, we propose a novel approach for object detection in drone-based videos, which includes the multi-model fusion detection, an efficient tracker and a new evaluation method for confidence of the track, and the false positive analysis with scene-level context information and inferences. The experimental results on VisDrone2018-VID [44] dataset demonstrate the effectiveness of the proposed approach.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_visdrone-mot2019thevisionmeetsdronemultipleobjecttrackingchallengeresults": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "VisDrone-MOT2019: The Vision Meets Drone Multiple Object Tracking Challenge Results",
    "authors": [
      "Longyin Wen",
      "Pengfei Zhu",
      "Dawei Du",
      "Xiao Bian",
      "Haibin Ling",
      "Qinghua Hu",
      "Jiayu Zheng",
      "Tao Peng",
      "Xinyao Wang",
      "Yue Zhang",
      "Liefeng Bo",
      "Hailin Shi",
      "Rui Zhu",
      "Ajit Jadhav",
      "Bing Dong",
      "Brejesh Lall",
      "Chang Liu",
      "Chunhui Zhang",
      "Dong Wang",
      "Feng Ni",
      "Filiz Bunyak",
      "Gaoang Wang",
      "Guizhong Liu",
      "Guna Seetharaman",
      "Guorong Li",
      "Hakan Ardo",
      "Haotian Zhang",
      "Hongyang Yu",
      "Huchuan Lu",
      "Jenq-Neng Hwang",
      "Jiatong Mu",
      "Jinrong Hu",
      "Kannappan Palaniappan",
      "Long Chen",
      "Lu Ding",
      "Martin Lauer",
      "Mikael Nilsson",
      "Noor M. Al-Shakarji",
      "Prerana Mukherjee",
      "Qingming Huang",
      "Robert Laganiere",
      "Shuhao Chen",
      "Siyang Pan",
      "Vinay Kaushik",
      "Wei Shi",
      "Wei Tian",
      "Weiqiang Li",
      "Xin Chen",
      "Xinyu Zhang",
      "Yanting Zhang",
      "Yanyun Zhao",
      "Yong Wang",
      "Yuduo Song",
      "Yuehan Yao",
      "Zhaotang Chen",
      "Zhenyu Xu",
      "Zhibin Xiao",
      "Zhihang Tong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Wen_VisDrone-MOT2019_The_Vision_Meets_Drone_Multiple_Object_Tracking_Challenge_Results_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Wen_VisDrone-MOT2019_The_Vision_Meets_Drone_Multiple_Object_Tracking_Challenge_Results_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The Vision Meets Drone Multiple Object Tracking (MOT) Challenge 2019 is the second annual activity focusing on evaluating multi-object tracking algorithms on drones, held in conjunction with the 17-th International Conference on Computer Vision (ICCV 2019). Results of 12 submitted MOT algorithms on the collected drone-based dataset are presented. Meanwhile, we also report the results of 6 state-of-the-art MOT algorithms, and provide a comprehensive analysis and discussion of the results. The results of all submissions are publicly available at the website: http://www.aiskyeye.com/. The challenge results show that MOT on drones is far from being solved. We believe the challenge can largely boost the research and development in MOT on drone platforms.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_visdrone-sot2019thevisionmeetsdronesingleobjecttrackingchallengeresults": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "VisDrone-SOT2019: The Vision Meets Drone Single Object Tracking Challenge Results",
    "authors": [
      "Dawei Du",
      "Pengfei Zhu",
      "Longyin Wen",
      "Xiao Bian",
      "Haibin Ling",
      "Qinghua Hu",
      "Jiayu Zheng",
      "Tao Peng",
      "Xinyao Wang",
      "Yue Zhang",
      "Liefeng Bo",
      "Hailin Shi",
      "Rui Zhu",
      "Bo Han",
      "Chunhui Zhang",
      "Guizhong Liu",
      "Han Wu",
      "Hao Wen",
      "Haoran Wang",
      "Jiaqing Fan",
      "Jie Chen",
      "Jie Gao",
      "Jie Zhang",
      "Jinghao Zhou",
      "Jinliu Zhou",
      "Jinwang Wang",
      "Jiuqing Wan",
      "Josef Kittler",
      "Kaihua Zhang",
      "Kaiqi Huang",
      "Kang Yang",
      "Kangkai Zhang",
      "Lianghua Huang",
      "Lijun Zhou",
      "Lingling Shi",
      "Lu Ding",
      "Ning Wang",
      "Peng Wang",
      "Qintao Hu",
      "Robert Laganiere",
      "Ruiyan Ma",
      "Ruohan Zhang",
      "Shanrong Zou",
      "Shengwei Zhao",
      "Shengyang Li",
      "Shengyin Zhu",
      "Shikun Li",
      "Shiming Ge",
      "Shiyu Xuan",
      "Tianyang Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Du_VisDrone-SOT2019_The_Vision_Meets_Drone_Single_Object_Tracking_Challenge_Results_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Du_VisDrone-SOT2019_The_Vision_Meets_Drone_Single_Object_Tracking_Challenge_Results_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The Vision Meets Drone (VisDrone2019) Single Object Tracking challenge is the second annual research activity focusing on evaluating single-object tracking algorithms on drones, held in conjunction with the International Conference on Computer Vision (ICCV 2019). The VisDrone-SOT2019 Challenge goes beyond its VisDrone-SOT2018 predecessor by introducing 25 more challenging sequences for long-term tracking. We evaluate and discuss the results of 22 participating algorithms and 19 state-of-the-art trackers on the collected dataset. The challenge results are publicly available at the website: http://www.aiskyeye.com/. WeexpecttheVisDrone-SOTchallenge to boost the research in single object tracking field.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_visdrone-det2019thevisionmeetsdroneobjectdetectioninimagechallengeresults": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "VisDrone-DET2019: The Vision Meets Drone Object Detection in Image Challenge Results",
    "authors": [
      "Dawei Du",
      "Pengfei Zhu",
      "Longyin Wen",
      "Xiao Bian",
      "Haibin Lin",
      "Qinghua Hu",
      "Tao Peng",
      "Jiayu Zheng",
      "Xinyao Wang",
      "Yue Zhang",
      "Liefeng Bo",
      "Hailin Shi",
      "Rui Zhu",
      "Aashish Kumar",
      "Aijin Li",
      "Almaz Zinollayev",
      "Anuar Askergaliyev",
      "Arne Schumann",
      "Binjie Mao",
      "Byeongwon Lee",
      "Chang Liu",
      "Changrui Chen",
      "Chunhong Pan",
      "Chunlei Huo",
      "Da Yu",
      "DeChun Cong",
      "Dening Zeng",
      "Dheeraj Reddy Pailla",
      "Di Li",
      "Dong Wang",
      "Donghyeon Cho",
      "Dongyu Zhang",
      "Furui Bai",
      "George Jose",
      "Guangyu Gao",
      "Guizhong Liu",
      "Haitao Xiong",
      "Hao Qi",
      "Haoran Wang",
      "Heqian Qiu",
      "HongLiang Li",
      "Huchuan Lu",
      "Ildoo Kim",
      "Jaekyum Kim",
      "Jane Shen",
      "Jihoon Lee",
      "Jing Ge",
      "Jingjing Xu",
      "Jingkai Zhou",
      "Jonas Meier",
      "Jun Won Choi",
      "Junhao Hu",
      "Junyi Zhang",
      "Junying Huang",
      "Kaiqi Huang",
      "Keyang Wang",
      "Lars Sommer",
      "Lei Jin",
      "Lei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Du_VisDrone-DET2019_The_Vision_Meets_Drone_Object_Detection_in_Image_Challenge_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Du_VisDrone-DET2019_The_Vision_Meets_Drone_Object_Detection_in_Image_Challenge_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently, automatic visual data understanding from drone platforms becomes highly demanding. To facilitate the study, the Vision Meets Drone Object Detection in Image Challenge is held the second time in conjunction with the 17-th International Conference on Computer Vision (ICCV 2019), focuses on image object detection on drones. Results of 33 object detection algorithms are presented. For each participating detector, a short description is provided in the appendix. Our goal is to advance the state-of-the-art detection algorithms and provide a comprehensive evaluation platform for them. The evaluation protocol of the VisDrone-DET2019 Challenge and the comparison results of all the submitted detectors on the released dataset are publicly available at the website: http: //www.aiskyeye.com/. The results demonstrate that there still remains a large room for improvement for object detection algorithms on drones.\r",
    "code_link": ""
  },
  "iccv2019_visdrone_visdrone-vid2019thevisionmeetsdroneobjectdetectioninvideochallengeresults": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VISDrone",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Vision Meets Drones: A Challenge",
    "title": "VisDrone-VID2019: The Vision Meets Drone Object Detection in Video Challenge Results",
    "authors": [
      "Pengfei Zhu",
      "Dawei Du",
      "Longyin Wen",
      "Xiao Bian",
      "Haibin Ling",
      "Qinghua Hu",
      "Tao Peng",
      "Jiayu Zheng",
      "Xinyao Wang",
      "Yue Zhang",
      "Liefeng Bo",
      "Hailin Shi",
      "Rui Zhu",
      "Bing Dong",
      "Dheeraj Reddy Pailla",
      "Feng Ni",
      "Guangyu Gao",
      "Guizhong Liu",
      "Haitao Xiong",
      "Jing Ge",
      "Jingkai Zhou",
      "Jinrong Hu",
      "Lin Sun",
      "Long Chen",
      "Martin Lauer",
      "Qiong Liu",
      "Sai Saketh Chennamsetty",
      "Ting Sun",
      "Tong Wu",
      "Varghese Alex Kollerathu",
      "Wei Tian",
      "Weida Qin",
      "Xier Chen",
      "Xingjie Zhao",
      "Yanchao Lian",
      "Yinan Wu",
      "Ying Li",
      "Yingping Li",
      "Yiwen Wang",
      "Yuduo Song",
      "Yuehan Yao",
      "Yunfeng Zhang",
      "Zhaoliang Pi",
      "Zhaotang Chen",
      "Zhenyu Xu",
      "Zhibin Xiao",
      "Zhipeng Luo",
      "Ziming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VISDrone/Zhu_VisDrone-VID2019_The_Vision_Meets_Drone_Object_Detection_in_Video_Challenge_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VISDrone/Zhu_VisDrone-VID2019_The_Vision_Meets_Drone_Object_Detection_in_Video_Challenge_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video object detection has drawn great attention recently. The Vision Meets Drone Object Detection in Video Challenge 2019 (VisDrone-VID2019) is held to advance the state-of-the-art in video object detection for videos captured by drones. Specifically, there are 13 teams participating the challenge. We also report the results of 6 state-of-the-art detectors on the collected dataset. A short description is provided in the appendix for each participating detector. We present the analysis and discussion of the challenge results. Both the dataset and the challenge results are publicly available at the challenge website: http://www.aiskyeye.com/.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_count,cropandrecognisefine-grainedrecognitioninthewild": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "Count, Crop and Recognise: Fine-Grained Recognition in the Wild",
    "authors": [
      "Max Bain",
      "Arsha Nagrani",
      "Daniel Schofield",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Bain_Count_Crop_and_Recognise_Fine-Grained_Recognition_in_the_Wild_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Bain_Count_Crop_and_Recognise_Fine-Grained_Recognition_in_the_Wild_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The goal of this paper is to label all the animal individuals present in every frame of a video. Unlike previous methods that have principally concentrated on labelling face tracks, we aim to label individuals even when their faces are not visible. We make the following contributions: (i) we introduce a 'Count, Crop and Recognise' (CCR) multi-stage recognition process for frame level labelling. The Count and Recognise stages involve specialised CNNs for the task, and we show that this simple staging gives a substantial boost in performance; (ii) we compare the recall using frame based labelling to both face and body track based labelling, and demonstrate the advantage of frame based with CCR for the specified goal; (iii) we introduce a new dataset for chimpanzee recognition in the wild; and (iv) we apply a high-granularity visualisation technique to further understand the learned CNN features for the recognition of chimpanzee individuals.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_geo-awarenetworksforfine-grainedrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "Geo-Aware Networks for Fine-Grained Recognition",
    "authors": [
      "Grace Chu",
      "Brian Potetz",
      "Weijun Wang",
      "Andrew Howard",
      "Yang Song",
      "Fernando Brucher",
      "Thomas Leung",
      "Hartwig Adam"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Chu_Geo-Aware_Networks_for_Fine-Grained_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Chu_Geo-Aware_Networks_for_Fine-Grained_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Fine-grained recognition distinguishes among categories with subtle visual differences. In order to differentiate between these challenging visual categories, it is helpful to leverage additional information. Geolocation is a rich source of additional information that can be used to improve fine-grained classification accuracy, but has been understudied. Our contributions to this field are twofold. First, to the best of our knowledge, this is the first paper which systematically examined various ways of incorporating geolocation information into fine-grained image classification through the use of geolocation priors, post-processing or feature modulation. Secondly, to overcome the situation where no fine-grained dataset has complete geolocation information, we release two fine-grained datasets with geolocation by providing complementary information to existing popular datasets - iNaturalist and YFCC100M. By leveraging geolocation information we improve top-1 accuracy in iNaturalist from 70.1% to 79.0% for a strong baseline image-only model. Comparing several models, we found that best performance was achieved by a post-processing model that consumed the output of the image-only baseline alongside geolocation. However, for a resource-constrained model (MobileNetV2), performance was better with a feature modulation model that trains jointly over pixels and geolocation: accuracy increased from 59.6% to 72.2%. Our work makes a strong case for incorporating geolocation information in fine-grained recognition models for both server and on-device.\r",
    "code_link": "https://github.com/visipedia/fg"
  },
  "iccv2019_cvwc_greatapedetectioninchallengingjunglecameratrapfootageviaattention-basedspatialandtemporalfeatureblending": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "Great Ape Detection in Challenging Jungle Camera Trap Footage via Attention-Based Spatial and Temporal Feature Blending",
    "authors": [
      "Xinyu Yang",
      "Majid Mirmehdi",
      "Tilo Burghardt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Yang_Great_Ape_Detection_in_Challenging_Jungle_Camera_Trap_Footage_via_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Yang_Great_Ape_Detection_in_Challenging_Jungle_Camera_Trap_Footage_via_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose the first multi-frame video object detection framework trained to detect great apes. It is applicable to challenging camera trap footage in complex jungle environments and extends a traditional feature pyramid architecture by adding self-attention driven feature blending in both the spatial as well as the temporal domain. We demonstrate that this extension can detect distinctive species appearance and motion signatures despite significant partial occlusion. We evaluate the framework using 500 camera trap videos of great apes from the Pan African Programme containing 180K frames, which we manually annotated with accurate per-frame animal bounding boxes. These clips contain significant partial occlusions, challenging lighting, dynamic backgrounds, and natural camouflage effects. We show that our approach performs highly robustly and significantly outperforms frame-based detectors. We also perform detailed ablation studies and a validation on the full ILSVRC 2015 VID data corpus to demonstrate wider applicability at adequate performance levels. We conclude that the framework is ready to assist human camera trap inspection efforts. We publish code, weights, and ground truth annotations with this paper.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_elpephantsafine-graineddatasetforelephantre-identification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "ELPephants: A Fine-Grained Dataset for Elephant Re-Identification",
    "authors": [
      "Matthias Korschens",
      "Joachim Denzler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Korschens_ELPephants_A_Fine-Grained_Dataset_for_Elephant_Re-Identification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Korschens_ELPephants_A_Fine-Grained_Dataset_for_Elephant_Re-Identification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Despite many possible applications, machine learning and computer vision approaches are very rarely utilized in biodiversity monitoring. One reason for this might be that automatic image analysis in biodiversity research often poses a unique set of challenges, some of which are not commonly found in many popular datasets. Thus, suitable image datasets are necessary for the development of appropriate algorithms tackling these challenges. In this paper we introduce the ELPephants dataset, a re-identification dataset, which contains 276 elephant individuals in 2078 images following a long-tailed distribution. It offers many different challenges, like fine-grained differences between the individuals, inferring a new view on the elephant from only one training side, aging effects on the animals and large differences in skin color. We also present a baseline approach, which is a system using a YOLO object detector, feature extraction of ImageNet features and discrimination using a support vector machine. This system achieves a top-1 accuracy of 56% and top-10 accuracy of 80% on the ELPephants dataset.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_deepbees-buildingandscalingconvolutionalneuronalnetsforfastandlarge-scalevisualmonitoringofbeehives": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "DeepBees - Building and Scaling Convolutional Neuronal Nets For Fast and Large-Scale Visual Monitoring of Bee Hives",
    "authors": [
      "Julian Marstaller",
      "Frederic Tausch",
      "Simon Stock"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Marstaller_DeepBees_-_Building_and_Scaling_Convolutional_Neuronal_Nets_For_Fast_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Marstaller_DeepBees_-_Building_and_Scaling_Convolutional_Neuronal_Nets_For_Fast_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The decline of bee populations is a global trend and a severe threat to the ecosystem as well as to pollinator-dependent industries. Factor analysis and preventive measures are based on snapshot information. Information about the health state of a hive is infrequently acquired and remains labor-intensive and costly. In this paper, we describe a system that enables near-time, scalable, and cost-efficient monitoring of beehives using computer vision and deep learning. The systems pipeline consists of four major components. First, hardware at the hive gate is capturing the in and out streams of bees. Secondly, an on-edge inference for bee localization and tracking of single entities. Thirdly, a cloud infrastructure for device and data management with near-time sampling from devices. Fourthly, a cloud-hosted deep convolutional neuronal net inferring entity-based health insights. This MultiNet architecture, which we named DeepBees, is the main focus of this paper. We describe the development of the architecture and the acquisition of training data. The overall system is currently deployed by apic.ai and monitors 49 beehives in Karlsruhe in the south of Germany.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_learningdeepfeaturesforgiantpandagenderclassificationusingfaceimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "Learning Deep Features for Giant Panda Gender Classification using Face Images",
    "authors": [
      "Hongnian Wang",
      "Han Su",
      "Peng Chen",
      "Rong Hou",
      "Zhihe Zhang",
      "Weiyi Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Wang_Learning_Deep_Features_for_Giant_Panda_Gender_Classification_using_Face_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Wang_Learning_Deep_Features_for_Giant_Panda_Gender_Classification_using_Face_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Giant panda (panda) has lived on earth for at least eight million years and is known as the living fossil. It is also a vulnerable species which requires urgent protection. It is essential to conduct population survey collecting information of their population, density, age structure, and gender ratio so as to design protection schemes and measure their effectiveness. However, it is challenging to accurately and timely obtain gender ratio of pandas because their pelage lacks distinguishable gender patterns and panda is sparsely distributed population in large habitats. All current approaches rely heavily on manual collection of samples in the wild, which are time consuming, costly, or even dangerous. With the widely deployed camera traps, if the gender of pandas can be determined from images, it is possible to monitor panda gender ratio in different regions in real-time. However, no such study was done. In this paper, a deep learning method is developed to study the distinctiveness of panda face for gender classification, in which the largest panda image dataset with 6,549 panda face images collected from 100 male and 121 female pandas is established. The experimental results show that panda faces contain some gender information, although they look very similar to human vision.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_pose-guidedcomplementaryfeatureslearningforamurtigerre-identification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "Pose-Guided Complementary Features Learning for Amur Tiger Re-Identification",
    "authors": [
      "Ning Liu",
      "Qijun Zhao",
      "Nan Zhang",
      "Xinhua Cheng",
      "Jianing Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Liu_Pose-Guided_Complementary_Features_Learning_for_Amur_Tiger_Re-Identification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Liu_Pose-Guided_Complementary_Features_Learning_for_Amur_Tiger_Re-Identification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Re-identifying different animal individuals is of significant importance to animal behavior and ecology research and protecting endangered species. This paper focuses on Amur tiger re-identification (re-ID) using computer vision (CV) technology. State-of-the-art CV-based Amur tiger re-ID methods extract local features from different body parts of tigers based on stand-alone pose estimation methods. Consequently, they are limited by the pose estimation accuracy and suffer from self-occluded body parts. Instead of estimating elaborated body poses, this paper simplifies tiger poses as right-headed or left-headed and utilizes this information as an auxiliary pose classification task to supervise the feature learning. To further enhance the feature discriminativeness, this paper learns multiple complementary features by steering different feature extraction network branches towards different regions of the tiger body via erasing activated regions from input tiger images. By fusing the pose-guided complementary features, this paper effectively improves the Amur tiger re-ID accuracy as demonstrated in the evaluation experiments on two test datasets. The code and data of this paper are publicly available at https://github.com/liuning-scu-cn/AmurTigerReID.\r",
    "code_link": "https://github.com/liuning-scu-cn/AmurTigerReID"
  },
  "iccv2019_cvwc_ahybridapproachtotigerre-identification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "A Hybrid Approach to Tiger Re-Identification",
    "authors": [
      "Ankita Shukla",
      "connor anderson",
      "Gullal Sigh Cheema",
      "Pei Gao",
      "Suguru Onda",
      "Divyam Anshumaan",
      "Saket Anand",
      "Ryan Farrell"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Shukla_A_Hybrid_Approach_to_Tiger_Re-Identification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Shukla_A_Hybrid_Approach_to_Tiger_Re-Identification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Visual data analytics is increasingly becoming an important part of wildlife monitoring and conservation strategies. In this work, we discuss our solution to the image-based Amur tiger re-identification (Re-ID) challenge hosted by the CVWC Workshop at ICCV 2019. Various factors like poor quality images, lighting and pose variations, and limited images per identity make tiger Re-ID a difficult task for deep learning models. Consequently, we propose to utilize both deep learning and traditional SIFT descriptor-based matching for tiger re-identification. The proposed deep network is based on a DenseNet model, fine-tuned by minimizing a classification cross-entropy loss regularized by a pairwise KL-divergence loss that promotes better semantically discriminative features. We also utilize several data transformations to improve the model's robustness and generalization across views and image quality variations. We establish the efficacy of our approach on the 'Plain Re-ID' challenge task by reporting results on the pre-cropped tiger Re-ID dataset. To further test our Re-ID model's robustness to detection quality, we also report results on the 'Wild Re-ID' task, which incorporates learning a tiger detection model. We show that our model is able to perform well on both the plain and wild Re-ID tasks. Code will be available at https://github.com/FGVC/DelPro.\r",
    "code_link": "https://github.com/FGVC/DelPro"
  },
  "iccv2019_cvwc_astrongbaselinefortigerre-idanditsbagoftricks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "A Strong Baseline for Tiger Re-ID and its Bag of Tricks",
    "authors": [
      "Jiwen Yu",
      "Haibo Su",
      "Junnan Liu",
      "Zhizheng Yang",
      "Zhouyangzi Zhang",
      "Yixin Zhu",
      "Lu Yang",
      "Bingliang Jiao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Yu_A_Strong_Baseline_for_Tiger_Re-ID_and_its_Bag_of_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Yu_A_Strong_Baseline_for_Tiger_Re-ID_and_its_Bag_of_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " As an instance-level recognition task, person re-identification methods always calculate local features by horizontal pooling. It is based on a simple assumption that pedestrians always stand vertically. But as to wildlife re-identification task, we can not make similar assumption since the various view-angles of wildlife. In this paper, we propose a novel dynamic partial matching method. In our module, global feature learning benefits greatly from local feature learning, which performs an alignment/matching by flipping local features and calculating the shortest path between them. Besides the partial matching method, we also consider a series of data augmentation methods such as flip as new id, random whitening, random crop and so on. And we also use an example sampling strategy, i.e., hard negative mining, for training. In addition, we ensemble the models with different backbones and epochs using imagenet pre-trained models. Extensive experiments validate the superiority of our method for tiger Re-ID. Code has been released at https://github.com/vvictoryuki/tiger_reid_pytorch.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_fastandefficientmodelforreal-timetigerdetectioninthewild": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "Fast and Efficient Model for Real-Time Tiger Detection In The Wild",
    "authors": [
      "Orest Kupyn",
      "Dmitry Pranchuk"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Kupyn_Fast_and_Efficient_Model_for_Real-Time_Tiger_Detection_In_The_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Kupyn_Fast_and_Efficient_Model_for_Real-Time_Tiger_Detection_In_The_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The highest accuracy object detectors to date are based either on a two-stage approach such as Fast R-CNN or one-stage detectors such as Retina-Net or SSD with deep and complex backbones. In this paper we present TigerNet - simple yet efficient FPN based network architecture for Amur Tiger Detection in the wild. The model has 600k parameters, requires 0.071 GFLOPs per image and can run on the edge devices (smart cameras) in near real time. In addition, we introduce a two-stage semi-supervised learning via pseudo-labelling learning approach to distill the knowledge from the larger networks. For ATRW-ICCV 2019 tiger detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods.\r",
    "code_link": ""
  },
  "iccv2019_cvwc_part-poseguidedamurtigerre-identification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVWC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Wildlife Conservation",
    "title": "Part-Pose Guided Amur Tiger Re-Identification",
    "authors": [
      "Cen Liu",
      "Rong Zhang",
      "Lijun Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVWC/Liu_Part-Pose_Guided_Amur_Tiger_Re-Identification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVWC/Liu_Part-Pose_Guided_Amur_Tiger_Re-Identification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we present our solution to tiger re-identification (re-ID) in both the plain and the wild tracks in the 2019 Computer Vision for Wild life Conservation Challenge (CVWC2019). We introduce a novel part-pose guided framework for the tiger re-ID task, which consists of two part streams and one full stream based on the pose characteristics of tiger. Considering missing and inaccurate pose annotations, the two part streams are used as a regulator to guide the full stream in learning and aligning the local features in the training stage. We only use the learnt full stream for the tiger re-ID task in the inference stage. The proposed model has the advantage that despite requiring pose information at training time it is not needed during inference, so it is particularly suitable for tiger re-ID in the wild. Our proposed method outperforms the state-of-the-art and finished top in both the PlainID and WildID competitions at CVWC2019. The source of code will be public available at https://github.com/LcenArthas/CVWC2019-Amur-Tiger-Re-ID\r",
    "code_link": ""
  },
  "iccv2019_vrmi_domain-agnosticlearningwithanatomy-consistentembeddingforcross-modalityliversegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Domain-Agnostic Learning With Anatomy-Consistent Embedding for Cross-Modality Liver Segmentation",
    "authors": [
      "Junlin Yang",
      "Nicha C. Dvornek",
      "Fan Zhang",
      "Juntang Zhuang",
      "Julius Chapiro",
      "MingDe Lin",
      "James S. Duncan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Yang_Domain-Agnostic_Learning_With_Anatomy-Consistent_Embedding_for_Cross-Modality_Liver_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Yang_Domain-Agnostic_Learning_With_Anatomy-Consistent_Embedding_for_Cross-Modality_Liver_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Domain Adaptation (DA) has the potential to greatly help the generalization of deep learning models. However, the current literature usually assumes to transfer the knowledge from the source domain to a specific known target domain. Domain Agnostic Learning (DAL) proposes a new task of transferring knowledge from the source domain to data from multiple heterogeneous target domains. In this work, we propose the Domain-Agnostic Learning framework with Anatomy-Consistent Embedding (DALACE) that works on both domain-transfer and task-transfer to learn a disentangled representation, aiming to not only be invariant to different modalities but also preserve anatomical structures for the DA and DAL tasks in cross-modality liver segmentation. We validated and compared our model with state-of-the-art methods, including CycleGAN, Task Driven Generative Adversarial Network (TD-GAN), and Domain Adaptation via Disentangled Representations (DADR). For the DA task, our DALACE model outperformed CycleGAN, TD-GAN, and DADR with DSC of 0.847 compared to 0.721, 0.793 and 0.806. For the DAL task, our model improved the performance with DSC of 0.794 from 0.522, 0.719 and 0.742 by CycleGAN, TD-GAN, and DADR. Further, we visualized the success of disentanglement, which added human interpretability of the learned meaningful representations. Through ablation analysis, we specifically showed the concrete benefits of disentanglement for downstream tasks and the role of supervision for better disentangled representation with segmentation consistency to be invariant to domains with the proposed Domain-Agnostic Module (DAM) and to preserve anatomical information with the proposed Anatomy-Preserving Module (APM).\r",
    "code_link": ""
  },
  "iccv2019_vrmi_unimodal-uniformconstrainedwassersteintrainingformedicaldiagnosis": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Unimodal-Uniform Constrained Wasserstein Training for Medical Diagnosis",
    "authors": [
      "Xiaofeng Liu",
      "Xu Han",
      "Yukai Qiao",
      "Yi Ge",
      "Site Li",
      "Jun Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Liu_Unimodal-Uniform_Constrained_Wasserstein_Training_for_Medical_Diagnosis_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Liu_Unimodal-Uniform_Constrained_Wasserstein_Training_for_Medical_Diagnosis_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The labels in medical diagnosis task are usually discrete and successively distributed. For example, the Diabetic Retinopathy Diagnosis (DR) involves five health risk levels: no DR (0), mild DR (1), moderate DR (2), severe DR (3) and proliferative DR (4). This labeling system is common for medical disease. Previous methods usually construct a multi-binary-classification task or propose some re-parameter schemes in the output unit. In this paper, we target on this task from the perspective of loss function. More specifically, the Wasserstein distance is utilized as an alternative, explicitly incorporating the inter-class correlations by pre-defining its ground metric. Then, the ground metric which serves as a linear, convex or concave increasing function w.r.t. the Euclidean distance in a line is explored from an optimization perspective. Meanwhile, this paper also proposes of constructing the smoothed target labels that model the inlier and outlier noises by using a unimodal-uniform mixture distribution. Different from the one-hot setting, the smoothed label endues the computation of Wasserstein distance with more challenging features. With either one-hot or smoothed target label, this paper systematically concludes the practical closed-form solution. We evaluate our method on several medical diagnosis tasks (e.g., Diabetic Retinopathy and Ultrasound Breast dataset) and achieve state-of-the-art performance.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_deepmultiresolutioncellularcommunitiesforsemanticsegmentationofmulti-gigapixelhistologyimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Deep Multiresolution Cellular Communities for Semantic Segmentation of Multi-Gigapixel Histology Images",
    "authors": [
      "Sajid Javed",
      "Arif Mahmood",
      "Naoufel Werghi",
      "Nasir Rajpoot"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Javed_Deep_Multiresolution_Cellular_Communities_for_Semantic_Segmentation_of_Multi-Gigapixel_Histology_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Javed_Deep_Multiresolution_Cellular_Communities_for_Semantic_Segmentation_of_Multi-Gigapixel_Histology_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Tissue phenotyping in cancer histology images is a fundamental step in computational pathology. Automatic tools for tissue phenotyping assist pathologists for digital profiling of the tumor microenvironment. Recently, deep learning and classical machine learning methods have been proposed for tissue phenotyping. However, these methods do not integrate the cellular community interaction features which present biological significance in tissue phenotyping context. In this paper, we propose to exploit deep multiresolution cellular communities for tissue phenotyping from multi-level cell graphs and show that such communities offer better performance compared to the deep learning and texture-based methods. We propose to use deep features extracted from two distinct layers of a deep neural network at the cell-level, in order to construct cellular graphs encoding cellular interactions at multiple scales. From these graphs, we extract cellular interaction-based features, which are then employed to construct patch-level graphs. Multiresolution communities are detected by considering the patch-level graphs as layers of multi-level graphs, and also by proposing novel objective function based on non-negative matrix factorization. We report results of our experiments on two datasets for colon cancer tissue phenotyping and demonstrate excellent performance of the proposed algorithm as compared to current state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_kneelkneeanatomicallandmarklocalizationusinghourglassnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "KNEEL: Knee Anatomical Landmark Localization Using Hourglass Networks",
    "authors": [
      "Aleksei Tiulpin",
      "Iaroslav Melekhov",
      "Simo Saarakkala"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Tiulpin_KNEEL_Knee_Anatomical_Landmark_Localization_Using_Hourglass_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Tiulpin_KNEEL_Knee_Anatomical_Landmark_Localization_Using_Hourglass_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper addresses the challenge of localization of anatomical landmarks in knee X-ray images at different stages of osteoarthritis (OA). Landmark localization can be viewed as regression problem, where the landmark position is directly predicted by using the region of interest or even full-size images leading to large memory footprint, especially in case of high resolution medical images. In this work, we propose an efficient deep neural networks framework with an hourglass architecture utilizing a soft-argmax layer to directly predict normalized coordinates of the landmark points. We provide an extensive evaluation of different regularization techniques and various loss functions to understand their influence on the localization performance. Furthermore, we introduce the concept of transfer learning from low-budget annotations, and experimentally demonstrate that such approach is improving the accuracy of landmark localization. Compared to the prior methods, we validate our model on two datasets that are independent from the train data and assess the performance of the method for different stages of OA severity. The proposed approach demonstrates better generalization performance compared to the current state-of-the-art.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_deeplimadeeplearningbasedlesionidentificationinmammograms": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "DeepLIMa: Deep Learning Based Lesion Identification in Mammograms",
    "authors": [
      "Zhenjie Cao",
      "Zhicheng Yang",
      "Xiaoyan Zhuo",
      "Ruei-Sung Lin",
      "Shibin Wu",
      "Lingyun Huang",
      "Mei Han",
      "Yanbo Zhang",
      "Jie Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Cao_DeepLIMa_Deep_Learning_Based_Lesion_Identification_in_Mammograms_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Cao_DeepLIMa_Deep_Learning_Based_Lesion_Identification_in_Mammograms_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Mammography is a major technique for early detection of breast cancer, typically through detection of masses or calcifications. However, how to help radiologists efficiently recognize these lesions remains a challenging problem. In this paper, we propose comprehensive deep learning based solutions to respectively detect masses and segment calcifications in mammograms. To achieve the optimal mass detection performance, our method combines Faster R-CNN with Feature Pyramid Networks, Focal Loss, and Non-Local Neural Networks. We thoroughly compare the proposed method and competing methods on three public datasets and an in-house dataset. The best detection results on our in-house dataset are an average precision of 0.933 and a recall of 0.976. Regarding calcification segmentation, we design a series of pre-processing methods including window adjustment, breast region extraction and artifact removal to normalize mammograms. A U-Net model with group normalization is then applied to segment calcifications. The proposed method is validated on our in-house dataset using a newly designed evaluation metric. The experimental results have demonstrated the great potential for this task.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_breasttumorcellularityassessmentusingdeepneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Breast Tumor Cellularity Assessment Using Deep Neural Networks",
    "authors": [
      "Alexander Rakhlin",
      "Aleksei Tiulpin",
      "Alexey A. Shvets",
      "Alexandr A. Kalinin",
      "Vladimir I. Iglovikov",
      "Sergey Nikolenko"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Rakhlin_Breast_Tumor_Cellularity_Assessment_Using_Deep_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Rakhlin_Breast_Tumor_Cellularity_Assessment_Using_Deep_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Breast cancer is one of the main causes of death worldwide. Histopathological cellularity assessment of residual tumors in post-surgical tissues is used to analyze a tumor's response to a therapy. Correct cellularity assessment increases the chances of getting an appropriate treatment and facilitates the patient's survival. In current clinical practice, tumor cellularity is manually estimated by pathologists; this process is tedious and prone to errors or low agreement rates between assessors. In this work, we evaluated three strong novel Deep Learning-based approaches for automatic assessment of tumor cellularity from post-treated breast surgical specimens stained with hematoxylin and eosin. We validated the proposed methods on the BreastPathQ SPIE challenge dataset that consisted of 2395 image patches selected from whole slide images acquired from 64 patients. Compared to expert pathologist scoring, our best performing method yielded the Cohen's kappa coefficient of 0.69 (vs. 0.42 previously known in literature) and the intra-class correlation coefficient of 0.89 (vs. 0.83). Our results suggest that Deep Learning-based methods have a significant potential to alleviate the burden on pathologists, enhance the diagnostic workflow, and, thereby, facilitate better clinical outcomes in breast cancer treatment.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_retinalimageclassificationviavasculature-guidedsequentialattention": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Retinal Image Classification via Vasculature-Guided Sequential Attention",
    "authors": [
      "Mengliu Zhao",
      "Ghassan Hamarneh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Zhao_Retinal_Image_Classification_via_Vasculature-Guided_Sequential_Attention_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Zhao_Retinal_Image_Classification_via_Vasculature-Guided_Sequential_Attention_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Age-related macular degeneration and diabetic retinopathy are diseases of increasing prevalence globally in recent years. Traditionally, diagnosing these diseases relied on manual visual inspection by experts, which was costly, time-consuming and laborious as it required closely examining high-resolution color fundus images. More recently, deep learning networks have shown great potential in predicting diseases from retinal images. However, being purely data-driven, these networks are susceptible to overfitting and their training requires large annotated data. In this paper, we propose to enrich deep learning-based fundus image classifiers with prior knowledge on special structures in the retina implicated with the disease. In particular, we leverage vessel priors to guide the attention mechanism of deep learning architectures. In addition, we leverage a bi-directional dual-layer LSTM module to learn the inter-dependencies between a sequence of prior-guided attention maps deployed across the depth of the disease classification network. Results on the clinical datasets show the proposed method could bring performance improvement by as much as 8%?\r",
    "code_link": ""
  },
  "iccv2019_vrmi_cgc-netcellgraphconvolutionalnetworkforgradingofcolorectalcancerhistologyimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "CGC-Net: Cell Graph Convolutional Network for Grading of Colorectal Cancer Histology Images",
    "authors": [
      "Yanning Zhou",
      "Simon Graham",
      "Navid Alemi Koohbanani",
      "Muhammad Shaban",
      "Pheng-Ann Heng",
      "Nasir Rajpoot"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Zhou_CGC-Net_Cell_Graph_Convolutional_Network_for_Grading_of_Colorectal_Cancer_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Zhou_CGC-Net_Cell_Graph_Convolutional_Network_for_Grading_of_Colorectal_Cancer_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Colorectal cancer (CRC) grading is typically carried out by assessing the degree of gland formation within histology images. To do this, it is important to consider the overall tissue micro-environment by assessing the cell-level information along with the morphology of the gland. However, current automated methods for CRC grading typically utilise small image patches and therefore fail to incorporate the entire tissue micro-architecture for grading purposes. To overcome the challenges of CRC grading, we present a novel cell-graph convolutional neural network (CGC-Net) that converts each large histology image into a graph, where each node is represented by a nucleus within the original image and cellular interactions are denoted as edges between these nodes according to node similarity. The CGC-Net utilises nuclear appearance features in addition to the spatial location of nodes to further boost the performance of the algorithm. To enable nodes to fuse multi-scale information, we introduce Adaptive GraphSage, which is a graph convolution technique that combines multi-level features in a data-driven way. Furthermore, to deal with redundancy in the graph, we propose a sampling technique that removes nodes in areas of dense nuclear activity. We show that modeling the image as a graph enables us to effectively consider a much larger image (around 16x larger) than traditional patch-based approaches and model the complex structure of the tissue micro-environment. We construct cell graphs with an average of over 3,000 nodes on a large CRC histology image dataset and report state-of-the-art results as compared to recent patch-based as well as contextual patch-based techniques, demonstrating the effectiveness of our method.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_usingthetripletlossfordomainadaptationinwce": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Using the Triplet Loss for Domain Adaptation in WCE",
    "authors": [
      "Pablo Laiz",
      "Jordi Vitria",
      "Santi Segui"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Laiz_Using_the_Triplet_Loss_for_Domain_Adaptation_in_WCE_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Laiz_Using_the_Triplet_Loss_for_Domain_Adaptation_in_WCE_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Wireless Capsule Endoscopy (WCE) is a minimally-invasive procedure that, based on a vitamin-size camera that is swallowed by the patient, allows the visualization of the entire gastrointestinal tract. This technology was developed 20 years ago to perform useful and safe studies of different bowel disorders. However, especially the number of captured images and their difficult interpretation has hindered its deployment in some clinical scenarios. Deep learning methods have the necessary capacity to deal with WCE image interpretation, but training good models is still an open problem for some bowel disorders due to the fact that obtaining a sufficiently large set of positive cases, for the creation and validation of the model, is an arduous task. Moreover, technological advances are rapidly moving forward proposing new hardware able to obtain images with a substantially improved quality. Given these two facts, it is obvious that highly accurate models can only be built by considering heterogeneous datasets composed of images captured by different cameras, and if training methods are able to find invariances with respect to the image acquisition systems. In this paper, we study the use of deep metric learning, based on the triplet loss function, to improve the generalization of a model over different datasets from different versions of WCE hardware. The obtained results show evidence that with just a few labeled images from a newer camera set, a model that has been trained with images from older systems can be easily adapted to the new environment.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_bi-directionalconvlstmu-netwithdensleyconnectedconvolutions": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions",
    "authors": [
      "Reza Azad",
      "Maryam Asadi-Aghbolaghi",
      "Mahmood Fathy",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Azad_Bi-Directional_ConvLSTM_U-Net_with_Densley_Connected_Convolutions_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Azad_Bi-Directional_ConvLSTM_U-Net_with_Densley_Connected_Convolutions_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years, deep learning-based networks have achieved state-of-the-art performance in medical image segmentation. Among the existing networks, U-Net has been successfully applied on medical image segmentation. In this paper, we propose an extension of U-Net, Bi-directional ConvLSTM U-Net with Densely connected convolutions (BCDU-Net), for medical image segmentation, in which we take full advantages of U-Net, bi-directional ConvLSTM (BConvLSTM) and the mechanism of dense convolutions. Instead of a simple concatenation in the skip connection of U-Net, we employ BConvLSTM to combine the feature maps extracted from the corresponding encoding path and the previous decoding up-convolutional layer in a non-linear way. To strengthen feature propagation and encourage feature reuse, we use densely connected convolutions in the last convolutional layer of the encoding path. Finally, we can accelerate the convergence speed of the proposed network by employing batch normalization (BN). The proposed model is evaluated on three datasets of: retinal blood vessel segmentation, skin lesion segmentation, and lung nodule segmentation, achieving state-of-the-art performance.\r",
    "code_link": "https://github.com/rezazad68/BCDU-Net"
  },
  "iccv2019_vrmi_upi-netsemanticcontourdetectioninplacentalultrasound": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "UPI-Net: Semantic Contour Detection in Placental Ultrasound",
    "authors": [
      "Huan Qi",
      "Sally Collins",
      "J. Alison Noble"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Qi_UPI-Net_Semantic_Contour_Detection_in_Placental_Ultrasound_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Qi_UPI-Net_Semantic_Contour_Detection_in_Placental_Ultrasound_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Semantic contour detection is a challenging problem that is often met in medical imaging, of which placental image analysis is a particular example. In this paper, we investigate utero-placental interface (UPI) detection in 2D placental ultrasound images by formulating it as a semantic contour detection problem. As opposed to natural images, placental ultrasound images contain specific anatomical structures thus have unique geometry. We argue it would be beneficial for UPI detectors to incorporate global context modelling in order to reduce unwanted false positive UPI predictions. Our approach, namely UPI-Net, aims to capture long-range dependencies in placenta geometry through lightweight global context modelling and effective multi-scale feature aggregation. We perform a subject-level 10-fold nested cross-validation on a placental ultrasound database (4,871 images with labelled UPI from 49 scans). Experimental results demonstrate that, without introducing considerable computational overhead, UPI-Net yields the highest performance in terms of standard contour detection metrics, compared to other competitive benchmarks.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_rethnetobject-by-objectlearningfordetectingfacialskinproblems": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "RethNet: Object-by-Object Learning for Detecting Facial Skin Problems",
    "authors": [
      "Shohrukh Bekmirzaev",
      "Seoyoung Oh",
      "Sangwook Yo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Bekmirzaev_RethNet_Object-by-Object_Learning_for_Detecting_Facial_Skin_Problems_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Bekmirzaev_RethNet_Object-by-Object_Learning_for_Detecting_Facial_Skin_Problems_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Semantic segmentation is a hot topic in computer vision where the most challenging tasks of object detection and recognition have been handling by the success of semantic segmentation approaches. We propose a concept of objectby-object learning technique to detect 11 types of facial skin lesions using semantic segmentation methods. Detecting individual skin lesion in a dense group is a challenging task, because of ambiguities in the appearance of the visual data. We observe that there exist co-occurrent visual relations between object classes (e.g., wrinkle and age spot, or papule and whitehead, etc.). In fact, rich contextual information significantly helps to handle the issue. Therefore, we propose REthinker blocks that are composed of the locally constructed convLSTM/Conv3D layers and SE module as a one-shot attention mechanism whose responsibility is to increase network's sensitivity in the local and global contextual representation that supports to capture ambiguously appeared objects and co-occurrence interactions between object classes. Experiments show that our proposed model reached MIoU of 79.46% on the test of a prepared dataset, representing a 15.34% improvement over Deeplab v3+ (MIoU of 64.12%).\r",
    "code_link": ""
  },
  "iccv2019_vrmi_prostatecancerinferenceviaweakly-supervisedlearningusingalargecollectionofnegativemri": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Prostate Cancer Inference via Weakly-Supervised Learning using a Large Collection of Negative MRI",
    "authors": [
      "Ruiming Cao",
      "Xinran Zhong",
      "Fabien Scalzo",
      "Steven Raman",
      "Kyunghyun Sung"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Cao_Prostate_Cancer_Inference_via_Weakly-Supervised_Learning_using_a_Large_Collection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Cao_Prostate_Cancer_Inference_via_Weakly-Supervised_Learning_using_a_Large_Collection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent advances in medical imaging techniques have led to significant improvements in the management of prostate cancer (PCa). In particular, multi-parametric MRI (mp-MRI) continues to gain clinical acceptance as the preferred imaging technique for non-invasive detection and grading of PCa. However, the machine learning-based diagnosis systems for PCa are often constrained by the limited access to accurate lesion ground truth annotations for training. The performance of the machine learning system is highly dependable on both quality and quantity of lesion annotations associated with histopathologic findings, resulting in limited scalability and clinical validation. Here, we propose the baseline MRI model to alternatively learn the appearance of mp-MRI using radiology-confirmed negative MRI cases via weakly supervised learning. Since PCa lesions are case-specific and highly heterogeneous, it is assumed to be challenging to synthesize PCa lesions using the baseline MRI model, while it would be relatively easier to synthesize the normal appearance in mp-MRI. We then utilize the baseline MRI model to infer the pixel-wise suspiciousness of PCa by comparing the original and synthesized MRI with two distance functions. We trained and validated the baseline MRI model using 1,145 negative prostate mp-MRI scans. For evaluation, we used separated 232 mp-MRI scans, consisting of both positive and negative MRI cases. The 116 positive MRI scans were annotated by radiologists, confirmed with post-surgical whole-gland specimens. The suspiciousness map was evaluated by receiver operating characteristic (ROC) analysis for PCa lesions versus non-PCa regions classification and free-response receiver operating characteristic (FROC) analysis for PCa localization. Our proposed method achieved 0.84 area under the ROC curve and 77.0% sensitivity at one false positive per patient in FROC analysis.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_buildingabreast-sentencedatasetitsusefulnessforcomputer-aideddiagnosis": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Building a Breast-Sentence Dataset: Its Usefulness for Computer-Aided Diagnosis",
    "authors": [
      "Hyebin Lee",
      "Seong Tae Kim",
      "Yong Man Ro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Lee_Building_a_Breast-Sentence_Dataset_Its_Usefulness_for_Computer-Aided_Diagnosis_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Lee_Building_a_Breast-Sentence_Dataset_Its_Usefulness_for_Computer-Aided_Diagnosis_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years, it is verified that the deep learning network is able to process not only images but also time-series information. Since breast image analysis plays a big role in the diagnosis of breast cancer, there have been a large number of attempts to apply the deep learning method for an accurate diagnosis. With the advance of deep learning approaches, the possibility of using medical reports (in natural language) has been increased. However, there is no public medical report dataset associated with the breast image. Instead, in the conventional public breast mammography datasets, the characteristics of breast cancer are annotated according to the standardized term (Breast Imaging-Reporting and Data System). In this study, a breast-sentence dataset is proposed to investigate its usefulness in computer-aided diagnosis. Based on the conventional breast mammography datasets, we annotated sentences in the natural language according to the standardized terms (defined in Breast Imaging-Reporting and Data System) in conventional breast mammography datasets. In the experiments, we show three use cases to verify the usefulness of the breast-sentence dataset: 1) CAD framework with radiologist's input, 2) the use of sentence dataset in training a CAD, and 3) visual pointing guided by sentence.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_improvingrobustnessofdeeplearningbasedkneemrisegmentationmixupandadversarialdomainadaptation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Improving Robustness of Deep Learning Based Knee MRI Segmentation: Mixup and Adversarial Domain Adaptation",
    "authors": [
      "Egor Panfilov",
      "Aleksei Tiulpin",
      "Stefan Klein",
      "Miika T. Nieminen",
      "Simo Saarakkala"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Panfilov_Improving_Robustness_of_Deep_Learning_Based_Knee_MRI_Segmentation_Mixup_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Panfilov_Improving_Robustness_of_Deep_Learning_Based_Knee_MRI_Segmentation_Mixup_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Degeneration of articular cartilage (AC) is actively studied in knee osteoarthritis (OA) research via magnetic resonance imaging (MRI). Segmentation of AC tissues from MRI data is an essential step in quantification of their damage. Deep learning (DL) based methods have shown potential in this realm and are the current state-of-the-art, however, their robustness to heterogeneity of MRI acquisition settings remains an open problem. In this study, we investigated two modern regularization techniques - mixup and adversarial unsupervised domain adaptation (UDA) - to improve the robustness of DL-based knee cartilage segmentation to new MRI acquisition settings. Our validation setup included two datasets produced by different MRI scanners and using distinct data acquisition protocols. We assessed the robustness of automatic segmentation by comparing mixup and UDA approaches to a strong baseline method at different OA severity stages and, additionally, in relation to anatomical locations. Our results showed that for moderate changes in knee MRI data acquisition settings both approaches may provide notable improvements in the robustness, which are consistent for all stages of the disease and affect the clinically important areas of the knee joint. However, mixup may be considered as a recommended approach, since it is more computationally efficient and does not require additional data from the target acquisition setup.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_photometrictransformernetworksandlabeladjustmentforbreastdensityprediction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Photometric Transformer Networks and Label Adjustment for Breast Density Prediction",
    "authors": [
      "Lee Jaehwan",
      "Yoo Donggeun",
      "Kim Hyo-Eun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Jaehwan_Photometric_Transformer_Networks_and_Label_Adjustment_for_Breast_Density_Prediction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Jaehwan_Photometric_Transformer_Networks_and_Label_Adjustment_for_Breast_Density_Prediction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Grading breast density is highly sensitive to normalization settings of digital mammogram as the density is tightly correlated with the distribution of pixel intensity. Also, the grade varies with readers due to uncertain grading criteria. These issues are inherent in the density assessment of digital mammography. They are problematic when designing a computer-aided prediction model for breast density and become worse if the data comes from multiple sites. In this paper, we proposed two novel deep learning techniques for breast density prediction: 1) photometric transformation which adaptively normalizes the input mammograms, and 2) label distillation which adjusts the label by using its output prediction. The photometric transformer network predicts optimal parameters for photometric transformation on the fly, learned jointly with the main prediction network. The label distillation, a type of pseudo-label techniques, is intended to mitigate the grading variation. We experimentally showed that the proposed methods are beneficial in terms of breast density prediction, resulting in significant performance improvement compared to various previous approaches.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_branding-fusionofmetadataandmusculoskeletalradiographsformulti-modaldiagnosticrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "Branding - Fusion of Meta Data and Musculoskeletal Radiographs for Multi-Modal Diagnostic Recognition",
    "authors": [
      "Obioma Pelka",
      "Felix Nensa",
      "Christoph M. Friedrich"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Pelka_Branding_-_Fusion_of_Meta_Data_and_Musculoskeletal_Radiographs_for_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Pelka_Branding_-_Fusion_of_Meta_Data_and_Musculoskeletal_Radiographs_for_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Data fusion techniques provide opportunities for combining information from multiple domains, such as meta and medical report data with radiology images. This helps to obtain knowledge of enriched quality. The objective of this paper is to fuse automatically generated image keywords with radiographs, enabling multi-modal image representations for body part and abnormality recognition. As manual annotation is often impractical, time-consuming and prone to errors, automatic visual recognition and annotation of radiographs is a fundamental step towards computer-aided interpretation. As the number of digital medical images taken daily rapidly increases, there is a need to create systems capable of appropriately detecting and classifying anatomy and abnormality in radiology images. The Long Short-Term Memory (LSTM) based Recurrent Neural Network (RNN) Show-and-Tell model is adopted for keyword generation. The presented work fuses multi-modal information by incorporating automatically generated keywords into radiographs via augmentation. This leads to enriched sufficient features, with which deep learning systems are trained. To demonstrate the proposed approach, evaluation is computed on the Musculoskeletal Radiographs (MURA) using two classification schemes. Prediction accuracy was higher for all classification schemes using the proposed approach with 95.93 % for anatomic regions and 81.5 % for abnormality classification, respectively.\r",
    "code_link": ""
  },
  "iccv2019_vrmi_whitenner-blindimagedenoisingvianoisewhitenesspriors": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VRMI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Recognition for Medical Images",
    "title": "WhiteNNer-Blind Image Denoising via Noise Whiteness Priors",
    "authors": [
      "Saeed Izadi",
      "Zahra Mirikharaji",
      "Mengliu Zhao",
      "Ghassan Hamarneh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VRMI/Izadi_WhiteNNer-Blind_Image_Denoising_via_Noise_Whiteness_Priors_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VRMI/Izadi_WhiteNNer-Blind_Image_Denoising_via_Noise_Whiteness_Priors_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The accuracy of medical imaging-based diagnostics is directly impacted by the quality of the collected images. A passive approach to improve image quality is one that lags behind improvements in imaging hardware, awaiting better sensor technology of acquisition devices. An alternative, active strategy is to utilize prior knowledge of the imaging system to directly post-process and improve the acquired images. Traditionally, priors about the image properties are taken into account to restrict the solution space. However, few techniques exploit the prior about the noise properties. In this paper, we propose a neural network-based model for disentangling the signal and noise components of an input noisy image, without the need for any ground truth training data. We design a unified loss function that encodes priors about signal as well as noise estimate in the form of regularization terms. Specifically, by using total variation and piecewise constancy priors along with noise whiteness priors such as auto-correlation and stationary losses, our network learns to decouple an input noisy image into the underlying signal and noise components. We compare our proposed method to Noise2Noise and Noise2Self, as well as non-local mean and BM3D, on three public confocal laser endomicroscopy datasets. Experimental results demonstrate the superiority of our network compared to state-of-the-art in terms of PSNR and SSIM.\r",
    "code_link": ""
  },
  "iccv2019_dfw_arcfacefordisguisedfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "ArcFace for Disguised Face Recognition",
    "authors": [
      "Jiankang Deng",
      "Stefanos Zafeririou"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Deng_ArcFace_for_Disguised_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Deng_ArcFace_for_Disguised_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Even though deep face recognition is extensively explored and remarkable advances have been achieved on large-scale in-the-wild dataset, disguised face recognition receives much less attention. Face feature embedding targeting on intra-class compactness and inter-class discrepancy is very challenging as high intra-class diversity and inter-class similarity are very common on the disguised face recognition dataset. In this report, we give the technical details of our submission to the DFW2019 challenge. By using our RetinaFace for face detection and alignment and ArcFace for face feature embedding, we achieve state-of-the-art performance on the DFW2019 challenge.\r",
    "code_link": ""
  },
  "iccv2019_dfw_basnenrichingfeaturerepresentationusingbipartiteauxiliarysupervisionsforfaceanti-spoofing": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "BASN: Enriching Feature Representation Using Bipartite Auxiliary Supervisions for Face Anti-Spoofing",
    "authors": [
      "Taewook Kim",
      "YongHyun Kim",
      "Inhan Kim",
      "Daijin Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Kim_BASN_Enriching_Feature_Representation_Using_Bipartite_Auxiliary_Supervisions_for_Face_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Kim_BASN_Enriching_Feature_Representation_Using_Bipartite_Auxiliary_Supervisions_for_Face_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Face anti-spoofing is an important task to assure the security of face recognition systems. To be applicable to unconstrained real-world environments, generalization capabilities of the face anti-spoofing methods are required. In this work, we present a face anti-spoofing method with robust generalization ability to unseen environments. To achieve our goal, we suggest bipartite auxiliary supervision to properly guide networks to learn generalizable features. We propose a bipartite auxiliary supervision network (BASN) that comprehensively utilizes the suggested supervision to accurately detect presentation attacks. We evaluate our method by conducting experiments on public benchmark datasets and we achieve state-of-the-art performances.\r",
    "code_link": ""
  },
  "iccv2019_dfw_dualattentionmobdensenet(damdnet)forrobust3dfacealignment": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "Dual Attention MobDenseNet(DAMDNet) for Robust 3D Face Alignment",
    "authors": [
      "Lei Jiang",
      "Xiao-Jun Wu",
      "Josef Kittler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Jiang_Dual_Attention_MobDenseNetDAMDNet_for_Robust_3D_Face_Alignment_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Jiang_Dual_Attention_MobDenseNetDAMDNet_for_Robust_3D_Face_Alignment_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " 3D face alignment of monocular images is a crucial process in the recognition of faces with disguise.3D face reconstruction facilitated by alignment can restore the face structure which is helpful in detcting disguise interference.This paper proposes a dual attention mechanism and an efficient end-to-end 3D face alignment framework.We build a stable network model through Depthwise Separable Convolution, Densely Connected Convolutional and Lightweight Channel Attention Mechanism. In order to enhance the ability of the network model to extract the spatial features of the face region, we adopt Spatial Group-wise Feature enhancement module to improve the representation ability of the network. Different loss functions are applied jointly to constrain the 3D parameters of a 3D Morphable Model (3DMM) and its 3D vertices. We use a variety of data enhancement methods and generate large virtual pose face data sets to solve the data imbalance problem. The experiments on the challenging AFLW, AFLW2000-3D datasets show that our algorithm significantly improves the accuracy of 3D face alignment. Our experiments using the field DFW dataset show that DAMDNet exhibits excellent performance in the 3D alignment and reconstruction of challenging disguised faces.The model parameters and the complexity of the proposed method are also reduced significantly.\r",
    "code_link": ""
  },
  "iccv2019_dfw_enhancinghumanfacerecognitionwithaninterpretableneuralnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "Enhancing Human Face Recognition with an Interpretable Neural Network",
    "authors": [
      "Timothy Zee",
      "Geeta Gali",
      "Ifeoma Nwogu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Zee_Enhancing_Human_Face_Recognition_with_an_Interpretable_Neural_Network_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Zee_Enhancing_Human_Face_Recognition_with_an_Interpretable_Neural_Network_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The purpose of this work is to determine if the ability to interpret a convolutional neural network (CNN) architecture can enhance human performance, pertaining to face recognition. We are interested in distinguishing between the faces of two similar-looking actresses of Indian origin, who have only a few discriminating features. This recognition task proved challenging for humans who were not previously familiar with the actresses (novices) as they performed only just better than random. When asked to perform the same task, humans who were more familiar with the actresses (experts) performed significantly better. We attempted the same task with a Siamese CNN which performed as well as the experts. We therefore became interested in applying any new knowledge obtained from the CNN to aid in improving the distinguishing abilities of other novices. This was accomplished by generating activation maps from the CNN. The maps showed what parts of the input face images created the highest activations in the last convolutional layer of the network. Using \"fooling'\" techniques, we also investigated what spatial locations on the face were most responsible for confusing one actress for the other. Empirically, the cheekbones and foreheads were determined to be the strongest differentiating features between the actresses. By providing this information verbally to a new set of novices, we successfully raised the human recognition rates by 11%. For this work, we therefore successfully increased human understanding pertaining to facial recognition via post-hoc interpretability of a CNN.\r",
    "code_link": ""
  },
  "iccv2019_dfw_tensorlinearregressionanditsapplicationtocolorfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "Tensor Linear Regression and Its Application to Color Face Recognition",
    "authors": [
      "Quanxue Gao",
      "Jiafeng Cheng",
      "Deyan Xie",
      "Pu Zhang",
      "Wei Xia",
      "Qianqian Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Gao_Tensor_Linear_Regression_and_Its_Application_to_Color_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Gao_Tensor_Linear_Regression_and_Its_Application_to_Color_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Linear regression has achieved the promising preliminary results for face classification. But, most existing methods are incapable of tackling color images classification. The major reason is that they need to transform each color image to a vector or matrix, leading to the loss of multidimensional structure information embedded in color images. To address this problem, we study the tensor linear regression problem, and develop a novel tensor low-rank method, which utilizes tensor-Singular Value Decomposition (t-SVD) based tensor nuclear norm to emphasize the spatial structure embedded in color images. Applying it to color face classification, extensive experiments on three datasets demonstrate that our method is superior to several state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_dfw_featureensemblenetworkswithre-rankingforrecognizingdisguisedfacesinthewild": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "Feature Ensemble Networks with Re-Ranking for Recognizing Disguised Faces in the Wild",
    "authors": [
      "Arulkumar Subramaniam",
      "Ajay Narayanan Sridhar",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Subramaniam_Feature_Ensemble_Networks_with_Re-Ranking_for_Recognizing_Disguised_Faces_in_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Subramaniam_Feature_Ensemble_Networks_with_Re-Ranking_for_Recognizing_Disguised_Faces_in_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recognizing a person's face images with intentional/unintentional disguising effects such as make-up, plastic surgery, artificial wearables (hats, eye-glasses) is a challenging task. We propose a Feature EnsemBle Network (FEBNet) for recognizing Disguised Faces in the Wild (DFW). FEBNet encompasses multiple base networks (SE-ResNet50, Inception-ResNet-V1) pretrained on large-scale face recognition datasets (MS-Celeb-1M, VGGFace2) and fine-tuned on DFW training dataset. During the fine-tuning phase, we propose to use two novel objective functions, namely, 1) Category loss, 2) Impersonator Triplet loss along with two prevalent objective functions: Identity loss, Inter-person Triplet loss. To further improve the performance, we apply a state-of-the-art re-ranking strategy as a post-processing step. Extensive ablation studies and evaluation results show that FEBNet significantly outperforms the baseline models.\r",
    "code_link": ""
  },
  "iccv2019_dfw_disguisedfacesinthewild2019": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "Disguised Faces in the Wild 2019",
    "authors": [
      "Maneet Singh",
      "Mohit Chawla",
      "Richa Singh",
      "Mayank Vatsa",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Singh_Disguised_Faces_in_the_Wild_2019_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Singh_Disguised_Faces_in_the_Wild_2019_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Disguised face recognition has wide-spread applicability in scenarios such as law enforcement, surveillance, and access control. Disguise accessories such as sunglasses, masks, scarves, or make-up modify or occlude different facial regions which makes face recognition a challenging task. In order to understand and benchmark the state-of-the-art on face recognition in the presence of disguise variations, the Disguised Faces in the Wild 2019 (DFW2019) competition has been organized. This paper summarizes the outcome of the competition in terms of the dataset used for evaluation, a brief review of the algorithms employed by the participants for this task, and the results obtained. The DFW2019 dataset has been released with four evaluation protocols and baseline results obtained from two deep learning-based state-of-the-art face recognition models. The DFW2019 dataset has also been analyzed with respect to degrees of difficulty: (i) easy, (ii) medium, and (iii) hard. The dataset has been released as part of the International Workshop on Disguised Faces in the Wild at International Conference on Computer Vision (ICCV), 2019.\r",
    "code_link": ""
  },
  "iccv2019_dfw_facerepresentationlearningusingcompositemini-batches": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DFW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Disguised Faces in the Wild",
    "title": "Face Representation Learning using Composite Mini-Batches",
    "authors": [
      "Evgeny Smirnov",
      "Andrei Oleinik",
      "Aleksandr Lavrentev",
      "Elizaveta Shulga",
      "Vasiliy Galyuk",
      "Nikita Garaev",
      "Margarita Zakuanova",
      "Aleksandr Melnikov"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DFW/Smirnov_Face_Representation_Learning_using_Composite_Mini-Batches_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DFW/Smirnov_Face_Representation_Learning_using_Composite_Mini-Batches_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Mini-batch construction strategy is an important part of the deep representation learning. Different strategies have their advantages and limitations. Usually only one of them is selected to create mini-batches for training. However, in many cases their combination can be more efficient than using only one of them. In this paper, we propose Composite Mini-Batches - a technique to combine several mini-batch sampling strategies in one training process. The main idea is to compose mini-batches from several parts, and use different sampling strategy for each part. With this kind of mini-batch construction, we combine the advantages and reduce the limitations of the individual mini-batch sampling strategies. We also propose Interpolated Embeddings and Priority Class Sampling as complementary methods to improve the training of face representations. Our experiments on a challenging task of disguised face recognition confirm the advantages of the proposed methods.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_learningdisentangledrepresentationsviaindependentsubspaces": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Learning Disentangled Representations via Independent Subspaces",
    "authors": [
      "Maren Awiszus",
      "Hanno Ackermann",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Awiszus_Learning_Disentangled_Representations_via_Independent_Subspaces_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Awiszus_Learning_Disentangled_Representations_via_Independent_Subspaces_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Image generating neural networks are mostly viewed as black boxes, where any change in the input can have a number of globally effective changes on the output. In this work, we propose a method for learning disentangled representations to allow for localized image manipulations. We use face images as our example of choice. Depending on the image region, identity and other facial attributes can be modified. The proposed network can transfer parts of a face such as shape and color of eyes, hair, mouth, etc.directly between persons while all other parts of the face remain unchanged. The network allows to generate modified images which appear like realistic images. Our model learns disentangled representations by weak supervision. We propose a localized resnet autoencoder optimized using several loss functions including a loss based on the semantic segmentation, which we interpret as masks, and a loss which enforces disentanglement by decomposition of the latent space into statistically independent subspaces. We evaluate the proposed solution w.r.t. disentanglement and generated image quality. Convincing results are demonstrated using the CelebA dataset.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_uncalibratednon-rigidfactorisationbyindependentsubspaceanalysis": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Uncalibrated Non-Rigid Factorisation by Independent Subspace Analysis",
    "authors": [
      "Sami Sebastian Brandt",
      "Hanno Ackermann",
      "Stella Grasshof"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Brandt_Uncalibrated_Non-Rigid_Factorisation_by_Independent_Subspace_Analysis_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Brandt_Uncalibrated_Non-Rigid_Factorisation_by_Independent_Subspace_Analysis_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a general, prior-free approach for the uncalibrated non-rigid structure-from-motion problem for modelling and analysis of non-rigid objects such as human faces. We recover the non-rigid affine structure and motion from 2D point correspondences by assuming that (1) the non-rigid shapes are generated by a linear combination of rigid 3D basis shapes, (2) that the non-rigid shapes are affine in nature, i.e., they can be modelled as deviations from the mean, rigid shape, (3) and that the basis shapes are statistically independent. In contrast to the majority of existing works, no statistical prior is assumed for the structure and motion apart from the assumption that underlying basis shapes are statistically independent. The independent 3D shape bases are recovered by independent subspace analysis (ISA). Likewise, in contrast to the most previous approaches, no calibration information is assumed for affine cameras; the reconstruction is solved up to a global affine ambiguity that makes our approach simple and efficient. In the experiments, we evaluated the method with several standard data sets including a real face expression data set of 7200 faces with 2D point correspondences and unknown 3D structure and motion for which we obtained promising results.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_robustdiscriminationandgenerationoffacesusingcompact,disentangledembeddings": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Robust Discrimination and Generation of Faces using Compact, Disentangled Embeddings",
    "authors": [
      "Bjoern Browatzki",
      "Christian Wallraven"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Browatzki_Robust_Discrimination_and_Generation_of_Faces_using_Compact_Disentangled_Embeddings_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Browatzki_Robust_Discrimination_and_Generation_of_Faces_using_Compact_Disentangled_Embeddings_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Current solutions to discriminative and generative tasks in computer vision exist separately and often lack interpretability and explainability. Using faces as our application domain, here we present an architecture that is based around two core ideas that address these issues: first, our framework learns an unsupervised, low-dimensional embedding of faces using an adversarial autoencoder that is able to synthesize high-quality face images. Second, a supervised disentanglement splits the low-dimensional embedding vector into four sub-vectors, each of which contains separated information about one of four major face attributes (pose, identity, expression, and style) that can be used both for discriminative tasks and for manipulating all four attributes in an explicit manner. The resulting architecture achieves state-of-the-art image quality, good discrimination and face retrieval results on each of the four attributes, and supports various face editing tasks using a face representation of only 99 dimensions. Finally, we apply the architecture's robust image synthesis capabilities to visually debug label-quality issues in an existing face dataset.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_tensorsubspacelearningandclassificationtensorlocaldiscriminantembeddingforhyperspectralimage": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Tensor Subspace Learning and Classification: Tensor Local Discriminant Embedding for Hyperspectral Image",
    "authors": [
      "Lei He",
      "Hongwei Yang",
      "Lina Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/He_Tensor_Subspace_Learning_and_Classification_Tensor_Local_Discriminant_Embedding_for_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/He_Tensor_Subspace_Learning_and_Classification_Tensor_Local_Discriminant_Embedding_for_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Hyperspectral image (HSI) has shown promising results in many fields because of its high spectral resolution. However, the redundancy of spectral dimension seriously affects the classification of HSI. Therefore, many popular dimension reduction (DR) algorithms are proposed and subspace learning algorithm is a typical one. In HSI, cube data is traditionally flatted into 1-D vector, so spatial information is completely ignored in most dimension reduction algorithms. The tensor representation for HSI considers both the spatial information and cubic properties simultaneously, so that tensor subspace learning can be naturally introduced into DR for HSI. In this paper, a tensor local discriminant embedding (TLDE) is proposed for DR and classification of HSI. TLDE can take full advantage of spatial structure and spectral information and map a high dimensional space into a low dimensional space by three projection matrices trained. TLDE can be more discriminative by calculating an intrinsic graph and a penalty graph. The experimental results on two real datasets demonstrate that TLDE is effective and works well even when the training samples are small.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_tensortraindecompositionforefficientmemorysavinginperceptualfeature-maps": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Tensor Train Decomposition for Efficient Memory Saving in Perceptual Feature-Maps",
    "authors": [
      "Taehyeon Kim",
      "Jieun Lee",
      "Yoonsik Choe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Kim_Tensor_Train_Decomposition_for_Efficient_Memory_Saving_in_Perceptual_Feature-Maps_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Kim_Tensor_Train_Decomposition_for_Efficient_Memory_Saving_in_Perceptual_Feature-Maps_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The perceptual loss functions have been used successfully in image transformation for capturing high-level features from images in pre-trained convolutional neural networks (CNNs). Standard perceptual losses require numerous parameters to compare differences in feature-maps on both an input image and a target image; thus, it is not affordable for resource-constrained devices in terms of utilizing a feature-maps. Hence, we propose a compressed perceptual losses oriented Tensor Train (TT) decomposition on the feature-maps. Additionally, to decide an optimal TT-ranks, the proposed algorithm used the global analytic solution of Variational Bayesian Matrix Factorization (VBMF). Therefore, in proposed method, the low-rank approximated feature-maps consist of salient features by virtue of these two techniques. To the best of our knowledge, we are the first to consider curtailing redundancies in feature-maps via low-rank TT-decomposition. Experimental results in style transfer tasks demonstrate that our method not only yields similar qualitative and quantitative results as that of the original version, but also reduces memory requirement by approximately 77%.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_low-ranktensortracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Low-Rank Tensor Tracking",
    "authors": [
      "Sajid javed",
      "Jorge Dias",
      "Naoufel Werghi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/javed_Low-Rank_Tensor_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/javed_Low-Rank_Tensor_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Visual object tracking is an important step for many computer vision applications. Visual tracking becomes more challenging when the target object observes severe occlusion, lighting variations, background clutter, and deformation difficulties to name a few. In the literature, low-rank matrix decomposition methods have shown to be a potential solution for visual tracking in many complex scenarios. These methods first arrange the particles of the target object in a 2-D data matrix and then perform convex optimization to solve the low-rank objective function. However, these methods show performance degradation in the presence of the aforementioned challenges. Because these methods do not consider the intrinsic structure of the target particles, therefore, the object loses its spatial appearance or consistency. To address these challenges, we propose a new low-rank tensor decomposition model for robust object tracking. Our proposed low-rank tensor tracker considers the multi-dimensional data of target particles. We employ the recently proposed tensor-tensor product-based singular value decomposition and a new tensor nuclear norm to promote the intrinsic structure correlation among the target particles. Experimental evaluations on 20 challenging tracking sequences demonstrate the excellent performance of the proposed tracker as compared with state-of-the-art trackers.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_structuringautoencoders": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Structuring Autoencoders",
    "authors": [
      "Marco Rudolph",
      "Bastian Wandt",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Rudolph_Structuring_Autoencoders_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Rudolph_Structuring_Autoencoders_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural networks which learn a low dimensional representation of data and are additionally enriched with a desired structure in this low dimensional space. While traditional Autoencoders have proven to structure data naturally they fail to discover semantic structure that is hard to recognize in the raw data. The SAE solves the problem by enhancing a traditional Autoencoder using weak supervision to form a structured latent space. In the experiments we demonstrate, that the structured latent space allows for a much more efficient data representation for further tasks such as classification for sparsely labeled data, an efficient choice of data to label, and morphing between classes. To demonstrate the general applicability of our method, we show experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2 and on a dataset of 3D human shapes.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_structure-constrainedfeatureextractionbyautoencodersforsubspaceclustering": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Structure-Constrained Feature Extraction by Autoencoders for Subspace Clustering",
    "authors": [
      "Kewei Tang",
      "Kaiqiang Xu",
      "Zhixun Su",
      "Wei Jiang",
      "Xiaonan Luo",
      "Xiyan Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Tang_Structure-Constrained_Feature_Extraction_by_Autoencoders_for_Subspace_Clustering_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Tang_Structure-Constrained_Feature_Extraction_by_Autoencoders_for_Subspace_Clustering_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Clustering the data points drawn from a union of subspaces, i.e., subspace clustering, is a hot topic in recent years. The assumption of the nonlinear subspace is more general for the real-world data, but also more difficult for the traditional methods. The deep neural network is a powerful technique extracting nonlinear features, so the autoencoders are usually adopted to handle this unsupervised problem. However, how to add constraints on the features to make them more effective is not heavily addressed by previous work. In this paper, we consider both the global and local structure of the features in the autoencoders by the low-rank property and Laplace operator, respectively. The low-rank property can make the learned features favoring similarity extraction by self-representation and the Laplace operator can help our method explore the useful information in the data set. Note that our way of placing constraints can also be employed in other deep neural networks. In fact, our method is closely associated with previous work. It can be viewed as the more general case of the structured autoencoders. Extensive experiments demonstrate the effectiveness of our method.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_deepclosed-formsubspaceclustering": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Deep Closed-Form Subspace Clustering",
    "authors": [
      "Junghoon Seo",
      "Jamyoung Koo",
      "Taegyun Jeon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Seo_Deep_Closed-Form_Subspace_Clustering_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Seo_Deep_Closed-Form_Subspace_Clustering_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose Deep Closed-Form Subspace Clustering (DCFSC), a new embarrassingly simple model for subspace clustering with learning non-linear mapping. Compared with the previous deep subspace clustering (DSC) techniques, our DCFSC does not have any parameters at all for the self-expressive layer. Instead, DCFSC utilizes the implicit data-driven self-expressive layer derived from closed-form shallow auto-encoder. Moreover, DCFSC also has no complicated optimization scheme, unlike the other subspace clustering methods. With its extreme simplicity, DCFSC has significant memory-related benefits over the existing DSC method, especially on the large dataset. Several experiments showed that our DCFSC model had enough potential to be a new reference model for subspace clustering on large-scale high-dimensional dataset.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_panoramicvideoseparationwithonlinegrassmannianrobustsubspaceestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Panoramic Video Separation with Online Grassmannian Robust Subspace Estimation",
    "authors": [
      "Kyle Gilman",
      "Laura Balzano"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Gilman_Panoramic_Video_Separation_with_Online_Grassmannian_Robust_Subspace_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Gilman_Panoramic_Video_Separation_with_Online_Grassmannian_Robust_Subspace_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work, we propose a new total variation (TV)-regularized robust principal component analysis (RPCA) algorithm for panoramic video data with incremental gradient descent on the Grassmannian. The resulting algorithm has performance competitive with state-of-the-art panoramic RPCA algorithms and can be computed frame-by-frame to separate foreground/background in video with a freely moving camera and heavy sparse noise. We show that our algorithm scales favorably in computation time and memory. Finally we compare foreground detection accuracy and computation time of our method versus several existing methods.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_topologicallabellingofsceneusingbackground/foregroundseparationandepipolargeometry": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Topological Labelling of Scene using Background/Foreground Separation and Epipolar Geometry",
    "authors": [
      "Hiroki Hiraoka",
      "Atsushi Imiya"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Hiraoka_Topological_Labelling_of_Scene_using_BackgroundForeground_Separation_and_Epipolar_Geometry_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Hiraoka_Topological_Labelling_of_Scene_using_BackgroundForeground_Separation_and_Epipolar_Geometry_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The robust Principal Component Analysis (rPCA) efficiently separates an image into the foreground and background regions. The stixels provide middle-level expression of a scene using vertical columnar-superpixels of pixels with same depth computed from a pair of stereo image. Combining the classification of pixels by rPCA and depth map, topological labelling of pixels of each frame in an image sequence is achieved. The algorithm constructs static stixels and moving boxes of an image sequence from background and foreground regions, respectively. The algorithm also estimates free-space for motion planning from background regions as a collection of horizontal columnar-superpixels parallel to the epipolar lines.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_completemovingobjectdetectioninthecontextofrobustsubspacelearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Complete Moving Object Detection in the Context of Robust Subspace Learning",
    "authors": [
      "Maryam Sultana",
      "Arif Mahmood",
      "Thierry Bouwmans",
      "Soon Ki Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Sultana_Complete_Moving_Object_Detection_in_the_Context_of_Robust_Subspace_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Sultana_Complete_Moving_Object_Detection_in_the_Context_of_Robust_Subspace_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Complete moving object detection plays a vital role in many applications of computer vision. For instance, depth estimation, scene understanding, object interaction, semantic segmentation, accident detection and avoidance in case of moving vehicles on a highway. However, it becomes challenging in the presence of dynamic backgrounds, camouflage, bootstrapping, varying illumination conditions, and noise. Over the past decade, robust subspace learning based methods addressed the moving objects detection problem with excellent performance. However, the moving objects detected by these methods are incomplete, unable to generate the occluded parts. Indeed, complete or occlusion-free moving object detection is still challenging for these methods. In the current work, we address this challenge by proposing a conditional Generative Adversarial Network (cGAN) conditioned on non-occluded moving object pixels during training. It therefore learns the subspace spanned by the moving objects covering all the dynamic variations and semantic information. While testing, our proposed Complete cGAN (CcGAN) is able to generate complete occlusion free moving objects in challenging conditions. The experimental evaluations of our proposed method are performed on SABS benchmark dataset and compared with 14 state-of-the-art methods, including both robust subspace and deep learning based methods. Our experiments demonstrate the superiority of our proposed model over both types of existing methods.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_classifyingandcomparingapproachestosubspaceclusteringwithmissingdata": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Classifying and Comparing Approaches to Subspace Clustering with Missing Data",
    "authors": [
      "Connor Lane",
      "Ron Boger",
      "Chong You",
      "Manolis Tsakiris",
      "Benjamin Haeffele",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Lane_Classifying_and_Comparing_Approaches_to_Subspace_Clustering_with_Missing_Data_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Lane_Classifying_and_Comparing_Approaches_to_Subspace_Clustering_with_Missing_Data_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years, many methods have been proposed for the task of subspace clustering with missing data (SCMD), and its complementary problem, high-rank matrix completion (HRMC). Given incomplete data drawn from a union of subspaces, these methods aim to simultaneously cluster each data point and recover the unobserved entries. In this work, we review the current state of this literature. We organize the existing methods into five distinct families and discuss their relative strengths and weaknesses. This classification exposes some gaps in the current literature, which we fill by introducing a few natural extensions of prior methods. Finally, we provide a thorough and unbiased evaluation of representative methods on synthetic data. Our experiments demonstrate a clear advantage for alternating between projected zero-filled sparse subspace clustering, and per-group matrix completion. Understanding why this intuitive but heuristic method performs well is an open problem for future theoretical study.\r",
    "code_link": ""
  },
  "iccv2019_rsl-cv_adaptiveonlinek-subspaceswithcooperativere-initialization": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RSL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Robust Subspace Learning and Applications in Computer Vision",
    "title": "Adaptive Online k-Subspaces with Cooperative Re-Initialization",
    "authors": [
      "Connor Lane",
      "Benjamin Haeffele",
      "Rene Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RSL-CV/Lane_Adaptive_Online_k-Subspaces_with_Cooperative_Re-Initialization_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RSL-CV/Lane_Adaptive_Online_k-Subspaces_with_Cooperative_Re-Initialization_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a simple but principled cooperative re-initialization (CoRe) approach to k-subspaces, which also applies to k-means by viewing it as a particular case. CoRe optimizes an ensemble of identical k-subspace models and leverages their aggregate knowledge by greedily exchanging clusters throughout optimization. Further, we introduce an adaptive k-subspaces formulation with split low-rank regularization designed to adapt both the number of subspaces and their dimensions. Moreover, we present a highly scalable online algorithm based on stochastic gradient descent. In experiments on synthetic and real image data, we show that our proposed CoRe method significantly improves upon the standard probabilistic farthest insertion (i.e. k-means++) initialization approach-particularly when k is large. We further demonstrate the improved robustness of our proposed formulation, and the scalability and improved optimization performance of our SGD-based algorithm.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_enhancedmemorynetworkforvideosegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Enhanced Memory Network for Video Segmentation",
    "authors": [
      "Zhishan Zhou",
      "Lejian Ren",
      "Pengfei Xiong",
      "Yifei Ji",
      "Peisen Wang",
      "Haoqiang Fan",
      "Si Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Zhou_Enhanced_Memory_Network_for_Video_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Zhou_Enhanced_Memory_Network_for_Video_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper proposes an Enhanced Memory Network (EMN) for semi-supervised video object segmentation. Space-Time Memory Networks has proven the effectiveness of the abundant use of guidance information. To further improve the accuracy of unknown and small targets, we propose to perform fined-grained segmentation based on the correlation attention map. We introduce a siamese network to obtain the semantic similarity and relevance between the tracking objects and the whole image. The feature map extracted from the siamese network on the cropped image is multiplied onto the whole feature map as the attention of proposal objects. Also, an ASPP module is employed to increase the semantic receptive filed to further improve the segmentation accuracy on different scale. Based on the multi-object combination and multi-scale ensemble, the proposed algorithm achieves the first place on the YouTube-VOS 2019 Semi-supervised Video Object Segmentation Challenge with a J&F mean score of 81.8%.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_motion-guidedspatialtimeattentionforvideoobjectsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Motion-Guided Spatial Time Attention for Video Object Segmentation",
    "authors": [
      "Qiang Zhou",
      "Zilong Huang",
      "Lichao Huang",
      "Yongchao Gong",
      "Han Shen",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Zhou_Motion-Guided_Spatial_Time_Attention_for_Video_Object_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Zhou_Motion-Guided_Spatial_Time_Attention_for_Video_Object_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a novel motion-guided attention module to implant the spatial and time consistency in the correlation map of the current frame with the historical frames. Unlike other mask propagation based methods, our method regards the previous mask as a strong prior instead of concatenating it to the current frame or feature for propagation. Additionally, to reduce the gap between training and testing phase, we propose an improved optimization strategy, named sequence learning, which feeds a video in chronological order into the end-to-end network instead of several random-sampling frames when training. Sequence learning helps our model be better aware of the concept of tracking and recognition of object. We evaluated the proposed algorithm on the second YouTube-VOS test-challenge set and achieved a J&F mean score of 81.7%, ranked the second place on the VOS track. In the challenge, our method only uses ResNet-50 as the backbone and our score is very slightly worse than the first place score, i.e., 0.1%, which implies that our VOS framework is the state-of-the-art one.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_goingdeeperintoembeddinglearningforvideoobjectsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Going Deeper Into Embedding Learning for Video Object Segmentation",
    "authors": [
      "Zongxin Yang",
      "Peike Li",
      "Qianyu Feng",
      "Yunchao Wei",
      "Yi Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Yang_Going_Deeper_Into_Embedding_Learning_for_Video_Object_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Yang_Going_Deeper_Into_Embedding_Learning_for_Video_Object_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we investigate the principles of consistent training, between given reference and predicted sequence, for better embedding learning of semi-supervised video object segmentation. To accurately segment the target objects given the mask at the first frame, we realize that the expected feature embeddings of any consecutive frames should satisfy the following properties: 1) global consistency in terms of both foreground object(s) and background; 2) robust local consistency under a various object moving rate; 3) environment consistency between the training and inference process; 4) receptive consistency between the receptive fields of network and the variable scales of objects; 5) sampling consistency between foreground and background pixels to avoid training bias. With the principles in mind, we carefully design a simple pipeline to lift both accuracy and efficiency for video object segmentation effectively. With the ResNet-101 as the backbone, our single model achieves a J&F score of 81.0% on the validation set of Youtube-VOS benchmark without any bells and whistles. By applying multi-scale & flip augmentation at the testing stage, the accuracy can be further boosted to 82.4%. Code will be made available.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_towardsgoodpracticesforvideoobjectsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Towards Good Practices for Video Object Segmentation",
    "authors": [
      "Dongdong Yu",
      "Kai Su",
      "Hengkai Guo",
      "Jian Wang",
      "Kaihui Zhou",
      "Yuanyuan Huang",
      "Minghui Dong",
      "Jie Shao",
      "Changhu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Yu_Towards_Good_Practices_for_Video_Object_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Yu_Towards_Good_Practices_for_Video_Object_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Semi-supervised video object segmentation is an interesting yet challenging task in machine learning. In this work, we conduct a series of refinements with the propagation-based video object segmentation method and empirically evaluate their impact on the final model performance through ablation study. By taking all the refinements, we improve the space-time memory networks to achieve a Overall of 79.1 on the Youtube-VOS Challenge 2019.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_exploringthecombinationofpremvos,boltvosandunovostforthe2019youtube-voschallenge": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Exploring the Combination of PReMVOS, BoLTVOS and UnOVOST for the 2019 YouTube-VOS Challenge",
    "authors": [
      "Jonathon Luiten",
      "Paul Voigtlaender",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Luiten_Exploring_the_Combination_of_PReMVOS_BoLTVOS_and_UnOVOST_for_the_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Luiten_Exploring_the_Combination_of_PReMVOS_BoLTVOS_and_UnOVOST_for_the_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video Object Segmentation is the task of tracking and segmenting objects in a video given the first-frame mask of objects to be tracked. There have been a number of different successful paradigms for tackling this task, from creating object proposals and linking them in time as in PReMVOS, to detecting objects to be tracked conditioned on the given first-frame as in BoLTVOS, and creating tracklets based on motion consistency before merging these into long-term tracks as in UnOVOST. In this paper we explore how these three different approaches can be combined into a novel Video Object Segmentation algorithm. We evaluate our approach on the 2019 Youtube-VOS challenge where we obtain 6th place with an overall score of 71.5%.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_videoinstancesegmentation2019awinningapproachforcombineddetection,segmentation,classificationandtracking.": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Video Instance Segmentation 2019: A Winning Approach for Combined Detection, Segmentation, Classification and Tracking.",
    "authors": [
      "Jonathon Luiten",
      "Philip Torr",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Luiten_Video_Instance_Segmentation_2019_A_Winning_Approach_for_Combined_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Luiten_Video_Instance_Segmentation_2019_A_Winning_Approach_for_Combined_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video Instance Segmentation (VIS) is the task of localizing all objects in a video, segmenting them, tracking them throughout the video and classifying them into a set of predefined classes. In this work, divide VIS into these four parts: detection, segmentation, tracking and classification. We then develop algorithms for performing each of these four sub tasks individually, and combine these into a complete solution for VIS. Our solution is an adaptation of UnOVOST, the current best performing algorithm for Unsupervised Video Object Segmentation, to this VIS task. We benchmark our algorithm on the 2019 YouTube-VIS Challenge, where we obtain first place with an mAP score of 46.7%.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_anempiricalstudyofdetection-basedvideoinstancesegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "An Empirical Study of Detection-Based Video Instance Segmentation",
    "authors": [
      "Qiang Wang",
      "Yi He",
      "Xiaoyun Yang",
      "Zhao Yang",
      "Philip Torr"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Wang_An_Empirical_Study_of_Detection-Based_Video_Instance_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Wang_An_Empirical_Study_of_Detection-Based_Video_Instance_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video instance segmentation (VIS) is a composite task that requires the joint detection, tracking, and segmentation of objects in a video. In this work, we introduce a complete framework for VIS, which integrates the strengths of instance segmentation and general object tracking in addressing the unique challenges of VIS. In developing the framework, we investigate effective ways of coordinating the two components for maximum benefits while thoroughly investigate their separate contributions. Our approach improves over the official baseline by an absolute 14.4% in mAP and achieves the second place in the 2019 YouTubeVIS challenge.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_dualembeddinglearningforvideoinstancesegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Dual Embedding Learning for Video Instance Segmentation",
    "authors": [
      "Qianyu Feng",
      "Zongxin Yang",
      "Peike Li",
      "Yunchao Wei",
      "Yi Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Feng_Dual_Embedding_Learning_for_Video_Instance_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Feng_Dual_Embedding_Learning_for_Video_Instance_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a novel framework to generate high-quality segmentation results in a two-stage style, aiming at video instance segmentation task which requires simultaneous detection, segmentation and tracking of instances. To address this multi-task efficiently, we opt to first select high-quality detection proposals in each frame. The categories of the proposals are calibrated with the global context of video. Then, each selected proposal is extended temporally by a bi-directional Instance-Pixel Dual-Tracker (IPDT) which synchronizes the tracking on both instance-level and pixel-level. The instance-level module concentrates on distinguishing the target instance from other objects while the pixel-level module focuses more on the local feature of the instance. Our proposed method achieved a competitive result of mAP 45.0% on the Youtube-VOS dataset, ranking the 3rd in Track 2 of the 2nd Large-scale Video Object Segmentation Challenge.\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_temporalfeatureaugmentednetworkforvideoinstancesegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Temporal Feature Augmented Network for Video Instance Segmentation",
    "authors": [
      "Minghui Dong",
      "Jian Wang",
      "Yuanyuan Huang",
      "Dongdong Yu",
      "Kai Su",
      "Kaihui Zhou",
      "Jie Shao",
      "Shiping Wen",
      "Changhu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Dong_Temporal_Feature_Augmented_Network_for_Video_Instance_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Dong_Temporal_Feature_Augmented_Network_for_Video_Instance_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a temporal feature augmented network for video instance segmentation. Video instance segmentation task can be split into two subtasks: instance segmentation and tracking. Similar to the previous work, a track head is added to an instance segmentation network to track object instances across frames. Then the network can performing detection, segmentation and tracking tasks simultaneously. We choose the Cascade-RCNN as the basic instance segmentation network. Besides, in order to make better use of the rich information contained in the video, a temporal feature augmented module is introduced to the network. When performing instance segmentation task on a single frame, information from other frames in the same video will be included and the performance of instance segmentation task can be effectively improved. Moreover, experiments show that the temporal feature augmented module can effectively alleviate the problem of motion blur and pose variation\r",
    "code_link": ""
  },
  "iccv2019_youtube-vos_spatio-temporalattentionnetworkforvideoinstancesegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "YouTube-VOS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large-Scale Video Object Segmentation Challenge",
    "title": "Spatio-Temporal Attention Network for Video Instance Segmentation",
    "authors": [
      "Xiaoyu Liu",
      "Haibing Ren",
      "Tingmeng Ye"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/YouTube-VOS/Liu_Spatio-Temporal_Attention_Network_for_Video_Instance_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/YouTube-VOS/Liu_Spatio-Temporal_Attention_Network_for_Video_Instance_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a method named spatio-temporal attention network for video instance segmentation. The spatio-temporal attention network can estimate the global correlation map between the successive frames and transfers it to the attention map. Added with the attention information, the new features may enhance the response of the instance for pre-defined categories. Therefore, the detection, segmentation and tracking accuracy will be greatly improved. Experimental result shows that combined with MaskTrack R-CNN, it may improve the video instance segmentation accuracy from 0.293 to 0.400@Youtube VIS test dataset with a single model. Our method took the 6th place in the video instance segmentation track of the 2nd Large-scale Video Object Segmentation Challenge.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_squeezedbilinearpoolingforfine-grainedvisualcategorization": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Squeezed Bilinear Pooling for Fine-Grained Visual Categorization",
    "authors": [
      "Qiyu Liao",
      "Dadong Wang",
      "Hamish Holewa",
      "Min Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Liao_Squeezed_Bilinear_Pooling_for_Fine-Grained_Visual_Categorization_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Liao_Squeezed_Bilinear_Pooling_for_Fine-Grained_Visual_Categorization_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a supervised selection based method to decrease both the computation and the feature dimension of the original bilinear pooling. Different from currently existing compressed second-order pooling methods, the proposed selection method is matrix normalization applicable. Moreover, by extracting the selected highly semantic feature channels, we proposed the Fisher- Recurrent-Attention structure and achieved state-of-the-art fine-grained classification results among the VGG-16 based models.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_learningtofindcorrelatedfeaturesbymaximizinginformationflowinconvolutionalneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Learning to Find Correlated Features by Maximizing Information Flow in Convolutional Neural Networks",
    "authors": [
      "Wei Shen",
      "Fei Li",
      "Rujie Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Shen_Learning_to_Find_Correlated_Features_by_Maximizing_Information_Flow_in_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Shen_Learning_to_Find_Correlated_Features_by_Maximizing_Information_Flow_in_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Training convolutional neural networks for image classification tasks usually causes information loss. Although most of the time the information is redundant with respect to the target task, there are still cases where discriminative information is also discarded. For example, if images that belong to the same category have multiple correlated features, the model may only learn a subset of the features and ignore the rest. This may not be a problem unless the classification in the test set highly relies on the ignored features. We argue that the discard of the correlated discriminative information is partially caused by the fact that the minimization of the classification loss doesn't ensure to learn all discriminative information but only the most discriminative information given the training set. To address this problem, we propose an information flow maximization (IFM) loss as a regularization term to find the discriminative correlated features. With less information loss the classifier can make predictions based on more informative features. We validate our method on the shiftedMNIST dataset and show the effectiveness of IFM loss in learning representative and discriminative features.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_exploringdynamicroutingasapoolinglayer": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Exploring Dynamic Routing As A Pooling Layer",
    "authors": [
      "Lei Zhao",
      "Lei Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Zhao_Exploring_Dynamic_Routing_As_A_Pooling_Layer_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Zhao_Exploring_Dynamic_Routing_As_A_Pooling_Layer_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Dynamic routing is a routing-by-agreement mechanism which is important for achieving the equivariance and invariance properties for capsule network (CapsNet). It is valuable to explore the nature of dynamic routing for better understanding of the capsule idea and further improving the performance of neural networks. This paper explores the dynamic routing from the pooling perspective. We modify the original dynamic routing algorithm for better applying it in traditional Convolutional Neural Networks (CNNs) as a pooling layer. We also use a parameter l in softmax to smoothly adjust the sparsity in the routing, which leads to lower cost compared to the original dynamic routing. We experimentally show that the dynamic routing can be applied to beyond the capsule network to improve the performance of CNNs, and the coupling coefficients generated by the routing can be used to generate heatmaps which provide visual explanations to some extent. Further, the proposed dynamic routing method, combining a CNN backbone, achieves better results with much fewer parameters than the baselines on aff-NIST and multi-MNIST tasks.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_interpretingintentionallyflawedmodelswithlinearprobes": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Interpreting Intentionally Flawed Models with Linear Probes",
    "authors": [
      "Mara Graziani",
      "Henning Muller",
      "Vincent Andrearczyk"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Graziani_Interpreting_Intentionally_Flawed_Models_with_Linear_Probes_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Graziani_Interpreting_Intentionally_Flawed_Models_with_Linear_Probes_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The representational differences between generalizing networks and intentionally flawed models can be insightful on the dynamics of network training. Do memorizing networks, e.g. networks that learn random label correspondences, focus on specific patterns in the data to memorize the labels? Are the features learned by a generalizing network affected by randomization of the model parameters? In high-risk applications such as medical, legal or financial domains, highlighting the representational differences that help generalization may be even more important than the model performance itself. In this paper, we probe the activations of intermediate layers with linear classification and regression. Results show that the bias towards simple solutions of generalizing networks is maintained even when statistical irregularities are intentionally introduced.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_functionnormsforneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Function Norms for Neural Networks",
    "authors": [
      "Amal Rannen-Triki",
      "Maxim Berman",
      "Vladimir Kolmogorov",
      "Matthew B. Blaschko"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Rannen-Triki_Function_Norms_for_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Rannen-Triki_Function_Norms_for_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep neural networks (DNNs) have become increasingly important due to their excellent empirical performance on a wide range of problems. However, regularization is generally achieved by indirect means, largely due to the complex set of functions defined by a network and the difficulty in measuring function complexity. There exists no method in the literature for additive regularization based on a norm of the function, as is classically considered in statistical learning theory. In this work, we study the tractability of function norms for deep neural networks with ReLU activations. We provide, to the best of our knowledge, the first proof in the literature of the NP-hardness of computing function norms of DNNs of 3 or more layers. We also highlight a fundamental difference between shallow and deep networks. In the light on these results, we propose a new regularization strategy based on approximate function norms, and show its efficiency on a segmentation task with a DNN.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_opensetrecognitionthroughdeepneuralnetworkuncertaintydoesout-of-distributiondetectionrequiregenerativeclassifiers?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Open Set Recognition Through Deep Neural Network Uncertainty: Does Out-of-Distribution Detection Require Generative Classifiers?",
    "authors": [
      "Martin Mundt",
      "Iuliia Pliushch",
      "Sagnik Majumder",
      "Visvanathan Ramesh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Mundt_Open_Set_Recognition_Through_Deep_Neural_Network_Uncertainty_Does_Out-of-Distribution_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Mundt_Open_Set_Recognition_Through_Deep_Neural_Network_Uncertainty_Does_Out-of-Distribution_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present an analysis of predictive uncertainty based out-of-distribution detection for different approaches to estimate various models' epistemic uncertainty and contrast it with extreme value theory based open set recognition. While the former alone does not seem to be enough to overcome this challenge, we demonstrate that uncertainty goes hand in hand with the latter method. This seems to be particularly reflected in a generative model approach, where we show that posterior based open set recognition outperforms discriminative models and predictive uncertainty based outlier rejection, raising the question of whether classifiers need to be generative in order to know what they have not seen.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_directvalidationoftheinformationbottleneckprinciplefordeepnets": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Direct Validation of the Information Bottleneck Principle for Deep Nets",
    "authors": [
      "Adar Elad",
      "Doron Haviv",
      "Yochai Blau",
      "Tomer Michaeli"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Elad_Direct_Validation_of_the_Information_Bottleneck_Principle_for_Deep_Nets_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Elad_Direct_Validation_of_the_Information_Bottleneck_Principle_for_Deep_Nets_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The information bottleneck (IB) has been suggested as a fundamental principle governing performance in deep neural nets (DNNs). This idea sparked research on the information plane dynamics during training with the cross-entropy loss, and on using the IB of some \"bottleneck\" layer as a loss function. However, the claim that reaching the maximal value of the IB Lagrangian in each layer leads to optimal performance, was in fact never directly confirmed. In this paper, we propose a direct way of validating this hypothesis, using layer-by-layer training with the IB loss. In accordance with the original theory, we train each DNN layer explicitly with the IB objective (and without any classification loss), and freeze it before moving on to train the next layer. While mutual information (MI) is generally hard to estimate in high dimensions, we show that in the case of MI between DNN layers, this can be done quite accurately using a modification of the recently proposed mutual information neural estimator. Interestingly, we find that layer-by-layer training with the IB loss leads to accuracy which is on-par with end-to-end training with the cross entropy loss. This is, thus, the first direct experimental illustration of the link between the IB value in each layer, and a net's performance.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_lautumregularizationforsemi-supervisedtransferlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Lautum Regularization for Semi-Supervised Transfer Learning",
    "authors": [
      "Daniel Jakubovitz",
      "Miguel R. D. Rodrigues",
      "Raja Giryes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Jakubovitz_Lautum_Regularization_for_Semi-Supervised_Transfer_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Jakubovitz_Lautum_Regularization_for_Semi-Supervised_Transfer_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Transfer learning is a very important tool in deep learning as it allows propagating information from one \"source dataset\" to another \"target dataset\", especially in the case of a small number of training examples in the latter. Yet, discrepancies between the underlying distributions of the source and target data are commonplace and are known to have a substantial impact on algorithm performance. In this work we suggest a novel information theoretic approach for the analysis of the performance of deep neural networks in the context of transfer learning. We focus on the task of semi-supervised transfer learning, in which unlabeled samples from the target dataset are available during the network training on the source dataset. Our theory suggests that one may improve the transferability of a deep neural network by imposing a Lautum information based regularization that relates the network weights to the target data. We demonstrate the effectiveness of the proposed approach in various transfer learning experiments.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_anoveladversarialinferenceframeworkforvideopredictionwithactioncontrol": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "A Novel Adversarial Inference Framework for Video Prediction with Action Control",
    "authors": [
      "Zhihang Hu",
      "Jason Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Hu_A_Novel_Adversarial_Inference_Framework_for_Video_Prediction_with_Action_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Hu_A_Novel_Adversarial_Inference_Framework_for_Video_Prediction_with_Action_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The ability of predicting future frames in video sequences, known as video prediction, is an appealing yet challenging task in computer vision. This task requires an in-depth representation of video sequences and a deep understanding of real-word causal rules. Existing approaches often result in blur predictions and lack the ability of action control. To tackle these problems, we propose a framework, called VPGAN, which employs an adversarial inference model and a cycle-consistency loss function to empower the framework to obtain more accurate predictions. In addition, we incorporate a conformal mapping network structure into VPGAN to enable action control for generating desirable future frames. In this way, VPGAN is able to produce fake videos of an object moving along a specific direction. Experimental results show that a combination of VPGAN with some pre-trained image segmentation models outperforms existing stochastic video prediction methods.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_efficientpriorsforscalablevariationalinferenceinbayesiandeepneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Efficient Priors for Scalable Variational Inference in Bayesian Deep Neural Networks",
    "authors": [
      "Ranganath Krishnan",
      "Mahesh Subedar",
      "Omesh Tickoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Krishnan_Efficient_Priors_for_Scalable_Variational_Inference_in_Bayesian_Deep_Neural_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Krishnan_Efficient_Priors_for_Scalable_Variational_Inference_in_Bayesian_Deep_Neural_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Stochastic variational inference for Bayesian deep neural networks (DNNs) requires specifying priors and approximate posterior distributions for neural network weights. Specifying meaningful weight priors is a challenging problem, particularly for scaling variational inference to deeper architectures involving high dimensional weight space. Based on empirical Bayes approach, we propose Bayesian MOdel Priors Extracted from Deterministic DNN (MOPED) method to choose meaningful prior distributions over weight space using deterministic weights derived from the pretrained DNNs of equivalent architecture. We empirically evaluate the proposed approach on real-world applications including image classification, video activity recognition and audio classification tasks with varying complex neural network architectures. The proposed method enables scalable variational inference with faster training convergence and provides reliable uncertainty quantification.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_ugllifacealignmentestimatinguncertaintywithgaussianlog-likelihoodloss": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "UGLLI Face Alignment: Estimating Uncertainty with Gaussian Log-Likelihood Loss",
    "authors": [
      "Abhinav Kumar",
      "Tim K. Marks",
      "Wenxuan Mou",
      "Chen Feng",
      "Xiaoming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Kumar_UGLLI_Face_Alignment_Estimating_Uncertainty_with_Gaussian_Log-Likelihood_Loss_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Kumar_UGLLI_Face_Alignment_Estimating_Uncertainty_with_Gaussian_Log-Likelihood_Loss_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Modern face alignment methods have become quite accurate at predicting the locations of facial landmarks, but they do not typically estimate the uncertainty of their predicted locations. In this paper, we present a novel frame-work for jointly predicting facial landmark locations and the associated uncertainties, modeled as 2D Gaussian distributions, using Gaussian log-likelihood loss. Not only does our joint estimation of uncertainty and landmark locations yield state-of-the-art estimates of the uncertainty of predicted landmark locations, but it also yields state-of-the-art estimates for the landmark locations (face alignment). Our method's estimates of the uncertainty of landmarks' predicted locations could be used to automatically identify input images on which face alignment fails, which can be critical for downstream tasks.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_openvinodeeplearningworkbenchcomprehensiveanalysisandtuningofneuralnetworksinference": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "OpenVINO Deep Learning Workbench: Comprehensive Analysis and Tuning of Neural Networks Inference",
    "authors": [
      "Yury Gorbachev",
      "Mikhail Fedorov",
      "Iliya Slavutin",
      "Artyom Tugarev",
      "Marat Fatekhov",
      "Yaroslav Tarkan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Gorbachev_OpenVINO_Deep_Learning_Workbench_Comprehensive_Analysis_and_Tuning_of_Neural_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Gorbachev_OpenVINO_Deep_Learning_Workbench_Comprehensive_Analysis_and_Tuning_of_Neural_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A task of maximizing deep learning neural networks performance is a challenging and actual goal of modern hardware and software development. Regardless the huge variety of optimization techniques and emerging dedicated hardware platforms, the process of tuning the performance of the neural network is hard. It requires configuring dozens of hyper parameters of optimization algorithms, selecting appropriate metrics, benchmarking the intermediate solutions to choose the best method, platform etc. Moreover, it is required to setup the hardware for the specific inference target. This paper introduces a sophisticated software solution (Deep Learning Workbench) that provides interactive user interface, simplified process of 8-bit quantization, speeding up convolutional operations using the Winograds minimal filtering algorithms, measuring accuracy of the resulting model. The proposed software is built over the open source OpenVINO framework and supports huge range of modern deep learning models.\r",
    "code_link": ""
  },
  "iccv2019_sdl-cv_stochasticrelationalnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Stochastic Relational Network",
    "authors": [
      "Kang Min Yoo",
      "Hyun Soo Jo",
      "Hanbit Lee",
      "Jeeseung Han",
      "Sang-goo Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Yoo_Stochastic_Relational_Network_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Yoo_Stochastic_Relational_Network_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Reasoning about relations among a set of objects is one of the key aspects of human intelligence, and Relational Networks (RNs) are one of the classes of architectures that specializes in such relational reasoning. However, RNs are limited in their general applicability due to significant (quadratic) complexity of all-pair comparative operations. In this paper, we propose Stochastic RN (SRN) that learns to prune distractors and pick task-related objects that are crucial for relational reasoning, thereby reducing forward and backward computation costs with minimal sacrifice. We empirically show that our approach is effective in a real-world visual question-answering task, where vanilla RNs might be computationally expensive to run due to the sheer number of candidate objects for each image.\r",
    "code_link": "https://github.com/kaniblu/pythia-srn"
  },
  "iccv2019_sdl-cv_onthegeometryofrectifierconvolutionalneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "On the Geometry of Rectifier Convolutional Neural Networks",
    "authors": [
      "Matteo Gamba",
      "Hossein Azizpour",
      "Stefan Carlsson",
      "Marten Bjorkman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Gamba_On_the_Geometry_of_Rectifier_Convolutional_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Gamba_On_the_Geometry_of_Rectifier_Convolutional_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " While recent studies have shed light on the expressivity, complexity and compositionality of convolutional networks, the real inductive bias of the family of functions reachable by gradient descent on natural data is still unknown. By exploiting symmetries in the preactivation space of convolutional layers, we present preliminary empirical evidence of regularities in the preimage of trained rectifier networks, in terms of arrangements of polytopes, and relate it to the nonlinear transformations applied by the network to its input.\r",
    "code_link": "https://github.com/magamba/cones"
  },
  "iccv2019_sdl-cv_attackagnosticstatisticalmethodforadversarialdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SDL-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Statistical Deep Learning in Computer Vision",
    "title": "Attack Agnostic Statistical Method for Adversarial Detection",
    "authors": [
      "Sambuddha Saha",
      "Aashish Kumar",
      "Pratyush Sahay",
      "George Jose",
      "Srinivas Kruthiventi",
      "Harikrishna Muralidhara"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SDL-CV/Saha_Attack_Agnostic_Statistical_Method_for_Adversarial_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SDL-CV/Saha_Attack_Agnostic_Statistical_Method_for_Adversarial_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep Learning based AI systems have shown great promise in various domains such as vision, audio, autonomous systems (vehicles, drones), etc. Recent research on neural networks has shown the susceptibility of deep networks to adversarial attacks - a technique of adding small perturbations to the inputs which can fool a deep network into misclassifying them. Developing defenses against such adversarial attacks is an active research area, with some approaches proposing robust models that are immune to such adversaries, while other techniques attempt to detect such adversarial inputs. In this paper, we present a novel statistical approach for adversarial detection in image classification. Our approach is based on constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class. For this purpose, we make use of various statistical distances such as ED (Energy Distance), MMD (Maximum Mean Discrepancy) for adversarial detection, and analyze the performance of each metric. We experimentally show that our approach achieves good adversarial detection performance on MNIST and CIFAR-10 datasets irrespective of the attack method, sample size and the degree of adversarial perturbation.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_estimationofabsolutescaleinmonocularslamusingsyntheticdata": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data",
    "authors": [
      "Danila Rukhovich",
      "Daniel Mouritzen",
      "Ralf Kaestner",
      "Martin Rufli",
      "Alexander Velizhev"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Rukhovich_Estimation_of_Absolute_Scale_in_Monocular_SLAM_Using_Synthetic_Data_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Rukhovich_Estimation_of_Absolute_Scale_in_Monocular_SLAM_Using_Synthetic_Data_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper addresses the problem of scale estimation in monocular SLAM by estimating absolute distances between camera centers of consecutive image frames. These estimates would improve the overall performance of classical (not deep) SLAM systems and allow metric feature locations to be recovered from a single monocular camera. We propose several network architectures that lead to an improvement of scale estimation accuracy over the state of the art. In addition, we exploit a possibility to train the neural network only with synthetic data derived from a computer graphics simulator. Our key insight is that, using only synthetic training inputs, we can achieve similar scale estimation accuracy as that obtained from real data. This fact indicates that fully annotated simulated data is a viable alternative to existing deep-learning-based SLAM systems trained on real (unlabeled) data. Our experiments with unsupervised domain adaptation also show that the difference in visual appearance between simulated and real data does not affect scale estimation results. Our method operates with low-resolution images (0.03 MP), which makes it practical for real-time SLAM applications with a monocular camera.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_short-termpredictionandmulti-camerafusiononsemanticgrids": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Short-Term Prediction and Multi-Camera Fusion on Semantic Grids",
    "authors": [
      "Lukas Hoyer",
      "Patrick Kesper",
      "Anna Khoreva",
      "Volker Fischer"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Hoyer_Short-Term_Prediction_and_Multi-Camera_Fusion_on_Semantic_Grids_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Hoyer_Short-Term_Prediction_and_Multi-Camera_Fusion_on_Semantic_Grids_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " An environment representation (ER) is a substantial part of every autonomous system. It introduces a common interface between perception and other system components, such as decision making, and allows downstream algorithms to deal with abstract data without knowledge of the used sensor. In this work, we propose and evaluate a novel architecture that generates an egocentric, grid-based, predictive, and semantically-interpretable ER, which we call semantic grid. We show that our approach supports the spatio-temporal fusion of multiple camera sequences and short-term prediction in such an ER. Our design utilizes a strong semantic segmentation network together with depth and egomotion estimates to first extract semantic information from multiple camera streams and then transform these separately into egocentric temporally-aligned bird's-eye view grids. A deep encoder-decoder network is trained to fuse a stack of these grids into a unified semantic grid and to predict the dynamics of its surrounding. We evaluate this representation on real-world sequences of Cityscapes and show that our architecture can make accurate predictions in complex sensor fusion scenarios and significantly outperforms a model-driven baseline in a category-based evaluation.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_probabilisticvehiclereconstructionusingamulti-taskcnn": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Probabilistic Vehicle Reconstruction Using a Multi-Task CNN",
    "authors": [
      "Max Coenen",
      "Franz Rottensteiner"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Coenen_Probabilistic_Vehicle_Reconstruction_Using_a_Multi-Task_CNN_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Coenen_Probabilistic_Vehicle_Reconstruction_Using_a_Multi-Task_CNN_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The retrieval of the 3D pose and shape of objects from images is an ill-posed problem. A common way to object reconstruction is to match entities such as keypoints, edges, or contours of a deformable 3D model, used as shape prior, to their corresponding entities inferred from the image. However, such approaches are highly sensitive to model initialisation, imprecise keypoint localisations and/or illumination conditions. In this paper, we present a probabilistic approach for shape-aware 3D vehicle reconstruction from stereo images that leverages the outputs of a novel multi-task CNN. Specifically, we train a CNN that outputs probability distributions for the vehicle's orientation and for both, vehicle keypoints and wireframe edges. Together with 3D stereo information we integrate the predicted distributions into a common probabilistic framework. We believe that the CNN-based detection of wireframe edges reduces the sensitivity to illumination conditions and object contrast and that using the raw probability maps instead of inferring keypoint positions reduces the sensitivity to keypoint localisation errors. We show that our method achieves state-of-the-art results, evaluating our method on the challenging KITTI benchmark and on our own new 'Stereo-Vehicle' dataset.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_unsupervisedlabeledlanemarkersusingmaps": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Unsupervised Labeled Lane Markers Using Maps",
    "authors": [
      "Karsten Behrendt",
      "Ryan Soussan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Behrendt_Unsupervised_Labeled_Lane_Markers_Using_Maps_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Behrendt_Unsupervised_Labeled_Lane_Markers_Using_Maps_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Large and diverse annotated datasets can significantly increase the accuracy of machine learning models. However, human annotations can be cost and time intensive, and generating 3D information and connectivity for image features using manual annotations can be difficult and error-prone. We therefore propose to automatically annotate lane markers in images and assign attributes to each marker such as 3D positions by using map data. Our method projects map lane markers into image space for far distances and relies on a sample-based optimization to refine projections and increase the accuracy of the labels. As part of this work, we publish the Unsupervised LLAMAS dataset of 100,042 labeled lane marker images from about 350 km recorded drives which make this one of the largest high-quality lane marker datasets that is freely available. We estimate that manually annotating a dataset of this size would take several person years. The dataset contains pixel-level annotations of dashed lane markers, 2D and 3D endpoints for each marker, and lane associations to link markers. With the dataset, we create and open source benchmark challenges for binary marker segmentation, lane-dependent pixel-level segmentation, and lane border regression to enable a straightforward comparison of different detection approaches at https://unsupervised-llamas.com.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_boxyvehicledetectioninlargeimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Boxy Vehicle Detection in Large Images",
    "authors": [
      "Karsten Behrendt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Behrendt_Boxy_Vehicle_Detection_in_Large_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Behrendt_Boxy_Vehicle_Detection_in_Large_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Camera-based object detection and automated driving in general have greatly improved over the last few years. Parts of these improvements can be attributed to public datasets which allow researchers around the world to work with data that would often be too expensive to collect and annotate for individual teams. Current vehicle detection datasets and approaches often focus on axis-aligned bounding boxes or semantic segmentation. Axis-aligned bounding boxes often misrepresent vehicle sizes and may intrude into neighboring lanes. While pixel level segmentations are more accurate, they can be hard to process and leverage for trajectory planning systems. We therefore present the Boxy dataset for image-based vehicle detection. Boxy is one of the largest public vehicle detection datasets with 1.99 million annotated vehicles in 200,000 images, including sunny, rainy, and nighttime driving. If possible, vehicle annotations are split into their visible sides to give the impression of 3D boxes for a more accurate representation with little overhead. Five megapixel images with annotations down to a few pixels make this dataset especially challenging. With Boxy, we provide initial benchmark challenges for bounding box, polygon, and real-time detections. All benchmarks are open-source so that additional metrics and benchmarks may be added at https://boxy-dataset.com.\r",
    "code_link": "https://github.com/udacity/self-driving-car"
  },
  "iccv2019_cvrsuad_shelfnetforfastsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "ShelfNet for Fast Semantic Segmentation",
    "authors": [
      "Juntang Zhuang",
      "Junlin Yang",
      "Lin Gu",
      "Nicha Dvornek"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Zhuang_ShelfNet_for_Fast_Semantic_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Zhuang_ShelfNet_for_Fast_Semantic_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we present ShelfNet, a novel architecture for accurate fast semantic segmentation. Different from the single encoder-decoder structure, ShelfNet has multiple encoder-decoder branch pairs with skip connections at each spatial level, which looks like a shelf with multiple columns. The shelf-shaped structure can be viewed as an ensemble of multiple deep and shallow paths, thus improving accuracy. We significantly reduce computation burden by reducing channel number, at the same time achieving high accuracy with this unique structure. In addition, we propose a shared-weight strategy in the residual block which reduces parameter number without sacrificing performance. Compared with popular non real-time methods such as PSPNet, our ShelfNet achieves 4x faster inference speed with similar accuracy on PASCAL VOC dataset. Compared with real-time segmentation models such as BiSeNet, our model achieves higher accuracy at comparable speed on the Cityscapes Dataset, enabling the application in speed-demanding tasks such as street-scene understanding for autonomous driving. Furthermore, our ShelfNet achieves 79.0% mIoU on Cityscapes Dataset with ResNet34 backbone, outper-forming PSPNet and BiSeNet with large backbones such as ResNet101. Through extensive experiments, we validated the superior performance of ShelfNet. We provide link to the implementation https://github.com/ juntang-zhuang/ShelfNet-lw-cityscapes.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_monocular3dobjectdetectionwithpseudo-lidarpointcloud": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud",
    "authors": [
      "Xinshuo Weng",
      "Kris Kitani"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Weng_Monocular_3D_Object_Detection_with_Pseudo-LiDAR_Point_Cloud_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Weng_Monocular_3D_Object_Detection_with_Pseudo-LiDAR_Point_Cloud_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Monocular 3D scene understanding tasks, such as object size estimation, heading angle estimation and 3D localization, is challenging. Successful modern-day methods for 3D scene understanding require the use of a 3D sensor. On the other hand, single image-based methods have significantly worse performance. In this work, we aim at bridging the performance gap between 3D sensing and 2D sensing for 3D object detection by enhancing LiDAR-based algorithms to work with single image input. Specifically, we perform monocular depth estimation and lift the input image to a point cloud representation, which we call pseudo-LiDAR point cloud. Then we can train a LiDAR-based 3D detection network with our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D detection algorithms, we detect 2D object proposals in the input image and extract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an oriented 3D bounding box is detected for each frustum. To handle the large amount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a 2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding box to have a high overlap with its corresponding 2D proposal after projecting onto the image; (2) use the instance mask instead of the bounding box as the representation of 2D proposals, in order to reduce the number of points not belonging to the object in the point cloud frustum. Through our evaluation on the KITTI benchmark, we achieve the top-ranked performance on both bird's eye view and 3D object detection among all monocular methods, effectively quadrupling the performance over previous state-of-the-art. Our code is available at https://github.com/xinshuoweng/Mono3D_PLiDAR.\r",
    "code_link": "https://github.com/xinshuoweng/Mono3D_PLiDAR"
  },
  "iccv2019_cvrsuad_roadsceneunderstandingbyoccupancygridlearningfromsparseradarclustersusingsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Road Scene Understanding by Occupancy Grid Learning from Sparse Radar Clusters using Semantic Segmentation",
    "authors": [
      "Liat Sless",
      "Bat El Shlomo",
      "Gilad Cohen",
      "Shaul Oron"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Sless_Road_Scene_Understanding_by_Occupancy_Grid_Learning_from_Sparse_Radar_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Sless_Road_Scene_Understanding_by_Occupancy_Grid_Learning_from_Sparse_Radar_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Occupancy grid mapping is an important component in road scene understanding for autonomous driving. It encapsulates information of the drivable area, road obstacles and enables safe autonomous driving. Radars are an emerging sensor in autonomous vehicle vision, becoming more widely used due to their long range sensing, low cost, and robustness to severe weather conditions. Despite recent advances in deep learning technology, occupancy grid mapping from radar data is still mostly done using classical filtering approaches. In this work, we propose learning the inverse sensor model used for occupancy grid mapping from clustered radar data. This is done in a data driven approach that leverages computer vision techniques. This task is very challenging due to data sparsity and noise characteristics of the radar sensor. The problem is formulated as a semantic segmentation task and we show how it can be learned using lidar data for generating ground truth. We show both qualitatively and quantitatively that our learned occupancy net outperforms classic methods by a large margin using the recently released NuScenes real-world driving data.\r",
    "code_link": "https://github.com/liat-s/radar_occupancy"
  },
  "iccv2019_cvrsuad_reverseandboundaryattentionnetworkforroadsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Reverse and Boundary Attention Network for Road Segmentation",
    "authors": [
      "Jee-Young Sun",
      "Seung-Wook Kim",
      "Sang-Won Lee",
      "Ye-Won Kim",
      "Sung-Jea Ko"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Sun_Reverse_and_Boundary_Attention_Network_for_Road_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Sun_Reverse_and_Boundary_Attention_Network_for_Road_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Road segmentation is an essential task to perceive the driving environment in autonomous driving and advanced driver assistance systems. With the development of deep learning, road segmentation has achieved great progress in recent years. However, there still remain some problems including the inaccurate road boundary and the illumination variations such as shadows and over-exposure regions. To solve these problems, we propose a residual learning-based network architecture with residual refinement module composed of the reverse attention and boundary attention units for road segmentation. The network first predicts a coarse road region from deeper-level feature maps and gradually refines the prediction by learning the residual in a top-down approach. The reverse and boundary attention units in residual refinement module guide the network to focus on the features in the previously missing region and the region near the road boundary. In addition, we introduce the boundary-aware weighted loss to reduce the false prediction. Experimental results demonstrate that the proposed approach outperforms the state-of-the-art methods in terms of the segmentation accuracy in various benchmark datasets for traffic scene understanding.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_smallobstacleavoidancebasedonrgb-dsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Small Obstacle Avoidance Based on RGB-D Semantic Segmentation",
    "authors": [
      "Minjie Hua",
      "Yibing Nan",
      "Shiguo Lian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Hua_Small_Obstacle_Avoidance_Based_on_RGB-D_Semantic_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Hua_Small_Obstacle_Avoidance_Based_on_RGB-D_Semantic_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper presents a novel obstacle avoidance system for road robots equipped with RGB-D sensor that captures scenes of its way forward. The purpose of the system is to have road robots move around autonomously and constantly without any collision even with small obstacles, which are often missed by existing solutions. For each input RGB-D image, the system uses a new two-stage semantic segmentation network followed by the morphological processing to generate the accurate semantic map containing road and obstacles. Based on the map, the local path planning is applied to avoid possible collision. Additionally, optical flow supervision and motion blurring augmented training scheme is applied to improve temporal consistency between adjacent frames and overcome the disturbance caused by camera shake. Various experiments are conducted to show that the proposed architecture obtains high performance both in indoor and outdoor scenarios.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_robustabsoluteandrelativeposeestimationofacentralcamerasystemfrom2d-3dlinecorrespondences": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Robust Absolute and Relative Pose Estimation of a Central Camera System from 2D-3D Line Correspondences",
    "authors": [
      "Hichem Abdellali",
      "Robert Frohlich",
      "Zoltan Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Abdellali_Robust_Absolute_and_Relative_Pose_Estimation_of_a_Central_Camera_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Abdellali_Robust_Absolute_and_Relative_Pose_Estimation_of_a_Central_Camera_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a new algorithm for estimating the absolute and relative pose of a camera system composed of general central projection cameras such as perspective and omnidirectional cameras. First, we derive a minimal solver for the minimal case of 3 line pairs per camera, which is used within a RANSAC algorithm for outlier filtering. Second, we also formulate a direct least squares solver which finds an optimal solution in case of noisy (but inlier) 2D-3D line pairs. Both solver relies on Grobner basis, hence they provide an accurate solution within a few milliseconds in Matlab. The algorithm has been validated on a large synthetic dataset as well as real data. Experimental results confirm the stable and real-time performance under realistic outlier ratio and noise on the line parameters. Comparative tests show that our method compares favorably to the latest state of the art algorithms.\r",
    "code_link": "https://github.com/laurentkneip/polyjam"
  },
  "iccv2019_cvrsuad_end-to-endlanedetectionthroughdifferentiableleast-squaresfitting": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "End-to-end Lane Detection through Differentiable Least-Squares Fitting",
    "authors": [
      "Wouter Van Gansbeke",
      "Bert De Brabandere",
      "Davy Neven",
      "Marc Proesmans",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Van_Gansbeke_End-to-end_Lane_Detection_through_Differentiable_Least-Squares_Fitting_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Van_Gansbeke_End-to-end_Lane_Detection_through_Differentiable_Least-Squares_Fitting_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Lane detection is typically tackled with a two-step pipeline in which a segmentation mask of the lane markings is predicted first, and a lane line model like a parabola or spline is fitted to the post-processed mask next. The problem with such a two-step approach is that the parameters of the network are not optimized for the true task of interest (estimating the lane curvature parameters) but for a proxy task (segmenting the lane markings), resulting in suboptimal performance. In this work, we propose a method to train a lane detector in an end-to-end manner, directly regressing the lane parameters. The architecture consists of two components: a deep network that predicts a segmentation-ike weight map for each lane line, and a differentiable least-squares fitting module that returns for each map the parameters of the best-fitting curve in the weighted least-squares sense. These parameters can subsequently be supervised with a loss function of choice. Our method relies on the observation that it is possible to backpropagate through a least-squares fitting procedure. This leads to an end-to-end method where the features are optimized for the true task of interest: the network implicitly learns to generate features that prevent instabilities during the model fitting step, as opposed to two-step pipelines that need to handle outliers with heuristics. Additionally, the system is not just a black box but offers a degree of interpretability because the intermediately generated segmentation-like weight maps can be inspected and visualized. Code and a video is available at github.com/wvangansbeke/LaneDetection_End2End.\r",
    "code_link": "https://github.com/wvangansbeke/LaneDetection"
  },
  "iccv2019_cvrsuad_temporalcoherenceforactivelearninginvideos": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Temporal Coherence for Active Learning in Videos",
    "authors": [
      "Javad Zolfaghari Bengar",
      "Abel Gonzalez-Garcia",
      "Gabriel Villalonga",
      "Bogdan Raducanu",
      "Hamed Habibi Aghdam",
      "Mikhail Mozerov",
      "Antonio M. Lopez",
      "Joost van de Weijer"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Bengar_Temporal_Coherence_for_Active_Learning_in_Videos_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Bengar_Temporal_Coherence_for_Active_Learning_in_Videos_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Autonomous driving systems require huge amounts of data to train. Manual annotation of this data is time-consuming and prohibitively expensive since it involves human resources. Therefore, active learning emerged as an alternative to ease this effort and to make data annotation more manageable. In this paper, we introduce a novel active learning approach for object detection in videos by exploiting temporal coherence. Our active learning criterion is based on the estimated number of errors in terms of false positives and false negatives. The detections obtained by the object detector are used to define the nodes of a graph and tracked forward and backward to temporally link the nodes. Minimizing an energy function defined on this graphical model provides estimates of both false positives and false negatives. Additionally, we introduce a synthetic video dataset, called SYNTHIA-AL, specially designed to evaluate active learning for video object detection in road scenes. Finally, we show that our approach outperforms active learning baselines tested on two datasets.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_vehicledetectionwithautomotiveradarusingdeeplearningonrange-azimuth-dopplertensors": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Vehicle Detection With Automotive Radar Using Deep Learning on Range-Azimuth-Doppler Tensors",
    "authors": [
      "Bence Major",
      "Daniel Fontijne",
      "Amin Ansari",
      "Ravi Teja Sukhavasi",
      "Radhika Gowaikar",
      "Michael Hamilton",
      "Sean Lee",
      "Slawomir Grzechnik",
      "Sundar Subramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Major_Vehicle_Detection_With_Automotive_Radar_Using_Deep_Learning_on_Range-Azimuth-Doppler_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Major_Vehicle_Detection_With_Automotive_Radar_Using_Deep_Learning_on_Range-Azimuth-Doppler_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Radar has been a key enabler of advanced driver assistance systems in automotive for over two decades. Being an inexpensive, all-weather and long-range sensor that simultaneously provides velocity measurements, radar is expected to be indispensable to the future of autonomous driving. Traditional radar signal processing techniques often cannot distinguish reflections from objects of interest from clutter and are generally limited to detecting peaks in the received signal. These peak detection methods effectively collapse the image-like radar signal into a sparse point cloud. In this paper, we demonstrate a deep-learning-based vehicle detection solution which operates on the image-like tensor instead of the point cloud resulted by peak detection.To the best of our knowledge, we are the first to implement such a system.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_exploitingtemporalityforsemi-supervisedvideosegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "Exploiting Temporality for Semi-Supervised Video Segmentation",
    "authors": [
      "Radu Sibechi",
      "Olaf Booij",
      "Nora Baka",
      "Peter Bloem"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Sibechi_Exploiting_Temporality_for_Semi-Supervised_Video_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Sibechi_Exploiting_Temporality_for_Semi-Supervised_Video_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years, there has been remarkable progress in supervised image segmentation. Video segmentation is less explored, despite the temporal dimension being highly informative. Semantic labels, e.g. that cannot be accurately detected in the current frame, may be inferred by incorporating information from previous frames. However, video segmentation is challenging due to the amount of data that needs to be processed and, more importantly, the cost involved in obtaining ground truth annotations for each frame. In this paper, we tackle the issue of label scarcity by using consecutive frames of a video, where only one frame is annotated. We propose a deep, end-to-end trainable model which leverages temporal information in order to make use of easy to acquire unlabeled data. Our network architecture relies on a novel interconnection of two components: a fully convolutional network to model spatial information and temporal units that are employed at intermediate levels of the convolutional network in order to propagate information through time. The main contribution of this work is the guidance of the temporal signal through the network. We show that only placing a temporal module between the encoder and decoder is suboptimal (baseline). Our extensive experiments on the CityScapes dataset indicate that the resulting model can leverage unlabeled temporal frames and significantly outperform both the frame-by-frame image segmentation and the baseline approach.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_lu-netanefficientnetworkfor3dlidarpointcloudsemanticsegmentationbasedonend-to-end-learned3dfeaturesandu-net": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "LU-Net: An Efficient Network for 3D LiDAR Point Cloud Semantic Segmentation Based on End-to-End-Learned 3D Features and U-Net",
    "authors": [
      "Pierre Biasutti",
      "Vincent Lepetit",
      "Jean-Francois Aujol",
      "Mathieu Bredif",
      "Aurelie Bugeau"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Biasutti_LU-Net_An_Efficient_Network_for_3D_LiDAR_Point_Cloud_Semantic_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Biasutti_LU-Net_An_Efficient_Network_for_3D_LiDAR_Point_Cloud_Semantic_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose LU-Net - for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efficiently solves the problem as an image processing problem. We first extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can finally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efficient. In this way, we can exploit both the 3D nature of the data and the specificity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.\r",
    "code_link": ""
  },
  "iccv2019_cvrsuad_ibetyouarewronggamblingadversarialnetworksforstructuredsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVRSUAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Road Scene Understanding and Autonomous Driving",
    "title": "I Bet You Are Wrong: Gambling Adversarial Networks for Structured Semantic Segmentation",
    "authors": [
      "Laurens Samson",
      "Nanne van Noord",
      "Olaf Booij",
      "Michael Hofmann",
      "Efstratios Gavves",
      "Mohsen Ghafoorian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVRSUAD/Samson_I_Bet_You_Are_Wrong_Gambling_Adversarial_Networks_for_Structured_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVRSUAD/Samson_I_Bet_You_Are_Wrong_Gambling_Adversarial_Networks_for_Structured_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Adversarial training has been recently employed for realizing structured semantic segmentation, in which the aim is to preserve higher-level scene structural consistencies in dense predictions. However, as we show, value-based discrimination between the predictions from the segmentation network and ground-truth annotations can hinder the training process from learning to improve structural qualities as well as disabling the network from properly expressing uncertainties. In this paper, we rethink adversarial training for semantic segmentation and propose to reformulate the fake/real discrimination framework with a correct/incorrect training objective. More specifically, we replace the discriminator with a \"gambler\" network that learns to spot and distribute its budget in areas where the predictions are clearly wrong, while the segmenter network tries to leave no clear clues for the gambler where to bet. Empirical evaluation on two road-scene semantic segmentation tasks shows that not only does the proposed method re-enable expressing uncertainties, it also improves pixel-wise and structure-based metrics.\r",
    "code_link": ""
  },
  "iccv2019_rlq_extremelowresolutionactionrecognitionwithspatial-temporalmulti-headself-attentionandknowledgedistillation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Extreme Low Resolution Action Recognition with Spatial-Temporal Multi-Head Self-Attention and Knowledge Distillation",
    "authors": [
      "Didik Purwanto",
      "Rizard Renanda Adhi Pramono",
      "Yie-Tarng Chen",
      "Wen-Hsien Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Purwanto_Extreme_Low_Resolution_Action_Recognition_with_Spatial-Temporal_Multi-Head_Self-Attention_and_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Purwanto_Extreme_Low_Resolution_Action_Recognition_with_Spatial-Temporal_Multi-Head_Self-Attention_and_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper proposes a two-stream network with a novel spatial-temporal multi-head self-attention mechanism for action recognition in extreme low resolution (LR) videos. The new approach first utilizes a super resolution (SR) mechanism to provide better visual information to facilitate the network training. To provide more discriminative spatio-temporal features, a knowledge distillation scheme that consists of teacher and student models is employed to enhance the network model using the knowledge from a high resolution (HR) model. Moreover, the two-stream network is combined with a new spatial-temporal multi-head self-attention network to efficaciously learn the long-term temporal dependency. Simulations demonstrate that the proposed method surpasses the state-of-the-art works for extreme LR action recognition on two widespread HMDB-51 and IXMAS datasets.\r",
    "code_link": ""
  },
  "iccv2019_rlq_onlinemulti-taskclusteringforhumanmotionsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Online Multi-Task Clustering for Human Motion Segmentation",
    "authors": [
      "Gan Sun",
      "Yang Cong",
      "Lichen Wang",
      "Zhengming Ding",
      "Yun Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Sun_Online_Multi-Task_Clustering_for_Human_Motion_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Sun_Online_Multi-Task_Clustering_for_Human_Motion_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Human motion segmentation in time space becomes attractive recently due to its wide range of potential applications on action recognition, event detection, and scene understanding tasks. However, most existing state-of-the-arts address this problem upon an offline and single-agent scenario, while there are a lot of urgent requirements to segment videos captured from multiple agents for real-time application (e.g., surveillance system). In this paper, we propose an Online Multi-task Clustering (OMTC) model for an online and multi-agent segmentation scenario, where each agent corresponds to one task. Specifically, a linear autoencoder framework is designed to project motion sequences into a common motion-aware space across multiple collaborating tasks, while the decoder obtains motion-aware representation of each task via a temporal preserved regularizer. To tackle distribution shifts problem between each pair of tasks, the task-specific projections are further proposed to align representation across the motion segmentation tasks. By this way, significant motion knowledge can be shared among multiple tasks, and the temporal data structures are also well preserved. For the model optimization, an efficient and effective online optimization mechanism is derived to solve the large-scale formulation in real-time applications. Experiment results on Keck, MAD and our collected human motion datasets demonstrate the robustness, high-accuracy and efficiency of our OMTC model.\r",
    "code_link": ""
  },
  "iccv2019_rlq_imagedeconvolutionwithdeepimageandkernelpriors": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Image Deconvolution with Deep Image and Kernel Priors",
    "authors": [
      "Zhunxuan Wang",
      "Zipei Wang",
      "Qiqi Li",
      "Hakan Bilen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Wang_Image_Deconvolution_with_Deep_Image_and_Kernel_Priors_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Wang_Image_Deconvolution_with_Deep_Image_and_Kernel_Priors_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Image deconvolution is the process of recovering convolutional degraded images, which is always a hard inverse problem because of its mathematically ill-posed property. On the success of the recently proposed deep image prior (DIP), we build an image deconvolution model with deep image and kernel priors (DIKP). DIP is a learning-free representation which uses neural net structures to express image prior information, and it showed great success in many energy-based models, e.g. denoising, super-resolution, inpainting. Instead, our DIKP model uses such priors in image deconvolution to model not only images but also kernels, combining the ideas of traditional learning-free deconvolution methods with neural nets. In this paper, we show that DIKP improve the performance of learning-free image deconvolution, and we experimentally demonstrate this on the standard benchmark of six standard test images in terms of PSNR and visual effects.\r",
    "code_link": ""
  },
  "iccv2019_rlq_featureaggregationnetworkforvideofacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Feature Aggregation Network for Video Face Recognition",
    "authors": [
      "Zhaoxiang Liu",
      "Huan Hu",
      "Jinqiang Bai",
      "Shaohua Li",
      "Shiguo Lian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Liu_Feature_Aggregation_Network_for_Video_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Liu_Feature_Aggregation_Network_for_Video_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper aims to learn a compact representation of a video for video face recognition task. We make the following contributions: first, we propose a meta attention-based aggregation scheme which adaptively and fine-grained weighs the feature along each feature dimension among all frames to form a compact and discriminative representation. It makes the best to exploit the valuable or discriminative part of each frame to promote the performance of face recognition, without discarding or despising low quality frames as usual methods do. Second, we build a feature aggregation network comprised of a feature embedding module and a feature aggregation module. The embedding module is a convolutional neural network used to extract a feature vector from a face image, while the aggregation module consists of cascaded two meta attention blocks which adaptively aggregate the feature vectors into a single fixed-length representation. The network can deal with arbitrary number of frames, and is insensitive to frame order. Third, we validate the performance of proposed aggregation scheme. Experiments on publicly available datasets, such as YouTube face dataset and IJB-A dataset, show the effectiveness of our method, and it achieves competitive performances on both the verification and identification protocols.\r",
    "code_link": ""
  },
  "iccv2019_rlq_recognizingcompressedvideoschallengesandpromises": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Recognizing Compressed Videos: Challenges and Promises",
    "authors": [
      "Reza Pourreza",
      "Amir Ghodrati",
      "Amirhossein Habibian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Pourreza_Recognizing_Compressed_Videos_Challenges_and_Promises_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Pourreza_Recognizing_Compressed_Videos_Challenges_and_Promises_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper studies the effect of quality degradation, caused by lossy video compression, on video recognition. We investigate how the state of the art video enhancement restores the video quality needed for an effective video recognition. Furthermore, we study the impact of various enhancement objectives, namely pixel-level, feature-level, and adversarial, on action recognition performance. Our experiments demonstrate that the models trained on pixel-level loss perform well in terms of visual quality but they hurt the accuracy of action recognition due to over smoothing discriminative features. On the other hand, models trained on perceptual and adversarial loss types not only generate better perceptual quality but also further improve the action recognition performance.\r",
    "code_link": ""
  },
  "iccv2019_rlq_evidencebasedfeatureselectionandcollaborativerepresentationtowardslearningbasedpsfestimationformotiondeblurring": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Evidence Based Feature Selection and Collaborative Representation Towards Learning Based PSF Estimation for Motion Deblurring",
    "authors": [
      "Rohan Raju Dhanakshirur",
      "Ramesh Ashok Tabib",
      "Ujwala Patil",
      "Uma Mudenagudi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Dhanakshirur_Evidence_Based_Feature_Selection_and_Collaborative_Representation_Towards_Learning_Based_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Dhanakshirur_Evidence_Based_Feature_Selection_and_Collaborative_Representation_Towards_Learning_Based_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The motion blur in an image is due to the relative motion between the camera and the scene being captured. Due to the degraded quality of the motion-blurred images, it is challenging to use them in different applications such as text detection, scene understanding, content-based image retrieval, etc. Typically, a motion-blurred image is modeled as a convolution between the un-blurred image and a Point Spread Function (PSF). Motion de-blurring is sensitive to the estimated PSF. In this paper, we propose to address the problem of motion deblurring by estimating PSF using a learning-based approach. We model motion blur as a function of length and angle and propose to estimate these parameters using a learning-based framework. It is challenging to find distinct features to precisely learn the extent of motion blur through deep learning. To address this, we model an evidence-based technique to select the relevant features for learning from a set of features, based on the confidence generated by combining the evidences using Dempster Shafer Combination Rule (DSCR). We propose to use Clustering and Collaborative Representation (CCR) of feature spaces to learn length and angle. We model the deblurred image as an MRF (Markov Random Field) and use MAP (maximum a posteriori) estimate as the final solution. We demonstrate the results on real and synthetic datasets and compare the results with different state of art methods using various quality metrics and vision tools.\r",
    "code_link": ""
  },
  "iccv2019_rlq_snidersinglenoisyimagedenoisingandrectificationforimprovinglicenseplaterecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "SNIDER: Single Noisy Image Denoising and Rectification for Improving License Plate Recognition",
    "authors": [
      "Younkwan Lee",
      "Juhyun Lee",
      "Hoyeon Ahn",
      "Moongu Jeon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Lee_SNIDER_Single_Noisy_Image_Denoising_and_Rectification_for_Improving_License_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Lee_SNIDER_Single_Noisy_Image_Denoising_and_Rectification_for_Improving_License_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we present an algorithm for real-world license plate recognition (LPR) from a low-quality image. Our method is built upon a framework that includes denoising and rectification, and each task is conducted by Convolutional Neural Networks. Existing denoising and rectification have been treated separately as a single network in previous research. In contrast to the previous work, we here propose an end-to-end trainable network for image recovery, Single Noisy Image DEnoising and Rectification (SNIDER), which focuses on solving both the problems jointly. It overcomes those obstacles by designing a novel network to address the denoising and rectification jointly. Moreover, we propose a way to leverage optimization with the auxiliary tasks for multi-task fitting and novel training losses. Extensive experiments on two challenging LPR datasets demonstrate the effectiveness of our proposed method in recovering the high-quality license plate image from the low-quality one and show that the the proposed method outperforms other state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_rlq_lowqualityvideofacerecognitionmulti-modeaggregationrecurrentnetwork(marn)": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Low Quality Video Face Recognition: Multi-Mode Aggregation Recurrent Network (MARN)",
    "authors": [
      "Sixue Gong",
      "Yichun Shi",
      "Anil Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Gong_Low_Quality_Video_Face_Recognition_Multi-Mode_Aggregation_Recurrent_Network_MARN_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Gong_Low_Quality_Video_Face_Recognition_Multi-Mode_Aggregation_Recurrent_Network_MARN_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Face recognition performance deteriorates when face images are of very low quality. For low quality video sequences, however, more discriminative features can be obtained by aggregating the information in video frames. We propose a Multi-mode Aggregation Recurrent Network (MARN) for real-world low-quality video face recognition. Unlike existing recurrent networks (RNNs), MARN is robust against overfitting since it learns to aggregate pre-trained embeddings. Compared with quality-aware aggregation methods, MARN utilizes the video context and learns multiple attention vectors adaptively. Empirical results on three video face recognition datasets, IJB-S, YTF, and PaSC show that MARN significantly boosts the performance on the low quality video dataset while achieves comparable results on high quality video datasets.\r",
    "code_link": "https://github.com/inlmouse/MS-Celeb-1M"
  },
  "iccv2019_rlq_non-discriminativedataorweakmodel?ontherelativeimportanceofdataandmodelresolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Non-Discriminative Data or Weak Model? On the Relative Importance of Data and Model Resolution",
    "authors": [
      "Mark Sandler",
      "Jonathan Baccash",
      "Andrey Zhmoginov",
      "Andrew Howard"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Sandler_Non-Discriminative_Data_or_Weak_Model_On_the_Relative_Importance_of_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Sandler_Non-Discriminative_Data_or_Weak_Model_On_the_Relative_Importance_of_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We explore the question of how the resolution of the input image (\"input resolution\") affects the performance of a neural network when compared to the resolution of the hidden layers (\"internal resolution\"). Adjusting these characteristics is frequently used as a hyperparameter providing a trade-off between model performance and accuracy. An intuitive interpretation is that the reduced information content in the low-resolution input causes decay in the accuracy. In this paper, we show that up to a point, the input resolution alone plays little role in the network performance, and it is the internal resolution that is the critical driver of model quality. We then build on these insights to develop novel neural network architectures that we call Isometric Neural Networks. These models maintain a fixed internal resolution throughout their entire depth. We demonstrate that they lead to high accuracy models with low activation footprint and parameter count.\r",
    "code_link": ""
  },
  "iccv2019_rlq_areadversarialrobustnessandcommonperturbationrobustnessindependantattributes?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Are Adversarial Robustness and Common Perturbation Robustness Independant Attributes ?",
    "authors": [
      "Alfred Laugros",
      "Alice Caplier",
      "Matthieu Ospici"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Laugros_Are_Adversarial_Robustness_and_Common_Perturbation_Robustness_Independant_Attributes__ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Laugros_Are_Adversarial_Robustness_and_Common_Perturbation_Robustness_Independant_Attributes__ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Neural Networks have been shown to be sensitive to common perturbations such as blur, Gaussian noise, rotations, etc. They are also vulnerable to some artificial malicious corruptions called adversarial examples. The adversarial examples study has recently become very popular and it sometimes even reduces the term \"adversarial robustness\" to the term \"robustness\". Yet, we do not know to what extent the adversarial robustness is related to the global robustness. Similarly, we do not know if a robustness to various common perturbations such as translations or contrast losses for instance, could help with adversarial corruptions. We intend to study the links between the robustnesses of neural networks to both perturbations. With our experiments, we provide one of the first benchmark designed to estimate the robustness of neural networks to common perturbations. We show that increasing the robustness to carefully selected common perturbations, can make neural networks more robust to unseen common perturbations. We also prove that adversarial robustness and robustness to common perturbations are independent. Our results make us believe that neural network robustness should be addressed in a broader sense.\r",
    "code_link": ""
  },
  "iccv2019_rlq_generativelyinferentialco-trainingforunsuperviseddomainadaptation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Generatively Inferential Co-Training for Unsupervised Domain Adaptation",
    "authors": [
      "Can Qin",
      "Lichen Wang",
      "Yulun Zhang",
      "Yun Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Qin_Generatively_Inferential_Co-Training_for_Unsupervised_Domain_Adaptation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Qin_Generatively_Inferential_Co-Training_for_Unsupervised_Domain_Adaptation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep Neural Networks (DNNs) have greatly boosted the performance on a wide range of computer vision and machine learning tasks. Despite such achievements, DNN is hungry for enormous high-quality (HQ) training data, which are expensive and time-consuming to collect. To tackle this challenge, domain adaptation (DA) could help learning a model by leveraging the knowledge of low-quality (LQ) data (i.e., source domain), while generalizing well on label-scarce HQ data (i.e., target domain). However, existing methods have two problems. First, they mainly focus on the high-level feature alignment while neglecting low-level mismatch. Second, there exists a class-conditional distribution shift even features being well aligned. To solve these problems, we propose a novel Generatively Inferential Co-Training (GICT) framework for Unsupervised Domain Adaptation (UDA). GICT is based on cross-domain feature generation and a specifically designed co-training strategy. Feature generation adapts the representation at low level by translating images across domains. Co-training is employed to bridge conditional distribution shift by assigning high-confident pseudo labels on target domain inferred from two distinct classifiers. Extensive experiments on multiple tasks including image classification and semantic segmentation demonstrate the effectiveness of GICT approach.\r",
    "code_link": ""
  },
  "iccv2019_rlq_unsuperviseddeepfeaturetransferforlowresolutionimageclassification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Unsupervised Deep Feature Transfer for Low Resolution Image Classification",
    "authors": [
      "Yuanwei Wu",
      "Ziming Zhang",
      "Guanghui Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Wu_Unsupervised_Deep_Feature_Transfer_for_Low_Resolution_Image_Classification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Wu_Unsupervised_Deep_Feature_Transfer_for_Low_Resolution_Image_Classification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a simple while effective unsupervised deep feature transfer algorithm for low resolution image classification. No fine-tuning on convenet filters is required in our method. We use pre-trained convenet to extract features for both high-and low-resolution images, and then feed them into a two-layer feature transfer network for knowledge transfer. A SVM classifier is learned directly using these transferred low resolution features. Our network can be embedded into the state-of-the-art deep neural networks as a plug-in feature enhancement module. It preserves data structures in feature space for high resolution images, and transfers the distinguishing features from a well-structured source domain (high resolution features space) to a not well-organized target domain (low resolution features space). Extensive experiments on VOC2007 test set show that the proposed method achieves significant improvements over the baseline of using feature extraction.\r",
    "code_link": ""
  },
  "iccv2019_rlq_indoordepthcompletionwithboundaryconsistencyandself-attention": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Indoor Depth Completion with Boundary Consistency and Self-Attention",
    "authors": [
      "Yu-Kai Huang",
      "Tsung-Han Wu",
      "Yueh-Cheng Liu",
      "Winston H. Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Huang_Indoor_Depth_Completion_with_Boundary_Consistency_and_Self-Attention_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Huang_Indoor_Depth_Completion_with_Boundary_Consistency_and_Self-Attention_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Depth estimation features are helpful for 3D recognition. Commodity-grade depth cameras are able to capture depth and color image in real-time. However, glossy, transparent or distant surface cannot be scanned properly by the sensor. As a result, enhancement and restoration from sensing depth is an important task. Depth completion aims at filling the holes that sensors fail to detect, which is still a complex task for machine to learn. Traditional hand-tuned methods have reached their limits, while neural network based methods tend to copy and interpolate the output from surrounding depth values. This leads to blurred boundaries, and structures of the depth map are lost. Consequently, our main work is to design an end-to-end network improving completion depth maps while maintaining edge clarity. We utilize self-attention mechanism, previously used in image inpainting fields, to extract more useful information in each layer of convolution so that the complete depth map is enhanced. In addition, we propose boundary consistency concept to enhance the depth map quality and structure. Experimental results validate the effectiveness of our self-attention and boundary consistency schema, which outperforms previous state-of-the-art depth completion work on Matterport3D dataset.\r",
    "code_link": ""
  },
  "iccv2019_rlq_intra-camerasupervisedpersonre-identificationanewbenchmark": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Intra-Camera Supervised Person Re-Identification: A New Benchmark",
    "authors": [
      "Xiangping Zhu",
      "Xiatian Zhu",
      "Minxian Li",
      "Vittorio Murino",
      "Shaogang Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Zhu_Intra-Camera_Supervised_Person_Re-Identification_A_New_Benchmark_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Zhu_Intra-Camera_Supervised_Person_Re-Identification_A_New_Benchmark_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Existing person re-identification (re-id) methods rely mostly on a large set of inter-camera identity labelled training data, requiring a tedious data collection and annotation process therefore leading to poor scalability in practical re-id applications. To overcome this fundamental limitation, we consider person re-identification without inter-camera identity association but only with identity labels independently annotated within each individual camera-view. This eliminates the most time-consuming and tedious inter-camera identity labeling process in order to significantly reduce the amount of human efforts required during annotation. It hence gives rise to a more scalable and more feasible learning scenario, which we call Intra-Camera Supervised (ICS) person re-id. Under this ICS setting with weaker label supervision, we formulate a Multi-Task Multi-Label (MTML) deep learning method. Given no inter-camera association, MTML is specially designed for self-discovering the inter-camera identity correspondence. This is achieved by inter-camera multi-label learning under a joint multi-task inference framework. In addition, MTML can also efficiently learn the discriminative re-id feature representations by fully using the available identity labels within each camera-view. Extensive experiments demonstrate the performance superiority of our MTML model over the state-of-the-art alternative methods on three large-scale person re-id datasets in the proposed intra-camera supervised learning setting.\r",
    "code_link": ""
  },
  "iccv2019_rlq_unsupervisedoutlierdetectioninappearance-basedgazeestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Unsupervised Outlier Detection in Appearance-Based Gaze Estimation",
    "authors": [
      "Zhaokang Chen",
      "Didan Deng",
      "Jimin Pi",
      "Bertram E. Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Chen_Unsupervised_Outlier_Detection_in_Appearance-Based_Gaze_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Chen_Unsupervised_Outlier_Detection_in_Appearance-Based_Gaze_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Appearance-based gaze estimation maps RGB images to estimates of gaze directions. One problem in gaze estimation is that there always exist low-quality samples (outliers) in which the eyes are barely visible. These low-quality samples are mainly caused by blinks, occlusions (e.g. by eye glasses), blur (e.g. due to motion) and failures of the eye landmark detection. Training on these outliers degrades the performance of gaze estimators, since they have no or limited information about gaze directions. It is also risky to give estimates based on these images in real-world applications, as these estimates may be unreliable. To solve this problem, we propose an algorithm that detects outliers without supervision. Based on the input images with only gaze labels, the proposed algorithm learns to predict a gaze estimates and an additional confidence score, which alleviates the impact of outliers during learning. We evaluated this algorithm on the MPIIGaze dataset and on an internal dataset. In cross-subject evaluation, our experimental results show that the proposed algorithm results in a better gaze estimator (8% improvement). The proposed algorithm is also able to reliably detect outliers during testing, with a precision of 0.71 when the recall is 0.63.\r",
    "code_link": ""
  },
  "iccv2019_rlq_gsr-marglobalsuper-resolutionforpersonmulti-attributerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "GSR-MAR: Global Super-Resolution for Person Multi-Attribute Recognition",
    "authors": [
      "Thomhert Suprapto Siadari",
      "Mikyong Han",
      "Hyunjin Yoon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Siadari_GSR-MAR_Global_Super-Resolution_for_Person_Multi-Attribute_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Siadari_GSR-MAR_Global_Super-Resolution_for_Person_Multi-Attribute_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Person attribute recognition aims to predict attribute labels based on person's appearance usually captured from surveillance cameras. It is a challenging problem in computer vision due to poor imaging quality with complex background clutter and unconstrained viewing conditions from various angles and distances between person and surveillance cameras. In this paper, we address such a problem using an end-to-end network called Global Super-Resolution for Multi Attribute Recognition (GSR-MAR). GSR-MAR integrates a conversion process of low-resolution input images into high-resolution images and predicts person attributes from input images. Before performing the classification process, GSR-MAR not only converts low-resolution images to high-resolution images to recover details of image textures but also captures larger context information by using large separable convolutional layers. The experiment results on two popular benchmark datasets demonstrate the performance improvement and effectiveness of our GSR-MAR model over competing baselines.\r",
    "code_link": ""
  },
  "iccv2019_rlq_state-of-the-artinactionunconstrainedtextdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "State-of-the-Art in Action: Unconstrained Text Detection",
    "authors": [
      "Diep Thi Ngoc Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Nguyen_State-of-the-Art_in_Action_Unconstrained_Text_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Nguyen_State-of-the-Art_in_Action_Unconstrained_Text_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we stage five real-world scenarios for six state-of-the-art text detection methods in order to evaluate how competent they are with new data without any training process. Moreover, this paper analyzes the architecture design of those methods to reveal the influence of pipeline choices on the detection quality. The setup of experimental studies are straight-forward: we collect and manually annotate test data, we reimplement the pretrained models of the state-of-the-art methods, then we evaluate and analyze how well each method achieve in each of our collected datasets. We found that most of the state-of-the-art methods are competent at detecting textual information in unseen data, however, some are more readily used for real-world applications. Surprisingly, we also found that the choice of a post-processing algorithm correlates strongly with the performance of the corresponding method. We expect this paper would serve as a reference for researchers as well as application developers in the field. All collected data with ground truth annotation and their detected results is publicly available at our Github repository: https://github.com/chupibk/HBlab-rlq19.\r",
    "code_link": ""
  },
  "iccv2019_rlq_real-timeage-invariantfacerecognitioninvideosusingthescatternetinceptionhybridnetwork(sihn)": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Real-Time Age-Invariant Face Recognition in Videos Using the ScatterNet Inception Hybrid Network (SIHN)",
    "authors": [
      "Saurabh Bodhe",
      "Prathamesh Kapse",
      "Amarjot Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Bodhe_Real-Time_Age-Invariant_Face_Recognition_in_Videos_Using_the_ScatterNet_Inception_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Bodhe_Real-Time_Age-Invariant_Face_Recognition_in_Videos_Using_the_ScatterNet_Inception_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Face recognition has become a vital component of various safety and security systems with applications in safety and security systems, law enforcement applications, access control etc. Ageing makes face recognition challenging as the facial features evolve over time. In this paper, we propose a ScatterNet Inception Hybrid Network (SIHN) network that learns deep features for age-invariant face recognition. The trained system is evaluated on a separate dataset of 200 videos corresponding to 100 celebrities collected from public sources. These videos contain faces recorded at different locations, scales, rotations, illumination and ages. Experimental results evaluated over 27000 frames show that the proposed method can achieve state-of-the-art performance on both our video dataset as well as the other widely used datasets for age-invariant face datasets such as CACD and FG-NET. The system finds the individuals of interest from the videos in real-time at 18 fps. This research also introduces the Celebrities Video Aging (CVA) dataset used for evaluating the deep network which hopefully may encourage researchers interested in using deep learning for age-invariant face recognition.\r",
    "code_link": ""
  },
  "iccv2019_rlq_recognizingtinyfaces": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "RLQ",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Real-World Recognition From Low-Quality Images and Videos",
    "title": "Recognizing Tiny Faces",
    "authors": [
      "Siva Chaitanya Mynepalli",
      "Peiyun Hu",
      "Deva Ramanan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/RLQ/Mynepalli_Recognizing_Tiny_Faces_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/RLQ/Mynepalli_Recognizing_Tiny_Faces_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Objects are naturally captured over a continuous range of distances, causing dramatic changes in appearance, especially at low resolutions. Recognizing such small objects at range is an open challenge in object recognition. In this paper, we explore solutions to this problem by tackling the fine-grained task of face recognition. State-of-the-art embeddings aim to be scale-invariant by extracting representations in a canonical coordinate frame (by resizing a face window to a resolution of say, 224x224 pixels). However, it is well known in the psychophysics literature that human vision is decidedly scale variant: humans are much less accurate at lower resolutions. Motivated by this, we explore scale-variant multiresolution embeddings that explicitly disentangle factors of variation across resolution and scale. Importantly, multiresolution embeddings can adapt in size and complexity to the resolution of input image on-the-fly (e.g., high resolution input images produce more detailed representations that result in better recognition performance). Compared to state-of-the-art \"one-size-fits-all\" approaches, our embeddings dramatically reduce error for small faces by at least 70% on standard benchmarks (i.e. IJBC, LFW and MegaFace).\r",
    "code_link": ""
  },
  "iccv2019_gaze_ageneralizedandrobustmethodtowardspracticalgazeestimationonsmartphone": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone",
    "authors": [
      "Tianchu Guo",
      "Yongchao Liu",
      "Hui Zhang",
      "Xiabing Liu",
      "Youngjun Kwak",
      "Byung In Yoo",
      "Jae-Joon Han",
      "Changkyu Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GAZE/Guo_A_Generalized_and_Robust_Method_Towards_Practical_Gaze_Estimation_on_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GAZE/Guo_A_Generalized_and_Robust_Method_Towards_Practical_Gaze_Estimation_on_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Gaze estimation for ordinary smart phone, e.g. estimating where the user is looking at on the phone screen, can be applied in various applications. However, the widely used appearance-based CNN methods still have two issues for practical adoption. First, due to the limited dataset, gaze estimation is very likely to suffer from over-fitting, leading to poor accuracy at run time. Second, the current methods are usually not robust, i.e. their prediction results having notable jitters even when the user is performing gaze fixation, which degrades user experience greatly. For the first issue, we propose a new tolerant and talented (TAT) training scheme, which is an iterative random knowledge distillation framework enhanced with cosine similarity pruning and aligned orthogonal initialization. The knowledge distillation is a tolerant teaching process providing diverse and informative supervision. The enhanced pruning and initialization is a talented learning process prompting the network to escape from the local minima and re-born from a better start. For the second issue, we define a new metric to measure the robustness of gaze estimator, and propose an adversarial training based Disturbance with Ordinal loss (DwO) method to improve it. The experimental results show that our TAT method achieves state-of-the-art performance on GazeCapture dataset, and that our DwO method improves the robustness while keeping comparable accuracy.\r",
    "code_link": ""
  },
  "iccv2019_gaze_learningtopersonalizeinappearance-basedgazetracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "Learning to Personalize in Appearance-Based Gaze Tracking",
    "authors": [
      "Erik Linden",
      "Jonas Sjostrand",
      "Alexandre Proutiere"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GAZE/Linden_Learning_to_Personalize_in_Appearance-Based_Gaze_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GAZE/Linden_Learning_to_Personalize_in_Appearance-Based_Gaze_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Personal variations severely limit the performance of appearance-based gaze tracking. Adapting to these variations using standard neural network model-adaption methods is difficult. The problems range from overfitting, due to small amounts of training data, to underfitting, due to restrictive model architectures. We tackle these problems by introducing SPatial Adaptive GaZe Estimator (SPAZE ). By modeling personal variations as a low-dimensional latent parameter space, SPAZE provides just enough adaptability to capture the range of personal variations without being prone to overfitting. Calibrating SPAZE for a new person reduces to solving a small and simple optimization problem. SPAZE achieves an error of 2.70 degrees on the MPIIGaze dataset, improving on the state-of-the-art by 14 %. We contribute to gaze tracking research by empirically showing that personal variations are well-modeled as a 3-dimensional latent parameter space for each eye. We show that this low-dimensionality is expected by examining model-based approaches to gaze tracking.\r",
    "code_link": ""
  },
  "iccv2019_gaze_on-devicefew-shotpersonalizationforreal-timegazeestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "On-Device Few-Shot Personalization for Real-Time Gaze Estimation",
    "authors": [
      "Junfeng He",
      "Khoi Pham",
      "Nachiappan Valliappan",
      "Pingmei Xu",
      "Chase Roberts",
      "Dmitry Lagun",
      "Vidhya Navalpakkam"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GAZE/He_On-Device_Few-Shot_Personalization_for_Real-Time_Gaze_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GAZE/He_On-Device_Few-Shot_Personalization_for_Real-Time_Gaze_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In this paper, we present on-device few-shot personalization methods for 2D gaze estimation. The proposed supervised method achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses only unlabeled facial images to improve gaze estimation accuracy. Our best personalized model achieves 24-26% better accuracy (measured by mean error) on phones compared to the state-of-the-art using <=5 calibration points per user. It is also computationally efficient, requiring 20x fewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as using gaze for accessibility, gaming and human-computer interaction while running entirely on-device in real-time.\r",
    "code_link": ""
  },
  "iccv2019_gaze_rt-beneadatasetandbaselinesforreal-timeblinkestimationinnaturalenvironments": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "RT-BENE: A Dataset and Baselines for Real-Time Blink Estimation in Natural Environments",
    "authors": [
      "Kevin Cortacero",
      "Tobias Fischer",
      "Yiannis Demiris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GAZE/Cortacero_RT-BENE_A_Dataset_and_Baselines_for_Real-Time_Blink_Estimation_in_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GAZE/Cortacero_RT-BENE_A_Dataset_and_Baselines_for_Real-Time_Blink_Estimation_in_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years gaze estimation methods have made substantial progress, driven by the numerous application areas including human-robot interaction, visual attention estimation and foveated rendering for virtual reality headsets. However, many gaze estimation methods typically assume that the subject's eyes are open; for closed eyes, these methods provide irregular gaze estimates. Here, we address this assumption by first introducing a new open-sourced dataset with annotations of the eye-openness of more than 200,000 eye images, including more than 10,000 images where the eyes are closed. We further present baseline methods that allow for blink detection using convolutional neural networks. In extensive experiments, we show that the proposed baselines perform favourably in terms of precision and recall. We further incorporate our proposed RT-BENE baselines in the recently presented RT-GENE gaze estimation framework where it provides a real-time inference of the openness of the eyes. We argue that our work will benefit both gaze estimation and blink estimation methods, and we take steps towards unifying these methods.\r",
    "code_link": "https://github.com/matterport/Mask_RCNN"
  },
  "iccv2019_gaze_salgazepersonalizinggazeestimationusingvisualsaliency": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "SalGaze: Personalizing Gaze Estimation using Visual Saliency",
    "authors": [
      "Zhuoqing Chang",
      "J. Matias Di Martino",
      "Qiang Qiu",
      "Steven Espinosa",
      "Guillermo Sapiro"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GAZE/Chang_SalGaze_Personalizing_Gaze_Estimation_using_Visual_Saliency_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GAZE/Chang_SalGaze_Personalizing_Gaze_Estimation_using_Visual_Saliency_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Traditional gaze estimation methods typically require explicit user calibration to achieve high accuracy. This process is cumbersome and recalibration is often required when there are changes in factors such as illumination and pose. To address this challenge, we introduce SalGaze, a framework that utilizes saliency information in the visual content to transparently adapt the gaze estimation algorithm to the user without explicit user calibration. We design an algorithm to transform a saliency map into a differentiable loss map that can be used for the optimization of CNN-based models. SalGaze is also able to greatly augment standard point calibration data with implicit video saliency calibration data using a unified framework. We show accuracy improvements over 24% using our technique on existing methods.\r",
    "code_link": ""
  },
  "iccv2019_hbu_neighbourhoodcontextembeddingsindeepinversereinforcementlearningforpredictingpedestrianmotionoverlongtimehorizons": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Neighbourhood Context Embeddings in Deep Inverse Reinforcement Learning for Predicting Pedestrian Motion Over Long Time Horizons",
    "authors": [
      "Tharindu Fernando",
      "Simon Denman",
      "Sridha Sridharan",
      "Clinton Fookes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Fernando_Neighbourhood_Context_Embeddings_in_Deep_Inverse_Reinforcement_Learning_for_Predicting_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Fernando_Neighbourhood_Context_Embeddings_in_Deep_Inverse_Reinforcement_Learning_for_Predicting_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Predicting crowd behaviour in the distant future has increased in prominence among the computer vision community as it provides intelligence and flexibility for autonomous systems, enabling the early detection of abnormal events and better and more natural interactions between humans and autonomous systems such as driverless vehicles and field robots. Despite the fact that Deep Inverse Reinforcement Learning (D-IRL) based modelling paradigms offer flexibility and robustness when anticipating human behaviour across long time horizons, compared to their supervised learning counterparts, no existing state-of-the-art D-IRL methods consider path planning in situations where there are multiple moving pedestrians in the environment. To address this, we present a novel recurrent neural network based method for embedding pedestrian dynamics in a D-IRL setting, where there are multiple moving agents. We propose to capture the motion of the pedestrian of interest as well as the motion of other pedestrians in the neighbourhood through Long-Short-Term Memory networks. The neighbourhood dynamics are encoded into a feature map, preserving the spatial integrity of the observed trajectories. Utilising the maximum-entropy based non-linear inverse reinforcement learning framework, we map these features to a reward map. We perform extensive evaluations on the publicly available Stanford Drone and SAIVT Multi-Spectral Trajectory datasets where the proposed method exhibits robustness towards lengthier predictions into the distant future, demonstrating the importance of capturing the dynamic evolution of the environment using the proposed embedding scheme.\r",
    "code_link": "https://github.com/yfzhang/vehicle-motion-forecasting"
  },
  "iccv2019_hbu_weakly-supervisedcompletionmomentdetectionusingtemporalattention": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Weakly-Supervised Completion Moment Detection using Temporal Attention",
    "authors": [
      "Farnoosh Heidarivincheh",
      "Majid Mirmehdi",
      "Dima Damen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Heidarivincheh_Weakly-Supervised_Completion_Moment_Detection_using_Temporal_Attention_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Heidarivincheh_Weakly-Supervised_Completion_Moment_Detection_using_Temporal_Attention_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Monitoring the progression of an action towards completion offers fine grained insight into the actor's behaviour. In this work, we target detecting the completion moment of actions, that is the moment when the action's goal has been successfully accomplished. This has potential applications from surveillance to assistive living and human-robot interactions. Previous effort required human annotations of the completion moment for training (i.e. full supervision). In this work, we present an approach for moment detection from weak video-level labels. Given both complete and incomplete sequences, of the same action, we learn temporal attention, along with accumulated completion prediction from all frames in the sequence. We also demonstrate how the approach can be used when completion moment supervision is available. We evaluate and compare our approach on actions from three datasets, namely HMDB, UCF101 and RGBD-AC, and show that temporal attention improves detection in both weakly-supervised and fully-supervised settings.\r",
    "code_link": ""
  },
  "iccv2019_hbu_uncertainty-awareanticipationofactivities": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Uncertainty-Aware Anticipation of Activities",
    "authors": [
      "Yazan Abu Farha",
      "Juergen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Abu_Farha_Uncertainty-Aware_Anticipation_of_Activities_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Abu_Farha_Uncertainty-Aware_Anticipation_of_Activities_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Anticipating future activities in video is a task with many practical applications. While earlier approaches are limited to just a few seconds in the future, the prediction time horizon has just recently been extended to several minutes in the future. However, as increasing the predicted time horizon, the future becomes more uncertain and models that generate a single prediction fail at capturing the different possible future activities. In this paper, we address the uncertainty modelling for predicting long-term future activities. Both an action model and a length model are trained to model the probability distribution of the future activities. At test time, we sample from the predicted distributions multiple samples that correspond to the different possible sequences of future activities. Our model is evaluated on two challenging datasets and shows a good performance in capturing the multi-modal future activities without compromising the accuracy when predicting a single sequence of future activities.\r",
    "code_link": ""
  },
  "iccv2019_hbu_deepfakevideodetectionthroughopticalflowbasedcnn": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Deepfake Video Detection through Optical Flow Based CNN",
    "authors": [
      "Irene Amerini",
      "Leonardo Galteri",
      "Roberto Caldelli",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Amerini_Deepfake_Video_Detection_through_Optical_Flow_Based_CNN_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Amerini_Deepfake_Video_Detection_through_Optical_Flow_Based_CNN_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent advances in visual media technology have led to new tools for processing and, above all, generating multimedia contents. In particular, modern AI-based technologies have provided easy-to-use tools to create extremely realistic manipulated videos. Such synthetic videos, named Deep Fakes, may constitute a serious threat to attack the reputation of public subjects or to address the general opinion on a certain event. According to this, being able to individuate this kind of fake information becomes fundamental. In this work, a new forensic technique able to discern between fake and original video sequences is given; unlike other state-of-the-art methods which resorts at single video frames, we propose the adoption of optical flow fields to exploit possible inter-frame dissimilarities. Such a clue is then used as feature to be learned by CNN classifiers. Preliminary results obtained on FaceForensics++ dataset highlight very promising performances.\r",
    "code_link": ""
  },
  "iccv2019_hbu_dancedancegenerationmotiontransferforinternetvideos": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Dance Dance Generation: Motion Transfer for Internet Videos",
    "authors": [
      "Yipin Zhou",
      "Zhaowen Wang",
      "Chen Fang",
      "Trung Bui",
      "Tamara Berg"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Zhou_Dance_Dance_Generation_Motion_Transfer_for_Internet_Videos_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Zhou_Dance_Dance_Generation_Motion_Transfer_for_Internet_Videos_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This work presents computational methods for transferring body movements from one person to another with videos collected in the wild. Specifically, we train a personalized model on a single video from the Internet which can generate videos of this target person driven by the motions of other people. Our model is built on two generative networks: a human (foreground) synthesis net which generates photo-realistic imagery of the target person in a novel pose, and a fusion net which combines the generated foreground with the scene (background), adding shadows or reflections as needed to enhance realism. We validate the the efficacy of our proposed models over baselines with qualitative and quantitative evaluations as well as a subjective test.\r",
    "code_link": ""
  },
  "iccv2019_hbu_attributespreservingfacede-identification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Attributes Preserving Face De-Identification",
    "authors": [
      "bin yan",
      "mingtao pei",
      "zhengang nie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Yan_Attributes_Preserving_Face_De-Identification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Yan_Attributes_Preserving_Face_De-Identification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a Face de-identification method to remove the identification information of a person while maintaining all the face attributes such as expression, age and gender. Motivated by the k-Same algorithm, our method consists of three steps: first, k face images are selected randomly. These k face images may contain same or different face attributes with the test face image. Secondly, ELEGANT model is employed to transfer attributes from the test face to the k selected faces. After attributes transferring, the k selected faces have the same attributes as the test face. Then we average the k selected faces as the de-identified image of the test face. Experimental results show that our method can de-identify a face image while preserving all of its attributes effectively.\r",
    "code_link": ""
  },
  "iccv2019_hbu_measuringcrowdcollectivenessviaglobalmotioncorrelation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Measuring Crowd Collectiveness via Global Motion Correlation",
    "authors": [
      "Ling Mei",
      "Jianghuang Lai",
      "Zeyu Chen",
      "Xiaohua Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Mei_Measuring_Crowd_Collectiveness_via_Global_Motion_Correlation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Mei_Measuring_Crowd_Collectiveness_via_Global_Motion_Correlation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Crowd collectiveness refers to the behavior consistency of crowd scenes, which reflects the degree of collective movements among massive individuals in crowd systems. The existing methods focus on measuring the discrepancy of motion direction among the individuals. However, few studies consider the magnitude discrepancy of velocity in a crowd and the collectiveness among different crowds, which can also affect the overall crowd collectiveness. In this paper, we propose a novel descriptor which combines intra-crowd collectiveness with inter-crowd collectiveness to solve the problem. For intra-crowd collectiveness, we introduce the energy spread process to identify the impacting factors of collectiveness, then measure the collectiveness of individuals within a crowd cluster by computing their similarities of magnitude and direction from the optical flow. For inter-crowd collectiveness, we assess the motion consistency among various crowd clusters generated from collective merging. Experimental results demonstrate that how the new collectiveness descriptor improves performance on three different crowd datasets, thus validating the superiority of the proposed descriptor.\r",
    "code_link": ""
  },
  "iccv2019_hbu_facialposeestimationbydeeplearningfromlabeldistributions": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Facial Pose Estimation by Deep Learning from Label Distributions",
    "authors": [
      "Zhaoxiang Liu",
      "Zezhou Chen",
      "Jinqiang Bai",
      "Shaohua Li",
      "Shiguo Lian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Liu_Facial_Pose_Estimation_by_Deep_Learning_from_Label_Distributions_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Liu_Facial_Pose_Estimation_by_Deep_Learning_from_Label_Distributions_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Facial pose estimation has gained a lot of attentions in many practical applications, such as human-robot interaction, gaze estimation and driver monitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is becoming more and more popular. However, facial pose estimation suffers from a key challenge: the lack of sufficient training data for many poses, especially for large poses. Inspired by the observation that the faces under close poses look similar, we reformulate the facial pose estimation as a label distribution learning problem, considering each face image as an example associated with a Gaussian label distribution rather than a single label, and construct a convolutional neural network which is trained with a multi-loss function on AFLW dataset and 300W-LP dataset to predict the facial poses directly from color image. Extensive experiments are conducted on several popular benchmarks, including AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant advantage over other state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_hbu_fitting,comparison,andalignmentoftrajectoriesonpositivesemi-definitematriceswithapplicationtoactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Fitting, Comparison, and Alignment of Trajectories on Positive Semi-Definite Matrices with Application to Action Recognition",
    "authors": [
      "Benjamin Szczapa",
      "Mohamed Daoudi",
      "Stefano Berretti",
      "Alberto Del Bimbo",
      "Pietro Pala",
      "Estelle Massart"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Szczapa_Fitting_Comparison_and_Alignment_of_Trajectories_on_Positive_Semi-Definite_Matrices_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Szczapa_Fitting_Comparison_and_Alignment_of_Trajectories_on_Positive_Semi-Definite_Matrices_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we tackle the problem of action recognition using body skeletons extracted from video sequences. Our approach lies in the continuity of recent works representing video frames by Gramian matrices that describe a trajectory on the Riemannian manifold of positive-semidefinite matrices of fixed rank. Compared to previous work, the manifold of fixed-rank positive-semidefinite matrices is endowed with a different metric, and we resort to different algorithms for the curve fitting and temporal alignment steps. We evaluated our approach on three publicly available datasets (UTKinect-Action3D, KTH-Action and UAVGesture). The results of the proposed approach are competitive with respect to state-of-the-art methods, while only involving body skeletons.\r",
    "code_link": ""
  },
  "iccv2019_hbu_fallspredictionbasedonbodykeypointsandseq2seqarchitecture": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Falls Prediction Based on Body Keypoints and Seq2Seq Architecture",
    "authors": [
      "Minjie Hua",
      "Yibing Nan",
      "Shiguo Lian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Hua_Falls_Prediction_Based_on_Body_Keypoints_and_Seq2Seq_Architecture_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Hua_Falls_Prediction_Based_on_Body_Keypoints_and_Seq2Seq_Architecture_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper presents a novel approach for predicting the falls of people in advance from monocular video. First, all persons in the observed frames are detected and tracked with the coordinates of their body keypoints being extracted meanwhile. A keypoints vectorization method is exploited to eliminate irrelevant information in the initial coordinate representation. Then, the observed keypoint sequence of each person is input to the pose prediction module adapted from sequence-to-sequence(seq2seq) architecture to predict the future keypoint sequence. Finally, the predicted pose is analyzed by the falls classifier to judge whether the person will fall down in the future. The pose prediction module and falls classifier are trained separately and tuned jointly using Le2i dataset, which contains 191 videos of various normal daily activities as well as falls performed by several actors. The contrast experiments with mainstream raw RGB-based models show the accuracy improvement of utilizing body keypoints in falls classification. Moreover, the precognition of falls is proved effective by comparisons between models that with and without the pose prediction module.\r",
    "code_link": ""
  },
  "iccv2019_hbu_voiceactivitydetectionbyupperbodymotionanalysisandunsuperviseddomainadaptation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Voice Activity Detection by Upper Body Motion Analysis and Unsupervised Domain Adaptation",
    "authors": [
      "Muhammad Shahid",
      "Cigdem Beyan",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Shahid_Voice_Activity_Detection_by_Upper_Body_Motion_Analysis_and_Unsupervised_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Shahid_Voice_Activity_Detection_by_Upper_Body_Motion_Analysis_and_Unsupervised_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present a novel vision-based voice activity detection (VAD) method that relies only on automatic upper body motion (UBM) analysis. Traditionally, VAD is performed using audio features only, but the use of visual cues instead of audio can be desirable especially when audio is not available such as due to technical, ethical or legal issues. Psychology literature confirms that the way people move while speaking is different from while they are not speaking. This motivates us to claim that an effective representation of UBM can be used to detect \"Who is Speaking and When\". On the other hand, the way people move during their speech varies a lot from culture to culture, and even person to person in the same culture. This results in unrelated UBM representations, such that the distribution of training and test data becomes disparate. To overcome this, we combine stacked sparse autoencoders and simple subspace alignment methods while a classifier is jointly learned using the VAD labels of the training data only. This yields new domain invariant feature representations for training and test data, showing improved VAD results. Our approach is applicable to any person without requiring re-training. The tests applied on a publicly available real-life VAD dataset show better results as compared to the state-of-the-art video-only VAD methods. Moreover, the ablation study justifies the superiority of the proposed method and demonstrates the positive contribution of each component.\r",
    "code_link": ""
  },
  "iccv2019_hbu_poseandexpressionrobustageestimationvia3dfacereconstructionfromasingleimage": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Pose and Expression Robust Age Estimation via 3D Face Reconstruction from a Single Image",
    "authors": [
      "Nedko Savov",
      "Minh Ngo",
      "Sezer Karaoglu",
      "Hamdi Dibeklioglu",
      "Theo Gevers"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Savov_Pose_and_Expression_Robust_Age_Estimation_via_3D_Face_Reconstruction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Savov_Pose_and_Expression_Robust_Age_Estimation_via_3D_Face_Reconstruction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we present a deep learning architecture that exploits 3D face reconstruction to obtain a robust age estimation. To this end, effective representation is learned through an expression-, pose-, illumination-, reflectance-, and geometry-aware deep model reconstructing a 3D face from a single 2D image. The 3D face reconstruction network is combined with an appearance-based age estimation network, where the face reconstruction features are jointly learned with the visual ones. Experiments on large-scale datasets show that our method obtains promising results and outperforms state-of-the-art methods, especially in the presence of strong expressions and large pose variations. Furthermore, cross-dataset experiments show that the proposed method is able to generalize more effectively as opposed to the state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_hbu_robustclothwarpingviamulti-scalepatchadversariallossforvirtualtry-onframework": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Robust Cloth Warping via Multi-Scale Patch Adversarial Loss for Virtual Try-On Framework",
    "authors": [
      "Kumar Ayush",
      "Surgan Jandial",
      "Ayush Chopra",
      "Mayur Hemani",
      "Balaji Krishnamurthy"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Ayush_Robust_Cloth_Warping_via_Multi-Scale_Patch_Adversarial_Loss_for_Virtual_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Ayush_Robust_Cloth_Warping_via_Multi-Scale_Patch_Adversarial_Loss_for_Virtual_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the rapid growth of online commerce, image-based virtual try-on systems for fitting new in-shop garments onto a person image presents an exciting opportunity to deliver interactive customer experience. Current state-of-the-art methods achieve this in a two-stage pipeline, where the first stage transforms the in-shop cloth into fitting the body shape of the target person and the second stage employs an image composition module to seamlessly integrate the transformed in-shop cloth onto the target person image. In the present work, we introduce a multi-scale patch adversarial loss for training the warping module of a state-of-the-art virtual try-on network. We show that the proposed loss produces robust transformation of clothes to fit the body shape while preserving texture details, which in turn improves image composition in the second stage. We perform extensive evaluations of the proposed loss on the try-on performance and show significant performance improvement over the existing state-of-the-art method.\r",
    "code_link": ""
  },
  "iccv2019_hbu_theinstantaneousaccuracyanovelmetricfortheproblemofonlinehumanbehaviourrecognitioninuntrimmedvideos": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "The Instantaneous Accuracy: a Novel Metric for the Problem of Online Human Behaviour Recognition in Untrimmed Videos",
    "authors": [
      "Marcos Baptista-Rios",
      "Roberto J. Lopez-Sastre",
      "Fabian Caba-Heilbron",
      "Jan van Gemert",
      "F. Javier Acevedo-Rodriguez",
      "Saturnino Maldonado-Bascon"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Baptista-Rios_The_Instantaneous_Accuracy_a_Novel_Metric_for_the_Problem_of_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Baptista-Rios_The_Instantaneous_Accuracy_a_Novel_Metric_for_the_Problem_of_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The problem of Online Human Behavior Recognition in untrimmed videos, aka Online Action Detection (OAD), needs to be revisited. Unlike traditional off-line action detection approaches, where the evaluation metrics are clear and well established, in the OAD setting we find few works and no consensus on evaluation protocols to be used. In this paper we introduce a novel online metric, the Instantaneous Accuracy (IA), that exhibits an online nature, solving most of the limitations of the previous (off-line) metrics. We conduct a thorough experimental evaluation on the TVSeries dataset, comparing the performance of various baseline methods with the state of the art. Our results confirm the problems of the previous evaluation protocols, and suggest that an IA-based protocol is more adequate to the online scenario for human behaviour understanding.\r",
    "code_link": ""
  },
  "iccv2019_hbu_faketalkerdetecteffectiveandpracticalrealisticneuraltalkingheaddetectionwithahighlyunbalanceddataset": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "FakeTalkerDetect: Effective and Practical Realistic Neural Talking Head Detection with a Highly Unbalanced Dataset",
    "authors": [
      "Hyeonseong Jeon",
      "Youngoh Bang",
      "Simon S. Woo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Jeon_FakeTalkerDetect_Effective_and_Practical_Realistic_Neural_Talking_Head_Detection_with_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Jeon_FakeTalkerDetect_Effective_and_Practical_Realistic_Neural_Talking_Head_Detection_with_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Detecting realistic fake images and videos is an increasingly important and urgent problem because they can be maliciously used. In this work, we propose FakeTalkerDetect, which is based on siamese networks to detect the recently proposed realistic talking head with few-shot learning. Unlike conventional methods, we propose to use pre-trained models with only a few real images for fine-tuning in siamese networks to effectively detect the fake images in a highly unbalanced data setting. Our FakeTalkerDetect achieves the overall accuracy 98.81% accuracy in detecting fake images generated from the latest neural talking head models. In particular, our preliminary work also demonstrates the effectiveness for the highly unbalanced dataset.\r",
    "code_link": ""
  },
  "iccv2019_hbu_temporalaccumulativefeaturesforsignlanguagerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HBU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Human Behavior Understanding",
    "title": "Temporal Accumulative Features for Sign Language Recognition",
    "authors": [
      "Ahmet Alp Kindiroglu",
      "Ogulcan Ozdemir",
      "Lale Akarun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HBU/Kindiroglu_Temporal_Accumulative_Features_for_Sign_Language_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HBU/Kindiroglu_Temporal_Accumulative_Features_for_Sign_Language_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a set of features called temporal accumulative features (TAF) for representing and recognizing isolated sign language gestures. By incorporating sign language specific constructs to better represent the unique linguistic characteristic of sign language videos, we have devised an efficient and fast SLR method for recognizing isolated sign language gestures. The proposed method is an HSV based accumulative video representation where keyframes based on the linguistic movement-hold model are represented by different colors. We also incorporate hand shape information and using a small scale convolutional neural network, demonstrate that sequential modeling of accumulative features for linguistic subunits improves upon baseline classification results.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_enhancingvisualembeddingsthroughweaklysupervisedcaptioningforzero-shotlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Enhancing Visual Embeddings through Weakly Supervised Captioning for Zero-Shot Learning",
    "authors": [
      "Matteo Bustreo",
      "Jacopo Cavazza",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Bustreo_Enhancing_Visual_Embeddings_through_Weakly_Supervised_Captioning_for_Zero-Shot_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Bustreo_Enhancing_Visual_Embeddings_through_Weakly_Supervised_Captioning_for_Zero-Shot_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Visual features designed for image classification have shown to be useful in zero-shot learning (ZSL) when generalizing towards classes not seen during training. In this paper, we argue that a more effective way of building visual features for ZSL is to extract them through captioning, in order not just to classify an image but, instead, to describe it. However, modern captioning models rely on a massive level of supervision, e.g up to 15 extended descriptions per instance provided by humans, which is simply not available for ZSL benchmarks. In the latter in fact, the available annotations inform about the presence/absence of attributes within a fixed list only. Worse, attributes are seldom annotated at the image level, but rather, at the class level only: because of this, the annotation cannot be visually grounded. In this paper, we deal with such a weakly supervised regime to train an end-to-end LSTM captioner, whose backbone CNN image encoder can provide better features for ZSL. Our enhancement of visual features, called \"VisEn\", is compatible with any generic ZSL method, without requiring changes in its pipeline (a part from adapting hyper-parameters). Experimentally, VisEn is capable of sharply improving recognition performance on unseen classes, as we demonstrate thorough an ablation study which encompasses different ZSL approaches. Further, on the challenging fine-grained CUB dataset, VisEn improves by margin state-of-the-art methods, by using visual descriptors of one order of magnitude smaller\r",
    "code_link": ""
  },
  "iccv2019_mdalc_protogantowardsfewshotlearningforactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "ProtoGAN: Towards Few Shot Learning for Action Recognition",
    "authors": [
      "Sai Kumar Dwivedi",
      "Vikram Gupta",
      "Rahul Mitra",
      "Shuaib Ahmed",
      "Arjun Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Dwivedi_ProtoGAN_Towards_Few_Shot_Learning_for_Action_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Dwivedi_ProtoGAN_Towards_Few_Shot_Learning_for_Action_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Few-shot learning (FSL) for action recognition is a challenging task of recognizing novel action categories which are represented by few instances in the training data. In a more generalized FSL setting (G-FSL), both seen as well as novel action categories need to be recognized. Conventional classifiers suffer due to inadequate data in FSL setting and inherent bias towards seen action categories in G-FSL setting. In this paper, we address this problem by proposing a novel ProtoGAN framework which synthesizes additional examples for novel categories by conditioning a conditional generative adversarial network with class prototype vectors. These class prototype vectors are learnt using a Class Prototype Transfer Network (CPTN) from examples of seen categories. Our synthesized examples for a novel class are semantically similar to real examples belonging to that class and is used to train a model exhibiting better generalization towards novel classes. We support our claim by performing extensive experiments on three datasets: UCF101, HMDB51 and Olympic-Sports. To the best of our knowledge, we are the first to report the results for G-FSL and provide a strong benchmark for future research. We also outperform the state-of-the-art method in FSL for all the aforementioned datasets.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_deepmetrictransferforlabelpropagationwithlimitedannotateddata": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Deep Metric Transfer for Label Propagation with Limited Annotated Data",
    "authors": [
      "Bin Liu",
      "Zhirong Wu",
      "Han Hu",
      "Stephen Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Liu_Deep_Metric_Transfer_for_Label_Propagation_with_Limited_Annotated_Data_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Liu_Deep_Metric_Transfer_for_Label_Propagation_with_Limited_Annotated_Data_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We study object recognition under the constraint that each object class is only represented by very few observations. Semi-supervised learning, transfer learning, and few-shot recognition all concern with achieving fast generalization with few labeled data. In this paper, we propose a generic framework that utilize unlabeled data to aid generalization for all three tasks. Our approach is to create much more training data through label propagation from the few labeled examples to a vast collection of unannotated images. The main contribution of the paper is that we show such a label propagation scheme can be highly effective when the similarity metric used for propagation is transferred from other related domains. We test various combinations of supervised and unsupervised metric learning methods with various label propagation algorithms. We find that our framework is very generic without being sensitive to any specific techniques. By taking advantage of unlabeled data in this way, we achieve significant improvements on all three tasks. Code is availble at http://github.com/Microsoft/metric-transfer.pytorch.\r",
    "code_link": "https://github.com/brain-research/realistic-ssl-evaluation"
  },
  "iccv2019_mdalc_task-discriminativedomainalignmentforunsuperviseddomainadaptation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Task-Discriminative Domain Alignment for Unsupervised Domain Adaptation",
    "authors": [
      "Behnam Gholami",
      "Pritish Sahu",
      "Minyoung Kim",
      "Vladimir Pavlovic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Gholami_Task-Discriminative_Domain_Alignment_for_Unsupervised_Domain_Adaptation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Gholami_Task-Discriminative_Domain_Alignment_for_Unsupervised_Domain_Adaptation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Domain Adaptation (DA), the process of effectively adapting task models learned on one domain, the source, to other related but distinct domains, the targets, with no or minimal retraining, is typically accomplished using the process of source-to-target manifold alignment. However, this process often leads to unsatisfactory adaptation performance, in part because it ignores the task-specific structure of the data. In this paper, we improve the performance of DA by introducing a discriminative discrepancy measure which takes advantage of auxiliary information available in the source and the target domains to better align the source and target distributions. Specifically, we leverage the cohesive clustering structure within individual data manifolds, associated with different tasks, to improve the alignment. This structure is explicit in the source, where the task labels are available, but is implicit in the target, making the problem challenging. We address the challenge by devising a deep DA framework, which combines a new task-driven domain alignment discriminator with domain regularizers that en-courage the shared features as task-specific and domain invariant, and prompt the task model to be data structure preserving, guiding its decision boundaries through the low density data regions. We validate our framework on standard benchmarks, including Digits (MNIST, USPS, SVHN, MNIST-M), PACS, and VisDA. Our results show that our proposal model consistently outperforms the state-of-the-art in unsupervised domain adaptation.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_bayesian3dconvnetsforactionrecognitionfromfewexamples": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Bayesian 3D ConvNets for Action Recognition from Few Examples",
    "authors": [
      "Martin de la Riva",
      "Pascal Mettes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/de_la_Riva_Bayesian_3D_ConvNets_for_Action_Recognition_from_Few_Examples_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/de_la_Riva_Bayesian_3D_ConvNets_for_Action_Recognition_from_Few_Examples_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A core challenge in action recognition from videos is obtaining sufficient training examples to train deep networks. This holds especially for action tasks from non-standard sensors such as infra-red cameras. In this work, we investigate Bayesian 3D ConvNets for action recognition when training examples are scarce. This work connects 3D ConvNets, a state-of-the-art approach for action recognition, with Bayesian networks, which have shown to be effective regularizers for deep networks. We do so by extending Bayes by Backprop to 3D ConvNets. Experimental evaluation on three small-scale action datasets from both RGB and infra-red sensors shows that Bayesian 3D ConvNets have a better test generalizing than standard 3D ConvNets. We find that, the more scarce the number of training examples per action, the better our Bayesian 3D ConvNets performs, highlighting the potential of Bayesian learning in the video domain.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_inputandweightspacesmoothingforsemi-supervisedlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Input and Weight Space Smoothing for Semi-Supervised Learning",
    "authors": [
      "Safa Cicek",
      "Stefano Soatto"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Cicek_Input_and_Weight_Space_Smoothing_for_Semi-Supervised_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Cicek_Input_and_Weight_Space_Smoothing_for_Semi-Supervised_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose regularizing the empirical loss for semi-supervised learning by acting on both the input (data) space, and the weight (parameter) space. We propose a method to perform such smoothing, which combines known input-space smoothing with a novel weight-space smoothing, based on a min-max (adversarial) optimization. The resulting Adversarial Block Coordinate Descent (ABCD) algorithm performs gradient ascent with a small learning rate for a random subset of the weights, and standard gradient descent on the remaining weights in the same mini-batch. It is simple to implement and achieves state-of-the-art performance.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_pickinggroupsinsteadofsamplesacloselookatstaticpool-basedmeta-activelearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Picking Groups Instead of Samples: A Close Look at Static Pool-Based Meta-Active Learning",
    "authors": [
      "Ignasi Mas",
      "Ramon Morros",
      "Veronica Vilaplana"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Mas_Picking_Groups_Instead_of_Samples_A_Close_Look_at_Static_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Mas_Picking_Groups_Instead_of_Samples_A_Close_Look_at_Static_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Active Learning techniques are used to tackle learning problems where obtaining training labels is costly. In this work we use Meta-Active Learning to learn to select a subset of samples from a pool of unsupervised input for further annotation. This scenario is called Static Pool-based Meta-Active Learning. We propose to extend existing approaches by performing the selection in a manner that, unlike previous works, can handle the selection of each sample based on the whole selected subset.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_zero-shotsemanticsegmentationviavariationalmapping": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Zero-Shot Semantic Segmentation via Variational Mapping",
    "authors": [
      "Naoki Kato",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Kato_Zero-Shot_Semantic_Segmentation_via_Variational_Mapping_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Kato_Zero-Shot_Semantic_Segmentation_via_Variational_Mapping_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We have witnessed the explosive success of deep neural networks (DNNs). However, DNNs typically assume a large amount of training data, and this is not always available in practical scenarios. In this paper, we present zero-shot semantic segmentation, where a model that has never seen the target class during training. For this purpose, we propose variational mapping, which facilitates effective learning by mapping the class label embedding vectors from the semantic space to the visual space. Experimental results using Pascal VOC 2012 show that our proposed method can achieve a mean intersection over union (mIoU) of 42.2, and we believe that this can serve as a baseline for similar research in the future.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_retro-actionslearningclosebytime-reversingopenvideos": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Retro-Actions: Learning 'Close' by Time-Reversing 'Open' Videos",
    "authors": [
      "Will Price",
      "Dima Damen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Price_Retro-Actions_Learning_Close_by_Time-Reversing_Open_Videos_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Price_Retro-Actions_Learning_Close_by_Time-Reversing_Open_Videos_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We investigate video transforms that result in class-homogeneous label-transforms. These are video transforms that consistently maintain or modify the labels of all videos in each class. We propose a general approach to discover invariant classes, whose transformed examples maintain their label; pairs of equivariant classes, whose transformed examples exchange their labels; and novel-generating classes, whose transformed examples belong to a new class outside the dataset. Label transforms offer additional supervision previously unexplored in video recognition benefiting data augmentation and enabling zero-shot learning opportunities by learning a class from transformed videos of its counterpart. Amongst such video transforms, we study horizontal-flipping, time-reversal, and their composition. We highlight errors in naively using horizontal-flipping as a form of data augmentation in video. Next, we validate the realism of time-reversed videos through a human perception study where people exhibit equal preference for forward and time-reversed videos. Finally, we test our approach on two datasets, Jester and Something-Something, evaluating the three video transforms for zero-shot learning and data augmentation. Our results show that gestures such as 'zooming in' can be learnt from 'zooming out' in a zero-shot setting, as well as more complex actions with state transitions such as 'digging something out of something' from 'burying something in something'.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_metamodulegenerationforfastfew-shotincrementallearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Meta Module Generation for Fast Few-Shot Incremental Learning",
    "authors": [
      "Shudong Xie",
      "Yiqun Li",
      "Dongyun Lin",
      "Tin Lay Nwe",
      "Sheng Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Xie_Meta_Module_Generation_for_Fast_Few-Shot_Incremental_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Xie_Meta_Module_Generation_for_Fast_Few-Shot_Incremental_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " There are two challenging problems in applying standard Deep Neural Networks (DNNs) for incremental learning with a few examples: (i) DNNs do not perform well when little training data is available; (ii) DNNs suffer from catastrophic forgetting when used for incremental class learning. To simultaneously address both problems, we propose Meta Module Generation (MetaMG), a meta-learning method that enables a module generator to rapidly generate a category module from a few examples for a scalable classification network to recognize a new category. The old categories are not forgotten after new categories are added in. Comprehensive experiments conducted on 4 datasets show that our method is promising for fast incremental learning in few-shot setting. Further experiments on the miniImageNet dataset show that even it is not specially designed for the N-wayK-shot learning problem, MetaMG can sitll perform relatively well especially for 20-way K-shot setting.\r",
    "code_link": "https://github.com/xialeiliu/awesome-incremental-learning"
  },
  "iccv2019_mdalc_adversarialjoint-distributionlearningfornovelclasssketch-basedimageretrieval": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Adversarial Joint-Distribution Learning for Novel Class Sketch-Based Image Retrieval",
    "authors": [
      "Anubha Pandey",
      "Ashish Mishra",
      "Vinay Kumar Verma",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Pandey_Adversarial_Joint-Distribution_Learning_for_Novel_Class_Sketch-Based_Image_Retrieval_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Pandey_Adversarial_Joint-Distribution_Learning_for_Novel_Class_Sketch-Based_Image_Retrieval_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In the information retrieval task, sketch-based image retrieval (SBIR) has drawn significant attention owing to the ease with which sketches can be drawn. The existing deep learning methods for the SBIR are very unrealistic in the real scenario, and its performance reduces drastically for unseen class test examples. Recently, Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) has drawn a lot of attention due to its ability to retrieve the novel/unseen class images at test time. These methods try to project sketch features into the image domain by learning a distribution conditioned on the sketch. We propose a new framework for ZS-SBIR that models joint distribution between the sketch and image domain using a generative adversarial network. The joint distribution modeling ability of our generative model helps to reduce the domain gap between the sketches and images. Our framework helps to synthesize the novel class image features using sketch features. The generative ability of our model for the unseen/novel classes, conditioned on sketch feature, allows it to perform well on the seen as well as unseen class sketches. We conduct extensive experiments on two widely used SBIR benchmark datasets-Sketchy and Tu-Berlin and obtain significant improvement over the existing state-of-the-art. We will release the code publicly for reproducibility of results.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_weaklysupervisedoneshotsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Weakly Supervised One Shot Segmentation",
    "authors": [
      "Hasnain Raza",
      "Mahdyar Ravanbakhsh",
      "Tassilo Klein",
      "Moin Nabi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Raza_Weakly_Supervised_One_Shot_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Raza_Weakly_Supervised_One_Shot_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " One-shot learning is a challenging discipline of machine learning since it gnaws at the concept of learning from large amounts of data. This is akin to making machine learning algorithms generalize from a few examples, much like how humans learn. We explore another novel dimension to this problem, of using weak supervision (labels only) in the one-shot domain, and specifically analyse it in the context of semantic segmentation. This is a challenging problem since we operate in the scarcity of data and supervision. We present a simple yet effective approach, whereby exploiting information from the base training classes in the current one-shot segmentation set-up allows for weak supervision to be easily used. We show that this strategy can be leveraged to achieve nearly the same results as full supervision, but with no pixel annotations, allowing fully automated segmentation. Comparisons to several fully supervised methods show convincing results. As well as better results than a weakly supervised baseline. Also presented is a baseline for generalized segmentation under one-shot and weak supervision assumptions.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_objectgroundingviaiterativecontextreasoning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Object Grounding via Iterative Context Reasoning",
    "authors": [
      "Lei Chen",
      "Mengyao Zhai",
      "Jiawei He",
      "Greg Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Chen_Object_Grounding_via_Iterative_Context_Reasoning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Chen_Object_Grounding_via_Iterative_Context_Reasoning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we tackle the problem of weakly-supervised object grounding. For an image and a set of queries extracted from its description, the goal is to localize each query in the image. In a weakly-supervised setting, ground-truth query groundings are not accessible at training time. We propose a novel approach for weakly-supervised object grounding through iterative context reasoning in which we update query representations and region representations iteratively conditioning on each other. Such iterative contextual refinement gradually resolves ambiguity and vagueness in the queries and regions, thus helping to resolve challenges in grounding. We show the effectiveness of our proposed model on two challenging video object grounding datasets.\r",
    "code_link": ""
  },
  "iccv2019_mdalc_zero-shothyperspectralimagedenoisingwithseparableimageprior": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MDALC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Multi-Discipline Approach for Learning Concepts - Zero-Shot, One-Shot, Few-Shot and Beyond",
    "title": "Zero-Shot Hyperspectral Image Denoising With Separable Image Prior",
    "authors": [
      "Ryuji Imamura",
      "Tatsuki Itasaka",
      "Masahiro Okuda"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MDALC/Imamura_Zero-Shot_Hyperspectral_Image_Denoising_With_Separable_Image_Prior_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MDALC/Imamura_Zero-Shot_Hyperspectral_Image_Denoising_With_Separable_Image_Prior_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Supervised learning with a convolutional neural network is recognized as a powerful means of image restoration. However, most such methods have been designed for application to grayscale and/or color images; therefore, they have limited success when applied to hyperspectral image restoration. This is partially owing to large datasets being difficult to collect, and also the heavy computational load associated with the restoration of an image with many spectral bands. To address this difficulty, we propose a novel self-supervised learning strategy for application to hyperspectral image restoration. Our method automatically creates a training dataset from a single degraded image and trains a denoising network without any clear images. Another notable feature of our method is the use of a separable convolutional layer. We undertake experiments to prove that a separable network allows us to acquire the prior of a hyperspectral image and to realize efficient restoration. We demonstrate the validity of our method through extensive experiments and show that our method has better characteristics than those that are currently regarded as state-of-the-art.\r",
    "code_link": "https://github.com/separable-imageprior/self-supervised-hyperspectral-image-restoration"
  },
  "iccv2019_eh_potsacarobustaxisestimatorforaxiallysymmetricpotfragments": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EH",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - E-Heritage and Dunhuang Challenge",
    "title": "PotSAC: A Robust Axis Estimator for Axially Symmetric Pot Fragments",
    "authors": [
      "Je Hyeong Hong",
      "Young Min Kim",
      "Koang-Chul Wi",
      "Jinwook Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EH/Hong_PotSAC_A_Robust_Axis_Estimator_for_Axially_Symmetric_Pot_Fragments_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EH/Hong_PotSAC_A_Robust_Axis_Estimator_for_Axially_Symmetric_Pot_Fragments_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The task of virtually reassembling an axially symmetric pot from its fragments can be greatly simplified by utilizing the constraints induced by the pot's axis of symmetry. This requires accurate estimation of the axis for each sherd, whose 3D data typically contain gross outliers arising from surface artifacts, noisy surface normals and unfiltered data along the break surface. In this work, we propose a simple two-stage robust axis estimator, PotSAC, which is based on a variant of the random sample consensus (RANSAC) algorithm followed by robust nonlinear least squares refinement. Unlike previous work which have either compensated the axis estimation accuracy for robustness against outliers or vice versa, our method can handle the aforementioned outlier sources without compromising its accuracy. This is achieved by carefully designing the method to combine and extend the advantage of each key prior work. Experimental results on real scanned fragments demonstrate the effectiveness of our method, paving the way towards high quality reassembly of symmetric potteries.\r",
    "code_link": ""
  },
  "iccv2019_eh_craquelureasagraphapplicationofimageprocessingandgraphneuralnetworkstothedescriptionoffracturepatterns": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EH",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - E-Heritage and Dunhuang Challenge",
    "title": "Craquelure as a Graph: Application of Image Processing and Graph Neural Networks to the Description of Fracture Patterns",
    "authors": [
      "Oleksii Sidorov",
      "Jon Yngve Hardeberg"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EH/Sidorov_Craquelure_as_a_Graph_Application_of_Image_Processing_and_Graph_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EH/Sidorov_Craquelure_as_a_Graph_Application_of_Image_Processing_and_Graph_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Cracks on a painting is not a defect but an inimitable signature of an artwork which can be used for origin examination, aging monitoring, damage identification, and even forgery detection. This work presents the development of a new methodology and corresponding toolbox for the extraction and characterization of information from an image of a craquelure pattern. The proposed approach processes craquelure network as a graph. The graph representation captures the network structure via mutual organization of junctions and fractures. Furthermore, it is invariant to any geometrical distortions. At the same time, our tool extracts the properties of each node and edge individually, which allows to characterize the pattern statistically. We illustrate benefits from the graph representation and statistical features individually using novel Graph Neural Network and hand-crafted descriptors correspondingly. However, we also show that the best performance is achieved when both techniques are merged into one framework. We perform experiments on the dataset for paintings' origin classification and demonstrate that our approach outperforms existing techniques by a large margin.\r",
    "code_link": ""
  },
  "iccv2019_eh_attention-awareage-agnosticvisualplacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EH",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - E-Heritage and Dunhuang Challenge",
    "title": "Attention-Aware Age-Agnostic Visual Place Recognition",
    "authors": [
      "Ziqi Wang",
      "Jiahui Li",
      "Seyran Khademi",
      "Jan van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EH/Wang_Attention-Aware_Age-Agnostic_Visual_Place_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EH/Wang_Attention-Aware_Age-Agnostic_Visual_Place_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A cross-domain visual place recognition (VPR) task is proposed in this work, i.e., matching images of the same architectures depicted in different domains. VPR is commonly treated as an image retrieval task, where a query image from an unknown location is matched with relevant instances from geo-tagged gallery database. Different from conventional VPR settings where the query images and gallery images come from the same domain, we propose a more common but challenging setup where the query images are collected under a new unseen condition. The two domains involved in this work are contemporary streetview images of Amsterdam from the Mapillary dataset (source domain) and historical images of the same city from Beeldbank dataset (target domain). We tailored an age-invariant feature learning CNN that can focus on domain invariant objects and learn to match images based on a weakly supervised ranking loss. We propose an attention aggregation module that is robust to domain discrepancy between the train and the test data. Further, a multi-kernel maximum mean discrepancy (MK-MMD) domain adaptation loss is adopted to improve the cross-domain ranking performance. Both attention and adaptation modules are unsupervised while the ranking loss uses weak supervision. Visual inspection shows that the attention module focuses on built forms while the dramatically changing environment are less weighed. Our proposed CNN achieves state of the art results (99% accuracy) on the single-domain VPR task and 20% accuracy at its best on the cross-domain VPR task, revealing the difficulty of age-invariant VPR.\r",
    "code_link": ""
  },
  "iccv2019_eh_end-to-endpartialconvolutionsneuralnetworksfordunhuanggrottoeswall-paintingrestoration": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EH",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - E-Heritage and Dunhuang Challenge",
    "title": "End-to-End Partial Convolutions Neural Networks for Dunhuang Grottoes Wall-Painting Restoration",
    "authors": [
      "Tianxiu Yu",
      "Cong Lin",
      "Shijie Zhang",
      "Shaodi You *",
      "Xiaohong Ding",
      "Jian Wu",
      "Jiawan Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EH/Yu_End-to-End_Partial_Convolutions_Neural_Networks_for_Dunhuang_Grottoes_Wall-Painting_Restoration_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EH/Yu_End-to-End_Partial_Convolutions_Neural_Networks_for_Dunhuang_Grottoes_Wall-Painting_Restoration_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we focus on training a deep neural network to in-paint and restore the historical painting of Dunhuang Grottoes. Dunhuang Grottoes is more than 1000 years old and the wall-painting on the grottoes has suffered from various deterioration. The ground truth does not exist either. Furthermore, learning the style of the artists is not straight forward because the wall-paintings are created by thousands of artists over more 400-500 years. As the very first attempt to solve this problem, we propose an end-to-end image restoration model for Dunhuang wall-painting. The end-to-end image restoration model employ U-net with partially convoluational layers to construct, which is capable in restoring non-rigid deteriorated content given a loss content mask and a wall-painting image. To learn the various artists style from real data, the training set and validation set are collected by using a zooming-in-like and random cropping approach on the digital RGB images photographed on the healthy Grotto-painting. We also synthesize the deteriorated paintings from real data. To ensure the synthetic content in the masked region is consistent to the ground truths in term of texture, colors, artistic style and free of unnecessary noises, the loss function is in a hybrid form that comprises transition variation loss, content loss and style loss. Our contributions are of three folds: 1) proposed using partial convolutional U-net in restoring wall-paintings; 2) the method is tested in restoring highly non-rigid and irregular deteriorated regions; 3) two types of masks are designed for simulating deteriorations and experimental results are satisfactory.\r",
    "code_link": ""
  },
  "iccv2019_hvu_recurrentconvolutionsforcausal3dcnns": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "Recurrent Convolutions for Causal 3D CNNs",
    "authors": [
      "Gurkirt SingH",
      "Fabio Cuzzolin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/SingH_Recurrent_Convolutions_for_Causal_3D_CNNs_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/SingH_Recurrent_Convolutions_for_Causal_3D_CNNs_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently, three dimensional (3D) convolutional neural networks (CNNs) have emerged as dominant methods to capture spatiotemporal representations in videos, by adding to pre-existing 2D CNNs a third, temporal dimension. Such 3D CNNs, however, are anti-causal (i.e., they exploit information from both the past and the future frames to produce feature representations, thus preventing their use in online settings), constrain the temporal reasoning horizon to the size of the temporal convolution kernel, and are not temporal resolution-preserving for video sequence-to-sequence modelling, as, for instance, in action detection. To address these serious limitations, here we present a new 3D CNN architecture for the causal/online processing of videos. Namely, we propose a novel Recurrent Convolutional Network (RCN), which relies on recurrence to capture the temporal context across frames at each network level. Our network decomposes 3D convolutions into (1) a 2D spatial convolution component, and (2) an additional hidden state 1 x 1 convolution, applied across time. The hidden state at any time t is assumed to depend on the hidden state at t - 1 and on the current output of the spatial convolution component. As a result, the proposed network: (i) produces causal outputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal resolution. Our experiments on the largescale large Kinetics and MultiThumos datasets show that the proposed method performs comparably to anti-causal 3D CNNs, while being causal and using fewer parameters.\r",
    "code_link": ""
  },
  "iccv2019_hvu_levelselectornetworkforoptimizingaccuracy-specificitytrade-offs": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "Level Selector Network for Optimizing Accuracy-Specificity Trade-Offs",
    "authors": [
      "Ahsan Iqbal",
      "Juergen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/Iqbal_Level_Selector_Network_for_Optimizing_Accuracy-Specificity_Trade-Offs_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/Iqbal_Level_Selector_Network_for_Optimizing_Accuracy-Specificity_Trade-Offs_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the increase in visual categories that become more and more fine-granular, maintaining high accuracy is a challenge. As the visual world can be organized in a semantic hierarchy, which is usually in form of a directed acyclic graph of many levels of abstraction, a classifier should be able to select an appropriate level trading off specificity for accuracy in case of uncertainty. In this work, we study the problem of finding accuracy vs. specificity trade-offs. To this end, we propose a Level Selector network, which selects the class granularity for the class prediction for an image or video, and a self-supervision based training strategy to train the Level Selector network. We show as part of the empirical evaluation, that our approach achieves superior results compared to the current state of the art on large-scale image and video datasets.\r",
    "code_link": ""
  },
  "iccv2019_hvu_end-to-endvideocaptioning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "End-to-End Video Captioning",
    "authors": [
      "Silvio Olivastri",
      "Gurkirt Singh",
      "Fabio Cuzzolin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/Olivastri_End-to-End_Video_Captioning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/Olivastri_End-to-End_Video_Captioning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Building correspondences across different modalities, such as video and language, has recently become critical in many visual recognition applications, such as video captioning. Inspired by machine translation, recent models tackle this task using an encoder-decoder strategy. The (video) encoder is traditionally a Convolutional Neural Network (CNN), while the decoding (for language generation) is done using a Recurrent Neural Network (RNN). Current state-of-the-art methods, however, train encoder and decoder separately. CNNs are pretrained on object and/or action recognition tasks and used to encode video-level features. The decoder is then optimised on such static features to generate the video's description. This disjoint setup is arguably sub-optimal for input (video) to output (description) mapping. In this work, we propose to optimise both encoder and decoder simultaneously in an end-to-end fashion. In a two-stage training setting, we first initialise our architecture using pre-trained encoders and decoders - then, the entire network is trained end-to-end in a fine-tuning stage to learn the most relevant features for video caption generation. In our experiments, we use GoogLeNet and Inception-ResNet-v2 as encoders and an original Soft-Attention (SA-) LSTM as a decoder. Analogously to gains observed in other computer vision problems, we show that end-to-end training significantly improves over the traditional, disjoint training process. We evaluate our End-to-End (EtENet) Networks on the Microsoft Research Video Description (MSVD) and the MSR Video to Text (MSR-VTT) benchmark datasets, showing how EtENet achieves state-of-the-art performance across the board.\r",
    "code_link": ""
  },
  "iccv2019_hvu_videorepresentationlearningbydensepredictivecoding": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "Video Representation Learning by Dense Predictive Coding",
    "authors": [
      "Tengda Han",
      "Weidi Xie",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/Han_Video_Representation_Learning_by_Dense_Predictive_Coding_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/Han_Video_Representation_Learning_by_Dense_Predictive_Coding_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.\r",
    "code_link": "https://github.com/TengdaHan/DPC"
  },
  "iccv2019_hvu_towardssegmentinganythingthatmoves": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "Towards Segmenting Anything That Moves",
    "authors": [
      "Achal Dave",
      "Pavel Tokmakov",
      "Deva Ramanan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/Dave_Towards_Segmenting_Anything_That_Moves_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/Dave_Towards_Segmenting_Anything_That_Moves_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Detecting and segmenting individual objects, regardless of their category, is crucial for many applications such as action detection or robotic interaction. While this problem has been well-studied under the classic formulation of spatio-temporal grouping, state-of-the-art approaches do not make use of learning-based methods. To bridge this gap, we propose a simple learning-based approach for spatio-temporal grouping. Our approach leverages motion cues from optical flow as a bottom-up signal for separating objects from each other. Motion cues are then combined with appearance cues that provide a generic objectness prior for capturing the full extent of objects. We show that our approach outperforms all prior work on the benchmark FBMS dataset. One potential worry with learning-based methods is that they might overfit to the particular type of objects that they have been trained on. To address this concern, we propose two new benchmarks for generic, moving object detection, and show that our model matches top-down methods on common categories, while significantly out-performing both top-down and bottom-up methods on never-before-seen categories.\r",
    "code_link": ""
  },
  "iccv2019_hvu_video-textcomplianceactivityverificationbasedonnaturallanguageinstructions": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "Video-Text Compliance: Activity Verification Based on Natural Language Instructions",
    "authors": [
      "Mayoore Jaiswal",
      "Frank Liu",
      "Anupama Jagannathan",
      "Anne Gattiker",
      "Inseok Hwang",
      "Jinho Lee",
      "Matthew Tong",
      "Sahil Dureja",
      "Soham Shah",
      "Peter Hofstee",
      "Valerie Chen",
      "Suvadip Paul",
      "Rogerio Feris"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/Jaiswal_Video-Text_Compliance_Activity_Verification_Based_on_Natural_Language_Instructions_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/Jaiswal_Video-Text_Compliance_Activity_Verification_Based_on_Natural_Language_Instructions_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We define a new multi-modal compliance problem, which is to determine if the human activity in a given video is in compliance with an associated text instruction. Solutions to the compliance problem could enable automatic compliance checking and efficient feedback in many real-world settings. To this end, we introduce the Video-Text Compliance (VTC) dataset, which contains videos of atomic activities, along with text instructions and compliance labels. The VTC dataset is constructed by an auto-augmentation technique, preserves privacy, and contains over 1.2 million frames. Finally, we present ComplianceNet, a novel end-to-end trainable compliance network that improves the baseline accuracy by 27.5% on average when trained on the VTC dataset. We plan to release the VTC dataset to the community for future research.\r",
    "code_link": ""
  },
  "iccv2019_hvu_interpretablespatio-temporalattentionforvideoactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "Interpretable Spatio-Temporal Attention for Video Action Recognition",
    "authors": [
      "Lili Meng",
      "Bo Zhao",
      "Bo Chang",
      "Gao Huang",
      "Wei Sun",
      "Frederick Tung",
      "Leonid Sigal"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/Meng_Interpretable_Spatio-Temporal_Attention_for_Video_Action_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/Meng_Interpretable_Spatio-Temporal_Attention_for_Video_Action_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Inspired by the observation that humans are able to process videos efficiently by only paying attention where and when it is needed, we propose an interpretable and easy plug-in spatial-temporal attention mechanism for video action recognition. For spatial attention, we learn a saliency mask to allow the model to focus on the most salient parts of the feature maps. For temporal attention, we employ a convolutional LSTM based attention mechanism to identify the most relevant frames from an input video. Further, we propose a set of regularizers to ensure that our attention mechanism attends to coherent regions in space and time. Our model not only improves video action recognition accuracy, but also localizes discriminative regions both spatially and temporally, despite being trained in a weakly-supervised manner with only classification labels (no bounding box labels or time frame temporal labels). We evaluate our approach on several public video action recognition datasets with ablation studies. Furthermore, we quantitatively and qualitatively evaluate our model's ability to localize discriminative regions spatially and critical frames temporally. Experimental results demonstrate the efficacy of our approach, showing superior or comparable accuracy with the state-of-the-art methods while increasing model interpretability.\r",
    "code_link": ""
  },
  "iccv2019_hvu_markovdecisionprocessforvideogeneration": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Large Scale Holistic Video Understanding",
    "title": "Markov Decision Process for Video Generation",
    "authors": [
      "Vladyslav Yushchenko",
      "Nikita Araslanov",
      "Stefan Roth"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HVU/Yushchenko_Markov_Decision_Process_for_Video_Generation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HVU/Yushchenko_Markov_Decision_Process_for_Video_Generation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We identify two pathological cases of temporal inconsistencies in video generation: video freezing and video looping. To better quantify the temporal diversity, we propose a class of complementary metrics that are effective, easy to implement, data agnostic, and interpretable. Further, we observe that current state-of-the-art models are trained on video samples of fixed length thereby inhibiting long-term modeling. To address this, we reformulate the problem of video generation as a Markov Decision Process (MDP). The underlying idea is to represent motion as a stochastic process with an infinite forecast horizon to overcome the fixed length limitation and to mitigate the presence of temporal artifacts. We show that our formulation is easy to integrate into the state-of-the-art MoCoGAN framework. Our experiments on the Human Actions and UCF-101 datasets demonstrate that our MDP-based model is more memory efficient and improves the video quality both in terms of the new and established metrics.\r",
    "code_link": "https://github.com/sergeytulyakov/mocogan"
  },
  "iccv2019_coview_enhancingtemporalactionlocalizationwithtransferlearningfromactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CoView",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Comprehensive Video Understanding in the Wild",
    "title": "Enhancing Temporal Action Localization with Transfer Learning from Action Recognition",
    "authors": [
      "Ahsan Iqbal",
      "Alexander Richard",
      "Juergen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CoView/Iqbal_Enhancing_Temporal_Action_Localization_with_Transfer_Learning_from_Action_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CoView/Iqbal_Enhancing_Temporal_Action_Localization_with_Transfer_Learning_from_Action_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Temporal localization of actions in videos has been of increasing interest in recent years. However, most existing approaches rely on complex architectures that are either expensive to train, inefficient at inference time, or require thorough and careful architecture engineering. Classical action recognition on pre-segmented clips, on the other hand, benefits from sophisticated deep architectures that paved the way for highly reliable video clip classifiers. In this paper, we propose to use transfer learning to leverage the good results from action recognition for temporal localization. We apply a network that is inspired by the classical bag-of-words model for transfer learning and show that the resulting framewise class posteriors already provide good results without explicit temporal modeling. Further, we show that combining these features with a deep but simple convolutional network achieves state of the art results on two challenging action localization datasets.\r",
    "code_link": "https://github.com/alexanderrichard/coview2019"
  },
  "iccv2019_coview_temporalu-netsforvideosummarizationwithsceneandactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CoView",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Comprehensive Video Understanding in the Wild",
    "title": "Temporal U-Nets for Video Summarization with Scene and Action Recognition",
    "authors": [
      "Heeseung Kwon",
      "Woohyun Shim",
      "Minsu Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CoView/Kwon_Temporal_U-Nets_for_Video_Summarization_with_Scene_and_Action_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CoView/Kwon_Temporal_U-Nets_for_Video_Summarization_with_Scene_and_Action_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " While videos contain long-term temporal information with diverse contents, existing approaches to video understanding usually focus on a short trimmed video clip with a specific content such as a particular action or object. For comprehensive understanding of untrimmed videos, we address an integrated video task of video summarization with scene and action recognition. We propose a novel convolutional neural network architecture for handling untrimmed videos with multiple contents. The proposed architecture is an encoder-decoder structure where the encoder captures long-term temporal dynamics from an entire video and the decoder predicts detailed temporal information of multiple contents of the video. Two-stream processing is adopted for obtaining feature representations, one for focusing on the spatial information and the other for the temporal information. We evaluate the proposed method on the benchmark of the Challenge on Comprehensive Video Understanding in the Wild (CoVieW 2019), and the experimental results demonstrate that our method achieves outstanding performance\r",
    "code_link": ""
  },
  "iccv2019_coview_videosummarizationbylearningrelationshipsbetweenactionandscene": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CoView",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Comprehensive Video Understanding in the Wild",
    "title": "Video Summarization by Learning Relationships between Action and Scene",
    "authors": [
      "Jungin Park",
      "Jiyoung Lee",
      "Sangryul Jeon",
      "Kwanghoon Sohn"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CoView/Park_Video_Summarization_by_Learning_Relationships_between_Action_and_Scene_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CoView/Park_Video_Summarization_by_Learning_Relationships_between_Action_and_Scene_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a novel deep architecture for video summarization in untrimmed videos that simultaneously recognizes action and scene classes for every video segments. Our networks accomplish this through a multi-task fusion approach based on two types of attention modules to explore semantic correlations between action and scene in the videos. The proposed networks consist of the feature embedding networks and attention inference networks to stochastically leverage the inferred action and scene feature representations. Additionally, we design a new center loss function that learns the feature representations by enforcing to minimize the intra-class variations and to maximize the inter-class variations. Our model achieves a score of 0.8409 for summarization and accuracy of 0.7294 for action and scene recognition on test set of CoVieW'19 dataset, which is ranked 3rd.\r",
    "code_link": ""
  },
  "iccv2019_coview_videomultitasktransformernetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CoView",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Comprehensive Video Understanding in the Wild",
    "title": "Video Multitask Transformer Network",
    "authors": [
      "Hongje Seong",
      "Junhyuk Hyun",
      "Euntai Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CoView/Seong_Video_Multitask_Transformer_Network_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CoView/Seong_Video_Multitask_Transformer_Network_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose the Multitask Transformer Network for multitasking on untrimmed video. To analyze the untrimmed video, it needs to capture important frame and region in the spatio-temporal domain. Therefore, we utilize the Transformer Network, which can capture the useful features from CNN representations through an attention mechanism. Motivated by the Action Transformer Network, which is a repurposed model of the Transformer for video, we modified the concept of query which was specialized only for action recognition on the trimmed video to fit the untrimmed video. In addition, we modified the structure of the Transformer unit to the pre-activation structure for identity mapping on residual connections. We also utilize the class conversion matrix (CCM), one of the feature fusion methods, to share the information of different tasks. Combining our Transformer structure and CCM, the Multitask Transformer Network is proposed for multitasking on untrimmed video. Eventually, our model evaluated on CoVieW 2019, and we enhanced the performance through post-processing based on prediction results that suitable to the CoVieW 2019 evaluation metric. In CoVieW 2019 challenge, we placed fourth on final rank while first on scene and action score.\r",
    "code_link": ""
  },
  "iccv2019_coview_comprehensivevideounderstandingvideosummarizationwithcontent-basedvideorecommenderdesign": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CoView",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Comprehensive Video Understanding in the Wild",
    "title": "Comprehensive Video Understanding: Video Summarization with Content-Based Video Recommender Design",
    "authors": [
      "Yudong Jiang",
      "Kaixu Cui",
      "Bo Peng",
      "Changliang Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CoView/Jiang_Comprehensive_Video_Understanding_Video_Summarization_with_Content-Based_Video_Recommender_Design_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CoView/Jiang_Comprehensive_Video_Understanding_Video_Summarization_with_Content-Based_Video_Recommender_Design_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video summarization aims to extract keyframes/shots from a long video. Previous methods mainly take diversity and representativeness of generated summaries as prior knowledge in algorithm design. In this paper, we formulate video summarization as a content-based recommender problem, which should distill the most useful content from a long video for users who suffer from information overload. A scalable deep neural network is proposed on predicting if one video segment is a useful segment for users by explicitly modelling both segment and video. Moreover, we accomplish scene and action recognition in untrimmed videos in order to find more correlations among different aspects of video understanding tasks. Also, our paper will discuss the effect of audio and visual features in summarization task. We also extend our work by data augmentation and multi-task learning for preventing the model from early-stage overfitting. The final results of our model win the first place in ICCV 2019 CoView Workshop Challenge Track.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_efficientreal-timecamerabasedestimationofheartrateanditsvariability": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Efficient Real-Time Camera Based Estimation of Heart Rate and Its Variability",
    "authors": [
      "Amogh Gudi",
      "Marian Bittner",
      "Roelof Lochmans",
      "Jan van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Gudi_Efficient_Real-Time_Camera_Based_Estimation_of_Heart_Rate_and_Its_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Gudi_Efficient_Real-Time_Camera_Based_Estimation_of_Heart_Rate_and_Its_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Remote photo-plethysmography (rPPG) uses a remotely placed camera to estimating a person's heart rate (HR). Similar to how heart rate can provide useful information about a person's vital signs, insights about the underlying physio/psychological conditions can be obtained from heart rate variability (HRV). HRV is a measure of the fine fluctuations in the intervals between heart beats. However, this measure requires temporally locating heart beats with a high degree of precision. We introduce a refined and efficient real-time rPPG pipeline with novel filtering and motion suppression that not only estimates heart rate more accurately, but also extracts the pulse waveform to time heart beats and measure heart rate variability. This method requires no rPPG specific training and is able to operate in real-time. We validate our method on a self-recorded dataset under an idealized lab setting, and show state-of-the-art results on two public dataset with realistic conditions (VicarPPG and PURE).\r",
    "code_link": ""
  },
  "iccv2019_cvpm_onthevectorspaceinphotoplethysmographyimaging": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "On the Vector Space in Photoplethysmography Imaging",
    "authors": [
      "Christian Pilz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Pilz_On_the_Vector_Space_in_Photoplethysmography_Imaging_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Pilz_On_the_Vector_Space_in_Photoplethysmography_Imaging_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We study the vector space of visible wavelength intensities from face videos widely used as input features in Photoplethysmography Imaging (PPGI). Based upon theoretical principles of group invariance in the Euclidean space, we derive a change of the topology where the corresponding distance between successive measurements is defined as geodesic on a Riemannian manifold. This lower dimensional embedding of the sensor signal unifies the invariance properties with respect to translation of the features as discussed by several former approaches. The resulting operator acts implicitly on the feature space without requiring any kind of prior knowledge and parameter tuning. The resulting feature's time varying quasi-periodic shaping naturally occurs in form of the canonical state space representation according to the known diffusion process of blood volume changes. This reduces drastically computational complexity and consequently simplifies the implementation. Experiments from face videos on two public databases have shown a competitive estimation performances of heart rate and robustness in comparison with already available methods.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_a-malautomaticmotionassessmentlearningfromproperlyperformedmotionsin3dskeletonvideos": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "A-MAL: Automatic Motion Assessment Learning from Properly Performed Motions in 3D Skeleton Videos",
    "authors": [
      "Tal Hakim",
      "Ilan Shimshoni"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Hakim_A-MAL_Automatic_Motion_Assessment_Learning_from_Properly_Performed_Motions_in_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Hakim_A-MAL_Automatic_Motion_Assessment_Learning_from_Properly_Performed_Motions_in_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Assessment of motion quality has recently gained high demand in a variety of domains. The ability to automatically assess subject motion in videos that were captured by cheap devices, such as Kinect cameras, is essential for monitoring clinical rehabilitation processes, for improving motor skills and for motion learning tasks. The need to pay attention to low-level details while accurately tracking the motion stages, makes this task very challenging. In this work, we introduce A-MAL, an automatic, strong motion assessment learning algorithm that only learns from properly-performed motion videos without further annotations, powered by a deviation time-segmentation algorithm, a parameter relevance detection algorithm, a novel time-warping algorithm that is based on automatic detection of common temporal points-of-interest and a textual-feedback generation mechanism. We demonstrate our method on motions from the Fugl-Meyer Assessment (FMA) test, which is typically held by occupational therapists in order to monitor patients' recovery processes after strokes.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_whogoesthere?exploitingsilhouettesandwearablesignalsforsubjectidentificationinmulti-personenvironments": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Who Goes There? Exploiting Silhouettes and Wearable Signals for Subject Identification in Multi-Person Environments",
    "authors": [
      "Alessandro Masullo",
      "Tilo Burghardt",
      "Dima Damen",
      "Toby Perrett",
      "Majid Mirmehdi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Masullo_Who_Goes_There_Exploiting_Silhouettes_and_Wearable_Signals_for_Subject_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Masullo_Who_Goes_There_Exploiting_Silhouettes_and_Wearable_Signals_for_Subject_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The re-identification of people in private environments is a rather complicated task, not only from a technical standpoint but also for the ethical issues connected to it. The lack of a privacy-sensitive technology to monitor specific individuals prevents the uptake of assistive systems, for example in Ambient Assisted Living and health monitoring applications. Our approach adopts a deep learning multimodal framework to match silhouette video clips and accelerometer signals to identify and re-identify the subjects of interest within a multi-person environment. Brief sequences, which may be as short as only 3 seconds, are encoded within a latent space where simple Euclidean distance can be used to discriminate the matching. Identities are only revealed in terms of accelerometer carriers, and the use of silhouettes instead of RGB signals helps to ring-fence privacy concerns. We train our method on the SPHERE Calorie Dataset, for which we show an average area under the ROC curve of 76.3%. We also propose a novel triplet loss for which we demonstrate improving performances and convergence speeds.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_dynamicfacialmodelsforvideo-baseddimensionalaffectestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Dynamic Facial Models for Video-Based Dimensional Affect Estimation",
    "authors": [
      "Siyang Song",
      "Enrique Sanchez-Lozano",
      "Mani Kumar Tellamekala",
      "Linlin Shen",
      "Alan Johnston",
      "Michel Valstar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Song_Dynamic_Facial_Models_for_Video-Based_Dimensional_Affect_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Song_Dynamic_Facial_Models_for_Video-Based_Dimensional_Affect_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Dimensional affect estimation from a face video is a challenging task, mainly due to the large number of possible facial displays made up of a set of behaviour primitives including facial muscle actions. The displays vary not only in composition but also in temporal evolution, with each display composed of behaviour primitives with varying in their short and long-term characteristics. Most existing work models affect relies on complex hierarchical recurrent models unable to capture short-term dynamics well. In this paper, we propose to encode these short-term facial shape and appearance dynamics in an image, where only the semantic meaningful information is encoded into the dynamic face images. We also propose binary dynamic facial masks to remove 'stable pixels' from the dynamic images. This process allows filtering of non-dynamic information, i.e. only pixels that have changed in the sequence are retained. Then, the final proposed Dynamic Facial Model (DFM) encodes both filtered facial appearance and shape dynamics of a image sequence preceding to the given frame into a three-channel raster image. A CNN-RNN architecture is tasked with modelling primarily the long-term changes. Experiments show that our dynamic face images achieved superior performance over the standard RGB face images on dimensional affect prediction task.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_multimodaldeepmodelsforpredictingaffectiveresponsesevokedbymovies": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Multimodal Deep Models for Predicting Affective Responses Evoked by Movies",
    "authors": [
      "Ha Thi Phuong Thao",
      "Dorien Herremans",
      "Gemma Roig"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Thao_Multimodal_Deep_Models_for_Predicting_Affective_Responses_Evoked_by_Movies_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Thao_Multimodal_Deep_Models_for_Predicting_Affective_Responses_Evoked_by_Movies_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The goal of this study is to develop and analyze multimodal models for predicting experienced affective responses of viewers watching movie clips. We develop hybrid multimodal prediction models based on both the video and audio of the clips. For the video content, we hypothesize that both image content and motion are crucial features for evoked emotion prediction. To capture such information, we extract features from RGB frames and optical flow using pre-trained neural networks. For the audio model, we compute an enhanced set of low-level descriptors including intensity, loudness, cepstrum, linear predictor coefficients, pitch and voice quality. Both visual and audio features are then concatenated to create audio-visual features, which are used to predict the evoked emotion. To classify the movie clips into the corresponding affective response categories, we propose two approaches based on deep neural network models. The first one is based on fully connected layers without memory on the time component, the second incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). We perform a thorough analysis of the importance of each feature set. Our experiments reveal that in our set-up, predicting emotions at each time step independently gives slightly better accuracy performance than with the LSTM. Interestingly, we also observe that the optical flow is more informative than the RGB in videos, and overall, models using audio features are more accurate than those based on video features when making the final prediction of evoked emotions.\r",
    "code_link": "https://github.com/jeffreyhuang1/twostream-action-recognition"
  },
  "iccv2019_cvpm_single-imagefacialexpressionrecognitionusingdeep3dre-centralization": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Single-Image Facial Expression Recognition Using Deep 3D Re-Centralization",
    "authors": [
      "Zhipeng Bao",
      "Shaodi You",
      "Lin Gu",
      "Zhenglu Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Bao_Single-Image_Facial_Expression_Recognition_Using_Deep_3D_Re-Centralization_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Bao_Single-Image_Facial_Expression_Recognition_Using_Deep_3D_Re-Centralization_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Facial expression recognition (FER) aims to encode expression information from faces. Previous studies often hold the assumption that human subjects should properly face the camera. Such a laboratory-controlled condition, however, is too rigid for in-wide applications. To tackle this issue, we propose a single image facial expression recognition method that is robust to face orientation and light conditions. We achieved this by proposing a novel face re-centralization method by reconstructing a 3D face model from a single image. We then propose a novel end-to-end deep neural network that utilizes both re-centralized 3D model and landmarks for FER task. A comprehensive evaluation on three real-world datasets illustrates that the proposed model outperforms the state-of-the-art techniques in both large-scale and small-scale datasets. The superiority of our model on effectiveness and robustness is also demonstrated in both laboratory conditions and wild images.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_clinicalscenesegmentationwithtinydatasets": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Clinical Scene Segmentation with Tiny Datasets",
    "authors": [
      "Thomas J. Smith",
      "Michel Valstar",
      "Don Sharkey",
      "John Crowe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Smith_Clinical_Scene_Segmentation_with_Tiny_Datasets_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Smith_Clinical_Scene_Segmentation_with_Tiny_Datasets_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Many clinical procedures could benefit from automatic scene segmentation and subsequent action recognition. Using Convolutional Neural Networks to semantically segment meaningful parts of an image or video is still an unsolved problem. This becomes even more apparent when only a small dataset is available. Whilst using RGB as the input is sufficient for a large labelled dataset, achieving high accuracy on a small dataset directly from RGB is difficult. This is because the ratio of free image dimensions to the number of training images is very high, resulting in unavoidable underfitting. We show that the addition of superpixels to represent an image in our network improves the semantic segmentation, and that superpixels can be learned to be detected by Convolutional Neural Networks if those superpixels are appropriately represented. Here we present a novel representation for superpixels, multichannel connected graphs (MCGs). We show how using pre-trained deep learned superpixels used in an end-to-end manner achieve good semantic segmentation results without the need for large quantities of labelled data, by training with only 20 instances for 23 classes.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_performanceevaluationofvisualobjectdetectionandtrackingalgorithmsusedinremotephotoplethysmography": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Performance Evaluation of Visual Object Detection and Tracking Algorithms Used in Remote Photoplethysmography",
    "authors": [
      "Changchen Zhao",
      "Peiyi Mei",
      "Shoushuai Xu",
      "Yongqiang Li",
      "Yuanjing Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Zhao_Performance_Evaluation_of_Visual_Object_Detection_and_Tracking_Algorithms_Used_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Zhao_Performance_Evaluation_of_Visual_Object_Detection_and_Tracking_Algorithms_Used_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " While most existing remote photoplethysmography (rPPG) approaches employ off-the-shelf visual object detection and tracking algorithms, these algorithms may not be well suited for rPPG problem. The detection and tracking algorithms are designed to be robust to fast deformations, non-distinctive color, fast translations, etc. while rPPG cares about background intervention, region consistency, smoothness of the traces, etc. Hence, there is a gap between a good detection and tracking algorithm and the rPPG measurement accuracy. This paper aims at bridging this gap by evaluating the performance of popular detection and tracking algorithms widely used in rPPG methods. We establish a processing pipeline and choose four detection and tracking algorithms. Experiments are conducted on two publicly available datasets and one self-collected dataset. We find three key factors that affect the rPPG accuracy: 1) stability of the tracking trajectory, 2) content consistency, and 3) robustness to deformation and fast translation. This study highlights the need for developing novel detection and tracking algorithms dedicated to rPPG and gives some useful insights.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_camera-basedon-lineshortcessationofbreathingdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Camera-Based On-Line Short Cessation of Breathing Detection",
    "authors": [
      "Ilde Lorato",
      "Sander Stuijk",
      "Mohammed Meftah",
      "Wim Verkruijsse",
      "Gerard de Haan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Lorato_Camera-Based_On-Line_Short_Cessation_of_Breathing_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Lorato_Camera-Based_On-Line_Short_Cessation_of_Breathing_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Apnea detection is extremely important in neonatal settings because hypoxia can lead to permanent impairment. Short cessations of breathing are very common in infants and could be used for example for the prediction of longer apneas. The aim of this study is to investigate the accuracy of our on-line cessation of breathing detector. Signals obtained through camera-based respiration monitoring were analyzed in five infants with 91 annotated cessations of breathing. The method proposed is based on the comparison of short-term and long-term standard deviations allowing the detection of sudden amplitude reduction in the signal with a low latency. A new strategy able to detect short cessations of breathing on-line was successfully validated yielding an average accuracy of 93%.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_contact-freemonitoringofphysiologicalparametersinpeoplewithprofoundintellectualandmultipledisabilities": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Contact-Free Monitoring of Physiological Parameters in People With Profound Intellectual and Multiple Disabilities",
    "authors": [
      "Gasper Slapnicar",
      "Erik Dovgan",
      "Pia Cuk",
      "Mitja Lustrek"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Slapnicar_Contact-Free_Monitoring_of_Physiological_Parameters_in_People_With_Profound_Intellectual_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Slapnicar_Contact-Free_Monitoring_of_Physiological_Parameters_in_People_With_Profound_Intellectual_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper presents a contact-free method for physiological parameter estimation in people with profound intellectual and multiple disabilities (PIMD). We used an existing state-of-the-art algorithm Plane-Orthogonal-to-Skin (POS) in order to obtain an initial remote photoplethysmogram (rPPG) reconstruction from facial videos. We enhanced this signal by applying a long-short-term-memory (LSTM) neural network to the initial PPG reconstruction. Evaluation of our method on a public database DEAP has shown heart rate (HR) error of 8.09 beats-per-minute, suprpassing the state-of-the-art POS algorithm implementation, which had error of 13.36 BPM. More importantly, a good correlation between our predictions and ground-truth HRs has been observed. The method is currently being implemented as part of a system which aims to monitor people with PIMD in real time in order to obtain information about their physiological and psychological state and in turn increase their quality of life.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_microexpressionclassificationusingfacialcoloranddeeplearningmethods": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Micro Expression Classification using Facial Color and Deep Learning Methods",
    "authors": [
      "Hadas Shahar",
      "Hagit Hel-Or"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Shahar_Micro_Expression_Classification_using_Facial_Color_and_Deep_Learning_Methods_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Shahar_Micro_Expression_Classification_using_Facial_Color_and_Deep_Learning_Methods_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Micro emotions are a unique type of facial expression as they are involuntary and very brief. They usually occur when a person attempts to suppress or hide their emotions. Lasting less than 500 ms, they can be very hard to detect even for the human eye, however, since they reveal a person's true feelings, they are of extreme interest in many fields of study. Most approaches to automatic detection and classification of micro emotions rely on detecting the small residual facial movements. In this paper, we propose to exploit an aspect of the human face which is much harder to subdue, namely, facial color change due to blood flow during expression of emotion. We propose a system that evaluates color change during micro emotion expression and successfully classifies the emotion type. This approach is unique in that it disregards the motion related aspects of the expression, and relies entirely on the facial color. We show that our system improves over movement based approaches.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_athermalcamerabasedcontinuousbodytemperaturemeasurementsystem": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "A Thermal Camera Based Continuous Body Temperature Measurement System",
    "authors": [
      "Jia-Wei Lin",
      "Ming-Hung Lu",
      "Yuan-Hsiang Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Lin_A_Thermal_Camera_Based_Continuous_Body_Temperature_Measurement_System_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Lin_A_Thermal_Camera_Based_Continuous_Body_Temperature_Measurement_System_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Body temperature acting an important role in medicine, a number of diseases are characterized by a change in human body temperature. Monitoring body temperature also allows the doctor to track the effectiveness of treatments. But current continuous body temperature measurement (CBTM) system is mainly limited by reaction time, movement noise, and labor requirement. In addition, the traditional contact body temperature measurement has the problem of wasting consumables and causing discomfort. To address above issues, we present a non-contact, automatic CBTM system using a single thermal camera. By applying deep-learning based face detection, object tracking, and calibrated conversion equation, we can successfully extract subject's forehead temperature in real-time. The experimental results show that the overall mean absolute error (MAE) and root-mean-squared-error (RMSE) of our proposed framework compared with industrial instrument are 0.375 degC and 0.439 degC, respectively.\r",
    "code_link": "https://github.com/chuanqi305/MobileNet-SSD"
  },
  "iccv2019_cvpm_architecturaltricksfordeeplearninginremotephotoplethysmography": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Architectural Tricks for Deep Learning in Remote Photoplethysmography",
    "authors": [
      "Mikhail Kopeliovich",
      "Yuriy Mironenko",
      "Mikhail Petrushan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Kopeliovich_Architectural_Tricks_for_Deep_Learning_in_Remote_Photoplethysmography_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Kopeliovich_Architectural_Tricks_for_Deep_Learning_in_Remote_Photoplethysmography_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Architectural improvements are studied for convolutional network performing estimation of heart rate (HR) values on color signal patches. Color signals are time series of color components averaged over facial regions recorded by webcams in two scenarios: Stationary (without motion of a person) and Mixed Motion (different motion patterns of a person). HR estimation problem is addressed as a classification task, where classes correspond to different heart rate values within the admissible range of [40; 125] bpm. Both adding convolutional filtering layers after fully connected layers and involving combined loss function where first component is a cross entropy and second is a squared error between the network output and smoothed one-hot vector, lead to better performance of HR estimation model in Stationary and Mixed Motion scenarios.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_impactofsympatheticactivationinimagingphotoplethysmography": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Impact of Sympathetic Activation in Imaging Photoplethysmography",
    "authors": [
      "Alexander Woyczyk",
      "Stefan Rasche",
      "Sebastian Zaunseder"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Woyczyk_Impact_of_Sympathetic_Activation_in_Imaging_Photoplethysmography_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Woyczyk_Impact_of_Sympathetic_Activation_in_Imaging_Photoplethysmography_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Photplethysmography (PPG) is known to reflect changes in sympathetic tone. This contribution investigates the behaviour of imaging photoplethysmography (iPPG) upon sympathetic activation. To that end, we assessed the impact of a distal painful stimulus on the facial iPPG and contralateral finger PPG waveforms. Our results show that alternating components of both signals behave differently. As expected, the alternating component of the finger PPG signal shows a significant and persistent decrease upon stimulus (p < 0.001). The alternating component of the iPPG signal shows only a slight decrease followed by a fast increase (p < 0.01). The found behaviour might be explained by different degrees of sympathetic responsiveness in the extremities and in the face. Sympathetic activation increases cardiac output and triggers general vasoconstriction. Extremities show highly pronounced vasoconstriction which decreases the alternating component. The facial vasoconstriction is comparatively small. As a result, local vasoconstriction might cause a short-term decrease followed by the contrary effect, namely an increase in the alternating component, driven by an increased systemic pulse pressure. Our finding has relevance for the interpretation of iPPG signals and the design of future use cases beyond remote heart rate assessment. In particular, care should be taken when expectations on the finger PPG are to be transferred to iPPG.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_combatingtheimpactofvideocompressiononnon-contactvitalsignmeasurementusingsupervisedlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Combating the Impact of Video Compression on Non-Contact Vital Sign Measurement Using Supervised Learning",
    "authors": [
      "Ewa Nowara",
      "Daniel McDuff"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Nowara_Combating_the_Impact_of_Video_Compression_on_Non-Contact_Vital_Sign_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Nowara_Combating_the_Impact_of_Video_Compression_on_Non-Contact_Vital_Sign_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Imaging photoplethysmography (iPPG) and imaging ballistocardiography (iBCG) are popular approaches for unobtrusive camera-based measurement of vital signs. These involve recovering pulse signals from very subtle variations in video pixel intensities, which are easily corrupted by noise. Therefore, while the signal might be easy to obtain from high quality uncompressed videos, the signal-to-noise ratio drops linearly with video bit-rate. Uncompressed videos require large amounts of storage making them prohibitive to store, stream and transfer in large quantities. By learning compression specific models we show that supervised learning can be used to increase the signal-to-noise ratio (SNR) of pulse signals and reduce the mean absolute error (MAE) of heart rate estimates extracted from temporally compressed videos. We perform a systematic evaluation of the performance of our algorithm showing that the network trained on compressed videos consistently outperforms the model trained on the original less compressed compressed videos, both on videos with and without significant head motions. We found improvements in SNR of up to 8 dB and MAE of 6 BPM.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_modelingonthefeasibilityofcamera-basedbloodglucosemeasurement": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Modeling on the Feasibility of Camera-Based Blood Glucose Measurement",
    "authors": [
      "Yiyin Wang",
      "Wenjin Wang",
      "Mark van Gastel",
      "Gerard de Haan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Wang_Modeling_on_the_Feasibility_of_Camera-Based_Blood_Glucose_Measurement_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Wang_Modeling_on_the_Feasibility_of_Camera-Based_Blood_Glucose_Measurement_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Monitoring of blood glucose levels is crucial for diabetics to manage their lives. However, the current gold-standard requires taking invasive blood samples, which is painful and can lead to infections. In this paper, we investigate the feasibility of using a regular camera (with silicon image sensors) to estimate the blood glucose levels remotely as claimed by recent studies. The physiological challenge is the small volume fraction and low absorption of glucose in the human body as compared to other absorbers. The glucose-induced variations in light intensity from both the non-pulsating and the pulsating part of the reflected optical signal are modeled in the visible to near-infrared wavelength range. The simulation results suggest that it is unlikely to detect the blood glucose based on either the DC or AC component of skin reflected light. The optical responses caused by glucose changes are minor as compared to other physiological factors (e.g. skin temperature, SaO2, water concentration). This, combined with the coarse sampling of the light spectrum by regular cameras render the measurement infeasible.\r",
    "code_link": ""
  },
  "iccv2019_cvpm_predictingheartratevariationsofdeepfakevideosusingneuralode": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Physiological Measurement",
    "title": "Predicting Heart Rate Variations of Deepfake Videos using Neural ODE",
    "authors": [
      "Steven Fernandes",
      "Sunny Raj",
      "Eddy Ortiz",
      "Iustina Vintila",
      "Margaret Salter",
      "Gordana Urosevic",
      "Sumit Jha"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVPM/Fernandes_Predicting_Heart_Rate_Variations_of_Deepfake_Videos_using_Neural_ODE_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVPM/Fernandes_Predicting_Heart_Rate_Variations_of_Deepfake_Videos_using_Neural_ODE_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deepfake is a technique used to manipulate videos using computer code. It involves replacing the face of a person in a video with the face of another person. The automation of video manipulation means that deepfakes are becoming more prevalent and easier to implement. This can be credited to the emergence of apps like FaceApp and FakeApp, which allow users to create their own deepfake videos using their smartphones. It has hence become essential to detect fake videos, to avoid the spread of false information. A recent study shows that the heart rate of fake videos can be used to distinguish original and fake videos. In the study presented, we obtained the heart rate of original videos and trained the state-of-the-art Neural Ordinary Differential Equations (Neural-ODE) model. We then created deepfake videos using commercial software. The average loss obtained for ten original videos is 0.010927, and ten donor videos are 0.010041. The trained Neural-ODE was able to predict the heart rate of our 10 deepfake videos generated using commercial software and 320 deepfake videos of deepfakeTIMI database. To best of our knowledge, this is the first attempt to train a Neural-ODE on original videos to predict the heart rate of fake videos.\r",
    "code_link": ""
  },
  "iccv2019_sgrl_visualrelationshipsasfunctionsenablingfew-shotscenegraphprediction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "Visual Relationships as Functions:Enabling Few-Shot Scene Graph Prediction",
    "authors": [
      "Apoorva Dornadula",
      "Austin Narcomey",
      "Ranjay Krishna",
      "Michael Bernstein",
      "Fei-Fei Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Dornadula_Visual_Relationships_as_FunctionsEnabling_Few-Shot_Scene_Graph_Prediction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Dornadula_Visual_Relationships_as_FunctionsEnabling_Few-Shot_Scene_Graph_Prediction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Scene graph prediction -- classifying the set of objects and predicates in a visual scene -- requires substantial training data. The long-tailed distribution of relationships can be an obstacle for such approaches, however, as they can only be trained on the small set of predicates that carry sufficient labels. We introduce the first scene graph prediction model that supports few-shot learning of predicates, enabling scene graph approaches to generalize to a set of new predicates. First, we introduce a new model of predicates as functions that operate on object features or image locations. Next, we define a scene graph model where these functions are trained as message passing protocols within a new graph convolution framework. We train the framework with a frequently occurring set of predicates and show that our approach outperforms those that use the same amount of supervision by 1.78 at recall@50 and performs on par with other scene graph models. Next, we extract object representations generated by the trained predicate functions to train few-shot predicate classifiers on rare predicates with as few as 1 labeled example. When compared to strong baselines like transfer learning from existing state-of-the-art representations, we show improved 5-shot performance by 4.16 recall@1. Finally, we show that our predicate functions generate interpretable visualizations, enabling the first interpretable scene graph model.\r",
    "code_link": ""
  },
  "iccv2019_sgrl_spatialresiduallayeranddenseconnectionblockenhancedspatialtemporalgraphconvolutionalnetworkforskeleton-basedactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition",
    "authors": [
      "Cong Wu",
      "Xiao-Jun Wu",
      "Josef Kittler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Wu_Spatial_Residual_Layer_and_Dense_Connection_Block_Enhanced_Spatial_Temporal_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Wu_Spatial_Residual_Layer_and_Dense_Connection_Block_Enhanced_Spatial_Temporal_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent research has shown that modeling the dynamic joint features of the human body by a graph convolutional network (GCN) is a groundbreaking approach for skeleton-based action recognition, especially for the recognition of the body-motion, human-object and human-human interactions. Nevertheless, how to model and utilize coherent skeleton information comprehensively is still an open problem. In order to capture the rich spatiotemporal information and utilize features more effectively, we introduce a spatial residual layer and a dense connection block enhanced spatial temporal graph convolutional network. More specifically, our work introduces three aspects. Firstly, we extend spatial graph convolution to spatial temporal graph convolution of cross-domain residual to extract more precise and informative spatiotemporal feature, and reduce the training complexity by feature fusion in the, so-called, spatial residual layer. Secondly, instead of simply superimposing multiple similar layers, we use dense connection to take full advantage of the global information. Thirdly, we combine the above mentioned two components to create a spatial temporal graph convolutional network (ST-GCN), referred to as SDGCN. The proposed graph representation has a new structure. We perform extensive experiments on two large datasets: Kinetics and NTU-RGB+D. Our method achieves a great improvement in performance compared to the mainstream methods. We evaluate our method quantitatively and qualitatively, thus proving its effectiveness.\r",
    "code_link": ""
  },
  "iccv2019_sgrl_detectingvisualrelationshipsusingboxattention": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "Detecting Visual Relationships Using Box Attention",
    "authors": [
      "Alexander Kolesnikov",
      "Alina Kuznetsova",
      "Christoph Lampert",
      "Vittorio Ferrari"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Kolesnikov_Detecting_Visual_Relationships_Using_Box_Attention_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Kolesnikov_Detecting_Visual_Relationships_Using_Box_Attention_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a new model for detecting visual relationships, such as \"person riding motorcycle\" or \"bottle on table\". This task is an important step towards comprehensive structured mage understanding, going beyond detecting individual objects. Our main novelty is a Box Attention mechanism that allows to model pairwise interactions between objects using standard object detection pipelines. The resulting model is conceptually clean, expressive and relies on well-justified training and prediction procedures. Moreover, unlike previously proposed approaches, our model does not introduce any additional complex components or hyperparameters on top of those already required by the underlying detection model. We conduct an experimental evaluation on two datasets, V-COCO and Open Images, demonstrating strong quantitative and qualitative results.\r",
    "code_link": "https://github.com/s-gupta/v-coco"
  },
  "iccv2019_sgrl_attention-translation-relationnetworkforscalablescenegraphgeneration": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "Attention-Translation-Relation Network for Scalable Scene Graph Generation",
    "authors": [
      "Nikolaos Gkanatsios",
      "Vassilis Pitsikalis",
      "Petros Koutras",
      "Petros Maragos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Gkanatsios_Attention-Translation-Relation_Network_for_Scalable_Scene_Graph_Generation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Gkanatsios_Attention-Translation-Relation_Network_for_Scalable_Scene_Graph_Generation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We find that most Scene Graph Generation approaches suffer from two limitations as they: 1) use generic attention mechanisms and dataset-specific statistics that supersede visual features and 2) treat \"no interaction\" as an extra, both noisy and dominant, class and prune graph edges manually or applying simple filters. As a result, such approaches do not scale up on different settings and specifications. We propose a three-stage pipeline that employs Multi-Head Attention driven by language and spatial features, Translation Embeddings and Multi-Tasking to detect an interacting pair of objects. Our attentional scheme is able to maximize the visual features' interpretability, as well as to capture the nature of datasets of different scales, while multi-tasking robustly resolves the bias of the background class. We present an experimental overview of the related literature, unveil a multitude of evaluation inconsistencies and provide quantitative and qualitative support with experiments on a variety of datasets, where our approach performs on par or even outperforms current state-of-the-art.\r",
    "code_link": "https://github.com/deeplab-ai/atr-net"
  },
  "iccv2019_sgrl_synthrel0towardsadiagnosticdatasetforrelationalrepresentationlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "SynthRel0: Towards a Diagnostic Dataset for Relational Representation Learning",
    "authors": [
      "Daniel Dorda",
      "Moin Nabi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Dorda_SynthRel0_Towards_a_Diagnostic_Dataset_for_Relational_Representation_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Dorda_SynthRel0_Towards_a_Diagnostic_Dataset_for_Relational_Representation_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This work analyses the sources of complexity in scene graph proposal problems, and develops a mathematical framework for efficiently designing synthetic relationship models. An entropy based metric is proposed for measuring the ambiguity of relational datasets. Using these tools, a first approximation to a synthetic dataset is given, and experiments with a simple baseline are performed to show how the difficulty of the proposed task changes with varying dataset parameters, like missing annotation ratio and feature granularity. These experiments illuminate the desirable qualities of future synthetic relationship datasets.\r",
    "code_link": ""
  },
  "iccv2019_sgrl_scenegraphpredictionwithlimitedlabels": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "Scene Graph Prediction with Limited Labels",
    "authors": [
      "Vincent Chen",
      "Paroma Varma",
      "Ranjay Krishna",
      "Michael Bernstein",
      "Christophe Re",
      "Li Fei Fei"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Chen_Scene_Graph_Prediction_with_Limited_Labels_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Chen_Scene_Graph_Prediction_with_Limited_Labels_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Visual knowledge bases such as Visual Genome power numerous applications in computer vision, including visual question answering and captioning, but suffer from sparse, incomplete relationships. All scene graph models to date are limited to training on a small set of visual relationships that have thousands of training labels each. Hiring human annotators is expensive, and using textual knowledge base completion methods are incompatible with visual data. In this paper, we introduce a semi-supervised method that assigns probabilistic relationship labels to a large number of unlabeled images using few labeled examples. We analyze visual relationships to suggest two types of image-agnostic features that are used to generate noisy heuristics, whose outputs are aggregated using a factor graph-based generative model. With as few as 10 labeled examples per relationship, the generative model creates enough training data to train any existing state-of-the-art scene graph model. We demonstrate that our method outperforms all baseline approaches on scene graph prediction by5.16 recall@100 for PREDCLS. In our limited label setting, we define a complexity metric for relationships that serves as an indicator (R^2 = 0.778) for conditions under which our method succeeds over transfer learning, the de-facto approach for training with limited labels.\r",
    "code_link": ""
  },
  "iccv2019_sgrl_triplet-awarescenegraphembeddings": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "Triplet-Aware Scene Graph Embeddings",
    "authors": [
      "Brigit Schroeder",
      "Subarna Tripathi",
      "Hanlin Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Schroeder_Triplet-Aware_Scene_Graph_Embeddings_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Schroeder_Triplet-Aware_Scene_Graph_Embeddings_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Scene graphs have become an important form of structured knowledge for tasks such as visual relation detection, visual question answering, and image retrieval. While visualizing and interpreting word embeddings is well understood, scene graph embeddings have not been fully explored. In this work, we train scene graph embeddings in a layout generation task with varying forms of supervision, specifically introducing triplet supervision and data augmentation. We see a significant performance increase in both metrics that measure the goodness of layout prediction, mean intersection-over-union (mIoU) (52.3% vs. 49.2%) and relation score (61.7% vs. 54.1%), after the addition of triplet supervision and data augmentation. To understand how these different methods effect the scene graph representation, we apply several new visualization and evaluation methods to explore the evolution of the scene graph embedding. We find that triplet supervision significantly improves the embedding separability, which is highly correlated with performance of the layout prediction model.\r",
    "code_link": ""
  },
  "iccv2019_sgrl_atopologicalgraph-basedrepresentationfordenoisinglowqualitybinaryimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "SGRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Scene Graph Representation and Learning",
    "title": "A Topological Graph-Based Representation for Denoising Low Quality Binary Images",
    "authors": [
      "Catherine Potts",
      "Liping Yang",
      "Diane Oyen",
      "Brendt Wohlberg"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/SGRL/Potts_A_Topological_Graph-Based_Representation_for_Denoising_Low_Quality_Binary_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/SGRL/Potts_A_Topological_Graph-Based_Representation_for_Denoising_Low_Quality_Binary_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Scanned images of patent or historical documents often contain localized zigzag noise introduced by the digitizing process; yet when viewed as a whole image, global structures are apparent to humans, but not to machines. Existing denoising methods work well for natural images, but not for binary diagram images, which makes feature extraction difficult for computer vision and machine learning methods and algorithms. We propose a topological graph-based representation to tackle this denoising problem. The graph representation emphasizes the shapes and topology of diagram images, making it ideal for use in machine learning applications such as classification and matching of scientific diagram images. Our approach and algorithms provide essential structure and lay important foundation for computer vision such as scene graph-based applications, because topological relations and spatial arrangement among objects in images are captured and stored in our skeleton graph. In addition, while the parameters for almost all pixel-based methods are not adaptive, our method is robust in that it only requires one parameter and it is adaptive. Experimental comparisons with existing methods show the effectiveness of our approach.\r",
    "code_link": ""
  },
  "iccv2019_wider_cross-modalpersonsearchacoarse-to-fineframeworkusingbi-directionaltext-imagematching": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "Cross-Modal Person Search: A Coarse-to-Fine Framework using Bi-Directional Text-Image Matching",
    "authors": [
      "Xiaojing Yu",
      "Tianlong Chen",
      "Yang Yang",
      "Michael Mugo",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Yu_Cross-Modal_Person_Search_A_Coarse-to-Fine_Framework_using_Bi-Directional_Text-Image_Matching_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Yu_Cross-Modal_Person_Search_A_Coarse-to-Fine_Framework_using_Bi-Directional_Text-Image_Matching_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Searching person images from a gallery based on natural language descriptions remains to be a challenging and under-explored cross-modal retrieval problem. To improve the accuracy off an image-based retrieval task, e.g., person re-identification (Person Re-Id), re-ranking is known to be an effective post-processing tool. In this paper, we extend re-ranking from uni-modal retrieval to cross-modal retrieval for the first time, and develop a bi-directional coarse-to-fine framework (BCF) for cross-modal person search. Built on a recent state-of-the-art Person Re-Id model, BCF exploits first text-to-image and then image-to-text relevance, in a two-stage refinement fashion. BCF ranks competitively against a strong baseline on the newly-introduced WIDER Person Search dataset, boosting validation set performance by 9.01%(top-1)/3.87%(mAP) for val1 and 6.60%(top-1)/3.49%(mAP) for val2, respectively. With a high score, our solution ranks competitively in the ICCV 2019 WIDER Person Search by Language Challenge.\r",
    "code_link": ""
  },
  "iccv2019_wider_fusingtwodirectionsincross-domainadaptionforreallifepersonsearchbylanguage": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "Fusing Two Directions in Cross-Domain Adaption for Real Life Person Search by Language",
    "authors": [
      "Kai Niu",
      "Yan Huang",
      "Liang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Niu_Fusing_Two_Directions_in_Cross-Domain_Adaption_for_Real_Life_Person_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Niu_Fusing_Two_Directions_in_Cross-Domain_Adaption_for_Real_Life_Person_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Person search by language is an important application in video surveillance. The existing huge visual-semantic discrepancy and the cross-domain difficulty of emerging pedestrian images with new identities while no language description for training in real life application make this problem non-trivial to be addressed. In this paper, we first propose a concise and effective framework for image-sentence alignment to deal with the visual-semantic discrepancy. Second, we innovatively fuse the two opposite directions, i.e., source to target and target to source, for cross-domain adaption. Extensive experiments have validated the significant superiority of the proposed method on both source domain and target domain, and we have obtained the state-of-the-art performance and won the 1st place in competition.\r",
    "code_link": ""
  },
  "iccv2019_wider_whatfaceandbodyshapescantellusaboutheight": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "What Face and Body Shapes Can Tell Us About Height",
    "authors": [
      "Semih Gunel",
      "Helge Rhodin",
      "Pascal Fua"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Gunel_What_Face_and_Body_Shapes_Can_Tell_Us_About_Height_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Gunel_What_Face_and_Body_Shapes_Can_Tell_Us_About_Height_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recovering a person's height from a single image is important for virtual garment fitting, autonomous driving and surveillance. However, it is also very challenging without absolute scale information. Here, we examine the rarely addressed case, where camera parameters and scene geometry are all unknown. Under this circumstances, scale is inherently ambiguous, and height can only be inferred from those statistics that are intrinsic to human anatomy and can be estimated from images directly, such as articulated pose, bone-length proportions, and facial features. Our contribution is twofold. First, we create a new human-height dataset that is three magnitudes larger than existing ones, by mining explicit height labels and propagating them to additional images through face recognition and assignment consistency. Second, we test a wide range of machine learning models (linear, shallow, and deep models) to capture the relation between image content and human height. We also show that performance is predominantly limited by dataset size. Our central finding is that height can only be estimated with large uncertainty. The remaining high variance demonstrates that the geometrically motivated scale ambiguity persists into the age of deep learning, which has important implications for how to pose monocular reconstruction, such as 3D human pose estimation, in a scale invariant way.\r",
    "code_link": ""
  },
  "iccv2019_wider_bayesiangait-basedgenderidentification(bggi)networkonindividualswearinglooselyfittedclothing": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "Bayesian Gait-Based Gender Identification (BGGI) Network on Individuals Wearing Loosely Fitted Clothing",
    "authors": [
      "Amarjot Singh",
      "Aman Kumar",
      "Anisha Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Singh_Bayesian_Gait-Based_Gender_Identification_BGGI_Network_on_Individuals_Wearing_Loosely_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Singh_Bayesian_Gait-Based_Gender_Identification_BGGI_Network_on_Individuals_Wearing_Loosely_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Suspicious individuals often attempt to hide their identity to avoid detection by safety and security systems. Wearing clothes of the opposite gender is one of the several techniques used by these individuals. Several promising attempts have been made to recognize gender using gait recognition. However, these systems only focused on recognizing gender for individuals who wore tightly fitting attire which made it easier to detect the body joints further making it possible to differentiate both genders. In this work, we attempt to solve a challenging real-world problem faced by security agencies in which the individuals mask their identity by wearing loosely fitted clothes (LFC) of the opposite gender. LFC makes it difficult to locate the body joints in effect making the gender classification, in this situation, a complicated problem. We propose a Bayesian Gait-based Gender Identification (BGGI) technique that is used for gender recognition in LFC conditions, in dense real-world videos. This research releases the loosely fitted clothes individuals (LFCI) dataset used for training the deep network. This may encourage researchers interested in using deep learning for this task. The pose estimation and gender recognition achieve great performance with state-of-the-art techniques.\r",
    "code_link": ""
  },
  "iccv2019_wider_single-stagejointfacedetectionandalignment": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "Single-Stage Joint Face Detection and Alignment",
    "authors": [
      "Jiankang Deng",
      "Jia Guo",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Deng_Single-Stage_Joint_Face_Detection_and_Alignment_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Deng_Single-Stage_Joint_Face_Detection_and_Alignment_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In practice, there are huge demands to localize faces in images and videos under unconstrained pose variation, illumination change, severe occlusion and low resolution, which pose a great challenge to existing face detectors. This challenge report presents a single-stage joint face detection and alignment method. In detail, we employ feature pyramid network, single-stage detection, context modelling, multi-task learning and cascade regression to construct a practical face detector. On the Wider Face Hard validation subset, our single model achieves state-of-the-art performance (92.0% AP) compared with both academic and commercial face detectors for detecting unconstrained faces in cluttered scenes. In the Wider Face AND PERSON CHALLENGE 2019, our ensemble model achieves 56.66% average AP (runner-up) in the face detection track. To facilitate further research on the topic, the training code and models have been provided publicly available.\r",
    "code_link": ""
  },
  "iccv2019_wider_adensenetbasedrobustfacedetectionframework": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "A Densenet Based Robust Face Detection Framework",
    "authors": [
      "Abhilash Nandy"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Nandy_A_Densenet_Based_Robust_Face_Detection_Framework_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Nandy_A_Densenet_Based_Robust_Face_Detection_Framework_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Face Detection has become important in various real-life applications such as face recognition, kinship verification, video surveillance, sentiment analysis using videos, etc. There has been significant progress in this field in recent years, thanks to the evolution of deep convolutional neural networks (CNNs). Images taken in real-world scenarios vary a lot in various aspects such as lighting, scale, pose, etc. WIDER FACE dataset contains such images, and is hence, quite challenging. In this paper, we propose a solution which takes the DSFD (Dual Shot Face Detector) as a baseline network, and we apply some tweaks to the network to improve performance with lesser memory usage and inference time. Specifically, we use a Densenet backbone, use focal loss function for classification, a function of IoU (Intersection over Union) metric as a regression loss function, and lastly, use the max-out operation before predicting class probabilities. Consequently, the proposed solution achieves state-of-the-art performance on the WIDER FACE Dataset, with added advantages of being more scalable and taking lesser time to infer than its original DSFD baseline. Also, it gives better face detection performance than many other state-of-the-art face detection frameworks.\r",
    "code_link": ""
  },
  "iccv2019_wider_asemi-supervisedmaximummarginmetriclearningapproachforsmallscalepersonre-identification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "A Semi-Supervised Maximum Margin Metric Learning Approach for Small Scale Person Re-Identification",
    "authors": [
      "T M Feroz Ali",
      "Subhasis Chaudhuri"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Ali_A_Semi-Supervised_Maximum_Margin_Metric_Learning_Approach_for_Small_Scale_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Ali_A_Semi-Supervised_Maximum_Margin_Metric_Learning_Approach_for_Small_Scale_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In video surveillance, person re-identification is the task of searching person images in non-overlapping cameras. Though supervised methods for person re-identification have attained impressive performance, obtaining large scale cross-view labeled training data is very expensive. However, unlabelled data is available in abundance. In this paper, we propose a semi-supervised metric learning approach that can utilize information in unlabelled data with the help of a few labelled training samples. We also address the small sample size problem that inherently occurs due to the few labeled training data. Our method learns a discriminative space where within class samples collapse to singular points, achieving the least within class variance, and then use a maximum margin criterion over a high dimensional kernel space to maximally separate the distinct class samples. A maximum margin criterion with two levels of high dimensional mappings to kernel space is used to obtain better cross-view discrimination of the identities. Cross-view affinity learning with reciprocal nearest neighbor constraints is used to mine new pseudo-classes from the unlabelled data and update the distance metric iteratively. We attain state-of-the-art performance on four challenging datasets with a large margin.\r",
    "code_link": ""
  },
  "iccv2019_wider_castsearchviatwo-streamlabelpropagation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "WIDER",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - WIDER Face and Person Challenge",
    "title": "Cast Search via Two-Stream Label Propagation",
    "authors": [
      "Jhih-Ciang Wu",
      "Bing-Jhang Lin",
      "Bing-Yuan Zeng",
      "Li-Chen Fu",
      "Chiou-Shann Fuh",
      "Tyng-Luh Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/WIDER/Wu_Cast_Search_via_Two-Stream_Label_Propagation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/WIDER/Wu_Cast_Search_via_Two-Stream_Label_Propagation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We address the problem of Cast Search by Portraits (CSP) where a facial portrait of a cast member is provided to retrieve from a given video clip those frames containing the query target. The underlying CSP formulation is related to the task of person re-identification. However, CSP is more challenging in that the provided query image is only a portrait of a certain cast member, and the instances of the target in the candidate video could have a very different visual appearance. Such drastic visual variations are not common in addressing the problem of person re-id. We propose a two-stream network architecture for tackling the CSP challenge and also participate in the public CSP competition. The overall outcome in the competition is promising and worth further effort to improve our proposed model.\r",
    "code_link": ""
  },
  "iccv2019_viral_unsupervisedteacher-studentmodelforlarge-scalevideoretrieval": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ViRaL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Video Retrieval Methods and Their Limitations",
    "title": "Unsupervised Teacher-Student Model for Large-Scale Video Retrieval",
    "authors": [
      "Dong Liang",
      "Lanfen Lin",
      "Rui Wang",
      "Jie Shao",
      "Changhu Wang",
      "Yei-Wei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ViRaL/Liang_Unsupervised_Teacher-Student_Model_for_Large-Scale_Video_Retrieval_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ViRaL/Liang_Unsupervised_Teacher-Student_Model_for_Large-Scale_Video_Retrieval_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the growth of video-sharing platforms and social media applications, video retrieval plays an import role in many aspects, such as copyright infringement detection, event classification, personalized recommendation, and etc. The content-based video retrieval presents the following two main challenges: (i) Distribution inconsistency for feature representation from the source domain to the target domain. (ii) Difficulty of video aggregation by sufficiently incorporating frame-based information. In this paper, we propose an unsupervised teacher-student model (UTS Net) to improve the performance of the content-based video retrieval tasks: (i) A teacher-student model maintaining the global consistency for feature representation from different domains and retaining the local inconsistency within the intra-batch data; (ii) A simple but effective video retrieval pipeline integrating the frame-level binarized feature. Our proposed framework experimentally outperforms the state-of-the-art approach on the DSVR, CSVR, and ISVR tasks in the FIVR datasets, and achieves a mean average precision of 76%, 72%, and 61%, respectively.\r",
    "code_link": ""
  },
  "iccv2019_viral_fusionofmultimodalembeddingsforad-hocvideosearch": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ViRaL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Video Retrieval Methods and Their Limitations",
    "title": "Fusion of Multimodal Embeddings for Ad-Hoc Video Search",
    "authors": [
      "Danny Francis",
      "Phuong Anh Nguyen",
      "Benoit Huet",
      "Chong-Wah Ngo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ViRaL/Francis_Fusion_of_Multimodal_Embeddings_for_Ad-Hoc_Video_Search_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ViRaL/Francis_Fusion_of_Multimodal_Embeddings_for_Ad-Hoc_Video_Search_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The challenge of Ad-Hoc Video Search (AVS) originates from free-form (i.e., no pre-defined vocabulary) and free-style (i.e., natural language) query description. Bridging the semantic gap between AVS queries and videos becomes highly difficult as evidenced from the low retrieval accuracy of AVS benchmarking in TRECVID. In this paper, we study a new method to fuse multimodal embeddings which have been derived based on completely disjoint datasets. This method is tested on two datasets for two distinct tasks: on MSR-VTT for unique video retrieval and on V3C1 for multiple videos retrieval.\r",
    "code_link": ""
  },
  "iccv2019_viral_instance-basedvideosearchviamulti-taskretrievalandre-ranking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ViRaL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Video Retrieval Methods and Their Limitations",
    "title": "Instance-Based Video Search via Multi-Task Retrieval and Re-Ranking",
    "authors": [
      "Zhicheng Zhao",
      "Guanyu Chen",
      "Chong Chen",
      "Xinyu Li",
      "Xuanlu Xiang",
      "Yanyun Zhao",
      "Fei Su"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ViRaL/Zhao_Instance-Based_Video_Search_via_Multi-Task_Retrieval_and_Re-Ranking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ViRaL/Zhao_Instance-Based_Video_Search_via_Multi-Task_Retrieval_and_Re-Ranking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the rapid growth of video data, instance-based video search (INS), i.e., retrieving videos according to specific objects, places, actions etc., has become more and more practical and important. In this paper, a novel INS framework based on multi-task retrieval and re-ranking is proposed to retrieve particular person doing specific action. Firstly, a face matching scheme is designed to match the target persons from videos. Secondly, an object detection network and an improved two-pathway key-pose estimation network (IECO) are introduced to explore semantic depen-dences between static visual object and person's behavior. Based on the dependences, an initial INS ranklist is obtained. Thirdly, via encoding absolute and relative positions of person's poses, a new relative pose representation (RPR) method is presented. Finally, regarding RPR as the input, a light action recognition network is constructed to re-rank INS results. The experimental results on HMDB, UCF101, JHMDB and BBC Eastenders datasets demonstrate the effectiveness of the proposed INS framework.\r",
    "code_link": ""
  },
  "iccv2019_clvl_areweaskingtherightquestionsinmovieqa?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Closing the Loop Between Vision and Language",
    "title": "Are we Asking the Right Questions in MovieQA?",
    "authors": [
      "Bhavan Jasani",
      "Rohit Girdhar",
      "Deva Ramanan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CLVL/Jasani_Are_we_Asking_the_Right_Questions_in_MovieQA_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CLVL/Jasani_Are_we_Asking_the_Right_Questions_in_MovieQA_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Joint vision and language tasks like visual question answering are fascinating because they explore high-level understanding, but at the same time, can be more prone to language biases. In this paper, we explore the biases in the MovieQA dataset and propose a strikingly simple model which can exploit them. We find that using the right word embedding is of utmost importance. By using an appropriately-trained word embedding, about half the Question-Answers (QAs) can be answered by looking at the questions and answers alone, completely ignoring narrative context from video clips, subtitles, and movie scripts. Compared to the best published papers on the leaderboard, our simple question+answer only model improves accuracy by 5% for video + subtitle category, 5% for subtitle, 15% for DVS and 6% higher for scripts.\r",
    "code_link": ""
  },
  "iccv2019_clvl_sun-spotanrgb-ddatasetwithspatialreferringexpressions": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Closing the Loop Between Vision and Language",
    "title": "SUN-Spot: An RGB-D Dataset With Spatial Referring Expressions",
    "authors": [
      "Cecilia Mauceri",
      "Martha Palmer",
      "Christoffer Heckman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CLVL/Mauceri_SUN-Spot_An_RGB-D_Dataset_With_Spatial_Referring_Expressions_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CLVL/Mauceri_SUN-Spot_An_RGB-D_Dataset_With_Spatial_Referring_Expressions_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We introduce a new dataset, SUN-Spot, for localizing objects using spatial referring expressions (REs). SUN-Spot is the only RE dataset which uses RGB-D images. It also contains a greater average number of spatial prepositions and more cluttered scenes than previous RE datasets. Using a simple baseline, we show that including a depth channel in RE models can improve performance on both generation and comprehension.\r",
    "code_link": ""
  },
  "iccv2019_clvl_evaluatingtext-to-imagematchingusingbinaryimageselection(bison)": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CLVL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Closing the Loop Between Vision and Language",
    "title": "Evaluating Text-to-Image Matching using Binary Image Selection (BISON)",
    "authors": [
      "Hexiang Hu",
      "Ishan Misra",
      "Laurens van der Maaten"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CLVL/Hu_Evaluating_Text-to-Image_Matching_using_Binary_Image_Selection_BISON_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CLVL/Hu_Evaluating_Text-to-Image_Matching_using_Binary_Image_Selection_BISON_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Providing systems the ability to relate linguistic and visual content is one of the hallmarks of computer vision. Tasks such as text-based image retrieval and image captioning were designed to test this ability, but come with evaluation measures that have high variance or are difficult to interpret. We study an alternative task for systems that match text and images: given a text query, the system is asked to select the image that best matches the query from a pair of semantically similar images. The system's accuracy on this Binary Image SelectiON (BISON) task provides a robust and interpretable measure of its ability to match linguistic content with fine-grained visual structure. We gather a BISON dataset that complements the COCO dataset and use it to evaluate modern text-based image retrieval systems.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_eenaefficientevolutionofneuralarchitecture": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "EENA: Efficient Evolution of Neural Architecture",
    "authors": [
      "Hui Zhu",
      "Zhulin An",
      "Chuanguang Yang",
      "Kaiqiang Xu",
      "Erhu Zhao",
      "Yongjun Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Zhu_EENA_Efficient_Evolution_of_Neural_Architecture_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Zhu_EENA_Efficient_Evolution_of_Neural_Architecture_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Latest algorithms for automatic neural architecture search perform remarkable but are basically directionless in search space and computational expensive in the training of every intermediate architecture. In this paper, we propose a method for efficient architecture search called EENA (Efficient Evolution of Neural Architecture). Due to the elaborately designed mutation and crossover operations, the evolution process can be guided by the information have already been learned. Therefore, less computational effort will be required while the searching and training time can be reduced significantly. On CIFAR-10 classification, EENA using minimal computational resources (0.65 GPU-days) can design highly effective neural architecture which achieves 2.56% test error with 8.47M parameters. Furthermore, the best architecture discovered is also transferable for CIFAR-100.\r",
    "code_link": "https://github.com/ICCV-5-EENA/EENA"
  },
  "iccv2019_neurarch_sequentiallyaggregatedconvolutionalnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Sequentially Aggregated Convolutional Networks",
    "authors": [
      "Yiwen Huang",
      "Pinglai Ou",
      "Rihui Wu",
      "Ziyong Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Huang_Sequentially_Aggregated_Convolutional_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Huang_Sequentially_Aggregated_Convolutional_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Modern deep networks generally implement a certain form of shortcut connections to alleviate optimization difficulties. However, we observe that such network topology alters the nature of deep networks. In many ways, these networks behave similarly to aggregated wide networks. We thus exploit the aggregation nature of shortcut connections at a finer architectural level and place them within wide convolutional layers. We end up with a sequentially aggregated convolutional (SeqConv) layer that combines the benefits of both wide and deep representations by aggregating features of various depths in sequence. The proposed SeqConv serves as a drop-in replacement of regular wide convolutional layers and thus could be handily integrated into any backbone network. We apply SeqConv to widely adopted backbones including ResNet and ResNeXt, and conduct experiments for image classification on public benchmark datasets. Our ResNet based network with a model size of ResNet-50 easily surpasses the performance of the 2.35x larger ResNet-152, while our ResNeXt based model sets a new state-of-the-art accuracy on ImageNet classification for networks with similar model complexity. The code and pre-trained models of our work are publicly available at https://github.com/GroupOfAlchemists/SeqConv.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_resourceefficient3dconvolutionalneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Resource Efficient 3D Convolutional Neural Networks",
    "authors": [
      "Okan Kopuklu",
      "Neslihan Kose",
      "Ahmet Gunduz",
      "Gerhard Rigoll"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Kopuklu_Resource_Efficient_3D_Convolutional_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Kopuklu_Resource_Efficient_3D_Convolutional_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently, convolutional neural networks with 3D kernels (3D CNNs) have been very popular in computer vision community as a result of their superior ability of extracting spatio-temporal features within video frames compared to 2D CNNs. Although there has been great advances recently to build resource efficient 2D CNN architectures considering memory and power budget, there is hardly any similar resource efficient architectures for 3D CNNs. In this paper, we have converted various well-known resource efficient 2D CNNs to 3D CNNs and evaluated their performance on three major benchmarks in terms of classification accuracy for different complexity levels. We have experimented on (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester dataset to inspect their ability to capture motion patterns, and (3) UCF-101 to inspect the applicability of transfer learning. We have evaluated the run-time performance of each model on a single Titan XP GPU and a Jetson TX2 embedded system. The results of this study show that these models can be utilized for different types of real-world applications since they provide real-time performance with considerable accuracies and memory usage. Our analysis on different complexity levels shows that the resource efficient 3D CNNs should not be designed too shallow or narrow in order to save complexity. The codes and pretrained models used in this work are publicly available.\r",
    "code_link": "https://github.com/okankop/Efficient-3DCNNs"
  },
  "iccv2019_neurarch_cross-granularityattentionnetworkforsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Cross-Granularity Attention Network for Semantic Segmentation",
    "authors": [
      "Lingyu Zhu",
      "Tinghuai Wang",
      "Emre Aksu",
      "Joni-Kristian Kamarainen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Zhu_Cross-Granularity_Attention_Network_for_Semantic_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Zhu_Cross-Granularity_Attention_Network_for_Semantic_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Despite the remarkable progress of semantic segmentation in recent years, much remains to be addressed in order to achieve better semantic coherence and boundary delineation. In this paper, we propose a novel convolutional neural network (CNN) architecture for semantic segmentation which explicitly addresses these two issues. Specifically, we propose a categorical attention mechanism to propagate consistent category-oriented information across multi-granularity contextual interpretations to close the semantic gap residing in CNN feature hierarchy. This novel design alleviates the semantic information loss during the feature combination and transformation process in decoder network. We further integrate a contour branch in our architecture to enhance the boundary awareness of the semantic feature derived in the form of a novel element-wise contour attention at each level of feature hierarchy. Additionally, we introduce a cross-granularity contour enhancement mechanism to propagate rich boundary cues from early layers to deep layers. We perform extensive quantitative evaluations in close proximity to object boundaries which confirms its superior effectiveness in boundary delineation. These novel mechanisms which boost the essentials in segmentation, i.e., region-wise semantic coherence and accurate object contour localization, allow our architecture \"MeshNet\" to obtain state-of-the-art performance on two challenging datasets, i.e., PASCAL VOC 2012 and Cityscapes.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_understandingtheeffectsofpre-trainingforobjectdetectorsviaeigenspectrum": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Understanding the Effects of Pre-Training for Object Detectors via Eigenspectrum",
    "authors": [
      "Yosuke Shinya",
      "Edgar Simo-Serra",
      "Taiji Suzuki"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Shinya_Understanding_the_Effects_of_Pre-Training_for_Object_Detectors_via_Eigenspectrum_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Shinya_Understanding_the_Effects_of_Pre-Training_for_Object_Detectors_via_Eigenspectrum_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " ImageNet pre-training has been regarded as essential for training accurate object detectors for a long time. Recently, it has been shown that object detectors trained from randomly initialized weights can be on par with those fine-tuned from ImageNet pre-trained models. However, the effects of pre-training and the differences caused by pre-training are still not fully understood. In this paper, we analyze the eigenspectrum dynamics of the covariance matrix of each feature map in object detectors. Based on our analysis on ResNet-50, Faster R-CNN with FPN, and Mask R-CNN, we show that object detectors trained from ImageNet pre-trained models and those trained from scratch behave differently from each other even if both object detectors have similar accuracy. Furthermore, we propose a method for automatically determining the widths (the numbers of channels) of object detectors based on the eigenspectrum. We train Faster R-CNN with FPN from randomly initialized weights, and show that our method can reduce27% of the parameters of ResNet-50 without increasing Multiply-Accumulate operations and losing accuracy. Our results indicate that we should develop more appropriate methods for transferring knowledge from image classification to object detection (or other tasks).\r",
    "code_link": "https://github.com/roytseng-tw/Detectron"
  },
  "iccv2019_neurarch_hm-nasefficientneuralarchitecturesearchviahierarchicalmasking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking",
    "authors": [
      "Shen Yan",
      "Biyi Fang",
      "Faen Zhang",
      "Yu Zheng",
      "Xiao Zeng",
      "Mi Zhang",
      "Hui Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Yan_HM-NAS_Efficient_Neural_Architecture_Search_via_Hierarchical_Masking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Yan_HM-NAS_Efficient_Neural_Architecture_Search_via_Hierarchical_Masking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The use of automatic methods, often referred to as Neural Architecture Search (NAS), in designing neural network architectures has recently drawn considerable attention. In this work, we present an efficient NAS approach, named HM-NAS, that generalizes existing weight sharing based NAS approaches. Existing weight sharing based NAS approaches still adopt hand designed heuristics to generate architecture candidates. As a consequence, the space of architecture candidates is constrained in a subset of all possible architectures, making the architecture search results sub-optimal. HM-NAS addresses this limitation via two innovations. First, HM-NAS incorporates a multi-level architecture encoding scheme to enable searching for more flexible network architectures. Second, it discards the hand designed heuristics and incorporates a hierarchical masking scheme that automatically learns and determines the optimal architecture. Compared to state-of-the-art weight sharing based approaches, HM-NAS is able to achieve better architecture search performance and competitive model evaluation accuracy. Without the constraint imposed by the hand designed heuristics, our searched networks contain more flexible and meaningful architectures that existing weight sharing based NAS approaches are not able to discover.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_binarydensenetdevelopinganarchitectureforbinaryneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "BinaryDenseNet: Developing an Architecture for Binary Neural Networks",
    "authors": [
      "Joseph Bethge",
      "Haojin Yang",
      "Marvin Bornstein",
      "Christoph Meinel"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Bethge_BinaryDenseNet_Developing_an_Architecture_for_Binary_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Bethge_BinaryDenseNet_Developing_an_Architecture_for_Binary_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Binary Neural Networks (BNNs) show promising progress in reducing computational and memory costs, but suffer from substantial accuracy degradation compared to their real-valued counterparts on large-scale datasets, e.g., ImageNet. In this work we study existing BNN architectures and revisit the commonly used technique to include scaling factors. We suggest several architectural design principles for BNNs, based on our studies on architectures. Guided by our principles we develop a novel BNN architecture BinaryDenseNet, which is the first architecture specifically created for BNNs to the best of our knowledge. In our experiments, BinaryDenseNet achieves 18.6% and 7.6% relative improvement over the well-known XNOR-Network and the current state-of-the-art Bi-Real Net in terms of top-1 accuracy on ImageNet, respectively. Further, we show the competitiveness of our BinaryDenseNet regarding memory requirements and computational complexity.\r",
    "code_link": "https://github.com/hpi-xnor/BMXNet-v2"
  },
  "iccv2019_neurarch_efficientstructuredpruningandarchitecturesearchingforgroupconvolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Efficient Structured Pruning and Architecture Searching for Group Convolution",
    "authors": [
      "Ruizhe Zhao",
      "Wayne Luk"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Zhao_Efficient_Structured_Pruning_and_Architecture_Searching_for_Group_Convolution_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Zhao_Efficient_Structured_Pruning_and_Architecture_Searching_for_Group_Convolution_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Efficient inference of Convolutional Neural Networks is a thriving topic recently. It is desirable to achieve the maximal test accuracy under given inference budget constraints when deploying a pre-trained model. Network pruning is a commonly used technique but it may produce irregular sparse models that can hardly gain actual speed-up. Group convolution is a promising pruning target due to its regular structure; however, incorporating such structure into the pruning procedure is challenging. It is because structural constraints are hard to describe and can make pruning intractable to solve. The need for configuring group convolution architecture, i.e., the number of groups, to maximise test accuracy also increases difficulty. This paper presents an efficient method to address this challenge. We formulate group convolution pruning as finding the optimal channel permutation to impose structural constraints and solve it efficiently by heuristics. We also apply local search to exploring group configuration based on estimated pruning cost to maximise test accuracy. Compared to prior work, results show that our method produces competitive group convolution models for various tasks within a shorter pruning period and enables rapid group configuration exploration subject to inference budget constraints.\r",
    "code_link": "https://github.com/kumasento/gconv-prune"
  },
  "iccv2019_neurarch_gcnetnon-localnetworksmeetsqueeze-excitationnetworksandbeyond": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond",
    "authors": [
      "Yue Cao",
      "Jiarui Xu",
      "Stephen Lin",
      "Fangyun Wei",
      "Han Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_attentionroutingbetweencapsules": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Attention Routing Between Capsules",
    "authors": [
      "Jaewoong Choi",
      "Hyun Seo",
      "Suii Im",
      "Myungjoo Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Choi_Attention_Routing_Between_Capsules_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Choi_Attention_Routing_Between_Capsules_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a new capsule network architecture called Attention Routing CapsuleNet (AR CapsNet). We replace the dynamic routing and squash activation function of the capsule network with dynamic routing (CapsuleNet) with the attention routing and capsule activation. The attention routing is a routing between capsules through an attention module. The attention routing is a fast forward-pass while keeping spatial information. On the other hand, the intuitive interpretation of the dynamic routing is finding a centroid of the prediction capsules. Thus, the squash activation function and its variant focus on preserving a vector orientation. However, the capsule activation focuses on performing a capsule-scale activation function. We evaluate our proposed model on the MNIST, affNIST, and CIFAR-10 classification tasks. The proposed model achieves higher accuracy with fewer parameters (x0.65 in the MNIST, x0.82 in the CIFAR-10) and less training time than CapsuleNet (x0.19 in the MNIST, x0.35 in the CIFAR-10). These results validate that designing a capsule-scale operation is a key factor to implement the capsule concept. Also, our experiment shows that our proposed model is transformation equivariant as CapsuleNet. As we perturb each element of the output capsule, the decoder attached to the output capsules shows global variations. Further experiments show that the difference in the capsule features caused by applying affine transformations on an input image is significantly aligned in one direction.\r",
    "code_link": "https://github.com/XifengGuo/CapsNet-Keras"
  },
  "iccv2019_neurarch_4-connectedshiftresidualnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "4-Connected Shift Residual Networks",
    "authors": [
      "Andrew Brown",
      "Pascal Mettes",
      "Marcel Worring"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Brown_4-Connected_Shift_Residual_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Brown_4-Connected_Shift_Residual_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The shift operation was recently introduced as an alternative to spatial convolutions. The operation moves subsets of activations horizontally and/or vertically. Spatial convolutions are then replaced with shift operations followed by point-wise convolutions, significantly reducing computational costs. In this work, we investigate how shifts should best be applied to high accuracy CNNs. We apply shifts of two different neighbourhood groups to ResNet on ImageNet: the originally introduced 8-connected (8C) neighbourhood shift and the less well studied 4-connected (4C) neighbourhood shift. We find that when replacing ResNet's spatial convolutions with shifts, both shift neighbourhoods give equal ImageNet accuracy, showing the sufficiency of small neighbourhoods for large images. Interestingly, when incorporating shifts to all point-wise convolutions in residual networks, 4-connected shifts outperform 8-connected shifts. Such a 4-connected shift setup gives the same accuracy as full residual networks while reducing the number of parameters and FLOPs by over 40%. We then highlight that without spatial convolutions, ResNet's downsampling/upsampling bottleneck channel structure is no longer needed. We show a new, 4C shift-based residual network, much shorter than the original ResNet yet with a higher accuracy for the same computational cost. This network is the highest accuracy shift-based network yet shown, demonstrating the potential of shifting in deep neural networks.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_adaptiveconvolutionalkernels": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Adaptive Convolutional Kernels",
    "authors": [
      "Julio Zamora Esquivel",
      "Adan Cruz Vargas",
      "Paulo Lopez Meyer",
      "Omesh Tickoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Esquivel_Adaptive_Convolutional_Kernels_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Esquivel_Adaptive_Convolutional_Kernels_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The quest for increased computer vision recognition performance has led to the development of high complexity neural network architectures, each time with evolving deeper topologies. To avoid high computing resource requirements of such complex networks, and to enable operation on devices with limited resources, this work introduces the concept of adaptive kernels applied to convolutional layers. Motivated by the non-linear perception response in human visual cells, the input image is used to define the weights of a dynamically changing kernel, named adaptive kernel. This novel adaptive kernel is used to perform a second convolution operation over the input image in order to generate the output features. Adaptive kernels enable accurate recognition with significant lower memory requirements; this is accomplished by reducing the number of kernels and the number of layers needed as compared to typical CNN configurations. Additionally, the use of adaptive kernels allow the decrease by 2X the number of epochs required for training, and the number of activation function computations. Our experimental results show a reduction of 66X of the parameters needed of a CNN compared to LeNet when evaluated with the MNIST dataset, maintaining >99% of accuracy. Additionally, when using adaptive kernels implemented in a ResNet18, we observed a higher performance when compared to a known ResNet100 reported in the literature for CIFAR10 and it also gets better accuracy for ImageNet database.\r",
    "code_link": "https://github.com/adapconv/adaptive-cnn"
  },
  "iccv2019_neurarch_adaptiveactivationfunctionsusingfractionalcalculus": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Adaptive Activation Functions Using Fractional Calculus",
    "authors": [
      "Julio Zamora Esquivel",
      "Adan Cruz Vargas",
      "Rodrigo Camacho Perez",
      "Paulo Lopez Meyer",
      "Hector Cordourier",
      "Omesh Tickoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Esquivel_Adaptive_Activation_Functions_Using_Fractional_Calculus_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Esquivel_Adaptive_Activation_Functions_Using_Fractional_Calculus_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We introduce a generalization methodology for the automatic selection of the activation functions inside a neural network, taking advantage of concepts defined in fractional calculus. This methodology enables the neural network to define and optimize its own activation functions during the training process, by defining the fractional order of the derivative of a given primitive activation function, tuned as an additional training hyper-parameter. By following this approach, the neurons inside the network can adjust their activation functions, e.g. from MLP to RBF networks, to best fit the input data, and reduce the output error. The result show the benefits of using this technique implemented on a ResNet18 topology by outperforming the accuracy of a ResNet100 trained with CIFAR10 reported in the literature.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_squeezenasfastneuralarchitecturesearchforfastersemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "SqueezeNAS: Fast Neural Architecture Search for Faster Semantic Segmentation",
    "authors": [
      "Albert Shaw",
      "Daniel Hunter",
      "Forrest Landola",
      "Sammy Sidhu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Shaw_SqueezeNAS_Fast_Neural_Architecture_Search_for_Faster_Semantic_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Shaw_SqueezeNAS_Fast_Neural_Architecture_Search_for_Faster_Semantic_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " For real time applications utilizing Deep Neural Networks (DNNs), it is critical that the models achieve high-accuracy on the target task and low-latency inference on the target computing platform. While Neural Architecture Search (NAS) has been effectively used to develop low-latency networks for image classification, there has been relatively little effort to use NAS to optimize DNN architectures for other vision tasks. In this work, we present what we believe to be the first proxyless hardware-aware search targeted for dense semantic segmentation. With this approach, we advance the state-of-the-art accuracy for latency-optimized networks on the Cityscapes semantic segmentation dataset. Our latency-optimized small SqueezeNAS network achieves 68.02% validation class mIOU with less than 35 ms inference times on the NVIDIA Xavier. Our latency-optimized large SqueezeNAS network achieves 73.62% class mIOU with less than 100 ms inference times. We demonstrate that significant performance gains are possible by utilizing NAS to find networks optimized for both the specific task and inference hardware. We also present detailed analysis comparing our networks to recent state-of-the-art architectures. The SqueezeNAS models are available for download here: https://github.com/ashaw596/squeezenas\r",
    "code_link": "https://github.com/ashaw596/squeezenas"
  },
  "iccv2019_neurarch_matrixnetsanewdeeparchitectureforobjectdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Matrix Nets: A New Deep Architecture for Object Detection",
    "authors": [
      "Abdullah Rashwan",
      "Agastya Kalra",
      "Pascal Poupart"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Rashwan_Matrix_Nets_A_New_Deep_Architecture_for_Object_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Rashwan_Matrix_Nets_A_New_Deep_Architecture_for_Object_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present Matrix Nets (xNets), a new deep architecture for object detection. xNets map objects with different sizes and aspect ratios into layers where the sizes and the aspect ratios of the objects within their layers are nearly uniform. Hence, xNets provide a scale and aspect ratio aware architecture. We leverage xNets to enhance key-points based object detection. Our architecture achieves mAP of 47.8 on MS COCO, which is higher than any other single-shot detector while using half the number of parameters and training 3x faster than the next best architecture.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_localizingoccluderswithcompositionalconvolutionalnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Localizing Occluders with Compositional Convolutional Networks",
    "authors": [
      "Adam Kortylewski",
      "Qing Liu",
      "Huiyu Wang",
      "Zhishuai Zhang",
      "Alan Yuille"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Kortylewski_Localizing_Occluders_with_Compositional_Convolutional_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Kortylewski_Localizing_Occluders_with_Compositional_Convolutional_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Compositional convolutional networks are generative models of neural network features, that achieve state of the art results when classifying partially occluded objects, even when they have not been exposed to occluded objects during training. While previous results showed the potential of CompositionalNets at localizing occluders, this remains to be confirmed quantitatively. In this work, we study the performance of CompositionalNets at localizing occluders in an image. We propose to extend the original model with a mixture of von-Mises-Fisher distributions. We show that this extension increases the model's ability to localize occluders in an image while retaining an exceptional performance at classifying partially occluded objects.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_msnetstructuralwiredneuralarchitecturesearchforinternetofthings": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "MSNet: Structural Wired Neural Architecture Search for Internet of Things",
    "authors": [
      "Hsin-Pai Cheng",
      "Tunhou Zhang",
      "Yukun Yang",
      "Feng Yan",
      "Harris Teague",
      "Yiran Chen",
      "Hai Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Cheng_MSNet_Structural_Wired_Neural_Architecture_Search_for_Internet_of_Things_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Cheng_MSNet_Structural_Wired_Neural_Architecture_Search_for_Internet_of_Things_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The prosperity of Internet of Things (IoT) calls for efficient ways of designing extremely compact yet accurate DNN models. Both the cell-based neural architecture search methods and the recently proposed graph based methods fall short in finding high quality IoT models due to the search flexibility, accuracy density, and node dependency limitations. In this paper, we propose a new graphbased neural architecture search methodology MSNAS for crafting highly compact yet accurate models for IoT devices. MSNAS supports flexible search space and can accumulate learned knowledge in a meta-graph to increase accuracy density. By adopting structural wiring architecture, MSNAS reduces the dependency between nodes, which allows more compact models without sacrificing accuracy. The preliminary experimental results on IoT applications demonstrate that the MSNet crafted by MSNAS outperforms MobileNetV2 and MnasNet by 3.0% in accuracy, with 20% less peak memory consumption and similar Multi-Adds.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_hologanunsupervisedlearningof3drepresentationsfromnaturalimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "HoloGAN: Unsupervised Learning of 3D Representations From Natural Images",
    "authors": [
      "Thu Nguyen-Phuoc",
      "Chuan Li",
      "Lucas Theis",
      "Christian Richardt",
      "Yong-Liang Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.\r",
    "code_link": "https://github.com/LynnHo/DCGAN-LSGAN-WGAN-WGAN-GPTensorflow"
  },
  "iccv2019_neurarch_searchingforaccuratebinaryneuralarchitectures": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Searching for Accurate Binary Neural Architectures",
    "authors": [
      "Mingzhu Shen",
      "Kai Han",
      "Chunjing Xu",
      "Yunhe Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Shen_Searching_for_Accurate_Binary_Neural_Architectures_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Shen_Searching_for_Accurate_Binary_Neural_Architectures_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Binary neural networks have attracted tremendous attention due to the efficiency for deploying them on mobile devices. Since the weak expression ability of binary weights and features, their accuracy is usually much lower than that of full-precision (i.e. 32-bit) models. Here we present a new frame work for automatically searching for compact but accurate binary neural networks. In practice, number of channels in each layer will be encoded into the search space and optimized using the evolutionary algorithm. Experiments conducted on benchmark datasets and neural architectures demonstrate that our searched binary networks can achieve the performance of full-precision models with acceptable increments on model sizes and calculations.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_advgan++harnessinglatentlayersforadversarygeneration": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "AdvGAN++: Harnessing Latent Layers for Adversary Generation",
    "authors": [
      "Surgan Jandial",
      "Puneet Mangla",
      "Sakshi Varshney",
      "Vineeth Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Jandial_AdvGAN_Harnessing_Latent_Layers_for_Adversary_Generation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Jandial_AdvGAN_Harnessing_Latent_Layers_for_Adversary_Generation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Adversarial examples are fabricated examples, indistinguishable from the original image that mislead neural networks and drastically lower their performance. Recently proposed AdvGAN, a GAN based approach, takes input image as a prior for generating adversaries to target a model. In this work, we show how latent features can serve as better priors than input images for adversary generation by proposing AdvGAN++, a version of AdvGAN that achieves higher attack rates than AdvGAN and at the same time generates perceptually realistic images on MNIST and CIFAR-10 datasets.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_layer-wiseinvertibilityforextremememorycostreductionofcnntraining": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Layer-Wise Invertibility for Extreme Memory Cost Reduction of CNN Training",
    "authors": [
      "Tristan Hascoet",
      "Quentin Febvre",
      "Weihao Zhuang",
      "Yasuo Ariki",
      "Tetsuya Takiguchi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Hascoet_Layer-Wise_Invertibility_for_Extreme_Memory_Cost_Reduction_of_CNN_Training_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Hascoet_Layer-Wise_Invertibility_for_Extreme_Memory_Cost_Reduction_of_CNN_Training_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Convolutional Neural Networks (CNN) have demonstrated state-of-the-art results on various computer vision problems. However, training CNNs require specialized GPU with large memory. GPU memory has been a major bottleneck of the CNN training procedure, limiting the size of both inputs and model architectures. Given the ubiquity of CNN in computer vision, optimizing the memory consumption of CNN training would have wide spread practical benefits. Recently, reversible neural networks have been proposed to alleviate this memory bottleneck by recomputing hidden activations through inverse operations during the backward pass of the backpropagation algorithm. In this paper, we push this idea to extreme and design a reversible neural network with minimal training memory consumption. The result demonstrated that we can train CIFAR10 dataset on Nvidia GTX750 GPU only with 1GB memory and achieve 93% accuracy within 67 minutes.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_investigatingconvolutionalneuralnetworksusingspatialorderness": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Investigating Convolutional Neural Networks using Spatial Orderness",
    "authors": [
      "Rohan Ghosh",
      "Anupam K. Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Ghosh_Investigating_Convolutional_Neural_Networks_using_Spatial_Orderness_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Ghosh_Investigating_Convolutional_Neural_Networks_using_Spatial_Orderness_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Convolutional Neural Networks (CNN) have been pivotal to the success of many state-of-the-art classification problems, in a wide variety of domains (for e.g. vision, speech, graphs and medical imaging). A commonality within those domains is the presence of hierarchical, spatially agglomerative local-to-global interactions within the data. For instance in natural images, neighboring pixels are more likely contain similar values than non-neighboring pixels which are further apart. To that end, we propose a statistical metric called spatial orderness, which quantifies the extent to which the input data (2D) obeys the underlying spatial ordering at various scales. In our experiments, we mainly find that adding convolutional layers to a CNN could be counterproductive for data bereft of spatial order at higher scales. Furthermore, we present a theoretical analysis (and empirical validation) of the spatial orderness of network weights, where we find that using smaller kernel sizes leads to kernels of greater spatial orderness and vice-versa.\r",
    "code_link": ""
  },
  "iccv2019_neurarch_whydoesdata-drivenbeattheory-drivencomputervision?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "NeurArch",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Neural Architects",
    "title": "Why Does Data-Driven Beat Theory-Driven Computer Vision?",
    "authors": [
      "John Tsotsos",
      "Iuliia Kotseruba",
      "Alexander Andreopoulos",
      "Yulong Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/NeurArch/Tsotsos_Why_Does_Data-Driven_Beat_Theory-Driven_Computer_Vision_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/NeurArch/Tsotsos_Why_Does_Data-Driven_Beat_Theory-Driven_Computer_Vision_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper proposes that despite the success of deep learning methods in computer vision, the dominance we see would not have been possible by the methods of deep learning alone: the tacit change has been the evolution of empirical practice in computer vision. We demonstrate this by examining the distribution of sensor settings in vision datasets, only one potential dataset bias, and performance of both classic and deep learning algorithms under various camera settings. This reveals a strong mismatch between optimal performance ranges of theory-driven algorithms and sensor setting distributions in common vision datasets.\r",
    "code_link": ""
  },
  "iccv2019_3drw_towardsdense3dreconstructionformixedrealityinhealthcareclassicalmulti-viewstereovsdeeplearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "Towards Dense 3D Reconstruction for Mixed Reality in Healthcare: Classical Multi-View Stereo vs Deep Learning",
    "authors": [
      "Kristina Prokopetc",
      "Romain Dupont"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Prokopetc_Towards_Dense_3D_Reconstruction_for_Mixed_Reality_in_Healthcare_Classical_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Prokopetc_Towards_Dense_3D_Reconstruction_for_Mixed_Reality_in_Healthcare_Classical_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Faithfully reproducing surroundings in 3D is a key-component in Mixed Reality for medical training in neonatology, where a user sees a hospital room in a Virtual Reality helmet while retaining tangible interaction with a baby mannequin and various medical tools. Deep learning solutions have high claims against classical methods but their performance in real-life application remains unclear. To fill this blank, we present a comparative study of depth map based Multi-View Stereo methods for dense 3D reconstruction. We compare classical state-of-the-art methods to their learned counterparts and assess their robustness to weakly-textured and reflective surfaces as well as accuracy on thin structures both globally and locally. We also analyze the effect of depth filtering along with computational effort. Our experiments reveal various factors which contribute to the performance gap between the methods that we discuss in detail. This study is the first to evaluate traditional dense geometry reconstruction methods against brand-new deep learning models. It helps to better understand what suits best the challenges of hospital environments. Furthermore, it builds a solid analytic ground to underscore the strengths and weaknesses of the learned methods.\r",
    "code_link": ""
  },
  "iccv2019_3drw_cnn-basedcostvolumeanalysisasconfidencemeasurefordensematching": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "CNN-Based Cost Volume Analysis as Confidence Measure for Dense Matching",
    "authors": [
      "Max Mehltretter",
      "Christian Heipke"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Mehltretter_CNN-Based_Cost_Volume_Analysis_as_Confidence_Measure_for_Dense_Matching_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Mehltretter_CNN-Based_Cost_Volume_Analysis_as_Confidence_Measure_for_Dense_Matching_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Due to its capability to identify erroneous disparity assignments in dense stereo matching, confidence estimation is beneficial for a wide range of applications, e.g. autonomous driving, which needs a high degree of confidence as mandatory prerequisite. Especially, the introduction of deep learning based methods resulted in an increasing popularity of this field in recent years, caused by a significantly improved accuracy. Despite this remarkable development, most of these methods rely on features learned from disparity maps only, not taking into account the corresponding 3-dimensional cost volumes. However, it was already demonstrated that with conventional methods based on hand-crafted features this additional information can be used to further increase the accuracy. In order to combine the advantages of deep learning and cost volume based features, in this paper, we propose a novel Convolutional Neural Network (CNN) architecture to directly learn features for confidence estimation from volumetric 3D data. An extensive evaluation on three datasets using three common dense stereo matching techniques demonstrates the generality and state-of-the-art accuracy of the proposed method.\r",
    "code_link": ""
  },
  "iccv2019_3drw_silhouette-assisted3dobjectinstancereconstructionfromaclutteredscene": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "Silhouette-Assisted 3D Object Instance Reconstruction from a Cluttered Scene",
    "authors": [
      "Lin Li",
      "Salman Khan",
      "Nick Barnes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Li_Silhouette-Assisted_3D_Object_Instance_Reconstruction_from_a_Cluttered_Scene_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Li_Silhouette-Assisted_3D_Object_Instance_Reconstruction_from_a_Cluttered_Scene_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The objective of our work is to reconstruct 3D object instances from a single RGB image of a cluttered scene. 3D object instance reconstruction is an ill-posed problem due to the presence of heavily occluded and truncated objects, and self-occlusions that lead to substantial regions of unseen areas. Previous works for 3D reconstruction take clues from object silhouettes to carve reconstructed outputs. In this paper, we explore two ways to include silhouette learnable in the network for 3D instance reconstruction from a single cluttered scene image. To this end, in the first approach, we automatically generate instance-specific silhouettes that are compactly encoded within our network design and used to improve the reconstructed 3D shapes; in the second approach, we find an efficient design to regularize object reconstruction explicitly. Experimental results on the SUNCG dataset show that our methods have better performance than the state-of-the-art.\r",
    "code_link": ""
  },
  "iccv2019_3drw_learnedsemanticmulti-sensordepthmapfusion": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "Learned Semantic Multi-Sensor Depth Map Fusion",
    "authors": [
      "Denys Rozumnyi",
      "Ian Cherabier",
      "Marc Pollefeys",
      "Martin Oswald"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Rozumnyi_Learned_Semantic_Multi-Sensor_Depth_Map_Fusion_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Rozumnyi_Learned_Semantic_Multi-Sensor_Depth_Map_Fusion_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Volumetric depth map fusion based on truncated signed distance functions has become a standard method and is used in many 3D reconstruction pipelines. In this paper, we are generalizing this classic method in multiple ways: 1) Semantics: Semantic information enriches the scene representation and is incorporated into the fusion process. 2) Multi-Sensor: Depth information can originate from different sensors or algorithms with very different noise and outlier statistics which are considered during data fusion. 3) Scene denoising and completion: Sensors can fail to recover depth for certain materials and light conditions, or data is missing due to occlusions. Our method denoises the geometry, closes holes and computes a watertight surface for every semantic class. 4) Learning: We propose a neural network reconstruction method that unifies all these properties within a single powerful framework. Our method learns sensor or algorithm properties jointly with semantic depth fusion and scene completion and can also be used as an expert system, e.g. to unify the strengths of various photometric stereo algorithms. Our approach is the first to unify all these properties. Experimental evaluations on both synthetic and real data sets demonstrate clear improvements.\r",
    "code_link": ""
  },
  "iccv2019_3drw_landmark-guideddeformationtransferoftemplatefacialexpressionsforautomaticgenerationofavatarblendshapes": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "Landmark-Guided Deformation Transfer of Template Facial Expressions for Automatic Generation of Avatar Blendshapes",
    "authors": [
      "Hayato Onizuka",
      "Diego Thomas",
      "Hideaki Uchiyama",
      "Rin-ichiro Taniguchi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Onizuka_Landmark-Guided_Deformation_Transfer_of_Template_Facial_Expressions_for_Automatic_Generation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Onizuka_Landmark-Guided_Deformation_Transfer_of_Template_Facial_Expressions_for_Automatic_Generation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Blendshape models are commonly used to track and re-target facial expressions to virtual avatars using RGB-D cameras and without using any facial marker. When using blendshape models, the target avatar model must possess a set of key-shapes that can be blended depending on the estimated facial expression. Creating realistic set of key-shapes is extremely difficult and requires time and professional expertise. As a consequence, blendshape-based re-targeting technology can only be used with a limited amount of pre-built avatar models, which is not attractive for the large public. In this paper, we propose an automatic method to easily generate realistic key-shapes of any avatar that map directly to the source blendshape model (the user is only required to select a few facial landmarks on the avatar mesh). By doing so, captured facial motion can be easily re-targeted to any avatar, even when the avatar has largely different shape and topology compared with the source template mesh. Our experimental results show the accuracy of our proposed method compared with the state-of-the-art method for mesh deformation transfer.\r",
    "code_link": ""
  },
  "iccv2019_3drw_sharpnetfastandaccuraterecoveryofoccludingcontoursinmonoculardepthestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "SharpNet: Fast and Accurate Recovery of Occluding Contours in Monocular Depth Estimation",
    "authors": [
      "Michael Ramamonjisoa",
      "Vincent Lepetit"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Ramamonjisoa_SharpNet_Fast_and_Accurate_Recovery_of_Occluding_Contours_in_Monocular_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Ramamonjisoa_SharpNet_Fast_and_Accurate_Recovery_of_Occluding_Contours_in_Monocular_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We introduce SharpNet, a method that predicts an accurate depth map given a single input color image, with a particular attention to the reconstruction of occluding contours: Occluding contours are an important cue for object recognition, and for realistic integration of virtual objects in Augmented Reality, but they are also notoriously difficult to reconstruct accurately. For example, they are a challenge for stereo-based reconstruction methods, as points around an occluding contour are only visible in one of the two views. Inspired by recent methods that introduce normal estimation to improve depth prediction, we introduce novel terms to constrain normals, depth and occluding contours predictions. Since ground truth depth is difficult to obtain with pixel-perfect accuracy along occluding contours, we use synthetic images for training, followed by fine-tuning on real data. We demonstrate our approach on the challenging NYUv2-Depth dataset, and show that our method outperforms the state-of-the-art along occluding contours, while performing on par with the best recent methods for the rest of the images. Its accuracy along the occluding contours is actually better than the \"ground truth\" acquired by a depth camera based on structured light. We show this by introducing a new benchmark based on NYUv2-Depth for evaluating occluding contours in monocular reconstruction, which is our second contribution.\r",
    "code_link": "https://github.com/MichaelRamamonjisoa/SharpNet"
  },
  "iccv2019_3drw_adirectleast-squaressolutiontomulti-viewabsoluteandrelativeposefrom2d-3dperspectivelinepairs": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "A Direct Least-Squares Solution to Multi-View Absolute and Relative Pose from 2D-3D Perspective Line Pairs",
    "authors": [
      "Hichem Abdellali",
      "Robert Frohlich",
      "Zoltan Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Abdellali_A_Direct_Least-Squares_Solution_to_Multi-View_Absolute_and_Relative_Pose_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Abdellali_A_Direct_Least-Squares_Solution_to_Multi-View_Absolute_and_Relative_Pose_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a new algorithm for estimating the absolute and relative pose of a multi-view camera system. We derive a direct least squares solver using Grobner basis which works both for the minimal case (set of 3 line pairs for each camera) and the general case using all inlier 2D-3D line pairs for a multi-view camera system. The algorithm has been validated on a large synthetic dataset as well as real data. Experimental results confirm the stable and real-time performance under realistic outlier ratio and noise on the line parameters. Comparative tests show that our method compares favorably to the latest state of the art algorithms.\r",
    "code_link": "https://github.com/laurentkneip/polyjam"
  },
  "iccv2019_3drw_counterfactualdepthfromasinglergbimage": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "Counterfactual Depth from a Single RGB Image",
    "authors": [
      "Theerasit Issaranon",
      "Chuhang Zou",
      "David Forsyth"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Issaranon_Counterfactual_Depth_from_a_Single_RGB_Image_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Issaranon_Counterfactual_Depth_from_a_Single_RGB_Image_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We describe a method that predicts, from a single RGB image, a depth map that describes the scene when a masked object is removed - we call this \"counterfactual depth\" that models hidden scene geometry together with the observations. Our method works for the same reason that scene completion works: the spatial structure of objects is simple. But we offer a much higher resolution representation of space than current scene completion methods, as we operate at pixel-level precision and do not rely on a voxel representation. Furthermore, we do not require RGBD inputs. Our method uses a standard encoder-decoder architecture, and with a decoder modified to accept an object mask. We describe a small evaluation dataset that we have collected, which allows inference about what factors affect reconstruction most strongly. Using this dataset, we show that our depth predictions for masked objects are better than other baselines.\r",
    "code_link": ""
  },
  "iccv2019_3drw_leveragingvisionreconstructionpipelinesforsatelliteimagery": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "Leveraging Vision Reconstruction Pipelines for Satellite Imagery",
    "authors": [
      "Kai Zhang",
      "Noah Snavely",
      "Jin Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Zhang_Leveraging_Vision_Reconstruction_Pipelines_for_Satellite_Imagery_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Zhang_Leveraging_Vision_Reconstruction_Pipelines_for_Satellite_Imagery_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Reconstructing 3D geometry from satellite imagery is an important and growing topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context.\r",
    "code_link": "https://github.com/openMVG/openMVG"
  },
  "iccv2019_3drw_3dshapereconstructionofplantrootsinacylindricaltankfrommultiviewimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "3D Shape Reconstruction of Plant Roots in a Cylindrical Tank From Multiview Images",
    "authors": [
      "Takeshi Masuda"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Masuda_3D_Shape_Reconstruction_of_Plant_Roots_in_a_Cylindrical_Tank_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Masuda_3D_Shape_Reconstruction_of_Plant_Roots_in_a_Cylindrical_Tank_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a method for reconstructing a 3D shape of live plant roots submerged in a transparent cylindrical hydroponic tank from multiple-view images for root phenotyping. The proposed method does not assume special devices and careful setups, but the geometry and material of the tank are assumed known. First, we estimate the intrinsic and extrinsic camera parameters by the SfM algorithm, and the scale and axis of the tank are estimated by chamfer matching. Second, we apply the ray tracing considering the refraction for each view, the input images are mapped to the voxels, and then multiview voxels at the same location are robustly merged to reconstruct the 3D shape. Finally, the root feature extracted from the voxel is binarized and thinned by applying the 3D Canny operator. The proposed method was applied to real a dataset, and the reconstruction results are presented.\r",
    "code_link": ""
  },
  "iccv2019_3drw_learningdensewidebaselinestereomatchingforpeople": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "Learning Dense Wide Baseline Stereo Matching for People",
    "authors": [
      "Akin Caliskan",
      "Armin Mustafa",
      "Evren Imre",
      "Adrian Hilton"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Caliskan_Learning_Dense_Wide_Baseline_Stereo_Matching_for_People_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Caliskan_Learning_Dense_Wide_Baseline_Stereo_Matching_for_People_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Existing methods for stereo work on narrow baseline image pairs giving limited performance between wide baseline views. This paper proposes a framework to learn and estimate dense stereo for people from wide baseline image pairs. A synthetic people stereo patch dataset (S2P2) is introduced to learn wide baseline dense stereo matching for people. The proposed framework not only learns human specific features from synthetic data but also exploits pooling layer and data augmentation to adapt to real data. The network learns from the human specific stereo patches from the proposed dataset for wide-baseline stereo estimation. In addition to patch match learning, a stereo constraint is introduced in the framework to solve wide baseline stereo reconstruction of humans. Quantitative and qualitative performance evaluation against state-of-the-art methods of proposed method demonstrates improved wide baseline stereo reconstruction on challenging datasets. We show that it is possible to learn stereo matching from synthetic people dataset and improve performance on real datasets for stereo reconstruction of people from narrow and wide baseline stereo data.\r",
    "code_link": ""
  },
  "iccv2019_3drw_fullfusionaframeworkforsemanticreconstructionofdynamicscenes": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "FullFusion: A Framework for Semantic Reconstruction of Dynamic Scenes",
    "authors": [
      "Mihai Bujanca",
      "Mikel Lujan",
      "Barry Lennox"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Bujanca_FullFusion_A_Framework_for_Semantic_Reconstruction_of_Dynamic_Scenes_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Bujanca_FullFusion_A_Framework_for_Semantic_Reconstruction_of_Dynamic_Scenes_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Assuming that scenes are static is common in SLAM research. However, the world is complex, dynamic, and features interactive agents. Mobile robots operating in a variety of environments in real-life scenarios require an advanced level of understanding of their surroundings. Therefore, it is crucial to find effective ways of representing the world in its dynamic complexity, beyond the geometry of static scene elements. We present a framework that enables incremental reconstruction of semantically-annotated 3D models in dynamic settings using commodity RGB-D sensors. Our method is the first to perform semantic reconstruction of non-rigidly deforming objects along with a static background. FullFusion is a step towards enabling robots to have a deeper and richer understanding of their surroundings, and can facilitate the study of interaction and scene dynamics. To showcase the potential of FullFusion, we provide a quantitative and qualitative evaluation on a baseline implementation which employs specific reconstruction and segmentation pipelines. It is, however, important to highlight that the modular design of the framework allows us to easily replace any of the components with new or existing counterparts.\r",
    "code_link": ""
  },
  "iccv2019_3drw_humanmeshnetpolygonalmeshrecoveryofhumans": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "HumanMeshNet: Polygonal Mesh Recovery of Humans",
    "authors": [
      "Abbhinav Venkat",
      "Chaitanya Patel",
      "Yudhik Agrawal",
      "Avinash Sharma"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Venkat_HumanMeshNet_Polygonal_Mesh_Recovery_of_Humans_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Venkat_HumanMeshNet_Polygonal_Mesh_Recovery_of_Humans_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " 3D Human Body Reconstruction from a monocular image is an important problem in computer vision with applications in virtual and augmented reality platforms, animation industry, en-commerce domain, etc. While several of the existing works formulate it as a volumetric or parametric learning with complex and indirect reliance on re-projections of the mesh, we would like to focus on implicitly learning the mesh representation. To that end, we propose a novel model, HumanMeshNet, that regresses a template mesh's vertices, as well as receives a regularization by the 3D skeletal locations in a multi-branch, multi-task setup. The image to mesh vertex regression is further regularized by the neighborhood constraint imposed by mesh topology ensuring smooth surface reconstruction. The proposed paradigm can theoretically learn local surface deformations induced by body shape variations and can therefore learn high-resolution meshes going ahead. We show comparable performance with SoA (in terms of surface and joint error) with far lesser computational complexity, modeling cost and therefore real-time reconstructions on three publicly available datasets. We also show the generalizability of the proposed paradigm for a similar task of predicting hand mesh models. Given these initial results, we would like to exploit the mesh topology in an explicit manner going ahead.\r",
    "code_link": ""
  },
  "iccv2019_3drw_tobundleadjustornotacomparisonofrelativegeolocationcorrectionstrategiesforsatellitemulti-viewstereo": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "To Bundle Adjust or Not: A Comparison of Relative Geolocation Correction Strategies for Satellite Multi-View Stereo",
    "authors": [
      "Roger Mari",
      "Carlo de Franchis",
      "Enric Meinhardt-Llopis",
      "Gabriele Facciolo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Mari_To_Bundle_Adjust_or_Not_A_Comparison_of_Relative_Geolocation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Mari_To_Bundle_Adjust_or_Not_A_Comparison_of_Relative_Geolocation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The generation of up-to-date accurate 3D models from multi-view satellite images has recently become a hot research topic. A well-known challenge of this problem is to put all cameras into a common frame of reference, since depending on the satellite geopositioning equipment the camera parameters may contain errors of up to tens of meters on the ground. In this context, bundle adjustment based techniques, relying on the identification of a set of tie-points and the correction of the camera models to make them coincident, have become a generally accepted practice. However, new approaches capable of producing state-of-the-art results without the use of prior bundle adjustment have also been proposed. This work aims to compare both strategies and assess the practical impact of using bundle adjustment for 3D reconstruction from multi-view satellite images.\r",
    "code_link": ""
  },
  "iccv2019_3drw_3dtexturingfrommulti-datesatelliteimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DRW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Reconstruction in the Wild",
    "title": "3D Texturing From Multi-Date Satellite Images",
    "authors": [
      "Enric Meinhardt-Llopis",
      "Marie d'Autume"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DRW/Meinhardt-Llopis_3D_Texturing_From_Multi-Date_Satellite_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DRW/Meinhardt-Llopis_3D_Texturing_From_Multi-Date_Satellite_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper addresses the problem of point cloud textu-ration of urban areas from multiple satellite images. Ouralgorithm is well-suited for the case where the images areobtained from different dates, where the dynamic ranges areincompatible and the position of the shadows is different.The method relies on a robust, PDE-based, fusion of themultiple candidate textures for each surface patch, and op-tionally on a geometric criterion to remove the shadows ofthe known objects. We showcase the results by building a3D model of some areas of Boulogne Sur Mer (Argentine) such that all facades are correctly textured, with uniformcolours and without shadows, even if for each individualinput image only one side of the buildings was visible.\r",
    "code_link": "https://github.com/cmla/s2p"
  },
  "iccv2019_vot_theseventhvisualobjecttrackingvot2019challengeresults": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "The Seventh Visual Object Tracking VOT2019 Challenge Results",
    "authors": [
      "Matej Kristan",
      "Jiri Matas",
      "Ales Leonardis",
      "Michael Felsberg",
      "Roman Pflugfelder",
      "Joni-Kristian Kamarainen",
      "Luka Cehovin Zajc",
      "Ondrej Drbohlav",
      "Alan Lukezic",
      "Amanda Berg",
      "Abdelrahman Eldesokey",
      "Jani Kapyla",
      "Gustavo Fernandez",
      "Abel Gonzalez-Garcia",
      "Alireza Memarmoghadam",
      "Andong Lu",
      "Anfeng He",
      "Anton Varfolomieiev",
      "Antoni Chan",
      "Ardhendu Shekhar Tripathi",
      "Arnold Smeulders",
      "Bala Suraj Pedasingu",
      "Bao Xin Chen",
      "Baopeng Zhang",
      "Baoyuan Wu",
      "Bi Li",
      "Bin He",
      "Bin Yan",
      "Bing Bai",
      "Bing Li",
      "Bo Li",
      "Byeong Hak Kim",
      "Byeong Hak Ki"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Kristan_The_Seventh_Visual_Object_Tracking_VOT2019_Challenge_Results_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Kristan_The_Seventh_Visual_Object_Tracking_VOT2019_Challenge_Results_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The Visual Object Tracking challenge VOT2019 is the seventh annual tracker benchmarking activity organized by the VOT initiative. Results of 81 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis as well as the standard VOT methodology for long-term tracking analysis. The VOT2019 challenge was composed of five challenges focusing on different tracking domains: (i) VOTST2019 challenge focused on short-term tracking in RGB, (ii) VOT-RT2019 challenge focused on \"real-time\" shortterm tracking in RGB, (iii) VOT-LT2019 focused on longterm tracking namely coping with target disappearance and reappearance. Two new challenges have been introduced: (iv) VOT-RGBT2019 challenge focused on short-term tracking in RGB and thermal imagery and (v) VOT-RGBD2019 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2019, VOT-RT2019 and VOT-LT2019 datasets were refreshed while new datasets were introduced for VOT-RGBT2019 and VOT-RGBD2019. The VOT toolkit has been updated to support both standard shortterm, long-term tracking and tracking with multi-channel imagery. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website.\r",
    "code_link": ""
  },
  "iccv2019_vot_semi-automaticannotationofobjectsinvisual-thermalvideo": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "Semi-Automatic Annotation of Objects in Visual-Thermal Video",
    "authors": [
      "Amanda Berg",
      "Joakim Johnander",
      "Flavie Durand de Gevigney",
      "Jorgen Ahlberg",
      "Michael Felsberg"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Berg_Semi-Automatic_Annotation_of_Objects_in_Visual-Thermal_Video_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Berg_Semi-Automatic_Annotation_of_Objects_in_Visual-Thermal_Video_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep learning requires large amounts of annotated data. Manual annotation of objects in video is, regardless of annotation type, a tedious and time-consuming process. In particular, for scarcely used image modalities human annotation is hard to justify. In such cases, semi-automatic annotation provides an acceptable option. In this work, a recursive, semi-automatic annotation method for video is presented. The proposed method utilizes a state-of-the-art video object segmentation method to propose initial annotations for all frames in a video based on only a few manual object segmentations. In the case of a multi-modal dataset, the multi-modality is exploited to refine the proposed annotations even further. The final tentative annotations are presented to the user for manual correction. The method is evaluated on a subset of the RGBT-234 visual-thermal dataset reducing the workload for a human annotator with approximately 78% compared to full manual annotation. Utilizing the proposed pipeline, sequences are annotated for the VOT-RGBT 2019 challenge.\r",
    "code_link": ""
  },
  "iccv2019_vot_multi-modalfusionforend-to-endrgb-ttracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "Multi-Modal Fusion for End-to-End RGB-T Tracking",
    "authors": [
      "Lichao Zhang",
      "Martin Danelljan",
      "Abel Gonzalez-Garcia",
      "Joost van de Weijer",
      "Fahad Shahbaz Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Zhang_Multi-Modal_Fusion_for_End-to-End_RGB-T_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Zhang_Multi-Modal_Fusion_for_End-to-End_RGB-T_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose an end-to-end tracking framework for fusing the RGB and TIR modalities in RGB-T tracking. Our baseline tracker is DiMP (Discriminative Model Prediction), which employs a carefully designed target prediction network trained end-to-end using a discriminative loss. We analyze the effectiveness of modality fusion in each of the main components in DiMP, i.e. feature extractor, target estimation network, and classifier. We consider several fusion mechanisms acting at different levels of the framework, including pixel-level, feature-level and response-level. Our tracker is trained in an end-to-end manner, enabling the components to learn how to fuse the information from both modalities. As data to train our model, we generate a large-scale RGB-T dataset by considering an annotated RGB tracking dataset (GOT-10k) and synthesizing paired TIR images using an image-to-image translation approach. We perform extensive experiments on VOT-RGBT2019 dataset and RGBT210 dataset, evaluating each type of modality fusing on each model component. The results show that the proposed fusion mechanisms improve the performance of the single modality counterparts. We obtain our best results when fusing at the feature-level on both the IoU-Net and the model predictor, obtaining an EAO score of 0.391 on VOT-RGBT2019 dataset. With this fusion mechanism we achieve the state-of-the-art performance on RGBT210 dataset.\r",
    "code_link": ""
  },
  "iccv2019_vot_multi-adapterrgbttracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "Multi-Adapter RGBT Tracking",
    "authors": [
      "Cheng Long Li",
      "Andong Lu",
      "Ai Hua Zheng",
      "Zhengzheng Tu",
      "Jin Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Li_Multi-Adapter_RGBT_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Li_Multi-Adapter_RGBT_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The task of RGBT tracking aims to take the complementary advantages from visible spectrum and thermal infrared data to achieve robust visual tracking, and receives more and more attention in recent years. Existing works focus on modality-specific information integration by introducing modality weights to achieve adaptive fusion or learning robust feature representations of different modalities. Although these methods could effectively deploy the modality-specific properties, they ignore the potential values of modality-shared cues as well as instance-aware information, which are crucial for effective fusion of different modalities in RGBT tracking. In this paper, we propose a novel Multi-Adapter convolutional Network (MANet) to jointly perform modality-shared, modality-specific and instance-aware feature learning in an end-to-end trained deep framework for RGBT tracking. We design three kinds of adapters within our network. In a specific, the generality adapter is to extract shared object representations, the modality adapter aims at encoding modality-specific information to deploy their complementary advantages, and the instance adapter is to model the appearance properties and temporal variations of a certain object. Moreover, to reduce computational complexity for real-time demand of visual tracking, we design a parallel structure of generic adapter and modality adapter. Extensive experiments on two RGBT tracking benchmark datasets demonstrate the outstanding performance of the proposed tracker against other state-ofthe-art RGB and RGBT tracking algorithms.\r",
    "code_link": ""
  },
  "iccv2019_vot_visualobjecttrackingbyusingrankingloss": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "Visual Object Tracking by Using Ranking Loss",
    "authors": [
      "Hakan Cevikalp",
      "Hasan Saribas",
      "Burak Benligiray",
      "Sinem Kahvecioglu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Cevikalp_Visual_Object_Tracking_by_Using_Ranking_Loss_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Cevikalp_Visual_Object_Tracking_by_Using_Ranking_Loss_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper introduces a novel deep neural network tracker for robust object tracking. To this end, we employ a ranking loss which provides a fine-tuning of the target object position and returns more precise bounding boxes framing the target object. This is achieved by systematically learning to give higher scores to the candidate regions better framing the target object than the regions that frame the object with less accuracy. As a result, the risk of tracking error accumulation and drifts are largely mitigated, and the object is tracked more successfully. When the proposed network is used with a simple yet effective model update rule, our proposed tracker achieves the state-of-the-art results on all tested challenging tracking datasets. Especially, our results on the OTB (Object Tracking Benchmark) datasets are very promising. The proposed tracker outperforms both deep neural network and correlation filter based trackers, MDNet and ECO, by about 2%, which is a significant improvement over the previous state-of-the-art.\r",
    "code_link": ""
  },
  "iccv2019_vot_fastvisualobjecttrackingusingellipsefittingforrotatedboundingboxes": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "Fast Visual Object Tracking using Ellipse Fitting for Rotated Bounding Boxes",
    "authors": [
      "Bao Xin Chen",
      "John Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Chen_Fast_Visual_Object_Tracking_using_Ellipse_Fitting_for_Rotated_Bounding_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Chen_Fast_Visual_Object_Tracking_using_Ellipse_Fitting_for_Rotated_Bounding_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we demonstrate a novel algorithm that uses ellipse fitting to estimate the bounding box rotation angle and size with the segmentation(mask) on the target for online and real-time visual object tracking. Our method, SiamMask_E, improves the bounding box fitting procedure of the state-of-the-art object tracking algorithm SiamMask and still retains a fast-tracking frame rate (80 fps) on a system equipped with GPU (GeForce GTX 1080 Ti or higher). We tested our approach on the visual object tracking datasets (VOT2016, VOT2018, and VOT2019) that were labeled with rotated bounding boxes. By comparing with the original SiamMask, we achieved an improved Accuracy of 64.5% and 30.3% EAO on VOT2019, which is 4.9% and 2% higher than the original SiamMask. The implementation is available on GitHub: https://github.com/baoxinchen/siammask_e.\r",
    "code_link": ""
  },
  "iccv2019_vot_visualtrackingbymeansofdeepreinforcementlearningandanexpertdemonstrator": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "Visual Tracking by Means of Deep Reinforcement Learning and an Expert Demonstrator",
    "authors": [
      "Matteo Dunnhofer",
      "Niki Martinel",
      "Gian Luca Foresti",
      "Christian Micheloni"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Dunnhofer_Visual_Tracking_by_Means_of_Deep_Reinforcement_Learning_and_an_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Dunnhofer_Visual_Tracking_by_Means_of_Deep_Reinforcement_Learning_and_an_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In the last decade many different algorithms have been proposed to track a generic object in videos. Their execution on recent large-scale video datasets can produce a great amount of various tracking behaviours. New trends in Reinforcement Learning showed that demonstrations of an expert agent can be efficiently used to speed-up the process of policy learning. Taking inspiration from such works and from the recent applications of Reinforcement Learning to visual tracking, we propose two novel trackers, A3CT, which exploits demonstrations of a state-of-the-art tracker to learn an effective tracking policy, and A3CTD, that takes advantage of the same expert tracker to correct its behaviour during tracking. Through an extensive experimental validation on the GOT-10k, OTB-100, LaSOT, UAV123 and VOT benchmarks, we show that the proposed trackers achieve state-of-the-art performance while running in real-time.\r",
    "code_link": ""
  },
  "iccv2019_vot_intra-frameobjecttrackingbydeblatting": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "VOT",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Visual Object Tracking Challenge",
    "title": "Intra-Frame Object Tracking by Deblatting",
    "authors": [
      "Jan Kotera",
      "Denys Rozumnyi",
      "Filip Sroubek",
      "Jiri Matas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/VOT/Kotera_Intra-Frame_Object_Tracking_by_Deblatting_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/VOT/Kotera_Intra-Frame_Object_Tracking_by_Deblatting_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Objects moving at high speed along complex trajectories often appear in videos, especially videos of sports. Such objects elapse non-negligible distance during exposure time of a single frame and therefore their position in the frame is not well defined. They appear as semi-transparent streaks due to the motion blur and cannot be reliably tracked by standard trackers. We propose a novel approach called Tracking by Deblatting based on the observation that motion blur is directly related to the intra-frame trajectory of an object. Blur is estimated by solving two intertwined inverse problems, blind deblurring and image matting, which we call deblatting. The trajectory is then estimated by fitting a piecewise quadratic curve, which models physically justifiable trajectories. As a result, tracked objects are precisely localized with higher temporal resolution than by conventional trackers. The proposed TbD tracker was evaluated on a newly created dataset of videos with ground truth obtained by a high-speed camera using a novel Trajectory-IoU metric that generalizes the traditional Intersection over Union and measures the accuracy of the intra-frame trajectory. The proposed method outperforms baseline both in recall and trajectory accuracy.\r",
    "code_link": ""
  },
  "iccv2019_adw_conditionalvehicletrajectoriespredictionincarlaurbanenvironment": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Conditional Vehicle Trajectories Prediction in CARLA Urban Environment",
    "authors": [
      "Thibault Buhet",
      "Emilie Wirbel",
      "Xavier Perrotton"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Buhet_Conditional_Vehicle_Trajectories_Prediction_in_CARLA_Urban_Environment_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Buhet_Conditional_Vehicle_Trajectories_Prediction_in_CARLA_Urban_Environment_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Imitation learning is becoming more and more successful for autonomous driving. End-to-end (raw signal to command) performs well on relatively simple tasks (lane keeping and navigation). Mid-to-mid (environment abstraction to mid-level trajectory representation) or direct perception (raw signal to performance) approaches strive to handle more complex, real life environment and tasks (e.g. complex intersection). In this work, we show that complex urban situations can be handled with raw signal input and mid-level representation. We build a hybrid end-to-mid approach predicting trajectories for neighbor vehicles and for the ego vehicle with a conditional navigation goal. We propose an original architecture inspired from social pooling LSTM taking low and mid level data as input and producing trajectories as polynomials of time. We introduce a label augmentation mechanism to get the level of generalization that is required to control a vehicle. The performance is evaluated on CARLA 0.8 benchmark, showing significant improvements over previously published state of the art.\r",
    "code_link": ""
  },
  "iccv2019_adw_rangeadaptationfor3dobjectdetectioninlidar": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Range Adaptation for 3D Object Detection in LiDAR",
    "authors": [
      "Ze Wang",
      "Sihao Ding",
      "Ying Li",
      "Minming Zhao",
      "Sohini Roychowdhury",
      "Andreas Wallin",
      "Guillermo Sapiro",
      "Qiang Qiu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Wang_Range_Adaptation_for_3D_Object_Detection_in_LiDAR_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Wang_Range_Adaptation_for_3D_Object_Detection_in_LiDAR_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " LiDAR-based 3D object detection plays a crucial role in modern autonomous driving systems. LiDAR data often exhibit severe changes in properties across different observation ranges. In this paper, we explore cross-range adaptation for 3D object detection using LiDAR, i.e., far-range observations are adapted to near-range. This way, far-range detection is optimized for similar performance to near-range one. We adopt a bird-eyes view (BEV) detection framework to perform the proposed model adaptation. Our model adaptation consists of an adversarial global adaptation, and a fine-grained local adaptation. The proposed cross-range adaptation framework is validated on three state-of-the-art LiDAR based object detection networks, and we consistently observe performance improvement on the far-range objects, without adding any auxiliary parameters to the model. To the best of our knowledge, this paper is the first attempt to study cross-range LiDAR adaptation for object detection in point clouds. To demonstrate the generality of the proposed adaptation framework, experiments on more challenging cross-device adaptation are further conducted, and a new LiDAR dataset with high-quality annotated point clouds is released to promote future research.\r",
    "code_link": "https://github.com/traveller59/second.pytorch"
  },
  "iccv2019_adw_adherentraindropremovalwithself-supervisedattentionmapsandspatio-temporalgenerativeadversarialnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Adherent Raindrop Removal with Self-Supervised Attention Maps and Spatio-Temporal Generative Adversarial Networks",
    "authors": [
      "Stefano Alletto",
      "Casey Carlin",
      "Luca Rigazio",
      "Yasunori Ishii",
      "Sotaro Tsukizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Alletto_Adherent_Raindrop_Removal_with_Self-Supervised_Attention_Maps_and_Spatio-Temporal_Generative_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Alletto_Adherent_Raindrop_Removal_with_Self-Supervised_Attention_Maps_and_Spatio-Temporal_Generative_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the rapid increase of outdoor computer vision applications requiring robustness to adverse weather conditions such as automotive and robotics, the loss in image quality that is due to raindrops adherent to the camera lenses is becoming a major concern. In this paper we propose to remove raindrops and improve image quality in the spatio-temporal domain by leveraging the inherent robustness of adopting motion cues and the restorative capabilities of conditional generative adversarial networks. We first propose a competitive single-image baseline capable of estimating the raindrop locations in a self-supervised manner, and then use it to bootstrap our novel spatio-temporal architecture. This shows encouraging performance when compared to both state of the art single-image de-raining methods, and recent video-to-video translation approaches.\r",
    "code_link": ""
  },
  "iccv2019_adw_deeptrailerassistdeeplearningbasedtrailerdetection,trackingandarticulationangleestimationonautomotiverear-viewcamera": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "DeepTrailerAssist: Deep Learning Based Trailer Detection, Tracking and Articulation Angle Estimation on Automotive Rear-View Camera",
    "authors": [
      "Ashok Dahal",
      "Jakir Hossen",
      "Chennupati Sumanth",
      "Ganesh Sistu",
      "Kazumi Malhan",
      "Muhammad Amasha",
      "Senthil Yogamani"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Dahal_DeepTrailerAssist_Deep_Learning_Based_Trailer_Detection_Tracking_and_Articulation_Angle_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Dahal_DeepTrailerAssist_Deep_Learning_Based_Trailer_Detection_Tracking_and_Articulation_Angle_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Trailers are commonly used for transport of goods and recreational materials. Even for experienced drivers, manoeuvres with trailers, especially reversing can be complex and stressful. Thus driver assistance systems are very useful in these scenarios. They are typically achieved by a single rear-view fisheye camera perception algorithms. There is no public dataset for this problem and hence there is very little academic literature on this topic. This motivated us to present all the trailer assist use cases in detail and propose a deep learning based solution for trailer perception problems. Using our proprietary dataset comprising of 11 different trailer types, we achieve a reasonable detection accuracy using a lightweight real-time network running at 30 fps on a low power embedded system. The dataset will be released as a companion to our recently published dataset [24] to encourage further research in this area.\r",
    "code_link": ""
  },
  "iccv2019_adw_spatio-temporalactiongraphnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Spatio-Temporal Action Graph Networks",
    "authors": [
      "Roei Herzig",
      "Elad Levi",
      "Huijuan Xu",
      "Hang Gao",
      "Eli Brosh",
      "Xiaolong Wang",
      "Amir Globerson",
      "Trevor Darrell"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Herzig_Spatio-Temporal_Action_Graph_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Herzig_Spatio-Temporal_Action_Graph_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Events defined by the interaction of objects in a scene are often of critical importance; yet important events may have insufficient labeled examples to train a conventional deep model to generalize to future object appearance. Activity recognition models that represent object interactions explicitly have the potential to learn in a more efficient manner than those that represent scenes with global descriptors. We propose a novel inter-object graph representation for activity recognition based on a disentangled graph embedding with direct observation of edge appearance. In contrast to prior efforts, our approach uses explicit appearance for high order relations derived from object-object interaction, formed over regions that are the union of the spatial extent of the constituent objects. We employ a novel factored embedding of the graph structure, disentangling a representation hierarchy formed over spatial dimensions from that found over temporal variation. We demonstrate the effectiveness of our model on the Charades activity recognition benchmark, as well as a new dataset of driving activities focusing on multi-object interactions with near-collision events. Our model offers significantly improved performance compared to baseline approaches without object-graph representations, or with previous graph-based models.\r",
    "code_link": "https://github.com/roeiherz/STAG-Nets"
  },
  "iccv2019_adw_multi-viewreprojectionarchitecturefororientationestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Multi-View Reprojection Architecture for Orientation Estimation",
    "authors": [
      "Hee Min Choi",
      "Hyoa Kang",
      "Yoonsuk Hyun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Choi_Multi-View_Reprojection_Architecture_for_Orientation_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Choi_Multi-View_Reprojection_Architecture_for_Orientation_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In autonomous driving scenarios, pose estimation of surrounding vehicles and other objects is required in decision making and planning. This paper proposes Multi-View Reprojection Architecture, a flexible, highly accurate architecture to adopt any 2D detection network and extend it to regress the orientation of object's 3D bounding box and its dimensions. In contrast to previous techniques, our network incorporates geometric constraints on 3D box imposed by 2D detection box. In particular, we regress dimensions, orientation and reprojected boxes in multi-view obtained from novel 3D reconstruction layer using perspective geometry. In 3D reconstruction layer, we use an iterative refinement strategy to accurately recover 3D boxes even when 2D boxes are truncated. The proposed architecture is shown to outperform state-of-the-art methods on the challenging KITTI car orientation benchmark and obtain top results on 3D detection benchmark while running in real time, making it suitable for autonomous vehicles.\r",
    "code_link": ""
  },
  "iccv2019_adw_advancedpedestriandatasetaugmentationforautonomousdriving": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Advanced Pedestrian Dataset Augmentation for Autonomous Driving",
    "authors": [
      "Antonin Vobecky",
      "Michal Uricar",
      "David Hurych",
      "Radoslav Skoviera"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Vobecky_Advanced_Pedestrian_Dataset_Augmentation_for_Autonomous_Driving_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Vobecky_Advanced_Pedestrian_Dataset_Augmentation_for_Autonomous_Driving_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Having the ability of generating people images in arbitrary, yet admissible, pose is a crucial prerequisite for Autonomous Driving applications. Firstly, because the existing datasets are quite limited in the human pose variation and appearance. Secondly, because the strict safety requirements call for the ability of validation on rare situations. Generating realistically looking people images is very challenging problem due to various transformations of individual body parts [2,6] self occlusions etc. We propose a novel approach for person image generation. Our approach allows generating people images in a required pose, indicated by specific pose keypoints and deals with occlusions. We build on top of the recent prevailing success of Generative Adversarial Networks [10]. Our contributions comprise of the networks architecture, as well as the novel loss terms specifically designed to generate visually appealing pedestrians fitting the surrounding environment well.\r",
    "code_link": ""
  },
  "iccv2019_adw_rotinvmtlrotationinvariantmultinetonfisheyeimagesforautonomousdrivingapplications": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "RotInvMTL: Rotation Invariant MultiNet on Fisheye Images for Autonomous Driving Applications",
    "authors": [
      "Bruno Arsenali",
      "Prashanth Viswanath",
      "Jelena Novosel"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Arsenali_RotInvMTL_Rotation_Invariant_MultiNet_on_Fisheye_Images_for_Autonomous_Driving_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Arsenali_RotInvMTL_Rotation_Invariant_MultiNet_on_Fisheye_Images_for_Autonomous_Driving_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Precise understanding of the scene around the car is of the utmost importance to achieve autonomous driving. Convolutional neural networks (CNNs) have been widely used for road scene understanding in the last few years with great success. Surround view (SV) systems with fisheye cameras have been in production in various cars and trucks for close to a decade. However, there are very few CNNs that are employed directly on SV systems due to the fisheye nature of its cameras. Typically, correction of fisheye distortion is applied to the data before it is processed by the CNNs, thereby increasing the system complexity and also reducing the field of view (FOV). In this paper, we propose RotInvMTL: a multi-task network (MTL) to perform joint semantic segmentation, boundary prediction, and object detection directly on raw fisheye images. We propose a rotation invariant object detection decoder that adapts to fisheye distortion and show that it outperforms YOLOv2 by 9% mAP. By combining the MTL outputs, an accurate foot-point information and a rough instance level segmentation may be obtained, both of which are critical for automotive applications. In conclusion, RotInvMTL is an efficient network that performs well for autonomous driving applications.\r",
    "code_link": ""
  },
  "iccv2019_adw_softprototypingcameradesignsforcardetectionbasedonaconvolutionalneuralnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Soft Prototyping Camera Designs for Car Detection Based on a Convolutional Neural Network",
    "authors": [
      "Zhenyi Liu",
      "Trisha Lian",
      "Joyce Farrell",
      "Brian Wandell"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Liu_Soft_Prototyping_Camera_Designs_for_Car_Detection_Based_on_a_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Liu_Soft_Prototyping_Camera_Designs_for_Car_Detection_Based_on_a_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Imaging systems are increasingly used as input to convolutional neural networks (CNN) for object detection; we would like to design cameras that are optimized for this purpose. It is impractical to build different cameras and then acquire and label the necessary data for every potential camera design; creating software simulations of the camera in context (soft prototyping) is the only realistic approach. We implemented soft-prototyping tools that can quantitatively simulate image radiance and camera designs to create realistic images that are input to a convolutional neural network for car detection. We used these methods to quantify the effect that critical hardware components (pixel size), sensor control (exposure algorithms) and image processing (gamma and demosaicing algorithms) have upon average precision of car detection. We quantify (a) the relationship between pixel size and the ability to detect cars at different distances, (b) the penalty for choosing a poor exposure duration, and (c) the ability of the CNN to perform car detection for a variety of post-acquisition processing algorithms. These results show that the optimal choices for car detection are not constrained by the same metrics used for image quality in consumer photography. It is better to evaluate camera designs for CNN applications using soft prototyping with task-specific metrics rather than consumer photography metrics.\r",
    "code_link": "https://github.com/iset/iset3d"
  },
  "iccv2019_adw_fusemodnetreal-timecameraandlidarbasedmovingobjectdetectionforrobustlow-lightautonomousdriving": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "FuseMODNet: Real-Time Camera and LiDAR Based Moving Object Detection for Robust Low-Light Autonomous Driving",
    "authors": [
      "Hazem Rashed",
      "Mohamed Ramzy",
      "Victor Vaquero",
      "Ahmad El Sallab",
      "Ganesh Sistu",
      "Senthil Yogamani"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Rashed_FuseMODNet_Real-Time_Camera_and_LiDAR_Based_Moving_Object_Detection_for_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Rashed_FuseMODNet_Real-Time_Camera_and_LiDAR_Based_Moving_Object_Detection_for_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Moving object detection is a critical task for autonomous vehicles. As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. Motion can be perceived using temporal information such as optical flow. Conventional optical flow computation is based on camera sensors only, which makes it prone to failure in conditions with low illumination. On the other hand, LiDAR sensors are independent of illumination, as they measure the time-of-flight of their own emitted lasers. In this work we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors. We demonstrate the impact of our algorithm on KITTI dataset where we simulate a low-light environment creating a novel dataset \"Dark-KITTI\". We obtain a 10.1 % relative improvement on Dark-KITTI, and a 4.25 % improvement on standard KITTI relative to our baselines. The proposed algorithm runs at 29 fps on a standard desktop GPU using 256x1224 resolution images.\r",
    "code_link": ""
  },
  "iccv2019_adw_fishyscapesabenchmarkforsafesemanticsegmentationinautonomousdriving": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Fishyscapes: A Benchmark for Safe Semantic Segmentation in Autonomous Driving",
    "authors": [
      "Hermann Blum",
      "Paul-Edouard Sarlin",
      "Juan Nieto",
      "Roland Siegwart",
      "Cesar Cadena"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Blum_Fishyscapes_A_Benchmark_for_Safe_Semantic_Segmentation_in_Autonomous_Driving_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Blum_Fishyscapes_A_Benchmark_for_Safe_Semantic_Segmentation_in_Autonomous_Driving_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep learning has enabled impressive progress in the accuracy of semantic segmentation. Yet, the ability to estimate uncertainty and detect anomalies is key for safety-critical applications like autonomous driving. Existing uncertainty estimates have mostly been evaluated on simple tasks, and it is unclear whether these methods generalize to more complex scenarios. We present Fishyscapes, the first public benchmark for uncertainty estimation in the real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects in front of the vehicle. We adapt state-of-the-art methods to recent semantic segmentation models and compare approaches based on softmax confidence, Bayesian learning, and embedding density. Our results show that anomaly detection is far from solved even for ordinary situations, while our benchmark allows measuring advancements beyond the state-of-the-art.\r",
    "code_link": ""
  },
  "iccv2019_adw_nads-netanimblearchitecturefordriverandseatbeltdetectionviaconvolutionalneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "NADS-Net: A Nimble Architecture for Driver and Seat Belt Detection via Convolutional Neural Networks",
    "authors": [
      "Sehyun Chun",
      "Nima Hamidi Ghalehjegh",
      "Joseph Choi",
      "Chris Schwarz",
      "John Gaspar",
      "Daniel McGehee",
      "Stephen Baek"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Chun_NADS-Net_A_Nimble_Architecture_for_Driver_and_Seat_Belt_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Chun_NADS-Net_A_Nimble_Architecture_for_Driver_and_Seat_Belt_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A new convolutional neural network (CNN) architecture for 2D driver/passenger pose estimation and seat belt detection is proposed in this paper. The new architecture is more nimble and thus more suitable for in-vehicle monitoring tasks compared to other generic pose estimation algorithms. The new architecture, named NADS-Net, utilizes the feature pyramid network (FPN) backbone with multiple detection heads to achieve the optimal performance for driver/passenger state detection tasks. The new architecture is validated on a new data set containing video clips of 100 drivers in 50 driving sessions that are collected for this study. The detection performance is analyzed under different demographic, appearance, and illumination conditions. The results presented in this paper may provide meaningful insights for the autonomous driving research community and automotive industry for future algorithm development and data collection.\r",
    "code_link": ""
  },
  "iccv2019_adw_oncontroltransitionsinautonomousdrivingaframeworkandanalysisforcharacterizingscenecomplexity": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "On Control Transitions in Autonomous Driving: A Framework and Analysis for Characterizing Scene Complexity",
    "authors": [
      "Nachiket Deo",
      "Nasha Meoli",
      "Akshay Rangesh",
      "Mohan Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Deo_On_Control_Transitions_in_Autonomous_Driving_A_Framework_and_Analysis_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Deo_On_Control_Transitions_in_Autonomous_Driving_A_Framework_and_Analysis_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " 'Take-overs' are safety critical events in conditionally autonomous vehicles. These are cases where vehicle control is transferred from the autonomous system to a human driver during failure modes of the system. Safe take-overs depend on two key factors; the readiness of the driver, and the complexity of the scene. While prior work has addressed driver readiness estimation, scene complexity estimation for control transitions remains an unexplored topic. In this paper, we focus on characterizing the complexity of driving scenes as perceived by human drivers during takeover events. To this end, we collect naturalistic driving data using a conditionally autonomous vehicle, equipped with cameras and LiDAR sensors. We mine a diverse set of scenarios using the LiDAR point cloud statistics. We then collect take-over complexity ratings in these scenarios assigned by raters with varying degrees of driving experience. We present an analysis of inter-rater agreement, and the average rated complexity conditioned on features of the surrounding environment, detected agents around the ego-vehicle, and ego-vehicle actions and motion states.\r",
    "code_link": ""
  },
  "iccv2019_adw_towardslearningmulti-agentnegotiationsviaself-play": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "Towards Learning Multi-Agent Negotiations via Self-Play",
    "authors": [
      "Yichuan Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Tang_Towards_Learning_Multi-Agent_Negotiations_via_Self-Play_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Tang_Towards_Learning_Multi-Agent_Negotiations_via_Self-Play_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Making sophisticated, robust, and safe sequential decisions is at the heart of intelligent systems. This is especially critical for planning in complex multi-agent environments, where agents need to anticipate other agents' intentions and possible future actions. Traditional methods formulate the problem as a Markov Decision Process, but the solutions often rely on various assumptions and become brittle when presented with corner cases. In contrast, deep reinforcement learning (Deep RL) has been very effective at finding policies by simultaneously exploring, interacting, and learning from environments. Leveraging the powerful Deep RL paradigm, we demonstrate that an iterative procedure of self-play can create progressively more diverse environments, leading to the learning of sophisticated and robust multi-agent policies. We demonstrate this in a challenging multi-agent simulation of merging traffic, where agents must interact and negotiate with others in order to successfully merge on or off the road. While the environment starts off simple, we increase its complexity by iteratively adding an increasingly diverse set of agents to the agent zoo as training progresses. Qualitatively, we find that through self-play, our policies automatically learn interesting behaviors such as defensive driving, overtaking, yielding, and the use of signal lights to communicate intentions to other agents. In addition, quantitatively, we show a dramatic improvement in the success rate of merging maneuvers from 63% to over 98%.\r",
    "code_link": ""
  },
  "iccv2019_adw_dbushumandrivingbehaviorunderstandingsystem": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ADW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Autonomous Driving",
    "title": "DBUS: Human Driving Behavior Understanding System",
    "authors": [
      "Max Guangyu Li",
      "Bo Jiang",
      "Zhengping Che",
      "Xuefeng Shi",
      "Mengyao Liu",
      "Yiping Meng",
      "Jieping Ye",
      "Yan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ADW/Li_DBUS_Human_Driving_Behavior_Understanding_System_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ADW/Li_DBUS_Human_Driving_Behavior_Understanding_System_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Human driving behavior understanding is a key ingredient for intelligent transportation systems. Either developing self-driving car drives like humans or building V2X systems to improve human driving experience, we need to understand how humans drive and interact with environments. Massive human driving data collected by top ride-sharing platforms and fleet management companies, offers the potential for in-depth understanding of human driving behavior. In this paper, we present DBUS, a real-time driving behavior understanding system which works with front-view videos, GPS/IMU signals collected from daily driving scenarios. Unlike previous work of driving behavior analysis, DBUS focuses on not only the recognition of basic driving actions but also the identification of driver's intentions and attentions. The analysis procedure is designed by mimicking the human intelligence for driving, powered with representation capability of deep neural networks as well as recent advances in visual perception, video temporal segmentation, attention mechanism, etc. Beyond systematic driving behavior analysis, DBUS also supports efficient behavior-based driving scenario search and retrieval, which is essential for practical application when working with large-scale human driving scenario dataset. We perform extensive evaluations of DBUS in term of inference accuracy of intentions, interpretability of inferred driver's attentions, as well as system efficiency. We also provide insightful intuitions as to why and how certain components work based on experience in the development of the system.\r",
    "code_link": ""
  },
  "iccv2019_lpcv_directfeedbackalignmentbasedconvolutionalneuralnetworktrainingforlow-poweronlinelearningprocessor": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "Direct Feedback Alignment Based Convolutional Neural Network Training for Low-Power Online Learning Processor",
    "authors": [
      "Donghyeon Han",
      "Hoi-jun Yoo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Han_Direct_Feedback_Alignment_Based_Convolutional_Neural_Network_Training_for_Low-Power_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Han_Direct_Feedback_Alignment_Based_Convolutional_Neural_Network_Training_for_Low-Power_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " There were many algorithms to substitute the backpropagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small. Furthermore, we examined the efficiency improvement by real chip implementation, and finally, DNN training accelerator with BDFA shows 35.3% lower power consumption compared with hardware which is optimized for BP.\r",
    "code_link": ""
  },
  "iccv2019_lpcv_efficientsingleimagesuper-resolutionviahybridresidualfeaturelearningwithcompactback-projectionnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "Efficient Single Image Super-Resolution via Hybrid Residual Feature Learning with Compact Back-Projection Network",
    "authors": [
      "Feiyang Zhu",
      "Qijun Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Zhu_Efficient_Single_Image_Super-Resolution_via_Hybrid_Residual_Feature_Learning_with_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Zhu_Efficient_Single_Image_Super-Resolution_via_Hybrid_Residual_Feature_Learning_with_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep learning methods have achieved state-of-the-art accuracy in single image super-resolution (SISR). Yet, how to achieve good balance between efficiency and accuracy in SISR is still an open issue. While most existing methods learn residual features only in low resolution (LR) space in order for higher efficiency, recent studies show that jointly learning residual features in LR and high resolution (HR) space is more preferred for accurate SISR. In this paper, we propose an efficient SISR method via learning hybrid residual features, based on which the residual HR image can be reconstructed. To fulfill hybrid residual feature learning, we propose a compact back-projection network that can simultaneously generate features in both LR and HR space by cascading up-and down-sampling layers with small-sized filters. Extensive experiments on four benchmark databases demonstrate that our proposed method can achieve high efficiency (i.e., small number of parameters and operations) while preserving state-of-the-art SR accuracy.\r",
    "code_link": ""
  },
  "iccv2019_lpcv_asystem-levelsolutionforlow-powerobjectdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "A System-Level Solution for Low-Power Object Detection",
    "authors": [
      "Fanrong Li",
      "Zitao Mo",
      "Peisong Wang",
      "Zejian Liu",
      "Jiayun Zhang",
      "Gang Li",
      "Qinghao Hu",
      "Xiangyu He",
      "Cong Leng",
      "Yang Zhang",
      "Jian Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Li_A_System-Level_Solution_for_Low-Power_Object_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Li_A_System-Level_Solution_for_Low-Power_Object_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Object detection has made impressive progress in recent years with the help of deep learning. However, state-of-the-art algorithms are both computation and memory intensive. Though many lightweight networks are developed for a trade-off between accuracy and efficiency, it is still a challenge to make it practical on an embedded device. In this paper, we present a system-level solution for efficient object detection on a heterogeneous embedded device. The detection network is quantized to low bits and allows efficient implementation with shift operators. In order to make the most of the benefits of low-bit quantization, we design a dedicated accelerator with programmable logic. Inside the accelerator, a hybrid dataflow is exploited according to the heterogeneous property of different convolutional layers. We adopt a straightforward but resource-friendly column-prior tiling strategy to map the computation-intensive convolutional layers to the accelerator that can support arbitrary feature size. Other operations can be performed on the low-power CPU cores, and the entire system is executed in a pipelined manner. As a case study, we evaluate our object detection system on a real-world surveillance video with input size of 512x512, and it turns out that the system can achieve an inference speed of 18 fps at the cost of 6.9W (with display) with an mAP of 66.4 verified on the PASCAL VOC 2012 dataset.\r",
    "code_link": ""
  },
  "iccv2019_lpcv_low-powerneuralnetworksforsemanticsegmentationofsatelliteimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "Low-Power Neural Networks for Semantic Segmentation of Satellite Images",
    "authors": [
      "Gaetan Bahl",
      "Lionel Daniel",
      "Matthieu Moretti",
      "Florent Lafarge"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Bahl_Low-Power_Neural_Networks_for_Semantic_Segmentation_of_Satellite_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Bahl_Low-Power_Neural_Networks_for_Semantic_Segmentation_of_Satellite_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Semantic segmentation methods have made impressive progress with deep learning. However, while achieving higher and higher accuracy, state-of-the-art neural networks overlook the complexity of architectures, which typically feature dozens of millions of trainable parameters. Consequently, these networks requires high computational resources and are mostly not suited to perform on edge devices with tight resource constraints, such as phones, drones, or satellites. In this work, we propose two highly compact neural network architectures for semantic segmentation of images, which are up to 100 000 times less complex than state-of-the-art architectures while approaching their accuracy. To decrease the complexity of existing networks, our main ideas consist in exploiting lightweight encoders and decoders with depth-wise separable convolutions and decreasing memory usage with the removal of skip connections between encoder and decoder. Our architectures are designed to be implemented on a basic FPGA such as the one featured on the Intel Altera Cyclone V family of SoCs. We demonstrate the potential of our solutions in the case of binary segmentation of remote sensing images, in particular for extracting clouds and trees from RGB satellite images.\r",
    "code_link": ""
  },
  "iccv2019_lpcv_enrichingvarietyoflayer-wiselearninginformationbygradientcombination": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "Enriching Variety of Layer-Wise Learning Information by Gradient Combination",
    "authors": [
      "Chien-Yao Wang",
      "Hong-Yuan Mark Liao",
      "Ping-Yang Chen",
      "Jun-Wei Hsieh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Wang_Enriching_Variety_of_Layer-Wise_Learning_Information_by_Gradient_Combination_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Wang_Enriching_Variety_of_Layer-Wise_Learning_Information_by_Gradient_Combination_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This study proposes to use the combination of gradient concept to enhance the learning capability of Deep Convolutional Networks (DCN), and four Partial Residual Networks-based (PRN-based) architectures are developed to verify above concept. The purpose of designing PRN is to provide as rich information as possible for each single layer. During the training phase, we propose to propagate gradient combinations rather than feature combinations. PRN can be easily applied in many existing network architectures, such as ResNet, feature pyramid network, etc., and can effectively improve their performance. Nowadays, more advanced DCNs are designed with the hierarchical semantic information of multiple layers, so the model will continue to deepen and expand. Due to the neat design of PRN, it can benefit all models, especially for lightweight models. In the MSCOCO object detection experiments, YOLO-v3-PRN maintains the same accuracy as YOLO-v3 with a 55% reduction of parameters and 35% reduction of computation, while increasing the speed of execution by twice. For lightweight models, YOLO-v3-tiny-PRN maintains the same accuracy under the condition of 37% less parameters and 38% less computation than YOLO-v3-tiny and increases the frame rate by up to 12 fps on the NVIDIA Jetson TX2 platform. The Pelee-PRN is 6.7% mAP@0.5 higher than Pelee, which achieves the state-of-the-art lightweight object detection. The proposed lightweight object detection model has been integrated with technologies such as multi-object tracking and license plate recognition, and it used in a commercial intelligent traffic flow analysis system as its edge computing equipment. There are already three countries and more than ten cities have deployed this technique into their traffic flow analysis systems.\r",
    "code_link": ""
  },
  "iccv2019_lpcv_real-timeobjectdetectiononlowpowerembeddedplatforms": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "Real-Time Object Detection On Low Power Embedded Platforms",
    "authors": [
      "George Jose",
      "Aashish Kumar",
      "Srinivas Kruthiventi S S",
      "Sambuddha Saha",
      "Harikrishna Muralidhara"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Jose_Real-Time_Object_Detection_On_Low_Power_Embedded_Platforms_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Jose_Real-Time_Object_Detection_On_Low_Power_Embedded_Platforms_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Low power real-time object detection is an interesting application in deep learning with applications in smart wearables, Advanced Driver Assistance Systems (ADAS), drone surveillance systems, etc. In this paper, we discuss the limitations with existing networks and enumerate the various factors to keep in mind while designing neural networks for a target hardware. Based on our experience of working with TI embedded platform, we provide a systematic approach for designing real time object detection networks on low power embedded platforms. First stage involves identifying the optimal layers for the hardware, by understanding it's computational and memory limitations. The next step is to use these layers to come up with a basic building block that has low computational complexity. The final stage involves using model compression techniques like sparsification/quantization to accelerate the inference process. Based on this design approach, we were able to come up with a low latency object detection model HX-LPNet that operates at 22 FPS on low power TDA2PX System on Chip(SoC) provided by Texas Instruments (TI)\r",
    "code_link": ""
  },
  "iccv2019_lpcv_512kibramisenough!livecamerafacerecognitiondnnonmcu": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU",
    "authors": [
      "Maxim Zemlyanikin",
      "Alexander Smorkalov",
      "Tatiana Khanova",
      "Anna Petrovicheva",
      "Grigory Serebryakov"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Small factor and ultra-low power devices are becoming more and more smart and capable even for deep learning network inference. And as the devices are \"small\", the challenge is becoming tougher. This paper covers full development and deployment pipeline of Face Recognition with a live camera - from model training and quantization to porting to RISC-V MCU with 512 kilobytes of internal RAM. Authors provide GreenWaves GAP8 SoC overview and the approaches for DNN model optimization and inference in the extreme environment. As the project outcome, authors were able to run Face Detection and Recognition with live QVGA camera and display preview on a battery-powered board.\r",
    "code_link": "https://github.com/xperience-ai/gap"
  },
  "iccv2019_lpcv_automatedmulti-stagecompressionofneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "Automated Multi-Stage Compression of Neural Networks",
    "authors": [
      "Julia Gusak",
      "Maksym Kholiavchenko",
      "Evgeny Ponomarev",
      "Larisa Markeeva",
      "Philip Blagoveschensky",
      "Andrzej Cichocki",
      "Ivan Oseledets"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Gusak_Automated_Multi-Stage_Compression_of_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Gusak_Automated_Multi-Stage_Compression_of_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Low-rank tensor approximations are very promising for compression of deep neural networks. We propose a new simple and efficient iterative approach, which alternates low-rank factorization with smart rank selection and fine-tuning. We demonstrate the efficiency of our method comparing to non-iterative ones. Our approach improves the compression rate while maintaining the accuracy for a variety of tasks.\r",
    "code_link": ""
  },
  "iccv2019_lpcv_on-deviceimageclassificationwithproxylessneuralarchitecturesearchandquantization-awarefine-tuning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LPCV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Low Power Computer Vision",
    "title": "On-Device Image Classification with Proxyless Neural Architecture Search and Quantization-Aware Fine-Tuning",
    "authors": [
      "Han Cai",
      "Tianzhe Wang",
      "Zhanghao Wu",
      "Kuan Wang",
      "Ji Lin",
      "Song Han"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " It is challenging to efficiently deploy deep learning models on resource-constrained hardware devices (e.g., mobile and IoT devices) with strict efficiency constraints (e.g., latency, energy consumption). We employ Proxyless Neural Architecture Search (ProxylessNAS) to auto design compact and specialized neural network architectures for the target hardware platform. ProxylessNAS makes latency differentiable, so we can optimize not only accuracy but also latency by gradient descent. Such direct optimization saves the search cost by 200x compared to conventional neural architecture search methods. Our work is followed by quantization-aware fine-tuning to further boost efficiency. In the Low Power Image Recognition Competition at CVPR'19, our solution won the 3rd place on the task of Real-Time Image Classification (online track).\r",
    "code_link": ""
  },
  "iccv2019_acvr_forcedspatialattentionfordriverfootactivityclassification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Forced Spatial Attention for Driver Foot Activity Classification",
    "authors": [
      "Akshay Rangesh",
      "Mohan Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Rangesh_Forced_Spatial_Attention_for_Driver_Foot_Activity_Classification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Rangesh_Forced_Spatial_Attention_for_Driver_Foot_Activity_Classification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper provides a simple solution for reliably solving image classification tasks tied to spatial locations of salient objects in the scene. Unlike conventional image classification approaches that are designed to be invariant to translations of objects in the scene, we focus on tasks where the output classes vary with respect to where an object of interest is situated within an image. To handle this variant of the image classification task, we propose augmenting the standard cross-entropy (classification) loss with a domain dependent Forced Spatial Attention (FSA) loss, which in essence compels the network to attend to specific regions in the image associated with the desired output class. To demonstrate the utility of this loss function, we consider the task of driver foot activity classification - where each activity is strongly correlated with where the driver's foot is in the scene. Training with our proposed loss function results in significantly improved accuracies, better generalization, and robustness against noise, while obviating the need for very large datasets.\r",
    "code_link": ""
  },
  "iccv2019_acvr_learningtonavigateroboticwheelchairsfromdemonstrationistraininginsimulationviable?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Learning to Navigate Robotic Wheelchairs from Demonstration: Is Training in Simulation Viable?",
    "authors": [
      "Mohammed Kutbi",
      "Yizhe Chang",
      "Bo Sun",
      "Philippos Mordohai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Kutbi_Learning_to_Navigate_Robotic_Wheelchairs_from_Demonstration_Is_Training_in_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Kutbi_Learning_to_Navigate_Robotic_Wheelchairs_from_Demonstration_Is_Training_in_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Learning from demonstration (LfD) enables robots to learn complex relationships between their state, perception and actions that are hard to express in an optimization framework. While people intuitively know what they would like to do in a given situation, they often have difficulty representing their decision process precisely enough to enable an implementation. Here, we are interested in robots that carry passengers, such as robotic wheelchairs, where user preferences, comfort and the feeling of safety are important for autonomous navigation. Balancing these requirements is not straightforward. While robots can be trained in an LfD framework in which users drive the robot according to their preferences, performing these demonstrations can be time-consuming, expensive, and possibly dangerous. Inspired by recent efforts for generating synthetic data for training autonomous driving systems, we investigate whether it is possible to train a robot based on simulations to reduce the time requirements, cost and potential risk. A key characteristic of our approach is that the input is not images, but the locations of people and obstacles relative to the robot. We argue that this allows us to transfer the classifier from the simulator to the physical world and to previously unseen environments that do not match the appearance of the training set. Experiments with 14 subjects providing physical and simulated demonstrations validate our claim.\r",
    "code_link": ""
  },
  "iccv2019_acvr_deeplearningperformanceinthepresenceofsignificantocclusions-anintelligenthouseholdrefrigeratorcase": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Deep Learning Performance in the Presence of Significant Occlusions - An Intelligent Household Refrigerator Case",
    "authors": [
      "Gregor Koporec",
      "Janez Pers"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Koporec_Deep_Learning_Performance_in_the_Presence_of_Significant_Occlusions_-_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Koporec_Deep_Learning_Performance_in_the_Presence_of_Significant_Occlusions_-_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Real-world environments, inhabited by people, still pose significant challenges to deep learning methods. Object occlusion is one of such problems. Humans deal with the occlusion in a complex way, by changing the viewpoint and using hands to manipulate the scene. However, not all robotic systems can do that due to cost or design constraints. The question we address in this paper is, how well modern object detection methods work on a model case of an intelligent household refrigerator, where numerous occlusions occur. To motivate our research, we actually performed a worldwide survey of refrigerator occupancy to realistically judge the extent of the problem, but the results could be generalized to any unstructured storage environment where people are in charge. The survey results enabled us to generate a dataset of photo-realistic renderings of a typical refrigerator interior, where the object identity, location, and the degree of the refrigerator occupancy are all readily available. Our results are represented as the Average Precision depending on a refrigerator occupancy for two well known deep models.\r",
    "code_link": "https://github.com/AlexeyAB/darknet"
  },
  "iccv2019_acvr_salientcontour-awarebasedtwicelearningstrategyforsaliencydetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Salient Contour-Aware Based Twice Learning Strategy for Saliency Detection",
    "authors": [
      "Chunbiao Zhu",
      "Wei Yan",
      "Shan Liu",
      "Thomas Li",
      "Ge Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Zhu_Salient_Contour-Aware_Based_Twice_Learning_Strategy_for_Saliency_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Zhu_Salient_Contour-Aware_Based_Twice_Learning_Strategy_for_Saliency_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Fully convolutional neural networks (FCNs) have shown outstanding performance in many computer vision tasks including salient object detection. However, most deep learning-based saliency detection models are too complicated. They cause difficulties in training. Additionally, the performance of those overly complex deep learning models is limited, and the price performance ratio of those complex models is very low. To address the problems of existing deep-learning-based methods, we introduce a new research field called saliency contour detection and design a new dataset for saliency contour detection. Inspired by the human sketching process, we propose a novel contour-aware algorithm using FCNs with a twice learning strategy for saliency detection, which imitates and dissects the process of human cognition. Extensive experimental evaluations demonstrate the effectiveness of our proposed method against other outstanding methods.\r",
    "code_link": "https://github.com/YanWei123/Salient-contour-aware-based-twicelearning-strategy-for-saliency-detection"
  },
  "iccv2019_acvr_deeplearningbasedwearableassistivesystemforvisuallyimpairedpeople": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Deep Learning Based Wearable Assistive System for Visually Impaired People",
    "authors": [
      "Yimin Lin",
      "Kai Wang",
      "Wanxin Yi",
      "Shiguo Lian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Lin_Deep_Learning_Based_Wearable_Assistive_System_for_Visually_Impaired_People_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Lin_Deep_Learning_Based_Wearable_Assistive_System_for_Visually_Impaired_People_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose a deep learning based assistive system to improve the environment perception experience of visually impaired (VI). The system is composed of a wearable terminal equipped with an RGBD camera and an earphone, a powerful processor mainly for deep learning inferences and a smart phone for touch-based interaction. A data-driven learning approach is proposed to predict safe and reliable walkable instructions using RGBD data and the established semantic map. This map is also used to help VI understand their 3D surrounding objects and layout through well-designed touchscreen interactions. The quantitative and qualitative experimental results show that our learning based obstacle avoidance approach achieves excellent results in both indoor and outdoor datasets with low-lying obstacles. Meanwhile, user studies have also been carried out in various scenarios and showed the improvement of VI's environment perception experience with our system.\r",
    "code_link": ""
  },
  "iccv2019_acvr_dynamicsubtitlesamultimodalvideoaccessibilityenhancementdedicatedtodeafandhearingimpairedusers": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Dynamic Subtitles: A Multimodal Video Accessibility Enhancement Dedicated to Deaf and Hearing Impaired Users",
    "authors": [
      "Ruxandra Tapu",
      "Bogdan Mocanu",
      "Titus Zaharia"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Tapu_Dynamic_Subtitles_A_Multimodal_Video_Accessibility_Enhancement_Dedicated_to_Deaf_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Tapu_Dynamic_Subtitles_A_Multimodal_Video_Accessibility_Enhancement_Dedicated_to_Deaf_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we introduce a novel dynamic subtitle positioning system designed to increase the accessibility of the deaf and hearing impaired people to video documents. Our framework places the subtitle in the near vicinity of the active speaker in order to allow the viewer to follow the visual content while regarding the textual information. The proposed system is based on a multimodal fusion of text, audio and visual information in order to detect and recognize the identity of the active speaker. The experimental evaluation, performed on a large dataset of more than 30 videos, validates the methodology with average accuracy and recognition rates superior to 92%. The subjective evaluation demonstrates the effectiveness of our approach outperforming both conventional (static) subtitling and other state of the art techniques in terms of enhancement of the overall viewing experience and eyestrain reduction.\r",
    "code_link": ""
  },
  "iccv2019_acvr_socialandscene-awaretrajectorypredictionincrowdedspaces": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Social and Scene-Aware Trajectory Prediction in Crowded Spaces",
    "authors": [
      "Matteo Lisotto",
      "Pasquale Coscia",
      "Lamberto Ballan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Lisotto_Social_and_Scene-Aware_Trajectory_Prediction_in_Crowded_Spaces_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Lisotto_Social_and_Scene-Aware_Trajectory_Prediction_in_Crowded_Spaces_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Mimicking human ability to forecast future positions or interpret complex interactions in urban scenarios, such as streets, shopping malls or squares, is essential to develop socially compliant robots or self-driving cars. Autonomous systems may gain advantage on anticipating human motion to avoid collisions or to naturally behave alongside people. To foresee plausible trajectories, we construct an LSTM (long short-term memory)-based model considering three fundamental factors: people interactions, past observations in terms of previously crossed areas and semantics of surrounding space. Our model encompasses several pooling mechanisms to join the above elements defining multiple tensors, namely social, navigation and semantic tensors. The network is tested in unstructured environments where complex paths emerge according to both internal (intentions) and external (other people, not accessible areas) motivations. As demonstrated, modeling paths unaware of social interactions or context information, is insufficient to correctly predict future positions. Experimental results corroborate the effectiveness of the proposed framework in comparison to LSTM-based models for human path prediction.\r",
    "code_link": ""
  },
  "iccv2019_acvr_arealisticface-to-faceconversationsystembasedondeepneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "A Realistic Face-to-Face Conversation System Based on Deep Neural Networks",
    "authors": [
      "Zezhou Chen",
      "Zhaoxiang Liu",
      "Huan Hu",
      "Jinqiang Bai",
      "Shiguo Lian",
      "Fuyuan Shi",
      "Kai Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Chen_A_Realistic_Face-to-Face_Conversation_System_Based_on_Deep_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Chen_A_Realistic_Face-to-Face_Conversation_System_Based_on_Deep_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " To improve the experiences of face-to-face conversation with avatar, this paper presents a novel conversation system. It is composed of two sequence-to-sequence models respectively for listening and speaking and a Generative Adversarial Network (GAN) based realistic avatar synthesizer. The models exploit the facial action and head pose to learn natural human reactions. Based on the models' output, the synthesizer uses the Pixel2Pixel model to generate realistic facial images. To show the improvement of our system, we use a 3D model based avatar driving scheme as a reference. We train and evaluate our neural networks with the data from ESPN shows. Experimental results show that our conversation system can generate natural facial reactions and realistic facial images.\r",
    "code_link": ""
  },
  "iccv2019_acvr_objectcaptioningandretrievalwithnaturallanguage": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Object Captioning and Retrieval with Natural Language",
    "authors": [
      "Anh Nguyen",
      "Quang D. Tran",
      "Thanh-Toan Do",
      "Ian Reid",
      "Darwin G. Caldwell",
      "Nikos G. Tsagarakis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Nguyen_Object_Captioning_and_Retrieval_with_Natural_Language_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Nguyen_Object_Captioning_and_Retrieval_with_Natural_Language_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We address the problem of jointly learning vision and language to understand the object in a fine-grained manner. The key idea of our approach is the use of object descriptions to provide the detailed understanding of an object. Based on this idea, we propose two new architectures to solve two related problems: object captioning and natural language-based object retrieval. The goal of the object captioning task is to simultaneously detect the object and generate its associated description, while in the object retrieval task, the goal is to localize an object given an input query. We demonstrate that both problems can be solved effectively using hybrid end-to-end CNN-LSTM networks. The experimental results on our new challenging dataset show that our methods outperform recent methods by a fair margin, while providing a detailed understanding of the object and having fast inference time. The source code will be made available.\r",
    "code_link": ""
  },
  "iccv2019_acvr_streetcrossingaidusinglight-weightcnnsforthevisuallyimpaired": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Street Crossing Aid Using Light-Weight CNNs for the Visually Impaired",
    "authors": [
      "Samuel Yu",
      "Heon Lee",
      "Junghoon Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Yu_Street_Crossing_Aid_Using_Light-Weight_CNNs_for_the_Visually_Impaired_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Yu_Street_Crossing_Aid_Using_Light-Weight_CNNs_for_the_Visually_Impaired_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we address an issue that the visually impaired commonly face while crossing intersections and propose a solution that takes form as a mobile application. The application utilizes a deep learning convolutional neural network model, LytNetV2, to output necessary information that the visually impaired may lack when without human companions or guide-dogs. A prototype of the application runs on iOS devices of versions 11 or above. It is designed for comprehensiveness, concision, accuracy, and computational efficiency through delivering the two most important pieces of information, pedestrian traffic light color and direction, required to cross the road in real-time. Furthermore, it is specifically aimed to support those facing financial burden as the solution takes the form of a free mobile application. Through the modification and utilization of key principles in MobileNetV3 such as depthwise seperable convolutions and squeeze-excite layers, the deep neural network model achieves a classification accuracy of 96% and average angle error of 6.15 degree, while running at a frame rate of 16.34 frames per second. Additionally, the model is trained as an image classifier, allowing for a faster and more accurate model. The network is able to outperform other methods such as object detection and non-deep learning algorithms in both accuracy and thoroughness. The information is delivered through both auditory signals and vibrations, and it has been tested on seven visually impaired and has received above satisfactory responses.\r",
    "code_link": ""
  },
  "iccv2019_acvr_active3dclassificationofmultipleobjectsinclutteredscenes": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Active 3D Classification of Multiple Objects in Cluttered Scenes",
    "authors": [
      "Yiming Wang",
      "Marco Carletti",
      "Francesco Setti",
      "Marco Cristani",
      "Alessio Del Bue"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Wang_Active_3D_Classification_of_Multiple_Objects_in_Cluttered_Scenes_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Wang_Active_3D_Classification_of_Multiple_Objects_in_Cluttered_Scenes_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Autonomous agents that need to effectively move and interact in a realistic environment have to be endowed with robust perception skills. Among many, accurate object classification is an essential supporting element for assistive robotics. However, realistic scenarios often present scenes with severe clutter, that dramatically degrades the performance of current object classification methods. This paper presents an active vision approach that improves the accuracy of 3D object classification through a next-best-view (NBV) paradigm to perform this complex task with ease. The next camera motion is chosen with the criteria that aim to avoid object self-occlusions while exploring as much as possible the surrounding area. An online 3D reconstruction module is exploited in our system in order to obtain a better canonical 3D representation of the scene while moving the sensor. By reducing the impact of occlusions, we show with both synthetic and real-world data that in a few moves the approach can surpass a state-of-the-art method, PointNet with single view object classification from depth data. In addition, we demonstrate our system in a practical scenario where depth sensor moves to search and classify a set of objects in cluttered scenes.\r",
    "code_link": ""
  },
  "iccv2019_acvr_videoindexingusingfaceappearanceandshottransitiondetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Video Indexing Using Face Appearance and Shot Transition Detection",
    "authors": [
      "Dario Cazzato",
      "Marco Leo",
      "Pierluigi Carcagni",
      "Cosimo Distante",
      "Javier Lorenzo-Navarro",
      "Holger Voos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Cazzato_Video_Indexing_Using_Face_Appearance_and_Shot_Transition_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Cazzato_Video_Indexing_Using_Face_Appearance_and_Shot_Transition_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The possibility to automatically index human faces in videos could lead to a wide range of applications such as automatic video content analysis, data mining, on-demand streaming, etc. Most relevant works in the literature gather full indexing of videos in real scenarios by exploiting additional media features (e.g. audio and text) that are fused with facial appearance information to make the whole frameworks accurate and robust. Anyway, there exist some application contexts where multimedia data are either not available or reliable and for which available solutions are not well suited. This paper tries to explore this challenging research path by introducing a new fully computer vision based video indexing pipeline. The system has been validated and tested in two different typical scenarios where no-multimedia data could be exploited: broadcasted political video documentaries and healthcare therapies sessions about non-verbal skills.\r",
    "code_link": ""
  },
  "iccv2019_acvr_home-basedphysicaltherapywithaninteractivecomputervisionsystem": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Home-Based Physical Therapy with an Interactive Computer Vision System",
    "authors": [
      "Yiwen Gu",
      "Shreya Pandit",
      "Elham Saraee",
      "Timothy Nordahl",
      "Terry Ellis",
      "Margrit Betke"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Gu_Home-Based_Physical_Therapy_with_an_Interactive_Computer_Vision_System_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Gu_Home-Based_Physical_Therapy_with_an_Interactive_Computer_Vision_System_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we present ExerciseCheck. ExerciseCheck is an interactive computer vision system that is sufficiently modular to work with different sources of human pose estimates, i.e., estimates from deep or traditional models that interpret RGB or RGB-D camera input. In a pilot study, we first compare the pose estimates produced by four deep models based on RGB input with those of the MS Kinect based on RGB-D data. The results indicate a performance gap that required us to choose the MS Kinect when we tested ExerciseCheck with Parkinson's disease patients in their homes. ExerciseCheck is capable of customizing exercises, capturing exercise information, evaluating patient performance, providing therapeutic feedback to the patient and the therapist, checking the progress of the user over the course of the physical therapy, and supporting the patient throughout this period. We conclude that ExerciseCheck is a user-friendly computer vision application that can assist patients by providing motivation and guidance to en-sure correct execution of the required exercises. Our re-sults also suggest that while there has been considerable progress in the field of pose estimation using deep learning, current deep learning models are not fully ready to replace RGB-D sensors, especially when the exercises involved are complex, and the patient population being accounted for has to be carefully tracked for its \"active range of motion.\"\r",
    "code_link": ""
  },
  "iccv2019_acvr_jointtrajectoryandfatigueanalysisinwheelchairusers": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "ACVR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Assistive Computer Vision and Robotics",
    "title": "Joint Trajectory and Fatigue Analysis in Wheelchair Users",
    "authors": [
      "Maddalena Sebastiani",
      "Nicola Garau",
      "Francesco De Natale",
      "Nicola Conci"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/ACVR/Sebastiani_Joint_Trajectory_and_Fatigue_Analysis_in_Wheelchair_Users_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/ACVR/Sebastiani_Joint_Trajectory_and_Fatigue_Analysis_in_Wheelchair_Users_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A successful rehabilitation process always requires both medical and infrastructural support. In this paper we focus on paraplegic wheelchair users, aiming at understanding the correlation between accuracy in guidance and muscular fatigue, while moving on a known training path. In particular, we study the trajectories performed and the corresponding shoulder forces distribution, while changing the inclination of the seat. At the motor level, the point of interest is the shoulder, as, in the prolonged use of wheelchairs with manual self-propulsion, it is generally source of pain. The objective is to demonstrate that there is a potential relationship between trajectory discontinuities and shoulder pain, and foster the development of best practices aimed at preventing the raise of shoulder-related pathologies, by correcting the user's movements and the wheelchair setup. This project is meant to be a first study of the subject, so far little addressed, and is not meant to be a clinical study. The experiments have been conducted within the premises of the Living Lab AUSILIA and validated with the help of experienced medical personnel.\r",
    "code_link": ""
  },
  "iccv2019_lsr_lightweightfacerecognitionchallenge": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "Lightweight Face Recognition Challenge",
    "authors": [
      "Jiankang Deng",
      "Jia Guo",
      "Debing Zhang",
      "Yafeng Deng",
      "Xiangju Lu",
      "Song Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Deng_Lightweight_Face_Recognition_Challenge_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Deng_Lightweight_Face_Recognition_Challenge_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Face representation using Deep Convolutional Neural Network (DCNN) embedding is the method of choice for face recognition. Current state-of-the-art face recognition systems can achieve high accuracy on existing in-the-wild datasets. However, most of these datasets employ quite limited comparisons during the evaluation, which does not simulate a real-world scenario, where extensive comparisons are encountered by a face recognition system. To this end, we propose two large-scale datasets (DeepGlint-Image with 1.8M images and IQIYI-Video with 0.2M videos) and define an extensive comparison metric (trillion-level pairs on the DeepGlint-Image dataset and billion-level pairs on the IQIYI-Video dataset) for an unbiased evaluation of deep face recognition models. To ensure fair comparison during the competition, we define light-model track and large-model track, respectively. Each track has strict constraints on computational complexity and model size. To the best of our knowledge, this is the most comprehensive and unbiased benchmarks for deep face recognition. To facilitate future research, the proposed datasets are released and the online test server is accessible as part of the Lightweight Face Recognition Challenge at the International Conference on Computer Vision, 2019.\r",
    "code_link": ""
  },
  "iccv2019_lsr_vargfacenetanefficientvariablegroupconvolutionalneuralnetworkforlightweightfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition",
    "authors": [
      "Mengjia Yan",
      "Mengao Zhao",
      "Zining Xu",
      "Qian Zhang",
      "Guoli Wang",
      "Zhizhong Su"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Yan_VarGFaceNet_An_Efficient_Variable_Group_Convolutional_Neural_Network_for_Lightweight_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Yan_VarGFaceNet_An_Efficient_Variable_Group_Convolutional_Neural_Network_for_Lightweight_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " To improve the discriminative and generalization ability of lightweight network for face recognition, we propose an efficient variable group convolutional network called VarGFaceNet. Variable group convolution is introduced by VarGNet to solve the conflict between small computational cost and the unbalance of computational intensity inside a block. We employ variable group convolution to design our network which can support large scale face identification while reduce computational cost and parameters. Specifically, we use a head setting to reserve essential information at the start of the network and propose a particular embedding setting to reduce parameters of fully-connected layer for embedding. To enhance interpretation ability, we employ an equivalence of angular distillation loss to guide our lightweight network and we apply recursive knowledge distillation to relieve the discrepancy between the teacher model and the student model. The champion of deepglint-light track of LFR (2019) challenge demonstrates the effectiveness of our model and approach. Implementation of VarGFaceNet will be released at https://github.com/zma-c-137/VarGFaceNet soon.\r",
    "code_link": "https://github.com/zma-c137/VarGFaceNet"
  },
  "iccv2019_lsr_improvedknowledgedistillationfortrainingfastlowresolutionfacerecognitionmodel": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "Improved Knowledge Distillation for Training Fast Low Resolution Face Recognition Model",
    "authors": [
      "Mengjiao Wang",
      "Rujie Liu",
      "Nada Hajime",
      "Abe Narishige",
      "Hidetsugu Uchida",
      "Tomoaki Matsunami"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Wang_Improved_Knowledge_Distillation_for_Training_Fast_Low_Resolution_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Wang_Improved_Knowledge_Distillation_for_Training_Fast_Low_Resolution_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Low resolution (LR) face recognition (FR) is a challenging, yet common problem for FR task, especially for surveillance scenario. The issue addressed here is not just to build a LR-FR model, more importantly to make it run fast. Here, the knowledge distillation method is adopted for our task, where the teacher's knowledge can be 'distilled' into a small student model by guiding its training process. For LRFR task, the original knowledge distillation scheme would update the teacher's weights first by tuning it using LR augmented train set, and then the student model is trained using same train set under updated teacher's guidance. The problem of this method is that the weights tuning of large teacher model is time-consuming, especially for large-scale dataset. In this paper, we proposed an improved scheme to enable us to avoid the teacher retraining and still be able to train the small model for LR-FR task. Here, different from the original scheme, the train sets for teacher and student model become different, where the train set for teacher model keeps unchanged and the one student is LR augmented. Therefore, it becomes unnecessary to update teacher model any more since the train set is the unchanged. Only the small student model needs to be trained under the original teacher's guidance. This can speed up the whole training process, especially for large-scale dataset. The different train sets for teacher and student will increase the data distribution discrepancy. To solve this problem, we constrained the multikernel maximum mean discrepancy between outputs to reduce this influence. Experimental results show our method can accelerate the training process by about 5 times, while preserving the accuracy. Our student model has same level coresponding author: wangmengjiao@cn.fujitsu.com with respect to state-of-art accuracy on LFW and SCFace. It can achieve 3x acceleration comparing to teacher model and only takes 35ms to run on a CPU.\r",
    "code_link": ""
  },
  "iccv2019_lsr_unknownidentityrejectionlossutilizingunlabeleddataforfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "Unknown Identity Rejection Loss: Utilizing Unlabeled Data for Face Recognition",
    "authors": [
      "Haiming Yu",
      "Yin Fan",
      "Keyu Chen",
      "He Yan",
      "Xiangju Lu",
      "Junhui Liu",
      "Danming Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Yu_Unknown_Identity_Rejection_Loss_Utilizing_Unlabeled_Data_for_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Yu_Unknown_Identity_Rejection_Loss_Utilizing_Unlabeled_Data_for_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Face recognition has advanced considerably with the availability of large-scale labeled datasets. However, how to further improve the performance with the easily accessible unlabeled dataset remains a challenge. In this paper, we propose the novel Unknown Identity Rejection (UIR) loss to utilize the unlabeled data. We categorize identities in unconstrained environment into the known set and the unknown set. The former corresponds to the identities that appear in the labeled training dataset while the latter is its complementary set. Besides training the model to accurately classify the known identities, we also force the model to reject unknown identities provided by the unlabeled dataset via our proposed UIR loss. In order to 'reject' faces of unknown identities, centers of the known identities are forced to keep enough margin from centers of unknown identities which are assumed to be approximated by the features of their samples. By this means, the discriminativeness of the face representations can be enhanced. Experimental results demonstrate that our approach can provide obvious performance improvement by utilizing the unlabeled data.\r",
    "code_link": "https://github.com/deepinsight/insightface"
  },
  "iccv2019_lsr_geometryguidedfeatureaggregationinvideofacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "Geometry Guided Feature Aggregation in Video Face Recognition",
    "authors": [
      "Baoyun Peng",
      "Xiao Jin",
      "Yichao Wu",
      "Dongsheng Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Peng_Geometry_Guided_Feature_Aggregation_in_Video_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Peng_Geometry_Guided_Feature_Aggregation_in_Video_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video-based face recognition has attracted a significant amount of research interest in both academia and industry due to its wide applications such as surveillance and security. Different from image-based face recognition, abundant information, extracted from a series of frames in a video, would contribute a lot to successful recognition. In other words, the key to improving video face recognition capability is aggregating and integrating profuse information within a video. Existing methods of feature aggregation across frames narrowly focus on the importance of a single frame, while ignoring the geometric relationship among frames in feature space. In this work, we present a geometry-based feature aggregation method rather than a better recognition model. It considers not only the importance of each frame but also the geometric relationship among frames in feature space, which yields more distinguishing video-level representation. Extensive evaluations on IJB-A and YTF datasets indicate that the proposed aggregation method considerably outperforms other feature aggregation methods.\r",
    "code_link": ""
  },
  "iccv2019_lsr_airfacelightweightandefficientmodelforfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "AirFace: Lightweight and Efficient Model for Face Recognition",
    "authors": [
      "Xianyang Li",
      "Feng Wang",
      "Qinghao Hu",
      "Cong Leng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Li_AirFace_Lightweight_and_Efficient_Model_for_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Li_AirFace_Lightweight_and_Efficient_Model_for_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the development of convolutional neural network, significant progress has been made in computer vision tasks. However, the commonly used loss function softmax loss and highly efficient network architectures for common visual tasks are not as effective for face recognition. In this paper, we propose a novel loss function named Li-ArcFace based on ArcFace. Li-ArcFace takes the value of the angle through a linear function as the target logit rather than through cosine function, which has better convergence and performance on low dimensional embedding feature learning for face recognition. In terms of network architecture, we improved the the perfomance of MobileFaceNet by increasing the network depth, width and adding attention module. Besides, we found some useful training tricks for face recognition. Under all the above effects, we won the second place in the deepglint-light challenge of LFR2019.\r",
    "code_link": "https://github.com/deepinsight/insightface"
  },
  "iccv2019_lsr_effectivemethodsforlightweightimage-basedandvideo-basedfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "Effective Methods for Lightweight Image-Based and Video-Based Face Recognition",
    "authors": [
      "Yidong Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Ma_Effective_Methods_for_Lightweight_Image-Based_and_Video-Based_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Ma_Effective_Methods_for_Lightweight_Image-Based_and_Video-Based_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Face recognition has achieved significant advances with the rise of deep convolutional neural networks (CNNs) and the development of large annotated datasets. However, how to design deep models in lightweight face recognition is still a challenge when aiming at mobile and embedded devices. In this paper, we focus on recent efficient CNN architectures, speedup skills and reduction methods to design models for lightweight face recognition. We combine octave convolution with MobileNet and ResNet for those models sensitive to computation complexity, replace feature output layer for those models sensitive to memory and explore network scaling for more powerful representation. Further, we extract a subset from the whole training dataset to speed up the performance evaluation of different models. We provide a scaling method on MobileFaceNet to boost the performance with the limit of computational cost, and propose a simple supplementary method for average pooling which throws up those noise frames based on the cluster information in video face recognition. With the upper bound of 1G FLOPs computation complexity and 20MB model size, our best model achieves 99.80% accuracy on LFW, 98.48% on AgeDB, 98% on CFP-FP and 97.67% TAR@FAR 10^6 on MegaFace.\r",
    "code_link": ""
  },
  "iccv2019_lsr_factorizingandreconstitutinglarge-kernelmbconvforlightweightfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "Factorizing and Reconstituting Large-Kernel MBConv for Lightweight Face Recognition",
    "authors": [
      "Yaqi Lyu",
      "Jing Jiang",
      "Kun Zhang",
      "Yilun Hua",
      "Miao Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Lyu_Factorizing_and_Reconstituting_Large-Kernel_MBConv_for_Lightweight_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Lyu_Factorizing_and_Reconstituting_Large-Kernel_MBConv_for_Lightweight_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In the past few years, Neural Architecture Search (NAS) has exhibited remarkable advances in terms of neural architecture design, especially on mobile devices. NAS normally use hand-craft MBConv as building block. However, they mainly searched for block-related hyperparameters, and the structure of MBConv itself was largely overlooked. This paper investigates that factorization and reconstitution can promote the efficiency of large-kernel MBConv and thus proposes FR-MBConv (Factorizing and Reconstituting large-kernel MBConv). Compared to large-kernel MBConv with the same receptive field, our FR-MBConv has fewer number of parameters and less computational cost, dramatically increased depth and nonlinearity. In addition, from the perspective of feature generation mechanism, FR-MBConv can be equivalent to more regular convolutions. We combine FR-MBConv with MobileNetV3 to build a lightweight face recognition model. Extensive experiments on face recognition benchmark demonstrate that our lightweight face recognition model outperforms the state-of-the-art lightweight model. Even on large scale face recognition benchmark IJB-B, IJB-C and MegaFace, our lightweight model also achieves comparable performance with large models.\r",
    "code_link": "https://github.com/deepinsight/insightface"
  },
  "iccv2019_lsr_towardsflops-constrainedfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "Towards Flops-Constrained Face Recognition",
    "authors": [
      "Yu Liu",
      "guanglu song",
      "manyuan zhang",
      "jihao liu",
      "yucong zhou",
      "junjie yan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Liu_Towards_Flops-Constrained_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Liu_Towards_Flops-Constrained_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Large scale face recognition is challenging especially when the computational budget is limited. Given a flops upper bound, the key is to find the optimal neural network architecture and optimization method. In this article, we introduce the solutions of team 'trojans' for the ICCV19 - Lightweight Face Recognition Challenge. Our team mainly focuses on the two 'large' tracks, image-based and video-based, respectively. The submissions of these two tracks are required to be one single model with computational budget no higher than 30 GFlops. We introduce a network architecture 'Efficient PolyFace', a novel loss function 'ArcNegFace', a novel frame aggregation method 'QAN++', together with a bag of useful tricks in our implementation (augmentations, regular face, label smoothing, anchor finetuning, etc.). Our basic model, 'Efficient PolyFace', takes 28.25 Gflops for the 'deepglint-large' image-based track, and the 'PolyFace+QAN++' solution takes 24.12 Gflops for the 'iQiyi-large' video-based track. These two solutions achieve 94.198% @ 1e-8 and 72.981% @ 1e-4 in the two tracks respectively, which are the state-of-the-art results.\r",
    "code_link": ""
  },
  "iccv2019_lsr_aprogressivelearningframeworkforunconstrainedfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "A Progressive Learning Framework for Unconstrained Face Recognition",
    "authors": [
      "Zhenhua Chai",
      "Shengxi Li",
      "Huanhuan Meng",
      "Shenqi Lai",
      "Xiaoming Wei",
      "Jianwei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Chai_A_Progressive_Learning_Framework_for_Unconstrained_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Chai_A_Progressive_Learning_Framework_for_Unconstrained_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The carefully designed backbone network, the increase of training data and the improved training skills have boosted the performance of modern face recognition systems. However, in some deployment cases which aim at model compactness and energy efficiency, some of the existing systems may fail due to the high complexity. Lightweight Face Recognition Challenge is proposed in order to make some progress in this direction and establishes a new comprehensive benchmark. In this challenge, we have designed a light weight backbone architecture and all the parameters are trained in a progressive way. Finally we achieve the 5th in track 1 and the 4th in track 3.\r",
    "code_link": ""
  },
  "iccv2019_lsr_agraphbasedunsupervisedfeatureaggregationforfacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "A Graph Based Unsupervised Feature Aggregation for Face Recognition",
    "authors": [
      "Yu Cheng",
      "Yanfeng Li",
      "Qiankun Liu",
      "Yuan Yao",
      "Venkata Sai Vijay Kumar Pedapudi",
      "Xiaotian Fan",
      "Chi Su",
      "Shengmei Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Cheng_A_Graph_Based_Unsupervised_Feature_Aggregation_for_Face_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Cheng_A_Graph_Based_Unsupervised_Feature_Aggregation_for_Face_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In most of the testing dataset, the images are collected from video clips or different environment conditions, which implies that the mutual information between pairs are significantly important. To address this problem and utilize this information, in this paper, we propose a graph-based unsupervised feature aggregation method for face recognition. Our method uses the inter-connection between pairs with a directed graph approach thus refine the pair-wise scores. First, based on the assumption that all features follow Gaussian distribution, we derive a iterative updating formula of features. Second, in discrete conditions, we build a directed graph where the affinity matrix is obtained from pair-wise similarities, and filtered by a pre-defined threshold along with K-nearest neighbor. Third, the affinity matrix is used to obtain a pseudo center matrix for the iterative update process. Besides evaluation on face recognition testing dataset, our proposed method can further be applied to semi-supervised learning to handle the unlabelled data for improving the performance of the deep models. We verified the effectiveness on 5 different datasets: IJB-C, CFP, YTF, TrillionPair and IQiYi Video dataset.\r",
    "code_link": ""
  },
  "iccv2019_lsr_shufflefacenetalightweightfacearchitectureforefficientandhighly-accuratefacerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LSR",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Lightweight Face Recognition Challenge",
    "title": "ShuffleFaceNet: A Lightweight Face Architecture for Efficient and Highly-Accurate Face Recognition",
    "authors": [
      "Yoanna Martindez-Diaz",
      "Luis S. Luevano",
      "Heydi Mendez-Vazquez",
      "Miguel Nicolas-Diaz",
      "Leonardo Chang",
      "Miguel Gonzalez-Mendoza"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LSR/Martindez-Diaz_ShuffleFaceNet_A_Lightweight_Face_Architecture_for_Efficient_and_Highly-Accurate_Face_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LSR/Martindez-Diaz_ShuffleFaceNet_A_Lightweight_Face_Architecture_for_Efficient_and_Highly-Accurate_Face_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The recent success of convolutional neural networks has led to the development of a variety of new effective and efficient architectures. However, few of them have been designed for the specific case of face recognition. Inspired on the state-of-the-art ShuffleNetV2 model, a lightweight face architecture is presented in this paper. The proposal, named ShuffleFaceNet, introduces significant modifications in order to improve face recognition accuracy. First, the Global Average Pooling layer is replaced by a Global Depth-wise Convolution layer, and Parametric Rectified Linear Unit is used as a non-linear activation function. Under the same experimental conditions, ShuffleFaceNet achieves significantly superior accuracy than the original ShuffleNetV2, maintaining the same speed and compact storage. In addition, extensive experiments conducted on three challenging benchmark face datasets, show that our proposal improves not only state-of-the-art lightweight models but also very deep face recognition models.\r",
    "code_link": ""
  },
  "iccv2019_mcmv_multi-videotemporalsynchronizationbymatchingposefeaturesofsharedmovingsubjects": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MCMV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Moving Cameras",
    "title": "Multi-Video Temporal Synchronization by Matching Pose Features of Shared Moving Subjects",
    "authors": [
      "Xinyi Wu",
      "Zhenyao Wu",
      "Yujun Zhang",
      "Lili Ju",
      "Song Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MCMV/Wu_Multi-Video_Temporal_Synchronization_by_Matching_Pose_Features_of_Shared_Moving_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MCMV/Wu_Multi-Video_Temporal_Synchronization_by_Matching_Pose_Features_of_Shared_Moving_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Collaborative analysis of videos taken by multiple motion cameras from different and time-varying views can help solve many computer vision problems. However, such collaborative analysis usually requires the videos to be temporally synchronized, which can be inaccurate if we solely rely on camera clock. In this paper, we propose to address this problem based on video content. More specifically, if multiple videos cover the same moving persons, these subjects shall exhibit identical pose and pose change at each aligned time point across these videos. Based on this idea, we develop a new Synchronization Network (SynNet) which includes a feature aggregation module, a matching cost volume and several classification layers to infer the time offset between different videos by exploiting view-invariant human pose features. We conduct comprehensive experiments on SYN, SPVideo and MPVideo datasets. The results show that the proposed method can accurately synchronize multiple motion-camera videos collected in real world.\r",
    "code_link": ""
  },
  "iccv2019_mcmv_liplearninginstancepropagationforvideoobjectsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MCMV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Moving Cameras",
    "title": "LIP: Learning Instance Propagation for Video Object Segmentation",
    "authors": [
      "Ye Lyu",
      "George Vosselman",
      "Gui-Song Xia",
      "Michael Ying Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MCMV/Lyu_LIP_Learning_Instance_Propagation_for_Video_Object_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MCMV/Lyu_LIP_Learning_Instance_Propagation_for_Video_Object_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years, the task of segmenting foreground objects from background in a video, i.e. video object segmentation (VOS), has received considerable attention. In this paper, we propose a single end-to-end trainable deep neural network, convolutional gated recurrent Mask-RCNN, for tackling the semi-supervised VOS task. We take advantage of both the instance segmentation network (Mask-RCNN) and the visual memory module (Conv-GRU) to tackle the VOS task. The instance segmentation network predicts masks for instances, while the visual memory module learns to selectively propagate information for multiple instances simultaneously, which handles the appearance change, the variation of scale and pose and the occlusions between objects. After offline and online training under purely instance segmentation losses, our approach is able to achieve satisfactory results without any post-processing or synthetic video data augmentation. Experimental results on DAVIS 2016 dataset and DAVIS 2017 dataset have demonstrated the effectiveness of our method for video object segmentation task.\r",
    "code_link": ""
  },
  "iccv2019_mcmv_5g-cageacontextandsituationalawarenesssystemforcitypublicsafetywithvideoprocessingatavirtualizedecosystem": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "MCMV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Moving Cameras",
    "title": "5G-CAGE: A Context and Situational Awareness System for City Public Safety with Video Processing at a Virtualized Ecosystem",
    "authors": [
      "Pedro E. Lopez-de-Teruel",
      "Manuel Gil Perez",
      "Felix J. Garcia Clemente",
      "Alberto Ruiz Garcia",
      "Gregorio Martinez Perez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/MCMV/Lopez-de-Teruel_5G-CAGE_A_Context_and_Situational_Awareness_System_for_City_Public_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/MCMV/Lopez-de-Teruel_5G-CAGE_A_Context_and_Situational_Awareness_System_for_City_Public_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this article we present 5G-CAGE, an ongoing project aimed to deploy a city safety solution that enables monitoring and analytics of video streams collected from distributed sources of a Smart City. Unlike current proposals based on inflexible architectures or limited networks, 5G-CAGE leverages 5G's high throughput and low latency, as well as its enhanced dynamism and adaptability with advanced virtualization-based technologies. In this context, 5G-CAGE defines a virtualization-enabled solution called City Object Detection (CODet), which allows recognizing interest objects in safety related situations, such as vehicles (e.g. license plates or brands), obstacles in emergency settings, or human faces recognition, to name a few. It can process multiple streams collected from fixed and moving cameras used as a distributed visual sensing system, adequately combining image processing and computer vision algorithms in a virtualized ecosystem. This paper presents initial tests in the specific task of locating and recognizing vehicle license plates, where the CODet virtualized solution has been successfully integrated and tested in the 5GINFIRE platform, an EU-funded project which provides a playground wherein new components, architectures, and APIs may be tried and proposed before being ported to 5G networks.\r",
    "code_link": ""
  },
  "iccv2019_r6d_cullnetcalibratedandposeawareconfidencescoresforobjectposeestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "R6D",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Recovering 6D Object Pose",
    "title": "CullNet: Calibrated and Pose Aware Confidence Scores for Object Pose Estimation",
    "authors": [
      "Kartik Gupta",
      "Lars Petersson",
      "Richard Hartley"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/R6D/Gupta_CullNet_Calibrated_and_Pose_Aware_Confidence_Scores_for_Object_Pose_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/R6D/Gupta_CullNet_Calibrated_and_Pose_Aware_Confidence_Scores_for_Object_Pose_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present a new approach for single view, image-based object pose estimation in real time. Specifically, the problem of culling false positives among several pose proposal estimates is addressed in this paper. Our proposed approach targets the problem of inaccurate confidence values predicted by CNNs which is used by many current methods to choose a final object pose prediction. We present a new network called CullNet, solving this task. CullNet takes pairs of pose masks rendered from a 3D model, and cropped regions in the original image as input. This is then used to calibrate the confidence scores of the pose proposals. This new set of confidence scores is found to be significantly more reliable for accurate object pose estimation as shown by our results. Our experimental results on multiple challenging datasets (LINEMOD and Occlusion LINEMOD) clearly reflects the utility of our proposed method. Our overall pose estimation pipeline outperforms state-of-the-art object pose estimation methods on these standard object pose estimation datasets. The code is available at https://github.com/kartikgupta-at-ANU/CullNet.\r",
    "code_link": "https://github.com/kartikgupta-at-ANU/CullNet"
  },
  "iccv2019_r6d_homebreweddbrgb-ddatasetfor6dposeestimationof3dobjects": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "R6D",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Recovering 6D Object Pose",
    "title": "HomebrewedDB: RGB-D Dataset for 6D Pose Estimation of 3D Objects",
    "authors": [
      "Roman Kaskman",
      "Sergey Zakharov",
      "Ivan Shugurov",
      "Slobodan Ilic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/R6D/Kaskman_HomebrewedDB_RGB-D_Dataset_for_6D_Pose_Estimation_of_3D_Objects_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/R6D/Kaskman_HomebrewedDB_RGB-D_Dataset_for_6D_Pose_Estimation_of_3D_Objects_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Among the most important prerequisites for creating and evaluating 6D object pose detectors are datasets with labeled 6D poses. With the advent of deep learning, demand for such datasets is growing continuously. Despite the fact that some of exist, they are scarce and typically have restricted setups, such as a single object per sequence, or they focus on specific object types, such as textureless industrial parts. Besides, two significant components are often ignored: training using only available 3D models instead of real data and scalability, i.e. training one method to detect all objects rather than training one detector per object. Other challenges, such as occlusions, changing light conditions and changes in object appearance, as well precisely defined benchmarks are either not present or are scattered among different datasets. In this paper we present a dataset for 6D pose estimation that covers the above-mentioned challenges, mainly targeting training from 3D models (both textured and textureless), scalability, occlusions, and changes in light conditions and object appearance. The dataset features 33 objects (17 toy, 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty. We also present a set of benchmarks to test various desired detector properties, particularly focusing on scalability with respect to the number of objects and resistance to changing light conditions, occlusions and clutter. We also set a baseline for the presented benchmarks using a state-of-the-art DPOD detector. Considering the difficulty of making such datasets, we plan to release the code allowing other researchers to extend this dataset or make their own datasets in the future.\r",
    "code_link": ""
  },
  "iccv2019_r6d_unsupervisedjoint3dobjectmodellearningand6dposeestimationfordepth-basedinstancesegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "R6D",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Recovering 6D Object Pose",
    "title": "Unsupervised Joint 3D Object Model Learning and 6D Pose Estimation for Depth-Based Instance Segmentation",
    "authors": [
      "Yuanwei Wu",
      "Tim Marks",
      "Anoop Cherian",
      "Siheng Chen",
      "Chen Feng",
      "Guanghui Wang",
      "Alan Sullivan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/R6D/Wu_Unsupervised_Joint_3D_Object_Model_Learning_and_6D_Pose_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/R6D/Wu_Unsupervised_Joint_3D_Object_Model_Learning_and_6D_Pose_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work, we propose a novel unsupervised approach to jointly learn the 3D object model and estimate the 6D poses of multiple instances of a same object, with applications to depth-based instance segmentation. The inputs are depth images, and the learned object model is represented by a 3D point cloud. Traditional 6D pose estimation approaches are not sufficient to address this problem, where neither a CAD model of the object nor the ground-truth 6D poses of its instances are available during training. To solve this problem, we propose to jointly optimize the model learning and pose estimation in an end-to-end deep learning framework. Specifically, our network produces a 3D object model and a list of rigid transformations on this model to generate instances, which when rendered must match the observed point cloud to minimizing the Chamfer distance. To render the set of instance point clouds with occlusions, the network automatically removes the occluded points in a given camera view. Extensive experiments evaluate our technique on several object models and varying number of instances in 3D point clouds. We demonstrate the application of our method to instance segmentation of depth images of small bins of industrial parts. Compared with popular baselines for instance segmentation, our model not only demonstrates competitive performance, but also learns a 3D object model that is represented as a 3D point cloud.\r",
    "code_link": ""
  },
  "iccv2019_r6d_anannotationsavedisanannotationearnedusingfullysynthetictrainingforobjectdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "R6D",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Recovering 6D Object Pose",
    "title": "An Annotation Saved is an Annotation Earned: Using Fully Synthetic Training for Object Detection",
    "authors": [
      "Stefan Hinterstoisser",
      "Olivier Pauly",
      "Hauke Heibel",
      "Marek Martina",
      "Martin Bokeloh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/R6D/Hinterstoisser_An_Annotation_Saved_is_an_Annotation_Earned_Using_Fully_Synthetic_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/R6D/Hinterstoisser_An_Annotation_Saved_is_an_Annotation_Earned_Using_Fully_Synthetic_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep learning methods typically require vast amounts of training data to reach their full potential. While some publicly available datasets exists, domain specific data always needs to be collected and manually labeled, an expensive, time consuming and error prone process. Training with synthetic data is therefore very lucrative, as dataset creation and labeling comes for free. We propose a novel method for creating purely synthetic training data for object detection. We leverage a large dataset of 3D background models and densely render them using full domain randomization. This yields background images with realistic shapes and texture on top of which we render the objects of interest. During training, the data generation process follows a curriculum strategy guaranteeing that all foreground models are presented to the network equally under all possible poses and conditions with increasing complexity. As a result, we entirely control the underlying statistics and we create optimal training samples at every stage of training. Using a challenging evaluation dataset with 64 retail objects, we demonstrate that our approach enables the training of detectors that compete favorably with models trained on real data while being at least two orders of magnitude more time and cost effective with respect to data annotation. Finally, our approach performs significantly better on the YCB-Video Dataset than DOPE - a state-of-the-art method in learning from synthetic data.\r",
    "code_link": ""
  },
  "iccv2019_r6d_arefined3dposedatasetforfine-grainedobjectcategories": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "R6D",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Recovering 6D Object Pose",
    "title": "A Refined 3D Pose Dataset for Fine-Grained Object Categories",
    "authors": [
      "Yaming Wang",
      "Xiao Tan",
      "Yi Yang",
      "Ziyu Li",
      "Xiao Liu",
      "Feng Zhou",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/R6D/Wang_A_Refined_3D_Pose_Dataset_for_Fine-Grained_Object_Categories_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/R6D/Wang_A_Refined_3D_Pose_Dataset_for_Fine-Grained_Object_Categories_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Most Existing 3D pose datasets of object categories are limited to generic object types and lack fine-grained information. In this work, we introduce a new large-scale dataset that consists of 409 fine-grained categories and 31,881 images with refined 3D pose annotation. Specifically, we augment three existing fine-grained object recognition datasets (StanfordCars, CompCars and FGVC-Aircraft) by finding a specific 3D model for each sub-category from ShapeNet and manually annotating each 2D image by adjusting a full set of 7 continuous perspective parameters. Since the fine-grained shapes allow the projection of 3D models to better fit the 2D images, we further improve the annotation quality by initializing from the human annotation and conducting local search of the pose parameters with the objective of maximizing the IoUs between the projected mask and the segmentation reference estimated from state-of-the-art segmentation networks. We provide a full statistics of the annotations with qualitative and quantitative comparisons suggesting that our dataset can be a complementary source for studying 3D pose estimation. The dataset can be downloaded at http://users.umiacs.umd.edu/ wym/3dpose.html.\r",
    "code_link": ""
  },
  "iccv2019_r6d_cornetgeneric3dcornersfor6dposeestimationofnewobjectswithoutretraining": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "R6D",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Recovering 6D Object Pose",
    "title": "CorNet: Generic 3D Corners for 6D Pose Estimation of New Objects without Retraining",
    "authors": [
      "Giorgia Pitteri",
      "Slobodan Ilic",
      "Vincent Lepetit"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/R6D/Pitteri_CorNet_Generic_3D_Corners_for_6D_Pose_Estimation_of_New_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/R6D/Pitteri_CorNet_Generic_3D_Corners_for_6D_Pose_Estimation_of_New_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present a novel approach to the detection and 3D pose estimation of objects in color images. Its main contribution is that it does not require any training phases nor data for new objects, while state-of-the-art methods typically require hours of training time and hundreds of training registered images. Instead, our method relies only on the objects' geometries. Our method focuses on objects with prominent corners, which covers a large number of industrial objects. We first learn to detect object corners of various shapes in images and also to predict their 3D poses, by using training images of a small set of objects. To detect a new object in a given image, we first identify its corners from its CAD model; we also detect the corners visible in the image and predict their 3D poses. We then introduce a RANSAC-like algorithm that robustly and efficiently detects and estimates the object's 3D pose by matching its corners on the CAD model with their detected counterparts in the image. Because we also estimate the 3D poses of the corners in the image, detecting only 1 or 2 corners is sufficient to estimate the pose of the object, which makes the approach robust to occlusions. We finally rely on a final check that exploits the full 3D geometry of the objects, in case multiple objects have the same corner spatial arrangement. The advantages of our approach make it particularly attractive for industrial contexts, and we demonstrate our approach on the challenging T-LESS dataset.\r",
    "code_link": ""
  },
  "iccv2019_r6d_satelliteposeestimationwithdeeplandmarkregressionandnonlinearposerefinement": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "R6D",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Recovering 6D Object Pose",
    "title": "Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement",
    "authors": [
      "Bo Chen",
      "Jiewei Cao",
      "Alvaro Parra",
      "Tat-Jun Chin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/R6D/Chen_Satellite_Pose_Estimation_with_Deep_Landmark_Regression_and_Nonlinear_Pose_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/R6D/Chen_Satellite_Pose_Estimation_with_Deep_Landmark_Regression_and_Nonlinear_Pose_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose an approach to estimate the 6DOF pose of a satellite, relative to a canonical pose, from a single image. Such a problem is crucial in many space proximity operations, such as docking, debris removal, and inter-spacecraft communications. Our approach combines machine learning and geometric optimisation, by predicting the coordinates of a set of landmarks in the input image, associating the landmarks to their corresponding 3D points on an a priori reconstructed 3D model, then solving for the object pose using non-linear optimisation. Our approach is not only novel for this specific pose estimation task, which helps to further open up a relatively new domain for machine learning and computer vision, but it also demonstrates superior accuracy and won the first place in the recent Kelvins Pose Estimation Challenge organised by the European Space Agency (ESA).\r",
    "code_link": ""
  },
  "iccv2019_hands_explicitspatiotemporaljointrelationlearningfortrackinghumanpose": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HANDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Observing and Understanding Hands in Action",
    "title": "Explicit Spatiotemporal Joint Relation Learning for Tracking Human Pose",
    "authors": [
      "Xiao Sun",
      "Chuankang Li",
      "Stephen Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HANDS/Sun_Explicit_Spatiotemporal_Joint_Relation_Learning_for_Tracking_Human_Pose_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HANDS/Sun_Explicit_Spatiotemporal_Joint_Relation_Learning_for_Tracking_Human_Pose_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present a method for human pose tracking that is based on learning spatiotemporal relationships among joints. Beyond generating the heatmap of a joint in a given frame, our system also learns to predict the offset of the joint from a neighboring joint in the frame. Additionally, it is trained to predict the displacement of the joint from its position in the previous frame, in a manner that can account for possibly changing joint appearance, unlike optical flow. These relational cues in the spatial domain and temporal domain are inferred in a robust manner by attending only to relevant areas in the video frames. By explicitly learning and exploiting these joint relationships, our system achieves state-of-the-art performance on standard benchmarks for various pose tracking tasks including 3D body pose tracking in RGB video, 3D hand pose tracking in depth sequences, and 3D hand gesture tracking in RGB video.\r",
    "code_link": ""
  },
  "iccv2019_hands_talkingwithyourhandsscalinghandgesturesandrecognitionwithcnns": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HANDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Observing and Understanding Hands in Action",
    "title": "Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs",
    "authors": [
      "Okan Kopuklu",
      "Yao Rong",
      "Gerhard Rigoll"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HANDS/Kopuklu_Talking_With_Your_Hands_Scaling_Hand_Gestures_and_Recognition_With_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HANDS/Kopuklu_Talking_With_Your_Hands_Scaling_Hand_Gestures_and_Recognition_With_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The use of hand gestures provides a natural alternative to cumbersome interface devices for Human-Computer Interaction (HCI) systems. As the technology advances and communication between humans and machines becomes more complex, HCI systems should also be scaled accordingly in order to accommodate the introduced complexities. In this paper, we propose a methodology to scale hand gestures by forming them with predefined gesture-phonemes, and a convolutional neural network (CNN) based framework to recognize hand gestures by learning only their constituents of gesture-phonemes. The total number of possible hand gestures can be increased exponentially by increasing the number of used gesture-phonemes. For this objective, we introduce a new benchmark dataset named Scaled Hand Gestures Dataset (SHGD) with only gesture-phonemes in its training set and 3-tuples gestures in the test set. In our experimental analysis, we achieve to recognize hand gestures containing one and three gesture-phonemes with an accuracy of 98.47% (in 15 classes) and 94.69% (in 810 classes), respectively. Our dataset, code and pretrained models are publicly available.\r",
    "code_link": ""
  },
  "iccv2019_hands_disentanglingposefromappearanceinmonochromehandimages": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HANDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Observing and Understanding Hands in Action",
    "title": "Disentangling Pose from Appearance in Monochrome Hand Images",
    "authors": [
      "Yikang LI",
      "Chris Twigg",
      "Yuting Ye",
      "Lingling Tao",
      "Xiaogang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HANDS/LI_Disentangling_Pose_from_Appearance_in_Monochrome_Hand_Images_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HANDS/LI_Disentangling_Pose_from_Appearance_in_Monochrome_Hand_Images_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Hand pose estimation from the monocular 2D image is challenging due to the variation in lighting, appearance, and background. While some success has been achieved using deep neural networks, they typically require collecting a large dataset that adequately samples all the axes of variation of hand images. It would, therefore, be useful to find a representation of hand pose which is independent of the image appearance(like hand texture, lighting, background), so that we can synthesize unseen images by mixing pose-appearance combinations. In this paper, we present a novel technique that disentangles the representation of pose from a complementary appearance factor in 2D monochrome images. We supervise this disentanglement process using a network that learns to generate images of hand using specified pose+ appearance features. Unlike previous work, we do not require image pairs with a matching pose; instead, we use the pose annotations already available and introduce a novel use of cycle consistency to ensure orthogonality between the factors. Experimental results show that our self-disentanglement scheme successfully decomposes the hand image into the pose and its complementary appearance features of comparable quality as the method using paired data. Additionally, training the model with extra synthesized images with unseen hand-appearance combinations by re-mixing pose and appearance factors from different images can improve the 2D pose estimation performance.\r",
    "code_link": ""
  },
  "iccv2019_hands_handposeensemblelearningbasedongroupingfeaturesofhandpointsets": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HANDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Observing and Understanding Hands in Action",
    "title": "Hand Pose Ensemble Learning Based on Grouping Features of Hand Point Sets",
    "authors": [
      "Tianqiang Zhu",
      "Yi Sun",
      "Xiaohong Ma",
      "Xiangbo Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HANDS/Zhu_Hand_Pose_Ensemble_Learning_Based_on_Grouping_Features_of_Hand_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HANDS/Zhu_Hand_Pose_Ensemble_Learning_Based_on_Grouping_Features_of_Hand_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we mainly consider using 3D point sets as input to deal with the task of 3D hand pose estimation. We make some improvements to PointNet++ structure, including proposing adaptive pooling which introduces the self-attention mechanism to make the network could select features itself, and putting forward an ensemble strategy to fully utilize hand features. These improvements can enhance the expressive ability of features and make full use of the information contained in features. In addition, we propose a data augmentation method for point net, which directly transforms the original point cloud data without the aid of simulation models. Experiments results on three hand pose datasets demonstrate that our method can achieve comparable performance with state-of-the-arts.\r",
    "code_link": ""
  },
  "iccv2019_hands_3dhandposeestimationfromrgbusingprivilegedlearningwithdepthdata": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HANDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Observing and Understanding Hands in Action",
    "title": "3D Hand Pose Estimation from RGB Using Privileged Learning with Depth Data",
    "authors": [
      "Shanxin Yuan",
      "Bjorn Stenger",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HANDS/Yuan_3D_Hand_Pose_Estimation_from_RGB_Using_Privileged_Learning_with_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HANDS/Yuan_3D_Hand_Pose_Estimation_from_RGB_Using_Privileged_Learning_with_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper proposes a method for 3D hand pose estimation given a large dataset of depth images with joint annotations, and a smaller dataset of depth and RGB image pairs with joint annotations. We explore different ways of using the depth data at the training stage to improve the pose estimation accuracy of a network that only takes RGB images as input. By using paired RGB and depth images, we are able to supervise the RGB-based network to learn middle layer features that mimic that of a network trained on largescale, accurately annotated depth data. Further, depth data provides accurate foreground masks, which are employed to learn better feature activations in the RGB network. During testing, when only RGB images are available, our method produces accurate 3D hand pose predictions. The method is also shown to perform well on the 2D hand pose estimation task. We validate the approach on three public datasets, and compare it to other published methods.\r",
    "code_link": ""
  },
  "iccv2019_hands_thejesterdatasetalarge-scalevideodatasetofhumangestures": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "HANDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Observing and Understanding Hands in Action",
    "title": "The Jester Dataset: A Large-Scale Video Dataset of Human Gestures",
    "authors": [
      "Joanna Materzynska",
      "Guillaume Berger",
      "Ingo Bax",
      "Roland Memisevic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Gesture recognition and its application in human-computer interfaces have been growing increasingly popular in recent years. Although many gestures can be recognized from a single image frame, to build a responsive, accurate system, that can recognize complex gestures with subtle differences between them we need large-scale real-world video datasets. In this work, we introduce the largest collection of short clips of videos of humans performing gestures in front of the camera. The dataset has been collected with the help of over 1300 different actors in their unconstrained environments. Additionally, we present an on-going gesture recognition challenge based on our dataset and the current results. We also describe how a baseline achieving over 93% recognition accuracy can be obtained with a simple 3D convolutional neural network.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_embarrassinglysimplebinaryrepresentationlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Embarrassingly Simple Binary Representation Learning",
    "authors": [
      "Yuming Shen",
      "Jie Qin",
      "Jiaxin Chen",
      "Li Liu",
      "Fan Zhu",
      "Ziyi Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Shen_Embarrassingly_Simple_Binary_Representation_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Shen_Embarrassingly_Simple_Binary_Representation_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, for binary latents and semantic labels respectively, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent 'learning to hash' paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.\r",
    "code_link": "https://github.com/ymcidence/JMLH"
  },
  "iccv2019_cefrl_unsupervisedextractionoflocalimagedescriptorsviarelativedistancerankingloss": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Unsupervised Extraction of Local Image Descriptors via Relative Distance Ranking Loss",
    "authors": [
      "Xin Yu",
      "Yurun Tian",
      "Fatih Porikli",
      "Richard Hartley",
      "Hongdong Li",
      "Huub Heijnen",
      "Vassileios Balntas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Yu_Unsupervised_Extraction_of_Local_Image_Descriptors_via_Relative_Distance_Ranking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Yu_Unsupervised_Extraction_of_Local_Image_Descriptors_via_Relative_Distance_Ranking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " State-of-the-art supervised local descriptor learning methods heavily rely on accurately labelled patches for training. However, since the process of labelling patches is laborious and inefficient, supervised training is limited by the availability and scale of training datasets. In comparison, unsupervised learning does not require burdensome data labelling; thus it is not restricted to a specific domain. Furthermore, extracting patches from training images in-volves minimal effort. Nevertheless, most of the existing unsupervised learning based methods are inherently inferior to the handcrafted local descriptors, such as the Scale-Invariant Feature Transform (SIFT). In this paper, we aim to leverage unlabelled data to learn descriptors for image patches by a deep convolutional neural network. We introduce a Relative Distance Ranking Loss (RDRL) that measures the deviation of a generated ranking order of patch similarities against a reference one. Specifically, our approach yields a patch similarity ranking based on the learned embedding of a neural network, and the ranking mechanism minimizes the proposed RDRL by mimicking a reference similarity ranking based on a competent handcrafted feature (i.e., SIFT). To our advantage, after the training process, our network is not only able to measure the patch similarity but also able to outperform SIFT by a large margin on several commonly used benchmark datasets as demonstrated in our extensive experiments.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_metric-basedregularizationandtemporalensembleformulti-tasklearningusingheterogeneousunsupervisedtasks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Metric-Based Regularization and Temporal Ensemble for Multi-Task Learning using Heterogeneous Unsupervised Tasks",
    "authors": [
      "Byung Cheol Song",
      "Dae Ha Kim",
      "Seung hyun Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Song_Metric-Based_Regularization_and_Temporal_Ensemble_for_Multi-Task_Learning_using_Heterogeneous_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Song_Metric-Based_Regularization_and_Temporal_Ensemble_for_Multi-Task_Learning_using_Heterogeneous_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " One of the ways to improve the performance of a target task is to learn the transfer of abundant knowledge of a pretrained network. However, learning of the pre-trained network requires high computation capability and large-scale labeled datasets. To mitigate the burden of large-scale labeling, learning in un/self-supervised manner can be a solution. In addition, using un-supervised multi-task learning, a generalized feature representation can be learned. However, un-supervised multi-task learning can be biased to a specific task. To overcome this problem, we propose the metric-based regularization term and temporal task ensemble (TTE) for multi-task learning. Since these two techniques prevent the entire network from learning in a state deviated to a specific task, it is possible to learn a generalized feature representation that appropriately reflects the characteristics of each task without biasing. Experimental results for three target tasks such as classification, object detection and embedding clustering prove that the TTE based multi-task framework is more effective than the state-of-the-art (SOTA) method in improving the performance of a target task.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_damewebdynamicmeanwithwhiteningensemblebinarizationforlandmarkretrievalwithouthumanannotation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "DAME WEB: DynAmic MEan with Whitening Ensemble Binarization for Landmark Retrieval without Human Annotation",
    "authors": [
      "Tsun-Yi Yang",
      "Duy Kien Nguyen",
      "Huub Heijnen",
      "Vassileios Balntas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Yang_DAME_WEB_DynAmic_MEan_with_Whitening_Ensemble_Binarization_for_Landmark_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Yang_DAME_WEB_DynAmic_MEan_with_Whitening_Ensemble_Binarization_for_Landmark_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work, we propose a simple yet effective module called DynAmic MEan (DAME) which allows a neural network to dynamically learn to aggregate feature maps at the pooling stage based on the input image, in order to generate global descriptors suitable for landmark retrieval. In contrast to Generalized Mean (GeM), which uses a predefined and static norm for pooling features into descriptors, we use a dynamic p-norm, with the p value being generated online by the model for each image. In addition, we utilize the introduced dynamic pooling method, to propose a novel feature whitening technique, Whitening Ensemble Binarization (WEB), to discover complementary information through multiple statistical projections. The memory cost of the proposed global binary descriptor is 8 times smaller than the state-of-the-art, while exhibiting similar or improved performance. To further demonstrate the power of DAME, we use it with features extracted from a fixed, pre-trained classification network, and illustrate that our dynamic p-norm is capable of learning to pool the classification features into global descriptors suitable for retrieval. Finally, by combining DAME with WEB, we achieve state-of-the-art results on challenging large-scale landmark retrieval benchmarks.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_moreaboutcovariancedescriptorsforimagesetcodinglog-euclideanframeworkbasedkernelmatrixrepresentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "More About Covariance Descriptors for Image Set Coding: Log-Euclidean Framework Based Kernel Matrix Representation",
    "authors": [
      "Kai-Xuan Chen",
      "Xiao-Jun Wu",
      "Jie-Yi Ren",
      "Rui Wang",
      "Josef Kittler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Chen_More_About_Covariance_Descriptors_for_Image_Set_Coding_Log-Euclidean_Framework_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Chen_More_About_Covariance_Descriptors_for_Image_Set_Coding_Log-Euclidean_Framework_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We consider a family of structural descriptors for visual data, namely covariance descriptors (CovDs) that lie on a non-linear symmetric positive definite (SPD) manifold, a special type of Riemannian manifolds. We propose an improved version of CovDs for image set coding by extending the traditional CovDs from Euclidean space to the SPD manifold. Specifically, the manifold of SPD matrices is a complete inner product space with the operations of logarithmic multiplication and scalar logarithmic multiplication defined in the Log-Euclidean framework. In this framework, we characterise covariance structure in terms of the arc-cosine kernel which satisfies Mercer's condition and propose the operation of mean centralization on SPD matrices. Furthermore, we combine arc-cosine kernels of different orders using mixing parameters learnt by kernel alignment in a supervised manner. Our proposed framework provides a lower-dimensional and more discriminative data representation for the task of image set classification. The experimental results demonstrate its superior performance, measured in terms of recognition accuracy, as compared with the state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_compactandefficientmultitasklearninginvision,languageandspeech": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Compact and Efficient Multitask Learning in Vision, Language and Speech",
    "authors": [
      "Mohammed Al-Rawi",
      "Ernest Valveny"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Al-Rawi_Compact_and_Efficient_Multitask_Learning_in_Vision_Language_and_Speech_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Al-Rawi_Compact_and_Efficient_Multitask_Learning_in_Vision_Language_and_Speech_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Across-domain multitask learning is a challenging area of computer vision and machine learning due to the intra-similarities among class distributions. Addressing this problem to cope with the human cognition system by considering inter and intra-class categorization and recognition complicates the problem even further. We propose in this work an effective holistic and hierarchical learning by using a text embedding layer on top of a deep learning model. We also propose a novel sensory discriminator approach to resolve the collisions between different tasks and domains. We then train the model concurrently on textual sentiment analysis, speech recognition, image classification, action recognition from video, and handwriting word spotting of two different scripts (Arabic and English). The model we propose successfully learned different tasks across multiple domains.\r",
    "code_link": "https://github.com/morawi/MLPHOC"
  },
  "iccv2019_cefrl_muffnetmulti-layerfeaturefederationformobiledeeplearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "MuffNet: Multi-Layer Feature Federation for Mobile Deep Learning",
    "authors": [
      "Hesen Chen",
      "Ming Lin",
      "Xiuyu Sun",
      "Qian Qi",
      "Hao Li",
      "Rong Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Chen_MuffNet_Multi-Layer_Feature_Federation_for_Mobile_Deep_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Chen_MuffNet_Multi-Layer_Feature_Federation_for_Mobile_Deep_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The increasing industrial demands to deploy deep neural networks on resource constrained mobile device motivates recent researches of efficient structure for deep learning. One popular approach is to densify network connectivity by sharing feature maps between layers. A side effect of this approach is that the volume of the feature maps and the convolution computation will exponentially blow up. In this work, we propose a novel structure, named Multi-Layer Feature Federation Network (MuffNet), to address this issue. The MuffNet is a densely connected network but con-sumes much less memory and computation at inference. The key idea of the MuffNet is to elaborately split the feature maps of one layer to different groups. Each feature map group is then shared only once with the other layer. In this way we maintain the network computation within bud-get while keeping the topology density of the network. On the theoretical side, we show that under the same compu-tational budget, MuffNet is a better universal approximator for functions containing high frequency components. We validate the superiority of MuffNet on popular image classification and object detection benchmark datasets. The extensive experiments show that MuffNet is more efficient especially for small models under 45 MFLOPs.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_beyondattributeshigh-orderattributefeaturesforzero-shotlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Beyond Attributes: High-Order Attribute Features for Zero-Shot Learning",
    "authors": [
      "Xiao-Bo Jin",
      "Guo-Sen Xie",
      "Kaizhu Huang",
      "Jianyu Miao",
      "Qiufeng Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Jin_Beyond_Attributes_High-Order_Attribute_Features_for_Zero-Shot_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Jin_Beyond_Attributes_High-Order_Attribute_Features_for_Zero-Shot_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, SeeNet with the high-order attribute features (SeeNet-HAF) is proposed to solve the challenging zero-shot learning (ZSL) task. The high-order attribute features aims to discover a more elaborate, discriminative high-order semantic vector for each class and can distill the correlation between the class attributes embedding into modeling. SeeNet-HAF consists of two branches. The upper stream is capable of dynamically localizing some discriminative object region, and then the high-order attribute supervision is incorporated to characterize the relationship between the class attributes. Meanwhile, the bottom stream discovers complementary object regions by erasing its discovered regions from the feature maps. In addition, we propose a fast hyperparameter search strategy. It takes both the breadth and precision of the search into account. Experiments on four standard benchmark datasets demonstrate the superiority of the SeeNet-HAF framework.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_augmentationinvarianttraining": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Augmentation Invariant Training",
    "authors": [
      "Weicong Chen",
      "Lu Tian",
      "Liwen Fan",
      "Yu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Chen_Augmentation_Invariant_Training_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Chen_Augmentation_Invariant_Training_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Data augmentation is acknowledged to help deep neural networks generalize better, while their augmentation transfer ability is far from satisfactory. The networks perform worse when tested with augmentations not used during training, which is also a manifestation of insufficient generalization ability. To address this problem, we carefully design a novel Augmentation invariant Loss (AiLoss) to assist networks to learn augmentation invariant by minimizing intra-augmentation variation. Based on AiLoss, we propose a simple yet efficient training strategy, Augmentation Invariant Training (AIT), to enhance the generalization ability of networks. Extensive experiments show that AIT can be applied to a variety of network architectures, and consistently improve their performance on CIFAR-10, CIFAR-100 and ImageNet without increasing computational cost. Further extending AIT to multiple networks, we propose multi-AIT to learn inter-network augmentation invariant, which achieves better performance in enhancing generalization ability. Moreover, further experiments present that networks trained with our strategy do obtain better augmentation transfer ability and learn features that are invariant to augmentations. Our source code is available at Github.\r",
    "code_link": "https://github.com/juicecwc/Augmentation-Invariant-Training"
  },
  "iccv2019_cefrl_largescalenear-duplicateimageretrievalviapatchembedding": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Large Scale Near-Duplicate Image Retrieval via Patch Embedding",
    "authors": [
      "Shangpeng Yan",
      "Xiaoyun Zhang",
      "Li Chen",
      "Wenbo Bao",
      "Zhiyong Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Yan_Large_Scale_Near-Duplicate_Image_Retrieval_via_Patch_Embedding_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Yan_Large_Scale_Near-Duplicate_Image_Retrieval_via_Patch_Embedding_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Large scale near-duplicate image retrieval (NDIR) relies on the Bag-of-Words methodology which quantizes local features into visual words. However, the direct match of these visual words typically leads to unpleasant mismatches due to quantization errors. To enhance the discriminability of the matching process, existing methods usually exploit hand-crafted contextual information, which has limited performance in complicated real-world scenarios. In contrast, we in this paper propose a trainable lightweight embedding network to extract local binary features. The network takes image patches as inputs and generates the binary code that can be efficiently stored in the inverted indexing file and helps discard mismatches immediately during the retrieval process. We improve the discriminability of the code by elaborately composing the training patches for network optimization, which consists of a proper inter-class (non-duplicate) patches selection and a rich intra-class (near-duplicate) patch generation. We evaluate our approach on the open NDIR dataset, INRIA CopyDays, and the experimental results show that our method performs favorably against the state-of-the-art algorithms. Furthermore, with a relatively short code length, our approach achieves higher query speed and lower storage occupation.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_vaclvariance-awarecross-layerregularizationforpruningdeepresidualnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "VACL: Variance-Aware Cross-Layer Regularization for Pruning Deep Residual Networks",
    "authors": [
      "Susan Gao",
      "Xin Liu",
      "Lung-Sheng Chien",
      "William Zhang",
      "Jose M. Alvarez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Gao_VACL_Variance-Aware_Cross-Layer_Regularization_for_Pruning_Deep_Residual_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Gao_VACL_Variance-Aware_Cross-Layer_Regularization_for_Pruning_Deep_Residual_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Improving weight sparsity is a common strategy for producing light-weight deep neural networks. However, pruning models with residual learning is more challenging. In this paper, we introduce a novel approach to address this problem. Our method puts the ith filters of layers connected by skip-connections into one regularization group. Additionally, we define Variance-Aware Cross-Layer (VACL) regularization which takes into account both the first and second-order statistics of the connected layers to constrain the variance within a group. Our approach can effectively improve the structural sparsity of residual models. For CIFAR10, the proposed method reduces a ResNet model by up to 79.5% with no accuracy drop, and reduces a ResNeXt model by up to 82% with < 1% accuracy drop. For ImageNet, it yields a pruned ratio of up to 63:3% with < 1% top-5 accuracy drop. Our experimental results show that the proposed approach significantly outperforms other state-of-the-art methods in terms of overall model size and accuracy.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_event-basedincrementalbroadlearningsystemforobjectclassification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Event-Based Incremental Broad Learning System for Object Classification",
    "authors": [
      "Shan Gao",
      "Guangqian Guo",
      "C. L. Philip Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Gao_Event-Based_Incremental_Broad_Learning_System_for_Object_Classification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Gao_Event-Based_Incremental_Broad_Learning_System_for_Object_Classification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of RGB values. Thousands of convolutional neural networks have emerged to process the frame-based images; however, there are few networks designed explicitly for the event-based data, which can fully take advantages of the asynchronous and high temporal resolution data. In this paper, we propose an incremental broad learning system to learn the event-based data in a flat network structure, which consists of feature nodes and enhancement nodes in one layer. The incremental learning strategy is developed for fast adding new nodes in a broad extension, yet it is almost impossible to add a filter or layer in the CNNs without retraining from the beginning. An SVD operation is coupled with the network extension to prevent the redundancy of the network structure. In experiments, our model outperforms the state of the arts, at the same time, 15x faster than the CNNs in training. It makes event cameras easier to be the nearly online training and inference applications.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_differential-evolution-basedgenerativeadversarialnetworksforedgedetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Differential-Evolution-Based Generative Adversarial Networks for Edge Detection",
    "authors": [
      "Wenbo Zheng",
      "Chao Gou",
      "Lan Yan",
      "Fei-Yue Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Zheng_Differential-Evolution-Based_Generative_Adversarial_Networks_for_Edge_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Zheng_Differential-Evolution-Based_Generative_Adversarial_Networks_for_Edge_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Since objects in natural scenarios possess various scales and aspect ratios, learning the rich edge information is very critical for vision-based tasks. Conventional generative adversarial networks (GANs) based methods for edge detection don't perform so well due to model collapse. In order to capture rich edge information and avoid model collapse as much as possible, we consider the learning of GANs as an evolutionary optimization and propose a novel method termed as differential-evolution-based generative adversarial networks (DEGAN) for richer edge detection. In particular, built upon GANs structure, we introduce an improved differential evolutionary algorithm to refine the input of generator, with fitness function evaluated by the discriminator. Experimental results on the well-known BSDS500 and NYUD benchmarks indicate our proposed DEGAN can achieve state-of-the-art performance while retaining a fast speed and validate its simplicity, effectiveness, and efficiency. The high quality of our results on edge detection with proposed DEGAN may promise to make other vision-based tasks work better.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_low-bitquantizationofneuralnetworksforefficientinference": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Low-bit Quantization of Neural Networks for Efficient Inference",
    "authors": [
      "Eli Kravchik",
      "Fan Yang",
      "Pavel Kisilev",
      "Yoni Choukroun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Kravchik_Low-bit_Quantization_of_Neural_Networks_for_Efficient_Inference_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Kravchik_Low-bit_Quantization_of_Neural_Networks_for_Efficient_Inference_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent machine learning methods use increasingly large deep neural networks to achieve state of the art results in various tasks. The gains in performance come at the cost of a substantial increase in computation and storage requirements. This makes real-time implementations on limited resources hardware a challenging task. One popular approach to address this challenge is to perform low-bit precision computations via neural network quantization. However, aggressive quantization generally entails a severe penalty in terms of accuracy, and often requires retraining of the network, or resorting to higher bit precision quantization. In this paper, we formalize the linear quantization task as a Minimum Mean Squared Error (MMSE) problem for both weights and activations, allowing low-bit precision inference without the need for full network retraining. We propose the analysis and the optimization of constrained MSE problems for performant hardware aware quantization. The proposed approach allows 4 bits integer (INT4) quantization for deployment of pretrained models on limited hardware resources. Multiple experiments on various network architectures show that the suggested method yields state of the art results with minimal loss of tasks accuracy.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_self-supervisedlearningofclassembeddingsfromvideo": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Self-Supervised Learning of Class Embeddings from Video",
    "authors": [
      "Olivia Wiles",
      "A. Sophia Koepke",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Wiles_Self-Supervised_Learning_of_Class_Embeddings_from_Video_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Wiles_Self-Supervised_Learning_of_Class_Embeddings_from_Video_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This work explores how to use self-supervised learning on videos to learn a class-specific image embedding that encodes pose and shape information in the form of landmarks. At train time, two frames of the same video of an object class (e.g. human upper body) are extracted and each encoded to an embedding. Conditioned on these embeddings, the decoder network is tasked to transform one frame into another. To successfully perform long range transformations (e.g. a wrist lowered in one image should be mapped to the same wrist raised in another), we introduce a new hierarchical probabilistic network decoder model. Once trained, the embedding can be used for a variety of downstream tasks and domains. We demonstrate our approach quantitatively on three distinct deformable object classes - human full bodies, upper bodies, faces - and show experimentally that the learned embeddings do indeed generalise. They achieve state-of-the-art performance in comparison to other self-supervised methods trained on the same datasets, and approach the performance of fully supervised methods.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_deeptotalvariationsupportvectornetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Deep Total Variation Support Vector Networks",
    "authors": [
      "Hichem Sahbi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Sahbi_Deep_Total_Variation_Support_Vector_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Sahbi_Deep_Total_Variation_Support_Vector_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Support vector machines (SVMs) have been successful in solving many computer vision tasks including image and video category recognition especially for small and mid-scale training problems. The principle of these non-parametric models is to learn hyperplanes that separate data belonging to different classes while maximizing their margins. However, SVMs constrain the learned hyperplanes to lie in the span of support vectors, fixed/taken from training data, and this reduces their representational power and may lead to limited generalization performances. In this paper, we relax this constraint and allow the support vectors to be learned (instead of being fixed/taken from training data) in order to better fit a given classification task. Our approach, referred to as deep total variation support vector machines, is parametric and relies on a novel deep architecture that learns not only the SVM and the kernel parameters but also the support vectors, resulting into highly effective classifiers. We also show (under a particular setting of the activation functions in this deep architecture) that a large class of kernels and their combinations can be learned. Experiments conducted on the challenging task of skeleton-based action recognition show the outperformance of our deep total variation SVMs w.r.t different baselines as well as the related work.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_ageestimationfromfacialpartsusingcompactmulti-streamconvolutionalneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Age Estimation From Facial Parts Using Compact Multi-Stream Convolutional Neural Networks",
    "authors": [
      "Marcus Angeloni",
      "Rodrigo de Freitas Pereira",
      "Helio Pedrini"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Angeloni_Age_Estimation_From_Facial_Parts_Using_Compact_Multi-Stream_Convolutional_Neural_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Angeloni_Age_Estimation_From_Facial_Parts_Using_Compact_Multi-Stream_Convolutional_Neural_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Age is a very useful property in the characterization of individuals, since it is an inherent biological attribute and plays a key role in many real-world applications such as preventing purchase of alcohol and tobacco by minors, human-computer interaction, soft biometrics, electronic customer relationship and as age synthesis in Forensic Art to find lost people. The aging process is influenced by external (health, lifestyle, smoking) and internal (genetics, gender) factors, which makes its estimation difficult for humans, and even more difficult for machines. In this work, we present and evaluate an age estimation approach in unconstrained images using facial parts (eyebrows, eyes, nose and mouth), cropped from the input images using landmarks, to feed a compact multi-stream convolutional neural network (CNN) architecture. Experimental results obtained in the challenging Adience benchmark with real-world images labeled with their respective age groups show that our method is competitive with the literature, even with a significantly smaller CNN and lower computational cost.\r",
    "code_link": "https://github.com/marcusangeloni/facialparts_age"
  },
  "iccv2019_cefrl_dynamicblocksparsereparameterizationofconvolutionalneuralnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Dynamic Block Sparse Reparameterization of Convolutional Neural Networks",
    "authors": [
      "Dharma teja Vooturi",
      "Girish Varma",
      "Kishore Kothapalli"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Vooturi_Dynamic_Block_Sparse_Reparameterization_of_Convolutional_Neural_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Vooturi_Dynamic_Block_Sparse_Reparameterization_of_Convolutional_Neural_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Sparse neural networks are efficient in both memory and compute when compared to dense neural networks. But on parallel hardware such as GPU, sparse neural networks result in small or no runtime performance gains. On the other hand, structured sparsity patterns like filter, channel and block sparsity result in large performance gains due to regularity induced by structure. Among structured sparsities, block sparsity is a generic structured sparsity pattern with filter and channel sparsity being sub cases of block sparsity. In this work, we focus on block sparsity and generate efficient block sparse convolutional neural networks using our approach DBSR (Dynamic block sparse reparameterization). Our DBSR approach, when applied on image classification task over Imagenet dataset, decreases parameters and FLOPS of ResneXt50 by a factor of 2x with only increase of 0.48 in Top-1 error. And when extended to the task of semantic segmentation, our approach reduces parameters and FLOPS by 30% and 20% respectively with only 1% decrease in mIoU for ERFNet over Cityscapes dataset. To ease developments in this line of work, we open sourced our code on github (https://github.com/idharmateja/bsnn).\r",
    "code_link": "https://github.com/idharmateja/bsnn"
  },
  "iccv2019_cefrl_dhasuperviseddeeplearningtohashwithanadaptivelossfunction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "DHA: Supervised Deep Learning to Hash with an Adaptive Loss Function",
    "authors": [
      "Jiehao Xu",
      "Chengyu Guo",
      "Qingjie Liu",
      "Jie Qin",
      "Yunhong Wang",
      "Li Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Xu_DHA_Supervised_Deep_Learning_to_Hash_with_an_Adaptive_Loss_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Xu_DHA_Supervised_Deep_Learning_to_Hash_with_an_Adaptive_Loss_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Hashing, which refers to the binary embedding of high-dimensional data, has been an effective solution for fast nearest neighbor retrieval in large-scale databases due to its computational and storage efficiency. Recently, deep learning to hash has been attracting increasing attention since it has shown great potential in improving retrieval quality by leveraging the strengths of deep neural networks. In this paper, we consider the problem of supervised hashing and propose an effective model (i.e., DHA), which is able to generate compact and discriminative binary codes while preserving semantic similarities of original data with an adaptive loss function. The key idea is that we scale and shift the loss function to avoid the saturation of gradients during training, and simultaneously adjust the loss to adapt to different levels of similarities of data. We evaluate the proposed DHA on three widely-used benchmarks, i.e., NUS-WIDE, CIFAR-10, and MS COCO. The state-of-the-art image retrieval performance clearly shows the effectiveness of our method in learning discriminative hash codes for nearest neighbor retrieval.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_sparsegenerativeadversarialnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Sparse Generative Adversarial Network",
    "authors": [
      "Shahin Mahdizadehaghdam",
      "Ashkan Panahi",
      "Hamid Krim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Mahdizadehaghdam_Sparse_Generative_Adversarial_Network_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Mahdizadehaghdam_Sparse_Generative_Adversarial_Network_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a new approach to Generative Adversarial Networks (GANs) to achieve an improved performance with additional robustness to its so-called and well-recognized mode collapse. We first proceed by mapping the desired data onto a frame-based space for a sparse representation to lift any limitation of small support features prior to learning the structure. To that end, we start by dividing an image into multiple patches and modifying the role of the generative network from producing an entire image, at once, to creating a sparse representation vector for each image patch. We synthesize an entire image by multiplying generated sparse representations to a pre-trained dictionary and assembling the resulting patches. This approach restricts the output of the generator to a particular structure, obtained by imposing a Union of Subspaces (UoS) model to the original training data, leading to more realistic images, while maintaining a desired diversity. To further regularize GANs in generating high-quality images and to avoid the notorious mode-collapse problem, we introduce a third player in GANs, called reconstructor. This player utilizes an auto-encoding scheme to ensure that first, the input-output relation in the generator is injective and second each real image corresponds to some input noise. We present a number of experiments, where the proposed algorithm shows a remarkably higher inception score compared to the equivalent conventional GANs.\r",
    "code_link": ""
  },
  "iccv2019_cefrl_efficientlearningonpointcloudswithbasispointsets": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CEFRL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Compact and Efficient Feature Representation and Learning in Computer Vision",
    "title": "Efficient Learning on Point Clouds with Basis Point Sets",
    "authors": [
      "Sergey Prokudin",
      "Christoph Lassner",
      "Javier Romero"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CEFRL/Prokudin_Efficient_Learning_on_Point_Clouds_with_Basis_Point_Sets_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CEFRL/Prokudin_Efficient_Learning_on_Point_Clouds_with_Basis_Point_Sets_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With an increased availability of 3D scanning technology, point clouds are moving into the focus of computer vision as a rich representation of everyday scenes. However, they are hard to handle for machine learning algorithms due to their unordered structure. One common approach is to apply occupancy grid mapping, which dramatically increases the amount of data stored and at the same time loses details through discretization. Recently, deep learning models were proposed to handle point clouds directly and achieve input permutation invariance. However, these architectures often use an increased number of parameters and are computationally inefficient. In this work we propose basis point sets (BPS) as a highly efficient and fully general way to process point clouds with machine learning algorithms. The basis point set representation is a residual representation that can be computed efficiently and can be used with standard neural network architectures and other machine learning algorithms. Using the proposed representation as the input to a simple fully connected network allows us to match the performance of PointNet on a shape classification task, while using three orders of magnitude less floating point operations. In a second experiment, we show how the proposed representation can be used for registering high resolution meshes to noisy 3D scans. Here, we present the first method for single-pass high-resolution mesh registration, avoiding time-consuming per-scan optimization and allowing real-time execution.\r",
    "code_link": "https://github.com/sergeyprokudin/bps"
  },
  "iccv2019_3dfaw_the2nd3dfacealignmentinthewildchallenge(3dfaw-video)densereconstructionfromvideo": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DFAW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Face Alignment in the Wild Challenge",
    "title": "The 2nd 3D Face Alignment in the Wild Challenge (3DFAW-Video): Dense Reconstruction From Video",
    "authors": [
      "Rohith Krishnan Pillai",
      "Laszlo Attila Jeni",
      "Huiyuan Yang",
      "Zheng Zhang",
      "Lijun Yin",
      "Jeffrey F. Cohn"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DFAW/Pillai_The_2nd_3D_Face_Alignment_in_the_Wild_Challenge_3DFAW-Video_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DFAW/Pillai_The_2nd_3D_Face_Alignment_in_the_Wild_Challenge_3DFAW-Video_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " 3D face alignment approaches have strong advantages over 2D with respect to representational power and robustness to illumination and pose. Over the past few years a number of research groups have made rapid advances in dense 3D alignment from 2D video and obtained impressive results. How these various methods compare is relatively unknown. Previous benchmarks addressed sparse 3D alignment and single image 3D reconstruction. No commonly accepted evaluation protocol exists for dense 3D face reconstruction from video with which to compare them. The 2nd 3D Face Alignment in the Wild from Videos (3DFAW-Video) Challenge extends the previous 3DFAW 2016 competition to the estimation of dense 3D facial structure from video. It presented a new large corpora of profile-to-profile face videos recorded under different imaging conditions and annotated with corresponding high-resolution 3D ground truth meshes. In this paper we outline the evaluation protocol, the data used, and the results. 3DFAW-Video is to be held in conjunction with the 2019 International Conference on Computer Vision, in Seoul, Korea.\r",
    "code_link": ""
  },
  "iccv2019_3dfaw_3dfaceshaperegressionfrom2dvideoswithmulti-reconstructionandmeshretrieval": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DFAW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Face Alignment in the Wild Challenge",
    "title": "3D Face Shape Regression From 2D Videos with Multi-Reconstruction and Mesh Retrieval",
    "authors": [
      "Xiaohu Shao",
      "Jiangjing Lyu",
      "Junliang Xing",
      "Lijun Zhang",
      "Xiaobo Li",
      "Xiangdong Zhou",
      "Yu Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DFAW/Shao_3D_Face_Shape_Regression_From_2D_Videos_with_Multi-Reconstruction_and_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DFAW/Shao_3D_Face_Shape_Regression_From_2D_Videos_with_Multi-Reconstruction_and_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper introduces our submission to the 2nd 3DFAW Challenge. To get a high-accuracy 3D dense face shape based on 2D videos or multiple images, a framework which is consist of multi-reconstruction branches and a mesh retrieval module, is proposed to effectively utilize the information of all frames and the results predicted by all branches. The recent state-of-the-art methods based on single-view and multi-view are introduced to form an ensemble of independent regression networks. The candidate 3D shape of each branch is synthesized by weighted linear combination of the results on all frames to boost the depth estimation and invisible regions reconstruction. Finally, the best fitting mesh is retrieved according to the distance between the synthesized texture and the ground truth texture. Experiment results show that our approach obtains competitive results near the accuracy of \"pseudo\" ground truths, and achieves superior performance over most of submissions by other teams in the testing phases.\r",
    "code_link": ""
  },
  "iccv2019_3dfaw_multi-view3dfacereconstructioninthewildusingsiamesenetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "3DFAW",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - 3D Face Alignment in the Wild Challenge",
    "title": "Multi-View 3D Face Reconstruction in the Wild Using Siamese Networks",
    "authors": [
      "Eduard Ramon",
      "Janna Escur",
      "Xavier Giro-i-Nieto"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/3DFAW/Ramon_Multi-View_3D_Face_Reconstruction_in_the_Wild_Using_Siamese_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/3DFAW/Ramon_Multi-View_3D_Face_Reconstruction_in_the_Wild_Using_Siamese_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work, we present a novel learning based approach to reconstruct 3D faces from a single or multiple images. Our method uses a simple yet powerful architecture based on siamese neural networks that helps to extract relevant features from each view while keeping the models small. Instead of minimizing multiple objectives, we propose to simultaneously learn the 3D shape and the individual camera poses by using a single term loss based on the reprojection error, which generalizes from one to multiple views. This allows to globally optimize the whole scene without having to tune any hyperparameters and to achieve low reprojection errors, which are important for further texture generation. Finally, we train our model on a large scale dataset with more than 6,000 facial scans. We report competitive results in 3DFAW 2019 challenge, showing the effectiveness of our method.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_improvingfashionlandmarkdetectionbydualattentionfeatureenhancement": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Improving Fashion Landmark Detection by Dual Attention Feature Enhancement",
    "authors": [
      "Ming Chen",
      "Yingjie Qin",
      "Lizhe Qi",
      "Yunquan Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Chen_Improving_Fashion_Landmark_Detection_by_Dual_Attention_Feature_Enhancement_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Chen_Improving_Fashion_Landmark_Detection_by_Dual_Attention_Feature_Enhancement_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Fashion landmark detection is a fundamental problem in visual fashion analyze, which aims at locating the precise coordinates of functional key points defined on clothes. Dozens of deep learning-based methods are proposed to address this problem. How to extract adequate and effective features is a critical point for this challenging task. In this paper, we propose the Dual Attention Feature Enhancement(DAFE) module, which strengthens the extracted features by adaptively reusing low-level image details and emphasizing informative parts. First, DAFE enhances the pixel-wise information through capturing the spatial details from low-level features by the guidance of attention matrix, which is generated from high-level ones. Second, DAFE emphasizes task-related features by modeling long-range relationships between channels. Experimental experiments on Deepfashion and FLD datasets demonstrate that our method achieves state-of-the-art performance, and our approach also achieves competitive results on Deepfashion2 Landmark Estimation Challenge.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_uvtonuvmappingtoconsiderthe3dstructureofahumaninimage-basedvirtualtry-onnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "UVTON: UV Mapping to Consider the 3D Structure of a Human in Image-Based Virtual Try-On Network",
    "authors": [
      "Shizuma Kubo",
      "Yusuke Iwasawa",
      "Masahiro Suzuki",
      "Yutaka Matsuo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Kubo_UVTON_UV_Mapping_to_Consider_the_3D_Structure_of_a_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Kubo_UVTON_UV_Mapping_to_Consider_the_3D_Structure_of_a_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Image-based virtual try-on is an area of research that is attracting attention as the demand for online apparel shopping continues to increase. The methods proposed thus far have focused on how to generate a dress-up image while preserving the clothing details. However, the posture of the model in the image is limited to an upright position, and other positions frequently do not work well. In this study, based on a kind of generative adversarial network (GAN) that utilizes UV mapping to consider the 3D structure of the human body, we propose a novel virtual try-on method called a UV Try-On Network (UVTON). We use a DensePose to estimate a point corresponding to the 3D surface of a human model for each pixel point of a 2D image and incorporate the estimated information into our model. It is thus possible to change the clothes of users holding various postures. Our proposed method uses UV mapping and two other modules. One module generates parts to be used in the mapping, and the other refines the image and produces a more realistic image. Based on both qualitative and quantitative comparison with existing methods, we experimentally demonstrated that our method achieved better results with various postures.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_fashionimageretrievalwithcapsulenetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Fashion Image Retrieval with Capsule Networks",
    "authors": [
      "Furkan Kinli",
      "Baris Ozcan",
      "Furkan Kirac"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Kinli_Fashion_Image_Retrieval_with_Capsule_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Kinli_Fashion_Image_Retrieval_with_Capsule_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this study, we investigate in-shop clothing retrieval performance of densely-connected Capsule Networks with dynamic routing. To achieve this, we propose Triplet-based design of Capsule Network architecture with two different feature extraction methods. In our design, Stacked-convolutional (SC) and Residual-connected (RC) blocks are used to form the input of capsule layers. Experimental results show that both of our designs outperform all variants of the baseline study, namely FashionNet, without relying on the landmark information. Moreover, when compared to the SOTA architectures on clothing retrieval, our proposed Triplet Capsule Networks achieve comparable recall rates only with half of parameters used in the SOTA architectures.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_theimaterialistfashionattributedataset": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "The iMaterialist Fashion Attribute Dataset",
    "authors": [
      "Sheng Guo",
      "Weilin Huang",
      "Xiao Zhang",
      "Prasanna Srikhanta",
      "Yin Cui",
      "Yuan Li",
      "Hartwig Adam",
      "Matthew R. Scott",
      "Serge Belongie"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Guo_The_iMaterialist_Fashion_Attribute_Dataset_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Guo_The_iMaterialist_Fashion_Attribute_Dataset_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Many Large-scale image databases such as ImageNet have significantly advanced image classification and other visual recognition tasks. However much of these datasets are constructed only for single-label and coarse object-level classification. For real-world applications, multiple labels and fine-grained categories are often needed, yet very few such datasets exist publicly, especially those of large-scale and high quality. In this work, we contribute to the community a new dataset called iMaterialist Fashion Attribute (iFashion-Attribute) to address this problem in the fashion domain. The dataset is constructed from over one million fashion images with a label space that includes 8 groups of 228 fine-grained attributes in total. Each image is annotated by experts with multiple, high-quality fashion attributes. The result is the first known million-scale multi-label and fine-grained image dataset. We conduct extensive experiments and provide baseline results with modern deep Convolutional Neural Networks (CNNs). Additionally, we demonstrate models pre-trained on iFashion-Attribute achieve superior transfer learning performance on fashion related tasks compared with pre-training from ImageNet or other fashion datasets.\r",
    "code_link": "https://github.com/visipedia/imat_fashion_comp"
  },
  "iccv2019_cvfad_translatingvisualartintomusic": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Translating Visual Art Into Music",
    "authors": [
      "Maximilian Muller-Eberstein",
      "Nanne van Noord"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Muller-Eberstein_Translating_Visual_Art_Into_Music_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Muller-Eberstein_Translating_Visual_Art_Into_Music_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The Synesthetic Variational Autoencoder (SynVAE) introduced in this research is able to learn a consistent mapping between visual and auditive sensory modalities in the absence of paired datasets. A quantitative evaluation on MNIST as well as the Behance Artistic Media dataset (BAM) shows that SynVAE is capable of retaining sufficient information content during the translation while maintaining cross-modal latent space consistency. In a qualitative evaluation trial, human evaluators were furthermore able to match musical samples with the images which generated them with accuracies of up to 73%.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_semanticallyconsistenthierarchicaltexttofashionimagesynthesiswithanenhanced-attentionalgenerativeadversarialnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Semantically Consistent Hierarchical Text to Fashion Image Synthesis with an Enhanced-Attentional Generative Adversarial Network",
    "authors": [
      "Kenan Emir Ak",
      "Joo Hwee Lim",
      "Jo Yew Tham",
      "Ashraf Kassim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Ak_Semantically_Consistent_Hierarchical_Text_to_Fashion_Image_Synthesis_with_an_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Ak_Semantically_Consistent_Hierarchical_Text_to_Fashion_Image_Synthesis_with_an_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we present the enhanced Attentional Generative Adversarial Network (e-AttnGAN) with improved training stability for text-to-image synthesis. e-AttnGAN's integrated attention module utilizes both sentence and word context features and performs feature-wise linear modulation (FiLM) to fuse visual and natural language representations. In addition to multimodal similarity learning for text and image features of AttnGAN, cosine and feature matching losses of real and generated images are included while employing a classification loss for \"significant attributes\". In order to improve the stability of the training and solve the issue of model collapse, spectral normalization and two-time scale update for the discriminator are used together with instance noise. Our experiments show that e-AttnGAN outperforms state-of-the-art methods on the FashionGen and DeepFashion-Synthesis datasets.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_poseguidedattentionformulti-labelfashionimageclassification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Pose Guided Attention for Multi-Label Fashion Image Classification",
    "authors": [
      "Beatriz Quintino Ferreira",
      "Joao P. Costeira",
      "Ricardo G. Sousa",
      "Liang-Yan Gui",
      "Joao P. Gomes"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Ferreira_Pose_Guided_Attention_for_Multi-Label_Fashion_Image_Classification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Ferreira_Pose_Guided_Attention_for_Multi-Label_Fashion_Image_Classification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a compact framework with guided attention for multi-label classification in the fashion domain. Our visual semantic attention model (VSAM) is supervised by automatic pose extraction creating a discriminative feature space. VSAM outperforms the state of the art for an in-house dataset and performs on pair with previous works on the DeepFashion dataset, even without using any landmark annotations. Additionally, we show that our semantic attention module brings robustness to large quantities of wrong annotations and provides more interpretable results.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_la-vitonanetworkforlooking-attractivevirtualtry-on": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "LA-VITON: A Network for Looking-Attractive Virtual Try-On",
    "authors": [
      "Hyug Jae Lee",
      "Rokkyu Lee",
      "Minseok Kang",
      "Myounghoon Cho",
      "Gunhan Park"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Lee_LA-VITON_A_Network_for_Looking-Attractive_Virtual_Try-On_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Lee_LA-VITON_A_Network_for_Looking-Attractive_Virtual_Try-On_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we propose an image-based virtual try-on network, LA-VITON, which allows the generation of high fidelity try-on images that preserves both the overall appearance and the characteristics of clothing items. The proposed network consists of two modules: Geometric Matching Module (GMM) and Try-On Module (TOM). To warp in-shop clothing item to the desired image of a person with high accuracy in GMM, grid interval consistency loss and an occlusion handling technique are proposed. Grid interval consistency loss regularizes transformation to prevent distortion of patterns in clothes and an occlusion handling technique encourages proper warping despite target bodies are covered by hair or arms. The following TOM synthesizes the final try-on image of the target person seamlessly with the warped clothes from GMM. Extensive experiments on fashion datasets show that the proposed method outperforms the state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_semanticsegmentationoffashionimagesusingfeaturepyramidnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Semantic Segmentation of Fashion Images Using Feature Pyramid Networks",
    "authors": [
      "John Martinsson",
      "Olof Mogren"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Martinsson_Semantic_Segmentation_of_Fashion_Images_Using_Feature_Pyramid_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Martinsson_Semantic_Segmentation_of_Fashion_Images_Using_Feature_Pyramid_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work, we approach the problem of semantically segmenting fashion images into different categories of clothing. This problem poses particular challenges because of the importance of both textural information and cues from shapes and context. To this end, we propose a fully convolutional neural network based on feature pyramid networks (FPN), together with a backbone consisting of the ResNeXt architecture. Our experimental evaluation shows that the proposed model achieves state-of-the-art results on two standard fashion benchmark datasets, and a qualitative study verifies its effectiveness when applied to typical fashion images. The approach has a modest memory footprint and can be used without a conditional random field (CRF) without much degradation of quality which makes our model preferable from a computational perspective. When comparing all methods without a CRF, our approach outperforms all state-of-the-art models on both datasets by a clear margin in all evaluated metrics. In fact, our approach achieves a higher accuracy without the CRF than the state-of-the-art models using CRFs.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_dropoutinducednoiseforco-creativegansystems": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Dropout Induced Noise for Co-Creative GAN Systems",
    "authors": [
      "Sabine Wieluch",
      "Friedhelm Schwenker"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Wieluch_Dropout_Induced_Noise_for_Co-Creative_GAN_Systems_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Wieluch_Dropout_Induced_Noise_for_Co-Creative_GAN_Systems_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper demonstrates how Dropout can be used in Generative Adversarial Networks to generate multiple different outputs to one input. This method is thought as an alternative to latent space exploration, especially if constraints in the input should be preserved, like in A-to-B translation tasks.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_deepgarmentimagemattingforavirtualtry-onsystem": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Deep Garment Image Matting for a Virtual Try-on System",
    "authors": [
      "Dongjoe Shin",
      "Yu Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Shin_Deep_Garment_Image_Matting_for_a_Virtual_Try-on_System_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Shin_Deep_Garment_Image_Matting_for_a_Virtual_Try-on_System_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " To improve online shopping experience, many fashion retailers try to provide high quality garment images, capturing fine details as well as various opacities. A skilled operator can deliver a satisfactory result using manual segmentation tools, but it is challenging to scale up this process to address seasonal demands. To balance the quality and the processing cost, we investigate the use of a deep learning based matting technique that can produce a high quality alpha map from an approximate garment segmentation. The proposed model adopts the deep image matting model, but we replace the refinement network with a sequence of recursive convolutional network (RCN) units. Our main motivation for this modification is that the fine garment details created by different materials are represented better with the mixture of the image features from different scales. Therefore, we need to construct deeper convolutional layers for better scale analysis but we also need to maintain the number of unknowns low as producing training data is expensive. The proposed RCN based refinement network can address these conflicting restrictions well and our experiments demonstrate that it can achieve a lower training loss and produce better prediction results than the baseline refinement model under the same training condition.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_clothingrecognitioninthewildusingtheamazoncatalog": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Clothing Recognition in the Wild using the Amazon Catalog",
    "authors": [
      "Fabian Caba Heilbron",
      "Bojan Pepik",
      "Zohar Barzelay",
      "Michael Donoser"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Heilbron_Clothing_Recognition_in_the_Wild_using_the_Amazon_Catalog_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Heilbron_Clothing_Recognition_in_the_Wild_using_the_Amazon_Catalog_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The emergence of online influencers, the explosion of video content, and the massive amount of movie collections have served as an advertising vehicle for the fashion industry. This trend has created the need for automated methods that recognize people's outfit in such image and video collections. However, existing computer vision solutions for fashion recognition require an enormous amount of labeled data for training, which is prohibitively expensive. In this work, we propose an approach to build clothing recognition models for real-world scenarios. Our approach exploits images from the Amazon Catalog as training data. By using the catalog data as an additional training source, we boost the recognition accuracy on the challenging real world images of the DeepFashion dataset achieving stateof-the-art performance. We introduce the first dataset for clothing recognition in movies. In this scenario, we find that the use of catalog data for training becomes even more crucial, as it provides an accuracy boost of 10%.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_regularizedadversarialtrainingforsingle-shotvirtualtry-on": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Regularized Adversarial Training for Single-Shot Virtual Try-On",
    "authors": [
      "Kotaro Kikuchi",
      "Kota Yamaguchi",
      "Edgar Simo-Serra",
      "Tetsunori Kobayashi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Kikuchi_Regularized_Adversarial_Training_for_Single-Shot_Virtual_Try-On_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Kikuchi_Regularized_Adversarial_Training_for_Single-Shot_Virtual_Try-On_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Spatially placing an object onto a background is an essential operation in graphic design and facilitates many different applications such as virtual try-on. The placing operation is formulated as a geometric inference problem for given foreground and background images, and has been approached by spatial transformer architecture. In this paper, we propose a simple yet effective regularization technique to guide the geometric parameters based on user-defined trust regions. Our approach stabilizes the training process of spatial transformer networks and achieves a high-quality prediction with single-shot inference. Our proposed method is independent of initial parameters, and can easily incorporate various priors to prevent different types of trivial solutions. Empirical evaluation with the Abstract Scenes and CelebA datasets shows that our approach achieves favorable results compared to baselines.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_aglobal-localembeddingmoduleforfashionlandmarkdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "A Global-Local Embedding Module for Fashion Landmark Detection",
    "authors": [
      "Sumin Lee",
      "Sungchan Oh",
      "Chanho Jung",
      "Changick Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Lee_A_Global-Local_Embedding_Module_for_Fashion_Landmark_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Lee_A_Global-Local_Embedding_Module_for_Fashion_Landmark_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Detecting fashion landmarks is a fundamental technique for visual clothing analysis. Due to the large variation and non-rigid deformation of clothes, localizing fashion landmarks suffers from large spatial variances across poses, scales, and styles. Therefore, understanding contextual knowledge of clothes is required for accurate landmark detection. To that end, in this paper, we propose a fashion landmark detection network with a global-local embedding module. The global-local embedding module is based on a non-local operation for capturing long-range dependencies and a subsequent convolution operation for adopting local neighborhood relations. With this processing, the network can consider both global and local contextual knowledge for a clothing image. We demonstrate that our proposed method has an excellent ability to learn advanced deep feature representations for fashion landmark detection. Experimental results on two benchmark datasets show that the proposed network outperforms the state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_artist-guidedsemiautomaticanimationcolorization": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Artist-Guided Semiautomatic Animation Colorization",
    "authors": [
      "Harrish Thasarathan",
      "Mehran Ebrahimi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Thasarathan_Artist-Guided_Semiautomatic_Animation_Colorization_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Thasarathan_Artist-Guided_Semiautomatic_Animation_Colorization_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " There is a delicate balance between automating repetitive work in creative domains while staying true to an artist's vision. The animation industry regularly outsources large animation workloads to foreign countries where labor is inexpensive and long hours are common. Automating part of this process can be incredibly useful for reducing costs and creating manageable workloads for major animation studios and outsourced artists. We present a method for automating line art colorization by keeping artists in the loop to successfully reduce this workload while staying true to an artist's vision. By incorporating color hints and temporal information to an adversarial image-to-image framework, we show that it is possible to meet the balance between automation and authenticity through artist's input to generate colored frames with temporal consistency.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_generatinghigh-resolutionfashionmodelimageswearingcustomoutfits": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Generating High-Resolution Fashion Model Images Wearing Custom Outfits",
    "authors": [
      "Gokhan Yildirim",
      "Nikolay Jetchev",
      "Roland Vollgraf",
      "Urs Bergmann"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Yildirim_Generating_High-Resolution_Fashion_Model_Images_Wearing_Custom_Outfits_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Yildirim_Generating_High-Resolution_Fashion_Model_Images_Wearing_Custom_Outfits_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Visualizing an outfit is an essential part of shopping for clothes. Due to the combinatorial aspect of combining fashion articles, the available images are limited to a pre-determined set of outfits. In this paper, we broaden these visualizations by generating high-resolution images of fashion models wearing a custom outfit under an input body pose. We show that our approach can not only transfer the style and the pose of one generated outfit to another, but also create realistic images of human bodies and garments.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_deepmetriclearningforcross-domainfashioninstanceretrieval": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Deep Metric Learning for Cross-Domain Fashion Instance Retrieval",
    "authors": [
      "Sarah Ibrahimi",
      "Nanne van Noord",
      "Zeno Geradts",
      "Marcel Worring"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Ibrahimi_Deep_Metric_Learning_for_Cross-Domain_Fashion_Instance_Retrieval_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Ibrahimi_Deep_Metric_Learning_for_Cross-Domain_Fashion_Instance_Retrieval_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The goal of this paper is to find an effective method to retrieve an image with a fashion instance from one domain based on a similar fashion instance image from a different domain. Where existing works focus on retrieving relevant shop images based on a consumer instance, we introduce the reverse task and treat both tasks equally in our training setup. We use several deep metric learning techniques to get baseline scores for these tasks on the DeepFashion2 dataset and we show how ensemble methods can be used to boost the performance.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_generativemodellingofsemanticsegmentationdatainthefashiondomain": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Generative Modelling of Semantic Segmentation Data in the Fashion Domain",
    "authors": [
      "Marie Korneliusson",
      "John Martinsson",
      "Olof Mogren"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Korneliusson_Generative_Modelling_of_Semantic_Segmentation_Data_in_the_Fashion_Domain_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Korneliusson_Generative_Modelling_of_Semantic_Segmentation_Data_in_the_Fashion_Domain_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work, we propose a method to generatively model the joint distribution of images and corresponding semantic segmentation masks using generative adversarial networks. We extend the Style-GAN architecture by iteratively growing the network during training, to add new output channels that model the semantic segmentation masks. We train the proposed method on a large dataset of fashion images and our experimental evaluation shows that the model produces samples that are coherent and plausible with semantic segmentation masks that closely match the semantics in the image.\r",
    "code_link": "https://github.com/NVlabs/stylegan"
  },
  "iccv2019_cvfad_fourier-cppnsforimagesynthesis": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Fourier-CPPNs for Image Synthesis",
    "authors": [
      "Mattie Tesfaldet",
      "Xavier Snelgrove",
      "David Vazquez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Tesfaldet_Fourier-CPPNs_for_Image_Synthesis_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Tesfaldet_Fourier-CPPNs_for_Image_Synthesis_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Compositional Pattern Producing Networks (CPPNs) are differentiable networks that independently map (x, y) pixel coordinates to (r, g, b) colour values. Recently, CPPNs have been used for creating interesting imagery for creative purposes, e.g., neural art. However their architecture biases generated images to be overly smooth, lacking high-frequency detail. In this work, we extend CPPNs to explicitly model the frequency information for each pixel output, capturing frequencies beyond the DC component. We show that our Fourier-CPPNs (F-CPPNs) provide improved visual detail for image synthesis.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_aweaklysupervisedadaptivetripletlossfordeepmetriclearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "A Weakly Supervised Adaptive Triplet Loss for Deep Metric Learning",
    "authors": [
      "Xiaonan Zhao",
      "Huan Qi",
      "Rui Luo",
      "Larry Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Zhao_A_Weakly_Supervised_Adaptive_Triplet_Loss_for_Deep_Metric_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Zhao_A_Weakly_Supervised_Adaptive_Triplet_Loss_for_Deep_Metric_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We address the problem of distance metric learning in visual similarity search, defined as learning an image embedding model which projects images into Euclidean space where semantically and visually similar images are closer and dissimilar images are further from one another. We present a weakly supervised adaptive triplet loss (ATL) capable of capturing fine-grained semantic similarity that encourages the learned image embedding models to generalize well on cross-domain data. The method uses weakly labeled product description data to implicitly determine fine grained semantic classes, avoiding the need to annotate large amounts of training data. We evaluate on the Amazon fashion retrieval benchmark and DeepFashion in-shop retrieval data. The method boosts the performance of triplet loss baseline by 10.6% on cross-domain data and out-performs the state-of-art model on all evaluation metrics.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_unsupervisedimage-to-videoclothingtransfer": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Unsupervised Image-to-Video Clothing Transfer",
    "authors": [
      "Albert Pumarola",
      "Vedanuj Goswami",
      "Francisco Vicente",
      "Fernando De la Torre",
      "Francesc Moreno-Noguer"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Pumarola_Unsupervised_Image-to-Video_Clothing_Transfer_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Pumarola_Unsupervised_Image-to-Video_Clothing_Transfer_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present a system to photo-realistically transfer the clothing of a person in a reference image into another person in an unconstrained image or video. Our architecture is based on a GAN equipped with a physical memory that updates an initially incomplete texture map of the clothes that is progressively completed with the new inferred occluded parts. The system is trained in an unsupervised manner. The results are visually appealing and open the possibility to be used in the future as a quick virtual try-on clothing system.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_walkingthroughshanshuigeneratingchineseshanshuipaintingsviareal-timetrackingofhumanposition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Walking Through Shanshui: Generating Chinese Shanshui Paintings via Real-Time Tracking of Human Position",
    "authors": [
      "Aven Le Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Le_Zhou_Walking_Through_Shanshui_Generating_Chinese_Shanshui_Paintings_via_Real-Time_Tracking_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Le_Zhou_Walking_Through_Shanshui_Generating_Chinese_Shanshui_Paintings_via_Real-Time_Tracking_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Shanshui is a traditional East Asian style of ink brush painting that depicts natural landscapes in a semi-abstract fashion. To create a Shanshui painting, ancient Chinese scholar-artists rely heavily on their travel experiences as well as their movements in natural spaces. In this paper, we propose an interactive system - \"Walking Through Shanshui\" - based on AI using Generative Adversarial Networks and various computer vision techniques. The system is an interactive art installation that helps bring the original experience of creating Shanshui to participants by tracking their movement through walking in a virtual space. It uses position tracking as input to generate Shanshui from participant's movements and to paint with a custom generative Sketch-to-Shanshui translation model. The system detects the participant's position in real-time and automatically traces it to generate a Shanshui painting instantly.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_class-basedstylingreal-timelocalizedstyletransferwithsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Class-Based Styling: Real-Time Localized Style Transfer with Semantic Segmentation",
    "authors": [
      "Lironne Kurzman",
      "David Vazquez",
      "Issam Laradji"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Kurzman_Class-Based_Styling_Real-Time_Localized_Style_Transfer_with_Semantic_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Kurzman_Class-Based_Styling_Real-Time_Localized_Style_Transfer_with_Semantic_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a Class-Based Styling method (CBS) that can map different styles for different object classes in real-time. CBS achieves real-time performance by carrying out two steps simultaneously. While a semantic segmentation method is used to obtain the mask of each object class in a video frame, a styling method is used to style that frame globally. Then an object class can be styled by combining the segmentation mask and the styled image. The user can also select multiple styles so that different object classes can have different styles in a single frame. For semantic segmentation, we leverage DABNet that achieves high accuracy, yet only has 0.76 million parameters and runs at 104 FPS. For the style transfer step, we use the popular real-time method proposed by Johnson et al. [7]. We evaluated CBS on a video of the CityScapes dataset and observed high-quality localized style transfer results for different object classes and real-time performance.\r",
    "code_link": "https://github.com/IssamLaradji/CBStyling"
  },
  "iccv2019_cvfad_poweringvirtualtry-onviaauxiliaryhumansegmentationlearning": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Powering Virtual Try-On via Auxiliary Human Segmentation Learning",
    "authors": [
      "Kumar Ayush",
      "Surgan Jandial",
      "Ayush Chopra",
      "Balaji Krishnamurthy"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Ayush_Powering_Virtual_Try-On_via_Auxiliary_Human_Segmentation_Learning_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Ayush_Powering_Virtual_Try-On_via_Auxiliary_Human_Segmentation_Learning_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Image-based virtual try-on for fashion has gained considerable attention recently. This task requires to fit an in-shop cloth image on a target model image. An efficient framework for this is composed of two stages: (1) warping the try-on cloth to align with the body shape and pose of the target model, and (2) an image composition module to seamlessly integrate the warped try-on cloth onto the target model image. Existing methods suffer from artifacts and distortions in their try-on output. In this work, we propose to use auxiliary learning to power an existing state-of-the-art virtual try-on network. We leverage prediction of human semantic segmentation (of the target model wearing the try-on cloth) as an auxiliary task and show that it allows the network to better model the bounds of the clothing item and human skin, thereby producing a better fit. Using exhaustive qualitative and quantitative evaluation we show that there is a significant improvement in the preservation of characteristics of the cloth and person in the final try-on result, thereby outperforming the existing state-of-the-art virtual try-on framework.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_leveragingclasshierarchyinfashionclassification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "Leveraging Class Hierarchy in Fashion Classification",
    "authors": [
      "Hyunsoo Cho",
      "Chaemin Ahn",
      "Kang Min Yoo",
      "Jinseok Seol",
      "Sang-goo Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Cho_Leveraging_Class_Hierarchy_in_Fashion_Classification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Cho_Leveraging_Class_Hierarchy_in_Fashion_Classification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The online commerce market has been growing rapidly, spurring interest in the deep fashion domain from the research community. Among various tasks in the fashion domain, the classification problem is the vital one, because metadata extraction through fashion classification has tremendous industrial value. A flurry of recent deep-learning based models have been proposed for the task and have showed great performances but they fail to capture the hierarchical nature of fashion annotations, such as 'pant' and 'skirt' both having 'bottom' as the superordinate. In this preliminary work, we propose a novel fashion classification model that works in a hierarchical manner. Experimental results on large fashion datasets show that our intuition, taking into account hierarchical dependencies between class labels, can help improve performance.\r",
    "code_link": ""
  },
  "iccv2019_cvfad_deepmarkone-shotclothingdetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Computer Vision for Fashion, Art and Design",
    "title": "DeepMark: One-Shot Clothing Detection",
    "authors": [
      "Alexey Sidnev",
      "Alexey Trushkov",
      "Maxim Kazakov",
      "Ivan Korolev",
      "Vladislav Sorokin"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CVFAD/Sidnev_DeepMark_One-Shot_Clothing_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CVFAD/Sidnev_DeepMark_One-Shot_Clothing_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The one-shot approach, DeepMark, for fast clothing detection as a modification of a multi-target network, CenterNet, is proposed in the paper. The state-of-the-art accuracy of 0.723 mAP for bounding box detection task and 0.532 mAP for landmark detection task on the DeepFashion2 Challenge dataset were achieved. The proposed architecture can be used effectively on the low-power devices.\r",
    "code_link": ""
  },
  "iccv2019_task-cv_incrementallearningtechniquesforsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Incremental Learning Techniques for Semantic Segmentation",
    "authors": [
      "Umberto Michieli",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Michieli_Incremental_Learning_Techniques_for_Semantic_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Michieli_Incremental_Learning_Techniques_for_Semantic_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep learning architectures exhibit a critical drop of performance due to catastrophic forgetting when they are required to incrementally learn new tasks. Contemporary incremental learning frameworks focus on image classification and object detection while in this work we formally introduce the incremental learning problem for semantic segmentation in which a pixel-wise labeling is considered. To tackle this task we propose to distill the knowledge of the previous model to retain the information about previously learned classes, whilst updating the current model to learn the new ones. We propose various approaches working both on the output logits and on intermediate features. In opposition to some recent frameworks, we do not store any image from previously learned classes and only the last model is needed to preserve high accuracy on these classes. The experimental evaluation on the Pascal VOC2012 dataset shows the effectiveness of the proposed approaches.\r",
    "code_link": "https://github.com/DrSleep/tensor\ufb02ow-deeplab-resnet"
  },
  "iccv2019_task-cv_multi-leveldomainadaptivelearningforcross-domaindetection": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Multi-Level Domain Adaptive Learning for Cross-Domain Detection",
    "authors": [
      "Rongchang Xie",
      "Fei Yu",
      "Jiachao Wang",
      "Yizhou Wang",
      "Li Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Xie_Multi-Level_Domain_Adaptive_Learning_for_Cross-Domain_Detection_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Xie_Multi-Level_Domain_Adaptive_Learning_for_Cross-Domain_Detection_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years, object detection has shown impressive results using supervised deep learning, but it remains challenging in a cross-domain environment. The variations of illumination, style, scale, and appearance in different domains can seriously affect the performance of detection models. Previous works use adversarial training to align global features across the domain shift and to achieve image information transfer. However, such methods do not effectively match the distribution of local features, resulting in limited improvement in cross-domain object detection. To solve this problem, we propose a multi-level domain adaptive model to simultaneously align the distributions of local-level features and global-level features. We evaluate our method with multiple experiments, including adverse weather adaptation, synthetic data adaptation, and cross camera adaptation. In most object categories, the proposed method achieves superior performance against state-of-the-art techniques, which demonstrates the effectiveness and robustness of our method.\r",
    "code_link": ""
  },
  "iccv2019_task-cv_improvingcnnclassifiersbyestimatingtest-timepriors": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Improving CNN Classifiers by Estimating Test-Time Priors",
    "authors": [
      "Milan Sulc",
      "Jiri Matas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Sulc_Improving_CNN_Classifiers_by_Estimating_Test-Time_Priors_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Sulc_Improving_CNN_Classifiers_by_Estimating_Test-Time_Priors_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The problem of different training and test set class priors is addressed in the context of CNN classifiers. We compare two approaches to the estimation of the unknown test priors: an existing Maximum Likelihood Estimation (MLE) method and a proposed Maximum a Posteriori (MAP) approach introducing a Dirichlet hyper-prior on the class prior probabilities. Experimental results show a significant improvement in the fine-grained classification tasks using known evaluation-time priors, increasing top-1 accuracy by 4.0% on the FGVC iNaturalist 2018 validation set and by 3.9% on the FGVCx Fungi 2018 validation set. Estimation of the unknown test set priors noticeably increases the accuracy on the PlantCLEF dataset, allowing a single CNN model to achieve state-of-the-art results and to outperform the competition-winning ensemble of 12 CNNs. The proposed MAP estimation increases the prediction accuracy by 2.8% on PlantCLEF 2017 and by 1.8% on FGVCx Fungi, where the MLE method decreases accuracy.\r",
    "code_link": ""
  },
  "iccv2019_task-cv_hallucinatingagnosticimagestogeneralizeacrossdomains": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Hallucinating Agnostic Images to Generalize Across Domains",
    "authors": [
      "Fabio Maria Carlucci",
      "Paolo Russo",
      "Tatiana Tommasi",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Carlucci_Hallucinating_Agnostic_Images_to_Generalize_Across_Domains_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Carlucci_Hallucinating_Agnostic_Images_to_Generalize_Across_Domains_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The ability to generalize across visual domains is crucial for the robustness of artificial recognition systems. Although many training sources may be available in real contexts, the access to even unlabeled target samples cannot be taken for granted, which makes standard unsupervised domain adaptation methods inapplicable in the wild. In this work we investigate how to exploit multiple sources by hallucinating a deep visual domain composed of images, possibly unrealistic, able to maintain categorical knowledge while discarding specific source styles. The produced agnostic images are the result of a deep architecture that applies pixel adaptation on the original source data guided by two adversarial domain classifier branches at image and feature level. Our approach is conceived to learn only from source data, but it seamlessly extends to the use of unlabeled target samples. Remarkable results for both multi-source domain adaptation and domain generalization support the power of hallucinating agnostic images in this framework.\r",
    "code_link": "https://github.com/fmcarlucci/ADAGE"
  },
  "iccv2019_task-cv_domainadaptationforvehicledetectionfrombirdseyeviewlidarpointclouddata": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Domain Adaptation for Vehicle Detection from Bird's Eye View LiDAR Point Cloud Data",
    "authors": [
      "Khaled Saleh",
      "Ahmed Abobakr",
      "Mohammed Attia",
      "Julie Iskander",
      "Darius Nahavandi",
      "Mohammed Hossny",
      "Saeid Nahvandi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Saleh_Domain_Adaptation_for_Vehicle_Detection_from_Birds_Eye_View_LiDAR_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Saleh_Domain_Adaptation_for_Vehicle_Detection_from_Birds_Eye_View_LiDAR_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Point cloud data from 3D LiDAR sensors are one of the most crucial sensor modalities for versatile safety-critical applications such as self-driving vehicles. Since the annotations of point cloud data is an expensive and time-consuming process, therefore recently the utilisation of simulated environments and 3D LiDAR sensors for this task started to get some popularity. However, the generated synthetic point cloud data are still missing the artefacts usually exist in point cloud data from real 3D LiDAR sensors. Thus, in this work, we are proposing a domain adaptation framework for bridging this gap between synthetic and real point cloud data. Our proposed framework is based on the deep cycle-consistent generative adversarial networks (CycleGAN) architecture. We have evaluated the performance of our proposed framework on the task of vehicle detection from a bird's eye view (BEV) point cloud images coming from real 3D LiDAR sensors. The framework has shown competitive results with an improvement of more than 7% in average precision score over other baseline approaches when tested on real BEV point cloud images.\r",
    "code_link": ""
  },
  "iccv2019_task-cv_towardsefficientinstancesegmentationwithhierarchicaldistillation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Towards Efficient Instance Segmentation with Hierarchical Distillation",
    "authors": [
      "Ziwei Deng",
      "Quan Kong",
      "Tomokazu Murakami"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Deng_Towards_Efficient_Instance_Segmentation_with_Hierarchical_Distillation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Deng_Towards_Efficient_Instance_Segmentation_with_Hierarchical_Distillation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently, instance segmentation models have been developed to a great promising accuracy on public benchmarks. However, these models are too heavy to be applied for real applications due to their low inference speed. In this paper, we propose a faster instance segmentation model utilizing a teacher-student learning framework that transfers the knowledge obtained by a well-trained teacher model to a lightweight student model. In addition to the conventional strategy of knowledge distillation in classification or semantic segmentation networks which are both single-task networks, we investigate a hierarchical distillation (H-Dis) framework for structure information distillation on multi-task learning based instance segmentation. H-Dis consists of two distillation schemes: representation distillation that distills pair-wise quantized feature maps shared by multi-heads, and semantic distillation that makes sure to distill each head information in an instance level. In particular, we present channel-wise distillation for the segmentation head to achieve instance-level mask knowledge transfer. To evaluate our approach, we carry out experiments with different settings of distillation methods on different datasets Pascal VOC and Cityscapes. Our experiments prove that our approach is effective for accelerating instance segmentation models with less accuracy drop under limited computing resources.\r",
    "code_link": ""
  },
  "iccv2019_task-cv_crossdomainimagematchinginpresenceofoutliers": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Cross Domain Image Matching in Presence of Outliers",
    "authors": [
      "Xin Liu",
      "Seyran Khademi",
      "Jan Van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Liu_Cross_Domain_Image_Matching_in_Presence_of_Outliers_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Liu_Cross_Domain_Image_Matching_in_Presence_of_Outliers_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Cross domain image matching between image collections from different source and target domains is challenging in times of deep learning due to i) limited variation of image conditions in a training set, ii) lack of paired-image labels during training, iii) the existing of outliers that makes image matching domains not fully overlap. To this end, we propose an end-to-end architecture that can match cross domain images without labels in the target domain and handle non-overlapping domains by outlier detection. We leverage domain adaptation and triplet constraints for training a network capable of learning domain invariant and identity distinguishable representations, and iteratively detecting the outliers with an entropy loss and our proposed weighted MK-MMD. Extensive experimental evidence on Office [17] dataset and our proposed datasets Shape, Pitts-CycleGAN shows that the proposed approach yields state-of-the-art cross domain image matching and outlier detection performance on different benchmarks. The code will be made publicly available.\r",
    "code_link": ""
  },
  "iccv2019_task-cv_unsuperviseddomainadaptationusingdeepnetworkswithcross-graftedstacks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "TASK-CV",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Transferring and Adapting Source Knowledge in Computer Vision and VisDA Challenge",
    "title": "Unsupervised Domain Adaptation using Deep Networks with Cross-Grafted Stacks",
    "authors": [
      "Jinyong Hou",
      "Xuejie Ding",
      "Jeremiah D. Deng",
      "Stephen Cranefield"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/TASK-CV/Hou_Unsupervised_Domain_Adaptation_using_Deep_Networks_with_Cross-Grafted_Stacks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/TASK-CV/Hou_Unsupervised_Domain_Adaptation_using_Deep_Networks_with_Cross-Grafted_Stacks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Current deep domain adaptation methods used in computer vision have mainly focused on learning discriminative and domain-invariant features across different domains. In this paper, we present a novel approach that bridges the domain gap by projecting the source and target domains into a common association space through an unsupervised \"cross-grafted representation stacking\" (CGRS) mechanism. Specifically, we construct variational auto-encoders (VAE) for the two domains, and form bidirectional associations by cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial networks (GAN) are employed for domain adaptation (DA), mapping the target domain data to the known label space of the source domain. The overall adaptation process hence consists of three phases: feature representation learning by VAEs, association generation, and association alignment by GANs. Experimental results demonstrate that our CGRS-DA approach outperforms the state-of-the-art on a number of unsupervised domain adaptation benchmarks.\r",
    "code_link": ""
  },
  "iccv2019_aim_edgeconnectstructureguidedimageinpaintingusingedgeprediction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "EdgeConnect: Structure Guided Image Inpainting using Edge Prediction",
    "authors": [
      "Kamyar Nazeri",
      "Eric Ng",
      "Tony Joseph",
      "Faisal Qureshi",
      "Mehran Ebrahimi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Nazeri_EdgeConnect_Structure_Guided_Image_Inpainting_using_Edge_Prediction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Nazeri_EdgeConnect_Structure_Guided_Image_Inpainting_using_Edge_Prediction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In recent years, many deep learning techniques have been applied to the image inpainting problem: the task of filling incomplete regions of an image. However, these models struggle to recover and/or preserve image structure especially when significant portions of the image are missing. We propose a two-stage model that separates the inpainting problem into structure prediction and image completion. Similar to sketch art, our model first predicts the image structure of the missing region in the form of edge maps. Predicted edge maps are passed to the second stage to guide the inpainting process. We evaluate our model end-to-end over publicly available datasets CelebA, CelebHQ, Places2, and Paris StreetView on images up to a resolution of 512 x 512. We demonstrate that this approach outperforms current state-of-the-art techniques quantitatively and qualitatively.\r",
    "code_link": ""
  },
  "iccv2019_aim_edge-informedsingleimagesuper-resolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "Edge-Informed Single Image Super-Resolution",
    "authors": [
      "Kamyar Nazeri",
      "Harrish Thasarathan",
      "Mehran Ebrahimi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Nazeri_Edge-Informed_Single_Image_Super-Resolution_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Nazeri_Edge-Informed_Single_Image_Super-Resolution_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The recent increase in the extensive use of digital imaging technologies has brought with it a simultaneous demand for higher-resolution images. We develop a novel \"edge-informed\" approach to single image super-resolution (SISR). The SISR problem is reformulated as an image inpainting task. We use a two-stage inpainting model as a baseline for super-resolution and show its effectiveness for different scale factors (x2, x4, x8) compared to basic interpolation schemes. This model is trained using a joint optimization of image contents (texture and color) and structures (edges). Quantitative and qualitative comparisons are included and the proposed model is compared with current state-of-the-art techniques. We show that our method of decoupling structure and texture reconstruction improves the quality of the final reconstructed high-resolution image.\r",
    "code_link": "https://github.com/knazeri/edge-informed-sisr"
  },
  "iccv2019_aim_smitstochasticmulti-labelimage-to-imagetranslation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "SMIT: Stochastic Multi-Label Image-to-Image Translation",
    "authors": [
      "Andres Romero",
      "Pablo Arbelaez",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Romero_SMIT_Stochastic_Multi-Label_Image-to-Image_Translation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Romero_SMIT_Stochastic_Multi-Label_Image-to-Image_Translation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Cross-domain mapping has been a very active topic in recent years. Given one image, its main purpose is to translate it to the desired target domain, or multiple domains in the case of multiple labels. This problem is highly challenging due to three main reasons: (i) unpaired datasets, (ii) multiple attributes, and (iii) the multimodality (e.g. style) associated with the translation. Most of the existing state-of-the-art has focused only on two reasons i.e., either on (i) and (ii) or (i) and (iii). In this work, we propose a joint framework (i, ii, iii) of diversity and multi-mapping image-to-image translations, using a single generator to conditionally produce countless and unique fake images that hold the underlying characteristics of the source image. Our system does not use style regularization, instead, it uses an embedding representation that we call domain embedding for both domain and style. Extensive experiments over different datasets demonstrate the effectiveness of our proposed approach in comparison with the state-of-the-art in both multi-label and multimodal problems. Additionally, our method is able to generalize under different scenarios: continuous style interpolation, continuous label interpolation, and fine-grained mapping.\r",
    "code_link": ""
  },
  "iccv2019_aim_sterefoefficientimagerefocusingwithstereovision": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "SteReFo: Efficient Image Refocusing with Stereo Vision",
    "authors": [
      "Benjamin Busam",
      "Matthieu Hog",
      "Steven McDonagh",
      "Gregory Slabaugh"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Busam_SteReFo_Efficient_Image_Refocusing_with_Stereo_Vision_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Busam_SteReFo_Efficient_Image_Refocusing_with_Stereo_Vision_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Whether to attract viewer attention to a particular object, give the impression of depth or simply reproduce human-like scene perception, shallow depth of field images are used extensively by professional and amateur photographers alike. To this end, high quality optical systems are used in DSLR cameras to focus on a specific depth plane while producing visually pleasing bokeh. We propose a physically motivated pipeline to mimic this effect from all-in-focus stereo images, typically retrieved by mobile cameras. It is capable to change the focal plane a posteriori at 76 FPS on KITTI images to enable real-time applications. As our portmanteau suggests, SteReFo interrelates stereo-based depth estimation and refocusing efficiently. In contrast to other approaches, our pipeline is simultaneously fully differentiable, physically motivated, and agnostic to scene content. It also enables computational video focus tracking for moving objects in addition to refocusing of static images. We evaluate our approach on the publicly available datasets SceneFlow, KITTI, CityScapes and quantify the quality of architectural changes.\r",
    "code_link": ""
  },
  "iccv2019_aim_3sgan3dshapeembeddedgenerativeadversarialnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "3SGAN: 3D Shape Embedded Generative Adversarial Networks",
    "authors": [
      "Fengdi Che",
      "Xiru Zhu",
      "Tianzi Yang",
      "Tzu-yu Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Che_3SGAN_3D_Shape_Embedded_Generative_Adversarial_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Che_3SGAN_3D_Shape_Embedded_Generative_Adversarial_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Despite recent advances in Generative Adversarial Models(GAN) for image generation, significant gaps remain concerning the generation of boundary and spatial structure. In this paper, we propose a new approach to generate edge and depth information combined with an RGB image to solve this problem. More specifically, we propose two new regularization models. Our first model enforces image-depth-edge alignments by controlling the second-order derivative of depth and the first-order derivative of RGB maps, enforcing smoothness and consistency. The second model leverages multiview synthesis to regularize RGB and depth by computing the difference between an expected rotated object compared to a conditionally generated view of the object; enforcing projection consistency enables the model to directly learn spatial structures and depths. To evaluate our approach, we generated an RGB-D dataset with edge contours from ShapeNet models. Furthermore, we utilized an existing RGB-D dataset, NYU Depth V2 with edges learned by the Holistically-nested Edge Detection model.\r",
    "code_link": ""
  },
  "iccv2019_aim_blindsingleimagereflectionsuppressionforfaceimagesusingdeepgenerativepriors": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "Blind Single Image Reflection Suppression for Face Images using Deep Generative Priors",
    "authors": [
      "Paramanand Chandramouli",
      "Kanchana Vaishnavi Gandikota"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Chandramouli_Blind_Single_Image_Reflection_Suppression_for_Face_Images_using_Deep_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Chandramouli_Blind_Single_Image_Reflection_Suppression_for_Face_Images_using_Deep_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The goal of single image reflection removal is to suppress unwanted merging of radiances from different surfaces in the scene. This is an inherently ill-posed and challenging problem. Conventional approaches use different assumptions and constraints on the background and reflected layers to solve this problem. Recently, deep learning-based approaches have been applied to this task. These methods require extensive amount of realistic data for training. In this paper, we propose to incorporate class-specific prior models for reducing the ill-posedness of the reflection separation task. Specifically, we use a pre-trained deep face-generative model for reflection supression from face images. We design an optimization scheme that effectively leverages the deep generative model and leads to a constrained solution space. Our method does not require training data corresponding to reflection separation task. We evaluate our proposed approach using both synthetic and real world facial images containing reflections and compare with existing state-of-the-art techniques. The results demonstrate advantages of our approach over the current state-of-the-art in single image reflection separation from faces.\r",
    "code_link": ""
  },
  "iccv2019_aim_imagedisentanglementanduncooperativere-entanglementforhigh-fidelityimage-to-imagetranslation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "Image Disentanglement and Uncooperative Re-Entanglement for High-Fidelity Image-to-Image Translation",
    "authors": [
      "Adam Harley",
      "Shih-En Wei",
      "Jason Saragih",
      "Katerina Fragkiadaki"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Harley_Image_Disentanglement_and_Uncooperative_Re-Entanglement_for_High-Fidelity_Image-to-Image_Translation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Harley_Image_Disentanglement_and_Uncooperative_Re-Entanglement_for_High-Fidelity_Image-to-Image_Translation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Cross-domain image-to-image translation should satisfy two requirements: (1) preserve the information that is common to both domains, and (2) generate convincing images covering variations that appear in the target domain. This is challenging, especially when there are no example translations available as supervision. Adversarial cycle consistency was recently proposed as a solution [29], with beautiful and creative results, yielding much follow-up work. However, augmented reality applications cannot readily use such techniques to provide users with compelling translations of real scenes, because the translations do not have high-fidelity constraints. In other words, current models are liable to change details that should be preserved: while re-texturing a face, they may alter the face's expression in an unpredictable way. In this paper, we introduce the problem of high-fidelity image-to-image translation, and present a method for solving it. Our main insight is that low-fidelity translations typically escape a cycle-consistency penalty, because the back-translator learns to compensate for the forward-translator's errors. We therefore introduce an optimization technique that prevents the networks from cooperating: simply train each network only when its input data is real. Prior works, in comparison, train each network with a mix of real and generated data. Experimental results show that our method accurately disentangles the factors that separate the domains, and converges to semantics-preserving translations that prior methods miss.\r",
    "code_link": ""
  },
  "iccv2019_aim_pfagananaesthetics-conditionalganforgeneratingphotographicfineart": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "PFAGAN: An Aesthetics-Conditional GAN for Generating Photographic Fine Art",
    "authors": [
      "Naila Murray"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Murray_PFAGAN_An_Aesthetics-Conditional_GAN_for_Generating_Photographic_Fine_Art_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Murray_PFAGAN_An_Aesthetics-Conditional_GAN_for_Generating_Photographic_Fine_Art_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work we consider the problem of generating aesthetically pleasing photography, sometimes termed photographic fine art (PFA). We cast this problem as a generative modeling task and use a conditional GAN framework. Recent works have shown that conditioning based on semantic information is beneficial for improving photo-realism. In this work we propose a novel GAN architecture which is able to generate photo-realistic images with a specified aesthetic quality by conditioning on both semantic and aesthetic information. To condition the generator, we propose a modified conditional batch normalization layer. To condition the discriminator, we use a joint probabilistic model of semantics and aesthetics to estimate the compatibility between an image (either real or generated) and the conditioning variable. We show quantitatively and qualitatively that our model, called PFAGAN, is able to generate images conditioned on semantic categories and aesthetic scores.\r",
    "code_link": ""
  },
  "iccv2019_aim_cangenerativeadversarialnetworksteachthemselvestextsegmentation?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "Can Generative Adversarial Networks Teach Themselves Text Segmentation?",
    "authors": [
      "Mohammed Al-Rawi",
      "Dena Bazazian",
      "Ernest Valveny"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Al-Rawi_Can_Generative_Adversarial_Networks_Teach_Themselves_Text_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Al-Rawi_Can_Generative_Adversarial_Networks_Teach_Themselves_Text_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In the information age in which we live, text segmentation from scene images is a vital prerequisite task used in many text understanding applications. Text segmentation is a difficult problem because of the potentially vast variation in text and scene landscape. Moreover, systems that learn to perform text segmentation usually need non-trivial annotation efforts. We present in this work a novel unsupervised method to segment text at the pixel-level from scene images. The model we propose, which relies on generative adversarial neural networks, segments text intelligently; and does not therefore need to associate the scene image that contains the text to the ground-truth of the text. The main advantage is thus skipping the need to obtain the pixel-level annotation dataset, which is normally required in training powerful text segmentation models. The results are promising, and to the best of our knowledge, constitute the first step towards reliable unsupervised text segmentation. Our work opens a new research path in unsupervised text segmentation and poses many research questions with a lot of trends available for further improvement.\r",
    "code_link": ""
  },
  "iccv2019_aim_quotientingimpertinentcamerakinematicsfor3dvideostabilization": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "Quotienting Impertinent Camera Kinematics for 3D Video Stabilization",
    "authors": [
      "Thomas W. Mitchel",
      "Christian Wulker",
      "Jin Seob Kim",
      "Sipu Ruan",
      "Gregory S. Chirikjian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Mitchel_Quotienting_Impertinent_Camera_Kinematics_for_3D_Video_Stabilization_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Mitchel_Quotienting_Impertinent_Camera_Kinematics_for_3D_Video_Stabilization_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the recent advent of methods that allow for real-time computation, dense 3D flows have become a viable basis for fast camera motion estimation. Most importantly, dense flows are more robust than the sparse feature matching techniques used by existing 3D stabilization methods, able to better handle large camera displacements and occlusions similar to those often found in consumer videos. Here we introduce a framework for 3D video stabilization that relies on dense scene flow alone. The foundation of this approach is a novel camera motion model that allows for real-world camera poses to be recovered directly from 3D motion fields. Moreover, this model can be extended to describe certain types of non-rigid artifacts that are commonly found in videos, such as those resulting from zooms. This framework gives rise to several robust regimes that produce high-quality stabilization of the kind achieved by prior full 3D methods while avoiding the fragility typically present in feature-based approaches. As an added benefit, our framework is fast: the simplicity of our motion model and efficient flow calculations combine to enable stabilization at a high frame rate.\r",
    "code_link": ""
  },
  "iccv2019_aim_augmentedrealitybasedrecommendationsbasedonperceptualshapestylecompatibilitywithobjectsintheviewpointandcolorcompatibilitywiththebackground": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AIM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Advances in Image Manipulation",
    "title": "Augmented Reality Based Recommendations Based on Perceptual Shape Style Compatibility with Objects in the Viewpoint and Color Compatibility with the Background",
    "authors": [
      "Kumar Tanmay",
      "Kumar Ayush"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AIM/Tanmay_Augmented_Reality_Based_Recommendations_Based_on_Perceptual_Shape_Style_Compatibility_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AIM/Tanmay_Augmented_Reality_Based_Recommendations_Based_on_Perceptual_Shape_Style_Compatibility_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Augmented Reality (AR) has been heralded as the next frontier in retail, but so far, has been mostly used to advertise or market products in a gimmicky way and its true potential in digital marketing remains unexploited. In this work, we leverage richer data coming from AR usage to make re-targeting much more persuasive via viewpoint image augmentation. Based on the user's purchase viewpoint visual, we identify relevant objects/products present in the viewpoint along with their style such that products with more style compatibility with those surrounding real-world objects can be recommended. We also use color compatibility with the background of the user's purchase viewpoint to select suitable product textures. We embed the recommended products in the viewpoint at the location of the initially browsed product with similar pose and scale. This makes the recommendations much more personalized and relevant which can increase conversions. Evaluation with user studies show that our system is able to make recommendations better than tag-based recommendations, and targeting using the viewpoint is better than that of usual product catalogs.\r",
    "code_link": ""
  },
  "iccv2019_openeds_smartoverlaysavisualsaliencydrivenlabelplacementforintelligenthuman-computerinterfaces": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "OpenEDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Eye Tracking for VR and AR",
    "title": "SmartOverlays: A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces",
    "authors": [
      "Srinidhi Hegde",
      "Jitender Maurya",
      "Ramya Hebbalaguppe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/OpenEDS/Hegde_SmartOverlays_A_Visual_Saliency_Driven_Label_Placement_for_Intelligent_Human-Computer_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/OpenEDS/Hegde_SmartOverlays_A_Visual_Saliency_Driven_Label_Placement_for_Intelligent_Human-Computer_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In augmented reality (AR), the computer generated labels assist in understanding a scene by addition of contextual information. However, naive label placement often results in clutter and occlusion impairing the effectiveness of AR visualization. For label placement, the main objectives to be satisfied are, non occlusion to scene of interest, the proximity of labels to the object, and, temporally coherent labels in a video/live feed. We present a novel method for the placement of labels corresponding to objects of interest in a video/live feed that satisfies the aforementioned objectives. Our proposed framework, SmartOverlays, first identifies the objects and generates corresponding labels using a YOLOv2 in a video frame; at the same time, Saliency Attention Model (SAM) learns eye fixation points that aid in predicting saliency maps for label placement; finally, computes Voronoi partitions of the video frame, choosing the centroids of objects as seed points, to place labels for satisfying the proximity constraints with the object of interest. In addition, our approach incorporates tracking the detected objects in a frame to facilitate temporal coherence between frames that enhances readability of labels. We measure the effectiveness of SmartOverlays framework using two objective metrics: (a) Label Occlusion over Saliency (LOS), and, (b) temporal jitter metric to quantify jitter in the label placement.\r",
    "code_link": ""
  },
  "iccv2019_openeds_assessmentofshift-invariantcnngazemappingsforps-ogeyemovementsensors": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "OpenEDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Eye Tracking for VR and AR",
    "title": "Assessment of Shift-Invariant CNN Gaze Mappings for PS-OG Eye Movement Sensors",
    "authors": [
      "Henry Griffith",
      "Dmytro Katrychuk",
      "Oleg Komogortsev"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/OpenEDS/Griffith_Assessment_of_Shift-Invariant_CNN_Gaze_Mappings_for_PS-OG_Eye_Movement_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/OpenEDS/Griffith_Assessment_of_Shift-Invariant_CNN_Gaze_Mappings_for_PS-OG_Eye_Movement_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Photosensor oculography (PS-OG) eye movement sensors offer desirable performance characteristics for integration within wireless head mounted devices (HMDs), including low power consumption and high sampling rates. To address the known performance degradation of these sensors due to HMD shifts, various machine learning techniques have been proposed for mapping sensor outputs to gaze location. This paper advances the understanding of a recently introduced convolutional neural network designed to provide shift invariant gaze mapping within a specified range of sensor translations. Performance is assessed for shift training examples which better reflect the distribution of values that would be generated through manual repositioning of the HMD during a dedicated collection of training data. The network is shown to exhibit comparable accuracy for this realistic shift distribution versus a previously considered rectangular grid, thereby enhancing the feasibility of in-field set-up. In addition, this work further demonstrates the practical viability of the proposed initialization process by demonstrating robust mapping performance versus training data scale. The ability to maintain reasonable accuracy for shifts extending beyond those introduced during training is also demonstrated.\r",
    "code_link": ""
  },
  "iccv2019_openeds_u2eyesabinoculardatasetforeyetrackingandgazeestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "OpenEDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Eye Tracking for VR and AR",
    "title": "U2Eyes: A Binocular Dataset for Eye Tracking and Gaze Estimation",
    "authors": [
      "Sonia Porta",
      "Benoit Bossavit",
      "Rafael Cabeza",
      "Andoni Larumbe-Bergera",
      "Gonzalo Garde",
      "Arantxa Villanueva"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/OpenEDS/Porta_U2Eyes_A_Binocular_Dataset_for_Eye_Tracking_and_Gaze_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/OpenEDS/Porta_U2Eyes_A_Binocular_Dataset_for_Eye_Tracking_and_Gaze_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Theory shows that huge amount of labelled data are needed in order to achieve reliable classification/regression methods when using deep/machine learning techniques. However, in the eye tracking field, manual annotation is not a feasible option due to the wide variability to be covered. Hence, techniques devoted to synthesizing images show up as an opportunity to provide vast amounts of annotated data. Considering that the well-known UnityEyes tool provides a framework to generate single eye images and taking into account that both eyes information can contribute to improve gaze estimation accuracy we present U2Eyes dataset, that is publicly available. It comprehends about 6 million of synthetic images containing binocular data. Furthermore, the physiology of the eye model employed is improved, simplified dynamics of binocular vision are incorporated and more detailed 2D and 3D labelled data are provided. Additionally, an example of application of the dataset is shown as work in progress. Employing U2Eyes as training framework Supervised Descent Method (SDM) is used for eyelids segmentation. The model obtained as result of the training process is then applied on real images from GI4E dataset showing promising results.\r",
    "code_link": ""
  },
  "iccv2019_openeds_eye-mmsminiaturemulti-scalesegmentationnetworkofkeyeye-regionsinembeddedapplications": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "OpenEDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Eye Tracking for VR and AR",
    "title": "Eye-MMS: Miniature Multi-Scale Segmentation Network of Key Eye-Regions in Embedded Applications",
    "authors": [
      "Fadi Boutros",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/OpenEDS/Boutros_Eye-MMS_Miniature_Multi-Scale_Segmentation_Network_of_Key_Eye-Regions_in_Embedded_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/OpenEDS/Boutros_Eye-MMS_Miniature_Multi-Scale_Segmentation_Network_of_Key_Eye-Regions_in_Embedded_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Segmentation of the iris or sclera is an essential processing block in ocular biometric systems. However, human-computer interaction, as in VR/AR applications, requires multiple region segmentation to enable smoother interaction and eye-tracking. Such application does not only demand highly accurate and generalizable segmentation, it requires such segmentation model to be appropriate for the limited computational power of embedded systems. This puts strict limits on the size of the deployed deep learning models. This work presents a miniature multi-scale segmentation network consisting of inter-connected convolutional modules. We present a baseline multi-scale segmentation network and modify it to reduce its parameters by more than 80 times, while reducing its accuracy by less than 3%, resulting in our Eye-MMS model containing only 80k parameters. This work is developed on the OpenEDS database and is conducted in preparation for the OpenEDS Semantic Segmentation Challenge.\r",
    "code_link": ""
  },
  "iccv2019_openeds_minenetadilatedcnnforsemanticsegmentationofeyefeatures": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "OpenEDS",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Eye Tracking for VR and AR",
    "title": "MinENet: A Dilated CNN for Semantic Segmentation of Eye Features",
    "authors": [
      "Jonathan Perry",
      "Amanda Fernandez"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/OpenEDS/Perry_MinENet_A_Dilated_CNN_for_Semantic_Segmentation_of_Eye_Features_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/OpenEDS/Perry_MinENet_A_Dilated_CNN_for_Semantic_Segmentation_of_Eye_Features_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Fast and accurate eye tracking is a critical task for a range of research in virtual and augmented reality, attention tracking, mobile applications, and medical analysis. While deep neural network models excel at image analysis tasks, existing approaches to segmentation often consider only one class, emphasize classification over segmentation, or come with prohibitively high resource costs. In this work, we propose MinENet, a minimized efficient neural network architecture designed for fast multi-class semantic segmentation. We demonstrate performance of MinENet on the OpenEDS Semantic Segmentation Challenge dataset, against a baseline model as well as standard state-of-the-art neural network architectures - a convolutional neural network (CNN) and a dilated CNN. Our encoder-decoder architecture improves accuracy of multi-class segmentation of eye features in this large-scale high-resolution dataset, while also providing a design that is demonstrably lightweight and efficient.\r",
    "code_link": ""
  },
  "iccv2019_dl4vslam_tridepthtriangularpatch-baseddeepdepthprediction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DL4VSLAM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Deep Learning for Visual SLAM",
    "title": "TriDepth: Triangular Patch-Based Deep Depth Prediction",
    "authors": [
      "Masaya Kaneko",
      "Ken Sakurada",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DL4VSLAM/Kaneko_TriDepth_Triangular_Patch-Based_Deep_Depth_Prediction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DL4VSLAM/Kaneko_TriDepth_Triangular_Patch-Based_Deep_Depth_Prediction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a novel and efficient representation for single-view depth estimation using Convolutional Neural Networks (CNNs). Point-cloud is generally used for CNN-based 3D scene reconstruction; however it has some drawbacks: (1) it is redundant as a representation for planar surfaces, and (2) no spatial relationships between points are available (e.g, texture and surface). As a more efficient representation, we introduce a triangular-patch-cloud, which represents the surface of the 3D structure using a set of triangular patches, and propose a CNN framework for its 3D structure estimation. In our framework, we create it by separating all the faces in a 2D mesh, which are determined adaptively from the input image, and estimate depths and normals of all the faces. Using a common RGBD-dataset, we show that our representation has a better or comparable performance than the existing point-cloud-based methods, although it has much less parameters.\r",
    "code_link": ""
  },
  "iccv2019_dl4vslam_spatialperceptionbyobject-awarevisualscenerepresentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DL4VSLAM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Deep Learning for Visual SLAM",
    "title": "Spatial Perception by Object-Aware Visual Scene Representation",
    "authors": [
      "Chung-Yeon Lee",
      "Hyundo Lee",
      "Injune Hwang",
      "Byoung-Tak Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DL4VSLAM/Lee_Spatial_Perception_by_Object-Aware_Visual_Scene_Representation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DL4VSLAM/Lee_Spatial_Perception_by_Object-Aware_Visual_Scene_Representation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Spatial perception is a fundamental ability necessary for autonomous mobile robots to move robustly and safely in the real-world. Recent advances in SLAM enabled a single camera-based system to concurrently build 3D maps of the world while tracking its location and orientation. However, such systems often fail to track themselves within the map and cannot recognize previously visited places due to the lack of reliable descriptions of the observed scenes. We present a spatial perception framework that uses an object-aware visual scene representation to enhance the spatial abilities. The proposed representation compensates for aberrations of conventional geometric scene representations by fusing those representations with semantic features extracted from perceived objects. We implemented this framework on a mobile robot platform to validate its performance in home situations. Further evaluations were conducted with the ScanNet dataset which provides large-scale 3D photo-realistic indoor scenes. Extensive tests show that our framework can reliably generate maps by reducing tracking-failure, and better recognize overlap in the map.\r",
    "code_link": ""
  },
  "iccv2019_dl4vslam_slamantic-leveragingsemanticstoimprovevslamindynamicenvironments": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DL4VSLAM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Deep Learning for Visual SLAM",
    "title": "SLAMANTIC - Leveraging Semantics to Improve VSLAM in Dynamic Environments",
    "authors": [
      "Matthias Schorghuber",
      "Daniel Steininger",
      "Yohann Cabon",
      "Martin Humenberger",
      "Margrit Gelautz"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DL4VSLAM/Schorghuber_SLAMANTIC_-_Leveraging_Semantics_to_Improve_VSLAM_in_Dynamic_Environments_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DL4VSLAM/Schorghuber_SLAMANTIC_-_Leveraging_Semantics_to_Improve_VSLAM_in_Dynamic_Environments_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we tackle the challenge for VSLAM of handling non-static environments. We propose to include semantic information obtained by deep learning methods in the traditional geometric pipeline. Specifically, we compute a confidence measure for each map point as a function of its semantic class (car, person, building, etc.) and its detection consistency over time. The confidence is then applied to guide the usage of each point in the mapping and localization stage. Points with high confidence are used to verify points with low confidence in order to select the final set of points for pose computation and mapping. Furthermore, we can handle map points whose state may change between static and dynamic (a car can be parked or in motion). Evaluating our method on public datasets, we show that it can successfully solve challenging situations in dynamic environments which cause state-of-the-art baseline VSLAM algorithms to fail and that it maintains performance on static scenes. Code is available at github.com/mthz/slamantic\r",
    "code_link": ""
  },
  "iccv2019_dl4vslam_camerarelocalizationbyexploitingmulti-viewconstraintsforscenecoordinatesregression": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DL4VSLAM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Deep Learning for Visual SLAM",
    "title": "Camera Relocalization by Exploiting Multi-View Constraints for Scene Coordinates Regression",
    "authors": [
      "Ming Cai",
      "Huangying Zhan",
      "Chamara Saroj Weerasekera",
      "Kejie Li",
      "Ian Reid"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DL4VSLAM/Cai_Camera_Relocalization_by_Exploiting_Multi-View_Constraints_for_Scene_Coordinates_Regression_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DL4VSLAM/Cai_Camera_Relocalization_by_Exploiting_Multi-View_Constraints_for_Scene_Coordinates_Regression_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a method for learning a scene coordinate regression model to perform accurate camera relocalization in a known environment from a single RGB image. Our method incorporates self-supervision for scene coordinates via multi-view geometric constraints to improve training. More specifically, we use an image-based warp error between different views of a scene point to improve the ability of the network to regress to the correct absolute scene coordinates of the point. For the warp error we explore both RGB values, and deep learned features, as the basis for the error. We provide a thorough analysis of the effect of each component in our framework and evaluate our method on both indoor and outdoor datasets. We show that compared to the coordinate regression model trained with single-view information, this multi-view constraint benefits the learning process and the final performance. It not only helps the networks converge faster compared to the model trained with single-view reprojection loss, but also improves the accuracy of the absolute pose estimation using a single RGB image compared to the prior art.\r",
    "code_link": ""
  },
  "iccv2019_dl4vslam_adversarialnetworksforcameraposeregressionandrefinement": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DL4VSLAM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Deep Learning for Visual SLAM",
    "title": "Adversarial Networks for Camera Pose Regression and Refinement",
    "authors": [
      "Mai Bui",
      "Christoph Baur",
      "Nassir Navab",
      "Slobodan Ilic",
      "Shadi Albarqouni"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DL4VSLAM/Bui_Adversarial_Networks_for_Camera_Pose_Regression_and_Refinement_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DL4VSLAM/Bui_Adversarial_Networks_for_Camera_Pose_Regression_and_Refinement_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Despite recent advances on the topic of direct camera pose regression using neural networks, accurately estimating the camera pose of a single RGB image still remains a challenging task. To address this problem, we introduce a novel framework based, in its core, on the idea of implicitly learning the joint distribution of RGB images and their corresponding camera poses using a discriminator network and adversarial learning. Our method allows not only to regress the camera pose from a single image, however, also offers a solely RGB-based solution for camera pose refinement using the discriminator network. Further, we show that our method can effectively be used to optimize the predicted camera poses and thus improve the localization accuracy. To this end, we validate our proposed method on the publicly available 7-Scenes dataset improving upon the results of direct camera pose regression methods.\r",
    "code_link": ""
  },
  "iccv2019_dl4vslam_howtoimprovecnn-based6-dofcameraposeestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DL4VSLAM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Deep Learning for Visual SLAM",
    "title": "How to Improve CNN-Based 6-DoF Camera Pose Estimation",
    "authors": [
      "Soroush Seifi",
      "Tinne Tuytelaars"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DL4VSLAM/Seifi_How_to_Improve_CNN-Based_6-DoF_Camera_Pose_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DL4VSLAM/Seifi_How_to_Improve_CNN-Based_6-DoF_Camera_Pose_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Convolutional neural networks (CNNs) and transfer learning have recently been used for 6 degrees of freedom (6-DoF) camera pose estimation. While they do not reach the same accuracy as visual SLAM-based approaches and are restricted to a specific environment, they excel in robustness and can be applied even to a single image. In this paper, we study PoseNet [1] and investigate modifications based on datasets' characteristics to improve the accuracy of the pose estimates. In particular, we emphasize the importance of field-of-view over image resolution; we present a data augmentation scheme to reduce overfitting; we study the effect of Long-Short-Term-Memory (LSTM) cells. Lastly, we combine these modifications and improve PoseNet's performance for monocular CNN based camera pose regression.\r",
    "code_link": ""
  },
  "iccv2019_dl4vslam_asystemframeworkforlocalizationandmappingusinghighresolutioncamerasofmobiledevices": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "DL4VSLAM",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Deep Learning for Visual SLAM",
    "title": "A System Framework for Localization and Mapping using High Resolution Cameras of Mobile Devices",
    "authors": [
      "Lifeng Liu",
      "Jian Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/DL4VSLAM/Liu_A_System_Framework_for_Localization_and_Mapping_using_High_Resolution_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/DL4VSLAM/Liu_A_System_Framework_for_Localization_and_Mapping_using_High_Resolution_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a hierarchical framework for processing high-resolution images on mobile devices for visual SLAM. It is based on the insights from analysis of new progress in primary features' detection, object detection and pose estimation. A rectification/unwarping operation is applied in regions of interest (ROIs) to improve the object/parts classification/detection performance; the object-part spatial relationships are created and contribute in map building, object detection, and localization; and a geometric constraints based pose refinement is followed to further improve the localization accuracy. Our design facilitates the more accurate pose estimating and localization using mobile devices for SLAM, and Augmented Reality/Mixed Reality applications.\r",
    "code_link": ""
  },
  "iccv2019_lci_removingimagingartifactsinelectronmicroscopyusinganasymmetricallycyclicadversarialnetworkwithoutpairedtrainingdata": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Removing Imaging Artifacts in Electron Microscopy using an Asymmetrically Cyclic Adversarial Network without Paired Training Data",
    "authors": [
      "Tran Minh Quan",
      "David Grant Colburn Hildebrand",
      "Kanggeun Lee",
      "Logan A. Thomas",
      "Aaron T. Kuan",
      "Wei-Chung Allen Lee",
      "Won-Ki Jeong"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Quan_Removing_Imaging_Artifacts_in_Electron_Microscopy_using_an_Asymmetrically_Cyclic_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Quan_Removing_Imaging_Artifacts_in_Electron_Microscopy_using_an_Asymmetrically_Cyclic_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose an asymmetrically cyclic adversarial network that performs denoising tasks to improve electron microscopy (EM) image analysis. Deep learning-based denoising methods have typically been trained either with matching pairs of noise-free and noise-corrupted images or by leveraging prior knowledge of noise distributions. Neither of these options is feasible in high-throughput EM imaging pipelines. Our proposed denoising method employs independently acquired noise-free, noise pattern, and noise-corrupted images to automatically learn the underlying noise model and generate denoised outputs. This method is based on three-way cyclic constraints with adversarial training of a deep network to improve the quality of acquired images without paired training data. Its utility is demonstrated for cases where imaging substrates add noise and where acquisition conditions contribute noise. We show that our method, which builds on the concept of CycleGAN, outperforms the current state-of-the-art denoising approaches Noise2Noise and Noise2Void, as well as other learning-based techniques.\r",
    "code_link": ""
  },
  "iccv2019_lci_lightweightandaccuraterecursivefractalnetworkforimagesuper-resolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Lightweight and Accurate Recursive Fractal Network for Image Super-Resolution",
    "authors": [
      "Juncheng Li",
      "Yiting Yuan",
      "Kangfu Mei",
      "Faming Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Li_Lightweight_and_Accurate_Recursive_Fractal_Network_for_Image_Super-Resolution_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Li_Lightweight_and_Accurate_Recursive_Fractal_Network_for_Image_Super-Resolution_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Convolutional neural networks have recently achieved great success in image super-resolution (SR). However, we notice an interesting phenomenon that these SR models are getting bigger, deeper, and more complex. Extensive models promote the development of SR, but the effectiveness, reproducibility and practical application prospects of these new models need further verification. In this paper, we propose a lightweight and accurate SR framework, named Super-Resolution Recursive Fractal Network (SRRFN). SRRFN introduces a flexible and diverse fractal module, which enables it to construct infinitely possible topological sub-structure through a simple component. We also introduce the recursive learning mechanism to maximize the use of model parameters. Extensive experiments show that our SRRFN achieves favorable performance against state-of-the-art methods with fewer parameters and less execution time. All code is available at https://github.com/MIVRC/SRRFN-PyTorch.\r",
    "code_link": ""
  },
  "iccv2019_lci_deepvideodeblurringthedevilisinthedetails": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Deep Video Deblurring: The Devil is in the Details",
    "authors": [
      "Jochen Gast",
      "Stefan Roth"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Gast_Deep_Video_Deblurring_The_Devil_is_in_the_Details_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Gast_Deep_Video_Deblurring_The_Devil_is_in_the_Details_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Video deblurring for hand-held cameras is a challenging task, since the underlying blur is caused by both camera shake and object motion. State-of-the-art deep networks exploit temporal information from neighboring frames, either by means of spatio-temporal transformers or by recurrent architectures. In contrast to these involved models, we found that a simple baseline CNN can perform astonishingly well when particular care is taken w.r.t. the details of model and training procedure. To that end, we conduct a comprehensive study regarding these crucial details, uncovering extreme differences in quantitative and qualitative performance. Exploiting these details allows us to boost the architecture and training procedure of a simple baseline CNN by a staggering 3.15dB, such that it becomes highly competitive w.r.t. cutting-edge networks. This raises the question whether the reported accuracy difference between models is always due to technical contributions or also subject to such orthogonal, but crucial details.\r",
    "code_link": ""
  },
  "iccv2019_lci_adaptiveptychleveragingimageadaptivegenerativepriorsforsubsampledfourierptychography": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Adaptive Ptych: Leveraging Image Adaptive Generative Priors for Subsampled Fourier Ptychography",
    "authors": [
      "Fahad Shamshad",
      "Asif Hanif",
      "Farwa Abbas",
      "Muhammad Awais",
      "Ali Ahmed"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Shamshad_Adaptive_Ptych_Leveraging_Image_Adaptive_Generative_Priors_for_Subsampled_Fourier_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Shamshad_Adaptive_Ptych_Leveraging_Image_Adaptive_Generative_Priors_for_Subsampled_Fourier_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently pretrained generative models have shown promising results for subsampled Fourier Ptychography (FP) in terms of quality of reconstruction for extremely low sampling rates. However, the representation capabilities of these pretrained generators do not capture the full distribution for complex classes of images, such as human faces or numbers, resulting in representation error. Moreover, recent studies have shown that these pretrained generative priors struggle at high-resolution in imaging inverse problems for reconstructing a faithful estimate of the true image, potentially due to mode collapse issue. To mitigate the issue of representation error of pretrained generative models for subsampled FP, we propose to make pretrained generator image adaptive by modifying it to better represent a single image (at test time) that is consistent with the subsampled FP measurements. Our experimental results demonstrate the superiority of the proposed approach over recent subsampled FP methods in terms of both quantitative metrics and visual quality.\r",
    "code_link": ""
  },
  "iccv2019_lci_deephyperspectralpriorsingle-imagedenoising,inpainting,super-resolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Deep Hyperspectral Prior: Single-Image Denoising, Inpainting, Super-Resolution",
    "authors": [
      "Oleksii Sidorov",
      "Jon Yngve Hardeberg"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Sidorov_Deep_Hyperspectral_Prior_Single-Image_Denoising_Inpainting_Super-Resolution_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Sidorov_Deep_Hyperspectral_Prior_Single-Image_Denoising_Inpainting_Super-Resolution_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep learning algorithms have demonstrated state-of-the-art performance in various tasks of image restoration. This was made possible through the ability of CNNs to learn from large exemplar sets. However, the latter becomes an issue for hyperspectral image processing where datasets commonly consist of just a few images. In this work, we propose a new approach to denoising, inpainting, and super-resolution of hyperspectral image data using intrinsic properties of a CNN without any training. The performance of the given algorithm is shown to be comparable to the performance of trained networks, while its application is not restricted by the availability of training data. This work is an extension of original \"deep prior\" algorithm to hyperspectral imaging domain and 3D-convolutional networks.\r",
    "code_link": "https://github.com/acecreamu/deep-hs-prior"
  },
  "iccv2019_lci_flickr1024alarge-scaledatasetforstereoimagesuper-resolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution",
    "authors": [
      "Yingqian Wang",
      "Longguang Wang",
      "Jungang Yang",
      "Wei An",
      "Yulan Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Wang_Flickr1024_A_Large-Scale_Dataset_for_Stereo_Image_Super-Resolution_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Wang_Flickr1024_A_Large-Scale_Dataset_for_Stereo_Image_Super-Resolution_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " With the popularity of dual cameras in recently released smart phones, a growing number of super-resolution (SR) methods have been proposed to enhance the resolution of stereo image pairs. However, the lack of high-quality stereo datasets has limited the research in this area. To facilitate the training and evaluation of novel stereo SR algorithms, in this paper, we present a large-scale stereo dataset named Flickr1024, which contains 1024 pairs of high-quality images and covers diverse scenarios. We first introduce the data acquisition and processing pipeline, and then compare several popular stereo datasets. Finally, we conduct cross-dataset experiments to investigate the potential benefits introduced by our dataset. Experimental results show that, as compared to the KITTI and Middlebury datasets, our Flickr1024 dataset can help to handle the over-fitting problem and significantly improves the performance of stereo SR methods. The Flickr1024 dataset is available online at: https://yingqianwang.github.io/Flickr1024.\r",
    "code_link": ""
  },
  "iccv2019_lci_semi-supervisedeyemakeuptransferbyswappinglearnedrepresentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Semi-Supervised Eye Makeup Transfer by Swapping Learned Representation",
    "authors": [
      "Feida Zhu",
      "Hongji Cao",
      "Zunlei Feng",
      "Yongqiang Zhang",
      "Wenbin Luo",
      "Hucheng Zhou",
      "Mingli Song",
      "Kai-Kuang Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Zhu_Semi-Supervised_Eye_Makeup_Transfer_by_Swapping_Learned_Representation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Zhu_Semi-Supervised_Eye_Makeup_Transfer_by_Swapping_Learned_Representation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper introduces an autoencoder structure to transfer the eye makeup from an arbitrary reference image to a source image realistically and faithfully using both synthetic paired data and unpaired data in a semi-supervised way. Different from the image domain transfer problem, our framework only needs one domain entity and follows an \"encoding-swap-decoding\" process. Makeup transfer is achieved by decoding the base representation from a source image and makeup representation from a reference image. Moreover, our method allows users to control the makeup degree by tuning makeup weight. To the best of our knowledge, there is no public large makeup dataset to evaluate data-driven approaches. We have collected a dataset of non-makeup images and with-makeup images of various eye makeup styles. Experiments demonstrate the effectiveness of our method with the state-of-the-art methods both qualitatively and quantitatively.\r",
    "code_link": ""
  },
  "iccv2019_lci_deepcameraafullyconvolutionalneuralnetworkforimagesignalprocessing": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Deep Camera: A Fully Convolutional Neural Network for Image Signal Processing",
    "authors": [
      "Sivalogeswaran Ratnasingam"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Ratnasingam_Deep_Camera_A_Fully_Convolutional_Neural_Network_for_Image_Signal_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Ratnasingam_Deep_Camera_A_Fully_Convolutional_Neural_Network_for_Image_Signal_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A conventional camera performs various signal processing steps sequentially to reconstruct an image from a raw Bayer image. When performing these processing in multiple stages the residual error from each stage accumulates in the image and degrades the quality of the final reconstructed image. In this paper, we present a fully convolutional neural network (CNN) to perform defect pixel correction, denoising, white balancing, exposure correction, demosaicing, color transform, and gamma encoding. To our knowledge, this is the first CNN trained end-to-end to perform the entire image signal processing pipeline in a camera. Through extensive experiments, we show that the proposed CNN based image signal processing system performs better than the conventional signal processing pipelines that perform the processing sequentially.\r",
    "code_link": ""
  },
  "iccv2019_lci_imagesuper-resolutionviaresidualblockattentionnetworks": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Image Super-Resolution via Residual Block Attention Networks",
    "authors": [
      "Tao Dai",
      "Hua Zha",
      "Yong Jiang",
      "Shu-Tao Xia"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Dai_Image_Super-Resolution_via_Residual_Block_Attention_Networks_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Dai_Image_Super-Resolution_via_Residual_Block_Attention_Networks_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently, deep convolutional neural networks (CNNs) have been widely used in image super-resolution (SR). Most state-of-the-art CNN-based SR methods focus on improving the performance by designing deeper and wider networks. However, 1) using deeper networks makes the network difficult to train; 2) the relationships of features have not been thoroughly explored, therefore hindering the representational power of CNNs. In this paper, we investigate an effective end-to-end neural structure for more powerful feature expression and feature correlation learning. Specifically, we propose a residual block attention networks (RBAN) framework, which consists of two types of attention modules to efficiently exploit the feature correlations in spatial and channel dimensions for stronger feature expression. The proposed RBAN framework is constituted of a series of residual attention groups, which is further composed of several repeated residual block attention block to not only fully exploit the hierarchical features from different convolutional layers but also efficiently capture the contextual information and interdependencies among channels. Experimental results demonstrate the superiority of our RBAN network over state-of-the-art SR methods in terms of both quantitive and visual quality.\r",
    "code_link": ""
  },
  "iccv2019_lci_onlineregularizationbydenoisingwithapplicationstophaseretrieval": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Online Regularization by Denoising with Applications to Phase Retrieval",
    "authors": [
      "Zihui Wu",
      "Yu Sun",
      "Jiaming Liu",
      "Ulugbek Kamilov"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Wu_Online_Regularization_by_Denoising_with_Applications_to_Phase_Retrieval_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Wu_Online_Regularization_by_Denoising_with_Applications_to_Phase_Retrieval_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Regularization by denoising (RED) is a powerful framework for solving imaging inverse problems. Most RED algorithms are iterative batch procedures, which limits their applicability to very large datasets. In this paper, we address this limitation by introducing a novel online RED (On-RED) algorithm, which processes a small subset of the data at a time. We establish the theoretical convergence of On-RED in convex settings and empirically discuss its effectiveness in non-convex ones by illustrating its applicability to phase retrieval. Our results suggest that On-RED is an effective alternative to the traditional RED algorithms when dealing with large datasets.\r",
    "code_link": ""
  },
  "iccv2019_lci_ridnetrecursiveinformationdistillationnetworkforcolorimagedenoising": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "RIDNet: Recursive Information Distillation Network for Color Image Denoising",
    "authors": [
      "Shengkai Zhuo",
      "Zhi Jin",
      "Wenbin Zou",
      "Xia Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Zhuo_RIDNet_Recursive_Information_Distillation_Network_for_Color_Image_Denoising_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Zhuo_RIDNet_Recursive_Information_Distillation_Network_for_Color_Image_Denoising_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Color image denoising is more challenging in effectiveness when compared with the grayscale one. Most existing methods play a certain role in efficiency or flexibility, but lack robustness to handle various noise levels, especially the severe noise. This keeps them away from being practically applied to color image denoising. To address this issue, we propose a robust CNN based denoiser, namely Recursive Information Distillation Network (RIDNet), to handle the denoising task at high noise levels. The proposed RIDNet simultaneously keeps the efficiency and flexibility by introducing the information distillation module and merging a tunable noise level map as the input, respectively. Experiment results on Additive White Gaussian Noise (AWGN) images demonstrate that our method outperforms most of the state-of-the-art color image denoisers.\r",
    "code_link": ""
  },
  "iccv2019_lci_ahvs-inspiredattentiontoimprovelossmetricsforcnn-basedperception-orientedsuper-resolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "A HVS-Inspired Attention to Improve Loss Metrics for CNN-Based Perception-Oriented Super-Resolution",
    "authors": [
      "Taimoor Tariq",
      "Juan Luis Gonzalez Bello",
      "Munchurl Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Tariq_A_HVS-Inspired_Attention_to_Improve_Loss_Metrics_for_CNN-Based_Perception-Oriented_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Tariq_A_HVS-Inspired_Attention_to_Improve_Loss_Metrics_for_CNN-Based_Perception-Oriented_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep Convolutional Neural Network (CNN) features have been demonstrated to be effective perceptual quality features. The perceptual loss, based on feature maps of pre-trained CNN's has proven to be remarkably effective for CNN based perceptual image restoration problems. In this work, taking inspiration from the the Human Visual System (HVS) and visual perception, we propose a spatial attention mechanism based on the dependency human contrast sensitivity on spatial frequency. We identify regions in input images, based on the underlying spatial frequency, which are not generally well reconstructed during Super-Resolution but are most important in terms of visual sensitivity. Based on this prior, we design a spatial attention map that is applied to feature maps in the perceptual loss and its variants, helping them to identify regions that are of more perceptual importance. The results demonstrate the our technique improves the ability of the perceptual loss and contextual loss to deliver more natural images in CNN based super-resolution.\r",
    "code_link": ""
  },
  "iccv2019_lci_deepreddeepimagepriorpoweredbyred": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "DeepRED: Deep Image Prior Powered by RED",
    "authors": [
      "Gary Mataev",
      "Peyman Milanfar",
      "Michael Elad"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Mataev_DeepRED_Deep_Image_Prior_Powered_by_RED_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Mataev_DeepRED_Deep_Image_Prior_Powered_by_RED_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Inverse problems in imaging are extensively studied, with a variety of strategies, tools, and theory that have been accumulated over the years. Recently, this field has been immensely influenced by the emergence of deep-learning techniques. One such contribution, which is the focus of this paper, is the Deep Image Prior (DIP) work by Ulyanov, Vedaldi, and Lempitsky (2018). DIP offers a new approach towards the regularization of inverse problems, obtained by forcing the recovered image to be synthesized from a given deep architecture. While DIP has been shown to be quite an effective unsupervised approach, its results still fall short when compared to state-of-the-art alternatives. In this work, we aim to boost DIP by adding an explicit prior, which enriches the overall regularization effect in order to lead to better-recovered images. More specifically, we propose to bring-in the concept of Regularization by Denoising (RED), which leverages existing denoisers for regularizing inverse problems. Our work shows how the two (DIP and RED) can be merged into a highly effective unsupervised recovery process while avoiding the need to differentiate the chosen denoiser, and leading to very effective results, demonstrated for several tested problems.\r",
    "code_link": ""
  },
  "iccv2019_lci_cnn-basedcross-datasetno-referenceimagequalityassessment": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "CNN-Based Cross-Dataset No-Reference Image Quality Assessment",
    "authors": [
      "Dan Yang",
      "Veli-Tapani Peltoketo",
      "Joni-Kristian Kamarainen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Yang_CNN-Based_Cross-Dataset_No-Reference_Image_Quality_Assessment_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Yang_CNN-Based_Cross-Dataset_No-Reference_Image_Quality_Assessment_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent works on no-reference image quality assessment (NR-IQA) have reported good performance for various datasets. However, they suffer from significant performance drops in cross-dataset evaluations which indicates poor generalization power. We propose a Siamese architecture and training procedures for cross-dataset deep NR-IQA that achieves clearly better performance. Moreover, we show that the architecture can be further boosted by i) pre-training with a large aesthetics dataset and ii) adding low-level quality cues, sharpness, tone and colourfulness, as additional features.\r",
    "code_link": ""
  },
  "iccv2019_lci_integratingdataandimagedomaindeeplearningforlimitedangletomographyusingconsensusequilibrium": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Integrating Data and Image Domain Deep Learning for Limited Angle Tomography using Consensus Equilibrium",
    "authors": [
      "Muhammad Usman Ghani",
      "W. Clem Karl"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Ghani_Integrating_Data_and_Image_Domain_Deep_Learning_for_Limited_Angle_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Ghani_Integrating_Data_and_Image_Domain_Deep_Learning_for_Limited_Angle_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Computed Tomography (CT) is a non-invasive imaging modality with applications ranging from healthcare to security. It reconstructs cross-sectional images of an object using a collection of projection data collected at different angles. Conventional methods, such as FBP, require that the projection data be uniformly acquired over the complete angular range. In some applications, it is not possible to acquire such data. Security is one such domain where non-rotational scanning configurations are being developed which violate the complete data assumption. Conventional methods produce images from such data that are filled with artifacts. The recent success of deep learning (DL) methods has inspired researchers to post-process these artifact laden images using deep neural networks (DNNs). This approach has seen limited success on real CT problems. Another approach has been to pre-process the incomplete data using DNNs aiming to avoid the creation of artifacts altogether. Due to imperfections in the learning process, this approach can still leave perceptible residual artifacts. In this work, we aim to combine the power of deep learning in both the data and image domains through a two-step process based on the consensus equilibrium (CE) framework. Specifically, we use conditional generative adversarial networks (cGANs) in both the data and the image domain for enhanced performance and efficient computation and combine them through a consensus process. We demonstrate the effectiveness of our approach on a real security CT dataset for a challenging 90 degree limited-angle problem. The same framework can be applied to other limited data problems arising in applications such as electron microscopy, non-destructive evaluation, and medical imaging.\r",
    "code_link": ""
  },
  "iccv2019_lci_blindunitarytransformlearningforinverseproblemsinlight-fieldimaging": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Blind Unitary Transform Learning for Inverse Problems in Light-Field Imaging",
    "authors": [
      "Cameron J. Blocker",
      "Jeffrey A. Fessler"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Blocker_Blind_Unitary_Transform_Learning_for_Inverse_Problems_in_Light-Field_Imaging_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Blocker_Blind_Unitary_Transform_Learning_for_Inverse_Problems_in_Light-Field_Imaging_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Light-field cameras have enabled a new class of digital post-processing techniques. Unfortunately, the sampling requirements needed to capture a 4D color light-field directly using a microlens array requires sacrificing spatial resolution and SNR in return for greater angular resolution. Because recovering the true light-field from focal-stack data is an ill-posed inverse problem, we propose using blind unitary transform learning (UTL) as a regularizer. UTL attempts to learn a set of filters that maximize the sparsity of the encoded representation. This paper investigates which dimensions of a light-field are most sparsifiable by UTL and lead to the best reconstruction performance. We apply the UTL regularizer to light-field inpainting and focal stack reconstruction problems and find it improves performance over traditional hand-crafted regularizers.\r",
    "code_link": ""
  },
  "iccv2019_lci_asimpleandrobustdeepconvolutionalapproachtoblindimagedenoising": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "A Simple and Robust Deep Convolutional Approach to Blind Image Denoising",
    "authors": [
      "Hengyuan Zhao",
      "Wenze Shao",
      "Bingkun Bao",
      "Haibo Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Zhao_A_Simple_and_Robust_Deep_Convolutional_Approach_to_Blind_Image_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Zhao_A_Simple_and_Robust_Deep_Convolutional_Approach_to_Blind_Image_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Image denoising, particularly Gaussian denoising, has achieved continuous success in the past decades. Although deep convolutional neural networks (CNNs) are also shown leading high-performance in Gaussian denoising just as in many other computer vision tasks, they are not competitive at all on real noisy photographs to representative classical methods such as BM3D and WNNM. In this paper, a simple yet robust method is proposed to improve the effectiveness and practicability of deep denoising models. In view of the difference between real-world noise in camera systems and additive white Gaussian noise (AWGN), the model learning has exploited clean-noisy image pairs newly produced built on a generalized signal dependent noise model. During the model inference, the proposed denoising model is not only blind to the noise type but also to the noise level. Meanwhile, in order to separate the noise from image content as full as possible, a new convolutional architecture is advocated for such a blind denoising task where a kind of lifting residual modules is specifically proposed for discriminative feature extraction. Experimental results on both simulated and real noisy images demonstrate that the proposed blind denoiser achieves fairly competitive or even better performance than state-of-the-art algorithms in terms of both quantitative and qualitative assessment. The codes of the proposed method are available at https://github.com/zhaohengyuan1/SDNet.\r",
    "code_link": "https://github.com/zhaohengyuan1/SDNet"
  },
  "iccv2019_lci_deepplug-and-playpriorforparallelmrireconstruction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Deep Plug-and-Play Prior for Parallel MRI Reconstruction",
    "authors": [
      "Ali Pour Yazdanpanah",
      "Onur Afacan",
      "Simon Warfield"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Yazdanpanah_Deep_Plug-and-Play_Prior_for_Parallel_MRI_Reconstruction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Yazdanpanah_Deep_Plug-and-Play_Prior_for_Parallel_MRI_Reconstruction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand and scan time directly depends on the number of acquired k-space samples. Conventional MRI reconstruction methods for fast MRI acquisition mostly relied on different regularizers which represent analytical models of sparsity. However, recent data-driven methods based on deep learning has resulted in promising improvements in image reconstruction algorithms. In this paper, we propose a deep plug-and-play prior framework for parallel MRI reconstruction problems which utilize a deep neural network (DNN) as an advanced denoiser within an iterative method. This, in turn, enables rapid acquisition of MR images with improved image quality. The proposed method was compared with the reconstructions using the clinical gold standard GRAPPA method. Our results with undersampled data demonstrate that our method can deliver considerably higher quality images at high acceleration factors in comparison to clinical gold standard method for MRI reconstructions. Our proposed reconstruction enables an increase in acceleration factor, and a reduction in acquisition time while maintaining high image quality.\r",
    "code_link": ""
  },
  "iccv2019_lci_superlearningasupervised-unsupervisedframeworkforlow-dosectimagereconstruction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "SUPER Learning: A Supervised-Unsupervised Framework for Low-Dose CT Image Reconstruction",
    "authors": [
      "Zhipeng Li",
      "Siqi Ye",
      "Yong Long",
      "Saiprasad Ravishankar"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Li_SUPER_Learning_A_Supervised-Unsupervised_Framework_for_Low-Dose_CT_Image_Reconstruction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Li_SUPER_Learning_A_Supervised-Unsupervised_Framework_for_Low-Dose_CT_Image_Reconstruction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent years have witnessed growing interest in machine learning-based models and techniques for low-dose X-ray CT (LDCT) imaging tasks. The methods can typically be categorized into supervised learning methods and unsupervised or model-based learning methods. Supervised learning methods have recently shown success in image restoration tasks. However, they often rely on large training sets. Model-based learning methods such as dictionary or transform learning do not require large or paired training sets and often have good generalization properties, since they learn general properties of CT image sets. Recent works have shown the promising reconstruction performance of methods such as PWLS-ULTRA that rely on clustering the underlying (reconstructed) image patches into a learned union of transforms. In this paper, we propose a new Supervised-UnsuPERvised (SUPER) reconstruction framework for LDCT image reconstruction that combines the benefits of supervised learning methods and (unsupervised) transform learning-based methods such as PWLS-ULTRA that involve highly image-adaptive clustering. The SUPER model consists of several layers, each of which includes a deep network learned in a supervised manner and an unsupervised iterative method that involves image-adaptive components. The SUPER reconstruction algorithms are learned in a greedy manner from training data. The proposed SUPER learning methods dramatically outperform both the constituent supervised learning-based networks and iterative algorithms for LDCT, and use much fewer iterations in the iterative reconstruction modules.\r",
    "code_link": ""
  },
  "iccv2019_lci_deeplearning-basedimagingusingsingle-lensandmulti-aperturediffractiveopticalsystems": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Deep Learning-Based Imaging using Single-Lens and Multi-Aperture Diffractive Optical Systems",
    "authors": [
      "Artem Nikonorov",
      "Viktoria Evdokimova",
      "Maksim Petrov",
      "Pavel Yakimov",
      "Sergey Bibikov",
      "Yuriy Yuzifovich",
      "Roman Skidanov",
      "Nikolay Kazanskiy"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Nikonorov_Deep_Learning-Based_Imaging_using_Single-Lens_and_Multi-Aperture_Diffractive_Optical_Systems_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Nikonorov_Deep_Learning-Based_Imaging_using_Single-Lens_and_Multi-Aperture_Diffractive_Optical_Systems_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The pressure to reduce weight and improve image quality of the imaging devices continues to push research in the area of flat optics with computational image reconstruction. This paper presents a new end-to-end framework applying two convolutional neural networks (CNNs) to reconstruct images captured with multilevel diffractive lenses (MDLs). We show that the patch-wise chromatic blur and image-wise context-aware color highlights, the distortions inherent to MDLs, can be successfully addressed with the suggested reconstruction pipeline. The generative adversarial network (GAN) is first used to remove image-wise color distortion, while a patch-wise network is then used to apply chromatic deblur. The proposed approach produces better image quality improvement than the context-independent color correction with a deconvolution-based chromatic deblur. We also show that the proposed end-to-end reconstruction is equally applicable for single-and multi-aperture MDL-based imaging systems.\r",
    "code_link": ""
  },
  "iccv2019_lci_deepcompressivesensingforvisualprivacyprotectioninflatcamimaging": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "LCI",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Learning for Computational Imaging",
    "title": "Deep Compressive Sensing for Visual Privacy Protection in FlatCam Imaging",
    "authors": [
      "Thuong Nguyen Canh",
      "Hajime Nagahara"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/LCI/Canh_Deep_Compressive_Sensing_for_Visual_Privacy_Protection_in_FlatCam_Imaging_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/LCI/Canh_Deep_Compressive_Sensing_for_Visual_Privacy_Protection_in_FlatCam_Imaging_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Detection followed by projection in conventional privacy cameras is vulnerable to software attacks that threaten to expose image sensor data. By multiplexing the incoming light with a coded mask, a FlatCam camera removes the spatial correlation and captures visually protected images. However, FlatCam imaging suffers from poor reconstruction quality and pays no attention to the privacy of visual information. In this paper, we propose a deep learning-based compressive sensing approach to reconstruct and protect sensitive regions from secured FlatCam measurements. We predict sensitive regions via facial segmentation and separate them from the captured measurements. Our deep compressive sensing network was trained with simulated data, and was tested on both simulated and real FlatCam data.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_adecoder-freeapproachforunsupervisedclusteringandmanifoldlearningwithrandomtripletmining": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "A Decoder-Free Approach for Unsupervised Clustering and Manifold Learning with Random Triplet Mining",
    "authors": [
      "Oliver Nina",
      "Jamison Moody",
      "Clarissa Milligan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Nina_A_Decoder-Free_Approach_for_Unsupervised_Clustering_and_Manifold_Learning_with_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Nina_A_Decoder-Free_Approach_for_Unsupervised_Clustering_and_Manifold_Learning_with_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Unsupervised clustering is a very relevant open area of research in machine learning with many applications in the real world. Learning the manifold in which images lie and measuring the proximity distance of the sample points to the clusters in their latent space is non-trivial. Recent deep learning methods have proposed the use of autoencoders for manifold learning and dimensionality reduction in an effort to better cluster image samples. However, offline training of autoencoders is cumbersome and rather tedious to update. Moreover, trained autoencoders tend to be biased towards the training set and are impractical for performing data augmentation. In this paper, we introduce a novel method that uses a triplet network architecture in order to replace autoencoders, thus avoiding the need to pre-train autoencoders offline. Because our framework can be trained online, we can train our network with data augmented pairs which allows us to build a more robust encoder and improve accuracy. In contrast to other clustering methods that require nearest neighbor comparisons at every step, our method introduces a novel and adaptive approach of choosing the samples to train which we call Random Triplet Mining. Our method remains competitive compared with other current methods while we obtain state of the art results on the Fashion-MNIST dataset.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_multi-viewpointnetfor3dsceneunderstanding": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Multi-View PointNet for 3D Scene Understanding",
    "authors": [
      "Maximilian Jaritz",
      "Jiayuan Gu",
      "Hao Su"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Jaritz_Multi-View_PointNet_for_3D_Scene_Understanding_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Jaritz_Multi-View_PointNet_for_3D_Scene_Understanding_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Fusion of 2D images and 3D point clouds is important because information from dense images can enhance sparse point clouds. However, fusion is challenging because 2D and 3D data live in different spaces. In this work, we propose MVPNet (Multi-View PointNet), where we aggregate 2D multi-view image features into 3D point clouds, and then use a point based network to fuse the features in 3D canonical space to predict 3D semantic labels. To this end, we introduce view selection along with a 2D-3D feature aggregation module. Extensive experiments show the benefit of leveraging features from dense images and reveal superior robustness to varying point cloud density compared to 3D-only methods. On the ScanNetV2 benchmark, our MVPNet significantly outperforms prior point cloud based approaches on the task of 3D Semantic Segmentation. It is much faster to train than the large networks of the sparse voxel approach. We provide solid ablation studies to ease the future design of 2D-3D fusion methods and their extension to other tasks, as we showcase for 3D instance segmentation.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_parametrichumanshapereconstructionviabidirectionalsilhouetteguidance": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Parametric Human Shape Reconstruction via Bidirectional Silhouette Guidance",
    "authors": [
      "Shuang Sun",
      "Chen Li",
      "Zhenhua Guo",
      "Yuwing Tai"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Sun_Parametric_Human_Shape_Reconstruction_via_Bidirectional_Silhouette_Guidance_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Sun_Parametric_Human_Shape_Reconstruction_via_Bidirectional_Silhouette_Guidance_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We present a method to reconstruct the body geometry of a person by aligning the skinned multi-person linear (SMPL) model to an unconstrained human image. In contrast to previous methods that regress the model parameters from a shared image feature, we decouple the regression of pose and shape parameters in two sub-networks so that we can use different backbone architectures to extract better and more specific features for each regression task while allowing the two sub-networks to work together by our final training loss. We have further proposed a novel bidirectional silhouette constraint to restrict the estimated body geometry. The silhouette constraint is weighted adaptively according to the accuracy of pose estimation in order to handle truncations, occlusions and complex human poses. Experimental results on Human3.6M and UP datasets show that our method outperforms state-of-the-art methods and fits the body segmentation better, especially under extreme human pose conditions.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_auto-encodingmeshesofanytopologywiththecurrent-splattingandexponentiationlayers": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Auto-Encoding Meshes of any Topology with the Current-Splatting and Exponentiation Layers",
    "authors": [
      "Alexandre Bone",
      "Olivier Colliot",
      "Stanley Durrleman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Bone_Auto-Encoding_Meshes_of_any_Topology_with_the_Current-Splatting_and_Exponentiation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Bone_Auto-Encoding_Meshes_of_any_Topology_with_the_Current-Splatting_and_Exponentiation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Deep learning has met key applications in image computing, but still lacks processing paradigms for meshes, i.e. collections of elementary geometrical parts such as points, segments or triangles. Meshes are both a powerful representation for geometrical objects, and a challenge for network architectures because of their inherent irregular structure. This work contributes to adapt classical deep learning paradigms to this particular type of data in three ways. First, we introduce the current-splatting layer which embeds meshes in a metric space, allowing the downstream network to process them without any assumption on their topology: they may be composed of varied numbers of elements or connected components, contain holes, or bear high levels of geometrical noise. Second, we adapt to meshes the exponentiation layer which, from an upstream image array, generates shapes with a diffeomorphic control over their topology. Third, we take advantage of those layers to devise a variational auto-encoding architecture, which we interpret as a generative statistical model that learns adapted low-dimensional representations for mesh data sets. An explicit norm-control layer ensures the correspondence between the latent-space Euclidean metric and the shape-space log-Euclidean one. We illustrate this method on simulated and real data sets, and show the practical relevance of the learned representation for visualization, classification and mesh synthesis.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_generalizingmonocular3dhumanposeestimationinthewild": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Generalizing Monocular 3D Human Pose Estimation in the Wild",
    "authors": [
      "Luyang Wang",
      "Yan Chen",
      "Zhenhua Guo",
      "Keyuan Qian",
      "Mude Lin",
      "Hongsheng Li",
      "Jimmy S. Ren"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Wang_Generalizing_Monocular_3D_Human_Pose_Estimation_in_the_Wild_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Wang_Generalizing_Monocular_3D_Human_Pose_Estimation_in_the_Wild_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The availability of the large-scale labeled 3D poses in the Human3.6M dataset plays an important role in advancing the algorithms for 3D human pose estimation from a still image. We observe that recent innovation in this area mainly focuses on new techniques that explicitly address the generalization issue when using this dataset, because this database is constructed in a highly controlled environment with limited human subjects and background variations. Despite such efforts, we can show that the results of the current methods are still error-prone especially when tested against the images taken in-the-wild. In this paper, we aim to tackle this problem from a different perspective. We propose a principled approach to generate high quality 3D pose ground truth given any in-the-wild image with a person inside. We achieve this by first devising a novel stereo inspired the neural network to directly map any 2D pose to high quality 3D counterpart. We then perform a carefully designed geometric searching scheme to further refine the joints. Based on this scheme, we build a large-scale dataset with 400,000 in-the-wild images and their corresponding 3D pose ground truth. This enables the training of a high quality neural network model, without specialized training scheme and auxiliary loss function, which performs favorably against the state-of-the-art 3D pose estimation methods. We also evaluate the generalization ability of our model both quantitatively and qualitatively. Results show that our approach convincingly outperforms the previous methods. We make our dataset and code publicly available.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_patch-basedreconstructionofatexturelessdeformable3dsurfacefromasinglergbimage": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Patch-Based Reconstruction of a Textureless Deformable 3D Surface from a Single RGB Image",
    "authors": [
      "Aggeliki Tsoli",
      "Antonis. A. Argyros"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Tsoli_Patch-Based_Reconstruction_of_a_Textureless_Deformable_3D_Surface_from_a_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Tsoli_Patch-Based_Reconstruction_of_a_Textureless_Deformable_3D_Surface_from_a_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a deep learning method for reconstructing a textureless deformable 3D surface from a single RGB image, under various lighting conditions. One of the challenges when training a neural network to predict the shape of a deformable object is that the object exhibits such a great deal of shape variation that it is essentially impractical to have a training set consisting of all possible deformations the object may realize. However, different areas of the deformable object may exhibit similar types of deformations, e.g. similar wrinkles might appear in different areas on the surface of a cloth. Motivated by this, we propose learning local models of shape variation from image patches that we then combine into a global reconstruction of the observed object. Initially, we divide the input image into overlapping patches and a zero-mean depth map as well as a normal map are estimated for each patch using deep learning. Stitching of depth maps is performed by finding the optimal translation of each patch depth map along the viewing direction of the camera and averaging the depth predictions of neighboring patches at their overlapping areas. Stitching of normal maps is performed by normalizing and averaging the normals predictions of neighboring patches at their overlapping areas. Finally, bilateral filtering is performed on the stitched depth and normal maps in order to perform fine-scale smoothing at the regions around patch boundaries. We show increased accuracy compared to previous work even in the presence of limited training data and more effective generalization to unseen objects.\r",
    "code_link": "https://github.com/bednarikjan/texless_defsurf_recon"
  },
  "iccv2019_gmdl_self-supervisedlearningofdepthandmotionunderphotometricinconsistency": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Self-Supervised Learning of Depth and Motion Under Photometric Inconsistency",
    "authors": [
      "Tianwei Shen",
      "Lei Zhou",
      "Zixin Luo",
      "Yao Yao",
      "Shiwei Li",
      "Jiahui Zhang",
      "Tian Fang",
      "Long Quan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Shen_Self-Supervised_Learning_of_Depth_and_Motion_Under_Photometric_Inconsistency_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Shen_Self-Supervised_Learning_of_Depth_and_Motion_Under_Photometric_Inconsistency_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The self-supervised learning of depth and pose from monocular sequences provides an attractive solution by using the photometric consistency of nearby frames as it depends much less on the ground-truth data. In this paper, we address the issue when previous assumptions of the self-supervised approaches are violated due to the dynamic nature of real-world scenes. Different from handling the noise as uncertainty, our key idea is to incorporate more robust geometric quantities and enforce internal consistency in the temporal image sequence. As demonstrated on commonly used benchmark datasets, the proposed method substantially improves the state-of-the-art methods on both depth and relative pose estimation for monocular image sequences, without adding inference overhead.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_liftingautoencodersunsupervisedlearningofafully-disentangled3dmorphablemodelusingdeepnon-rigidstructurefrommotion": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Lifting AutoEncoders: Unsupervised Learning of a Fully-Disentangled 3D Morphable Model Using Deep Non-Rigid Structure From Motion",
    "authors": [
      "Mihir Sahasrabudhe",
      "Zhixin Shu",
      "Edward Bartrum",
      "Riza Alp Guler",
      "Dimitris Samaras",
      "Iasonas Kokkinos"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Sahasrabudhe_Lifting_AutoEncoders_Unsupervised_Learning_of_a_Fully-Disentangled_3D_Morphable_Model_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Sahasrabudhe_Lifting_AutoEncoders_Unsupervised_Learning_of_a_Fully-Disentangled_3D_Morphable_Model_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work we introduce Lifting Autoencoders, a generative 3D surface-based model of object categories. We bring together ideas from non-rigid structure from motion, image formation, and morphable models to learn a controllable, geometric model of 3D categories in an entirely unsupervised manner from an unstructured set of images. We exploit the 3D geometric nature of our model and use normal information to disentangle appearance into illumination, shading, and albedo. We further use weak supervision to disentangle the non-rigid shape variability of human faces into identity and expression. We combine the 3D representation with a differentiable renderer to generate RGB images and append an adversarially trained refinement network to obtain sharp, photorealistic image reconstruction results. The learned generative model can be controlled in terms of interpretable geometry and appearance factors, allowing us to perform photorealistic image manipulation of identity, expression, 3D pose, and illumination properties.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_floorsareflatleveragingsemanticsforreal-timesurfacenormalprediction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Floors are Flat: Leveraging Semantics for Real-Time Surface Normal Prediction",
    "authors": [
      "Steven Hickson",
      "Karthik Raveendran",
      "Alireza Fathi",
      "Kevin Murphy",
      "Irfan Essa"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Hickson_Floors_are_Flat_Leveraging_Semantics_for_Real-Time_Surface_Normal_Prediction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Hickson_Floors_are_Flat_Leveraging_Semantics_for_Real-Time_Surface_Normal_Prediction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose 4 insights that help to significantly improve the performance of deep learning models that predict surface normals and semantic labels from a single RGB image. These insights are: (1) denoise the \"ground truth\" surface normals in the training set to ensure consistency with the semantic labels; (2) concurrently train on a mix of real and synthetic data, instead of pretraining on synthetic and finetuning on real; (3) jointly predict normals and semantics using a shared model, but only backpropagate errors on pixels that have valid training labels; (4) slim down the model and use grayscale instead of color inputs. Despite the simplicity of these steps, we demonstrate consistently improved state of the art results on several datasets, using a model that runs at 12 fps on a standard mobile phone.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_hyperparameter-freelossesformodel-basedmonocularreconstruction": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Hyperparameter-Free Losses for Model-Based Monocular Reconstruction",
    "authors": [
      "Eduard Ramon",
      "Guillermo Ruiz",
      "Thomas Batard",
      "Xavier Giro-i-Nieto"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Ramon_Hyperparameter-Free_Losses_for_Model-Based_Monocular_Reconstruction_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Ramon_Hyperparameter-Free_Losses_for_Model-Based_Monocular_Reconstruction_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This work proposes novel hyperparameter-free losses for single view 3D reconstruction with morphable models (3DMM). We dispense with the hyperparameters used in other works by exploiting geometry, so that the shape of the object and the camera pose are jointly optimized in a sole term expression. This simplification reduces the optimization time and its complexity. Moreover, we propose a novel implicit regularization technique based on random virtual projections that does not require additional 2D or 3D annotations. Our experiments suggest that minimizing a shape reprojection error together with the proposed implicit regularization is especially suitable for applications that require precise alignment between geometry and image spaces, such as augmented reality. We evaluate our losses on a large scale dataset with 3D ground truth and publish our implementations to facilitate reproducibility and public benchmarking in this field.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_momen(e)tflavorthemomentsinlearningtoclassifyshapes": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Momen(e)t: Flavor the Moments in Learning to Classify Shapes",
    "authors": [
      "Mor Joseph-Rivlin",
      "Alon Zvirin",
      "Ron Kimmel"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Joseph-Rivlin_Momenet_Flavor_the_Moments_in_Learning_to_Classify_Shapes_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Joseph-Rivlin_Momenet_Flavor_the_Moments_in_Learning_to_Classify_Shapes_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " A fundamental question in learning to classify 3D shapes is how to treat the data in a way that would allow us to construct efficient and accurate geometric processing and analysis procedures. Here, we restrict ourselves to networks that operate on point clouds. There were several attempts to treat point clouds as non-structured data sets by which a neural network is trained to extract discriminative properties. The idea of using 3D coordinates as class identifiers motivated us to extend this line of thought to that of shape classification by comparing attributes that could easily account for the shape moments. Here, we propose to add polynomial functions of the coordinates allowing the network to account for higher order moments of a given shape. Experiments on two benchmarks show that the suggested network is able to provide state of the art results and at the same token learn more efficiently in terms of memory and computational complexity.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_ageometricapproachtoobtainabirdseyeviewfromanimage": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "A Geometric Approach to Obtain a Bird's Eye View From an Image",
    "authors": [
      "Syed Ammar Abbas",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Abbas_A_Geometric_Approach_to_Obtain_a_Birds_Eye_View_From_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Abbas_A_Geometric_Approach_to_Obtain_a_Birds_Eye_View_From_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The objective of this paper is to rectify any monocular image by computing a homography matrix that transforms it to a geometrically correct bird's eye (overhead) view. We make the following contributions: (i) we show that the homography matrix can be parameterised with only four geometric parameters that specify the horizon line and the vertical vanishing point, or only two if the field of view or focal length is known; (ii) We introduce a novel representation for the geometry of a line or point (which can be at infinity) that is suitable for regression with a convolutional neural network (CNN); (iii) We introduce a large synthetic image dataset with ground truth for the orthogonal vanishing points, that can be used for training a CNN to predict these geometric entities; and finally (iv) We achieve state-of-the-art results on horizon detection, with 74.52% AUC on the Horizon Lines in the Wild dataset. Our method is fast and robust, and can be used to remove perspective distortion from videos in real time.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_rethinkingtaskandmetricsofinstancesegmentationon3dpointclouds": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Rethinking Task and Metrics of Instance Segmentation on 3D Point Clouds",
    "authors": [
      "Kosuke Arase",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Arase_Rethinking_Task_and_Metrics_of_Instance_Segmentation_on_3D_Point_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Arase_Rethinking_Task_and_Metrics_of_Instance_Segmentation_on_3D_Point_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Instance segmentation on 3D point clouds is one of the most extensively researched areas toward the realization of autonomous cars and robots. Certain existing studies have split input point clouds into small regions such as 1mx1m; one reason for this is that models in the studies cannot consume a large number of points because of the large space complexity. However, because such small regions occasionally include a very small number of instances belonging to the same class, an evaluation using existing metrics such as mAP is largely affected by the category recognition performance. To address these problems, we propose a new method with space complexity O(Np) such that large regions can be consumed, as well as novel metrics for tasks that are independent of the categories or size of the inputs. Our method learns a mapping from input point clouds to an embedding space, where the embeddings form clusters for each instance and distinguish instances using these clusters during testing. Our method achieves state-of-the-art performance using both existing and the proposed metrics. Moreover, we show that our new metric can evaluate the performance of a task without being affected by any other condition.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_render4completionsynthesizingmulti-viewdepthmapsfor3dshapecompletion": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Render4Completion: Synthesizing Multi-View Depth Maps for 3D Shape Completion",
    "authors": [
      "Tao Hu",
      "Zhizhong Han",
      "Abhinav Shrivastava",
      "Matthias Zwicker"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Hu_Render4Completion_Synthesizing_Multi-View_Depth_Maps_for_3D_Shape_Completion_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Hu_Render4Completion_Synthesizing_Multi-View_Depth_Maps_for_3D_Shape_Completion_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We propose a novel approach for 3D shape completion by synthesizing multi-view depth maps. While previous work for shape completion relies on volumetric representations, meshes, or point clouds, we propose to use multi-view depth maps from a set of fixed viewing angles as our shape representation. This allows us to be free of the memory limitations of volumetric representations and point clouds by casting shape completion into an image-to-image translation problem. Specifically, we render depth maps of the incomplete shape from a fixed set of viewpoints, and perform depth map completion in each view. Different from image-to-image translation networks that process each view separately, our novel multi-view completion net (MVCN) leverages information from all views of a 3D shape to help the completion of each single view. This enables MVCN to leverage more information from different depth views to achieve high accuracy in single depth view completion, and improve the consistency among the completed depth images in different views. Benefiting from the multi-view representation and novel network structure, MVCN significantly improves the accuracy of 3D shape completion in large-scale benchmarks compared to the state of the art.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_residualattentiongraphconvolutionalnetworkforgeometric3dsceneclassification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Residual Attention Graph Convolutional Network for Geometric 3D Scene Classification",
    "authors": [
      "Albert Mosella-Montoro",
      "Javier Ruiz-Hidalgo"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Mosella-Montoro_Residual_Attention_Graph_Convolutional_Network_for_Geometric_3D_Scene_Classification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Mosella-Montoro_Residual_Attention_Graph_Convolutional_Network_for_Geometric_3D_Scene_Classification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Geometric 3D scene classification is a very challenging task. Current methodologies extract the geometric information using only a depth channel provided by an RGB-D sensor. These kinds of methodologies introduce possible errors due to missing local geometric context in the depth channel. This work proposes a novel Residual Attention Graph Convolutional Network that exploits the intrinsic geometric context inside a 3D space without using any kind of point features, allowing the use of organized or unorganized 3D data. Experiments are done in NYU Depth v1 and SUN-RGBD datasets to study the different configurations and to demonstrate the effectiveness of the proposed method. Experimental results show that the proposed method outperforms current state-of-the-art in geometric 3D scene classification tasks.\r",
    "code_link": ""
  },
  "iccv2019_gmdl_learningtoreconstructsymmetricshapesusingplanarparameterizationof3dsurface": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "Learning to Reconstruct Symmetric Shapes using Planar Parameterization of 3D Surface",
    "authors": [
      "Hardik Jain",
      "Manuel Wollhaf",
      "Olaf Hellwich"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Jain_Learning_to_Reconstruct_Symmetric_Shapes_using_Planar_Parameterization_of_3D_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Jain_Learning_to_Reconstruct_Symmetric_Shapes_using_Planar_Parameterization_of_3D_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Shape priors have been a game changer to achieve robust 3D reconstruction. Prior knowledge encoded in trained networks has proven to be effective in generating images. Based on a similar paradigm, various methods were proposed to generate 3D shape from images. To generate a voxel or point cloud representation of 3D shapes these methods required adding an extra dimension to the deep network, to handle 3D data. Unlike these methods, we try to reconstruct 3D shape from images by using a parameterized representation of the shape. For a 3D model, the information is mainly concentrated on the surface. We perform iterative parameterization of the surface to obtain a planar representation. This representation is encoded with surface information to generate 2D geometry images, which can be conveniently learned using traditional deep neural networks without additional overhead. We propose an efficient iterative planar parameterization to represent regions of high Gaussian curvature in geometry images. Our experiments demonstrate that the proposed network learns detailed features and is able to reconstruct geometrically accurate shapes from single image. Our code is available at https://github.com/hrdkjain/LearningSymmetricShapes.\r",
    "code_link": "https://github.com/hrdkjain/LearningSymmetricShapes"
  },
  "iccv2019_gmdl_spiralnet++afastandhighlyefficientmeshconvolutionoperator": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "GMDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Geometry Meets Deep Learning",
    "title": "SpiralNet++: A Fast and Highly Efficient Mesh Convolution Operator",
    "authors": [
      "Shunwang Gong",
      "Lei Chen",
      "Michael Bronstein",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/GMDL/Gong_SpiralNet_A_Fast_and_Highly_Efficient_Mesh_Convolution_Operator_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/GMDL/Gong_SpiralNet_A_Fast_and_Highly_Efficient_Mesh_Convolution_Operator_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Intrinsic graph convolution operators with differentiable kernel functions play a crucial role in analyzing 3D shape meshes. In this paper, we present a fast and efficient intrinsic mesh convolution operator that does not rely on the intricate design of kernel function. We explicitly formulate the order of aggregating neighboring vertices, instead of learning weights between nodes, and then a fully connected layer follows to fuse local geometric structure information with vertex features. We provide extensive evidence showing that models based on this convolution operator are easier to train, and can efficiently learn invariant shape features. Specifically, we evaluate our method on three different types of tasks of dense shape correspondence, 3D facial expression classification, and 3D shape reconstruction, and show that it significantly outperforms state-of-the-art approaches while being significantly faster, without relying on shape descriptors.\r",
    "code_link": "https://github.com/sw-gong/spiralnet_plus"
  },
  "iccv2019_autonue_multi-tasklearningviascaleawarefeaturepyramidnetworksandeffectivejointhead": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AUTONUE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Multi-Task Learning via Scale Aware Feature Pyramid Networks and Effective Joint Head",
    "authors": [
      "Feng Ni",
      "Yuehan Yao"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AUTONUE/Ni_Multi-Task_Learning_via_Scale_Aware_Feature_Pyramid_Networks_and_Effective_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AUTONUE/Ni_Multi-Task_Learning_via_Scale_Aware_Feature_Pyramid_Networks_and_Effective_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " As a concise and classic framework for object detection and instance segmentation, Mask R-CNN achieves promising performance in both two tasks.However, considering stronger feature representation for Mask R-CNN fashion framework, there is room for improvement from two aspects. On the one hand, performing multi-task prediction needs more credible feature extraction and multi-scale features integration to handle objects with varied scales. In this paper, we address this problem by using a novel neck module called SA-FPN (Scale Aware Feature Pyramid Networks). With the enhanced feature representations, our model can accurately detect and segment the objects of multiple scales. On the other hand, in Mask R-CNN framework, isolation between parallel detection branch and instance segmentation branch exists, causing the gap between training and testing processes. To narrow this gap, we propose a unified head module named EJ-Head (Effective Joint Head) to combine two branches into one head, not only realizing the interaction between two tasks, but also enhancing the effectiveness of multi-task learning. Comprehensive experiments show that our proposed methods bring noticeable gains for object detection and instance segmentation. In particular, our model outperforms the original Mask R-CNN by 1 2 percent AP in both object detection and instance segmentation task on MS-COCO benchmark. Code will be available soon.\r",
    "code_link": ""
  },
  "iccv2019_autonue_desoilingdatasetrestoringsoiledareasonautomotivefisheyecameras": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AUTONUE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Desoiling Dataset: Restoring Soiled Areas on Automotive Fisheye Cameras",
    "authors": [
      "Michal Uricar",
      "Jan Ulicny",
      "Ganesh Sistu",
      "Hazem Rashed",
      "Pavel Krizek",
      "David Hurych",
      "Antonin Vobecky",
      "Senthil Yogamani"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AUTONUE/Uricar_Desoiling_Dataset_Restoring_Soiled_Areas_on_Automotive_Fisheye_Cameras_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AUTONUE/Uricar_Desoiling_Dataset_Restoring_Soiled_Areas_on_Automotive_Fisheye_Cameras_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Surround-view cameras became an integral part of autonomous driving setup. Being directly exposed to harsh environmental settings, they can get soiled easily. When cameras get soiled, the degradation of performance is usually more dramatic compared to other sensors. Having this on mind, we decided to design a dataset for measuring the performance degradation as well as to help constructing classifiers for soiling detection, or for trying to restore the soiled images, so we can increase the performance of the off-the-shelf classifiers. The proposed dataset contains 40+ approximately 1 minute long video sequences with paired image information of both clean and soiled nature. The dataset will be released as a companion to our recently published dataset [14] to encourage further research in this area. We constructed a CycleGAN architecture to produce de-soiled images and demonstrate 5% improvement in road detection and 3% improvement in detection of lanes and curbs.\r",
    "code_link": ""
  },
  "iccv2019_autonue_motionsegmentationviasynchronization": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AUTONUE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Motion Segmentation via Synchronization",
    "authors": [
      "Federica Arrigoni",
      "Tomas Pajdla"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AUTONUE/Arrigoni_Motion_Segmentation_via_Synchronization_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AUTONUE/Arrigoni_Motion_Segmentation_via_Synchronization_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper we consider the problem of segmenting points in a collection of images that contain multiple moving objects. Our contribution is three-fold: (i) we propose a matrix representation of segmentation that permits to formulate the problem in terms of \"synchronization\" of binary matrices; (ii) we derive a spectral solution to solve such a problem, which is inspired by previous works on synchronization of rotations, homographies, rigid motions and permutations; (iii) we explain how our solution can be interpreted in terms of spectral clustering. The proposed approach is validated on both synthetic and real scenarios, in addition to the Hopkins benchmark.\r",
    "code_link": ""
  },
  "iccv2019_autonue_real-timevehiclelocalizationusingon-boardvisualslamfordetectionandtracking": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AUTONUE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Real-Time Vehicle Localization using on-Board Visual SLAM for Detection and Tracking",
    "authors": [
      "Nabil Belbachir",
      "Nadia Noori",
      "Benyamin Akdemir"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AUTONUE/Belbachir_Real-Time_Vehicle_Localization_using_on-Board_Visual_SLAM_for_Detection_and_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AUTONUE/Belbachir_Real-Time_Vehicle_Localization_using_on-Board_Visual_SLAM_for_Detection_and_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper presents an on-board system based using a monocular vision system, auxiliary sensors and a visual SLAM method for real-time vehicle localization and tracking in natural outdoor environments. A monocular panamorph camera providing 360deg panoramic views has been integrated with a gyro, compass and accelerometer to form a sensor system for installation on-board vehicles. A direct method was applied for detection and tracking of environment based on the images and combined with a calibration of the keypoints using the auxiliary sensors for providing an accurate trajectory to the vehicle. An-board computer is used to facilitate a standalone deployment onboard vehicle. The system was developed and designed as an integral component of a security monitoring and tracking system for humanitarian missions and convoys operating in regions with poor or absence of GPS signal coverage. Preliminary results show the applicability of the system with acceptable accuracy compared to GPS after evaluation in natural outdoor environments in Europe.\r",
    "code_link": ""
  },
  "iccv2019_autonue_ongeneralizingdetectionmodelsforunconstrainedenvironments": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AUTONUE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "On Generalizing Detection Models for Unconstrained Environments",
    "authors": [
      "Prajjwal Bhargava"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AUTONUE/Bhargava_On_Generalizing_Detection_Models_for_Unconstrained_Environments_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AUTONUE/Bhargava_On_Generalizing_Detection_Models_for_Unconstrained_Environments_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Object detection has seen tremendous progress in recent years. However, current algorithms don't generalize well when tested on diverse data distributions. We address the problem of incremental learning in object detection on the India Driving Dataset (IDD). Our approach involves using multiple domain-specific classifiers and effective transfer learning techniques focussed on avoiding catastrophic forgetting. We evaluate our approach on the IDD and BDD100K dataset. Results show the effectiveness of our domain adaptive approach in the case of domain shifts in environments.\r",
    "code_link": ""
  },
  "iccv2019_autonue_largescalemultimodaldatacapture,evaluationandmaintenanceframeworkforautonomousdrivingdatasets": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "AUTONUE",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - AutoNUE: Autonomous Navigation in Unconstrained Environments",
    "title": "Large Scale Multimodal Data Capture, Evaluation and Maintenance Framework for Autonomous Driving Datasets",
    "authors": [
      "Nitheesh Lakshminarayana"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/AUTONUE/Lakshminarayana_Large_Scale_Multimodal_Data_Capture_Evaluation_and_Maintenance_Framework_for_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/AUTONUE/Lakshminarayana_Large_Scale_Multimodal_Data_Capture_Evaluation_and_Maintenance_Framework_for_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Autonomous driving (AD) programs today primarily depend on one or the other form of supervised Deep Learning (DL) models for their behavioral success. However, these DL models are only as good as the data on which they are trained, and their success depends immensely on their training data. Hence it is imperative that we create datasets of good quality. However, the process of collecting this real-world driving data and the infrastructure needed to evaluate and manage this large data is commonly unspoken and is challenging. To address this issue, we have developed an open-source framework and infrastructure to capture, evaluate, and maintain such multi-sensor data. In this paper, we discuss the motive for this framework, a process for evaluation and quality analysis, insights on data storage, distribution and management for large multimodal data and the key lessons learned collecting and maintaining huge volumes of data from long driving distances.\r",
    "code_link": "https://github.com/intel/drivingdata-collection-reference-kit"
  },
  "iccv2019_pbdl_singleimageintrinsicdecompositionwithdiscriminativefeatureencoding": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Single Image Intrinsic Decomposition with Discriminative Feature Encoding",
    "authors": [
      "Zongji Wang",
      "Feng Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PBDL/Wang_Single_Image_Intrinsic_Decomposition_with_Discriminative_Feature_Encoding_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PBDL/Wang_Single_Image_Intrinsic_Decomposition_with_Discriminative_Feature_Encoding_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Intrinsic image decomposition is an important and long-standing computer vision problem. Given a single input image, recovering the physical scene properties is ill-posed. In this work, we take the advantage of deep learning, which is proven to be highly efficient in solving the challenging computer vision problems including intrinsic image decomposition. Our focus lies in the feature encoding phase to extract discriminative features for different intrinsic layers from a single input image. To achieve this goal, we explore the distinctive characteristics between different intrinsic components in the high dimensional feature embedding space. We propose a feature divergence loss to force their high-dimensional embedding feature vectors to be separated efficiently. The feature distributions are also constrained to fit the real ones. In addition, we provide an approach to remove the data inconsistency in the MPI Sintel dataset, making it more proper for intrinsic image decomposition. Experimental results indicate that the proposed network structure is able to outperform the state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_pbdl_event-drivenvideoframesynthesis": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Event-Driven Video Frame Synthesis",
    "authors": [
      "Zihao W. Wang",
      "Weixin Jiang",
      "Kuan He",
      "Boxin Shi",
      "Aggelos Katsaggelos",
      "Oliver Cossairt"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PBDL/Wang_Event-Driven_Video_Frame_Synthesis_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PBDL/Wang_Event-Driven_Video_Frame_Synthesis_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Temporal Video Frame Synthesis (TVFS) aims at synthesizing novel frames at timestamps different from existing frames, which has wide applications in video codec, editing and analysis. In this paper, we propose a high frame-rate TVFS framework which takes hybrid input data from a low-speed frame-based sensor and a high-speed event-based sensor. Compared to frame-based sensors, event-based sensors report brightness changes at very high speed, which may well provide useful spatio-temoral information for high frame-rate TVFS. Therefore, we first introduce a differentiable fusion model to approximate the dual-modal physical sensing process, unifying a variety of TVFS scenarios, e.g., interpolation, prediction and motion deblur. Our differentiable model enables iterative optimization of the latent video tensor via autodifferentiation, which propagates the gradients of a loss function defined on the measured data. Our differentiable model-based reconstruction does not involve training, yet is parallelizable and can be implemented on machine learning platforms (such as TensorFlow). Second, we develop a deep learning strategy to enhance the results from the first step, which we refer as a residual \"denoising\" process. Our trained \"denoiser\" is beyond Gaussian denoising and shows properties such as contrast enhancement and motion awareness. We show that our framework is capable of handling challenging scenes including both fast motion and strong occlusions.\r",
    "code_link": ""
  },
  "iccv2019_pbdl_multi-levelandmulti-scalespatialandspectralfusioncnnforhyperspectralimagesuper-resolution": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Multi-Level and Multi-Scale Spatial and Spectral Fusion CNN for Hyperspectral Image Super-Resolution",
    "authors": [
      "Xian-Hua Han",
      "YinQiang Zheng",
      "Yen-Wei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PBDL/Han_Multi-Level_and_Multi-Scale_Spatial_and_Spectral_Fusion_CNN_for_Hyperspectral_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PBDL/Han_Multi-Level_and_Multi-Scale_Spatial_and_Spectral_Fusion_CNN_for_Hyperspectral_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Hyperspectral imaging simultaneously captures images of the same scene across many numbers of spectral channels, and has different applications from agriculture, astronomy to surveillance and mineralogy, to name a few. However, due to various hardware limitations, the current hyperspectral sensor only provides low-resolution (LR) hyperspectral images compared with the RGB images obtained from a common color camera. Thus fusing a LR hyperspectral image with the corresponding high-resolution (HR) RGB image to recover a HR hyperspectral image has attracted much attention, and is usually solved as an optimization problem with prior-knowledge constraints such as sparsity representation and spectral physical properties. Motivated by the great success of deep convolutional neural network (DCNN) in many computer vision tasks, this study aims to design a novel DCNN architecture for effectively fusing the LR hyperspectral and HR-RGB images. Taking consideration of the large resolution difference in spatial domain of the observed RGB and hyperspectral images, we propose a multi-scale DCNN via gradually reducing the feature sizes of the RGB images and increasing the feature sizes of the hyperspectral image for fusion. Furthermore, we integrate multi-level cost functions into the proposed multi-scale fusion CNN architecture for alleviating the gradient vanish problem in training procedure. Experiment results on benchmark datasets validate that the proposed multi-level and multi-scale spatial and spectral fusion CNNs outperforms the state-of-the-art methods in both quantitative values and visual qualities.\r",
    "code_link": ""
  },
  "iccv2019_pbdl_learningfromsyntheticphotorealisticraindropforsingleimageraindropremoval": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Learning From Synthetic Photorealistic Raindrop for Single Image Raindrop Removal",
    "authors": [
      "Zhixiang Hao",
      "Shaodi You",
      "Yu Li",
      "Kunming Li",
      "Feng Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PBDL/Hao_Learning_From_Synthetic_Photorealistic_Raindrop_for_Single_Image_Raindrop_Removal_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PBDL/Hao_Learning_From_Synthetic_Photorealistic_Raindrop_for_Single_Image_Raindrop_Removal_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Raindrops adhered to camera lens or windshield are inevitable in rainy scenes and can become an issue for many computer vision systems such as autonomous driving. Because raindrop appearance is affected by too many parameters, therefore it is unlikely to find an effective model based solution. Learning based methods are also problematic, because traditional learning method cannot properly model the complex appearance. Whereas deep learning method lacks sufficiently large and realistic training data. To solve it, in our work, we propose the first photo-realistic dataset of synthetic adherent raindrops for training. The rendering is physics based with consideration of the water dynamic, geometric and photometry. The dataset contains various types of rainy scenes and particularly the rainy driving scenes. Based on the modeling of raindrop imagery, we introduce a detection network which has the awareness of the raindrop refraction as well as its blurring. Based on that, we propose the removal network that can well recover the image structure. Rigorous experiments demonstrate the state-of-the-art performance of our proposed framework.\r",
    "code_link": ""
  },
  "iccv2019_pbdl_multispectralreconstructionfromreferenceobjectsinthescene": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PBDL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Physics Based Vision Meets Deep Learning",
    "title": "Multispectral Reconstruction From Reference Objects in the Scene",
    "authors": [
      "Nirit Nussbaum Hoffer",
      "Tomer Michaeli"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PBDL/Hoffer_Multispectral_Reconstruction_From_Reference_Objects_in_the_Scene_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PBDL/Hoffer_Multispectral_Reconstruction_From_Reference_Objects_in_the_Scene_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Hyperspectral imaging methods typically require dedicated cameras with extra optical elements (prisms, fibers, lenslet arrays), thus making them expensive and cumbersome to deploy. In this paper we explore a drastically different hyperspectral imaging approach, which requires no special optical components and can thus be used with any conventional camera. The idea is to place a reference object with a known spectrum (e.g. a black mask) within the field of view and to exploit the chromatic dependence of the Point Spread Function (PSF), in order to solve for the spectra of all other parts of the scene. We prove mathematically that chromatic-dependent blur cues alone are insufficient for fully recovering the spectrum of each pixel, even if the locations of edges in the (sharp) image are precisely known. Yet, we show that knowing the spectra at some of the pixels fully resolves this inherent ambiguity. We present an algorithm for solving the spectrum-from-reference inverse problem and illustrate its effectiveness through simulations as well as in a simple real world experiment\r",
    "code_link": ""
  },
  "iccv2019_epic_egovqa-anegocentricvideoquestionansweringbenchmarkdataset": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "EgoVQA - An Egocentric Video Question Answering Benchmark Dataset",
    "authors": [
      "Chenyou Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Fan_EgoVQA_-_An_Egocentric_Video_Question_Answering_Benchmark_Dataset_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Fan_EgoVQA_-_An_Egocentric_Video_Question_Answering_Benchmark_Dataset_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recently, much effort and attention has been devoted to Visual Question Answering (VQA) on static images and Video Question Answering (VideoQA) on third-person videos. In the meantime, first-person question answering has more natural use cases while this topic remains seldom studied. A typical meaningful scenario is an intelligent agent provides assistance to handicapped people to perceive the environment by the queries, localize objects and persons based on descriptions, and identify intentions of surrounding people to guide their reactions (e.g., shake hands or avoid punches). However, due to the lack of first-person video datasets, seldom study had been carried on first-person VideoQA task. To address this issue, we collected a novel egocentric VideoQA dataset called EgoVQA with 600 question-answer pairs with visual contents across 5,000 frames from 16 first-person videos. Various types of queries such as \"Who\", \"What\", \"How many\" are provided to form a semantically rich corpus. We use this database to evaluate the performance of four mainstream third-person VideoQA methods to illustrate their performance gap between first-person related questions and third-person related questions. We believe that EgoVQA dataset will facilitate future research on the imperative task of first-person VideoQA.\r",
    "code_link": "https://github.com/fanchenyou/EgoVQA.git"
  },
  "iccv2019_epic_simultaneoussegmentationandrecognitiontowardsmoreaccurateegogesturerecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Simultaneous Segmentation and Recognition: Towards More Accurate Ego Gesture Recognition",
    "authors": [
      "Tejo Chalasani",
      "Aljosa Smolic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Chalasani_Simultaneous_Segmentation_and_Recognition_Towards_More_Accurate_Ego_Gesture_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Chalasani_Simultaneous_Segmentation_and_Recognition_Towards_More_Accurate_Ego_Gesture_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Ego hand gestures can be used as an interface in AR and VR environments. While the context of an image is important for tasks like scene understanding, object recognition, image caption generation and activity recognition, it plays a minimal role in ego hand gesture recognition. An ego hand gesture used for AR and VR environments conveys the same information regardless of the background. With this idea in mind, we present our work on ego hand gesture recognition that produces embeddings from RBG images with ego hands, which are simultaneously used for ego hand segmentation and ego gesture recognition. To this extent, we achieved better recognition accuracy (96.9%) compared to the state of the art (92.2%) on the biggest ego hand gesture dataset available publicly. We present a gesture recognition deep neural network which recognises ego hand gestures from videos (videos containing a single gesture) by generating and recognising embeddings of ego hands from image sequences of varying length. We introduce the concept of simultaneous segmentation and recognition applied to ego hand gestures, present the network architecture, the training procedure and the results compared to the state of the art on the EgoGesture dataset\r",
    "code_link": ""
  },
  "iccv2019_epic_ego-semanticlabelingofscenefromdepthimageforvisuallyimpairedandblindpeople": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Ego-Semantic Labeling of Scene from Depth Image for Visually Impaired and Blind People",
    "authors": [
      "Chayma Zatout",
      "Slimane Larabi",
      "Ilyes Mendili",
      "Soedji Ablam Edoh Barnabe"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Zatout_Ego-Semantic_Labeling_of_Scene_from_Depth_Image_for_Visually_Impaired_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Zatout_Ego-Semantic_Labeling_of_Scene_from_Depth_Image_for_Visually_Impaired_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This work is devoted to scene understanding and motion ability improvement for visually impaired and blind people. We investigate how to exploit egocentric vision to provide semantic labeling of scene from head-mounted depth camera. More specifically, we propose a new method for locating ground from depth image whatever the camera's pose. The rest of planes of the scene are located using RANSAC method, semantically coded by their attributes and mapped as cylinders into a generated 3D scene which will serve as a feedback to users. Experiments are conducted and the obtained results are discussed.\r",
    "code_link": ""
  },
  "iccv2019_epic_manipulation-skillassessmentfromvideoswithspatialattentionnetwork": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Manipulation-Skill Assessment from Videos with Spatial Attention Network",
    "authors": [
      "Zhenqiang Li",
      "Yifei Huang",
      "Minjie Cai",
      "Yoichi Sato"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Li_Manipulation-Skill_Assessment_from_Videos_with_Spatial_Attention_Network_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Li_Manipulation-Skill_Assessment_from_Videos_with_Spatial_Attention_Network_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recent advances in computer vision have made it possible to automatically assess from videos the manipulation skills of humans in performing a task, which breeds many important applications in domains such as health rehabilitation and manufacturing. Previous methods of video-based skill assessment did not consider the spatial attention mechanism humans use in assessing videos, limiting their performance as only a small part of video regions is informative for skill assessment. Our motivation here is to estimate attention in videos that helps to focus on critically important video regions for better skill assessment. In particular, we propose a novel RNN-based spatial attention model that considers accumulated attention state from previous frames as well as high-level information about the progress of an undergoing task. We evaluate our approach on a newly collected dataset of infant grasping task and four existing datasets of hand manipulation tasks. Experiment results demonstrate that state-of-the-art performance can be achieved by considering attention in automatic skill assessment.\r",
    "code_link": ""
  },
  "iccv2019_epic_multitasklearningtoimproveegocentricactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Multitask Learning to Improve Egocentric Action Recognition",
    "authors": [
      "Georgios Kapidis",
      "Ronald Poppe",
      "Elsbeth van Dam",
      "Lucas Noldus",
      "Remco Veltkamp"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Kapidis_Multitask_Learning_to_Improve_Egocentric_Action_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Kapidis_Multitask_Learning_to_Improve_Egocentric_Action_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work we employ multitask learning to capitalize on the structure that exists in related supervised tasks to train complex neural networks. It allows training a network for multiple objectives in parallel, in order to improve performance on at least one of them by capitalizing on a shared representation that is developed to accommodate more information than it otherwise would for a single task. We employ this idea to tackle action recognition in egocentric videos by introducing additional supervised tasks. We consider learning the verbs and nouns from which action labels consist of and predict coordinates that capture the hand locations and the gaze-based visual saliency for all the frames of the input video segments. This forces the network to explicitly focus on cues from secondary tasks that it might otherwise have missed resulting in improved inference. Our experiments on EPIC-Kitchens and EGTEA Gaze+ show consistent improvements when training with multiple tasks over the single-task baseline. Furthermore, in EGTEA Gaze+ we outperform the state-of-the-art in action recognition by 3.84%. Apart from actions, our method produces accurate hand and gaze estimations as side tasks, without requiring any additional input at test time other than the RGB video clips.\r",
    "code_link": ""
  },
  "iccv2019_epic_theapplicabilityofcyclegansforpupilandeyelidsegmentation,datagenerationandimagerefinement": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "The Applicability of Cycle GANs for Pupil and Eyelid Segmentation, Data Generation and Image Refinement",
    "authors": [
      "Wolfgang Fuhl",
      "David Geisler",
      "Wolfgang Rosenstiel",
      "Enkelejda Kasneci"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Fuhl_The_Applicability_of_Cycle_GANs_for_Pupil_and_Eyelid_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Fuhl_The_Applicability_of_Cycle_GANs_for_Pupil_and_Eyelid_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Eye tracking is increasingly influencing scientific areas such as psychology, cognitive science, and human-computer interaction. Many eye trackers output the gaze location and the pupil center. However, other valuable information can also be extracted from the eyelids, such as the fatigue of a person. We evaluated Generative Adversarial Networks (GAN) for eyelid and pupil area segmentation, data generation, and image refinement. While the segmentation GAN performs the desired task, the others serve as supportive Networks. The trained data generation GAN does not require simulated data to increase the dataset, it simply uses existing data and creates subsets. The purpose of the refinement GAN, in contrast, is to simplify manual annotation by removing noise and occlusion in an image without changing the eye structure and pupil position. In addition 100,000 pupil and eyelid segmentations are made publicly available for images from the labeled pupils in the wild data set (DOWNLOAD). These will support further research in this area.\r",
    "code_link": ""
  },
  "iccv2019_epic_weakly-superviseddegreeofeye-closenessestimation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Weakly-Supervised Degree of Eye-Closeness Estimation",
    "authors": [
      "Eyasu Mequanint",
      "Shuai Zhang",
      "Bijan Forutanpour",
      "Yingyong Qi",
      "Ning Bi"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Mequanint_Weakly-Supervised_Degree_of_Eye-Closeness_Estimation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Mequanint_Weakly-Supervised_Degree_of_Eye-Closeness_Estimation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Following recent technological advances there is a growing interest in building non-intrusive methods that help us communicate with computing devices. In this regard, accurate information from eye is a promising input medium between a user and computing devices. In this paper we propose a method that captures the degree of eye closeness. Although many methods exist for detection of eyelid openness, they are inherently unable to satisfactorily perform in real world applications. Detailed eye state estimation is more important, in extracting meaningful information, than estimating whether eyes are open or closed. However, learning reliable eye state estimator requires accurate annotations which is cost prohibitive. In this work, we leverage synthetic face images which can be generated via computer graphics rendering techniques and automatically annotated with different levels of eye openness. These synthesized training data images, however, have a domain shift from real-world data. To alleviate this issue, we propose a weakly-supervised method which utilizes the accurate annotation from the synthetic data set, to learn accurate degree of eye openness, and the weakly labeled (open or closed) real world eye data set to control the domain shift. We introduce a data set of 1.3M synthetic face images with detail eye openness and eye gaze information, and 21k real-world images with open/closed annotation. The dataset will be released online upon acceptance. Extensive experiments validate the effectiveness of the proposed approach.\r",
    "code_link": ""
  },
  "iccv2019_epic_learningspatiotemporalattentionforegocentricactionrecognition": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Learning Spatiotemporal Attention for Egocentric Action Recognition",
    "authors": [
      "Minlong Lu",
      "Danping Liao",
      "Ze-Nian Li"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Lu_Learning_Spatiotemporal_Attention_for_Egocentric_Action_Recognition_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Lu_Learning_Spatiotemporal_Attention_for_Egocentric_Action_Recognition_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Recognizing camera wearers' actions from videos captured by the head-mounted camera is a challenging task. Previous methods often utilize attention models to characterize the relevant spatial regions to facilitate egocentric action recognition. Inspired by the recent advances of spatiotemporal feature learning using 3D convolutions, we propose a simple yet efficient module for learning spatiotemporal attention in egocentric videos with human gaze as supervision. Our model employs a two-stream architecture which consists of an appearance-based stream and motion-based stream. Each stream has the spatiotemporal attention module (STAM) to produce an attention map, which helps our model to focus on the relevant spatiotemporal regions of the video for action recognition. The experimental results demonstrate that our model is able to outperform the state-of-the-art methods by a large margin on the standard EGTEA Gaze+ dataset and produce attention maps that are consistent with human gaze.\r",
    "code_link": "https://github.com/ymlml/STAM"
  },
  "iccv2019_epic_first-personcamerasystemtoevaluatetenderdementia-careskill": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "First-Person Camera System to Evaluate Tender Dementia-Care Skill",
    "authors": [
      "Atsushi Nakazawa",
      "Miwako Honda"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Nakazawa_First-Person_Camera_System_to_Evaluate_Tender_Dementia-Care_Skill_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Nakazawa_First-Person_Camera_System_to_Evaluate_Tender_Dementia-Care_Skill_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this paper, we describe a wearable first-person video (FPV) analysis system for evaluating the skill levels of tender dementia-care technique. Using this system, caregivers can evaluate and elevate their care levels by themselves using the systems' feedbacks. From the FPVs of care sessions taken by wearable cameras worn by caregivers, we obtained the 3D facial distance, pose and eye-contact states between caregivers and receivers by using facial landmark detection and deep neural network (DNN)-based eye contact detection. We applied statistical analysis to these features and developed algorithms that provide scores for tender-care skill. To find and confirm our idea, we conducted chronological study to observe the progression of tender care-skill learning using care learners. First, we took FPVs while care training scenes involving novice caregivers, tender-care experts and middle-level students, and found major behavioural differences among them. Second, we performed the same experiments for the participants before and after training sessions of the care. As the result, we found the same behavioural difference between 1) novices and experts and 2) novices before and after taking training sessions. These results indicate that our FPV-based behavior analysis can evaluate the skill progression of the tender dementia-care.\r",
    "code_link": ""
  },
  "iccv2019_epic_ananalysisofhowdriverexperienceaffectseye-gazebehaviorforroboticwheelchairoperation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "An Analysis of How Driver Experience Affects Eye-Gaze Behavior for Robotic Wheelchair Operation",
    "authors": [
      "Yamato Maekawa",
      "Naoki Akai",
      "Takatsugu Hirayama",
      "Luis Yoichi Morales",
      "Daisuke Deguchi",
      "Yasutomo Kawanishi",
      "Ichiro Ide",
      "Hiroshi Murase"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Maekawa_An_Analysis_of_How_Driver_Experience_Affects_Eye-Gaze_Behavior_for_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Maekawa_An_Analysis_of_How_Driver_Experience_Affects_Eye-Gaze_Behavior_for_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Drivers obtain information on surrounding environment using their eyesights. Experienced eye-gaze behavior is needed when driving at places where multiple risks exist to prepare for and avoid them. In this work, we analyze the change in eye-gaze behavior in such situations while a driver gains experience on the operation of a robotic wheelchair. Accurate distance information in the traffic environment is important to analyze the eye-gaze behavior. However, almost all previous works analyze eye-gaze behavior in a 2D environment, so they could not obtain accurate distance information. For this reason, we analyze eye-gaze behavior in 3D space. Concretely, we developed a novel eye-gaze behavior analysis platform based on a robotic wheelchair and estimated the driver's attention in 3D space. We try to analyze the eye-gaze behavior considering a useful field-of-view in 3D space based on the distance information instead of only the fixation point to investigate the objects that a driver implicitly pays attention to and from where s/he focuses on them. Results show that novice drivers pay attention to a single risk at a time. In contrast, they pay more attention to multiple risks simultaneously as they gain experience. Additionally, we discuss what features are effective to model the eye-gaze behavior based on the results.\r",
    "code_link": ""
  },
  "iccv2019_epic_assessmentofopticalsee-throughheadmounteddisplaycalibrationforinteractiveaugmentedreality": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Assessment of Optical See-Through Head Mounted Display Calibration for Interactive Augmented Reality",
    "authors": [
      "Giorgio Ballestin",
      "Manuela Chessa",
      "Fabio Solari"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Ballestin_Assessment_of_Optical_See-Through_Head_Mounted_Display_Calibration_for_Interactive_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Ballestin_Assessment_of_Optical_See-Through_Head_Mounted_Display_Calibration_for_Interactive_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Interaction in Augmented Reality environments requires the precise alignment of virtual elements added to the real scene. This can be achieved if the egocentric perception of the augmented scene is coherent in both the virtual and the real reference frames. To this aim, a proper calibration of the complete system, composed of the Augmented Reality device, the user and the environment, should be performed. Over the years, several calibration techniques have been proposed, and objectively evaluating their performance has shown to be troublesome. Since only the user can assess the hologram alignment fidelity, most researchers quantify the calibration error with subjective data from user studies. This paper describes the calibration process of an optical see-through device, based on a visual alignment method, and proposes a technique to objectively quantify the residual misalignment error.\r",
    "code_link": ""
  },
  "iccv2019_epic_epic-tentanegocentricvideodatasetforcampingtentassembly": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "EPIC-Tent: An Egocentric Video Dataset for Camping Tent Assembly",
    "authors": [
      "Youngkyoon Jang",
      "Brian Sullivan",
      "Casimir Ludwig",
      "Iain Gilchrist",
      "Dima Damen",
      "Walterio Mayol-Cuevas"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Jang_EPIC-Tent_An_Egocentric_Video_Dataset_for_Camping_Tent_Assembly_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Jang_EPIC-Tent_An_Egocentric_Video_Dataset_for_Camping_Tent_Assembly_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " This paper presents an outdoor video dataset annotated with action labels, collected from 24 participants wearing two head-mounted cameras (GoPro and SMI eye tracker) while assembling a camping tent. In total, this is 5.4 hours of recordings. Tent assembly includes manual interactions with non-rigid objects such as spreading the tent, securing guylines, reading instructions, and opening a tent bag. An interesting aspect of the dataset is that it reflects participants' proficiency in completing or understanding the task. This leads to participant differences in action sequences and action durations. Our dataset, called EPIC-Tent, also has several new types of annotations for two synchronised egocentric videos. These include task errors, self-rated uncertainty and gaze position, in addition to the task action labels. We present baseline results on the EPIC-Tent dataset using a state-of-the-art method for offline and online action recognition and detection.\r",
    "code_link": ""
  },
  "iccv2019_epic_seeingandhearingegocentricactionshowmuchcanwelearn?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "EPIC",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Egocentric Perception Interaction and Computing",
    "title": "Seeing and Hearing Egocentric Actions: How Much Can We Learn?",
    "authors": [
      "Alejandro Cartas",
      "Jordi Luque",
      "Petia Radeva",
      "Carlos Segura",
      "Mariella Dimiccoli"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/EPIC/Cartas_Seeing_and_Hearing_Egocentric_Actions_How_Much_Can_We_Learn_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/EPIC/Cartas_Seeing_and_Hearing_Egocentric_Actions_How_Much_Can_We_Learn_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Our interaction with the world is an inherently multimodal experience. However, the understanding of human-to-object interactions has historically been addressed focusing on a single modality. In particular, a limited number of works have considered to integrate the visual and audio modalities for this purpose. In this work, we propose a multimodal approach for egocentric action recognition in a kitchen environment that relies on audio and visual information. Our model combines a sparse temporal sampling strategy with a late fusion of audio, spatial, and temporal streams. Experimental results on the EPIC-Kitchens dataset show that multimodal integration leads to better performance than unimodal approaches. In particular, we achieved a 5.18% improvement over the state of the art on verb classification.\r",
    "code_link": "https://github.com/gorayni/seeing_and"
  },
  "iccv2019_cromol_harvestinginformationfromcaptionsforweaklysupervisedsemanticsegmentation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Harvesting Information from Captions for Weakly Supervised Semantic Segmentation",
    "authors": [
      "Johann Sawatzky",
      "Debayan Banerjee",
      "Juergen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Sawatzky_Harvesting_Information_from_Captions_for_Weakly_Supervised_Semantic_Segmentation_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Sawatzky_Harvesting_Information_from_Captions_for_Weakly_Supervised_Semantic_Segmentation_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Since acquiring pixel-wise annotations for training convolutional neural networks for semantic image segmentation is time-consuming, weakly supervised approaches that only require class tags have been proposed. In this work, we propose another form of supervision, namely image captions as they can be found on the Internet. These captions have two advantages. They do not require additional curation as it is the case for the clean class tags used by current weakly supervised approaches and they provide textual context for the classes present in an image. To leverage such textual context, we deploy a multi-modal network that learns a joint embedding of the visual representation of the image and the textual representation of the caption. The network estimates text activation maps (TAMs) for class names as well as compound concepts, i.e. combinations of nouns and their attributes. The TAMs of compound concepts describing classes of interest substantially improve the quality of the estimated class activation maps which are then used to train a network for semantic segmentation. We evaluate our method on the COCO dataset where it achieves state of the art results for weakly supervised image segmentation.\r",
    "code_link": ""
  },
  "iccv2019_cromol_smile,behappy)emojiembeddingforvisualsentimentanalysis": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis",
    "authors": [
      "Ziad Al-Halah",
      "Andrew Aitken",
      "Wenzhe Shi",
      "Jose Caballero"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Al-Halah_Smile_Be_Happy__Emoji_Embedding_for_Visual_Sentiment_Analysis_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Al-Halah_Smile_Be_Happy__Emoji_Embedding_for_Visual_Sentiment_Analysis_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Due to the lack of large-scale datasets, the prevailing approach in visual sentiment analysis is to leverage models trained for object classification in large datasets like ImageNet. However, objects are sentiment neutral which hinders the expected gain of transfer learning for such tasks. In this work, we propose to overcome this problem by learning a novel sentiment-aligned image embedding that is better suited for subsequent visual sentiment analysis. Our embedding leverages the intricate relation between emojis and images in large-scale and readily available data from social media. Emojis are language-agnostic, consistent, and carry a clear sentiment signal which make them an excellent proxy to learn a sentiment aligned embedding. Hence, we construct a novel dataset of 4 million images collected from Twitter with their associated emojis. We train a deep neural model for image embedding using emoji prediction task as a proxy. Our evaluation demonstrates that the proposed embedding outperforms the popular object-based counterpart consistently across several sentiment analysis benchmarks. Furthermore, without bell and whistles, our compact, effective and simple embedding outperforms the more elaborate and customized state-of-the-art deep models on these public benchmarks. Additionally, we introduce a novel emoji representation based on their visual emotional response which support a deeper understanding of the emoji modality and their usage on social media.\r",
    "code_link": ""
  },
  "iccv2019_cromol_docrossmodalsystemsleveragesemanticrelationships?": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Do Cross Modal Systems Leverage Semantic Relationships?",
    "authors": [
      "Shah Nawaz",
      "M. Kamran Janjua",
      "Ignazio Gallo",
      "Arif Mahmood",
      "Alessandro Calefati",
      "Faisal Shafait"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Nawaz_Do_Cross_Modal_Systems_Leverage_Semantic_Relationships_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Nawaz_Do_Cross_Modal_Systems_Leverage_Semantic_Relationships_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Current cross modal retrieval systems are evaluated using R@K measure which does not leverage semantic relationships rather strictly follows the manually marked image text query pairs. Therefore, current systems do not generalize well for the unseen data in the wild. To handle this, we propose a new measure SemanticMap to evaluate the performance of cross modal systems. Our proposed measure evaluates the semantic similarity between the image and text representations in the latent embedding space. We also propose a novel cross modal retrieval system using a single stream network for bidirectional retrieval. The proposed system is based on a deep neural network trained using extended center loss, minimizing the distance of image and text descriptions in the latent space from the class centers. In our system, the text descriptions are also encoded as images which enabled us to use single stream network for both text and images. To the best of our knowledge, our work is the first of its kind in terms of employing a single stream network for cross modal retrieval systems. The proposed system is evaluated on two publicly available datasets including MSCOCO and Flickr30K and has shown comparable results to the current state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_cromol_two-streamvideoclassificationwithcross-modalityattention": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Two-Stream Video Classification with Cross-Modality Attention",
    "authors": [
      "Lu Chi",
      "Guiyu Tian",
      "Yadong Mu",
      "Qi Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Chi_Two-Stream_Video_Classification_with_Cross-Modality_Attention_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Chi_Two-Stream_Video_Classification_with_Cross-Modality_Attention_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Fusing multi-modality information is known to be able to effectively bring significant improvement in video classification. However, the most popular method up to now is still simply fusing each stream's prediction scores at the last stage. A valid question is whether there exists a more effective method to fuse information cross modality. With the development of attention mechanism in natural language processing, there emerge many successful applications of attention in the field of computer vision. In this paper, we propose a cross-modality attention operation, which can obtain information from other modality in a more effective way than two-stream. Correspondingly we implement a compatible block named CMA block, which is a wrapper of our proposed attention operation. CMA can be plugged into many existing architectures. In the experiments, we comprehensively compare our method with two-stream and non-local models widely used in video classification. All experiments clearly demonstrate strong performance superiority by our proposed method. We also analyze the advantages of the CMA block by visualizing the attention map, which intuitively shows how the block helps the final prediction.\r",
    "code_link": "https://github.com/ZhaofanQiu/pseudo-3d-residual-networks"
  },
  "iccv2019_cromol_deccnetdepthenhancedcrowdcounting": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "DECCNet: Depth Enhanced Crowd Counting",
    "authors": [
      "Shuo-Diao Yang",
      "Hung-Ting Su",
      "Winston H. Hsu",
      "Wen-Chin Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Yang_DECCNet_Depth_Enhanced_Crowd_Counting_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Yang_DECCNet_Depth_Enhanced_Crowd_Counting_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Crowd counting which aims to calculate the number of total instances on an image is a classic but crucial task that supports many applications. Most of the prior works are based on the RGB channels on the images and achieve satisfied performance. However, previous approaches suffer from counting highly congested region due to the incomplete and blurry shapes. In this paper, we present an effective crowd counting method, Depth Enhanced Crowd Counting Network (DECCNet), which leverages the estimated depth information with our novel Bidirectional Cross-modal Attention (BCA) mechanism. Utilizing the depth information enables our model to explicitly learn to pay attention to those congested regions on the basis of the depth information. Our BCA mechanism interactively fuses two different input modalities by learning to focus on the informative parts according to each other. In our experiments, we demonstrate that DECCNet outperforms the state-of-the-art on the two largest crowd counting datasets available, including UCF-QNRF, which has the highest crowd density. The visualized result shows that our method can accurately regress dense regions through leveraging depth information. Ablation studies also indicate that each component of our method is beneficial to final prediction.\r",
    "code_link": ""
  },
  "iccv2019_cromol_sequentiallearningforcross-modalretrieval": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Sequential Learning for Cross-Modal Retrieval",
    "authors": [
      "Ge Song",
      "Xiaoyang Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Song_Sequential_Learning_for_Cross-Modal_Retrieval_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Song_Sequential_Learning_for_Cross-Modal_Retrieval_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Cross-modal retrieval has attracted increasing attention with the rapid growth of multimodal data, but its learning paradigm under changing environment is less studied. Inspired by the recent achievement in the field of cognition mechanism on how the human brain acquires knowledge, we propose a new sequential learning method for cross-modal retrieval. In this method, a unified model is maintained to capture the common knowledge of various modalities but are learnt in a sequential manner such that it behaves adaptively according to the evolving distribution of different modalities, and needs no laborious alignment operations among multimodal data before learning. Furthermore, we propose a novel meta-learning based method to overcome the catastrophic forgetting encountered in sequential learning. Extensive experiments are conducted on three popular multimodal datasets, showing that our method achieves state-of-the-art cross-modal retrieval performance without any modal-alignment.\r",
    "code_link": ""
  },
  "iccv2019_cromol_distancebasedtrainingforcross-modalitypersonre-identification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Distance Based Training for Cross-Modality Person Re-Identification",
    "authors": [
      "Nihat Tekeli",
      "Ahmet Burak Can"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Tekeli_Distance_Based_Training_for_Cross-Modality_Person_Re-Identification_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Tekeli_Distance_Based_Training_for_Cross-Modality_Person_Re-Identification_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Cross-modality person re-identification between infrared (IR) and visible (VIS) domains is a challenging problem, which aims to identify persons in different spectrums, variety of camera specs, and broad illumination conditions. This paper proposes distance based training on an one-stream convolutional neural network architecture, in which network weights are shared between IR and VIS domains to learn discriminative features for person re-identification. The distance based score layer enables to train the network using distance metrics instead of the fully connected layer. Different distance metrics can be used for training and ranking stages. The proposed structure enables to extract discriminative features in the cross-modality data without using dedicated structures for each domain. Experimental results on a cross-modality person re-identification dataset indicate that the proposed approach outperforms the state-of-the-art methods.\r",
    "code_link": ""
  },
  "iccv2019_cromol_aestheticimagecaptioningfromweakly-labelledphotographs": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Aesthetic Image Captioning From Weakly-Labelled Photographs",
    "authors": [
      "Koustav Ghosal",
      "Aakanksha Rana",
      "Aljosa Smolic"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Ghosal_Aesthetic_Image_Captioning_From_Weakly-Labelled_Photographs_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Ghosal_Aesthetic_Image_Captioning_From_Weakly-Labelled_Photographs_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Aesthetic image captioning (AIC) refers to the multi-modal task of generating critical textual feedbacks for photographs. While in natural image captioning (NIC), deep models are trained in an end-to-end manner using large curated datasets such as MS-COCO, no such large-scale, clean dataset exists for AIC. Towards this goal, we propose an automatic cleaning strategy to create a benchmarking AIC dataset, by exploiting the images and noisy comments easily available from photography websites. We propose a probabilistic caption-filtering method for cleaning the noisy web-data, and compile a large-scale, clean dataset 'AVA-Captions', (230, 000 images with 5 captions per image). Additionally, by exploiting the latent associations between aesthetic attributes, we propose a strategy for training the convolutional neural network (CNN) based visual feature extractor, the first component of the AIC framework. The strategy is weakly supervised and can be effectively used to learn rich aesthetic representations, without requiring expensive ground-truth annotations. We finally show-case a thorough analysis of the proposed contributions using automatic metrics and subjective evaluations.\r",
    "code_link": ""
  },
  "iccv2019_cromol_jointwassersteinautoencodersforaligningmultimodalembeddings": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings",
    "authors": [
      "Shweta Mahajan",
      "Teresa Botschen",
      "Iryna Gurevych",
      "Stefan Roth"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Mahajan_Joint_Wasserstein_Autoencoders_for_Aligning_Multimodal_Embeddings_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Mahajan_Joint_Wasserstein_Autoencoders_for_Aligning_Multimodal_Embeddings_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " One of the key challenges in learning joint embeddings of multiple modalities, e.g. of images and text, is to ensure coherent cross-modal semantics that generalize across datasets. We propose to address this through joint Gaussian regularization of the latent representations. Building on Wasserstein autoencoders (WAEs) to encode the input in each domain, we enforce the latent embeddings to be similar to a Gaussian prior that is shared across the two domains, ensuring compatible continuity of the encoded semantic representations of images and texts. Semantic alignment is achieved through supervision from matching image-text pairs. To show the benefits of our semi-supervised representation, we apply it to cross-modal retrieval and phrase localization. We not only achieve state-of-the-art accuracy, but significantly better generalization across datasets, owing to the semantic continuity of the latent space.\r",
    "code_link": ""
  },
  "iccv2019_cromol_anadversarialapproachtodiscriminativemodalitydistillationforremotesensingimageclassification": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "CROMOL",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - CroMoL: Cross-Modal Learning in Real World",
    "title": "An Adversarial Approach to Discriminative Modality Distillation for Remote Sensing Image Classification",
    "authors": [
      "Shivam Pande",
      "Avinandan Banerjee",
      "Saurabh Kumar",
      "Biplab Banerjee",
      "Subhasis Chaudhuri"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/CROMOL/Pande_An_Adversarial_Approach_to_Discriminative_Modality_Distillation_for_Remote_Sensing_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/CROMOL/Pande_An_Adversarial_Approach_to_Discriminative_Modality_Distillation_for_Remote_Sensing_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " We deal with the problem of modality distillation for the purpose of remote sensing (RS) image classification by exploring the deep generative models. From the remote sensing perspective, this problem can also be considered in line with the missing bands problem frequently encountered due to sensor abnormality. It is expected that different modalities provide useful complementary information regarding a given task, thus leading to the training of a robust prediction model. Although training data may be collected from different sensor modalities, it is many a time possible that not all the information are readily available during the model inference phase. This paper tackles the problem by proposing a novel adversarial training driven hallucination architecture which is capable of learning discriminative feature representations corresponding to the missing modalities from the available ones during the test time. To this end, we follow a teacher-student model where the teacher is trained on the multimodal data (learning with privileged information) and the student model learns to subsequently distill the feature descriptors corresponding to the missing modality. Experimental results obtained on the benchmark hyperspectral (HSI) datasets and another dataset of multispectral (MS)-panchromatic (PAN) image pairs confirm the efficacy of the proposed approach. In particular, we find that the student model is consistently able to surpass the performance of the teacher model for HSI datasets.\r",
    "code_link": ""
  },
  "iccv2019_prereg_learningrepresentationalinvarianceinsteadofcategorization": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PreReg",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Should We Pre-Register Experiments in Computer Vision?",
    "title": "Learning Representational Invariance Instead of Categorization",
    "authors": [
      "Alex Hernandez-Garcia",
      "Peter Konig"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PreReg/Hernandez-Garcia_Learning_Representational_Invariance_Instead_of_Categorization_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PreReg/Hernandez-Garcia_Learning_Representational_Invariance_Instead_of_Categorization_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " The current most accurate models of image object categorization are deep neural networks trained on large labeled data sets. Minimizing a classification loss between the predictions of the network and the true labels has proven an effective way to learn discriminative functions of the object classes. However, recent studies have suggested that such models learn highly discriminative features that are not aligned with visual perception and might be at the root of adversarial vulnerability. Here, we propose to replace the classification loss with the joint optimization of invariance to identity-preserving transformations of images (data augmentation invariance), and the invariance to objects of the same category (class-wise invariance). We hypothesize that optimizing these invariance objectives might yield features more aligned with visual perception, more robust to adversarial perturbations, while still suitable for accurate object categorization.\r",
    "code_link": ""
  },
  "iccv2019_prereg_learningtoinpaintbyprogressivelygrowingthemaskregions": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PreReg",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Should We Pre-Register Experiments in Computer Vision?",
    "title": "Learning to Inpaint by Progressively Growing the Mask Regions",
    "authors": [
      "Mohamed Abbas Hedjazi",
      "Yakup Genc"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PreReg/Hedjazi_Learning_to_Inpaint_by_Progressively_Growing_the_Mask_Regions_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PreReg/Hedjazi_Learning_to_Inpaint_by_Progressively_Growing_the_Mask_Regions_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties to generate the correct structures and colors as the masked region grows large. This drawback is due to the training stability issue of the generative models. This work introduces a new curriculum-style training approach in the context of image inpainting. The proposed method increases the masked region size progressively in training time, during test time the user gives variable size and multiple holes at arbitrary locations. Incorporating such an approach in GANs may stabilize the training and provides better color consistencies and captures object continuities. We validate our approach on the MSCOCO and CelebA datasets. We report qualitative and quantitative comparisons of our training approach in different models.\r",
    "code_link": ""
  },
  "iccv2019_prereg_anempiricalstudyoftherelationbetweennetworkarchitectureandcomplexity": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PreReg",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Should We Pre-Register Experiments in Computer Vision?",
    "title": "An Empirical Study of the Relation Between Network Architecture and Complexity",
    "authors": [
      "Emir Konuk",
      "Kevin Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PreReg/Konuk_An_Empirical_Study_of_the_Relation_Between_Network_Architecture_and_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PreReg/Konuk_An_Empirical_Study_of_the_Relation_Between_Network_Architecture_and_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this preregistration submission, we propose an empirical study of how networks handle changes in complexity of the data. We investigate the effect of network capacity on generalization performance in the face of increasing data complexity. For this, we measure the generalization error for an image classification task where the number of classes steadily increases. We compare a number of modern architectures at different scales in this setting. The methodology, setup, and hypotheses described in this proposal were evaluated by peer review before experiments were conducted.\r",
    "code_link": ""
  },
  "iccv2019_prereg_extendingconvolutionalposemachinesforfaciallandmarklocalizationin3dpointclouds": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PreReg",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Should We Pre-Register Experiments in Computer Vision?",
    "title": "Extending Convolutional Pose Machines for Facial Landmark Localization in 3D Point Clouds",
    "authors": [
      "Eimear O' Sullivan"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PreReg/Sullivan_Extending_Convolutional_Pose_Machines_for_Facial_Landmark_Localization_in_3D_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PreReg/Sullivan_Extending_Convolutional_Pose_Machines_for_Facial_Landmark_Localization_in_3D_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " In this work we address the problem of landmark localization in 3D point clouds by extending the convolutional pose machine (CPM) architecture to facilitate landmark localization in 3D point clouds. Making use of PointNet++,we are able to construct an architecture that is invariant tothe ordering of an input point cloud. The sequential CPM architecture facilitates allows initial heatmaps to be iteratively refined in a series of point convolutional stages to yield robust landmark predictions. We propose to evaluate our approach for 3D facial landmark localization on benchmark face databases, BU-3DFE, BP4D-Spontaneous and BP4D+. The robustness of the approach to the size of the input point cloud will be assessed, and the contribution of the CPM stages will be evaluated in an ablation study.\r",
    "code_link": ""
  },
  "iccv2019_prereg_towardsgeneralizabledistanceestimationbyleveraginggraphinformation": {
    "conf_id": "ICCV2019",
    "conf_sub_id": "PreReg",
    "is_workshop": true,
    "conf_name": "ICCV2019_workshops - Should We Pre-Register Experiments in Computer Vision?",
    "title": "Towards Generalizable Distance Estimation By Leveraging Graph Information",
    "authors": [
      "John Kevin Cava",
      "Todd Houghton",
      "Hongbin Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/html/PreReg/Cava_Towards_Generalizable_Distance_Estimation_By_Leveraging_Graph_Information_ICCVW_2019_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/ICCV2019_workshops/../content_ICCVW_2019/papers/PreReg/Cava_Towards_Generalizable_Distance_Estimation_By_Leveraging_Graph_Information_ICCVW_2019_paper.pdf",
    "published": "2019-10",
    "summary": " Approximating the distance of objects present in an image remains an important problem for computer vision applications. Current SOTA methods rely on formulating this problem to convenience depth estimation at every pixel; however, there are limitations that make such solutions non-generalizable (i.e varying focal length). To address this issue, we propose reformulating distance approximation to a per-object detection problem and leveraging graph information extracted from the image to potentially achieve better generalizability on data acquired at multiple focal lengths.\r",
    "code_link": ""
  }
}