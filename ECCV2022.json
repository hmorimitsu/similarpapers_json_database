{
  "eccv2022_main_learningdepthfromfocusinthewild": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Depth from Focus in the Wild",
    "authors": [
      "Changyeon Won",
      "Hae-Gon Jeon"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/19_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610001.pdf",
    "published": "2020-08",
    "summary": "\"For better photography, most recent commercial cameras including smartphones have either adopted large-aperture lens to collect more light or used a burst mode to take multiple images within short times. These interesting features lead us to examine depth from focus/defocus. In this work, we present a convolutional neural network-based depth estimation from single focal stacks. Our method differs from relevant state-of-the-art works with three unique features. First, our method allows depth maps to be inferred in an end-to-end manner even with image alignment. Second, we propose a sharp region detection module to reduce blur ambiguities in subtle focus changes and weakly texture-less regions. Third, we design an effective downsampling module to ease flows of focal information in feature extractions. In addition, for the generalization of the proposed network, we develop a simulator to realistically reproduce the features of commercial cameras, such as changes in field of view, focal length and principal points. By effectively incorporating these three unique features, our network achieves the top rank in the DDFF 12-Scene benchmark on most metrics. We also demonstrate the effectiveness of the proposed method on various quantitative evaluations and real-world images taken from various off-the-shelf cameras compared with state-of-the-art methods. Our source code is publicly available at https://github.com/wcy199705/DfFintheWild.\""
  },
  "eccv2022_main_learning-basedpointcloudregistrationfor6dobjectposeestimationintherealworld": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World",
    "authors": [
      "Zheng Dang",
      "Lizhou Wang",
      "Yu Guo",
      "Mathieu Salzmann"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/69_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610018.pdf",
    "published": "2020-08",
    "summary": "\"In this work, we tackle the task of estimating the 6D pose of an object from point cloud data. While recent learning-based approaches to addressing this task have shown great success on synthetic datasets, we have observed them to fail in the presence of real-world data. We thus analyze the causes of these failures, which we trace back to the difference between the feature distributions of the source and target point clouds, and the sensitivity of the widely-used SVD-based loss function to the range of rotation between the two point clouds. We address the first challenge by introducing a new normalization strategy, Match Normalization, and the second via the use of a loss function based on the negative log likelihood of point correspondences. Our two contributions are general and can be applied to many existing learning-based 3D object registration frameworks, which we illustrate by implementing them in two of them, DCP and IDAM. Our experiments on the real-scene TUD-L, LINEMOD and Occluded-LINEMOD datasets evidence the benefits of our strategies. They allow for the first time learning-based 3D object registration methods to achieve meaningful results on real-world data. We therefore expect them to be key to the future development of point cloud registration methods.\""
  },
  "eccv2022_main_anend-to-endtransformermodelforcrowdlocalization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "An End-to-End Transformer Model for Crowd Localization",
    "authors": [
      "Dingkang Liang",
      "Wei Xu",
      "Xiang Bai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/127_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610037.pdf",
    "published": "2020-08",
    "summary": "\"Crowd localization, predicting head positions, is a more practical and high-level task than simply counting. Existing methods employ pseudo-bounding boxes or pre-designed localization maps, relying on complex post-processing to obtain the head positions. In this paper, we propose an elegant, end-to-end Crowd Localization TRansformer named CLTR that solves the task in the regression-based paradigm. The proposed method views the crowd localization as a direct set prediction problem, taking extracted features and trainable embeddings as input of the transformer-decoder. To reduce the ambiguous points and generate more reasonable matching results, we introduce a KMO-based Hungarian matcher, which adopts the nearby context as the external matching cost. Extensive experiments conducted on five datasets in various data settings show the effectiveness of our method. In particular, the proposed method achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech Part A datasets.\""
  },
  "eccv2022_main_few-shotsingle-view3dreconstructionwithmemorypriorcontrastivenetwork": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Few-Shot Single-View 3D Reconstruction with Memory Prior Contrastive Network",
    "authors": [
      "Zhen Xing",
      "Yijiang Chen",
      "Zhixin Ling",
      "Xiangdong Zhou",
      "Yu Xiang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/192_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610054.pdf",
    "published": "2020-08",
    "summary": "\"3D reconstruction of novel categories based on few-shot learning is appealing in real-world applications and attracts increasing research interests. Previous approaches mainly focus on how to design shape prior models for different categories. Their performance on unseen categories is not very competitive. In this paper, we present a Memory Prior Contrastive Network (MPCN) that can store shape prior knowledge in a few-shot learning based 3D reconstruction framework. With the shape memory, a multi-head attention module is proposed to capture different parts of a candidate shape prior and fuse these parts together to guide 3D reconstruction of novel categories. Besides, we introduce a 3D-aware contrastive learning method, which can not only complement the retrieval accuracy of memory network, but also better organize image features for downstream tasks. Compared with previous few-shot 3D reconstruction methods, MPCN can handle the inter-class variability without category annotations. Experimental results on a benchmark synthetic dataset and the Pascal3D+ real-world dataset show that our model outperforms the current state-of-the-art methods significantly.\""
  },
  "eccv2022_main_did-m3ddecouplinginstancedepthformonocular3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection",
    "authors": [
      "Liang Peng",
      "Xiaopei Wu",
      "Zheng Yang",
      "Haifeng Liu",
      "Deng Cai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/343_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610071.pdf",
    "published": "2020-08",
    "summary": "\"Monocular 3D detection has drawn much attention from the community due to its low cost and setup simplicity. It takes an RGB image as input and predicts 3D boxes in the 3D space. The most challenging sub-task lies in the instance depth estimation. Previous works usually use a direct estimation method. However, in this paper we point out that the instance depth on the RGB image is non-intuitive. It is coupled by visual depth clues and instance attribute clues, making it hard to be directly learned in the network. Therefore, we propose to reformulate the instance depth to the combination of the instance visual surface depth (visual depth) and the instance attribute depth (attribute depth). The visual depth is related to objects\u2019 appearances and positions on the image. By contrast, the attribute depth relies on objects\u2019 inherent attributes, which are invariant to the object affine transformation on the image. Correspondingly, we decouple the 3D location uncertainty into visual depth uncertainty and attribute depth uncertainty. By combining different types of depths and associated uncertainties, we can obtain the final instance depth. Furthermore, data augmentation in monocular 3D detection is usually limited due to the physical nature, hindering the boost of performance. Based on the proposed instance depth disentanglement strategy, we can alleviate this problem. Evaluated on KITTI, our method achieves new state-of-the-art results, and extensive ablation studies validate the effectiveness of each component in our method. The codes are released at https://github.com/SPengLiang/DID-M3D.\""
  },
  "eccv2022_main_adaptiveco-teachingforunsupervisedmonoculardepthestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Adaptive Co-Teaching for Unsupervised Monocular Depth Estimation",
    "authors": [
      "Weisong Ren",
      "Lijun Wang",
      "Yongri Piao",
      "Miao Zhang",
      "Huchuan Lu",
      "Ting Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/405_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610089.pdf",
    "published": "2020-08",
    "summary": "\"Unsupervised depth estimation using photometric losses suffers from local minimum and training instability. We address this issue by proposing an adaptive co-teaching framework to distill the learned knowledge from unsupervised teacher networks to a student network. We design an ensemble architecture for our teacher networks, integrating a depth basis decoder with multiple depth coefficient decoders. Depth prediction can then be formulated as a combination of the predicted depth bases weighted by coefficients. By further constraining their correlations, multiple coefficient decoders can yield a diversity of depth predictions, serving as the ensemble teachers. During the co-teaching step, our method allows different supervision sources from not only ensemble teachers but also photometric losses to constantly compete with each other, and adaptively select the optimal ones to teach the student, which effectively improves the ability of the student to jump out of the local minimum. Our method is shown to significantly benefit unsupervised depth estimation and sets new state of the art on both KITTI and Nuscenes datasets.\""
  },
  "eccv2022_main_fusinglocalsimilaritiesforretrieval-based3dorientationestimationofunseenobjects": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Fusing Local Similarities for Retrieval-Based 3D Orientation Estimation of Unseen Objects",
    "authors": [
      "Chen Zhao",
      "Yinlin Hu",
      "Mathieu Salzmann"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/444_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610106.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we tackle the task of estimating the 3D orientation of previously-unseen objects from monocular images. This task contrasts with the one considered by most existing deep learning methods which typically assume that the testing objects have been observed during training. To handle the unseen objects, we follow a retrieval-based strategy and prevent the network from learning object-specific features by computing multi-scale local similarities between the query image and synthetically-generated reference images. We then introduce an adaptive fusion module that robustly aggregates the local similarities into a global similarity score of pairwise images. Furthermore, we speed up the retrieval process by developing a fast retrieval strategy. Our experiments on the LineMOD, LineMOD-Occluded, and T-LESS datasets show that our method yields a significantly better generalization to unseen objects than previous works. Our code and pre-trained models are available at https://sailor-z.github.io/projects/Unseen_Object_Pose.html.\""
  },
  "eccv2022_main_lidarpointcloudguidedmonocular3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Lidar Point Cloud Guided Monocular 3D Object Detection",
    "authors": [
      "Liang Peng",
      "Fei Liu",
      "Zhengxu Yu",
      "Senbo Yan",
      "Dan Deng",
      "Zheng Yang",
      "Haifeng Liu",
      "Deng Cai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/655_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610123.pdf",
    "published": "2020-08",
    "summary": "\"Monocular 3D object detection is a challenging task in the self-driving and computer vision community. As a common practice, most previous works use manually annotated 3D box labels, where the annotating process is expensive. In this paper, we find that the precisely and carefully annotated labels may be unnecessary in monocular 3D detection, which is an interesting and counterintuitive finding. Using rough labels that are randomly disturbed, the detector can achieve very close accuracy compared to the one using the ground-truth labels. We delve into this underlying mechanism and then empirically find that: concerning the label accuracy, the 3D location part in the label is preferred compared to other parts of labels. Motivated by the conclusions above and considering the precise LiDAR 3D measurement, we propose a simple and effective framework, dubbed LiDAR point cloud guided monocular 3D object detection (LPCG). This framework is capable of either reducing the annotation costs or considerably boosting the detection accuracy without introducing extra annotation costs. Specifically, It generates pseudo labels from unlabeled LiDAR point clouds. Thanks to accurate LiDAR 3D measurements in 3D space, such pseudo labels can replace manually annotated labels in the training of monocular 3D detectors, since their 3D location information is precise. LPCG can be applied into any monocular 3D detector to fully use massive unlabeled data in a self-driving system. As a result, in KITTI benchmark, we take the first place on both monocular 3D and BEV (bird\u2019s-eye-view) detection with a significant margin. In Waymo benchmark, our method using 10% labeled data achieves comparable accuracy to the baseline detector using 100% labeled data. The codes are released at https://github.com/SPengLiang/LPCG.\""
  },
  "eccv2022_main_structuralcausal3dreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Structural Causal 3D Reconstruction",
    "authors": [
      "Weiyang Liu",
      "Zhen Liu",
      "Liam Paull",
      "Adrian Weller",
      "Bernhard Sch\u00f6lkopf"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/656_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610140.pdf",
    "published": "2020-08",
    "summary": "\"This paper considers the problem of unsupervised 3D object reconstruction from in-the-wild single-view images. Due to ambiguity and intrinsic ill-posedness, this problem is inherently difficult to solve and therefore requires strong regularization to achieve disentanglement of different latent factors. Unlike existing works that introduce explicit regularizations into objective functions, we look into a different space for implicit regularization -- the structure of latent space. Specifically, we restrict the structure of latent space to capture a topological causal ordering of latent factors (i.e., representing causal dependency as a directed acyclic graph). We first show that different causal orderings matter for 3D reconstruction, and then explore several approaches to find a task-dependent causal factor ordering. Our experiments demonstrate that the latent space structure indeed serves as an implicit regularization and introduces an inductive bias beneficial for reconstruction.\""
  },
  "eccv2022_main_3dhumanposeestimationusingm\u00f6biusgraphconvolutionalnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Human Pose Estimation Using M\u00f6bius Graph Convolutional Networks",
    "authors": [
      "Niloofar Azizi",
      "Horst Possegger",
      "Emanuele Rodol\u00e0",
      "Horst Bischof"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1049_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610158.pdf",
    "published": "2020-08",
    "summary": "\"3D human pose estimation is fundamental to understanding human behavior. Recently, promising results have been achieved by graph convolutional networks(GCNs), which achieve state-of-the-art performance and provide rather light-weight architectures. However, a major limitation of GCNs is their inability to encode all the transformations between joints explicitly. To address this issue, we propose a novel spectral GCN using the M\u00f6bius transformation (M\u00f6biusGCN). In particular, this allows us to directly and explicitly encode the transformation between joints, resulting in a significantly more compact representation. Compared to even the lightest architectures so far, our novel approach requires 90-98% fewer parameters, i.e. our lightest M\u00f6biusGCN uses only 0.042M trainable parameters. Besides the drastic parameter reduction, explicitly encoding the transformation of joints also enables us to achieve state-of-the-art results. We evaluate our approach on the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP, demonstrating both state-of-the-art results and the generalization capabilities of M\u00f6biusGCN.\""
  },
  "eccv2022_main_learningtotrainapointcloudreconstructionnetworkwithoutmatching": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning to Train a Point Cloud Reconstruction Network without Matching",
    "authors": [
      "Tianxin Huang",
      "Xuemeng Yang",
      "Jiangning Zhang",
      "Jinhao Cui",
      "Hao Zou",
      "Jun Chen",
      "Xiangrui Zhao",
      "Yong Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1235_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610177.pdf",
    "published": "2020-08",
    "summary": "\"Reconstruction networks for well-ordered data such as 2D images and 1D continuous signals are easy to optimize through element-wised squared errors, while permutation-arbitrary point clouds cannot be constrained directly because their points permutations are not fixed. Though existing works design algorithms to match two point clouds and evaluate shape errors based on matched results, they are limited by pre-defined matching processes. In this work, we propose a novel framework named PCLossNet which learns to train a point cloud reconstruction network without any matching. By training through an adversarial process together with the reconstruction network, PCLossNet can better explore the differences between point clouds and create more precise reconstruction results. Experiments on multiple datasets prove the superiority of our method, where PCLossNet can help networks achieve much lower reconstruction errors and extract more representative features, with about 4 times faster training efficiency than the commonly-used EMD loss. Our codes can be found in https://github.com/Tianxinhuang/PCLossNet.\""
  },
  "eccv2022_main_panoformerpanoramatransformerforindoor360\u00b0depthestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PanoFormer: Panorama Transformer for Indoor 360\u00b0 Depth Estimation",
    "authors": [
      "Zhijie Shen",
      "Chunyu Lin",
      "Kang Liao",
      "Lang Nie",
      "Zishuo Zheng",
      "Yao Zhao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1300_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610193.pdf",
    "published": "2020-08",
    "summary": "\"Existing panoramic depth estimation methods based on convolutional neural networks (CNNs) focus on removing panoramic distortions, failing to perceive panoramic structures efficiently due to the fixed receptive field in CNNs. This paper proposes the panorama Transformer (named PanoFormer) to estimate the depth in panorama images, with tangent patches from spherical domain, learnable token flows, and panorama specific metrics. In particular, we divide patches on the spherical tangent domain into tokens to reduce the negative effect of panoramic distortions. Since the geometric structures are essential for depth estimation, a self-attention module is redesigned with an additional learnable token flow. In addition, considering the characteristic of the spherical domain, we present two panorama-specific metrics to comprehensively evaluate the panoramic depth estimation models\u2019 performance. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art methods. At last, the proposed method can be effectively extended to solve semantic panorama segmentation, a similar pixel2pixel task. Code will be released upon acceptance.\""
  },
  "eccv2022_main_self-supervisedhumanmeshrecoverywithcross-representationalignment": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Self-supervised Human Mesh Recovery with Cross-Representation Alignment",
    "authors": [
      "Xuan Gong",
      "Meng Zheng",
      "Benjamin Planche",
      "Srikrishna Karanam",
      "Terrence Chen",
      "David Doermann",
      "Ziyan Wu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1534_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610210.pdf",
    "published": "2020-08",
    "summary": "\"Fully supervised human mesh recovery methods are data-hungry and have poor generalizability due to the limited availability and diversity of 3D-annotated benchmark datasets. Recent progress in self-supervised human mesh recovery has been made using synthetic-data-driven training paradigms where the model is trained from synthetic paired 2D representation (e.g., 2D keypoints and segmentation masks) and 3D mesh. However, on synthetic dense correspondence maps (i.e., IUV) few have been explored since the domain gap between synthetic training data and real testing data is hard to address for 2D dense representation. To alleviate this domain gap on IUV, we propose cross-representation alignment utilizing the complementary information from the robust but sparse representation (2D keypoints). Specifically, the alignment errors between initial mesh estimation and both 2D representations are forwarded into regressor and dynamically corrected in the following mesh regression. This adaptive cross-representation alignment explicitly learns from the deviations and captures complementary information: robustness from sparse representation and richness from dense representation. We conduct extensive experiments on multiple standard benchmark datasets and demonstrate competitive results, helping take a step towards reducing the annotation effort needed to produce state-of-the-art models in human mesh estimation.\""
  },
  "eccv2022_main_alignsdfpose-alignedsigneddistancefieldsforhand-objectreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction",
    "authors": [
      "Zerui Chen",
      "Yana Hasson",
      "Cordelia Schmid",
      "Ivan Laptev"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1549_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610229.pdf",
    "published": "2020-08",
    "summary": "\"Recent work achieved impressive progress towards joint reconstruction of hands and manipulated objects from monocular color images. Existing methods focus on two alternative representations in terms of either parametric meshes or signed distance fields (SDFs). On one side, parametric models can benefit from prior knowledge at the cost of limited shape deformations and mesh resolutions. Mesh models, hence, may fail to precisely reconstruct details such as contact surfaces of hands and objects. SDF-based methods, on the other side, can represent arbitrary details but are lacking explicit priors. In this work we aim to improve SDF models using priors provided by parametric representations. In particular, we propose a joint learning framework that disentangles the pose and the shape. We obtain hand and object poses from parametric models and use them to align SDFs in 3D space. We show that such aligned SDFs better focus on reconstructing shape details and improve reconstruction accuracy both for hands and objects. We evaluate our method and demonstrate significant improvements over the state of the art on the challenging ObMan and DexYCB benchmarks.\""
  },
  "eccv2022_main_areliableonlinemethodforjointestimationoffocallengthandcamerarotation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Reliable Online Method for Joint Estimation of Focal Length and Camera Rotation",
    "authors": [
      "Yiming Qian",
      "James H. Elder"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1737_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610247.pdf",
    "published": "2020-08",
    "summary": "\"Linear perspective cues deriving from regularities of the built environment can be used to recalibrate both intrinsic and extrinsic camera parameters online, but these estimates can be unreliable due to irregularities in the scene, uncertainties in line segment estimation and background clutter. Here we address this challenge through four initiatives. First, we use the PanoContext panoramic image dataset to curate a novel and realistic dataset of planar projections over a broad range of scenes, focal lengths and camera poses. Second, we use this novel dataset and the YorkUrbanDB to systematically evaluate the linear perspective deviation measures frequently found in the literature and show that the choice of deviation measure and likelihood model has a huge impact on reliability. Third, we use these findings to create a novel system for online camera calibration we call fR, and show that it outperforms the prior state of the art, substantially reducing error in estimated camera rotation and focal length. Our fourth contribution is a novel and efficient approach to estimating uncertainty that can dramatically improve online reliability for performance-critical applications by strategically selecting which frames to use for recalibration.\""
  },
  "eccv2022_main_ps-nerfneuralinverserenderingformulti-viewphotometricstereo": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PS-NeRF: Neural Inverse Rendering for Multi-View Photometric Stereo",
    "authors": [
      "Wenqi Yang",
      "Guanying Chen",
      "Chaofeng Chen",
      "Zhenfang Chen",
      "Kwan-Yee K. Wong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1832_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610263.pdf",
    "published": "2020-08",
    "summary": "\"Traditional multi-view photometric stereo (MVPS) methods are often composed of multiple disjoint stages, resulting in noticeable accumulated errors. In this paper, we present a neural inverse rendering method for MVPS based on implicit representation. Given multi-view images of a non-Lambertian object illuminated by multiple unknown directional lights, our method jointly estimates the geometry, materials, and lights. Our method first employs multi-light images to estimate per-view surface normal maps, which are used to regularize the normals derived from the neural radiance field. It then jointly optimizes the surface normals, spatially-varying BRDFs, and lights based on a shadow-aware differentiable rendering layer. After optimization, the reconstructed object can be used for novel-view rendering, relighting, and material editing. Experiments on both synthetic and real datasets demonstrate that our method achieves far more accurate shape reconstruction than existing MVPS and neural rendering methods. Our code and model can be found at https://ywq.github.io/psnerf.\""
  },
  "eccv2022_main_sharewiththyneighborssingle-viewreconstructionbycross-instanceconsistency": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Share with Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency",
    "authors": [
      "Tom Monnier",
      "Matthew Fisher",
      "Alexei A. Efros",
      "Mathieu Aubry"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1851_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610282.pdf",
    "published": "2020-08",
    "summary": "\"Approaches for single-view reconstruction typically rely on viewpoint annotations, silhouettes, the absence of background, multiple views of the same instance, a template shape, or symmetry. We avoid all such supervision and assumptions by explicitly leveraging the consistency between images of different object instances. As a result, our method can learn from large collections of unlabelled images depicting the same object category. Our main contributions are two ways for leveraging cross-instance consistency: (i) progressive conditioning, a training strategy to gradually specialize the model from category to instances in a curriculum learning fashion; and (ii) neighbor reconstruction, a loss enforcing consistency between instances having similar shape or texture. Also critical to the success of our method are: our structured autoencoding architecture decomposing an image into explicit shape, texture, pose, and background; an adapted formulation of differential rendering; and a new optimization scheme alternating between 3D and pose learning. We compare our approach, UNICORN, both on the diverse synthetic ShapeNet dataset - the classical benchmark for methods requiring multiple views as supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB) for which most methods require known templates and silhouette annotations. We also showcase applicability to more challenging real-world collections (CompCars, LSUN), where silhouettes are not available and images are not cropped around the object.\""
  },
  "eccv2022_main_towardscomprehensiverepresentationenhancementinsemantics-guidedself-supervisedmonoculardepthestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards Comprehensive Representation Enhancement in Semantics-Guided Self-Supervised Monocular Depth Estimation",
    "authors": [
      "Jingyuan Ma",
      "Xiangyu Lei",
      "Nan Liu",
      "Xian Zhao",
      "Shiliang Pu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1925_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610299.pdf",
    "published": "2020-08",
    "summary": "\"Semantics-guided self-supervised monocular depth estimation has been widely researched, owing to the strong cross-task correlation of depth and semantics. However, since depth estimation and semantic segmentation are fundamentally two types of tasks: one is regression while the other is classification, the distribution of depth feature and semantic feature are naturally different. Previous works that leverage semantic information in depth estimation mostly neglect such representational discrimination, which leads to insufficient representation enhancement of depth feature. In this work, we propose an attention-based module to enhance task-specific feature by addressing their feature uniqueness within instances. Additionally, we propose a metric learning based approach to accomplish comprehensive enhancement on depth feature by creating a separation between instances in feature space. Extensive experiments and analysis demonstrate the effectiveness of our proposed method. In the end, our method achieves the state-of-the-art performance on KITTI dataset.\""
  },
  "eccv2022_main_avatarcapanimatableavatarconditionedmonocularhumanvolumetriccapture": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture",
    "authors": [
      "Zhe Li",
      "Zerong Zheng",
      "Hongwen Zhang",
      "Chaonan Ji",
      "Yebin Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2057_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610317.pdf",
    "published": "2020-08",
    "summary": "\"To address the ill-posed problem caused by partial observations in monocular human volumetric capture, we present AvatarCap, a novel framework that introduces animatable avatars into the capture pipeline for high-fidelity reconstruction in both visible and invisible regions. Our method firstly creates an animatable avatar for the subject from a small number ( 20) of 3D scans as a prior. Then given a monocular RGB video of this subject, our method integrates information from both the image observation and the avatar prior, and accordingly reconstructs high-fidelity 3D textured models with dynamic details regardless of the visibility. To learn an effective avatar for volumetric capture from only few samples, we propose GeoTexAvatar, which leverages both geometry and texture supervisions to constrain the pose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned volumetric capture method that involves a canonical normal fusion and a reconstruction network is further proposed to integrate both image observations and avatar dynamics for high-fidelity reconstruction in both observed and invisible regions. Overall, our method enables monocular human volumetric capture with detailed and pose-dependent dynamics, and the experiments show that our method outperforms state of the art.\""
  },
  "eccv2022_main_cross-attentionofdisentangledmodalitiesfor3dhumanmeshrecoverywithtransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers",
    "authors": [
      "Junhyeong Cho",
      "Kim Youwang",
      "Tae-Hyun Oh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2116_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610336.pdf",
    "published": "2020-08",
    "summary": "\"Transformer encoder architectures have recently achieved state-of-the-art results on monocular 3D human mesh reconstruction, but they require a substantial number of parameters and expensive computations. Due to the large memory overhead and slow inference speed, it is difficult to deploy such models for practical use. In this paper, we propose a novel transformer encoder-decoder architecture for 3D human mesh reconstruction from a single image, called FastMETRO. We identify the performance bottleneck in the encoder-based transformers is caused by the token design which introduces high complexity interactions among input tokens. We disentangle the interactions via an encoder-decoder architecture, which allows our model to demand much fewer parameters and shorter inference time. In addition, we impose the prior knowledge of human body\u2019s morphological relationship via attention masking and mesh upsampling operations, which leads to faster convergence with higher accuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency, and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore, we validate its generalizability on FreiHAND.\""
  },
  "eccv2022_main_georefineself-supervisedonlinedepthrefinementforaccuratedensemapping": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "GeoRefine: Self-Supervised Online Depth Refinement for Accurate Dense Mapping",
    "authors": [
      "Pan Ji",
      "Qingan Yan",
      "Yuxin Ma",
      "Yi Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2124_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610354.pdf",
    "published": "2020-08",
    "summary": "\"We present a robust and accurate depth refinement system, named GeoRefine, for geometrically-consistent dense mapping from monocular sequences. GeoRefine consists of three modules: a hybrid SLAM module using learning-based priors, an online depth refinement module leveraging self-supervision, and a global mapping module via TSDF fusion. The proposed system is online by design and achieves great robustness and accuracy via: (i) a robustified hybrid SLAM that incorporates learning-based optical flow and/or depth; (ii) self-supervised losses that leverage SLAM outputs and enforce long-term geometric consistency; (iii) careful system design that avoids degenerate cases in online depth refinement. We extensively evaluate GeoRefine on multiple public datasets and reach as low as 5% absolute relative depth errors.\""
  },
  "eccv2022_main_multi-modalmaskedpre-trainingformonocularpanoramicdepthcompletion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Multi-modal Masked Pre-training for Monocular Panoramic Depth Completion",
    "authors": [
      "Zhiqiang Yan",
      "Xiang Li",
      "Kun Wang",
      "Zhenyu Zhang",
      "Jun Li",
      "Jian Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2269_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610372.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we formulate a potentially valuable panoramic depth completion (PDC) task as panoramic 3D cameras often produce 360\u00c2\u00b0 depth with missing data in complex scenes. Its goal is to recover dense panoramic depths from raw sparse ones and panoramic RGB images. To deal with the PDC task, we train a deep network that takes both depth and image as inputs for the dense panoramic depth recovery. However, it needs to face a challenging optimization problem of the network parameters due to its non-convex objective function. To address this problem, we propose a simple yet effective approach termed M\u00c2\u00b3PT: multi-modal masked pre-training. Specifically, during pre-training, we simultaneously cover up patches of the panoramic RGB image and sparse depth by shared random mask, then reconstruct the sparse depth in the masked regions. To our best knowledge, it is the first time that we show the effectiveness of masked pre-training in a multi-modal vision task, instead of the single-modal task resolved by masked autoencoders (MAE). Different from MAE where fine-tuning completely discards the decoder part of pre-training, there is no architectural difference between the pre-training and fine-tuning stages in our M\u00c2\u00b3PT as they only differ in the prediction density, which potentially makes the transfer learning more convenient and effective. Extensive experiments verify the effectiveness of M\u00c2\u00b3PT on three panoramic datasets. Notably, we improve the state-of-the-art baselines by averagely 29.2% in RMSE, 51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\""
  },
  "eccv2022_main_gitnetgeometricprior-basedtransformationforbirds-eye-viewsegmentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "GitNet: Geometric Prior-Based Transformation for Birds-Eye-View Segmentation",
    "authors": [
      "Shi Gong",
      "Xiaoqing Ye",
      "Xiao Tan",
      "Jingdong Wang",
      "Errui Ding",
      "Yu Zhou",
      "Xiang Bai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2449_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610390.pdf",
    "published": "2020-08",
    "summary": "\"Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving for its powerful spatial representation ability. It is challenging to estimate the BEV semantic maps from monocular images due to the spatial gap, since it is implicitly required to realize both the perspective-to-BEV transformation and segmentation. We present a novel two-stage Geometry PrIor-based Transformation framework named GitNet, consisting of (i) the geometry-guided pre-alignment and (ii) ray-based transformer. In the first stage, we decouple the BEV segmentation into the perspective image segmentation and geometric prior-based mapping, with explicit supervision by projecting the BEV semantic labels onto the image plane to learn visibility-aware features and learnable geometry to translate into BEV space. Second, the pre-aligned coarse BEV features are further deformed by ray-based transformers to take visibility knowledge into account. GitNet achieves the leading performance on the challenging nuScenes and Argoverse Datasets. The code will be publicly available.\""
  },
  "eccv2022_main_learningvisibilityforrobustdensehumanbodyestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Visibility for Robust Dense Human Body Estimation",
    "authors": [
      "Chun-Han Yao",
      "Jimei Yang",
      "Duygu Ceylan",
      "Yi Zhou",
      "Yang Zhou",
      "Ming-Hsuan Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2568_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610406.pdf",
    "published": "2020-08",
    "summary": "\"Estimating 3D human pose and shape from 2D images is a crucial yet challenging task. While prior methods with model-based representations can perform reasonably well on whole-body images, they often fail when parts of the body are occluded or outside the frame. Moreover, these results usually do not faithfully capture the human silhouettes due to their limited representation power of deformable models (e.g., representing only the naked body). An alternative approach is to estimate dense vertices of a predefined template body in the image space. Such representations are effective in localizing vertices within an image but cannot handle out-of-frame body parts. In this work, we learn dense human body estimation that is robust to partial observations. We explicitly model the visibility of human joints and vertices in the x, y, and z axes separately. The visibility in x and y axes help distinguishing out-of-frame cases, and the visibility in depth axis corresponds to occlusions (either self-occlusions or occlusions by other objects). We obtain pseudo ground-truths of visibility labels from dense UV correspondences and train a neural network to predict visibility along with 3D coordinates. We show that visibility can serve as 1) an additional signal to resolve depth ordering ambiguities of self-occluded vertices and 2) a regularization term when fitting a human body model to the predictions. Extensive experiments on multiple 3D human datasets demonstrate that visibility modeling significantly improves the accuracy of human body estimation, especially for partial-body cases. Our project page with code is at: https://github.com/chhankyao/visdb.\""
  },
  "eccv2022_main_towardshigh-fidelitysingle-viewholisticreconstructionofindoorscenes": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards High-Fidelity Single-View Holistic Reconstruction of Indoor Scenes",
    "authors": [
      "Haolin Liu",
      "Yujian Zheng",
      "Guanying Chen",
      "Shuguang Cui",
      "Xiaoguang Han"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2747_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610423.pdf",
    "published": "2020-08",
    "summary": "\"We present a new framework to reconstruct holistic 3D indoor scenes including both room background and indoor objects from single-view images. Existing methods can only produce 3D shapes of indoor objects with limited geometry quality because of the heavy occlusion of indoor scenes. To solve this, we propose an instance-aligned implicit function (InstPIFu) for detailed object reconstruction. Combining with instance-aligned attention module, our method is empowered to decouple mixed local features toward the occluded instances. Additionally, unlike previous methods that simply represents the room background as a 3D bounding box, depth map or a set of planes, we recover the fine geometry of the background via implicit representation. Extensive experiments on the SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets demonstrate that our method outperforms existing approaches in both background and foreground object reconstruction. Our code and model will be made publicly available.\""
  },
  "eccv2022_main_compnvsnovelviewsynthesiswithscenecompletion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CompNVS: Novel View Synthesis with Scene Completion",
    "authors": [
      "Zuoyue Li",
      "Tianxing Fan",
      "Zhenqiang Li",
      "Zhaopeng Cui",
      "Yoichi Sato",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2786_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610441.pdf",
    "published": "2020-08",
    "summary": "\"We introduce a scalable framework for novel view synthesis from RGB-D images with largely incomplete scene coverage. While generative neural approaches have demonstrated spectacular results on 2D images, they have not yet achieved similar photorealistic results in combination with scene completion where a spatial 3D scene understanding is essential. To this end, we propose a generative pipeline performing on a sparse grid-based neural scene representation to complete unobserved scene parts via a learned distribution of scenes in a 2.5D-3D-2.5D manner. We process encoded image features in 3D space with a geometry completion network and a subsequent texture inpainting network to extrapolate the missing area. Photorealistic image sequences can be finally obtained via consistency-relevant differentiable rendering. Comprehensive experiments show that the graphical outputs of our method outperform the state of the art, especially within unobserved scene parts.\""
  },
  "eccv2022_main_sketchsamplersketch-based3dreconstructionviaview-dependentdepthsampling": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SketchSampler: Sketch-Based 3D Reconstruction via View-Dependent Depth Sampling",
    "authors": [
      "Chenjian Gao",
      "Qian Yu",
      "Lu Sheng",
      "Yi-Zhe Song",
      "Dong Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2822_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610457.pdf",
    "published": "2020-08",
    "summary": "\"Reconstructing a 3D shape based on a single sketch image is challenging due to the large domain gap between a sparse, irregular sketch and a regular, dense 3D shape. Existing works try to employ the global feature extracted from sketch to directly predict the 3D coordinates, but they usually suffer from losing fine details that are not faithful to the input sketch. Through analyzing the 3D-to-2D projection process, we notice that the density map that characterizes the distribution of 2D point clouds (i.e., the probability of points projected at each location of the projection plane) can be used as a proxy to facilitate the reconstruction process. To this end, we first translate a sketch via an image translation network to a more informative 2D representation that can be used to generate a density map. Next, a 3D point cloud is reconstructed via a two-stage probabilistic sampling process: first recovering the 2D points (i.e., the x and y coordinates) by sampling the density map; and then predicting the depth (i.e., the z coordinate) by sampling the depth values at the ray determined by each 2D point. Extensive experiments are conducted, and both quantitative and qualitative results show that our proposed approach significantly outperforms other baseline methods.\""
  },
  "eccv2022_main_localbinsimprovingdepthestimationbylearninglocaldistributions": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "LocalBins: Improving Depth Estimation by Learning Local Distributions",
    "authors": [
      "Shariq Farooq Bhat",
      "Ibraheem Alhashim",
      "Peter Wonka"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2871_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610473.pdf",
    "published": "2020-08",
    "summary": "\"We propose a novel architecture for depth estimation from a single image. The architecture itself is based on the popular encoder-decoder architecture that is frequently used as a starting point for all dense regression tasks. We build on AdaBins which estimates a global distribution of depth values for the input image and evolve the architecture in two ways. First, instead of predicting global depth distributions, we predict depth distributions of local neighborhoods at every pixel. Second, instead of predicting depth distributions only towards the end of the decoder, we involve all layers of the decoder. We call this new architecture LocalBins. Our results demonstrate a clear improvement over the state-of-the-art in all metrics on the NYU-Depth V2 dataset. Code and pretrained models will be made publicly available.\""
  },
  "eccv2022_main_2dgansmeetunsupervisedsingle-view3dreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "2D GANs Meet Unsupervised Single-View 3D Reconstruction",
    "authors": [
      "Feng Liu",
      "Xiaoming Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2888_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610490.pdf",
    "published": "2020-08",
    "summary": "\"Recent research has shown that controllable image generation based on pre-trained GANs can benefit a wide range of computer vision tasks. However, less attention has been devoted to 3D vision tasks. In light of this, we propose a novel image-conditioned neural implicit field, which can leverage 2D supervisions from GAN-generated multi-view images and perform the single-view reconstruction of generic objects. Firstly, a novel offline StyleGAN-based generator is presented to generate plausible pseudo images with full control over the viewpoint. Then, we propose to utilize a neural implicit function, along with a differentiable renderer to learn 3D geometry from pseudo images with object masks and rough pose initializations. To further detect the unreliable supervisions, we introduce a novel uncertainty module to predict uncertainty maps, which remedy the negative effect of uncertain regions in pseudo images, leading to a better reconstruction performance. The effectiveness of our approach is demonstrated through superior single-view 3D reconstruction results of generic objects.\""
  },
  "eccv2022_main_infinitenature-zerolearningperpetualviewgenerationofnaturalscenesfromsingleimages": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images",
    "authors": [
      "Zhengqi Li",
      "Qianqian Wang",
      "Noah Snavely",
      "Angjoo Kanazawa"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2911_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610508.pdf",
    "published": "2020-08",
    "summary": "\"We present a method for learning to generate unbounded flythrough videos of natural scenes starting from a single view. This capability is learned from a collection of single photographs, without requiring camera poses or even multiple views of each scene. To achieve this, we propose a novel self-supervised view generation training paradigm where we sample and render virtual camera trajectories, including cyclic camera paths, allowing our model to learn stable view generation from a collection of single views. At test time, despite never having seen a video, our approach can take a single image and generate long camera trajectories comprised of hundreds of new views with realistic and diverse content. We compare our approach with recent state-of-the-art supervised view generation methods that require posed multi-view videos and demonstrate superior performance and synthesis quality. Our project webpage, including video results, is at infinite-nature-zero.github.io.\""
  },
  "eccv2022_main_semi-supervisedsingle-view3dreconstructionviaprototypeshapepriors": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors",
    "authors": [
      "Zhen Xing",
      "Hengduo Li",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3139_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610528.pdf",
    "published": "2020-08",
    "summary": "\"The performance of existing single-view 3D reconstruction methods heavily relies on large-scale of 3D annotations. However, such annotations are tedious and expensive to collect. Semi-supervised learning serves as an alternative way to mitigate the need for manual labels, but remains unexplored in 3D reconstruction. Inspired by the recent success of self-ensembling method in semi-supervised image classification task, we first propose SSP3D, a semi-supervised framework for 3D reconstruction. In particular, we introduce an attention-guided prototype shape prior module for guiding realistic object reconstruction. we further introduce a discriminator-guided module to incentivize better shape generation, as well as a regularizer to tolerate noisy training samples. On the ShapeNet benchmark, the proposed approach outperforms previous supervised methods by clear margins margin under various labeling ratios, ( i.e., 1%, 5%, 10% and 20%). Moreover, our approach also performs well when transferring to real-world Pix3D datasets under labeling ratios of 10%. We also demonstrate our method could transfer to novel categories with few novel supervised data. Experiments on the popular ShapeNet dataset show that our method outperforms the zero-shot baseline by over 12% and the current state-of-the-art by over 7% in the few-shot setting.\""
  },
  "eccv2022_main_bilateralnormalintegration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Bilateral Normal Integration",
    "authors": [
      "Xu Cao",
      "Hiroaki Santo",
      "Boxin Shi",
      "Fumio Okura",
      "Yasuyuki Matsushita"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3202_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610545.pdf",
    "published": "2020-08",
    "summary": "\"This paper studies the discontinuity preservation problem in recovering a surface from its surface normal map. To model discontinuities, we introduce the assumption that the surface to be recovered is semi-smooth, i.e., the surface is one-sided differentiable (hence one-sided continuous) everywhere in the horizontal and vertical directions. Under the semi-smooth surface assumption, we propose a bilaterally weighted functional for discontinuity preserving normal integration. The key idea is to relatively weight the one-sided differentiability at each point\u2019s two sides based on the definition of one-sided depth discontinuity. As a result, our method effectively preserves discontinuities and alleviates the under- or over-segmentation artifacts in the recovered surfaces compared to existing methods. Further, we unify the normal integration problem in the orthographic and perspective cases in a new way and show effective discontinuity preservation results in both cases.\""
  },
  "eccv2022_main_s$2$contactgraph-basednetworkfor3dhand-objectcontactestimationwithsemi-supervisedlearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "S$^2$Contact: Graph-Based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning",
    "authors": [
      "Tze Ho Elden Tse",
      "Zhongqun Zhang",
      "Kwang In Kim",
      "Ale\u0161 Leonardis",
      "Feng Zheng",
      "Hyung Jin Chang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3351_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610561.pdf",
    "published": "2020-08",
    "summary": "\"Being able to reason about the physical contacts between hands and objects is crucial in understanding hand-object manipulation. However, despite the efforts in accurate 3D annotations in hand and object datasets, there still exist gaps in 3D hand and object reconstructions. Recent works leverage contact maps to refine inaccurate hand-object pose estimations and generate grasps given object models. However, they require explicit 3D supervision which is seldom available and therefore, are limited to constrained settings, e.g., where thermal cameras observe residual heat left on manipulated objects. In this paper, we propose a novel semi-supervised framework that allows us to learn contact from monocular videos. Specifically, we leverage visual and geometric consistency constraints in large-scale datasets for generating pseudo-labels in semi-supervised learning and propose an efficient graph-based network to infer contact. Our semi-supervised learning framework achieves a favourable improvement over the existing supervised learning methods trained on data with \u2018limited\u2019 annotations. Notably, our proposed model is able to achieve superior results with less than half the network parameters and memory access cost when compared with the commonly-used PointNet-based approach. We show benefits from using a contact map that rules hand-object interactions to produce more accurate reconstructions. We further demonstrate that training with pseudo-labels can extend contact map estimations to out-of-domain objects and generalise better across multiple datasets.\""
  },
  "eccv2022_main_sc-wlstowardsinterpretablefeed-forwardcamerare-localization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SC-wLS: Towards Interpretable Feed-Forward Camera Re-localization",
    "authors": [
      "Xin Wu",
      "Hao Zhao",
      "Shunkai Li",
      "Yingdian Cao",
      "Hongbin Zha"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3498_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610578.pdf",
    "published": "2020-08",
    "summary": "\"Visual re-localization aims to recover camera poses in a known environment, which is vital for applications like robotics or augmented reality. Feed-forward absolute camera pose regression methods directly output poses by a network, but suffer from low accuracy. Meanwhile, scene coordinate based methods are accurate, but need iterative RANSAC post-processing, which brings challenges to efficient end-to-end training and inference. In order to have the best of both worlds, we propose a feed-forward method termed SC-wLS that exploits all scene coordinate estimates for weighted least squares pose regression. This differentiable formulation exploits a weight network imposed on 2D-3D correspondences, and requires pose supervision only. Qualitative results demonstrate the interpretability of learned weights. Evaluations on 7Scenes and Cambridge datasets show significantly promoted performance when compared with former feed-forward counterparts. Moreover, our SC-wLS method enables a new capability: self-supervised test-time adaptation on the weight network. Codes and models are publicly available.\""
  },
  "eccv2022_main_floatingfusiondepthfromtofandimage-stabilizedstereocameras": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "FloatingFusion: Depth from ToF and Image-Stabilized Stereo Cameras",
    "authors": [
      "Andreas Meuleman",
      "Hakyeong Kim",
      "James Tompkin",
      "Min H. Kim"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3503_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610595.pdf",
    "published": "2020-08",
    "summary": "\"High-accuracy per-pixel depth is vital for computational photography, so smartphones now have multimodal camera systems with time-of-flight (ToF) depth sensors and multiple color cameras. However, producing accurate high-resolution depth is still challenging due to the low resolution and limited active illumination power of ToF sensors. Fusing RGB stereo and ToF information is a promising direction to overcome these issues, but a key problem remains: to provide high-quality 2D RGB images, the main smartphone color sensor\u2019s lens is optically stabilized, resulting in an unknown pose for the floating lens that breaks the geometric relationships between the multimodal image sensors. Leveraging ToF depth estimates and a wide-angle RGB camera, we design an automatic calibration technique based on dense 2D/3D matching that can estimate camera pose intrinsic and distortion parameters of a stabilized main RGB sensor from a single snapshot. This lets us fuse stereo and ToF cues via a correlation volume. For fusion, we apply deep learning via a real-world training dataset with depth supervision estimated by a neural reconstruction method. For evaluation, we acquire a test dataset using a commercial high-power depth camera and show that our approach achieves higher accuracy than existing baselines.\""
  },
  "eccv2022_main_deltardepthestimationfromalight-weighttofsensorandrgbimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DELTAR: Depth Estimation from a Light-Weight ToF Sensor and RGB Image",
    "authors": [
      "Yijin Li",
      "Xinyang Liu",
      "Wenqi Dong",
      "Han Zhou",
      "Hujun Bao",
      "Guofeng Zhang",
      "Yinda Zhang",
      "Zhaopeng Cui"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3514_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610612.pdf",
    "published": "2020-08",
    "summary": "\"Light-weight time-of-flight (ToF) depth sensors are small, cheap, low-energy and have been massively deployed on mobile devices for the purposes like autofocus, obstacle detection, etc. However, due to their specific measurements (depth distribution in a region instead of the depth value at a certain pixel) and extremely low resolution, they are insufficient for applications requiring high-fidelity depth such as 3D reconstruction. In this paper, we propose DELTAR, a novel method to empower light-weight ToF sensors with the capability of measuring high resolution and accurate depth by cooperating with a color image. As the core of DELTAR, a feature extractor customized for depth distribution and an attention-based neural architecture is proposed to fuse the information from the color and ToF domain efficiently. To evaluate our system in real-world scenarios, we design a data collection device and propose a new approach to calibrate the RGB camera and ToF sensor. Experiments show that our method produces more accurate depth than existing frameworks designed for depth completion and depth super-resolution and achieves on par performance with a commodity-level RGB-D sensor. Code and data are available on the project webpage: https://zju3dv.github.io/deltar.\""
  },
  "eccv2022_main_3droomlayoutestimationfromacubemapofpanoramaimageviadeepmanhattanhoughtransform": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Room Layout Estimation from a Cubemap of Panorama Image via Deep Manhattan Hough Transform",
    "authors": [
      "Yining Zhao",
      "Chao Wen",
      "Zhou Xue",
      "Yue Gao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3606_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610630.pdf",
    "published": "2020-08",
    "summary": "\"Significant geometric structures can be compactly described by global wireframes in the estimation of 3D room layout from a single panoramic image. Based on this observation, we present an alternative approach to estimate the walls in 3D space by modeling long-range geometric patterns in a learnable Hough Transform block. We transform the image feature from a cubemap tile to the Hough space of a Manhattan world and directly map the feature to the geometric output. The convolutional layers not only learn the local gradient-like line features, but also utilize the global information to successfully predict occluded walls with a simple network structure. Unlike most previous work, the predictions are performed individually on each cubemap tile, and then assembled to get the layout estimation. Experimental results show that we achieve comparable results with recent state-of-the-art in prediction accuracy and performance. Code is available at https://github.com/Starrah/DMH-Net.\""
  },
  "eccv2022_main_rbp-poseresidualboundingboxprojectionforcategory-levelposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation",
    "authors": [
      "Ruida Zhang",
      "Yan Di",
      "Zhiqiang Lou",
      "Fabian Manhardt",
      "Federico Tombari",
      "Xiangyang Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3809_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610647.pdf",
    "published": "2020-08",
    "summary": "\"Category-level object pose estimation aims to predict the 6D pose as well as the 3D metric size of previously unseen objects from a known set of categories. Recent methods harness shape prior adaptation to map the observed point cloud into the canonical space and apply Umeyama\u2019s algorithm to recover the pose and size. However, their shape prior integration strategy boosts pose estimation indirectly, which leads to insufficient pose-sensitive feature extraction and slow inference speed. To tackle this problem, in this paper, we propose a novel geometry-guided Residual Object Bounding Box Projection network RBP-Pose that jointly predicts object pose and residual vectors describing the displacements from the shape-prior-indicated object surface projections on the bounding box towards real surface projections. Such definition of residual vectors is inherently zero-mean and relatively small, and explicitly encapsulates spatial cues of the 3D object for robust and accurate pose regression. We enforce geometry-aware consistency terms to align the predicted pose and residual vectors to further boost performance. Finally, to avoid overfitting and enhance the generalization ability of RBP-Pose, we propose an online non-linear shape augmentation scheme to promote shape diversity during training. Extensive experiments on NOCS datasets demonstrate that RBP-Pose surpasses all existing methods by a large margin, whilst achieving a real-time inference speed.\""
  },
  "eccv2022_main_monocular3dobjectreconstructionwithganinversion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Monocular 3D Object Reconstruction with GAN Inversion",
    "authors": [
      "Junzhe Zhang",
      "Daxuan Ren",
      "Zhongang Cai",
      "Chai Kiat Yeo",
      "Bo Dai",
      "Chen Change Loy"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3999_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610665.pdf",
    "published": "2020-08",
    "summary": "\"Recovering a textured 3D mesh from a monocular image is highly challenging, particularly for in-the-wild objects that lack 3D ground truths. In this work, we present MeshInversion, a novel framework to improve the reconstruction by exploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh synthesis. Reconstruction is achieved by searching for a latent space in the 3D GAN that best resembles the target mesh in accordance with the single view observation. Since the pre-trained GAN encapsulates rich 3D semantics in terms of mesh geometry and texture, searching within the GAN manifold thus naturally regularizes the realness and fidelity of the reconstruction. Importantly, such regularization is directly applied in the 3D space, providing crucial guidance of mesh parts that are unobserved in the 2D space. Experiments on standard benchmarks show that our framework obtains faithful 3D reconstructions with consistent geometry and texture across both observed and unobserved parts. Moreover, it generalizes well to meshes that are less commonly seen, such as the extended articulation of deformable objects. Code is released at https://github.com/junzhezhang/mesh-inversion.\""
  },
  "eccv2022_main_map-freevisualrelocalizationmetricposerelativetoasingleimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Map-Free Visual Relocalization: Metric Pose Relative to a Single Image",
    "authors": [
      "Eduardo Arnold",
      "Jamie Wynn",
      "Sara Vicente",
      "Guillermo Garcia-Hernando",
      "Aron Monszpart",
      "Victor Prisacariu",
      "Daniyar Turmukhambetov",
      "Eric Brachmann"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4029_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610682.pdf",
    "published": "2020-08",
    "summary": "\"Can we relocalize in a scene represented by a single reference image? Standard visual relocalization requires hundreds of images and scale calibration to build a scene-specific 3D map. In contrast, we propose Map-free Relocalization, i.e., using only one photo of a scene to enable instant, metric scaled relocalization. Existing datasets are not suitable to benchmark map-free relocalization, due to their focus on large scenes or their limited variability. Thus, we have constructed a new dataset of 655 small places of interest, such as sculptures, murals and fountains, collected worldwide. Each place comes with a reference image to serve as a relocalization anchor, and dozens of query images with known, metric camera poses. The dataset features changing conditions, stark viewpoint changes, high variability across places, and queries with low to no visual overlap with the reference image.We identify two viable families of existing methods to provide baseline results: relative pose regression, and feature matching combined with single-image depth prediction. While these methods show reasonable performance on some favorable scenes in our dataset, map-free relocalization proves to be a challenge that requires new, innovative solutions.\""
  },
  "eccv2022_main_self-distilledfeatureaggregationforself-supervisedmonoculardepthestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Self-Distilled Feature Aggregation for Self-Supervised Monocular Depth Estimation",
    "authors": [
      "Zhengming Zhou",
      "Qiulei Dong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4073_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610700.pdf",
    "published": "2020-08",
    "summary": "\"Self-supervised monocular depth estimation has received much attention recently in computer vision. Most of the existing works in literature aggregate multi-scale features for depth prediction via either straightforward concatenation or element-wise addition, however, such feature aggregation operations generally neglect the contextual consistency between multi-scale features. Addressing this problem, we propose the Self-Distilled Feature Aggregation (SDFA) module for simultaneously aggregating a pair of low-scale and high-scale features and maintaining their contextual consistency. The SDFA employs three branches to learn three feature offset maps respectively: one offset map for refining the input low-scale feature and the other two for refining the input high-scale feature under a designed self-distillation manner. Then, we propose an SDFA-based network for self-supervised monocular depth estimation, and design a self-distilled training strategy to train the proposed network with the SDFA module. Experimental results on the KITTI dataset demonstrate that the proposed method outperforms the comparative state-of-the-art methods in most cases. The code is available at https://github.com/ZM-Zhou/SDFA-Net_pytorch.\""
  },
  "eccv2022_main_planesvs.chairscategory-guided3dshapelearningwithoutany3dcues": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Planes vs. Chairs: Category-Guided 3D Shape Learning without Any 3D Cues",
    "authors": [
      "Zixuan Huang",
      "Stefan Stojanov",
      "Anh Thai",
      "Varun Jampani",
      "James M. Rehg"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4231_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610717.pdf",
    "published": "2020-08",
    "summary": "\"We present a novel 3D shape reconstruction method which learns to predict an implicit 3D shape representation from a single RGB image. Our approach uses a set of single-view images of multiple object categories without viewpoint annotation, forcing the model to learn across multiple object categories without 3D supervision. To facilitate learning with such minimal supervision, we use category labels to guide shape learning with a novel categorical metric learning approach. We also utilize adversarial and viewpoint regularization techniques to further disentangle the effects of viewpoint and shape. We obtain the first results for large-scale (more than 50 categories) single-viewpoint shape prediction using a single model. We are also the first to examine and quantify the benefit of class information in single-view supervised 3D shape reconstruction. Our method achieves superior performance over state-of-the-art methods on ShapeNet-13, ShapeNet-55 and Pascal3D+.\""
  },
  "eccv2022_main_mhr-netmultiple-hypothesisreconstructionofnon-rigidshapesfrom2dviews": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MHR-Net: Multiple-Hypothesis Reconstruction of Non-rigid Shapes from 2D Views",
    "authors": [
      "Haitian Zeng",
      "Xin Yu",
      "Jiaxu Miao",
      "Yi Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4241_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620001.pdf",
    "published": "2020-08",
    "summary": "\"We propose MHR-Net, a novel method for recovering Non-Rigid Shapes from Motion (NRSfM). MHR-Net aims to find a set of reasonable reconstructions for a 2D view, and it also selects the most likely reconstruction from the set. To deal with the challenging unsupervised generation of non-rigid shapes, we develop a new Deterministic Basis and Stochastic Deformation scheme in MHR-Net. The non-rigid shape is first expressed as the sum of a coarse shape basis and a flexible shape deformation, then multiple hypotheses are generated with uncertainty modeling of the deformation part. MHR-Net is optimized with reprojection loss on the basis and the best hypothesis. Furthermore, we design a new Procrustean Residual Loss, which reduces the rigid rotations between similar shapes and further improves the performance. Experiments show that MHR-Net achieves state-of-the-art reconstruction accuracy on Human3.6M, SURREAL and 300-VW datasets.\""
  },
  "eccv2022_main_depthmapdecompositionformonoculardepthestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Depth Map Decomposition for Monocular Depth Estimation",
    "authors": [
      "Jinyoung Jun",
      "Jae-Han Lee",
      "Chul Lee",
      "Chang-Su Kim"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4247_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620018.pdf",
    "published": "2020-08",
    "summary": "\"We propose a novel algorithm for monocular depth estimation that decomposes a metric depth map into a normalized depth map and scale features. The proposed network is composed of a shared encoder and three decoders, called G-Net, N-Net, and M-Net, which estimate gradient maps, a normalized depth map, and a metric depth map, respectively. M-Net learns to estimate metric depths more accurately using relative depth features extracted by G-Net and N-Net. The proposed algorithm has the advantage that it can use datasets without metric depth labels to improve the performance of metric depth estimation. Experimental results on various datasets demonstrate that the proposed algorithm not only provides competitive performance to state-of-the-art algorithms but also yields acceptable results even when only a small amount of metric depth data is available for its training.\""
  },
  "eccv2022_main_monitoreddistillationforpositivecongruentdepthcompletion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Monitored Distillation for Positive Congruent Depth Completion",
    "authors": [
      "Tian Yu Liu",
      "Parth Agrawal",
      "Allison Chen",
      "Byung-Woo Hong",
      "Alex Wong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4288_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620035.pdf",
    "published": "2020-08",
    "summary": "\"We propose a method to infer a dense depth map from a single image, its calibration, and the associated sparse point cloud. In order to leverage existing models (teachers) that produce putative depth maps, we propose an adaptive knowledge distillation approach that yields a positive congruent training process, wherein a student model avoids learning the error modes of the teachers. In the absence of ground truth for model selection and training, our method, termed Monitored Distillation, allows a student to exploit a blind ensemble of teachers by selectively learning from predictions that best minimize the reconstruction error for a given image. Monitored Distillation yields a distilled depth map and a confidence map, or \u201cmonitor\"\"\"\", for how well a prediction from a particular teacher fits the observed image. The monitor adaptively weights the distilled depth where if all of the teachers exhibit high residuals, the standard unsupervised image reconstruction loss takes over as the supervisory signal. On indoor scenes (VOID), we outperform blind ensembling baselines by 17.53% and unsupervised methods by 24.25%; we boast a 79% model size reduction while maintaining comparable performance to the best supervised method. For outdoors (KITTI), we tie for 5th overall on the benchmark despite not using ground truth. Code available at: https://github.com/alexklwong/mondi-python.\""
  },
  "eccv2022_main_resolution-freepointcloudsamplingnetworkwithdatadistillation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Resolution-Free Point Cloud Sampling Network with Data Distillation",
    "authors": [
      "Tianxin Huang",
      "Jiangning Zhang",
      "Jun Chen",
      "Yuang Liu",
      "Yong Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4326_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620053.pdf",
    "published": "2020-08",
    "summary": "\"Down-sampling algorithms are adopted to simplify the point clouds and save the computation cost on subsequent tasks. Existing learning-based sampling methods often need to train a big sampling network to support sampling under different resolutions, which must generate sampled points with the costly maximum resolution even if only low-resolution points need to be sampled. In this work, we propose a novel resolution-free point clouds sampling network to directly sample the original point cloud to different resolutions, which is conducted by optimizing non-learning-based initial sampled points to better positions. Besides, we introduce data distillation to assist the training process by considering the differences between task network outputs from original point clouds and sampled points. Experiments on point cloud reconstruction and recognition tasks demonstrate that our method can achieve SOTA performances with lower time and memory cost than existing learning-based sampling strategies. Codes are available at https://github.com/Tianxinhuang/PCDNet.\""
  },
  "eccv2022_main_organicpriorsinnon-rigidstructurefrommotion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Organic Priors in Non-rigid Structure from Motion",
    "authors": [
      "Suryansh Kumar",
      "Luc Van Gool"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4720_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620069.pdf",
    "published": "2020-08",
    "summary": "\"This paper advocates the use of organic priors in classical non-rigid structure from motion (NRSfM). By organic priors, we mean invaluable intermediate prior information intrinsic to the NRSfM matrix factorization theory. It is shown that such priors reside in the factorized matrices, and quite surprisingly, existing methods generally disregard them. The paper\u2019s main contribution is to put forward a simple, methodical, and practical method that can effectively exploit such organic priors to solve NRSfM. The proposed method does not make assumptions other than the popular one on the low-rank shape and offers a reliable solution to NRSfM under orthographic projection. Our work reveals that the accessibility of organic priors is independent of the camera motion and shape deformation type. Besides that, the paper provides insights into the NRSfM factorization---both in terms of shape and motion---and is the first approach to show the benefit of single rotation averaging for NRSfM. Furthermore, we outline how to effectively recover motion and non-rigid 3D shape using the proposed organic prior based approach and demonstrate results that outperform prior-free NRSfM performance by a significant margin. Finally, we present the benefits of our method via extensive experiments and evaluations on several benchmark datasets.\""
  },
  "eccv2022_main_perspectiveflowaggregationfordata-limited6dobjectposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation",
    "authors": [
      "Yinlin Hu",
      "Pascal Fua",
      "Mathieu Salzmann"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4807_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620087.pdf",
    "published": "2020-08",
    "summary": "\"Most recent 6D object pose estimation methods, including unsupervised ones, require many real training images. Unfortunately, for some applications, such as those in space or deep under water, acquiring real images, even unannotated, is virtually impossible. In this paper, we propose a method that can be trained solely on synthetic images, or optionally using a few additional real ones. Given a rough pose estimate obtained from a first network, it uses a second network to predict a dense 2D correspondence field between the image rendered using the rough pose and the real image and infers the required pose correction. This approach is much less sensitive to the domain shift between synthetic and real images than state-of-the-art methods. It performs on par with methods that require annotated real images for training when not using any, and outperforms them considerably when using as few as twenty real images.\""
  },
  "eccv2022_main_danbodisentangledarticulatedneuralbodyrepresentationsviagraphneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DANBO: Disentangled Articulated Neural Body Representations via Graph Neural Networks",
    "authors": [
      "Shih-Yang Su",
      "Timur Bagautdinov",
      "Helge Rhodin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4883_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620104.pdf",
    "published": "2020-08",
    "summary": "\"Deep learning greatly improved the realism of animatable human models by learning geometry and appearance from collections of 3D scans, template meshes, and multi-view imagery. High-resolution models enable photo-realistic avatars but at the cost of requiring studio settings not available to end users. Our goal is to create avatars directly from raw images without relying on expensive studio setups and surface tracking. While a few such approaches exist, those have limited generalization capabilities and are prone to learning spurious (chance) correlations between irrelevant body parts, resulting in implausible deformations and missing body parts on unseen poses. We introduce a three-stage method that induces two inductive biases to better disentangled pose-dependent deformation. First, we model correlations of body parts explicitly with a graph neural network. Second, to further reduce the effect of chance correlations, we introduce localized per-bone features that use a factorized volumetric representation and a new aggregation function. We demonstrate that our model produces realistic body shapes under challenging unseen poses and shows high-quality image synthesis. Our proposed representation strikes a better trade-off between model capacity, expressiveness, and robustness than competing methods. Project website: https://lemonatsu.github.io/danbo.\""
  },
  "eccv2022_main_chorecontact,humanandobjectreconstructionfromasinglergbimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"CHORE: Contact, Human and Object REconstruction from a Single RGB Image\"",
    "authors": [
      "Xianghui Xie",
      "Bharat Lal Bhatnagar",
      "Gerard Pons-Moll"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4894_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620121.pdf",
    "published": "2020-08",
    "summary": "\"Most prior works in perceiving 3D humans from images reason human in isolation without their surroundings. However, humans are constantly interacting with the surrounding objects, thus calling for models that can reason about not only the human but also the object and their interaction. The problem is extremely challenging due to heavy occlusions between humans and objects, diverse interaction types and depth ambiguity. In this paper, we introduce CHORE, a novel method that learns to jointly reconstruct the human and the object from a single RGB image. CHORE takes inspiration from recent advances in implicit surface learning and classical model-based fitting. We compute a neural reconstruction of human and object represented implicitly with two unsigned distance fields, a correspondence field to a parametric body and an object pose field. This allows us to robustly fit a parametric body model and a 3D object template, while reasoning about interactions. Furthermore, prior pixel-aligned implicit learning methods use synthetic data and make assumptions that are not met in the real data. We propose a elegant depth-aware scaling that allows more efficient shape learning on real data. Experiments show that our joint reconstruction learned with the proposed strategy significantly outperforms the SOTA. Our code and models are available at https://virtualhumans.mpi-inf.mpg.de/chore\""
  },
  "eccv2022_main_learnedvertexdescentanewdirectionfor3dhumanmodelfitting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learned Vertex Descent: A New Direction for 3D Human Model Fitting",
    "authors": [
      "Enric Corona",
      "Gerard Pons-Moll",
      "Guillem Aleny\u00e0",
      "Francesc Moreno-Noguer"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4918_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620141.pdf",
    "published": "2020-08",
    "summary": "\"We propose a novel optimization-based paradigm for 3D human shape fitting on images. In contrast to existing approaches that directly regress the parameters of a low-dimensional statistical body model (e.g. SMPL) from input images, we propose training a deep network that, given solely image features and an unfit mesh, predicts the directions of the vertices towards the 3D body mesh. At inference, we employ this network, dubbed LVD, within a gradient-descent optimization pipeline until its convergence, which typically occurs in a fraction of a second even when initializing all vertices into a single point. An exhaustive evaluation demonstrates that our approach is able to capture the underlying body of clothed people with very different body shapes, achieving a significant improvement compared to state-of-the-art. Additionally, the proposed formulation can generalize to other sources of input data, which we experimentally show on fitting 3D scans of full bodies and hands.\""
  },
  "eccv2022_main_self-calibratingphotometricstereobyneuralinverserendering": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Self-Calibrating Photometric Stereo by Neural Inverse Rendering",
    "authors": [
      "Junxuan Li",
      "Hongdong Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5007_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620160.pdf",
    "published": "2020-08",
    "summary": "\"This paper tackles the task of uncalibrated photometric stereo for 3D object reconstruction, where both the object shape, object reflectance, and lighting directions are unknown. This is an extremely difficult task, and the challenge is further compounded with the existence of the well-known generalized bas-relief (GBR) ambiguity in photometric stereo. Previous methods to resolve this ambiguity either rely on an overly simplified reflectance model, or assume special light distribution. We propose a new method that jointly optimizes object shape, light directions, and light intensities, all under general surfaces and lights assumptions. The specularities are used explicitly to resolve the GBR ambiguity via a neural inverse rendering process. We gradually fit specularities from shiny to rough using novel progressive specular bases. Our method leverages a physically based rendering equation by minimizing the reconstruction error on a per-object-basis. Our method demonstrates state-of-the-art accuracy in light estimation and shape recovery on real-world datasets.\""
  },
  "eccv2022_main_3dclothedhumanreconstructioninthewild": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Clothed Human Reconstruction in the Wild",
    "authors": [
      "Gyeongsik Moon",
      "Hyeongjin Nam",
      "Takaaki Shiratori",
      "Kyoung Mu Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5036_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620177.pdf",
    "published": "2020-08",
    "summary": "\"Although much progress has been made in 3D clothed human reconstruction, most of the existing methods fail to produce robust results from in-the-wild images, which contain diverse human poses and appearances. This is mainly due to the large domain gap between training datasets and in-the-wild datasets. The training datasets are usually synthetic ones, which contain rendered images from GT 3D scans. However, such datasets contain simple human poses and less natural image appearances compared to those of real in-the-wild datasets, which makes generalization of it to in-the-wild images extremely challenging. To resolve this issue, in this work, we propose ClothWild, a 3D clothed human reconstruction framework that firstly addresses the robustness on in-thewild images. First, for the robustness to the domain gap, we propose a weakly supervised pipeline that is trainable with 2D supervision targets of in-the-wild datasets. Second, we design a DensePose-based loss function to reduce ambiguities of the weak supervision. Extensive empirical tests on several public in-the-wild datasets demonstrate that our proposed ClothWild produces much more accurate and robust results than the state-of-the-art methods. The codes are available in here: https://github.com/hygenie1228/ClothWild_RELEASE.\""
  },
  "eccv2022_main_directedraydistancefunctionsfor3dscenereconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Directed Ray Distance Functions for 3D Scene Reconstruction",
    "authors": [
      "Nilesh Kulkarni",
      "Justin Johnson",
      "David F. Fouhey"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5180_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620193.pdf",
    "published": "2020-08",
    "summary": "\"We present an approach for full 3D scene reconstruction from a single new image that can be trained on realistic non-watertight scans. Our approach uses a predicted distance function, since these have shown promise in handling complex topologies and large spaces. We identify and analyze two key challenges for predicting these implicit functions from an image that have prevented their success on 3D scenes from a single image. First, we show that predicting a conventional scene distance from an image requires reasoning over a large receptive field. Second, we analytically show that the optimal output of a network that predicts these distance functions is often not a distance function. We propose an alternate approach, the Direct Ray Distance Function (DRDF), that avoids both challenges. We show that a deep network trained to predict DRDFs outperforms all other methods quantitatively and qualitatively on 3D reconstruction on Matterport3D, 3DFront, and ScanNet\""
  },
  "eccv2022_main_objectleveldepthreconstructionforcategorylevel6dobjectposeestimationfrommonocularrgbimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation from Monocular RGB Image",
    "authors": [
      "Zhaoxin Fan",
      "Zhenbo Song",
      "Jian Xu",
      "Zhicheng Wang",
      "Kejian Wu",
      "Hongyan Liu",
      "Jun He"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5287_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620212.pdf",
    "published": "2020-08",
    "summary": "\"Recently, RGBD-based category-level 6D object pose estimation has achieved promising improvement in performance, however, the requirement of depth information prohibits broader applications. In order to relieve this problem, this paper proposes a novel approach named Object Level Depth reconstruction Network (OLD-Net) taking only RGB images as input for category-level 6D object pose estimation. We propose to directly predict object-level depth from a monocular RGB image by deforming the category-level shape prior into object-level depth and the canonical NOCS representation. Two novel modules named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth Reconstruction (SDDR) module are introduced to learn high fidelity object-level depth and delicate shape representations. At last, the 6D object pose is solved by aligning the predicted canonical representation with the back-projected object-level depth. Extensive experiments on the challenging CAMERA25 and REAL275 datasets indicate that our model, though simple, achieves state-of-the-art performance.\""
  },
  "eccv2022_main_uncertaintyquantificationindepthestimationviaconstrainedordinalregression": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Uncertainty Quantification in Depth Estimation via Constrained Ordinal Regression",
    "authors": [
      "Dongting Hu",
      "Liuhua Peng",
      "Tingjin Chu",
      "Xiaoxing Zhang",
      "Yinian Mao",
      "Howard Bondell",
      "Mingming Gong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5351_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620229.pdf",
    "published": "2020-08",
    "summary": "\"Monocular Depth Estimation (MDE) is a task to predict a dense depth map from a single image. Despite the recent progress brought by deep learning, existing methods are still prone to errors due to the ill-posed nature of MDE. Hence depth estimation systems must be self-aware of possible mistakes to avoid disastrous consequences. This paper provides an uncertainty quantification method for supervised MDE models. From a frequentist view, we capture the uncertainty by predictive variance that consists of two terms: error variance and estimation variance. The former represents the noise of a depth value, and the latter measures the randomness in the depth regression model due to training on finite data. To estimate error variance, we perform constrained ordinal regression (ConOR) on discretized depth to estimate the conditional distribution of depth given image, and then compute the corresponding conditional mean and variance as the predicted depth and error variance estimator, respectively. Our work also leverages bootstrapping methods to infer estimation variance from re-sampled data. We perform experiments on both simulated and real data to validate the effectiveness of the proposed method. The results show that our approach produces accurate uncertainty estimates while maintaining high depth prediction accuracy.\""
  },
  "eccv2022_main_costdcnetcostvolumebaseddepthcompletionforasinglergb-dimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CostDCNet: Cost Volume Based Depth Completion for a Single RGB-D Image",
    "authors": [
      "Jaewon Kam",
      "Jungeon Kim",
      "Soongjin Kim",
      "Jaesik Park",
      "Seungyong Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5688_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620248.pdf",
    "published": "2020-08",
    "summary": "\"Successful depth completion from a single RGB-D image requires both extracting plentiful 2D and 3D features and merging these heterogeneous features appropriately. We propose a novel depth completion framework, CostDCNet, based on the cost volume-based depth estimation approach that has been successfully employed for multi-view stereo (MVS). The key to high-quality depth map estimation in the approach is constructing an accurate cost volume. To produce a quality cost volume tailored to single-view depth completion, we present a simple but effective architecture that can fully exploit the 3D information, three options to make an RGB-D feature volume, and per-plane pixel shuffle for efficient volume upsampling. Our CostDCNet framework consists of lightweight deep neural networks ( 1.8M parameters), running in real time ( 30ms). Nevertheless, thanks to our simple but effective design, CostDCNet demonstrates depth completion results comparable to or better than the state-of-the-art methods.\""
  },
  "eccv2022_main_shapoimplicitrepresentationsformulti-objectshape,appearance,andposeoptimization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization\"",
    "authors": [
      "Muhammad Zubair Irshad",
      "Sergey Zakharov",
      "Rare\u0219 Ambru\u0219",
      "Thomas Kollar",
      "Zsolt Kira",
      "Adrien Gaidon"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5770_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620266.pdf",
    "published": "2020-08",
    "summary": "\"Our method studies the complex task of object-centric 3D understanding from a single RGB-D observation. As it is an ill-posed problem, existing methods suffer from low performance for both 3D shape and 6D pose and size estimation in complex multi-object scenarios with occlusions. We present ShAPO, a method for joint multi-object detection, 3D textured reconstruction, 6D object pose and size estimation. Key to ShAPO is a single-shot pipeline to regress shape, appearance and pose latent codes along with the masks of each object instance, which is then further refined in a sparse-to-dense fashion. A novel disentangled shape and appearance database of priors is first learned to embed objects in their respective shape and appearance space. We also propose a novel, octree-based differentiable optimization step, allowing us to further improve object shape, pose and appearance simultaneously under the learned latent space, in an analysis-by synthesis fashion. Our novel joint implicit textured object representation allows us to accurately identify and reconstruct novel unseen objects without having access to their 3D meshes. Through extensive experiments, we show that our method, trained on simulated indoor scenes, accurately regresses the shape, appearance and pose of novel objects in the real-world with minimal fine-tuning. Our method significantly out-performs all baselines on the NOCS dataset with an 8% absolute improvement in mAP for 6D pose estimation.\""
  },
  "eccv2022_main_3dsiamesetransformernetworkforsingleobjecttrackingonpointclouds": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Siamese Transformer Network for Single Object Tracking on Point Clouds",
    "authors": [
      "Le Hui",
      "Lingpeng Wang",
      "Linghua Tang",
      "Kaihao Lan",
      "Jin Xie",
      "Jian Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5829_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620284.pdf",
    "published": "2020-08",
    "summary": "\"Siamese network based trackers formulate 3D single object tracking as cross-correlation learning between point features of a template and a search area. Due to the large appearance variation between the template and search area during tracking, how to learn the robust cross correlation between them for identifying the potential target in the search area is still a challenging problem. In this paper, we explicitly use Transformer to form a 3D Siamese Transformer network for learning robust cross correlation between the template and the search area of point clouds. Specifically, we develop a Siamese point Transformer network to learn shape context information of the target. Its encoder uses self-attention to capture non-local information of point clouds to characterize the shape information of the object, and the decoder utilizes cross-attention to upsample discriminative point features. After that, we develop an iterative coarse-to-fine correlation network to learn the robust cross correlation between the template and the search area. It formulates the cross-feature augmentation to associate the template with the potential target in the search area via cross attention. To further enhance the potential target, it employs the ego-feature augmentation that applies self-attention to the local k-NN graph of the feature space to aggregate target features. Experiments on the KITTI, nuScenes, and Waymo datasets show that our method achieves state-of-the-art performance on the 3D single object tracking task.\""
  },
  "eccv2022_main_objectwake-up3dobjectriggingfromasingleimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Object Wake-Up: 3D Object Rigging from a Single Image",
    "authors": [
      "Ji Yang",
      "Xinxin Zuo",
      "Sen Wang",
      "Zhenbo Yu",
      "Xingyu Li",
      "Bingbing Ni",
      "Minglun Gong",
      "Li Cheng"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5901_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620302.pdf",
    "published": "2020-08",
    "summary": "\"Given a single chair image, could we wake it up by reconstructing its 3D shape and skeleton, as well as animating its plausible articulations and motions, similar to that of human modeling? It is a new problem that not only goes beyond image-based object reconstruction but also involves articulated animation of generic objects in 3D, which could give rise to numerous downstream augmented and virtual reality applications. In this paper, we propose an automated approach to tackle the entire process of reconstruct such generic 3D objects, rigging and animation, all from single images. A two-stage pipeline has thus been proposed, which specifically contains a multi-head structure to utilize the deep implicit functions for skeleton prediction. Two in-house 3D datasets of generic objects with high-fidelity rendering and annotated skeletons have also been constructed. Empirically our approach demonstrated promising results; when evaluated on the related sub-tasks of 3D reconstruction and skeleton prediction, our results surpass those of the state-of-the-arts by a noticeable margin. Our code and datasets are made publicly available at the dedicated project website.\""
  },
  "eccv2022_main_integratedpifuintegratedpixelalignedimplicitfunctionforsingle-viewhumanreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "IntegratedPIFu: Integrated Pixel Aligned Implicit Function for Single-View Human Reconstruction",
    "authors": [
      "Kennard Yanting Chan",
      "Guosheng Lin",
      "Haiyu Zhao",
      "Weisi Lin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5915_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620319.pdf",
    "published": "2020-08",
    "summary": "\"We propose IntegratedPIFu, a new pixel-aligned implicit model that builds on the foundation set by PIFuHD. IntegratedPIFu shows how depth and human parsing information can be predicted and capitalized upon in a pixel-aligned implicit model. In addition, IntegratedPIFu introduces depth-oriented sampling, a novel training scheme that improve any pixel-aligned implicit model\u2019s ability to reconstruct important human features without noisy artefacts. Lastly, IntegratedPIFu presents a new architecture that, despite using less model parameters than PIFuHD, is able to improves the structural correctness of reconstructed meshes. Our results show that IntegratedPIFu significantly outperforms existing state-of-the-arts methods on single-view human reconstruction. We provide the code in our supplementary materials. Our code is available at https://github.com/kcyt/IntegratedPIFu.\""
  },
  "eccv2022_main_realisticone-shotmesh-basedheadavatars": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Realistic One-Shot Mesh-Based Head Avatars",
    "authors": [
      "Taras Khakhulin",
      "Vanessa Sklyarova",
      "Victor Lempitsky",
      "Egor Zakharov"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6023_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620336.pdf",
    "published": "2020-08",
    "summary": "\"We present a system for the creation of realistic one-shot mesh-based (ROME) human head avatars. From a single photograph, our system estimates the head mesh (with person-specific details in both the facial and non-facial head parts) as well as the neural texture encoding local photometric and geometric details. The resulting avatars are rigged and can be rendered using a deep rendering network, which is trained alongside the mesh and texture estimators on a dataset of in-the-wild videos. In the experiments, we observe that our system performs competitively both in terms of head geometry recovery and the quality of renders, especially for strong pose and expression changes.\""
  },
  "eccv2022_main_akendallshapespaceapproachto3dshapeestimationfrom2dlandmarks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Kendall Shape Space Approach to 3D Shape Estimation from 2D Landmarks",
    "authors": [
      "Martha Paskin",
      "Daniel Baum",
      "Mason N. Dean",
      "Christoph von Tycowicz"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6090_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620354.pdf",
    "published": "2020-08",
    "summary": "\"3D shapes provide substantially more information than 2D images. However, the acquisition of 3D shapes is sometimes very difficult or even impossible in comparison with acquiring 2D images, making it necessary to derive the 3D shape from 2D images. Although this is, in general, a mathematically ill-posed problem, it might be solved by constraining the problem formulation using prior information. Here, we present a new approach based on Kendall\u2019s shape space to reconstruct 3D shapes from single monocular 2D images. The work is motivated by an application to study the feeding behavior of the basking shark, an endangered species whose massive size and mobility render 3D shape data nearly impossible to obtain, hampering understanding of their feeding behaviors and ecology. 2D images of these animals in feeding position, however, are readily available. We compare our approach with state-of-the-art shape-based approaches, both on human stick models and on shark head skeletons. Using a small set of training shapes, we show that the Kendall shape space approach is substantially more robust than previous methods and results in plausible shapes. This is essential for the motivating application in which specimens are rare and therefore only few training shapes are available.\""
  },
  "eccv2022_main_neurallightfieldestimationforstreetsceneswithdifferentiablevirtualobjectinsertion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Neural Light Field Estimation for Street Scenes with Differentiable Virtual Object Insertion",
    "authors": [
      "Zian Wang",
      "Wenzheng Chen",
      "David Acuna",
      "Jan Kautz",
      "Sanja Fidler"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6516_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620370.pdf",
    "published": "2020-08",
    "summary": "\"We consider the challenging problem of outdoor lighting estimation for the goal of photorealistic virtual object insertion into photographs. Existing works on outdoor lighting estimation typically simplify the scene lighting into an environment map which cannot capture the spatially-varying lighting effects in outdoor scenes. In this work, we propose a neural approach that estimates the 5D HDR light field from a single image, and a differentiable object insertion formulation that enables end-to-end training with image-based losses that encourage realism. Specifically, we design a hybrid lighting representation tailored to outdoor scenes, which contains an HDR sky dome that handles the extreme intensity of the sun, and a volumetric lighting representation that models the spatially-varying appearance of the surrounding scene. With the estimated lighting, our shadow-aware object insertion is fully differentiable, which enables adversarial training over the composited image to provide additional supervisory signal to the lighting prediction. We experimentally demonstrate that our hybrid lighting representation is more performant than existing outdoor lighting estimation methods. We further show the benefits of our AR object insertion in an autonomous driving application, where we obtain performance gains for a 3D object detector when trained on our augmented data.\""
  },
  "eccv2022_main_perspectivephaseanglemodelforpolarimetric3dreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Perspective Phase Angle Model for Polarimetric 3D Reconstruction",
    "authors": [
      "Guangcheng Chen",
      "Li He",
      "Yisheng Guan",
      "Hong Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6667_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620387.pdf",
    "published": "2020-08",
    "summary": "\"Current polarimetric 3D reconstruction methods, including those in the well-established shape from polarization literature, are all developed under the orthographic projection assumption. In the case of a large field of view, however, this assumption does not hold and may result in significant reconstruction errors in methods that make this assumption. To address this problem, we present the perspective phase angle (PPA) model that is applicable to perspective cameras. Compared with the orthographic model, the proposed PPA model accurately describes the relationship between polarization phase angle and surface normal under perspective projection. In addition, the PPA model makes it possible to estimate surface normals from only one single-view phase angle map and does not suffer from the so-called \u00cf\u20ac-ambiguity problem. Experiments on real data show that the PPA model is more accurate for surface normal estimation with a perspective camera than the orthographic model.\""
  },
  "eccv2022_main_deepshadowneuralshapefromshadow": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DeepShadow: Neural Shape from Shadow",
    "authors": [
      "Asaf Karnieli",
      "Ohad Fried",
      "Yacov Hel-Or"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7476_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620403.pdf",
    "published": "2020-08",
    "summary": "\"This paper presents \u2018DeepShadow\u2019, a one-shot method for recovering the depth map and surface normals from photometric stereo shadow maps. Previous works that try to recover the surface normals from photometric stereo images treat cast shadows as a disturbance. We show that the self and cast shadows not only do not disturb 3D reconstruction, but can be used alone, as a strong learning signal, to recover the depth map and surface normals. We demonstrate that 3D reconstruction from shadows can even outperform shape-from-shading in certain cases. To the best of our knowledge, our method is the first to reconstruct 3D shape-from-shadows using neural networks. The method does not require any pre-training or expensive labeled data, and is optimized during inference time.\""
  },
  "eccv2022_main_cameraauto-calibrationfromthesteinerconicofthefundamentalmatrix": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Camera Auto-Calibration from the Steiner Conic of the Fundamental Matrix",
    "authors": [
      "Yu Liu",
      "Hui Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7605_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620419.pdf",
    "published": "2020-08",
    "summary": "\"This paper addresses the problem of camera auto-calibration from the fundamental matrix under general motion. The fundamental matrix can be decomposed into a symmetric part (a Steiner conic) and a skew-symmetric part (a fixed point), which we find useful for fully calibrating camera parameters. We first obtain a fixed line from the image of the symmetric, skew-symmetric parts of the fundamental matrix and the image of the absolute conic. Then the properties of this fixed line are presented and proved, from which new constraints on general eigenvectors between the Steiner conic and the image of the absolute conic are derived. We thus propose a method to fully calibrate the camera. First, the three camera intrinsic parameters, i.e., the two focal lengths and the skew, can be solved from our new constraints on the imaged absolute conic obtained from at least three images. On this basis, we can initialize and then iteratively restore the optimal pair of projection centers of the Steiner conic, thereby obtaining the corresponding vanishing lines and images of circular points. Finally, all five camera parameters are fully calibrated using images of circular points obtained from at least three images. Experimental results on synthetic and real data demonstrate that our method achieves state-of-the-art performance in terms of accuracy.\""
  },
  "eccv2022_main_super-resolution3dhumanshapefromasinglelow-resolutionimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Super-Resolution 3D Human Shape from a Single Low-Resolution Image",
    "authors": [
      "Marco Pesavento",
      "Marco Volino",
      "Adrian Hilton"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7765_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620435.pdf",
    "published": "2020-08",
    "summary": "\"We propose a novel framework to reconstruct super-resolution human shape from a single low-resolution input image. The approach overcomes limitations of existing approaches that reconstruct 3D human shape from a single image, which require high-resolution images together with auxiliary data such as surface normal or a parametric model to reconstruct high-detail shape. The proposed framework represents the reconstructed shape with a high-detail implicit function. Analogous to the objective of 2D image super-resolution, the approach learns the mapping from a low-resolution shape to its high-resolution counterpart and it is applied to reconstruct 3D shape detail from low-resolution images. The approach is trained end-to-end employing a novel loss function which estimates the information lost between a low and high-resolution representation of the same 3D surface shape. Evaluation for single image reconstruction of clothed people demonstrates that our method achieves high-detail surface reconstruction from low-resolution images without auxiliary data. Extensive experiments show that the proposed approach can estimate super-resolution human geometries with a significantly higher level of detail than that obtained with previous approaches when applied to low-resolution images.\""
  },
  "eccv2022_main_minimalneuralatlasparameterizingcomplexsurfaceswithminimalchartsanddistortion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion",
    "authors": [
      "Weng Fei Low",
      "Gim Hee Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/107_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620452.pdf",
    "published": "2020-08",
    "summary": "\"Explicit neural surface representations allow for exact and efficient extraction of the encoded surface at arbitrary precision, as well as analytic derivation of differential geometric properties such as surface normal and curvature. Such desirable properties, which are absent in its implicit counterpart, makes it ideal for various applications in computer vision, graphics and robotics. However, SOTA works are limited in terms of the topology it can effectively describe, distortion it introduces to reconstruct complex surfaces and model efficiency. In this work, we present Minimal Neural Atlas, a novel atlas-based explicit neural surface representation. At its core is a fully learnable parametric domain, given by an implicit probabilistic occupancy field defined on an open square of the parametric space. In contrast, prior works generally predefine the parametric domain. The added flexibility enables charts to admit arbitrary topology and boundary. Thus, our representation can learn a minimal atlas of 3 charts with distortion-minimal parameterization for surfaces of arbitrary topology, including closed and open surfaces with arbitrary connected components. Our experiments support the hypotheses and show that our reconstructions are more accurate in terms of the overall geometry, due to the separation of concerns on topology and geometry.\""
  },
  "eccv2022_main_extrudenetunsupervisedinversesketch-and-extrudeforshapeparsing": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing",
    "authors": [
      "Daxuan Ren",
      "Jianmin Zheng",
      "Jianfei Cai",
      "Jiatong Li",
      "Junzhe Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/194_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620468.pdf",
    "published": "2020-08",
    "summary": "\"Sketch-and-extrude is a common and intuitive modeling process in computer aided design. This paper studies the problem of learning the shape given in the form of point clouds by \u201cinverse\u201d sketch-and-extrude. We present ExtrudeNet, an unsupervised end-to-end network for discovering sketch and extrude from point clouds. Behind ExtrudeNet are two new technical components: 1) the use of a specially-designed rational B\u00e9zier representation for sketch and extrude, which can model extrusion with freeform sketches and conventional cylinder and box primitives as well; and 2) a numerical method for computing the signed distance field which is used in the network learning. This is the first attempt that uses machine learning to reverse engineer the sketch-and-extrude modeling process of a shape in an unsupervised fashion. ExtrudeNet not only outputs a compact, editable and interpretable representation of the shape that can be seamlessly integrated into modern CAD software, but also aligns with the standard CAD modeling process facilitating various editing applications, which distinguishes our work from existing shape parsing research. Code will be open-sourced upon acceptance.\""
  },
  "eccv2022_main_catreiterativepointcloudsalignmentforcategory-levelobjectposerefinement": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CATRE: Iterative Point Clouds Alignment for Category-Level Object Pose Refinement",
    "authors": [
      "Xingyu Liu",
      "Gu Wang",
      "Yi Li",
      "Xiangyang Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/326_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620485.pdf",
    "published": "2020-08",
    "summary": "\"While category-level 9DoF object pose estimation has emerged recently, previous correspondence-based or direct regression methods are both limited in accuracy due to the huge intra-category variances in object shape and color, etc. Orthogonal to them, this work presents a category-level object pose and size refiner CATRE, which is able to iteratively enhance pose estimate from point clouds to produce accurate results. Given an initial pose estimate, CATRE predicts a relative transformation between the initial pose and ground truth by means of aligning the partially observed point cloud and an abstract shape prior. In specific, we propose a novel disentangled architecture being aware of the inherent distinctions between rotation and translation/size estimation. Extensive experiments show that our approach remarkably outperforms state-of-the-art methods on REAL275, CAMERA25, and LM benchmarks up to a speed of approximately 85.32Hz, and achieves competitive results on category-level tracking. We further demonstrate that CATRE can perform pose refinement on unseen category. Code and trained models are available.\""
  },
  "eccv2022_main_optimizationoverdisentangledencodingunsupervisedcross-domainpointcloudcompletionviaocclusionfactormanipulation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Optimization over Disentangled Encoding: Unsupervised Cross-Domain Point Cloud Completion via Occlusion Factor Manipulation",
    "authors": [
      "Jingyu Gong",
      "Fengqi Liu",
      "Jiachen Xu",
      "Min Wang",
      "Xin Tan",
      "Zhizhong Zhang",
      "Ran Yi",
      "Haichuan Song",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/361_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620504.pdf",
    "published": "2020-08",
    "summary": "\"Recently, studies considering domain gaps in shape completion attracted more attention, due to the undesirable performance of supervised methods on real scans. They only noticed the gap in input scans, but ignored the gap in output prediction, which is specific for completion. In this paper, we disentangle partial scans into three (domain, shape, and occlusion) factors to handle the output gap in cross-domain completion. For factor learning, we design view-point prediction and domain classification tasks in a self-supervised manner and bring a factor permutation consistency regularization to ensure factor independence. Thus, scans can be completed by simply manipulating occlusion factors while preserving domain and shape information. To further adapt to instances in the target domain, we introduce an optimization stage to maximize the consistency between completed shapes and input scans. Extensive experiments on real scans and synthetic datasets show that ours outperforms previous methods by a large margin and is encouraging for the following works. Code is available at https://github.com/azuki-miho/OptDE.\""
  },
  "eccv2022_main_unsupervisedlearningof3dsemantickeypointswithmutualreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction",
    "authors": [
      "Haocheng Yuan",
      "Chen Zhao",
      "Shichao Fan",
      "Jiaxi Jiang",
      "Jiaqi Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/463_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620521.pdf",
    "published": "2020-08",
    "summary": "\"Semantic 3D keypoints are category-level semantic consistent points on 3D objects. Detecting 3D semantic keypoints is a foundation for a number of 3D vision tasks but remains challenging, due to the ambiguity of semantic information, especially when the objects are represented by unordered 3D point clouds. Existing unsupervised methods tend to generate category-level keypoints in implicit manners, making it difficult to extract high-level information, such as semantic labels and topology. From a novel mutual reconstruction perspective, we present an unsupervised method to generate consistent semantic keypoints from point clouds explicitly. To achieve this, we train our unsupervised model to reconstruct both the input object and other objects from the same category based on predicted keypoints. To the best of our knowledge, the proposed method is the first to mine 3D semantic consistent keypoints from a mutual reconstruction view. Experiments under various evaluation metrics as well as comparisons with the state-of-the-arts have verified the efficacy of our new solution to mining semantic consistent keypoints with mutual reconstruction.\""
  },
  "eccv2022_main_mvdecormulti-viewdensecorrespondencelearningforfine-grained3dsegmentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MvDeCor: Multi-View Dense Correspondence Learning for Fine-Grained 3D Segmentation",
    "authors": [
      "Gopal Sharma",
      "Kangxue Yin",
      "Subhransu Maji",
      "Evangelos Kalogerakis",
      "Or Litany",
      "Sanja Fidler"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/525_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620538.pdf",
    "published": "2020-08",
    "summary": "\"We propose to utilize self-supervised techniques in the 2D domain for fine-grained 3D shape segmentation tasks. This is inspired by the observation that view-based surface representations are more effective at modeling high-resolution surface details and texture than their 3D counterparts based on point clouds or voxel occupancy. Specifically, given a 3D shape, we render it from multiple views, and set up a dense correspondence learning task within the contrastive learning framework. As a result, the learned 2D representations are view-invariant and geometrically consistent, leading to better generalization when trained on a limited number of labeled shapes than alternatives based on self-supervision in 2D or 3D alone. Experiments on textured (RenderPeople) and untextured (PartNet) 3D datasets show that our method outperforms state-of-the-art alternatives in fine-grained part segmentation. The improvements over baselines are greater when only a sparse set of views is available for training or when shapes are textured, indicating that \\mvd benefits from both 2D processing and 3D geometric reasoning.\""
  },
  "eccv2022_main_suprasparseunifiedpart-basedhumanrepresentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SUPR: A Sparse Unified Part-Based Human Representation",
    "authors": [
      "Ahmed A. A. Osman",
      "Timo Bolkart",
      "Dimitrios Tzionas",
      "Michael J. Black"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/570_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620555.pdf",
    "published": "2020-08",
    "summary": "\"Statistical 3D shape models of the head, hands, and full body are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important information about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models: an expressive head (SUPR-Head), an articulated hand (SUPR-Hand), and a novel foot (SUPR-Foot). Note that feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet deform due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separate body parts to existing expressive human body models and body-part models and find that our suite of models generalizes better and captures the body parts\u2019 full range of motion. SUPR is publicly available for research purposes.\""
  },
  "eccv2022_main_revisitingpointcloudsimplificationalearnablefeaturepreservingapproach": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Revisiting Point Cloud Simplification: A Learnable Feature Preserving Approach",
    "authors": [
      "Rolandos Alexandros Potamias",
      "Giorgos Bouritsas",
      "Stefanos Zafeiriou"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/584_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620573.pdf",
    "published": "2020-08",
    "summary": "\"The recent advances in 3D sensing technology have made possible the capture of point clouds in significantly high resolution. However, increased detail usually comes at the expense of high storage, as well as computational costs in terms of processing and visualization operations. Mesh and Point Cloud simplification methods aim to reduce the complexity of 3D models while retaining visual quality and relevant salient features. Traditional simplification techniques usually rely on solving a time-consuming optimization problem, hence they are impractical for large-scale datasets. In an attempt to alleviate this computational burden, we propose a fast point cloud simplification method by learning to sample salient points. The proposed method relies on a graph neural network architecture trained to select an arbitrary, user-defined, number of points according to their latent encodings and re-arrange their positions so as to minimize the visual perception error. The approach is extensively evaluated on various datasets using several perceptual metrics. Importantly, our method is able to generalize to out-of-distribution shapes, hence demonstrating zero-shot capabilities.\""
  },
  "eccv2022_main_maskedautoencodersforpointcloudself-supervisedlearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Masked Autoencoders for Point Cloud Self-Supervised Learning",
    "authors": [
      "Yatian Pang",
      "Wenxiao Wang",
      "Francis E.H. Tay",
      "Wei Liu",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/800_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620591.pdf",
    "published": "2020-08",
    "summary": "\"As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud\u2019s properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. The pre-trained models achieve 85.18% accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5%-2.3% in the few-shot classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud. Codes are available at https://github.com/Pang-Yatian/Point-MAE.\""
  },
  "eccv2022_main_intrinsicneuralfieldslearningfunctionsonmanifolds": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Intrinsic Neural Fields: Learning Functions on Manifolds",
    "authors": [
      "Lukas Koestler",
      "Daniel Grittner",
      "Michael Moeller",
      "Daniel Cremers",
      "Zorah L\u00e4hner"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/927_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620609.pdf",
    "published": "2020-08",
    "summary": "\"Neural fields have gained significant attention in the computer vision community due to their excellent performance in novel view synthesis, geometry reconstruction, and generative modeling. Some of their advantages are a sound theoretic foundation and an easy implementation in current deep learning frameworks. While neural fields have been applied to signals on manifolds, e.g., for texture reconstruction, their representation has been limited to extrinsically embedding the shape into Euclidean space. The extrinsic embedding ignores known intrinsic manifold properties and is inflexible wrt. transfer of the learned function. To overcome these limitations, this work introduces intrinsic neural fields, a novel and versatile representation for neural fields on manifolds. Intrinsic neural fields combine the advantages of neural fields with the spectral properties of the Laplace-Beltrami operator. We show theoretically that intrinsic neural fields inherit many desirable properties of the extrinsic neural field framework but exhibit additional intrinsic qualities, like isometry invariance. In experiments, we show intrinsic neural fields can reconstruct high-fidelity textures from images with state-of-the-art quality and are robust to the discretization of the underlying manifold. We demonstrate the versatility of intrinsic neural fields by tackling various applications: texture transfer between deformed shapes & different shapes, texture reconstruction from real-world images with view dependence, and discretization-agnostic learning on meshes and point clouds.\""
  },
  "eccv2022_main_skeleton-freeposetransferforstylized3dcharacters": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Skeleton-Free Pose Transfer for Stylized 3D Characters",
    "authors": [
      "Zhouyingcheng Liao",
      "Jimei Yang",
      "Jun Saito",
      "Gerard Pons-Moll",
      "Yang Zhou"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1103_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620627.pdf",
    "published": "2020-08",
    "summary": "\"We present the first method that automatically transfers poses between stylized 3D characters without skeletal rigging. In contrast to previous attempts to learn pose transformations on fixed or topology-equivalent skeleton templates, our method focuses on a novel scenario to handle skeleton-free characters with diverse shapes, topologies, and mesh connectivities. The key idea of our method is to represent the characters in a unified articulation model so that the pose can be transferred through the correspondent parts. To achieve this, we propose a novel pose transfer network that predicts the character skinning weights and deformation transformations jointly to articulate the target character to match the desired pose. Our method is trained in a semi-supervised manner absorbing all existing character data with paired/unpaired poses and stylized shapes. It generalizes well to unseen stylized characters and inanimate objects. We conduct extensive experiments and demonstrate the effectiveness of our method on this novel task.\""
  },
  "eccv2022_main_maskeddiscriminationforself-supervisedlearningonpointclouds": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Masked Discrimination for Self-Supervised Learning on Point Clouds",
    "authors": [
      "Haotian Liu",
      "Mu Cai",
      "Yong Jae Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1209_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620645.pdf",
    "published": "2020-08",
    "summary": "\"Masked autoencoding has achieved great success for self-supervised learning in the image and language domains. However, mask based pretraining has yet to show benefits for point cloud understanding, likely due to standard backbones like PointNet being unable to properly handle the training versus testing distribution mismatch introduced by masking during training. In this paper, we bridge this gap by proposing a discriminative mask pretraining Transformer framework, MaskPoint, for point clouds. Our key idea is to represent the point cloud as discrete occupancy values (1 if part of the point cloud; 0 if not), and perform simple binary classification between masked object points and sampled noise points as the proxy task. In this way, our approach is robust to the point sampling variance in point clouds, and facilitates learning rich representations. We evaluate our pretrained models across several downstream tasks, including 3D shape classification, segmentation, and real-word object detection, and demonstrate state-of-the-art results while achieving a significant pretraining speedup (e.g., 4.1x on ScanNet) compared to the prior state-of-the-art Transformer baseline. Code is available at https://github.com/haotian-liu/MaskPoint.\""
  },
  "eccv2022_main_fbnetfeedbacknetworkforpointcloudcompletion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "FBNet: Feedback Network for Point Cloud Completion",
    "authors": [
      "Xuejun Yan",
      "Hongyu Yan",
      "Jingjing Wang",
      "Hang Du",
      "Zhihong Wu",
      "Di Xie",
      "Shiliang Pu",
      "Li Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1272_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620664.pdf",
    "published": "2020-08",
    "summary": "\"The rapid development of point cloud learning has driven point cloud completion into a new era. However, the information flows of most existing completion methods are solely feedforward, and high-level information is rarely reused to improve low-level feature learning. To this end, we propose a novel Feedback Network (FBNet) for point cloud completion, in which present features are efficiently refined by rerouting subsequent fine-grained ones. Firstly, partial inputs are fed to a Hierarchical Graph-based Network (HGNet) to generate coarse shapes. Then, we cascade several Feedback-Aware Completion (FBAC) Blocks and unfold them across time recurrently. Feedback connections between two adjacent time steps exploit fine-grained features to improve present shape generations. The main challenge of building feedback connections is the dimension mismatching between present and subsequent features. To address this, the elaborately designed point Cross Transformer exploits efficient information from feedback features via cross attention strategy and then refines present features with the enhanced feedback features. Quantitative and qualitative experiments on several datasets demonstrate the superiority of proposed FBNet compared to state-of-the-art methods on point completion task. The source code and model are available at https://github.com/hikvision-research/3DVision/tree/main/PointCompletion/FBNet.\""
  },
  "eccv2022_main_meta-sampleralmost-universalyettask-orientedsamplingforpointclouds": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Meta-Sampler: Almost-Universal yet Task-Oriented Sampling for Point Clouds",
    "authors": [
      "Ta-Ying Cheng",
      "Qingyong Hu",
      "Qian Xie",
      "Niki Trigoni",
      "Andrew Markham"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1516_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620682.pdf",
    "published": "2020-08",
    "summary": "\"Sampling is a key operation in point-cloud task and acts to increase computational efficiency and tractability by discarding redundant points. Universal sampling algorithms (e.g., Farthest Point Sampling) work without modification across different tasks, models, and datasets, but by their very nature are agnostic about the downstream task/model. As such, they have no implicit knowledge about which points would be best to keep and which to reject. Recent work has shown how task-specific point cloud sampling (e.g., SampleNet) can be used to outperform traditional sampling approaches by learning which points are more informative. However, these learnable samplers face two inherent issues: i) overfitting to a model rather than a task, and ii) requiring training of the sampling network from scratch, in addition to the task network, somewhat countering the original objective of down-sampling to increase efficiency. In this work, we propose an almost-universal sampler, in our quest for a sampler that can learn to preserve the most useful points for a particular task, yet be inexpensive to adapt to different tasks, models or datasets. We first demonstrate how training over multiple models for the same task (e.g., shape reconstruction) significantly outperforms the vanilla SampleNet in terms of accuracy by not overfitting the sample network to a particular task network. Second, we show how we can train an almost-universal meta-sampler across multiple tasks. This meta-sampler can then be rapidly fine-tuned when applied to different datasets, networks, or even different tasks, thus amortizing the initial cost of training.\""
  },
  "eccv2022_main_alevelsettheoryforneuralimplicitevolutionunderexplicitflows": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Level Set Theory for Neural Implicit Evolution under Explicit Flows",
    "authors": [
      "Ishit Mehta",
      "Manmohan Chandraker",
      "Ravi Ramamoorthi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1742_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620699.pdf",
    "published": "2020-08",
    "summary": "\"Coordinate-based neural networks parameterizing implicit surfaces have emerged as efficient representations of geometry. They effectively act as parametric level sets with the zero-level set defining the surface of interest. We present a framework that allows applying deformation operations defined for triangle meshes onto such implicit surfaces. Several of these operations can be viewed as energy-minimization problems that induce an instantaneous flow field on the explicit surface. Our method uses the flow field to deform parametric implicit surfaces by extending the classical theory of level sets. We also derive a consolidated view for existing methods on differentiable surface extraction and rendering, by formalizing connections to the level-set theory. We show that these methods drift from the theory and that our approach exhibits improvements for applications like surface smoothing, mean-curvature flow, inverse rendering and user-defined editing on implicit geometry.\""
  },
  "eccv2022_main_efficientpointcloudanalysisusinghilbertcurve": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Efficient Point Cloud Analysis Using Hilbert Curve",
    "authors": [
      "Wanli Chen",
      "Xinge Zhu",
      "Guojin Chen",
      "Bei Yu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1859_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620717.pdf",
    "published": "2020-08",
    "summary": "\"Previous state-of-the-art research on analyzing point cloud mainly rely on the voxelization quantization because it keeps the better spatial locality and geometry. However, these 3D voxelization methods and subsequent 3D convolution networks often bring the large computational overhead and GPU occupation. A straightforward alternative is to flatten 3D voxelization into 2D structure or utilize the pillar representation to perform the dimension reduction, while all of them would inevitably alter the spatial locality and 3D geometric information. In this way, we propose the HilbertNet to maintain the locality advantage of voxel-based methods while significantly reducing the computational cost. Here the key component is a new flattening mechanism based on Hilbert curve, which is a famous locality and geometry preserving function. Namely, if flattening 3D voxels using Hilbert curve encoding, the resulting structure will have similar spatial topology compared with original voxels. Through the Hilbert flattening, we can not only use 2D convolution (more lightweight than 3D convolution) to process voxels, but also incorporate technologies suitable in 2D space, such as transformer, to boost the performance. Our proposed HilbertNet achieves state-of-the-art performance on ShapeNet and ModelNet40 datasets with smaller cost and GPU occupation.\""
  },
  "eccv2022_main_tochspatio-temporalobject-to-handcorrespondenceformotionrefinement": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion Refinement",
    "authors": [
      "Keyang Zhou",
      "Bharat Lal Bhatnagar",
      "Jan Eric Lenssen",
      "Gerard Pons-Moll"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1991_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630001.pdf",
    "published": "2020-08",
    "summary": "\"We present TOCH, a method for refining incorrect 3D hand-object interaction sequences using a data prior. Existing hand trackers, especially those that rely on very few cameras, often produce visually unrealistic results with hand-object intersection or missing contacts. Although correcting such errors requires reasoning about temporal aspects of interaction, most previous works focus on static grasps and contacts. The core of our method are TOCH fields, a novel spatio-temporal representation for modeling correspondences between hands and objects during interaction. TOCH fields are a point-wise, object-centric representation, which encode the hand position relative to the object. Leveraging this novel representation, we learn a latent manifold of plausible TOCH fields with a temporal denoising auto-encoder. Experiments demonstrate that TOCH outperforms state-of-the-art 3D hand-object interaction models, which are limited to static grasps and contacts. More importantly, our method produces smooth interactions even before and after contact. Using a single trained TOCH model, we quantitatively and qualitatively demonstrate its usefulness for correcting erroneous sequences from off-the-shelf RGB/RGB-D hand-object reconstruction methods and transferring grasps across objects.\""
  },
  "eccv2022_main_laterflabelandtextdrivenobjectradiancefields": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "LaTeRF: Label and Text Driven Object Radiance Fields",
    "authors": [
      "Ashkan Mirzaei",
      "Yash Kant",
      "Jonathan Kelly",
      "Igor Gilitschenski"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2145_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630021.pdf",
    "published": "2020-08",
    "summary": "\"Obtaining 3D object representations is important for creating photo-realistic simulators and collecting assets for AR/VR applications. Neural fields have shown their effectiveness in learning a continuous volumetric representation of a scene from 2D images, but acquiring object representations from these models with weak supervision remains an open challenge. In this paper we introduce LaTeRF, a method for extracting an object of interest from a scene given 2D images of the entire scene and known camera poses, a natural language description of the object, and a small number of point-labels of object and non-object points in the input images. To faithfully extract the object from the scene, LaTeRF extends the NeRF formulation with an additional \u2018objectness\u2019 probability at each 3D point. Additionally, we leverage the rich latent space of a pre-trained CLIP model combined with our differentiable object renderer, to inpaint the occluded parts of the object. We demonstrate high-fidelity object extraction on both synthetic and real datasets and justify our design choices through an extensive ablation study.\""
  },
  "eccv2022_main_meshmaemaskedautoencodersfor3dmeshdataanalysis": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis",
    "authors": [
      "Yaqian Liang",
      "Shanshan Zhao",
      "Baosheng Yu",
      "Jing Zhang",
      "Fazhi He"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2319_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630038.pdf",
    "published": "2020-08",
    "summary": "\"Recently, self-supervised pre-training has advanced Vision Transformers on various tasks w.r.t. different data modalities, e.g., image and 3D point cloud data. In this paper, we explore this learning paradigm for 3D mesh data analysis based on Transformers. Since applying Transformer architectures to new modalities is usually non-trivial, we first adapt Vision Transformer to 3D mesh data processing, i.e., Mesh Transformer. In specific, we divide a mesh into several non-overlapping local patches with each containing the same number of faces and use the 3D position of each patch\u2019s center point to form positional embeddings. Inspired by MAE, we explore how pre-training on 3D mesh data with the Transformer-based structure benefits downstream 3D mesh analysis tasks. We first randomly mask some patches of the mesh and feed the corrupted mesh into Mesh Transformers. Then, through reconstructing the information of masked patches, the network is capable of learning discriminative representations for mesh data. Therefore, we name our method MeshMAE, which can yield state-of-the-art or comparable performance on mesh analysis tasks, i.e., classification and segmentation. In addition, we also conduct comprehensive ablation studies to show the effectiveness of key designs in our method.\""
  },
  "eccv2022_main_unsuperviseddeepmulti-shapematching": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Unsupervised Deep Multi-Shape Matching",
    "authors": [
      "Dongliang Cao",
      "Florian Bernard"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2459_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630056.pdf",
    "published": "2020-08",
    "summary": "\"3D shape matching is a long-standing problem in computer vision and computer graphics. While deep neural networks were shown to lead to state-of-the-art results in shape matching, existing learning-based approaches are limited in the context of multi-shape matching: (i) either they focus on matching pairs of shapes only and thus suffer from cycle-inconsistent multi-matchings, or (ii) they require an explicit template shape to address the matching of a collection of shapes. In this paper, we present a novel approach for deep multi-shape matching that ensures cycle-consistent multi-matchings while not depending on an explicit template shape. To this end, we utilise a shape-to-universe multi-matching representation that we combine with powerful functional map regularisation, so that our multi-shape matching neural network can be trained in a fully unsupervised manner. While the functional map regularisation is only considered during training time, functional maps are not computed for predicting correspondences, thereby allowing for fast inference. We demonstrate that our method achieves state-of-the-art results on several challenging benchmark datasets, and, most remarkably, that our unsupervised method even outperforms recent supervised methods.\""
  },
  "eccv2022_main_texturifygeneratingtextureson3dshapesurfaces": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Texturify: Generating Textures on 3D Shape Surfaces",
    "authors": [
      "Yawar Siddiqui",
      "Justus Thies",
      "Fangchang Ma",
      "Qi Shan",
      "Matthias Nie\u00dfner",
      "Angela Dai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2573_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630073.pdf",
    "published": "2020-08",
    "summary": "\"Texture cues on 3D objects are key to compelling visual representations, with the possibility to create high visual fidelity with inherent spatial consistency across different views. Since the availability of textured 3D shapes remains very limited, learning a 3D-supervised data-driven method that predicts a texture based on the 3D input is very challenging. We thus propose Texturify, a GAN-based method that leverages a 3D shape dataset of an object class and learns to reproduce the distribution of appearances observed in real images by generating high-quality textures. In particular, our method does not require any 3D color supervision or correspondence between shape geometry and images to learn the texturing of 3D objects. Texturify operates directly on the surface of the 3D objects by introducing face convolutional operators on a hierarchical 4-RoSy parametrization to generate plausible object-specific textures. Employing differentiable rendering and adversarial losses that critique individual views and consistency across views, we effectively learn the high-quality surface texturing distribution from real-world images. Experiments on car and chair shape collections show that our approach outperforms state of the art by an average of 22% in FID score.\""
  },
  "eccv2022_main_autoregressive3dshapegenerationviacanonicalmapping": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Autoregressive 3D Shape Generation via Canonical Mapping",
    "authors": [
      "An-Chieh Cheng",
      "Xueting Li",
      "Sifei Liu",
      "Min Sun",
      "Ming-Hsuan Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2586_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630091.pdf",
    "published": "2020-08",
    "summary": "\"With the capacity of modeling long-range dependencies in sequential data, transformers have shown remarkable performances in a variety of generative tasks such as image, audio, and text generation. Yet, taming them in generating less structured and voluminous data formats such as high-resolution point clouds have seldom been explored due to ambiguous sequentialization processes and infeasible computation burden. In this paper, we aim to further exploit the power of transformers and employ them for the task of 3D point cloud generation. The key idea is to decompose point clouds of one category into semantically aligned sequences of shape compositions, via a learned canonical space. These shape compositions can then be quantized and used to learn a context-rich composition codebook for point cloud generation. Experimental results on point cloud reconstruction and unconditional generation show that our model performs favorably against state-of-the-art approaches. Furthermore, our model can be easily extended to multi-modal shape completion as an application for conditional shape generation.\""
  },
  "eccv2022_main_pointtreetransformation-robustpointcloudencoderwithrelaxedk-dtrees": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PointTree: Transformation-Robust Point Cloud Encoder with Relaxed K-D Trees",
    "authors": [
      "Jun-Kun Chen",
      "Yu-Xiong Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2625_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630107.pdf",
    "published": "2020-08",
    "summary": "\"Being able to learn an effective semantic representation directly on raw point clouds has become a central topic in 3D understanding. Despite rapid progress, state-of-the-art encoders are restrictive to canonicalized point clouds, and have weaker than necessary performance when encountering geometric transformation distortions. To overcome this challenge, we propose PointTree, a general-purpose point cloud encoder that is robust to transformations based on relaxed K-D trees. Key to our approach is the design of the division rule in K-D trees by using principal component analysis (PCA). We use the structure of the relaxed K-D tree as our computational graph, and model the features as border descriptors which are merged with pointwise-maximum operation. In addition to this novel architecture design, we further improve the robustness by introducing pre-alignment -- a simple yet effective PCA-based normalization scheme. Our PointTree encoder combined with pre-alignment consistently outperforms state-of-the-art methods by large margins, for applications from object classification to semantic segmentation on various transformed versions of the widely-benchmarked datasets. Code and pre-trained models are available at https://github.com/immortalCO/PointTree.\""
  },
  "eccv2022_main_unifunitedneuralimplicitfunctionsforclothedhumanreconstructionandanimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "UNIF: United Neural Implicit Functions for Clothed Human Reconstruction and Animation",
    "authors": [
      "Shenhan Qian",
      "Jiale Xu",
      "Ziwei Liu",
      "Liqian Ma",
      "Shenghua Gao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2902_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630123.pdf",
    "published": "2020-08",
    "summary": "\"We propose united implicit functions (UNIF), a part-based method for clothed human reconstruction and animation with raw scans and skeletons as the input. Previous part-based methods for human reconstruction rely on ground-truth part labels from SMPL and thus are limited to minimal-clothed humans. In contrast, our method learns to separate parts from body motions instead of part supervision, thus can be extended to clothed humans and other articulated objects. Our Partition-from-Motion is achieved by a bone-centered initialization, a bone limit loss, and a section normal loss that ensure stable part division even when the training poses are limited. We also present a minimal perimeter loss for SDF to suppress extra surfaces and part overlapping. Another core of our method is an adjacent part seaming algorithm that produces non-rigid deformations to maintain the connection between parts which significantly relieves the part-based artifacts. Under this algorithm, we further propose \"\"Competing Parts\u201d, a method that defines blending weights by the relative position of a point to bones instead of the absolute position, avoiding the generalization problem of neural implicit functions with inverse LBS (linear blend skinning). We demonstrate the effectiveness of our method by clothed human body reconstruction and animation on the CAPE and the ClothSeq datasets.\""
  },
  "eccv2022_main_prifprimaryray-basedimplicitfunction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PRIF: Primary Ray-Based Implicit Function",
    "authors": [
      "Brandon Y. Feng",
      "Yinda Zhang",
      "Danhang Tang",
      "Ruofei Du",
      "Amitabh Varshney"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2921_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630140.pdf",
    "published": "2020-08",
    "summary": "\"We introduce a new implicit shape representation called Primary Ray-based Implicit Function (PRIF). In contrast to most existing approaches based on the signed distance function (SDF) which handles spatial locations, our representation operates on oriented rays. Specifically, PRIF is formulated to directly produce the surface hit point of a given input ray, without the expensive sphere-tracing operations, hence enabling efficient shape extraction and differentiable rendering. We demonstrate that neural networks trained to encode PRIF achieve successes in various tasks including single shape representation, category-wise shape generation, shape completion from sparse or noisy observations, inverse rendering for camera pose estimation, and neural rendering with color.\""
  },
  "eccv2022_main_pointclouddomainadaptationviamaskedlocal3dstructureprediction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Point Cloud Domain Adaptation via Masked Local 3D Structure Prediction",
    "authors": [
      "Hanxue Liang",
      "Hehe Fan",
      "Zhiwen Fan",
      "Yi Wang",
      "Tianlong Chen",
      "Yu Cheng",
      "Zhangyang Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3035_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630159.pdf",
    "published": "2020-08",
    "summary": "\"The superiority of deep learning based point cloud representations relies on large-scale labeled datasets, while the annotation of point clouds is notoriously expensive. One of the most effective solutions is to transfer the knowledge from existing labeled source data to unlabeled target data. However, domain bias typically hinders knowledge transfer and leads to accuracy degradation. In this paper, we propose a Masked Local Structure Prediction (MLSP) method to encode target data. Along with the supervised learning on the source domain, our method enables models to embed source and target data in a shared feature space. Specifically, we predict masked local structure via estimating point cardinality, position and normal. Our design philosophies lie in: 1) Point cardinality reflects basic structures (e.g., line, edge and plane) that are invariant to specific domains. 2) Predicting point positions in masked areas generalizes learned representations so that they are robust to incompletion-caused domain bias. 3) Point normal is generated by neighbors and thus robust to noise across domains. We conduct experiments on shape classification and semantic segmentation with different transfer permutations and the results demonstrate the effectiveness of our method. Code is available at https://github.com/VITA-Group/MLSP.\""
  },
  "eccv2022_main_clip-actortext-drivenrecommendationandstylizationforanimatinghumanmeshes": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes",
    "authors": [
      "Kim Youwang",
      "Kim Ji-Yeon",
      "Tae-Hyun Oh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3229_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630176.pdf",
    "published": "2020-08",
    "summary": "\"We propose CLIP-Actor, a text-driven motion recommendation and neural mesh stylization system for human mesh animation. CLIP-Actor animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. We build a text-driven human motion recommendation system by leveraging a large-scale human motion dataset with language labels. Given a natural language prompt, CLIP-Actor suggests a text-conforming human motion in a coarse-to-fine manner. Then, our novel zero-shot neural style optimization detailizes and texturizes the recommended mesh sequence to conform to the prompt in a temporally-consistent and pose-agnostic manner. This is distinctive in that prior work fails to generate plausible results when the pose of an artist-designed mesh does not conform to the text from the beginning. We further propose the spatio-temporal view augmentation and mask-weighted embedding attention, which stabilize the optimization process by leveraging multi-frame human motion and rejecting poorly rendered views. We demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt.\""
  },
  "eccv2022_main_planeformersfromsparseviewplanesto3dreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PlaneFormers: From Sparse View Planes to 3D Reconstruction",
    "authors": [
      "Samir Agarwala",
      "Linyi Jin",
      "Chris Rockwell",
      "David F. Fouhey"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3429_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630194.pdf",
    "published": "2020-08",
    "summary": "\"We present an approach for the planar surface reconstruction of a scene from images with limited overlap. This reconstruction task is challenging since it requires jointly reasoning about single image 3D reconstruction, correspondence between images, and the relative camera pose between images. Past work has proposed optimization-based approaches. We introduce a simpler approach, the PlaneFormer, that uses a transformer applied to 3D-aware plane tokens to perform 3D reasoning. Our experiments show that our approach is substantially more effective than prior work, and that several 3D-specific design decisions are crucial for its success.\""
  },
  "eccv2022_main_learningimplicittemplatesforpoint-basedclothedhumanmodeling": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Implicit Templates for Point-Based Clothed Human Modeling",
    "authors": [
      "Siyou Lin",
      "Hongwen Zhang",
      "Zerong Zheng",
      "Ruizhi Shao",
      "Yebin Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3747_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630211.pdf",
    "published": "2020-08",
    "summary": "\"We present FITE, a First-Implicit-Then-Explicit framework for modeling human avatars in clothing. Our framework first learns implicit surface templates representing the coarse clothing topology, and then employs the templates to guide the generation of point sets which further capture pose-dependent clothing deformations such as wrinkles. Our pipeline incorporates the merits of both implicit and explicit representations, namely, the ability to handle varying topology and the ability to efficiently capture fine details. We also propose diffused skinning to facilitate template training especially for loose clothing, and projection-based pose-encoding to extract pose information from mesh templates without predefined UV map or connectivity. Our code is publicly available at https://github.com/jsnln/fite.\""
  },
  "eccv2022_main_exploringthedevilingraphspectraldomainfor3dpointcloudattacks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks",
    "authors": [
      "Qianjiang Hu",
      "Daizong Liu",
      "Wei Hu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4378_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630230.pdf",
    "published": "2020-08",
    "summary": "\"With the maturity of depth sensors, point clouds have received increasing attention in various applications such as autonomous driving, robotics, surveillance, \\etc., while deep point cloud learning models have shown to be vulnerable to adversarial attacks. Existing attack methods generally add/delete points or perform point-wise perturbation over point clouds to generate adversarial examples in the data space, which may neglect the geometric characteristics of point clouds. Instead, we propose point cloud attacks from a new perspective---Graph Spectral Domain Attack (GSDA), aiming to perturb transform coefficients in the graph spectral domain that corresponds to varying certain geometric structure. In particular, we naturally represent a point cloud over a graph, and adaptively transform the coordinates of points into the graph spectral domain via graph Fourier transform (GFT) for compact representation. We then analyze the influence of different spectral bands on the geometric structure of the point cloud, based on which we propose to perturb the GFT coefficients in a learnable manner guided by an energy constraint loss function. Finally, the adversarial point cloud is generated by transforming the perturbed spectral representation back to the data domain via the inverse GFT (IGFT). Experimental results demonstrate the effectiveness of the proposed GSDA in terms of both imperceptibility and attack success rates under a variety of defense strategies.\""
  },
  "eccv2022_main_structure-awareeditablemorphablemodelfor3dfacialdetailanimationandmanipulation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Structure-Aware Editable Morphable Model for 3D Facial Detail Animation and Manipulation",
    "authors": [
      "Jingwang Ling",
      "Zhibo Wang",
      "Ming Lu",
      "Quan Wang",
      "Chen Qian",
      "Feng Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4426_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630248.pdf",
    "published": "2020-08",
    "summary": "\"Morphable models are essential for the statistical modeling of 3D faces. Previous works on morphable models mostly focus on large-scale facial geometry but ignore facial details. This paper augments morphable models in representing facial details by learning a Structure-aware Editable Morphable Model (SEMM). SEMM introduces a detail structure representation based on the distance field of wrinkle lines, jointly modeled with detail displacements to establish better correspondences and enable intuitive manipulation of wrinkle structure. Besides, SEMM introduces two transformation modules to translate expression blendshape weights and age values into changes in latent space, allowing effective semantic detail editing while maintaining identity. Extensive experiments demonstrate that the proposed model compactly represents facial details, outperforms previous methods in expression animation qualitatively and quantitatively, and achieves effective age editing and wrinkle line editing of facial details. Code and model are available at https://github.com/gerwang/facial-detail-manipulation.\""
  },
  "eccv2022_main_mofanerfmorphablefacialneuralradiancefield": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MoFaNeRF: Morphable Facial Neural Radiance Field",
    "authors": [
      "Yiyu Zhuang",
      "Hao Zhu",
      "Xusen Sun",
      "Xun Cao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4505_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630267.pdf",
    "published": "2020-08",
    "summary": "\"We propose a parametric model that maps free-view images into a vector space of coded facial shape, expression and appearance with a neural radiance field, namely Morphable Facial NeRF. Specifically, MoFaNeRF takes the coded facial shape, expression and appearance along with space coordinate and view direction as input to an MLP, and outputs the radiance of the space point for photo-realistic image synthesis. Compared with conventional 3D morphable models (3DMM), MoFaNeRF shows superiority in directly synthesizing photo-realistic facial details even for eyes, mouths, and beards. Also, continuous face morphing can be easily achieved by interpolating the input shape, expression and appearance codes. By introducing identity-specific modulation and texture encoder, our model synthesizes accurate photometric details and shows strong representation ability. Our model shows strong ability on multiple applications including image-based fitting, random generation, face rigging, face editing, and novel view synthesis. Experiments show that our method achieves higher representation ability than previous parametric models, and achieves competitive performance in several applications. To the best of our knowledge, our work is the first facial parametric model built upon a neural radiance field that can be used in fitting, generation and manipulation. The code and data is available at https://github.com/zhuhao-nju/mofanerf.\""
  },
  "eccv2022_main_pointinst3dsegmenting3dinstancesbypoints": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PointInst3D: Segmenting 3D Instances by Points",
    "authors": [
      "Tong He",
      "Wei Yin",
      "Chunhua Shen",
      "Anton van den Hengel"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4530_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630284.pdf",
    "published": "2020-08",
    "summary": "\"The current state-of-the-art methods in 3D instance segmentation typically involve a clustering step, despite the tendency towards heuristics, greedy algorithms, and a lack of robustness to the changes in data statistics. In contrast, we propose a fully convolutional 3D point cloud instance segmentation method that works in a per-point prediction fashion. In doing so it avoids the challenges that clustering-based methods face: introducing dependencies among different tasks of the model. We find the key to its success is assigning a suitable target to each sampled point. Instead of the commonly used static or distance-based assignment strategies, we propose to use an Optimal Transport approach to optimally assign target masks to the sampled points according to the dynamic matching costs. Our approach achieves promising results on both ScanNet and S3DIS benchmarks. The proposed approach removes intertask dependencies and thus represents a simpler and more flexible 3D instance segmentation framework than other competing methods, while achieving improved segmentation accuracy.\""
  },
  "eccv2022_main_cross-modal3dshapegenerationandmanipulation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Cross-Modal 3D Shape Generation and Manipulation",
    "authors": [
      "Zezhou Cheng",
      "Menglei Chai",
      "Jian Ren",
      "Hsin-Ying Lee",
      "Kyle Olszewski",
      "Zeng Huang",
      "Subhransu Maji",
      "Sergey Tulyakov"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4596_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630300.pdf",
    "published": "2020-08",
    "summary": "\"Creating and editing the shape and color of 3D objects require tremendous human effort and expertise. Compared to direct manipulation in 3D interfaces, 2D interactions such as sketches and scribbles are usually much more natural and intuitive for the users. In this paper, we propose a generic multi-modal generative model that couples the 2D modalities and implicit 3D representations through shared latent spaces. With the proposed model, versatile 3D generation and manipulation are enabled by simply propagating the editing from a specific 2D controlling modality through the latent spaces. For example, editing the 3D shape by drawing a sketch, re-colorizing the 3D surface via painting color scribbles on the 2D rendering, or generating 3D shapes of a certain category given one or a few reference images. Unlike prior works, our model does not require re-training or fine-tuning per editing task and is also conceptually simple, easy to implement, robust to input domain shifts, and flexible to diverse reconstruction on partial 2D inputs. We evaluate our framework on two representative 2D modalities of grayscale line sketches and rendered color images, and demonstrate that our method enables various shape manipulation and generation tasks with these 2D modalities.\""
  },
  "eccv2022_main_latentpartitionimplicitwithsurfacecodesfor3drepresentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Latent Partition Implicit with Surface Codes for 3D Representation",
    "authors": [
      "Chao Chen",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4607_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630318.pdf",
    "published": "2020-08",
    "summary": "\"Deep implicit functions have shown remarkable shape modeling ability in various 3D computer vision tasks. One drawback is that it is hard for them to represent a 3D shape as multiple parts. Current solutions learn various primitives and blend the primitives directly in the spatial space, which still struggle to approximate the 3D shape accurately. To resolve this problem, we introduce a novel implicit representation to represent a single 3D shape as a set of parts in the latent space, towards both highly accurate and plausibly interpretable shape modeling. Our insight here is that both the part learning and the part blending can be conducted much easier in the latent space than in the spatial space. We name our method Latent Partition Implicit (LPI), because of its ability of casting the global shape modeling into multiple local part modeling, which partitions the global shape unity. LPI represents a shape as Signed Distance Functions (SDFs) using surface codes. Each surface code is a latent code representing a part whose center is on the surface, which enables us to flexibly employ intrinsic attributes of shapes or additional surface properties. Eventually, LPI can reconstruct both the shape and the parts on the shape, both of which are plausible meshes. LPI is a multi-level representation, which can partition a shape into different numbers of parts after training. LPI can be learned without ground truth signed distances, point normals or any supervision for part partition. LPI outperforms the state-of-the-art methods under the widely used benchmarks in terms of reconstruction accuracy and modeling interpretability.\""
  },
  "eccv2022_main_implicitfieldsupervisionforrobustnon-rigidshapematching": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Implicit Field Supervision for Robust Non-rigid Shape Matching",
    "authors": [
      "Ramana Sundararaman",
      "Gautam Pai",
      "Maks Ovsjanikov"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4910_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630338.pdf",
    "published": "2020-08",
    "summary": "\"Establishing a correspondence between two non-rigidly deforming shapes is one of the most fundamental problems in visual computing. Existing methods often show weak resilience when presented with challenges innate to real-world data such as noise, outliers, self-occlusion etc. On the other hand, auto-decoders have demonstrated strong expressive power in learning geometrically meaningful latent embeddings. However, their use in shape analysis has been limited. In this paper, we introduce an approach based on an auto-decoder framework, that learns a continuous shape-wise deformation field over a fixed template. By supervising the deformation field for points on-surface and regularizing for points off-surface through a novel Signed Distance Regularization (SDR), we learn an alignment between the template and shape volumes. Trained on clean water-tight meshes, without any data-augmentation, we demonstrate compelling performance on compromised data and real-world scans.\""
  },
  "eccv2022_main_learningself-priorformeshdenoisingusingdualgraphconvolutionalnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Self-Prior for Mesh Denoising Using Dual Graph Convolutional Networks",
    "authors": [
      "Shota Hattori",
      "Tatsuya Yatagawa",
      "Yutaka Ohtake",
      "Hiromasa Suzuki"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4934_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630358.pdf",
    "published": "2020-08",
    "summary": "\"This study proposes a deep-learning framework for mesh denoising from a single noisy input, where two graph convolutional networks are trained jointly to filter vertex positions and facet normals apart. The prior obtained only from a single input is particularly referred to as a self-prior. The proposed method leverages the framework of the deep image prior (DIP), which obtains the self-prior for image restoration using a convolutional neural network (CNN). Thus, we obtain a denoised mesh without any ground-truth noise-free meshes. Compared to the original DIP that transforms a fixed random code into a noise-free image by the neural network, we reproduce vertex displacement from a fixed random code and reproduce facet normals from feature vectors that summarize local triangle arrangements. After tuning several hyperparameters with a few validation samples, our method achieved significantly higher performance than traditional approaches working with a single noisy input mesh. Moreover, its performance is better than the other methods using deep neural networks trained with a large-scale shape dataset. The independence of our method of either large-scale datasets or ground-truth noise-free mesh will allow us to easily denoise meshes whose shapes are rarely included in the shape datasets.\""
  },
  "eccv2022_main_diffconvanalyzingirregularpointcloudswithanirregularview": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "diffConv: Analyzing Irregular Point Clouds with an Irregular View",
    "authors": [
      "Manxi Lin",
      "Aasa Feragen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4972_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630375.pdf",
    "published": "2020-08",
    "summary": "\"Standard spatial convolutions assume input data with a regular neighborhood structure. Existing methods typically generalize convolution to the irregular point cloud domain by fixing a regular \"\"view\"\" through e.g. a fixed neighborhood size, where the convolution kernel size remains the same for each point. However, since point clouds are not as structured as images, the fixed neighbor number gives an unfortunate inductive bias. We present a novel graph convolution named Difference Graph Convolution (diffConv), which does not rely on a regular view. diffConv operates on spatially-varying and density-dilated neighborhoods, which are further adapted by a learned masked attention mechanism. Experiments show that our model is very robust to the noise, obtaining state-of-the-art performance in 3D shape classification and scene understanding tasks, along with a faster inference speed.\""
  },
  "eccv2022_main_pd-flowapointclouddenoisingframeworkwithnormalizingflows": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PD-Flow: A Point Cloud Denoising Framework with Normalizing Flows",
    "authors": [
      "Aihua Mao",
      "Zihui Du",
      "Yu-Hui Wen",
      "Jun Xuan",
      "Yong-Jin Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4988_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630392.pdf",
    "published": "2020-08",
    "summary": "\"Point cloud denoising aims to restore clean point clouds from raw observations corrupted by noise and outliers while preserving the fine-grained details. We present a novel deep learning-based denoising model, that incorporates normalizing flows and noise disentanglement techniques to achieve high denoising accuracy. Unlike the existing works that extract features of point clouds for point-wise correction, we formulate the denoising process from the perspective of distribution learning and feature disentanglement. By considering noisy point clouds as a joint distribution of clean points and noise, the denoised results can be derived from disentangling the noise counterpart from latent point representation, whereas the mapping between Euclidean and latent spaces is modeled by normalizing flows. We evaluate our method on synthesized 3D models and real-world datasets with various noise settings. Qualitative and quantitative results show that our method surpasses previous state-of-the-art deep learning-based approaches in terms of detail preservation and distribution uniformity. The source code is available at https://github.com/unknownue/pdflow.\""
  },
  "eccv2022_main_seedformerpatchseedsbasedpointcloudcompletionwithupsampletransformer": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SeedFormer: Patch Seeds Based Point Cloud Completion with Upsample Transformer",
    "authors": [
      "Haoran Zhou",
      "Yun Cao",
      "Wenqing Chu",
      "Junwei Zhu",
      "Tong Lu",
      "Ying Tai",
      "Chengjie Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5127_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630409.pdf",
    "published": "2020-08",
    "summary": "\"Point cloud completion has become increasingly popular among generation tasks of 3D point clouds, as it is a challenging yet indispensable problem to recover the complete shape of a 3D object from its partial observation. In this paper, we propose a novel SeedFormer to improve the ability of detail preservation and recovery in point cloud completion. Unlike previous methods based on a global feature vector, we introduce a new shape representation, namely Patch Seeds, which not only captures general structures from partial inputs but also preserves regional information of local patterns. Then, by integrating seed features into the generation process, we can recover faithful details for complete point clouds in a coarse-to-fine manner. Moreover, we devise an Upsample Transformer by extending the transformer structure into basic operations of point generators, which effectively incorporates spatial and semantic relationships between neighboring points. Qualitative and quantitative evaluations demonstrate that our method outperforms state-of-the-art completion networks on several benchmark datasets. Our code is available at https://github.com/hrzhou2/seedformer.\""
  },
  "eccv2022_main_deepmendlearningoccupancyfunctionstorepresentshapeforrepair": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DeepMend: Learning Occupancy Functions to Represent Shape for Repair",
    "authors": [
      "Nikolas Lamb",
      "Sean Banerjee",
      "Natasha Kholgade Banerjee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5293_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630426.pdf",
    "published": "2020-08",
    "summary": "\"We present DeepMend, a novel approach to reconstruct restorations to fractured shapes using learned occupancy functions. Existing shape repair approaches predict low-resolution voxelized restorations or smooth restorations, or require symmetries or access to a pre-existing complete oracle. We represent the occupancy of a fractured shape as the conjunction of the occupancy of an underlying complete shape and a break surface, which we model as functions of latent codes using neural networks. Given occupancy samples from a fractured shape, we estimate latent codes using an inference loss augmented with novel penalties to avoid empty or voluminous restorations. We use the estimated codes to reconstruct a restoration shape. We show results with simulated fractures on synthetic and real-world scanned objects, and with scanned real fractured mugs. Compared to existing approaches and to two baseline methods, our work shows state-of-the-art results in accuracy and avoiding restoration artifacts over non-fracture regions of the fractured shape.\""
  },
  "eccv2022_main_arepulsiveforceunitforgarmentcollisionhandlinginneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Repulsive Force Unit for Garment Collision Handling in Neural Networks",
    "authors": [
      "Qingyang Tan",
      "Yi Zhou",
      "Tuanfeng Wang",
      "Duygu Ceylan",
      "Xin Sun",
      "Dinesh Manocha"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5372_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630444.pdf",
    "published": "2020-08",
    "summary": "\"Despite recent success, deep learning-based methods for predicting 3D garment deformation under body motion suffer from interpenetration problems between the garment and the body. To address this problem, we propose a novel collision handling neural network layer called Repulsive Force Unit (ReFU). Based on the signed distance function (SDF) of the underlying body and the current garment vertex positions, ReFU predicts the per-vertex offsets that push any interpenetrating vertex to a collision-free configuration while preserving the fine geometric details. We show that ReFU is differentiable with trainable parameters and can be integrated into different network backbones that predict 3D garment deformations. Our experiments show that ReFU significantly reduces the number of collisions between the body and the garment and better preserves geometric details compared to prior methods based on collision loss or post-processing optimization.\""
  },
  "eccv2022_main_shape-posedisentanglementusingse(3)-equivariantvectorneurons": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Shape-Pose Disentanglement Using SE(3)-Equivariant Vector Neurons",
    "authors": [
      "Oren Katzir",
      "Dani Lischinski",
      "Daniel Cohen-Or"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5713_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630461.pdf",
    "published": "2020-08",
    "summary": "\"We introduce an unsupervised technique for encoding point clouds into a canonical shape representation, by disentangling shape and pose. Our encoder is stable and consistent, meaning that the shape encoding is purely pose-invariant, while the extracted rotation and translation are able to semantically align different input shapes of the same class to a common canonical pose. Specifically, we design an auto-encoder based on Vector Neuron Networks, a rotation-equivariant neural network, whose layers we extend to provide translation-equivariance in addition to rotation-equivariance only. The resulting encoder produces pose-invariant shape encoding by construction, enabling our approach to focus on learning a consistent canonical pose for a class of objects. Quantitative and qualitative experiments validate the superior stability and consistency of our approach.\""
  },
  "eccv2022_main_3dequivariantgraphimplicitfunctions": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Equivariant Graph Implicit Functions",
    "authors": [
      "Yunlu Chen",
      "Basura Fernando",
      "Hakan Bilen",
      "Matthias Nie\u00dfner",
      "Efstratios Gavves"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6067_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630477.pdf",
    "published": "2020-08",
    "summary": "\"In recent years, neural implicit representations have made remarkable progress in modeling of 3D shapes with arbitrary topology. In this work, we address two key limitations of such representations, in failing to capture local 3D geometric fine details, and to learn from and generalize to shapes with unseen 3D transformations. To this end, we introduce a novel family of graph implicit functions with equivariant layers that facilitates modeling fine local details and guaranteed robustness to various groups of geometric transformations, through local k-NN graph embeddings with sparse point set observations at multiple resolutions. Our method improves over the existing rotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet reconstruction task. We also show that our equivariant implicit function can be extended to other types of similarity transformations and generalizes to unseen translations and scaling.\""
  },
  "eccv2022_main_patchrddetail-preservingshapecompletionbylearningpatchretrievalanddeformation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PatchRD: Detail-Preserving Shape Completion by Learning Patch Retrieval and Deformation",
    "authors": [
      "Bo Sun",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Qixing Huang",
      "Siddhartha Chaudhuri"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6203_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630494.pdf",
    "published": "2020-08",
    "summary": "\"This paper introduces a data-driven shape completion approach that focuses on completing geometric details of missing regions of 3D shapes. We observe that existing generative methods do not have enough training data and representation capacity to synthesize plausible, fine-grained details with complex geometry and topology. Thus, our key insight is to copy and deform the patches from the partial input to complete the missing regions. This enables us to preserve the style of local geometric features, even if it is drastically different from the training data. Our fully automatic approach proceeds in two stages. First, we learn to retrieve candidate patches from the input shape. Second, we select and deform some of the retrieved candidates to seamlessly blend them into the complete shape. This method combines the advantages of the two most common completion methods: similarity-based single-instance completion, and completion by learning a shape space. We leverage repeating patterns by retrieving patches from the partial input, and learn global structural priors by using a neural network to guide the retrieval and deformation steps. Experimental results show that our approach considerably outperforms baseline approaches across multiple datasets and shape categories.\""
  },
  "eccv2022_main_3dshapesequenceofhumancomparisonandclassificationusingcurrentandvarifolds": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Shape Sequence of Human Comparison and Classification Using Current and Varifolds",
    "authors": [
      "Emery Pierson",
      "Mohamed Daoudi",
      "Sylvain Arguillere"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6963_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630514.pdf",
    "published": "2020-08",
    "summary": "\"In this paper we address the task of the comparison and the classification of 3D shape sequences of human. The non-linear dynamics of the human motion and the changing of the surface parametrization over the time make this task very challenging. To tackle this issue, we propose to embed the 3D shape sequences in an infinite dimensional space, the space of varifolds, endowed with an inner product that comes from a given positive definite kernel. More specifically, our approach involves two steps: 1) the surfaces are represented as varifolds, this representation induces metrics equivariant to rigid motions and invariant to parametrization; 2) the sequences of 3D shapes are represented by Gram matrices derived from their infinite dimensional Hankel matrices, and we use Frobenius distance between two Symmetric Positive definite (SPD) matrices to compare two sequences. Extensive experiments show that our method is competitive with state-of-the-art in 3D sequence motion retrieval.\""
  },
  "eccv2022_main_conditional-flownerfaccurate3dmodellingwithreliableuncertaintyquantification": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty Quantification",
    "authors": [
      "Jianxiong Shen",
      "Antonio Agudo",
      "Francesc Moreno-Noguer",
      "Adria Ruiz"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7028_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630531.pdf",
    "published": "2020-08",
    "summary": "\"A critical limitation of current methods based on Neural Radiance Fields (NeRF) is that they are unable to quantify the uncertainty associated with the learned appearance and geometry of the scene. This information is paramount in real applications such as medical diagnosis or autonomous driving where, to reduce potentially catastrophic failures, the confidence on the model outputs must be included into the decision-making process. In this context, we introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to incorporate uncertainty quantification into NeRF-based approaches. For this purpose, our method learns a distribution over all possible radiance fields modelling the scene which is used to quantify the uncertainty associated with the modelled scene. In contrast to previous approaches enforcing strong constraints over the radiance field distribution, CF-NeRF learns it in a flexible and fully data-driven manner by coupling Latent Variable Modelling and Conditional Normalizing Flows. This strategy allows to obtain reliable uncertainty estimation while preserving model expressivity. Compared to previous state-of-the-art methods proposed for uncertainty quantification in NeRF, our experiments show that the proposed method achieves significantly lower prediction errors and more reliable uncertainty values for synthetic novel view and depth-map estimation.\""
  },
  "eccv2022_main_unsupervisedpose-awarepartdecompositionforman-madearticulatedobjects": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Unsupervised Pose-Aware Part Decomposition for Man-Made Articulated Objects",
    "authors": [
      "Yuki Kawana",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7414_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630549.pdf",
    "published": "2020-08",
    "summary": "\"Man-made articulated objects exist widely in the real world. However, previous methods for unsupervised part decomposition are unsuitable for such objects because they assume a spatially fixed part location, resulting in inconsistent part parsing. In this paper, we propose PPD (unsupervised Pose-aware Part Decomposition) to address a novel setting that explicitly targets man-made articulated objects with mechanical joints, considering the part poses in part parsing. As an analysis-by-synthesis approach, We show that category-common prior learning for both part shapes and poses facilitates the unsupervised learning of (1) part parsing with abstracted part shapes, and (2) part poses as joint parameters under single-frame shape supervision. We evaluate our method on synthetic and real datasets, and we show that it outperforms previous works in consistent part parsing of the articulated objects based on comparable part pose estimation performance to the supervised baseline.\""
  },
  "eccv2022_main_meshudffastanddifferentiablemeshingofunsigneddistancefieldnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MeshUDF: Fast and Differentiable Meshing of Unsigned Distance Field Networks",
    "authors": [
      "Beno\u00eet Guillard",
      "Federico Stella",
      "Pascal Fua"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7511_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630566.pdf",
    "published": "2020-08",
    "summary": "\"Unsigned Distance Fields (UDFs) can be used to represent non-watertight surfaces. However, current approaches to converting them into explicit meshes tend to either be expensive or to degrade the accuracy. Here, we extend the marching cube algorithm to handle UDFs, both fast and accurately. Moreover, our approach to surface extraction is differentiable, which is key to using pretrained UDF networks to fit sparse data.\""
  },
  "eccv2022_main_spe-netboostingpointcloudanalysisviarotationrobustnessenhancement": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SPE-Net: Boosting Point Cloud Analysis via Rotation Robustness Enhancement",
    "authors": [
      "Zhaofan Qiu",
      "Yehao Li",
      "Yu Wang",
      "Yingwei Pan",
      "Ting Yao",
      "Tao Mei"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7651_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630582.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we propose a novel deep architecture tailored for 3D point cloud applications, named as SPE-Net. The embedded \"\"Selective Position Encoding (SPE)\"\" procedure relies on an attention mechanism that can effectively attend to the underlying rotation condition of the input. Such encoded rotation condition then determines which part of the network parameters to be focused on, and is shown to efficiently help reduce the degree of freedom of the optimization during training. This mechanism henceforth can better leverage the rotation augmentations through reduced training difficulties, making SPE-Net robust against rotated data both during training and testing. The new findings in our paper also urge us to rethink the relationship between the extracted rotation information and the actual test accuracy. Intriguingly, we reveal evidences that by locally encoding the rotation information through SPE-Net, the rotation-invariant features are still of critical importance in benefiting the test samples without any actual global rotation. We empirically demonstrate the merits of the SPE-Net and the associated hypothesis on four benchmarks, showing evident improvements on both rotated and unrotated test data over SOTA methods. Source code is available at https://github.com/ZhaofanQiu/SPE-Net.\""
  },
  "eccv2022_main_theshapepartslotmachinecontact-basedreasoningforgenerating3dshapesfromparts": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "The Shape Part Slot Machine: Contact-Based Reasoning for Generating 3D Shapes from Parts",
    "authors": [
      "Kai Wang",
      "Paul Guerrero",
      "Vladimir G. Kim",
      "Siddhartha Chaudhuri",
      "Minhyuk Sung",
      "Daniel Ritchie"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7704_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630599.pdf",
    "published": "2020-08",
    "summary": "\"We present the Shape Part Slot Machine, a new method for assembling novel 3D shapes from existing parts by performing contact-based reasoning. Our method represents each shape as a graph of \"\"slots,\"\" where each slot is a region of contact between two shape parts. Based on this representation, we design a graph-neural-network-based model for generating new slot graphs and retrieving compatible parts, as well as a gradient-descent-based optimization scheme for assembling the retrieved parts into a complete shape that respects the generated slot graph. This approach does not require any semantic part labels; interestingly, it also does not require complete part geometries---reasoning about the regions where parts connect proves sufficient to generate novel, high-quality 3D shapes. We demonstrate that our method generates shapes that outperform existing modeling-by-assembly approaches in terms of quality, diversity, and structural complexity.\""
  },
  "eccv2022_main_spatiotemporalself-attentionmodelingwithtemporalpatchshiftforactionrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Spatiotemporal Self-Attention Modeling with Temporal Patch Shift for Action Recognition",
    "authors": [
      "Wangmeng Xiang",
      "Chao Li",
      "Biao Wang",
      "Xihan Wei",
      "Xian-Sheng Hua",
      "Lei Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/334_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630615.pdf",
    "published": "2020-08",
    "summary": "\"Transformer-based methods have recently achieved great advancement on 2D image-based vision tasks. For 3D video-based tasks such as action recognition, however, directly applying spatiotemporal transformers on video data will bring heavy computation and memory burdens due to the largely increased number of patches and the quadratic complexity of self-attention computation. How to efficiently and effectively model the 3D self-attention of video data has been a great challenge for transformers. In this paper, we propose a Temporal Patch Shift (TPS) method for efficient 3D self-attention modeling in transformers for video-based action recognition. TPS shifts part of patches with a specific mosaic pattern in the temporal dimension, thus converting a vanilla spatial self-attention operation to a spatiotemporal one with little additional cost. As a result, we can compute 3D self-attention using nearly the same computation and memory cost as 2D self-attention. TPS is a plug-and-play module and can be inserted into existing 2D transformer models to enhance spatiotemporal feature learning. The proposed method achieves competitive performance with state-of-the-arts on Something-something V1 \\& V2, Diving-48, and Kinetics400 while being much more efficient on computation and memory cost. The source code of TPS can be found at https://github.com/MartinXM/TPS.\""
  },
  "eccv2022_main_proposal-freetemporalactiondetectionviaglobalsegmentationmasklearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning",
    "authors": [
      "Sauradip Nag",
      "Xiatian Zhu",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/977_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630632.pdf",
    "published": "2020-08",
    "summary": "\"Existing temporal action detection (TAD) methods rely on generating an overwhelmingly large number of proposals per video. This leads to complex model designs due to proposal generation and/or per-proposal action instance evaluation and the resultant high computational cost. In this work, for the first time, we propose a proposal-free Temporal Action detection model with Global Segmentation mask (TAGS). Our core idea is to learn a global segmentation mask of each action instance jointly at the full video length. The TAGS model differs significantly from the conventional proposal-based methods by focusing on global temporal representation learning to directly detect local start and end points of action instances without proposals. Further, by modeling TAD holistically rather than locally at the individual proposal level, TAGS needs a much simpler model architecture with lower computational cost. Extensive experiments show that despite its simpler design, TAGS outperforms existing TAD methods, achieving new state-of-the-art performance on two benchmarks. Importantly, it is 20x faster to train and 1.6x more efficient for inference. Our PyTorch implementation of TAGS is available at https://github.com/sauradip/TAGS\""
  },
  "eccv2022_main_semi-supervisedtemporalactiondetectionwithproposal-freemasking": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Semi-Supervised Temporal Action Detection with Proposal-Free Masking",
    "authors": [
      "Sauradip Nag",
      "Xiatian Zhu",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1003_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630649.pdf",
    "published": "2020-08",
    "summary": "\"Existing temporal action detection (TAD) methods rely on a large number of training data with segment-level annotations. Collecting and annotating such a training set is thus highly expensive and unscalable. Semi-supervised TAD (SS-TAD) alleviates this problem by leveraging unlabeled videos freely available at scale. However, SS-TAD is also a much more challenging problem than supervised TAD, and consequently much under-studied. Prior SS-TAD methods directly combine an existing proposal-based TAD method and a SSL method. Due to their sequential localization (e.g, proposal generation) and classification design, they are prone to proposal error propagation. To overcome this limitation, in this work we propose a novel Semi-supervised Temporal action detection model based on PropOsal-free Temporal mask (SPOT) with a parallel localization (mask generation) and classification architecture. Such a novel design effectively eliminates the dependence between localization and classification by cutting off the route for error propagation in-between. We further introduce an interaction mechanism between classification and localization for prediction refinement, and a new pretext task for self-supervised model pre-training. Extensive experiments on two standard benchmarks show that our SPOT outperforms state-of-the-art alternatives, often by a large margin. The PyTorch implementation of SPOT is available at https://github.com/sauradip/SPOT\""
  },
  "eccv2022_main_zero-shottemporalactiondetectionviavision-languageprompting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Zero-Shot Temporal Action Detection via Vision-Language Prompting",
    "authors": [
      "Sauradip Nag",
      "Xiatian Zhu",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1010_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630667.pdf",
    "published": "2020-08",
    "summary": "\"Existing temporal action detection (TAD) methods rely on large training data including segment-level annotations, limited to recognizing previously seen classes alone during inference. Collecting and annotating a large training set for each class of interest is costly and hence unscalable. Zero-shot TAD (ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize any unseen action classes. Meanwhile, ZS-TAD is also much more challenging with significantly less investigation. Inspired by the success of zero-shot image classification aided by vision-language (ViL) models such as CLIP, we aim to tackle the more complex TAD task. An intuitive method is to integrate an off-the-shelf proposal detector with CLIP style classification. However, due to the sequential localization (e.g, proposal generation) and classification design, it is prone to localization error propagation. To overcome this problem, in this paper we propose a novel zero-Shot Temporal Action detection model via vision-LanguagE prompting (STALE). Such a novel design effectively eliminates the dependence between localization and classification by breaking the route for error propagation in-between. We further introduce an interaction mechanism between classification and localization for improved optimization. Extensive experiments on standard ZS-TAD video benchmarks show that our STALE significantly outperforms state-of-the-art alternatives. Besides, our model also yields superior results on supervised TAD over recent strong competitors. The PyTorch implementation of STALE is available on https://github.com/sauradip/STALE.\""
  },
  "eccv2022_main_cycdaunsupervisedcycledomainadaptationtolearnfromimagetovideo": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CycDA: Unsupervised Cycle Domain Adaptation to Learn from Image to Video",
    "authors": [
      "Wei Lin",
      "Anna Kukleva",
      "Kunyang Sun",
      "Horst Possegger",
      "Hilde Kuehne",
      "Horst Bischof"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1118_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630684.pdf",
    "published": "2020-08",
    "summary": "\"Although action recognition has achieved impressive results over recent years, both collection and annotation of video training data are still time-consuming and cost intensive. Therefore, image-to-video adaptation has been proposed to exploit labeling-free web image source for adapting on unlabeled target videos. This poses two major challenges: (1) spatial domain shift between web images and video frames; (2) modality gap between image and video data. To address these challenges, we propose Cycle Domain Adaptation (CycDA), a cycle-based approach for unsupervised image-to-video domain adaptation. We leverage the joint spatial information in images and videos on the one hand and, on the other hand, train an independent spatio-temporal model to bridge the modality gap. We alternate between the spatial and spatio-temporal learning with knowledge transfer between the two in each cycle. We evaluate our approach on benchmark datasets for image-to-video as well as for mixed-source domain adaptation achieving state-of-the-art results and demonstrating the benefits of our cyclic adaptation.\""
  },
  "eccv2022_main_s2nsuppression-strengthennetworkforevent-basedrecognitionundervariantilluminations": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "S2N: Suppression-Strengthen Network for Event-Based Recognition under Variant Illuminations",
    "authors": [
      "Zengyu Wan",
      "Yang Wang",
      "Ganchao Tan",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1155_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630701.pdf",
    "published": "2020-08",
    "summary": "\"The emerging event-based sensors have demonstrated out-standing potential in visual tasks thanks to their high speed and high dynamic range. However, the event degradation due to imaging under low illumination obscures the correlation between event signals and brings uncertainty into event representation. Targeting this issue, we present a novel suppression-strengthen network (S2N) to augment the event feature representation after suppressing the influence of degradation. Specifically, a suppression sub-network is devised to obtain intensity mapping between the degraded and denoised enhancement frames by unsupervised learning. To further restrain the degradation\u2019s influence, a strengthen sub-network is presented to generate robust event representation by adaptively perceiving the local variations between the center and surrounding regions. After being trained on a single illumination condition, our S2N can be directly generalized to other illuminations to boost the recognition performance. Experimental results on three challenging recognition tasks demonstrate the superiority of our method. The codes and datasets could refer to https://github.com/wanzengy/S2N-Suppression-Strengthen-Network.\""
  },
  "eccv2022_main_cmdself-supervised3dactionrepresentationlearningwithcross-modalmutualdistillation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CMD: Self-Supervised 3D Action Representation Learning with Cross-Modal Mutual Distillation",
    "authors": [
      "Yunyao Mao",
      "Wengang Zhou",
      "Zhenbo Lu",
      "Jiajun Deng",
      "Houqiang Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1212_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630719.pdf",
    "published": "2020-08",
    "summary": "\"In 3D action recognition, there exists rich complementary information between skeleton modalities. Nevertheless, how to model and utilize this information remains a challenging problem for self-supervised 3D action representation learning. In this work, we formulate the cross-modal interaction as a bidirectional knowledge distillation problem. Different from classic distillation solutions that transfer the knowledge of a fixed and pre-trained teacher to the student, in this work, the knowledge is continuously updated and bidirectionally distilled between modalities. To this end, we propose a new Cross-modal Mutual Distillation (CMD) framework with the following designs. On the one hand, the neighboring similarity distribution is introduced to model the knowledge learned in each modality, where the relational information is naturally suitable for the contrastive frameworks. On the other hand, asymmetrical configurations are used for teacher and student to stabilize the distillation process and to transfer high-confidence information between modalities. By derivation, we find that the cross-modal positive mining in previous works can be regarded as a degenerated version of our CMD. We perform extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets. Our approach outperforms existing self-supervised methods and sets a series of new records. The code is available at https://github.com/maoyunyao/CMD\""
  },
  "eccv2022_main_expandinglanguage-imagepretrainedmodelsforgeneralvideorecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Expanding Language-Image Pretrained Models for General Video Recognition",
    "authors": [
      "Bolin Ni",
      "Houwen Peng",
      "Minghao Chen",
      "Songyang Zhang",
      "Gaofeng Meng",
      "Jianlong Fu",
      "Shiming Xiang",
      "Haibin Ling"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1398_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640001.pdf",
    "published": "2020-08",
    "summary": "\"Contrastive language-image pretraining has shown great success in learning visual-textual joint representation from web-scale data, demonstrating remarkable \u201czero-shot\u201d generalization ability for various image tasks. However, how to effectively expand such new language-image pretraining methods to video domains is still an open problem. In this work, we present a simple yet effective approach that adapts the pretrained language-image models to video recognition directly, instead of pretraining a new model from scratch. More concretely, to capture the long-range dependencies of frames along the temporal dimension, we propose a cross-frame attention mechanism that explicitly exchanges information across frames. Such module is lightweight and can be plugged into pretrained language-image models seamlessly. Moreover, we propose a video-specific prompting scheme, which leverages video content information for generating discriminative textual prompts. Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios. In particular, under fully-supervised settings, our approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using 12\u00d7 fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot experiments, our approach surpasses the current state-of-the-art methods by +7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In few-shot scenarios, our approach outperforms previous best methods by +32.1% and +23.1% when the labeled data is extremely limited. Code and models are available at https://github.com/microsoft/VideoX/tree/master/X-CLIP\""
  },
  "eccv2022_main_huntinggroupclueswithtransformersforsocialgroupactivityrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Hunting Group Clues with Transformers for Social Group Activity Recognition",
    "authors": [
      "Masato Tamura",
      "Rahul Vishwakarma",
      "Ravigopal Vennelakanti"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1482_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640018.pdf",
    "published": "2020-08",
    "summary": "\"This paper presents a novel framework for social group activity recognition. As an expanded task of group activity recognition, social group activity recognition requires recognizing multiple sub-group activities and identifying group members. Most existing methods tackle both tasks by refining region features and then summarizing them into activity features. Such heuristic feature design renders the effectiveness of features susceptible to incomplete person localization and disregards the importance of scene contexts. Furthermore, region features are sub-optimal to identify group members because the features may be dominated by those of people in the regions and have different semantics. To overcome these drawbacks, we propose to leverage attention modules in transformers to generate effective social group features. Our method is designed in such a way that the attention modules identify and then aggregate features relevant to social group activities, generating an effective feature for each social group. Group member information is embedded into the features and thus accessed by feed-forward networks. The outputs of feed-forward networks represent groups so concisely that group members can be identified with simple Hungarian matching between groups and individuals. Experimental results show that our method outperforms state-of-the-art methods on the Volleyball and Collective Activity datasets.\""
  },
  "eccv2022_main_contrastivepositiveminingforunsupervised3dactionrepresentationlearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Contrastive Positive Mining for Unsupervised 3D Action Representation Learning",
    "authors": [
      "Haoyuan Zhang",
      "Yonghong Hou",
      "Wenjing Zhang",
      "Wanqing Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1578_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640035.pdf",
    "published": "2020-08",
    "summary": "\"Recent contrastive based 3D action representation learning has made great progress. However, the strict positive/negative constraint is yet to be relaxed and the use of non-self positive is yet to be explored. In this paper, a Contrastive Positive Mining (CPM) framework is proposed for unsupervised skeleton 3D action representation learning. The CPM identifies non-self positives in a contextual queue to boost learning. Specifically, the siamese encoders are adopted and trained to match the similarity distributions of the augmented instances in reference to all instances in the contextual queue. By identifying the non-self positive instances in the queue, a positive-enhanced learning strategy is proposed to leverage the knowledge of mined positives to boost the robustness of the learned latent space against intra-class and inter-class diversity. Experimental results have shown that the proposed CPM is effective and outperforms the existing state-of-the-art unsupervised methods on the challenging NTU and PKU-MMD datasets.\""
  },
  "eccv2022_main_target-absenthumanattention": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Target-Absent Human Attention",
    "authors": [
      "Zhibo Yang",
      "Sounak Mondal",
      "Seoyoung Ahn",
      "Gregory Zelinsky",
      "Minh Hoai",
      "Dimitris Samaras"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2104_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640051.pdf",
    "published": "2020-08",
    "summary": "\"The prediction of human gaze behavior is important for building human-computer interactive systems that can anticipate a user\u2019s attention. Computer vision models have been developed to predict the fixations made by people as they search for target objects. But what about when the image has no target? Equally important is to know how people search when they cannot find a target, and when they would stop searching. In this paper, we propose a data-driven computational model that addresses the search-termination problem and predicts the scanpath of search fixations made by people searching for targets that do not appear in images. We model visual search as an imitation learning problem and represent the internal knowledge that the viewer acquires through fixations using a novel state representation that we call Foveated Feature Maps (FFMs). FFMs integrate a simulated foveated retina into a pretrained ConvNet that produces an in-network feature pyramid, all with minimal computational overhead. Our method integrates FFMs as the state representation in inverse reinforcement learning. Experimentally, we improve the state of the art in predicting human target-absent search behavior on the COCO-Search18 dataset.\""
  },
  "eccv2022_main_uncertainty-basedspatial-temporalattentionforonlineactiondetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Uncertainty-Based Spatial-Temporal Attention for Online Action Detection",
    "authors": [
      "Hongji Guo",
      "Zhou Ren",
      "Yi Wu",
      "Gang Hua",
      "Qiang Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2138_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640068.pdf",
    "published": "2020-08",
    "summary": "\"Online action detection aims at detecting the ongoing action in a streaming video. In this paper, we proposed an uncertainty-based spatial-temporal attention for online action detection. By explicitly modeling the distribution of model parameters, we extend the baseline models in a probabilistic manner. Then we quantify the predictive uncertainty and use it to generate spatial-temporal attention that focus on large mutual information regions and frames. For inference, we introduce a two-stream framework that combines the baseline model and the probabilistic model based on the input uncertainty. We validate the effectiveness of our method on three benchmark datasets: THUMOS-14, TVSeries, and HDD. Furthermore, we demonstrate that our method generalizes better under different views and occlusions, and is more robust when training with small-scale data.\""
  },
  "eccv2022_main_iwinhuman-objectinteractiondetectionviatransformerwithirregularwindows": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Iwin: Human-Object Interaction Detection via Transformer with Irregular Windows",
    "authors": [
      "Danyang Tu",
      "Xiongkuo Min",
      "Huiyu Duan",
      "Guodong Guo",
      "Guangtao Zhai",
      "Wei Shen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2219_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640085.pdf",
    "published": "2020-08",
    "summary": "\"This paper presents a new vision Transformer, named Iwin Transformer, which is specifically designed for human-object interaction (HOI) detection, a detailed scene understanding task involving a sequential process of human/object detection and interaction recognition. Iwin Transformer is a hierarchical Transformer which progressively performs token representation learning and token agglomeration within irregular windows. The irregular windows, achieved by augmenting regular grid locations with learned offsets, 1) eliminate redundancy in token representation learning, which leads to efficient humans/objects detection, and 2) enable the agglomerated tokens to align with humans/objects with different shapes, which facilitates the acquisition of highly-abstracted visual semantics for interaction recognition. The effectiveness and efficiency of Iwin Transformer are verified on the two standard HOI detection benchmark datasets, HICO-DET and V-COCO. Results show our method outperforms existing Transformers-based methods by large margins (3.7 mAP gain on HICO-DET and 2.0 mAP gain on V-COCO) with fewer training epochs (0.5 \u00d7).\""
  },
  "eccv2022_main_rethinkingzero-shotactionrecognitionlearningfromlatentatomicactions": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Rethinking Zero-Shot Action Recognition: Learning from Latent Atomic Actions",
    "authors": [
      "Yijun Qian",
      "Lijun Yu",
      "Wenhe Liu",
      "Alexander G. Hauptmann"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2233_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640102.pdf",
    "published": "2020-08",
    "summary": "\"To avoid the time-consuming annotating and retraining cycle in applying supervised action recognition models, Zero-Shot Action Recognition (ZSAR) has become a thriving direction. ZSAR requires models to recognize actions that never appear in the training set through bridging visual features and semantic representations. However, due to the complexity of actions, it remains challenging to transfer knowledge learned from source to target action domains. Previous ZSAR methods mainly focus on mitigating representation variance between source and target actions through integrating or applying new action-level features. However, the action-level features are coarse-grained and make the learned one-to-one bridge fragile to similar target actions. Meanwhile, integration or application of features usually requires extra computation or annotation. These methods didn\u2019t notice that two actions with different names may still share the same atomic action components. It enables humans to quickly understand an unseen action given a bunch of atomic actions learned from seen actions. Inspired by this, we propose Jigsaw Network (JigsawNet) which recognizes complex actions through unsupervisedly decomposing them into combinations of atomic actions and bridging group-to-group relationships between visual features and semantic representations. To enhance the robustness of the learned group-to-group bridge, we propose Group Excitation (GE) module to model intra-sample knowledge and Consistency Loss to enforce the model learn from inter-sample knowledge. Our JigsawNet achieves state-of-the-art performance on three benchmarks and surpasses previous works with noticeable margins.\""
  },
  "eccv2022_main_miningcross-personcuesforbody-partinteractivenesslearninginhoidetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection",
    "authors": [
      "Xiaoqian Wu",
      "Yong-Lu Li",
      "Xinpeng Liu",
      "Junyi Zhang",
      "Yuzhe Wu",
      "Cewu Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2328_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640119.pdf",
    "published": "2020-08",
    "summary": "\"Human-Object Interaction (HOI) detection plays a crucial role in activity understanding. Though significant progress has been made, interactiveness learning remains a challenging problem in HOI detection: existing methods usually generate redundant negative H-O pair proposals and fail to effectively extract interactive pairs. Though interactiveness has been studied in both whole body- and part- level and facilitates the H-O pairing, previous works only focus on the target person once (i.e., in a local perspective) and overlook the information of the other persons. In this paper, we argue that comparing body-parts of multi-person simultaneously can afford us more useful and supplementary interactiveness cues. That said, to learn body-part interactiveness from a global perspective: when classifying a target person\u2019s body-part interactiveness, visual cues are explored not only from herself/himself but also from other persons in the image. We construct body-part saliency maps based on self-attention to mine cross-person informative cues and learn the holistic relationships between all the body-parts. We evaluate the proposed method on widely-used benchmarks HICO-DET and V-COCO. With our new perspective, the holistic global-local body-part interactiveness learning achieves significant improvements over state-of-the-art. Our code is available at https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness.\""
  },
  "eccv2022_main_collaboratingdomain-sharedandtarget-specificfeatureclusteringforcross-domain3dactionrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Collaborating Domain-Shared and Target-Specific Feature Clustering for Cross-Domain 3D Action Recognition",
    "authors": [
      "Qinying Liu",
      "Zilei Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2404_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640135.pdf",
    "published": "2020-08",
    "summary": "\"In this work, we consider the problem of cross-domain 3D action recognition in the open-set setting, which has been rarely explored before. Specifically, there is a source domain and a target domain that contain the skeleton sequences with different styles and categories, and our purpose is to cluster the target data by utilizing the labeled source data and unlabeled target data. For such a challenging task, this paper presents a novel approach dubbed CoDT to collaboratively cluster the domain-shared features and target-specific features. CoDT consists of two parallel branches. One branch aims to learn domain-shared features with supervised learning in the source domain, while the other is to learn target-specific features using contrastive learning in the target domain. To cluster the features, we propose an online clustering algorithm that enables simultaneous promotion of robust pseudo label generation and feature clustering. Furthermore, to leverage the complementarity of domain-shared features and target-specific features, we propose a novel collaborative clustering strategy to enforce pair-wise relationship consistency between the two branches. We conduct extensive experiments on multiple cross-domain 3D action recognition datasets, and the results demonstrate the effectiveness of our method.\""
  },
  "eccv2022_main_isappearancefreeactionrecognitionpossible?": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Is Appearance Free Action Recognition Possible?",
    "authors": [
      "Filip Ilic",
      "Thomas Pock",
      "Richard P. Wildes"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2545_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640154.pdf",
    "published": "2020-08",
    "summary": "\"Intuition might suggest that motion and dynamic information are key to video-based action recognition. In contrast, there is evidence that state-of-the-art deep-learning video understanding architectures are biased toward static information available in single frames. Presently, a methodology and corresponding dataset to isolate the effects of dynamic information in video are missing. Their absence makes it difficult to understand how well contemporary architectures capitalize on dynamic vs. static information. We respond with a novel Appearance Free Dataset (AFD) for action recognition. AFD is devoid of static information relevant to action recognition in a single frame. Modeling of the dynamics is necessary for solving the task, as the action is only apparent through consideration of the temporal dimension. We evaluated 11 contemporary action recognition architectures on AFD as well as its related RGB video. Our results show a notable decrease in performance for all architectures on AFD compared to RGB. We also conducted a complimentary study with humans that shows their recognition accuracy on AFD and RGB is very similar and much better than the evaluated architectures on AFD. Our results motivate a novel architecture that revives explicit recovery of optical flow, within a contemporary design for best performance on AFD and RGB.\""
  },
  "eccv2022_main_learningspatial-preservedskeletonrepresentationsforfew-shotactionrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Spatial-Preserved Skeleton Representations for Few-Shot Action Recognition",
    "authors": [
      "Ning Ma",
      "Hongyi Zhang",
      "Xuhui Li",
      "Sheng Zhou",
      "Zhen Zhang",
      "Jun Wen",
      "Haifeng Li",
      "Jingjun Gu",
      "Jiajun Bu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3345_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640172.pdf",
    "published": "2020-08",
    "summary": "\"Few-shot action recognition aims to recognize few-labeled novel action classes and attracts growing attentions due to practical significance. Human skeletons provide explainable and data-efficient representation for this problem by explicitly modeling spatial-temporal relations among skeleton joints. However, existing skeleton-based spatial-temporal models tend to deteriorate the positional distinguishability of joints, which leads to fuzzy spatial matching and poor explainability. To address these issues, we propose a novel spatial matching strategy consisting of spatial disentanglement and spatial activation. The motivation behind spatial disentanglement is that we find more spatial information for leaf nodes (e.g., the \u201chand\u201d joint ) is beneficial to increase representation diversity for skeleton matching. To achieve spatial disentanglement, we encourage the skeletons to be represented in a full rank space with rank maximization constraint. Finally, an attention based spatial activation mechanism is introduced to incorporate the disentanglement, by adaptively adjusting the disentangled joints according to matching pairs. Extensive experiments on three skeleton benchmarks demonstrate that the proposed spatial matching strategy can be effectively inserted into existing temporal alignment frameworks, achieving considerable performance improvements as well as inherent explainability.\""
  },
  "eccv2022_main_dual-evidentiallearningforweakly-supervisedtemporalactionlocalization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Dual-Evidential Learning for Weakly-Supervised Temporal Action Localization",
    "authors": [
      "Mengyuan Chen",
      "Junyu Gao",
      "Shicai Yang",
      "Changsheng Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3504_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640190.pdf",
    "published": "2020-08",
    "summary": "\"Weakly-supervised temporal action localization (WS-TAL) aims to localize the action instances and recognize their categories with only video-level labels. Despite great progress, existing methods suffer from severe action-background ambiguity, which mainly comes from background noise introduced by aggregation operations and large intra-action variations caused by the task gap between classification and localization. To address this issue, we propose a generalized evidential deep learning (EDL) framework for WS-TAL, called Dual-Evidential Learning for Uncertainty modeling (DELU), which extends the traditional paradigm of EDL to adapt to the weakly-supervised multi-label classification goal. Specifically, targeting at adaptively excluding the undesirable background snippets, we utilize the video-level uncertainty to measure the interference of background noise to video-level prediction. Then, the snippet-level uncertainty is further induced for progressive learning, which gradually focuses on the entire action instances in an \u201ceasy-to-hard\u201d manner. Extensive experiments show that DELU achieves state-of-the-art performance on THUMOS14 and ActivityNet1.2 benchmarks. Our code is available in github.com/MengyuanChen21/ECCV2022-DELU.\""
  },
  "eccv2022_main_global-localmotiontransformerforunsupervisedskeleton-basedactionlearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Global-Local Motion Transformer for Unsupervised Skeleton-Based Action Learning",
    "authors": [
      "Boeun Kim",
      "Hyung Jin Chang",
      "Jungho Kim",
      "Jin Young Choi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4076_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640207.pdf",
    "published": "2020-08",
    "summary": "\"We propose a new transformer model for the task of unsupervised learning of skeleton motion sequences. The existing transformer model utilized for unsupervised skeleton-based action learning is learned the instantaneous velocity of each joint from adjacent frames without global motion information. Thus, the model has difficulties in learning the attention globally over whole-body motions and temporally distant joints. In addition, person-to-person interactions have not been considered in the model. To tackle the learning of whole-body motion, long-range temporal dynamics, and person-to-person interactions, we design a global and local attention mechanism, where, global body motions and local joint motions pay attention to each other. In addition, we propose a novel pretraining strategy, multi-interval pose displacement prediction, to learn both global and local attention in diverse time ranges. The proposed model successfully learns local dynamics of the joints and captures global context from the motion sequences. Our model outperforms state-of-the-art models by notable margins in the representative benchmarks.\""
  },
  "eccv2022_main_adafocusv3onunifiedspatial-temporaldynamicvideorecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AdaFocusV3: On Unified Spatial-Temporal Dynamic Video Recognition",
    "authors": [
      "Yulin Wang",
      "Yang Yue",
      "Xinhong Xu",
      "Ali Hassani",
      "Victor Kulikov",
      "Nikita Orlov",
      "Shiji Song",
      "Humphrey Shi",
      "Gao Huang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4120_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640224.pdf",
    "published": "2020-08",
    "summary": "\"Recent research has revealed that reducing the temporal and spatial redundancy are both effective approaches towards efficient video recognition, e.g., allocating the majority of computation to a task-relevant subset of frames or the most valuable image regions of each frame. However, in most existing works, either type of redundancy is typically modeled with another absent. This paper explores the unified formulation of spatial-temporal dynamic computation on top of the recently proposed AdaFocusV2 algorithm, contributing to an improved AdaFocusV3 framework. Our method reduces the computational cost by activating the expensive high-capacity network only on some small but informative 3D video cubes. These cubes are cropped from the space formed by frame height, width, and video duration, while their locations are adaptively determined with a light-weighted policy network on a per-sample basis. At test time, the number of the cubes corresponding to each video is dynamically configured, i.e., video cubes are processed sequentially until a sufficiently reliable prediction is produced. Notably, AdaFocusV3 can be effectively trained by approximating the non-differentiable cropping operation with the interpolation of deep features. Extensive empirical results on six benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2 and Diving48) demonstrate that our model is considerably more efficient than competitive baselines.\""
  },
  "eccv2022_main_panoramichumanactivityrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Panoramic Human Activity Recognition",
    "authors": [
      "Ruize Han",
      "Haomin Yan",
      "Jiacheng Li",
      "Songmiao Wang",
      "Wei Feng",
      "Song Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4477_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640242.pdf",
    "published": "2020-08",
    "summary": "\"To obtain a more comprehensive activity understanding for a crowded scene, in this paper, we propose a new problem of panoramic human activity recognition (PAR), which aims to simultaneously achieve the the recognition of individual actions, social group activities, and global activities. This is a challenging yet practical problem in real-world applications. To track this problem, we develop a novel hierarchical graph neural network to progressively represent and model the multi-granular human activities and mutual social relations for a crowd of people. We further build a benchmark to evaluate the proposed method and other related methods. Experimental results verify the rationality of the proposed PAR problem, the effectiveness of our method and the usefulness of the benchmark. We have released the source code and benchmark to the public for promoting the study on this problem.\""
  },
  "eccv2022_main_delvingintodetailssynopsis-to-detailnetworksforvideorecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Delving into Details: Synopsis-to-Detail Networks for Video Recognition",
    "authors": [
      "Shuxian Liang",
      "Xu Shen",
      "Jianqiang Huang",
      "Xian-Sheng Hua"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4539_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640259.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we explore the details in video recognition with the aim to improve the accuracy. It is observed that most failure cases in recent works fall on the mis-classifications among very similar actions (such as high kick vs. side kick) that need a capturing of fine-grained discriminative details. To solve this problem, we propose synopsis-to-detail networks for video action recognition. Firstly, a synopsis network is introduced to predict the top-k likely actions and generate the synopsis (location & scale of details and contextual features). Secondly, according to the synopsis, a detail network is applied to extract the discriminative details in the input and infer the final action prediction. The proposed synopsis-to-detail networks enable us to train models directly from scratch in an end-to-end manner and to investigate various architectures for synopsis/detail recognition. Extensive experiments on benchmark datasets, including Kinetics-400, Mini-Kinetics and Something-Something V1 & V2, show that our method is more effective and efficient than the competitive baselines. Code is available at: https://github.com/liang4sx/S2DNet.\""
  },
  "eccv2022_main_ageneralized\\&robustframeworkfortimestampsupervisionintemporalactionsegmentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Generalized \\& Robust Framework for Timestamp Supervision in Temporal Action Segmentation",
    "authors": [
      "Rahul Rahaman",
      "Dipika Singhania",
      "Alexandre Thiery",
      "Angela Yao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4788_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640276.pdf",
    "published": "2020-08",
    "summary": "\"In temporal action segmentation, Timestamp supervision requires only a handful of labeled frames per video sequence. For unlabelled frames, Timestamp works rely on assigning hard labels and performance rapidly collapses under subtle violations of the annotation assumptions. We propose a novel Expectation-Maximization (EM) based approach which leverages label uncertainty of unlabelled frames and is robust enough to accommodate possible annotation errors. With accurate Timestamp annotations, our proposed method produces state-of-the-art results and even exceeds the fully-supervised setup in several metrics and datasets. When applied to timestamp annotations with missed action segments, we show that our method remains stable in terms of performance. To further test the robustness of our formulation, we introduce a new challenging annotation setup of SkipTag supervision. SkipTag is a relaxation on timestamps to allow for annotations of any fixed number of random frames in a video, making it more flexible than Timestamp supervision while remaining competitive.\""
  },
  "eccv2022_main_few-shotactionrecognitionwithhierarchicalmatchingandcontrastivelearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Few-Shot Action Recognition with Hierarchical Matching and Contrastive Learning",
    "authors": [
      "Sipeng Zheng",
      "Shizhe Chen",
      "Qin Jin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4804_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640293.pdf",
    "published": "2020-08",
    "summary": "\"Few-shot action recognition aims to recognize actions in test videos based on limited annotated data of target action classes. The dominant approaches project videos into a metric space and classify videos via nearest neighboring. They mainly measure video similarities using global or temporal alignment alone, while an optimum matching should be multi-level. However, the complexity of learning coarse-to-fine matching quickly rises as we focus on finer-grained visual cues, and the lack of detailed local supervision is another challenge. In this work, we propose a hierarchical matching model to support comprehensive similarity measure at global, temporal and spatial levels via a zoom-in matching module.We further propose a mixed-supervised hierarchical contrastive learning (HCL) in training, which not only employs supervised contrastive learning to differentiate videos at different levels, but also utilizes cycle consistency as weak supervision to align discriminative temporal clips or spatial patches. Our model achieves state-of-the-art performance on four benchmarks especially under the most challenging 1-shot action recognition setting. Code will be provided in supplementary material.\""
  },
  "eccv2022_main_privharrecognizinghumanactionsfromprivacy-preservinglens": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PrivHAR: Recognizing Human Actions from Privacy-Preserving Lens",
    "authors": [
      "Carlos Hinojosa",
      "Miguel Marquez",
      "Henry Arguello",
      "Ehsan Adeli",
      "Li Fei-Fei",
      "Juan Carlos Niebles"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5080_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640310.pdf",
    "published": "2020-08",
    "summary": "\"The accelerated use of digital cameras prompts an increasing concern about privacy and security, particularly in applications such as action recognition. In this paper, we propose an optimizing framework to provide robust visual privacy protection along the human action recognition pipeline. Our framework parameterizes the camera lens to successfully degrade the quality of the videos to inhibit privacy attributes and protect against adversarial attacks while maintaining relevant features for activity recognition. We validate our approach with extensive simulations and hardware experiments.\""
  },
  "eccv2022_main_scale-awarespatio-temporalrelationlearningforvideoanomalydetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Scale-Aware Spatio-Temporal Relation Learning for Video Anomaly Detection",
    "authors": [
      "Guoqiu Li",
      "Guanxiong Cai",
      "Xingyu Zeng",
      "Rui Zhao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5111_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640328.pdf",
    "published": "2020-08",
    "summary": "\"Recent progress in video anomaly detection (VAD) has shown that feature discrimination is the key to effectively distinguishing anomalies from normal events. We observe that many anomalous events occur in limited local regions, and the severe background noise increases the difficulty of feature learning. In this paper, we propose a scale-aware weakly supervised learning approach to capture local and salient anomalous patterns from the background, using only coarse video-level labels as supervision. We achieve this by segmenting frames into non-overlapping patches and then capturing inconsistencies among different regions through our patch spatial relation (PSR) module, which consists of self-attention mechanisms and dilated convolutions. To address the scale variation of anomalies and enhance the robustness of our method, a multi-scale patch aggregation method is further introduced to enable local-to-global spatial perception by merging features of patches with different scales. Considering the importance of temporal cues, we extend the relation modeling from the spatial domain to the spatio-temporal domain with the help of the existing video temporal relation network to effectively encode the spatio-temporal dynamics in the video. Experimental results show that our proposed method achieves new state-of-the-art performance on UCF-Crime and ShanghaiTech benchmarks. Code are available at https://github.com/nutuniv/SSRL.\""
  },
  "eccv2022_main_compoundprototypematchingforfew-shotactionrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Compound Prototype Matching for Few-Shot Action Recognition",
    "authors": [
      "Yifei Huang",
      "Lijin Yang",
      "Yoichi Sato"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5118_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640346.pdf",
    "published": "2020-08",
    "summary": "\"Few-shot action recognition aims to recognize novel action classes using only a small number of labeled training samples. In this work, we propose a novel approach that first summarizes each video into compound prototypes consisting of a group of global prototypes and a group of focused prototypes, and then compares video similarity based on the prototypes. Each global prototype is encouraged to summarize a specific aspect from the entire video, for example, the start of the action or the evolution of the action. Since no clear annotation is provided for the global prototypes, we use a group of focused prototypes and to focus on certain timestamps in the video. We compare similarity by matching the compound prototypes between the support and query videos. The global prototypes are directly matched so that the actions can be compared from the same perspective, for example, whether two actions start similarly. For the focused prototypes, since actions have various temporal shifts in the videos, we apply bipartite matching to allow comparison of the same action on different timestamps. Extensive experiments demonstrate that our proposed method achieves state-of-the-art results on multiple benchmarks by a large margin. A detailed ablation study analyzes the importance of each group of prototypes in capturing different aspects of the video.\""
  },
  "eccv2022_main_continual3dconvolutionalneuralnetworksforreal-timeprocessingofvideos": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Continual 3D Convolutional Neural Networks for Real-Time Processing of Videos",
    "authors": [
      "Lukas Hedegaard",
      "Alexandros Iosifidis"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5612_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640364.pdf",
    "published": "2020-08",
    "summary": "\"We introduce Continual 3D Convolutional Neural Networks (Co3D CNNs), a new computational formulation of spatio-temporal 3D CNNs, in which videos are processed frame-by-frame rather than by clip. In online tasks demanding frame-wise predictions, Co3D CNNs dispense with the computational redundancies of regular 3D CNNs, namely the repeated convolutions over frames, which appear in overlapping clips. We show that Continual 3D CNNs can reuse preexisting 3D-CNN weights to reduce the per-prediction floating point operations (FLOPs) in proportion to the temporal receptive field while retaining similar memory requirements and accuracy. This is validated with multiple models on Kinetics-400 and Charades with remarkable results: CoX3D models attain state-of-the-art complexity/accuracy trade-offs on Kinetics-400 with 12.1-15.3x reductions of FLOPs and 2.3-3.8% improvements in accuracy compared to regular X3D models while reducing peak memory consumption by up to 48%. Moreover, we investigate the transient response of Co3D CNNs at start-up and perform extensive benchmarks of on-hardware processing characteristics for publicly available 3D CNNs.\""
  },
  "eccv2022_main_dynamicspatio-temporalspecializationlearningforfine-grainedactionrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Dynamic Spatio-Temporal Specialization Learning for Fine-Grained Action Recognition",
    "authors": [
      "Tianjiao Li",
      "Lin Geng Foo",
      "Qiuhong Ke",
      "Hossein Rahmani",
      "Anran Wang",
      "Jinghua Wang",
      "Jun Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5677_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640381.pdf",
    "published": "2020-08",
    "summary": "\"The goal of fine-grained action recognition is to successfully discriminate between action categories with subtle differences. To tackle this, we derive inspiration from the human visual system which contains specialized regions in the brain that are dedicated towards handling specific tasks. We design a novel Dynamic Spatio-Temporal Specialization (DSTS) module, which consists of specialized neurons that are only activated for a subset of samples that are highly similar. During training, the loss forces the specialized neurons to learn discriminative fine-grained differences to distinguish between these similar samples, improving fine-grained recognition. Moreover, a spatio-temporal specialization method further optimizes the architectures of the specialized neurons to capture either more spatial or temporal fine-grained information, to better tackle the large range of spatio-temporal variations in the videos. Lastly, we design an Upstream-Downstream Learning algorithm to optimize our model\u2019s dynamic decisions, allowing our DSTS module to generalize better. We obtain state-of-the-art performance on two widely-used fine-grained action recognition datasets. We will release our code.\""
  },
  "eccv2022_main_dynamiclocalaggregationnetworkwithadaptiveclustererforanomalydetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection",
    "authors": [
      "Zhiwei Yang",
      "Peng Wu",
      "Jing Liu",
      "Xiaotao Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6078_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640398.pdf",
    "published": "2020-08",
    "summary": "\"Existing methods for anomaly detection based on memory-augmented autoencoder (AE) have the following drawbacks: (1) Establishing a memory bank requires additional memory space. (2) The fixed number of prototypes from subjective assumptions ignores the data feature differences and diversity. To overcome these drawbacks, we introduce DLAN-AC, a Dynamic Local Aggregation Network with Adaptive Clusterer, for anomaly detection. First, The proposed DLAN can automatically learn and aggregate high-level features from the AE to obtain more representative prototypes, while freeing up extra memory space. Second, The proposed AC can adaptively cluster video data to derive initial prototypes with prior information. In addition, we also propose a dynamic redundant clustering strategy (DRCS) to enable DLAN for automatically eliminating feature clusters that do not contribute to the construction of prototypes. Extensive experiments on benchmarks demonstrate that DLAN-AC outperforms most existing methods, validating the effectiveness of our method. Our code is publicly available at https://github.com/Beyond-Zw/DLAN-AC.\""
  },
  "eccv2022_main_actionqualityassessmentwithtemporalparsingtransformer": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Action Quality Assessment with Temporal Parsing Transformer",
    "authors": [
      "Yang Bai",
      "Desen Zhou",
      "Songyang Zhang",
      "Jian Wang",
      "Errui Ding",
      "Yu Guan",
      "Yang Long",
      "Jingdong Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6093_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640416.pdf",
    "published": "2020-08",
    "summary": "\"Action Quality Assessment(AQA) is important for action understanding and resolving the task poses unique challenges due to subtle visual differences. Existing state-of-the-art methods typically rely on the holistic video representations for score regression or ranking, which limits the generalization to capture fine-grained intra-class variation. To overcome the above limitation, we propose a temporal parsing transformer to decompose the holistic feature into temporal part-level representations. Specifically, we utilize a set of learnable queries to represent the atomic temporal patterns for a specific action. Our decoding process converts the frame representations to a fixed number of temporally ordered part representations. To obtain the quality score, we adopt the state-of-the-art contrastive regression based on the part representations. Since existing AQA datasets do not provide temporal part-level labels or partitions, we propose two novel loss functions on the cross attention responses of the decoder: a ranking loss to ensure the learnable queries to satisfy the temporal order in cross attention and a sparsity loss to encourage the part representations to be more discriminative. Extensive experiments show that our proposed method outperforms prior work on three public AQA benchmarks by a considerable margin.\""
  },
  "eccv2022_main_entry-flippedtransformerforinferenceandpredictionofparticipantbehavior": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Entry-Flipped Transformer for Inference and Prediction of Participant Behavior",
    "authors": [
      "Bo Hu",
      "Tat-Jen Cham"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6660_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640433.pdf",
    "published": "2020-08",
    "summary": "\"Some group activities, such as team sports and choreographed dances, involve closely coupled interaction between participants. Here we investigate the tasks of inferring and predicting participant behavior, in terms of motion paths and actions, under such conditions. We narrow the problem to that of estimating how a set target participants react to the behavior of other observed participants. Our key idea is to model the spatio-temporal relations among participants in a manner that is robust to error accumulation during frame-wise inference and prediction. We propose a novel Entry-Flipped Transformer (EF-Transformer), which models the relations of participants by attention mechanisms on both spatial and temporal domains. Unlike typical transformers, we tackle the problem of error accumulation by flipping the order of query, key, and value entries, to increase the importance and fidelity of observed features in the current frame. Comparative experiments show that our EF-Transformer achieves the best performance on a newly-collected tennis doubles dataset, a Ceilidh dance dataset, and two pedestrian datasets. Furthermore, it is also demonstrated that our EF-Transformer is better at limiting accumulated errors and recovering from wrong estimations.\""
  },
  "eccv2022_main_pairwisecontrastivelearningnetworkforactionqualityassessment": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Pairwise Contrastive Learning Network for Action Quality Assessment",
    "authors": [
      "Mingzhe Li",
      "Hong-Bo Zhang",
      "Qing Lei",
      "Zongwen Fan",
      "Jinghua Liu",
      "Ji-Xiang Du"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6731_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640450.pdf",
    "published": "2020-08",
    "summary": "\"Considering the complexity of modeling diverse actions of athletes, action quality assessment (AQA) in sports is a challenging task. A common solution is to tackle this problem as a regression task that map the input video to the final score provided by referees. However, it ignores the subtle and critical difference between videos. To address this problem, a new pairwise contrastive learning network (PCLN) is proposed to concern these differences and form an end-to-end AQA model with basic regression network. Specifically, the PCLN encodes video pairs to learn relative scores between videos to improve the performance of basic regression network. Furthermore, a new consistency constraint is defined to guide the training of the proposed AQA model. In the testing phase, only the basic regression network is employed, which makes the proposed method simple but high accuracy. The proposed method is verified on the AQA-7 and MTL-AQA datasets. Several ablation studies are built to verify the effectiveness of each component in the proposed method. The experimental results show that the proposed method achieves the state-of-the-art performance.\""
  },
  "eccv2022_main_geometricfeaturesinformedmulti-personhuman-objectinteractionrecognitioninvideos": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Geometric Features Informed Multi-Person Human-Object Interaction Recognition in Videos",
    "authors": [
      "Tanqiu Qiao",
      "Qianhui Men",
      "Frederick W. B. Li",
      "Yoshiki Kubotani",
      "Shigeo Morishima",
      "Hubert P. H. Shum"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6986_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640467.pdf",
    "published": "2020-08",
    "summary": "\"Human-Object Interaction (HOI) recognition in videos is important for analyzing human activity. Most existing work focusing on visual features usually suffer from occlusion in the real-world scenarios. Such a problem will be further complicated when multiple people and objects are involved in HOIs. Consider that geometric features such as human pose and object position provide meaningful information to understand HOIs, we argue to combine the benefits of both visual and geometric features in HOI recognition, and propose a novel Two-level Geometric feature-informed Graph Convolutional Network (2G-GCN). The geometric-level graph models the interdependency between geometric features of humans and objects, while the fusion-level graph further fuses them with visual features of humans and objects. To demonstrate the novelty and effectiveness of our method in challenging scenarios, we propose a new multi-person HOI dataset (MPHOI-72). Extensive experiments on MPHOI-72 (multi-person HOI), CAD-120 (single-human HOI) and Bimanual Actions (two-hand HOI) datasets demonstrate our superior performance compared to state-of-the-arts.\""
  },
  "eccv2022_main_actionformerlocalizingmomentsofactionswithtransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "ActionFormer: Localizing Moments of Actions with Transformers",
    "authors": [
      "Chen-Lin Zhang",
      "Jianxin Wu",
      "Yin Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7278_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640485.pdf",
    "published": "2020-08",
    "summary": "\"Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer--a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at https://github.com/happyharrycn/actionformer_release.\""
  },
  "eccv2022_main_socialvaehumantrajectorypredictionusingtimewiselatents": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SocialVAE: Human Trajectory Prediction Using Timewise Latents",
    "authors": [
      "Pei Xu",
      "Jean-Bernard Hayet",
      "Ioannis Karamouzas"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7925_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640504.pdf",
    "published": "2020-08",
    "summary": "\"Predicting pedestrian movement is critical for human behavior analysis and also for safe and efficient human-agent interactions. However, despite significant advancements, it is still challenging for existing approaches to capture the uncertainty and multimodality of human navigation decision making. In this paper, we propose SocialVAE, a novel approach for human trajectory prediction. The core of SocialVAE is a timewise variational autoencoder architecture that exploits stochastic recurrent neural networks to perform prediction, combined with a social attention mechanism and a backward posterior approximation to allow for better extraction of pedestrian navigation strategies. We show that SocialVAE improves current state-of-the-art performance on several pedestrian trajectory prediction benchmarks, including the ETH/UCY benchmark, Stanford Drone Dataset, and SportVU NBA movement dataset.\""
  },
  "eccv2022_main_shapemattersdeformablepatchattack": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Shape Matters: Deformable Patch Attack",
    "authors": [
      "Zhaoyu Chen",
      "Bo Li",
      "Shuang Wu",
      "Jianghe Xu",
      "Shouhong Ding",
      "Wenqiang Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/689_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640522.pdf",
    "published": "2020-08",
    "summary": "\"Though deep neural networks (DNNs) have demonstrated excellent performance in computer vision, they are susceptible and vulnerable to carefully crafted adversarial examples which can mislead DNNs to incorrect outputs. Patch attack is one of the most threatening forms, which has the potential to threaten the security of real-world systems. Previous work always assumes patches to have fixed shapes, such as circles or rectangles, and it does not consider the shape of patches as a factor in patch attacks. To explore this issue, we propose a novel Deformable Patch Representation (DPR) that can harness the geometric structure of triangles to support the differentiable mapping between contour modeling and masks. Moreover, we introduce a joint optimization algorithm, named Deformable Adversarial Patch (DAPatch), which allows simultaneous and efficient optimization of shape and texture to enhance attack performance. We show that even with a small area, a particular shape can improve attack performance. Therefore, DAPatch achieves state-of-the-art attack performance by deforming shapes on GTSRB and ILSVRC2012 across various network architectures, and the generated patches can be threatening in the real world.\""
  },
  "eccv2022_main_frequencydomainmodelaugmentationforadversarialattack": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Frequency Domain Model Augmentation for Adversarial Attack",
    "authors": [
      "Yuyang Long",
      "Qilong Zhang",
      "Boheng Zeng",
      "Lianli Gao",
      "Xianglong Liu",
      "Jian Zhang",
      "Jingkuan Song"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/974_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640543.pdf",
    "published": "2020-08",
    "summary": "\"For black-box attacks, the gap between the substitute model and the victim model is usually large, which manifests as a weak attack performance. Motivated by the observation that the transferability of adversarial examples can be improved by attacking diverse models simultaneously, model augmentation methods which simulate different models by using transformed images are proposed. However, existing transformations for spatial domain do not translate to significantly diverse augmented models. To tackle this issue, we propose a novel spectrum simulation attack to craft more transferable adversarial examples against both normally trained and defense models. Specifically, we apply a spectrum transformation to the input and thus perform the model augmentation in the frequency domain. We theoretically prove that the transformation derived from frequency domain leads to a diverse spectrum saliency map, an indicator we proposed to reflect the diversity of substitute models. Notably, our method can be generally combined with existing attacks. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our method, e.g., attacking nine state-of-the-art defense models with an average success rate of 95.4%. Our code is available in https://github.com/yuyang-long/SSA.\""
  },
  "eccv2022_main_prior-guidedadversarialinitializationforfastadversarialtraining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Prior-Guided Adversarial Initialization for Fast Adversarial Training",
    "authors": [
      "Xiaojun Jia",
      "Yong Zhang",
      "Xingxing Wei",
      "Baoyuan Wu",
      "Ke Ma",
      "Jue Wang",
      "Xiaochun Cao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1780_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640560.pdf",
    "published": "2020-08",
    "summary": "\"Fast adversarial training (FAT) e\u00ef\u00ac\u20acectively improves the efficiency of standard adversarial training (SAT). However, initial FAT encounters catastrophic overfitting, i.e., the robust accuracy against adversarial attacks suddenly decreases to 0% during training. Though several FAT variants spare no e\u00ef\u00ac\u20acort to prevent overfitting, they sacrifice much calculation cost. In this paper, we explore the di\u00ef\u00ac\u20acerence between the training processes of SAT and FAT and observe that the attack success rate of adversarial examples (AEs) of FAT gets worse gradually in the late training stage, resulting in overfitting. The AEs are generated by the fast gradient sign method (FGSM) with a zero or random initialization. Based on the observation, we propose a prior-guided FGSM initialization method to avoid overfitting after investigating several initialization strategies, improving the quality of the AEs during the whole training process. The initialization is formed by leveraging historically generated AEs without additional calculation cost. We further provide a theoretical analysis for the proposed initialization method. Moreover, we also propose a simple yet e\u00ef\u00ac\u20acective regularizer based on the prior guided initialization, i.e., the currently generated perturbation should not deviate too much from the prior-guided initialization. The regularizer adopts both historical and current adversarial perturbations to guide the model learning. Evaluations on four datasets demonstrate that the proposed method can prevent catastrophic overfitting and outperform state-of-the-art FAT methods at a low computational cost.\""
  },
  "eccv2022_main_enhancedaccuracyandrobustnessviamulti-teacheradversarialdistillation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Enhanced Accuracy and Robustness via Multi-Teacher Adversarial Distillation",
    "authors": [
      "Shiji Zhao",
      "Jie Yu",
      "Zhenlong Sun",
      "Bo Zhang",
      "Xingxing Wei"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2638_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640577.pdf",
    "published": "2020-08",
    "summary": "\"Adversarial training is an effective approach for improving the robustness of deep neural networks against adversarial attacks. Although bringing reliable robustness, adversarial training (AT) will reduce the performance of identifying clean examples. Meanwhile, Adversarial training can bring more robustness for large models than small models. To improve the robust and clean accuracy of small models, we introduce the Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the adversarial training process of small models. Specifically, MTARD uses multiple large teacher models, including an adversarial teacher and a clean teacher to guide a small student model in the adversarial training by knowledge distillation. In addition, we design a dynamic training algorithm to balance the influence between the adversarial teacher and clean teacher models. A series of experiments demonstrate that our MTARD can outperform the state-of-the-art adversarial training and distillation methods against various adversarial attacks.\""
  },
  "eccv2022_main_lgvboostingadversarialexampletransferabilityfromlargegeometricvicinity": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity",
    "authors": [
      "Martin Gubri",
      "Maxime Cordy",
      "Mike Papadakis",
      "Yves Le Traon",
      "Koushik Sen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2864_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640594.pdf",
    "published": "2020-08",
    "summary": "\"We propose transferability from Large Geometric Vicinity (LGV), a new technique to increase the transferability of black-box adversarial attacks. LGV starts from a pretrained surrogate model and collects multiple weight sets from a few additional training epochs with a constant and high learning rate. LGV exploits two geometric properties that we relate to transferability. First, models that belong to a wider weight optimum are better surrogates. Second, we identify a subspace able to generate an effective surrogate ensemble among this wider optimum. Through extensive experiments, we show that LGV alone outperforms all (combinations of) four established test-time transformations by 1.8 to 59.9 percentage points. Our findings shed new light on the importance of the geometry of the weight space to explain the transferability of adversarial examples.\""
  },
  "eccv2022_main_alarge-scalemultiple-objectivemethodforblack-boxattackagainstobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Large-Scale Multiple-Objective Method for Black-Box Attack against Object Detection",
    "authors": [
      "Siyuan Liang",
      "Longkang Li",
      "Yanbo Fan",
      "Xiaojun Jia",
      "Jingzhi Li",
      "Baoyuan Wu",
      "Xiaochun Cao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3147_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640611.pdf",
    "published": "2020-08",
    "summary": "\"Recent studies have shown that detectors based on deep models are vulnerable to adversarial examples, even in the black-box scenario where the attacker cannot access the model information. Most existing attack methods aim to minimize the true positive rate, which often shows poor attack performance, as another sub-optimal bounding box may be detected around the attacked bounding box to be the new true positive one. To settle this challenge, we propose to minimize the true positive rate and maximize the false positive rate, which can encourage more false positive objects to block the generation of new true positive bounding boxes. It is modeled as a multi-objective optimization (MOP) problem, of which the generic algorithm can search the Pareto-optimal. However, our task has more than two million decision variables, leading to low searching efficiency. Thus, we extend the standard \\textbf{G}enetic \\textbf{A}lgorithm with \\textbf{R}andom \\textbf{S}ubset selection and \\textbf{D}ivide-and-\\textbf{C}onquer, called GARSDC, which significantly improves the efficiency. Moreover, to alleviate the sensitivity to population quality in generic algorithms, we generate a gradient-prior initial population, utilizing the transferability between different detectors with similar backbones. Compared with the state-of-art attack methods, GARSDC decreases by an average 12.0 in the mAP and queries by about 1000 times in extensive experiments. Our codes can be found at https://github.com/LiangSiyuan21/GARSDC.\""
  },
  "eccv2022_main_gradautoenergy-orientedattackondynamicneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "GradAuto: Energy-Oriented Attack on Dynamic Neural Networks",
    "authors": [
      "Jianhong Pan",
      "Qichen Zheng",
      "Zhipeng Fan",
      "Hossein Rahmani",
      "Qiuhong Ke",
      "Jun Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3150_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640628.pdf",
    "published": "2020-08",
    "summary": "\"Dynamic neural networks could adapt their structures or parameters based on different inputs. By reducing the computation redundancy for certain samples, it can greatly improve the computational efficiency without compromising the accuracy. In this paper, we investigate the robustness of dynamic neural networks against energy-oriented attacks. We present a novel algorithm, named GradAuto, to attack both dynamic depth and dynamic width models, where dynamic depth networks reduce redundant computation by skipping some intermediate layers while dynamic width networks adaptively activate a subset of neurons in each layer. Our GradAuto carefully adjusts the direction and the magnitude of the gradients to efficiently find an almost imperceptible perturbation for each input, which will activate more computation units during inference. In this way, GradAuto effectively boosts the computational cost of models with dynamic architectures. Compared to previous energy-oriented attack techniques, GradAuto obtains the state-of-the-art result and recovers 100\\% dynamic network reduced FLOPs on average for both dynamic depth and dynamic width models. Furthermore, we demonstrate that GradAuto offers us great control over the attacking process and could serve as one of the keys to unlock the potential of the energy-oriented attack.\""
  },
  "eccv2022_main_aspectralviewofrandomizedsmoothingundercommoncorruptionsbenchmarkingandimprovingcertifiedrobustness": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Spectral View of Randomized Smoothing under Common Corruptions: Benchmarking and Improving Certified Robustness",
    "authors": [
      "Jiachen Sun",
      "Akshay Mehra",
      "Bhavya Kailkhura",
      "Pin-Yu Chen",
      "Dan Hendrycks",
      "Jihun Hamm",
      "Z. Morley Mao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3225_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640645.pdf",
    "published": "2020-08",
    "summary": "\"Certified robustness guarantee gauges a model\u2019s resistance to test-time attacks and can assess the model\u2019s readiness for deployment in the real world. In this work, we explore a new problem setting to critically examine how the adversarial robustness guarantees change when state-of-the-art randomized smoothing-based certifications encounter common corruptions of the test data. Our analysis demonstrates a previously unknown vulnerability of these certifiably robust models to low-frequency corruptions such as weather changes, rendering these models unfit for deployment in the wild. To alleviate this issue, we propose a novel data augmentation scheme, FourierMix, that produces augmentations to improve the spectral coverage of the training data. Furthermore, we propose a new regularizer that encourages consistent predictions on noise perturbations of the augmented data to improve the quality of the smoothed models. We show that FourierMix helps eliminate the spectral bias of certifiably robust models, enabling them to achieve significantly better certified robustness on a range of corruption benchmarks. Our evaluation also uncovers the inability of current corruption benchmarks to highlight the spectral biases of the models. To this end, we propose a comprehensive benchmarking suite that contains corruptions from different regions in the spectral domain. Evaluation of models trained with popular augmentation methods on the proposed suite unveils their spectral biases. It also establishes the superiority of FourierMix trained models in achieving stronger certified robustness guarantees under corruptions over the entire frequency spectrum.\""
  },
  "eccv2022_main_improvingadversarialrobustnessof3dpointcloudclassificationmodels": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Improving Adversarial Robustness of 3D Point Cloud Classification Models",
    "authors": [
      "Guanlin Li",
      "Guowen Xu",
      "Han Qiu",
      "Ruan He",
      "Jiwei Li",
      "Tianwei Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3322_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640663.pdf",
    "published": "2020-08",
    "summary": "\"3D point cloud classification models based on deep neural networks were proven to be vulnerable to adversarial examples, with a quantity of novel attack techniques proposed by researchers recently. It is of paramount importance to preserve the robustness of 3D models under adversarial environments, considering their broad application in safety- and security-critical tasks. Unfortunately, existing defenses are not general enough to satisfactorily mitigate all types of attacks. In this paper, we design two innovative methodologies to improve the adversarial robustness of 3D point cloud classification models. (1) We introduce CCN, a novel point cloud architecture which can smooth and disrupt the adversarial perturbations. (2) We propose AMS, a novel data augmentation strategy to adaptively balance the model usability and robustness. Extensive evaluations indicate the integration of the two techniques provides much more robustness than existing defense solutions for 3D classification models. Our code can be found in https://github.com/GuanlinLee/CCNAMS.\""
  },
  "eccv2022_main_learningextremelylightweightandrobustmodelwithdifferentiableconstraintsonsparsityandconditionnumber": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Extremely Lightweight and Robust Model with Differentiable Constraints on Sparsity and Condition Number",
    "authors": [
      "Xian Wei",
      "Yangyu Xu",
      "Yanhui Huang",
      "Hairong Lv",
      "Hai Lan",
      "Mingsong Chen",
      "Xuan Tang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3458_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640679.pdf",
    "published": "2020-08",
    "summary": "\"Learning lightweight and robust deep learning models is an enormous challenge for safety-critical devices with limited computing and memory resources, owing to robustness against adversarial attacks being proportional to network capacity. The community has extensively explored the integration of adversarial training and model compression, such as weight pruning. However, lightweight models generated by highly pruned over-parameterized models lead to sharp drops in both robust and natural accuracy. It has been observed that the parameters of these models lie in ill-conditioned weight space, i.e., the condition number of weight matrices tend to be large enough that the model is not robust. In this work, we propose a framework for building extremely lightweight models, which combines tensor product with the differentiable constraints for reducing condition number and promoting sparsity. Moreover, the proposed framework is incorporated into adversarial training with the min-max optimization scheme. We evaluate the proposed approach on VGG-16 and Visual Transformer. Experimental results on datasets such as ImageNet, SVHN, and CIFAR-10 show that we can achieve an overwhelming advantage at a high compression ratio, e.g., 200 times.\""
  },
  "eccv2022_main_ribactowardsrobustandimperceptiblebackdoorattackagainstcompactdnn": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN",
    "authors": [
      "Huy Phan",
      "Cong Shi",
      "Yi Xie",
      "Tianfang Zhang",
      "Zhuohang Li",
      "Tianming Zhao",
      "Jian Liu",
      "Yan Wang",
      "Yingying Chen",
      "Bo Yuan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3867_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640697.pdf",
    "published": "2020-08",
    "summary": "\"Recently backdoor attack has become an emerging threat to the security of deep neural network (DNN) models. To date, most of the existing studies focus on backdoor attack against the uncompressed model; while the vulnerability of compressed DNNs, which are widely used in the practical applications, is little exploited yet. In this paper, we propose to study and develop Robust and Imperceptible Backdoor Attack against Compact DNN models (RIBAC). By performing systematic analysis and exploration on the important design knobs, we propose a framework that can learn the proper trigger patterns, model parameters and pruning masks in an efficient way. Thereby achieving high trigger stealthiness, high attack success rate and high model efficiency simultaneously. Extensive evaluations across different datasets, including the test against the state-of-the-art defense mechanisms, demonstrate the high robustness, stealthiness and model efficiency of RIBAC. Code is available at https://github.com/huyvnphan/ECCV2022-RIBAC\""
  },
  "eccv2022_main_boostingtransferabilityoftargetedadversarialexamplesviahierarchicalgenerativenetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks",
    "authors": [
      "Xiao Yang",
      "Yinpeng Dong",
      "Tianyu Pang",
      "Hang Su",
      "Jun Zhu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4374_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640714.pdf",
    "published": "2020-08",
    "summary": "\"Transfer-based adversarial attacks can evaluate model robustness in the black-box setting. Several methods have demonstrated impressive untargeted transferability, however, it is still challenging to efficiently produce targeted transferability. To this end, we develop a simple yet effective framework to craft targeted transfer-based adversarial examples, applying a hierarchical generative network. In particular, we contribute to amortized designs that well adapt to multi-class targeted attacks. Extensive experiments on ImageNet show that our method improves the success rates of targeted black-box attacks by a significant margin over the existing methods --- it reaches an average success rate of 29.1% against six diverse models based only on one substitute white-box model, which significantly outperforms the state-of-the-art gradient-based attack methods. Moreover, the proposed method is also more efficient beyond an order of magnitude than gradient-based methods.\""
  },
  "eccv2022_main_adaptiveimagetransformationsfortransfer-basedadversarialattack": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Adaptive Image Transformations for Transfer-Based Adversarial Attack",
    "authors": [
      "Zheng Yuan",
      "Jie Zhang",
      "Shiguang Shan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4557_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650001.pdf",
    "published": "2020-08",
    "summary": "\"Adversarial attacks provide a good way to study the robustness of deep learning models. One category of methods in transfer-based black-box attack utilizes several image transformation operations to improve the transferability of adversarial examples, which is effective, but fails to take the specific characteristic of the input image into consideration. In this work, we propose a novel architecture, called Adaptive Image Transformation Learner (AITL), which incorporates different image transformation operations into a unified framework to further improve the transferability of adversarial examples. Unlike the fixed combinational transformations used in existing works, our elaborately designed transformation learner adaptively selects the most effective combination of image transformations specific to the input image. Extensive experiments on ImageNet demonstrate that our method significantly improves the attack success rates on both normally trained models and defense models under various settings.\""
  },
  "eccv2022_main_generativemultiplaneimagesmakinga2dgan3d-aware": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Generative Multiplane Images: Making a 2D GAN 3D-Aware",
    "authors": [
      "Xiaoming Zhao",
      "Fangchang Ma",
      "David G\u00fcera",
      "Zhile Ren",
      "Alexander G. Schwing",
      "Alex Colburn"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4610_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650019.pdf",
    "published": "2020-08",
    "summary": "\"What is really needed to make an existing 2D GAN 3Daware? To answer this question, we modify a classical GAN, i.e., StyleGANv2, as little as possible. We find that only two modifications are absolutely necessary: 1) a multiplane image style generator branch which produces a set of alpha maps conditioned on their depth; 2) a pose conditioned discriminator. We refer to the generated output as a \u2018generative multiplane image\u2019 (GMPI) and emphasize that its renderings are not only high-quality but also guaranteed to be view-consistent, which makes GMPIs different from many prior works. Importantly, the number of alpha maps can be dynamically adjusted and can differ between training and inference, alleviating memory concerns and enabling fast training of GMPIs in less than half a day at a resolution of 1024^2. Our findings are consistent across three challenging and common high-resolution datasets, including FFHQ, AFHQv2 and MetFaces.\""
  },
  "eccv2022_main_advdorealisticadversarialattacksfortrajectoryprediction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AdvDO: Realistic Adversarial Attacks for Trajectory Prediction",
    "authors": [
      "Yulong Cao",
      "Chaowei Xiao",
      "Anima Anandkumar",
      "Danfei Xu",
      "Marco Pavone"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4832_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650036.pdf",
    "published": "2020-08",
    "summary": "\"Trajectory prediction is essential for autonomous vehicles (AVs) to plan correct and safe driving behaviors. While many prior works aim to achieve higher prediction accuracy, few studies the adversarial robustness of their methods. To bridge this gap, we propose to study the adversarial robustness of data-driven trajectory prediction systems. We devise an optimization-based adversarial attack framework that leverages a carefully-designed differentiable dynamic model to generate realistic adversarial trajectories. Empirically, we benchmark the adversarial robustness of state-of-the-art prediction models and show that our attack increases the prediction error for both general metrics and planning-aware metrics by more than 50% and 37%. We also show that our attack can lead an AV to drive off-road or collide into other vehicles in simulation. Finally, we demonstrate how to mitigate the adversarial attacks using an adversarial training scheme.\""
  },
  "eccv2022_main_adversarialcontrastivelearningviaasymmetricinfonce": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Adversarial Contrastive Learning via Asymmetric InfoNCE",
    "authors": [
      "Qiying Yu",
      "Jieming Lou",
      "Xianyuan Zhan",
      "Qizhang Li",
      "Wangmeng Zuo",
      "Yang Liu",
      "Jingjing Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4996_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650053.pdf",
    "published": "2020-08",
    "summary": "\"Contrastive learning (CL) has recently been applied to adversarial learning tasks. Such practice considers adversarial perturbations as additional positive samples of an instance, and by maximizing their agreements with each other, yields better adversarial robustness. However, this mechanism can be potentially flawed, since adversarial perturbations may cause instance-level identity confusion, which can impede CL performance by pulling together different instances with separate identities. To address this issue, we propose to treat adversarial samples unequally when contrasted to positive and negative samples, with an asymmetric InfoNCE objective (A-InfoNCE) that allows discriminating considerations of adversarial samples. Specifically, adversaries are viewed as inferior positives that induce weaker learning signals, or as hard negatives exhibiting higher contrast to other negative samples. In the asymmetric fashion, the adverse impacts of conflicting objectives between CL and adversarial learning can be effectively mitigated. Experiments show that our approach consistently outperforms existing Adversarial CL methods across different finetuning schemes without additional computational cost. The proposed A-InfoNCE is also a generic form that can be readily extended to different CL methods. Code is available at https://github.com/yqy2001/A-InfoNCE.\""
  },
  "eccv2022_main_onesizedoesnotfitalldata-adaptiveadversarialtraining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "One Size Does NOT Fit All: Data-Adaptive Adversarial Training",
    "authors": [
      "Shuo Yang",
      "Chang Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5491_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650070.pdf",
    "published": "2020-08",
    "summary": "\"Adversarial robustness is critical for deep learning models to defend against adversarial attacks. Although adversarial training is considered to be one of the most effective ways to improve the model\u2019s adversarial robustness, it usually yields models with lower natural accuracy. In this paper, we argue that, for the attackable examples, traditional adversarial training which utilizes a fixed size perturbation ball can create adversarial examples that deviate far away from the original class towards the target class. Thus, the model\u2019s performance on the natural target class will drop drastically, which leads to the decline of natural accuracy. To this end, we propose the Data-Adaptive Adversarial Training (DAAT) which adaptively adjusts the perturbation ball to a proper size for each of the natural examples with the help of a natural trained calibration network. Besides, a dynamic training strategy empowers the DAAT models with impressive robustness while retaining remarkable natural accuracy. Based on a toy example, we theoretically prove the recession of the natural accuracy caused by adversarial training and show how the data-adaptive perturbation size helps the model resist it. Finally, empirical experiments on benchmark datasets demonstrate the significant improvement of DAAT models on natural accuracy compared with strong baselines.\""
  },
  "eccv2022_main_unicruniversallyapproximatedcertifiedrobustnessviarandomizedsmoothing": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "UniCR: Universally Approximated Certified Robustness via Randomized Smoothing",
    "authors": [
      "Hanbin Hong",
      "Binghui Wang",
      "Yuan Hong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5812_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650086.pdf",
    "published": "2020-08",
    "summary": "\"We study certified robustness of machine learning classifiers against adversarial perturbations. In particular, we propose the first universally approximated certified robustness (UniCR) framework, which can approximate the robustness certification of \\emph{any} input on \\emph{any} classifier against \\emph{any} $\\ell_p$ perturbations with noise generated by \\emph{any} continuous probability distribution. Compared with the state-of-the-art certified defenses, UniCR provides many significant benefits: (1) the first universal robustness certification framework for the above 4 \u201cany\u201ds; (2) automatic robustness certification that avoids case-by-case analysis, (3) tightness validation of certified robustness, and (4) optimality validation of noise distributions used by randomized smoothing. We conduct extensive experiments to validate the above benefits of UniCR and the advantages of UniCR over state-of-the-art certified defenses against $\\ell_p$ perturbations.\""
  },
  "eccv2022_main_hardlyperceptibletrojanattackagainstneuralnetworkswithbitflips": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Hardly Perceptible Trojan Attack against Neural Networks with Bit Flips",
    "authors": [
      "Jiawang Bai",
      "Kuofeng Gao",
      "Dihong Gong",
      "Shu-Tao Xia",
      "Zhifeng Li",
      "Wei Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5837_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650103.pdf",
    "published": "2020-08",
    "summary": "\"The security of deep neural networks (DNNs) has attracted increasing attention due to their widespread use in various applications. Recently, the deployed DNNs have been demonstrated to be vulnerable to Trojan attacks, which manipulate model parameters with bit flips to inject a hidden behavior and activate it by a specific trigger pattern. However, all existing Trojan attacks adopt noticeable patch-based triggers (e.g., a square pattern), making them perceptible to humans and easy to be spotted by machines. In this paper, we present a novel attack, namely hardly perceptible Trojan attack (HPT). HPT crafts hardly perceptible Trojan images by utilizing the additive noise and per-pixel flow field to tweak the pixel values and positions of the original images, respectively. To achieve superior attack performance, we propose to jointly optimize bit flips, additive noise, and flow field. Since the weight bits of the DNNs are binary, this problem is very hard to be solved. We handle the binary constraint with equivalent replacement and provide an effective optimization algorithm. Extensive experiments on CIFAR-10, SVHN, and ImageNet datasets show that the proposed HPT can generate hardly perceptible Trojan images, while achieving comparable or better attack performance compared to the state-of-the-art methods. The code is available at: https://github.com/jiawangbai/HPT.\""
  },
  "eccv2022_main_robustnetworkarchitecturesearchviafeaturedistortionrestraining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Robust Network Architecture Search via Feature Distortion Restraining",
    "authors": [
      "Yaguan Qian",
      "Shenghui Huang",
      "Bin Wang",
      "Xiang Ling",
      "Xiaohui Guan",
      "Zhaoquan Gu",
      "Shaoning Zeng",
      "Wujie Zhou",
      "Haijiang Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5974_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650120.pdf",
    "published": "2020-08",
    "summary": "\"The vulnerability of DNNs severely limits the application of it in the security-sensitive domains. Most of the existing methods improve the robustness of models from weight optimization, such as adversarial training and regularization. However, the architecture is also a key factor to robustness, which is often neglected or underestimated. We propose Robust Network Architecture Search (RNAS) to obtain a robust network against adversarial attacks. We observe that an adversarial perturbation distorting the non-robust features in latent feature space can further aggravate misclassification. Based on this observation, we search the robust architecture through restricting feature distortion in the search process. Specifically, we define a network vulnerability metric based on feature distortion as a constraint in the search process. This process is modeled as a multi-objective bilevel optimization problem and an effective algorithm is proposed to solve this optimization. Extensive experiments conducted on CIFAR-10/100, SVHN and Tiny-ImageNet show that RNAS achieves the best robustness under various adversarial attacks compared with extensive baselines and state-of-the-art methods.\""
  },
  "eccv2022_main_secretgenprivacyrecoveryonpre-trainedmodelsviadistributiondiscrimination": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SecretGen: Privacy Recovery on Pre-trained Models via Distribution Discrimination",
    "authors": [
      "Zhuowen Yuan",
      "Fan Wu",
      "Yunhui Long",
      "Chaowei Xiao",
      "Bo Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6086_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650137.pdf",
    "published": "2020-08",
    "summary": "\"Transfer learning through the use of pre-trained models has become a growing trend for the machine learning community. Consequently, numerous pre-trained models are released online to facilitate further research. However, it raises extensive concerns on whether these pre-trained models would leak privacy-sensitive information of their training data. Thus, in this work, we aim to answer the following questions: \"\"Can we effectively recover private information from these pre-trained models? What are the sufficient conditions to retrieve such sensitive information?\"\" We first explore different statistical information which can discriminate the private training distribution from other distributions. Based on our observations, we propose a novel private data reconstruction framework, SecretGen, to effectively recover private information. Compared with previous methods which can recover private data with the ground true prediction of the targeted recovery instance, SecretGen does not require such prior knowledge, making it more practical. We conduct extensive experiments on different datasets under diverse scenarios to compare SecretGen with other baselines and provide a systematic benchmark to better understand the impact of different auxiliary information and optimization operations. We show that without prior knowledge about true class prediction, SecretGen is able to recover private data with similar performance compared with the ones that leverage such prior knowledge. If the prior knowledge is given, SecretGen will significantly outperform baseline methods. We also propose several quantitative metrics to further quantify the privacy vulnerability of pre-trained models, which will help the model selection for privacy-sensitive applications. Our code is available at: https://github.com/AI-secure/SecretGen.\""
  },
  "eccv2022_main_triangleattackaquery-efficientdecision-basedadversarialattack": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Triangle Attack: A Query-Efficient Decision-Based Adversarial Attack",
    "authors": [
      "Xiaosen Wang",
      "Zeliang Zhang",
      "Kangheng Tong",
      "Dihong Gong",
      "Kun He",
      "Zhifeng Li",
      "Wei Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6276_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650153.pdf",
    "published": "2020-08",
    "summary": "\"Decision-based attack poses a severe threat to real-world applications since it regards the target model as a black box and only accesses the hard prediction label. Great efforts have been made recently to decrease the number of queries; however, existing decision-based attacks still require thousands of queries in order to generate good quality adversarial examples. In this work, we find that a benign sample, the current and the next adversarial examples can naturally construct a triangle in a subspace for any iterative attacks. Based on the law of sines, we propose a novel Triangle Attack (TA) to optimize the perturbation by utilizing the geometric information that the longer side is always opposite the larger angle in any triangle. However, directly applying such information on the input image is ineffective because it cannot thoroughly explore the neighborhood of the input sample in the high dimensional space. To address this issue, TA optimizes the perturbation in the low frequency space for effective dimensionality reduction owing to the generality of such geometric property. Extensive evaluations on ImageNet dataset show that TA achieves a much higher attack success rate within 1,000 queries and needs a much less number of queries to achieve the same attack success rate under various perturbation budgets than existing decision-based attacks. With such high efficiency, we further validate the applicability of TA on real-world API, i.e., Tencent Cloud API.\""
  },
  "eccv2022_main_data-freebackdoorremovalbasedonchannellipschitzness": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Data-Free Backdoor Removal Based on Channel Lipschitzness",
    "authors": [
      "Runkai Zheng",
      "Rongjun Tang",
      "Jianze Li",
      "Li Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6305_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650171.pdf",
    "published": "2020-08",
    "summary": "\"Recent studies have shown that Deep Neural Networks (DNNs) are vulnerable to the backdoor attacks, which leads to malicious behaviors of DNNs when specific triggers are attached to the input images. It was further demonstrated that the infected DNNs possess a collection of channels, which are more sensitive to the backdoor triggers compared with normal channels. Pruning these channels was then shown to be effective in mitigating the backdoor behaviors. To locate those channels, it is natural to consider their Lipschitzness, which measures their sensitivity against worst-case perturbations on the inputs. In this work, we introduce a novel concept called Channel Lipschitz Constant (CLC), which is defined as the Lipschitz constant of the mapping from the input images to the output of each channel. Then we provide empirical evidences to show the strong correlation between an Upper bound of the CLC (UCLC) and the trigger-activated change on the channel activation. Since UCLC can be directly calculated from the weight matrices, we can detect the potential backdoor channels in a data-free manner, and do simple pruning on the infected DNN to repair the model. The proposed Channel Lipschitzness based Pruning (CLP) method is super fast, simple, data-free and robust to the choice of the pruning threshold. Extensive experiments are conducted to evaluate the efficiency and effectiveness of CLP, which achieves state-of-the-art results among the mainstream defense methods even without any data. Source codes are available at https://github.com/rkteddy/channel-Lipschitzness-based-pruning.\""
  },
  "eccv2022_main_black-boxdissectortowardserasing-basedhard-labelmodelstealingattack": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Black-Box Dissector: Towards Erasing-Based Hard-Label Model Stealing Attack",
    "authors": [
      "Yixu Wang",
      "Jie Li",
      "Hong Liu",
      "Yan Wang",
      "Yongjian Wu",
      "Feiyue Huang",
      "Rongrong Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6441_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650188.pdf",
    "published": "2020-08",
    "summary": "\"Previous studies have verified that the functionality of black-box models can be stolen with full probability outputs. However, under the more practical hard-label setting, we observe that existing methods suffer from catastrophic performance degradation. We argue this is due to the lack of rich information in the probability prediction and the overfitting caused by hard labels. To this end, we propose a novel hard-label model stealing method termed black-box dissector, which consists of two erasing-based modules. One is a CAM-driven erasing strategy that is designed to increase the information capacity hidden in hard labels from the victim model. The other is a random-erasing-based self-knowledge distillation module that utilizes soft labels from the substitute model to mitigate overfitting. Extensive experiments on four widely-used datasets consistently demonstrate that our method outperforms state-of-the-art methods, with an improvement of at most 8.27%. We also validate the effectiveness and practical potential of our method on real-world APIs and defense methods. Furthermore, our method promotes other downstream tasks, i.e., transfer adversarial attacks.\""
  },
  "eccv2022_main_learningenergy-basedmodelswithadversarialtraining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Energy-Based Models with Adversarial Training",
    "authors": [
      "Xuwang Yin",
      "Shiying Li",
      "Gustavo K. Rohde"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6532_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650204.pdf",
    "published": "2020-08",
    "summary": "\"We study a new approach to learning energy-based models (EBMs) based on adversarial training (AT). We show that (binary) AT learns a special kind of energy function that models the support of the data distribution, and the learning process is closely related to MCMC-based maximum likelihood learning of EBMs. We further propose improved techniques for generative modeling with AT, and demonstrate that this new approach is capable of generating diverse and realistic images. Aside from having competitive image generation performance to explicit EBMs, the studied approach is stable to train, is well-suited for image translation tasks, and exhibits strong out-of-distribution adversarial robustness. Our results demonstrate the viability of the AT approach to generative modeling, suggesting that AT is a competitive alternative approach to learning EBMs.\""
  },
  "eccv2022_main_adversariallabelpoisoningattackongraphneuralnetworksvialabelpropagation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Adversarial Label Poisoning Attack on Graph Neural Networks via Label Propagation",
    "authors": [
      "Ganlin Liu",
      "Xiaowei Huang",
      "Xinping Yi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6555_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650223.pdf",
    "published": "2020-08",
    "summary": "\"Graph neural networks (GNNs) have achieved outstanding performance in semi-supervised learning tasks with partially labeled graph structured data. However, labeling graph data for training is a challenging task, and inaccurate labels may mislead the training process to erroneous GNN models for node classification. In this paper, we consider label poisoning attacks on training data, where the labels of input data are modified by an adversary before training, to understand to what extent the state-of-the-art GNN models are resistant/vulnerable to such attacks. Specifically, we propose a label poisoning attack framework for graph convolutional networks (GCNs), inspired by the equivalence between label propagation and decoupled GCNs that separate message passing from neural networks. Instead of attacking the entire GCN models, we propose to attack solely label propagation for message passing. It turns out that a gradient-based attack on label propagation is effective and efficient towards the misleading of GCN training. More remarkably, such label attack can be topology-agnostic in the sense that the labels to be attacked can be efficiently chosen without knowing graph structures. Extensive experimental results demonstrate the effectiveness of the proposed method against state-of-the-art GCN-like models.\""
  },
  "eccv2022_main_revisitingouteroptimizationinadversarialtraining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Revisiting Outer Optimization in Adversarial Training",
    "authors": [
      "Ali Dabouei",
      "Fariborz Taherkhani",
      "Sobhan Soleymani",
      "Nasser M. Nasrabadi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7022_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650240.pdf",
    "published": "2020-08",
    "summary": "\"Despite the fundamental distinction between adversarial and natural training (AT and NT), AT methods generally adopt momentum SGD (MSGD) for the outer optimization. This paper aims to analyze this choice by investigating the overlooked role of outer optimization in AT. Our exploratory evaluations reveal that AT induces higher gradient norm and variance compared to NT. This phenomenon hinders the outer optimization in AT since the convergence rate of MSGD is highly dependent on the variance of the gradients. To this end, we propose an optimization method called ENGM which regularizes the contribution of each input example to the average mini-batch gradients. We prove that the convergence rate of ENGM is independent of the variance of the gradients, and thus, it is suitable for AT. We introduce a trick to reduce the computational cost of ENGM using empirical observations on the correlation between the norm of gradients w.r.t. the network parameters and input examples. Our extensive evaluations and ablation studies on CIFAR-10, CIFAR-100, and TinyImageNet demonstrate that ENGM and its variants consistently improve the performance of a wide range of AT methods. Furthermore, ENGM alleviates major shortcomings of AT including robust overfitting and high sensitivity to hyperparameter settings.\""
  },
  "eccv2022_main_zero-shotattributeattacksonfine-grainedrecognitionmodels": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Zero-Shot Attribute Attacks on Fine-Grained Recognition Models",
    "authors": [
      "Nasim Shafiee",
      "Ehsan Elhamifar"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7211_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650257.pdf",
    "published": "2020-08",
    "summary": "\"Zero-shot fine-grained recognition is an important classification task, whose goal is to recognize visually very similar classes, including the ones without training images. Despite recent advances on the development of zero-shot fine-grained recognition methods, the robustness of such models to adversarial attacks is not well understood. On the other hand, adversarial attacks have been widely studied for conventional classification with visually distinct classes. Such attacks, in particular, universal perturbations that are class-agnostic and ideally should generalize to unseen classes, however, cannot leverage or capture small distinctions among fine-grained classes. Therefore, we propose a compositional attribute-based framework for generating adversarial attacks on zero-shot fine-grained recognition models. To generate attacks that capture small differences between fine-grained classes, generalize well to previously unseen classes and can be applied in real-time, we propose to learn and compose multiple attribute-based universal perturbations (AUPs). Each AUP corresponds to an image-agnostic perturbation on a specific attribute. To build our attack, we compose AUPs with weights obtained by learning a class-attribute compatibility function. To learn the AUPs and the parameters of our model, we minimize a loss, consisting of a ranking loss and a novel utility loss, which ensures AUPs are effectively learned and utilized. By extensive experiments on three datasets for zero-shot fine-grained recognition, we show that our attacks outperform conventional universal classification attacks and transfer well between different recognition architectures.\""
  },
  "eccv2022_main_towardseffectiveandrobustneuraltrojandefensesviainputfiltering": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards Effective and Robust Neural Trojan Defenses via Input Filtering",
    "authors": [
      "Kien Do",
      "Haripriya Harikumar",
      "Hung Le",
      "Dung Nguyen",
      "Truyen Tran",
      "Santu Rana",
      "Dang Nguyen",
      "Willy Susilo",
      "Svetha Venkatesh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7224_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650277.pdf",
    "published": "2020-08",
    "summary": "\"Trojan attacks on deep neural networks are both dangerous and surreptitious. Over the past few years, Trojan attacks have advanced from using only a single input-agnostic trigger and targeting only one class to using multiple, input-specific triggers and targeting multiple classes. However, Trojan defenses have not caught up with this development. Most defense methods still make out-of-date assumptions about Trojan triggers and target classes, thus, can be easily circumvented by modern Trojan attacks. To deal with this problem, we propose two novel \"\"filtering\"\" defenses called Variational Input Filtering (VIF) and Adversarial Input Filtering (AIF) which leverage lossy data compression and adversarial learning respectively to effectively purify all potential Trojan triggers in the input at run time without making assumptions about the number of triggers/target classes or the input dependence property of triggers. In addition, we introduce a new defense mechanism called \"\"Filtering-then-Contrasting\"\" (FtC) which helps avoid the drop in classification accuracy on clean data caused by \"\"filtering\"\", and combine it with VIF/AIF to derive new defenses of this kind. Extensive experimental results and ablation studies show that our proposed defenses significantly outperform well-known baseline defenses in mitigating five advanced Trojan attacks including two recent state-of-the-art while being quite robust to small amounts of training data and large-norm triggers.\""
  },
  "eccv2022_main_scalingadversarialtrainingtolargeperturbationbounds": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Scaling Adversarial Training to Large Perturbation Bounds",
    "authors": [
      "Sravanti Addepalli",
      "Samyak Jain",
      "Gaurang Sriramanan",
      "R. Venkatesh Babu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7573_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650295.pdf",
    "published": "2020-08",
    "summary": "\"The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most Adversarial Training algorithms aim at defending attacks constrained within low magnitude Lp norm bounds, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness within larger bounds, against perturbations that may be perceptible, but do not change human (or Oracle) prediction. The presence of images that flip Oracle predictions and those that do not makes this a challenging setting for adversarial robustness. We discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds (such as an L-inf bound of 16/255 on CIFAR-10) while outperforming existing defenses (AWP, TRADES, PGD-AT) at standard bounds (8/255) as well.\""
  },
  "eccv2022_main_exploitingthelocalparaboliclandscapesofadversariallossestoaccelerateblack-boxadversarialattack": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exploiting the Local Parabolic Landscapes of Adversarial Losses to Accelerate Black-Box Adversarial Attack",
    "authors": [
      "Hoang Tran",
      "Dan Lu",
      "Guannan Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7718_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650311.pdf",
    "published": "2020-08",
    "summary": "\"Existing black-box adversarial attacks on image classifiers update the perturbation at each iteration from only a small number of queries of the loss function. Since the queries contain very limited information about the loss, black-box methods usually require much more queries than white-box methods. We propose to improve the query efficiency of black-box methods by exploiting the smoothness of the local loss landscape. However, many adversarial losses are not locally smooth with respect to pixel perturbations. To resolve this issue, our first contribution is to theoretically and experimentally justify that the adversarial losses of many standard and robust image classifiers behave like parabolas with respect to perturbations in the Fourier domain. Our second contribution is to exploit the parabolic landscape to build a quadratic approximation of the loss around the current state, and use this approximation to interpolate the loss value as well as update the perturbation without additional queries. Since the local region is already informed by the quadratic fitting, we use large perturbation steps to explore far areas. We demonstrate the efficiency of our method on MNIST, CIFAR-10 and ImageNet datasets for various standard and robust models, as well as on Google Cloud Vision. The experimental results show that exploiting the loss landscape can help significantly reduce the number of queries and increase the success rate. Our codes are available at https://github.com/HoangATran/BABIES.\""
  },
  "eccv2022_main_generativedomainadaptationforfaceanti-spoofing": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Generative Domain Adaptation for Face Anti-Spoofing",
    "authors": [
      "Qianyu Zhou",
      "Ke-Yue Zhang",
      "Taiping Yao",
      "Ran Yi",
      "Kekai Sheng",
      "Shouhong Ding",
      "Lizhuang Ma"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/16_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650328.pdf",
    "published": "2020-08",
    "summary": "\"Face anti-spoofing (FAS) approaches based on unsupervised domain adaption (UDA) have drawn growing attention due to promising performances for target scenarios. Most existing UDA FAS methods typically fit the trained models to the target domain via aligning the distribution of semantic high-level features. However, insufficient supervision of unlabeled target domains and neglect of low-level feature alignment degrade the performances of existing methods. To address these issues, we propose a novel perspective of UDA FAS that directly fits the target data to the models, i.e., stylizes the target data to the source-domain style via image translation, and further feeds the stylized data into the well-trained source model for classification. The proposed Generative Domain Adaptation (GDA) framework combines two carefully designed consistency constraints: 1) Inter-domain neural statistic consistency guides the generator in narrowing the inter-domain gap. 2) Dual-level semantic consistency ensures the semantic quality of stylized images. Besides, we propose intra-domain spectrum mixup to further expand target data distributions to ensure generalization and reduce the intra-domain gap. Extensive experiments and visualizations demonstrate the effectiveness of our method against the state-of-the-art methods.\""
  },
  "eccv2022_main_metagaitlearningtolearnanomnisampleadaptiverepresentationforgaitrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MetaGait: Learning to Learn an Omni Sample Adaptive Representation for Gait Recognition",
    "authors": [
      "Huanzhang Dou",
      "Pengyi Zhang",
      "Wei Su",
      "Yunlong Yu",
      "Xi Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1202_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650350.pdf",
    "published": "2020-08",
    "summary": "\"Gait recognition, which aims at identifying individuals by their walking patterns, has recently drawn increasing research attention. However, gait recognition still suffers from the conflicts between the limited binary visual clues of the silhouette and numerous covariates with diverse scales, which brings challenges to the model\u2019s adaptiveness. In this paper, we address this conflict by developing a novel MetaGait that learns to learn an omni sample adaptive representation. Towards this goal, MetaGait injects meta-knowledge, which could guide the model to perceive sample-specific properties, into the calibration network of the attention mechanism to improve the adaptiveness from the omni-scale, omni-dimension, and omni-process perspectives. Specifically, we leverage the meta-knowledge across the entire process, where Meta Triple Attention and Meta Temporal Pooling are presented respectively to adaptively capture omni-scale dependency from spatial/channel/temporal dimensions simultaneously and to adaptively aggregate temporal information through integrating the merits of three complementary temporal aggregation methods. Extensive experiments demonstrate the state-of-the-art performance of the proposed MetaGait. On CASIA-B, we achieve rank-1 accuracy of 98.7%, 96.0%, and 89.3% under three conditions, respectively. On OU-MVLP, we achieve rank-1 accuracy of 92.4%.\""
  },
  "eccv2022_main_gaitedgebeyondplainend-to-endgaitrecognitionforbetterpracticality": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "GaitEdge: Beyond Plain End-to-End Gait Recognition for Better Practicality",
    "authors": [
      "Junhao Liang",
      "Chao Fan",
      "Saihui Hou",
      "Chuanfu Shen",
      "Yongzhen Huang",
      "Shiqi Yu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2463_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650368.pdf",
    "published": "2020-08",
    "summary": "\"Gait is one of the most promising biometrics to identify individuals at a long distance. Although most previous methods have focused on recognizing the silhouettes, several end-to-end methods that extract gait features directly from RGB images perform better. However, we demonstrate that these end-to-end methods may inevitably suffer from the gait-irrelevant noises, i.e. low-level texture and color information. Experimentally, we design the cross-domain evaluation to support this view. In this work, we propose a novel end-to-end framework named GaitEdge which can effectively block gait-irrelevant information and release end-to-end training potential. Specifically, GaitEdge synthesizes the output of the pedestrian segmentation network and then feeds it to the subsequent recognition network, where the synthetic silhouettes consist of trainable edges of bodies and fixed interiors to limit the information that the recognition network receives. Besides, GaitAlign for aligning silhouettes is embedded into the GaitEdge without losing differentiability. Experimental results on CASIA-B and our newly built TTG-200 indicate that GaitEdge significantly outperforms the previous methods and provides a more practical end-to-end paradigm. All the source code are available at https://github.com/ShiqiYu/OpenGait.\""
  },
  "eccv2022_main_uia-vitunsupervisedinconsistency-awaremethodbasedonvisiontransformerforfaceforgerydetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "UIA-ViT: Unsupervised Inconsistency-Aware Method Based on Vision Transformer for Face Forgery Detection",
    "authors": [
      "Wanyi Zhuang",
      "Qi Chu",
      "Zhentao Tan",
      "Qiankun Liu",
      "Haojie Yuan",
      "Changtao Miao",
      "Zixiang Luo",
      "Nenghai Yu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6739_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650384.pdf",
    "published": "2020-08",
    "summary": "\"Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is time-consuming. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT. Thanks to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Specifically, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations that are derived from multivariate Gaussian estimation. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.\""
  },
  "eccv2022_main_effectivepresentationattackdetectiondrivenbyfacerelatedtask": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Effective Presentation Attack Detection Driven by Face Related Task",
    "authors": [
      "Wentian Zhang",
      "Haozhe Liu",
      "Feng Liu",
      "Raghavendra Ramachandra",
      "Christoph Busch"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7687_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650400.pdf",
    "published": "2020-08",
    "summary": "\"The robustness and generalization ability of Presentation Attack Detection (PAD) methods is critical to ensure the security of Face Recognition Systems (FRSs). However, in a real scenario, Presentation Attacks (PAs) are various and it is hard to predict the Presentation Attack Instrument (PAI) species that will be used by the attacker. Existing PAD methods are highly dependent on the limited training set and cannot generalize well to unknown PAI species. Unlike this specific PAD task, other face related tasks trained by huge amount of real faces (e.g. face recognition and attribute editing) can be effectively adopted into different application scenarios. Inspired by this, we propose to trade position of PAD and face related work in a face system and apply the free acquired prior knowledge from face related tasks to solve face PAD, so as to improve the generalization ability in detecting PAs. The proposed method, first introduces task specific features from other face related task, then, we design a Cross-Modal Adapter using a Graph Attention Network (GAT) to re-map such features to adapt to PAD task. Finally, face PAD is achieved by using the hierarchical features from a CNN-based PA detector and the re-mapped features. The experimental results show that the proposed method can achieve significant improvements in the complicated and hybrid datasets, when compared with the state-of-the-art methods. In particular, when training on the datasets OULU-NPU, CASIA-FASD, and Idiap Replay-Attack, we obtain HTER (Half Total Error Rate) of 5.48% for the testing dataset MSU-MFSD, outperforming the baseline by 7.39%. The source code is available at https://github.com/WentianZhang-ML/FRT-PAD.\""
  },
  "eccv2022_main_ppttoken-prunedposetransformerformonocularandmulti-viewhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PPT: Token-Pruned Pose Transformer for Monocular and Multi-View Human Pose Estimation",
    "authors": [
      "Haoyu Ma",
      "Zhe Wang",
      "Yifei Chen",
      "Deying Kong",
      "Liangjian Chen",
      "Xingwei Liu",
      "Xiangyi Yan",
      "Hao Tang",
      "Xiaohui Xie"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/46_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650416.pdf",
    "published": "2020-08",
    "summary": "\"Recently, the vision transformer and its variants have played an increasingly important role in both monocular and multi-view human pose estimation. Considering image patches as tokens, transformers can model the global dependencies within the entire image or across images from other views. However, global attention is computationally expensive. As a consequence, it is difficult to scale up these transformer-based methods to high-resolution features and many views. In this paper, we propose the token-Pruned Pose Transformer (PPT) for 2D human pose estimation, which can locate a rough human mask and performs self-attention only within selected tokens. Furthermore, we extend our PPT to multi-view human pose estimation. Built upon PPT, we propose a new cross-view fusion strategy, called human area fusion, which considers all human foreground pixels as corresponding candidates. Experimental results on COCO and MPII demonstrate that our PPT can match the accuracy of previous pose transformer methods while reducing the computation. Moreover, experiments on Human 3.6M and Ski-Pose demonstrate that our Multi-view PPT can efficiently fuse cues from multiple views and achieve new state-of-the-art results. Source code and trained model can be found at \\url{https://github.com/HowieMa/PPT}.\""
  },
  "eccv2022_main_avatarposerarticulatedfull-bodyposetrackingfromsparsemotionsensing": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion Sensing",
    "authors": [
      "Jiaxi Jiang",
      "Paul Streli",
      "Huajian Qiu",
      "Andreas Fender",
      "Larissa Laich",
      "Patrick Snape",
      "Christian Holz"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/74_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650434.pdf",
    "published": "2020-08",
    "summary": "\"Today\u2019s Mixed Reality head-mounted displays track the user\u2019s head pose in world space as well as the user\u2019s hands for interaction in both Augmented Reality and Virtual Reality scenarios. While this is adequate to support user input, it unfortunately limits users\u2019 virtual representations to just their upper bodies. Current systems thus resort to floating avatars, whose limitation is particularly evident in collaborative settings. To estimate full-body poses from the sparse input sources, prior work has incorporated additional trackers and sensors at the pelvis or lower body, which increases setup complexity and limits practical application in mobile settings. In this paper, we present AvatarPoser, the first learning-based method that predicts full-body poses in world coordinates using only motion input from the user\u2019s head and hands. Our method builds on a Transformer encoder to extract deep features from the input signals and decouples global motion from the learned local joint orientations to guide pose estimation. To obtain accurate full-body motions that resemble motion capture animations, we refine the arm joints\u2019 positions using an optimization routine with inverse kinematics to match the original tracking input. In our evaluation, AvatarPoser achieved new state-of-the-art results in evaluations on large motion capture datasets (AMASS). At the same time, our method\u2019s inference speed supports real-time operation, providing a practical interface to support holistic avatar control and representation for Metaverse applications.\""
  },
  "eccv2022_main_p-stmopre-trainedspatialtemporalmany-to-onemodelfor3dhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "P-STMO: Pre-trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation",
    "authors": [
      "Wenkang Shan",
      "Zhenhua Liu",
      "Xinfeng Zhang",
      "Shanshe Wang",
      "Siwei Ma",
      "Wen Gao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/196_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650453.pdf",
    "published": "2020-08",
    "summary": "\"This paper introduces a novel Pre-trained Spatial Temporal Many-to-One (P-STMO) model for 2D-to-3D human pose estimation task. To reduce the difficulty of capturing spatial and temporal information, we divide this task into two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I, a self-supervised pre-training sub-task, termed masked pose modeling, is proposed. The human joints in the input sequence are randomly masked in both spatial and temporal domains. A general form of denoising auto-encoder is exploited to recover the original 2D poses and the encoder is capable of capturing spatial and temporal dependencies in this way. In Stage II, the pre-trained encoder is loaded to STMO model and fine-tuned. The encoder is followed by a many-to-one frame aggregator to predict the 3D pose in the current frame. Especially, an MLP block is utilized as the spatial feature extractor in STMO, which yields better performance than other methods. In addition, a temporal downsampling strategy is proposed to diminish data redundancy. Extensive experiments on two benchmarks show that our method outperforms state-of-the-art methods with fewer parameters and less computational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on Human3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings a 1.5-7.1\u00d7 speedup to state-of-the-art methods. Code is available at https://github.com/paTRICK-swk/P-STMO.\""
  },
  "eccv2022_main_d\\&dlearninghumandynamicsfromdynamiccamera": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "D\\&D: Learning Human Dynamics from Dynamic Camera",
    "authors": [
      "Jiefeng Li",
      "Siyuan Bian",
      "Chao Xu",
      "Gang Liu",
      "Gang Yu",
      "Cewu Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/229_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650470.pdf",
    "published": "2020-08",
    "summary": "\"3D human pose estimation from a monocular video has recently seen significant improvements. However, most state-of-the-art methods are kinematics-based, which are prone to physically implausible motions with pronounced artifacts. Current dynamics-based methods can predict physically plausible motion but are restricted to simple scenarios with static camera view. In this work, we present D&D (Learning Human Dynamics from Dynamic Camera), which leverages the laws of physics to reconstruct 3D human motion from the in-the-wild videos with a moving camera. D&D introduces inertial force control (IFC) to explain the 3D human motion in the non-inertial local frame by considering the inertial forces of the dynamic camera. To learn the ground contact with limited annotations, we develop probabilistic contact torque (PCT), which is computed by differentiable sampling from contact probabilities and used to generate motions. The contact state can be weakly supervised by encouraging the model to generate correct motions. Furthermore, we propose an attentive PD controller that adjusts target pose states using temporal information to obtain smooth and accurate pose control. Our approach is entirely neural-based and runs without offline optimization or simulation in physics engines. Experiments on large-scale 3D human motion benchmarks demonstrate the effectiveness of D&D, where we exhibit superior performance against both state-of-the-art kinematics-based and dynamics-based methods. Code is available at https://github.com/Jeff-sjtu/DnD\""
  },
  "eccv2022_main_explicitocclusionreasoningformulti-person3dhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Explicit Occlusion Reasoning for Multi-Person 3D Human Pose Estimation",
    "authors": [
      "Qihao Liu",
      "Yi Zhang",
      "Song Bai",
      "Alan Yuille"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/615_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650488.pdf",
    "published": "2020-08",
    "summary": "\"Occlusion poses a great threat to monocular multi-person 3D human pose estimation due to large variability in terms of the shape, appearance, and position of occluders. While existing methods try to handle occlusion with pose priors/constraints, data augmentation, or implicit reasoning, they still fail to generalize to unseen poses or occlusion cases and may make large mistakes when multiple people are present. Inspired by the remarkable ability of humans to infer occluded joints from visible cues, we develop a method to explicitly model this process that significantly improves bottom-up multi-person human pose estimation with or without occlusions. First, we split the task into two subtasks: visible keypoints detection and occluded keypoints reasoning, and propose a Deeply Supervised Encoder Distillation (DSED) network to solve the second one. To train our model, we propose a Skeleton-guided human Shape Fitting (SSF) approach to generate pseudo occlusion labels on the existing datasets, enabling explicit occlusion reasoning. Experiments show that explicitly learning from occlusions improves human pose estimation. In addition, exploiting feature-level information of visible joints allows us to reason about occluded joints more accurately. Our method outperforms both the state-of-the-art top-down and bottom-up methods on several benchmarks.\""
  },
  "eccv2022_main_couchtowardscontrollablehuman-chairinteractions": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "COUCH: Towards Controllable Human-Chair Interactions",
    "authors": [
      "Xiaohan Zhang",
      "Bharat Lal Bhatnagar",
      "Sebastian Starke",
      "Vladimir Guzov",
      "Gerard Pons-Moll"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/852_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650508.pdf",
    "published": "2020-08",
    "summary": "\"Humans can interact with an object in the scene in many different ways, which are often associated with different modalities of contacting with the object. This creates a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of interacting with a particular object without considering fine-grained control of limb motion variations within one task. In this work, we drive this direction and study the problem of synthesizing scene interactions conditioned on a wide range of contact positions on the object. We pick human-chair interactions as an example. We propose a novel synthesis framework COUCH, which firstly plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows consistent quantitative and qualitative improvements over existing methods for learning human-object interactions.\""
  },
  "eccv2022_main_identity-awarehandmeshestimationandpersonalizationfromrgbimages": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Identity-Aware Hand Mesh Estimation and Personalization from RGB Images",
    "authors": [
      "Deying Kong",
      "Linguang Zhang",
      "Liangjian Chen",
      "Haoyu Ma",
      "Xiangyi Yan",
      "Shanlin Sun",
      "Xingwei Liu",
      "Kun Han",
      "Xiaohui Xie"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1076_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650526.pdf",
    "published": "2020-08",
    "summary": "\"Reconstructing 3D hand meshes from monocular RGB images has attracted increasing amount of attention due to its enormous potential applications in the field of AR/VR. Most state-of-the-art methods attempt to tackle this task in an anonymous manner. Specifically, the identity of the subject is ignored even though it is practically available in real applications where the user is unchanged in a continuous recording session. In this paper, we propose an identity-aware hand mesh estimation model, which can incorporate the identity information represented by the intrinsic shape parameters of the subject. We demonstrate the importance of the identity information by comparing the proposed identity-aware model to a baseline which treats subject anonymously. Furthermore, to handle the use case where the test subject is unseen, we propose a novel personalization pipeline to calibrate the intrinsic shape parameters using only a few unlabeled RGB images of the subject. Experiments on two large scale public datasets validate the state-of-the-art performance of our proposed method.\""
  },
  "eccv2022_main_c3pcross-domainposepriorpropagationforweaklysupervised3dhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "C3P: Cross-Domain Pose Prior Propagation for Weakly Supervised 3D Human Pose Estimation",
    "authors": [
      "Cunlin Wu",
      "Yang Xiao",
      "Boshen Zhang",
      "Mingyang Zhang",
      "Zhiguo Cao",
      "Joey Tianyi Zhou"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1364_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650544.pdf",
    "published": "2020-08",
    "summary": "\"This paper first proposes and solves weakly supervised 3D human pose estimation (HPE) problem in point cloud, via propagating the pose prior within unlabelled RGB-point cloud sequence to 3D domain. Our approach termed C3P does not require any labor-consuming 3D keypoint annotation for training. To this end, we propose to transfer 2D HPE annotation information within the existing large-scale RGB datasets (e.g., MS COCO) to 3D task, using unlabelled RGB-point cloud sequence easy to acquire for linking 2D and 3D domains. The self-supervised 3D HPE clues within point cloud sequence are also exploited, concerning spatial-temporal constraints on human body symmetry, skeleton length and joints\u2019 motion. And, a refined point set network structure for weakly supervised 3D HPE is proposed in encoder-decoder manner. The experiments on CMU Panoptic and ITOP datasets demonstrate that, our method can achieve the comparable results to the 3D fully supervised state-of-the-art counterparts. When large-scale unlabelled data (e.g., NTU RGB+D 60) is used, our approach can even outperform them under the more challenging cross-setup test setting. The source code is released at \"\"https://github.com/wucunlin/C3P\"\" for research use only.\""
  },
  "eccv2022_main_pose-ndfmodelinghumanposemanifoldswithneuraldistancefields": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields",
    "authors": [
      "Garvita Tiwari",
      "Dimitrije Anti\u0107",
      "Jan Eric Lenssen",
      "Nikolaos Sarafianos",
      "Tony Tung",
      "Gerard Pons-Moll"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1413_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650562.pdf",
    "published": "2020-08",
    "summary": "\"We present Pose-NDF, a continuous model for plausible human poses based on neural distance fields (NDFs). Pose or motion priors are important for generating realistic new poses and for reconstructing accurate poses from noisy or partial observations. Pose-NDF learns a manifold of plausible poses as the zero level set of a neural implicit function, extending the idea of modelling implicit surfaces in 3D to the high dimensional domain SO(3)K, where a human pose is defined by a single data point, represented by K quaternions. The resulting high dimensional implicit function can be differentiated with respect to the input poses and thus can be used to project arbitrary poses onto the manifold by using gradient descent on the set of 3-dimensional hyperspheres. In contrast to previous VAE-based human pose priors, which transform the pose space into a Gaussian distribution, we model the actual pose manifold, preserving the distances between poses. We demonstrate that this approach and thus, Pose-NDF, outperforms existing state-of-the-art methods as a prior in various downstream tasks, ranging from denoising real world human mocap data, pose recovery from occluded data to 3D pose reconstruction from images. Furthermore, we show that it can be used to generate more diverse poses by random sampling and projection than VAE based methods. We will release our code and pre-trained model for further research.\""
  },
  "eccv2022_main_cliffcarryinglocationinformationinfullframesintohumanposeandshapeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation",
    "authors": [
      "Zhihao Li",
      "Jianzhuang Liu",
      "Zhensong Zhang",
      "Songcen Xu",
      "Youliang Yan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1620_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650580.pdf",
    "published": "2020-08",
    "summary": "\"Top-down methods dominate the field of 3D human pose and shape estimation, because they are decoupled from human detection and allow researchers to focus on the core problem. However, cropping, their first step, discards the location information from the very beginning, which makes themselves unable to accurately predict the global rotation in the original camera coordinate system. To address this problem, we propose to Carry Location Information in Full Frames (CLIFF) into this task. Specifically, we feed more holistic features to CLIFF by concatenating the cropped image feature with its bounding box information. We calculate the 2D reprojection loss with a broader view of the full frame, taking a projection process similar to that of the person projected in the image. Fed and supervised by global-location-aware information, CLIFF directly predicts the global rotation along with more accurate articulated poses. Besides, we propose a pseudo-ground-truth annotator based on CLIFF, which provides high-quality 3D annotations for in-the-wild 2D datasets and offers crucial full supervision for regression-based methods. Extensive experiments on popular benchmarks show that CLIFF outperforms prior arts by a significant margin, and reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track).\""
  },
  "eccv2022_main_deciwatchasimplebaselinefor10\u00d7efficient2dand3dposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DeciWatch: A Simple Baseline for 10\u00d7 Efficient 2D and 3D Pose Estimation",
    "authors": [
      "Ailing Zeng",
      "Xuan Ju",
      "Lei Yang",
      "Ruiyuan Gao",
      "Xizhou Zhu",
      "Bo Dai",
      "Qiang Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1845_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650597.pdf",
    "published": "2020-08",
    "summary": "\"This paper proposes a simple baseline framework for video-based 2D/3D human pose estimation that can achieve 10 times efficiency improvement over existing works without any performance degradation, named DeciWatch. Unlike current solutions that estimate each frame in a video, DeciWatch introduces a simple yet effective sample-denoise-recover framework that only watches sparsely sampled frames, taking advantage of the continuity of human motions and the lightweight pose representation. Specifically, DeciWatch uniformly samples less than 10% video frames for detailed estimation, denoises the estimated 2D/3D poses with an efficient Transformer architecture, and then accurately recovers the rest of the frames using another Transformer-based network. Comprehensive experimental results on three video-based human pose estimation and body mesh recovery tasks with four datasets validate the efficiency and effectiveness of DeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.\""
  },
  "eccv2022_main_smoothnetaplug-and-playnetworkforrefininghumanposesinvideos": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos",
    "authors": [
      "Ailing Zeng",
      "Lei Yang",
      "Xuan Ju",
      "Jiefeng Li",
      "Jianyi Wang",
      "Qiang Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1848_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650615.pdf",
    "published": "2020-08",
    "summary": "\"When analyzing human motion videos, the output jitters from existing pose estimators are highly-unbalanced with varied estimation errors across frames. Most frames in a video are relatively easy to estimate and only suffer from slight jitters. In contrast, for rarely seen or occluded actions, the estimated positions of multiple joints largely deviate from the ground truth values for a consecutive sequence of frames, rendering significant jitters on them. To tackle this problem, we propose to attach a dedicated temporal-only refinement network to existing pose estimators for jitter mitigation, named SmoothNet. Unlike existing learning-based solutions that employ spatio-temporal models to co-optimize per-frame precision and temporal smoothness at all the joints, SmoothNet models the natural smoothness characteristics in body movements by learning the long-range temporal relations of every joint without considering the noisy correlations among joints. With a simple yet effective motion-aware fully-connected network, SmoothNet improves the temporal smoothness of existing pose estimators significantly and enhances the estimation accuracy of those challenging frames as a side-effect. Moreover, as a temporal-only model, a unique advantage of SmoothNet is its strong transferability across various types of estimators and datasets. Comprehensive experiments on five datasets with eleven popular backbone networks across 2D and 3D pose estimation and body recovery tasks demonstrate the efficacy of the proposed solution. Code is available at https://github.com/cure-lab/SmoothNet.\""
  },
  "eccv2022_main_posetransasimpleyeteffectiveposetransformationaugmentationforhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PoseTrans: A Simple yet Effective Pose Transformation Augmentation for Human Pose Estimation",
    "authors": [
      "Wentao Jiang",
      "Sheng Jin",
      "Wentao Liu",
      "Chen Qian",
      "Ping Luo",
      "Si Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2033_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650633.pdf",
    "published": "2020-08",
    "summary": "\"Human pose estimation aims to accurately estimate a wide variety of human poses. However, existing datasets often follow a long-tailed distribution that unusual poses only occupy a small portion, which further leads to the lack of diversity of rare poses. These issues result in the inferior generalization ability of current pose estimators. In this paper, we present a simple yet effective data augmentation method, termed Pose Transformation (PoseTrans), to alleviate the aforementioned problems. Specifically, we propose Pose Transformation Module (PTM) to create new training samples that have diverse poses and adopt a pose discriminator to ensure the plausibility of the augmented poses. Besides, we propose Pose Clustering Module (PCM) to measure the pose rarity and select the \u201crarest\u201d poses to help balance the long-tailed distribution. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method, especially on rare poses. Also, our method is efficient and simple to implement, which can be easily integrated into the training pipeline of existing pose estimation models.\""
  },
  "eccv2022_main_multi-person3dposeandshapeestimationviainversekinematicsandrefinement": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Multi-Person 3D Pose and Shape Estimation via Inverse Kinematics and Refinement",
    "authors": [
      "Junuk Cha",
      "Muhammad Saqlain",
      "GeonU Kim",
      "Mingyu Shin",
      "Seungryul Baek"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2418_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650650.pdf",
    "published": "2020-08",
    "summary": "\"Estimating 3D poses and shapes in the form of meshes from monocular RGB images is challenging. Obviously, it is more difficult than estimating 3D poses only in the form of skeletons or heatmaps. When interacting persons are involved, the 3D mesh reconstruction becomes more challenging due to the ambiguity introduced by person-to-person occlusions. To tackle the challenges, we propose a coarse-to-fine pipeline that benefits from 1) inverse kinematics from the occlusion-robust 3D skeleton estimation and 2) transformer-based relation-aware refinement techniques. In our pipeline, we first obtain occlusion-robust 3D skeletons for multiple persons from an RGB image. Then, we apply inverse kinematics to convert the estimated skeletons to deformable 3D mesh parameters. Finally, we apply the transformer-based mesh refinement that refines the obtained mesh parameters considering intra- and inter-person relations of 3D meshes. Via extensive experiments, we demonstrate the effectiveness of our method, outperforming state-of-the-arts on 3DPW, MuPoTS and AGORA datasets.\""
  },
  "eccv2022_main_overlookedposesactuallymakesensedistillingprivilegedknowledgeforhumanmotionprediction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Overlooked Poses Actually Make Sense: Distilling Privileged Knowledge for Human Motion Prediction",
    "authors": [
      "Xiaoning Sun",
      "Qiongjie Cui",
      "Huaijiang Sun",
      "Bin Li",
      "Weiqing Li",
      "Jianfeng Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3452_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650668.pdf",
    "published": "2020-08",
    "summary": "\"Previous works on human motion prediction follow the pattern of building a mapping relation between the sequence observed and the one to be predicted. However, due to the inherent complexity of multivariate time series data, it still remains a challenge to find the extrapolation relation between motion sequences. In this paper, we present a new prediction pattern, which introduces previously overlooked human poses, to implement the prediction task from the view of interpolation. These poses exist after the predicted sequence, and form the privileged sequence. To be specific, we first propose an InTerPolation learning Network (ITP-Network) that encodes both the observed sequence and the privileged sequence to interpolate the in-between predicted sequence, wherein the embedded Privileged-sequence-Encoder (Priv-Encoder) learns the privileged knowledge (PK) simultaneously. Then, we propose a Final Prediction Network (FP-Network) for which the privileged sequence is not observable, but is equipped with a novel PK-Simulator that distills PK learned from the previous network. This simulator takes as input the observed sequence, but approximates the behavior of Priv-Encoder, enabling FP-Network to imitate the interpolation process. Extensive experimental results demonstrate that our prediction pattern achieves state-of the-art performance on benchmarked H3.6M, CMU-Mocap and 3DPW datasets in both short-term and long-term predictions.\""
  },
  "eccv2022_main_structuraltriangulationaclosed-formsolutiontoconstrained3dhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Structural Triangulation: A Closed-Form Solution to Constrained 3D Human Pose Estimation",
    "authors": [
      "Zhuo Chen",
      "Xu Zhao",
      "Xiaoyue Wan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3470_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650685.pdf",
    "published": "2020-08",
    "summary": "\"We propose Structural Triangulation, a closed-form solution for optimal 3D human pose considering multi-view 2D pose estimations, calibrated camera parameters, and bone lengths. To start with, we focus on embedding structural constraints of human body in the process of 2D-to-3D inference using triangulation. Assume bone lengths are known in prior, then the inference process is formulated as a constrained optimization problem. By proper approximation, the closed-form solution to this problem is achieved. Further, we generalize our method with Step Constraint Algorithm to help converge when large error occurs in 2D estimations. In experiment, public datasets (Human3.6M and Total Capture) and synthesized data are used for evaluation. Our method achieves state-of-the-art results on Human3.6M Dataset when bone lengths are known and competitive results when they are not. The generality and efficiency of our method are also demonstrated.\""
  },
  "eccv2022_main_audio-drivenstylizedgesturegenerationwithflow-basedmodel": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Audio-Driven Stylized Gesture Generation with Flow-Based Model",
    "authors": [
      "Sheng Ye",
      "Yu-Hui Wen",
      "Yanan Sun",
      "Ying He",
      "Ziyang Zhang",
      "Yaoyuan Wang",
      "Weihua He",
      "Yong-Jin Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3948_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650701.pdf",
    "published": "2020-08",
    "summary": "\"Generating stylized audio-driven gestures for robots and virtual avatars has attracted increasing considerations recently. Existing methods require style labels (e.g. speaker identities), or complex preprocessing of the data to obtain style control parameters. In this paper, we propose a new end-to-end flow-based model, which can generate audio-driven gestures of arbitrary styles without the preprocessing procedure and style labels. To achieve this goal, we introduce a global encoder and a gesture perceptual loss into the classic generative flow model to capture both the global and local information. We conduct extensive experiments on two benchmark datasets: the TED Dataset and the Trinity Dataset. Both quantitative and qualitative evaluations show that the proposed model outperforms state-of-the-art models.\""
  },
  "eccv2022_main_self-constrainedinferenceoptimizationonstructuralgroupsforhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation",
    "authors": [
      "Zhehan Kan",
      "Shuoshuo Chen",
      "Zeng Li",
      "Zhihai He"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3985_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650718.pdf",
    "published": "2020-08",
    "summary": "\"We observe that human poses exhibit strong group-wise structural correlation and spatial coupling between keypoints due to the biological constraints of different body parts. This group-wise structural correlation can be explored to improve the accuracy and robustness of human pose estimation. In this work, we develop a self-constrained prediction-verification network to characterize and learn the structural correlation between keypoints during training. During the inference stage, the feedback information from the verification network allows us to perform further optimization of pose prediction, which significantly improves the performance of human pose estimation. Specifically, we partition the keypoints into groups according to the biological structure of human body. Within each group, the keypoints are further partitioned into two subsets, high-confidence base keypoints and low-confidence terminal keypoints. We develop a self-constrained prediction-verification network to perform forward and backward predictions between these keypoint subsets. One fundamental challenge in pose estimation, as well as in generic prediction tasks, is that there is no mechanism for us to verify if the obtained pose estimation or prediction results are accurate or not, since the ground truth is not available. Once successfully learned, the verification network serves as an accuracy verification module for the forward pose prediction. During the inference stage, it can be used to guide the local optimization of the pose estimation results of low-confidence keypoints with the self-constrained loss on high-confidence keypoints as the objective function. Our extensive experimental results on benchmark MS COCO and CrowdPose datasets demonstrate that the proposed method can significantly improve the pose estimation results.\""
  },
  "eccv2022_main_unrealegoanewdatasetforrobustegocentric3dhumanmotioncapture": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture",
    "authors": [
      "Hiroyasu Akada",
      "Jian Wang",
      "Soshi Shimada",
      "Masaki Takahashi",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4021_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660001.pdf",
    "published": "2020-08",
    "summary": "\"We present UnrealEgo, a new large-scale naturalistic dataset for egocentric 3D human pose estimation. UnrealEgo is based on an advanced concept of eyeglasses equipped with two fisheye cameras that can be used in unconstrained environments. We design their virtual prototype and attach them to 3D human models for stereo view capture. We next generate a large corpus of human motions. As a consequence, UnrealEgo is the first dataset to provide in-the-wild stereo images with the largest variety of motions among existing egocentric datasets. Furthermore, we propose a new benchmark method with a simple but effective idea of devising a 2D keypoint estimation module for stereo inputs to improve 3D human pose estimation. The extensive experiments show that our approach outperforms the previous state-of-the-art methods qualitatively and quantitatively. UnrealEgo and our source codes are available on our project web page.\""
  },
  "eccv2022_main_skeleton-partedgraphscatteringnetworksfor3dhumanmotionprediction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Skeleton-Parted Graph Scattering Networks for 3D Human Motion Prediction",
    "authors": [
      "Maosen Li",
      "Siheng Chen",
      "Zijing Zhang",
      "Lingxi Xie",
      "Qi Tian",
      "Ya Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4239_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660018.pdf",
    "published": "2020-08",
    "summary": "\"Graph convolutional network based methods that model the body joints\u2019 relations, have recently shown great promise in 3D skeleton-based human motion prediction. However, these methods have two critical issues: first, deep graph convolutions filter features within only limited graph spectrum band, losing sufficient information in the full band; second, using a single graph to model the whole body underestimates the diverse patterns in various body-parts. To address the first issue, we propose adaptive graph scattering, which leverages multiple trainable band-pass graph filters to decompose pose features into various graph spectrum bands to provide richer information, promoting more comprehensive feature extraction. To address the second issue, body parts are modeled separately to learn diverse dynamics, which enables finer feature extraction along the spatial dimensions. Integrating the above two designs, we propose a novel skeleton-parted graph scattering network (SPGSN). The cores of the model are cascaded multi-part graph scattering blocks (MPGSBs), which build adaptive graph scattering for large-band graph filtering on diverse body-parts, as well as fuse the decomposed features based on the inferred spectrum importance and body-part interactions. Extensive experiments have shown that SPGSN outperforms state-of-the-art methods by remarkable margins of 13.8%, 9.3% and 2.7% in terms of 3D mean per joint position error (MPJPE) on Human3.6M, CMU Mocap and 3DPW datasets, respectively.\""
  },
  "eccv2022_main_rethinkingkeypointrepresentationsmodelingkeypointsandposesasobjectsformulti-personhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation",
    "authors": [
      "William McNally",
      "Kanav Vats",
      "Alexander Wong",
      "John McPhee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4439_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660036.pdf",
    "published": "2020-08",
    "summary": "\"In keypoint estimation tasks such as human pose estimation, heatmap-based regression is the dominant approach despite possessing notable drawbacks: heatmaps intrinsically suffer from quantization error and require excessive computation to generate and post-process. Motivated to find a more efficient solution, we propose to model individual keypoints and sets of spatially related keypoints (i.e., poses) as objects within a dense single-stage anchor-based detection framework. Hence, we call our method KAPAO (pronounced \"\"Ka-Pow\"\"), for Keypoints And Poses As Objects. KAPAO is applied to the problem of single-stage multi-person human pose estimation by simultaneously detecting human pose and keypoint objects and fusing the detections to exploit the strengths of both object representations. In experiments, we observe that KAPAO is faster and more accurate than previous methods, which suffer greatly from heatmap post-processing. The accuracy-speed trade-off is especially favourable in the practical setting when not using test-time augmentation. Source code: https://github.com/wmcnally/kapao.\""
  },
  "eccv2022_main_virtualposelearninggeneralizable3dhumanposemodelsfromvirtualdata": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data",
    "authors": [
      "Jiajun Su",
      "Chunyu Wang",
      "Xiaoxuan Ma",
      "Wenjun Zeng",
      "Yizhou Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4551_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660054.pdf",
    "published": "2020-08",
    "summary": "\"While monocular 3D pose estimation seems to have achieved very accurate results on the public datasets, their generalization ability is largely overlooked. In this work, we perform a systematic evaluation of the existing methods and find that they get notably larger errors when tested on different cameras, human poses and appearance. To address the problem, we introduce VirtualPose, a two-stage learning framework to exploit the hidden \"\"free lunch\"\" specific to this task, i.e. generating infinite number of poses and cameras for training models at no cost. To that end, the first stage transforms images to abstract geometry representations (AGR), and then the second maps them to 3D poses. It addresses the generalization issue from two aspects: (1) the first stage can be trained on diverse 2D datasets to reduce the risk of over-fitting to limited appearance; (2) the second stage can be trained on diverse AGR synthesized from a large number of virtual cameras and poses. It outperforms the SOTA methods without using any paired images and 3D poses from the benchmarks, which paves the way for practical applications. Code is available at https://github.com/wkom/VirtualPose.\""
  },
  "eccv2022_main_poseurdirecthumanposeregressionwithtransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Poseur: Direct Human Pose Regression with Transformers",
    "authors": [
      "Weian Mao",
      "Yongtao Ge",
      "Chunhua Shen",
      "Zhi Tian",
      "Xinlong Wang",
      "Zhibin Wang",
      "Anton van den Hengel"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4552_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660071.pdf",
    "published": "2020-08",
    "summary": "\"We propose a direct, regression-based approach to 2D human pose estimation from single images. We formulate the problem as a sequence prediction task, which we solve using a Transformer network. This network directly learns a regression mapping from images to the keypoint coordinates, without resorting to intermediate representations such as heatmaps. This approach avoids much of the complexity associated with heatmap-based approaches. To overcome the feature misalignment issues of previous regression-based methods, we propose an attention mechanism that adaptively attends to the features that are most relevant to the target keypoints, considerably improving the accuracy. Importantly, our framework is end-to-end differentiable, and naturally learns to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII, two predominant pose-estimation datasets, demonstrate that our method significantly improves upon the state-of-the-art in regression-based pose estimation. More notably, ours is the first regression-based approach to perform favorably compared to the best heatmap-based pose estimation methods. Code is available at: https://github.com/aim-uofa/Poseur\""
  },
  "eccv2022_main_simccasimplecoordinateclassificationperspectiveforhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SimCC: A Simple Coordinate Classification Perspective for Human Pose Estimation",
    "authors": [
      "Yanjie Li",
      "Sen Yang",
      "Peidong Liu",
      "Shoukui Zhang",
      "Yunxiao Wang",
      "Zhicheng Wang",
      "Wankou Yang",
      "Shu-Tao Xia"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4591_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660088.pdf",
    "published": "2020-08",
    "summary": "\"The 2D heatmap-based approaches have dominated Human Pose Estimation (HPE) for years due to high performance. However, the long-standing quantization error problem in the 2D heatmap-based methods leads to several well-known drawbacks: 1) The performance for the low-resolution inputs is limited; 2) To improve the feature map resolution for higher localization precision, multiple costly upsampling layers are required; 3) Extra post-processing is adopted to reduce the quantization error. To address these issues, we aim to explore a brand new scheme, called SimCC, which reformulates HPE as two classification tasks for horizontal and vertical coordinates. The proposed SimCC uniformly divides each pixel into several bins, thus achieving sub-pixel localization precision and low quantization error. Benefiting from that, SimCC can omit additional refinement post-processing and exclude upsampling layers under certain settings, resulting in a more simple and effective pipeline for HPE. Extensive experiments conducted over COCO, CrowdPose, and MPII datasets show that SimCC outperforms heatmap-based counterparts, especially in low-resolution settings by a large margin. Code is now publicly available at https://github.com/leeyegy/SimCC.\""
  },
  "eccv2022_main_regularizingvectorembeddinginbottom-uphumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Regularizing Vector Embedding in Bottom-Up Human Pose Estimation",
    "authors": [
      "Haixin Wang",
      "Lu Zhou",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4631_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660105.pdf",
    "published": "2020-08",
    "summary": "\"The embedding-based method such as Associative Embedding is popular in bottom-up human pose estimation. Methods under this framework group candidate keypoints according to the predicted identity embeddings. However, the identity embeddings of different instances are likely to be linearly inseparable in some complex scenes, such as crowded scene or when the number of instances in the image is large. To reduce the impact of this phenomenon on keypoint grouping, we try to learn a sparse multidimensional embedding for each keypoint. We observe that the different dimensions of embeddings are highly linearly correlated. To address this issue, we impose an additional constraint on the embeddings during training phase. Based on the fact that the scales of instances usually have significant variations, we uilize the scales of instances to regularize the embeddings, which effectively reduces the linear correlation of embeddings and makes embeddings being sparse. We evaluate our model on CrowdPose Test and COCO Test-dev. Compared to vanilla Associative Embedding, our method has an impressive superiority in keypoint grouping, especially in crowded scenes with a large number of instances. Furthermore, our method achieves state-of-the-art results on CrowdPose Test (74.5 AP) and COCO Test-dev (72.8 AP), outperforming other bottom-up methods. Our code and pretrained models are available at https://github.com/CR320/CoupledEmbedding.\""
  },
  "eccv2022_main_avisualnavigationperspectiveforcategory-levelobjectposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Visual Navigation Perspective for Category-Level Object Pose Estimation",
    "authors": [
      "Jiaxin Guo",
      "Fangxun Zhong",
      "Rong Xiong",
      "Yun-Hui Liu",
      "Yue Wang",
      "Yiyi Liao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4677_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660121.pdf",
    "published": "2020-08",
    "summary": "\"This paper studies category-level object pose estimation based on a single monocular image. Recent advances in pose-aware generative models have paved the way for addressing this challenging task using analysis-by-synthesis. The idea is to sequentially update a set of latent variables,e.g., pose, shape, and appearance, of the generative model until the generated image best agrees with the observation. However, convergence and efficiency are two challenges of this inference procedure. In this paper, we take a deeper look at the inference of analysis-by-synthesis from the perspective of visual navigation, and investigate what is a good navigation policy for this specific task. We evaluate three different strategies, including gradient descent, reinforcement learning and imitation learning, via thorough comparisons in terms of convergence, robustness and efficiency. Moreover, we show that a simple hybrid approach leads to an effective and efficient solution. We further compare these strategies to state-of-the-art methods, and demonstrate superior performance on synthetic and real-world datasets leveraging off-the-shelf pose-aware generative models.\""
  },
  "eccv2022_main_fastervoxelposereal-time3dhumanposeestimationbyorthographicprojection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Faster VoxelPose: Real-Time 3D Human Pose Estimation by Orthographic Projection",
    "authors": [
      "Hang Ye",
      "Wentao Zhu",
      "Chunyu Wang",
      "Rujie Wu",
      "Yizhou Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4748_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660139.pdf",
    "published": "2020-08",
    "summary": "\"While the voxel-based methods have achieved promising results for multi-person 3D pose estimation from multi-cameras, they suffer from heavy computation burdens, especially for large scenes. We present Faster VoxelPose to address the challenge by re-projecting the feature volume to the three two-dimensional coordinate planes and estimating X, Y, Z coordinates from them separately. To that end, we first localize each person by a 3D bounding box by estimating a 2D box and its height based on the volume features projected to the xy-plane and z-axis, respectively. Then for each person, we estimate partial joint coordinates from the three coordinate planes separately which are then fused to obtain the final 3D pose. The method is free from costly 3D-CNNs and improves the speed of VoxelPose by ten times and meanwhile achieves competitive accuracy as the state-of-the-art methods, proving its potential in real-time applications.\""
  },
  "eccv2022_main_learningtofitmorphablemodels": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning to Fit Morphable Models",
    "authors": [
      "Vasileios Choutas",
      "Federica Bogo",
      "Jingjing Shen",
      "Julien Valentin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4927_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660156.pdf",
    "published": "2020-08",
    "summary": "\"Fitting parametric models of human bodies, hands or faces to sparse input signals in an accurate, robust, and fast manner has the promise of significantly improving immersion in AR and VR scenarios. A common first step in systems that tackle these problems is to regress the parameters of the parametric model directly from the input data. This approach is fast, robust, and is a good starting point for an iterative minimization algorithm. The latter searches for the minimum of an energy function, typically composed of a data term and priors that encode our knowledge about the problem\u2019s structure. While this is undoubtedly a very successful recipe, priors are often hand defined heuristics and finding the right balance between the different terms to achieve high quality results is a non-trivial task. Furthermore, converting and optimizing these systems to run in a performant way requires custom implementations that demand significant time investments from both engineers and domain experts. In this work, we build upon recent advances in learned optimization and propose an update rule inspired by the classic Levenberg-Marquardt algorithm. We show the effectiveness of the proposed neural optimizer on three problems, 3D body estimation from a head-mounted device, 3D body estimation from sparse 2D keypoints and face surface estimation from dense 2D landmarks. Our method can easily be applied to new model fitting problems and offers a competitive alternative to well-tuned \u2018traditional\u2019 model fitting pipelines, both in terms of accuracy and speed.\""
  },
  "eccv2022_main_egobodyhumanbodyshapeandmotionofinteractingpeoplefromhead-mounteddevices": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices",
    "authors": [
      "Siwei Zhang",
      "Qianli Ma",
      "Yan Zhang",
      "Zhiyin Qian",
      "Taein Kwon",
      "Marc Pollefeys",
      "Federica Bogo",
      "Siyu Tang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4963_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660176.pdf",
    "published": "2020-08",
    "summary": "\"Understanding social interactions from egocentric views is crucial for many applications, ranging from assistive robotics to AR/VR. Key to reasoning about interactions is to understand the body pose and motion of the interaction partner from the egocentric view. However, research in this area is severely hindered by the lack of datasets. Existing datasets are limited in terms of either size, capture/annotation modalities, ground-truth quality, or interaction diversity. We fill this gap by proposing EgoBody, a novel large-scale dataset for human pose, shape and motion estimation from egocentric views, during interactions in complex 3D scenes. We employ Microsoft HoloLens2 headsets to record rich egocentric data streams (including RGB, depth, eye gaze, head and hand tracking). To obtain accurate 3D ground truth, we calibrate the headset with a multi-Kinect rig and fit expressive SMPL-X body meshes to multi-view RGB-D frames, reconstructing 3D human shapes and poses relative to the scene, over time. We collect 68 sequences, spanning diverse interaction scenarios, and propose the first benchmark for 3D full-body pose and shape estimation of the social partner from egocentric views. We extensively evaluate state-of-the-art methods, highlight their limitations in the egocentric scenario, and address such limitations leveraging our high-quality annotations. Data and code will be available for research purposes.\""
  },
  "eccv2022_main_graspddifferentiablecontact-richgraspsynthesisformulti-fingeredhands": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Grasp'D: Differentiable Contact-Rich Grasp Synthesis for Multi-Fingered Hands",
    "authors": [
      "Dylan Turpin",
      "Liquan Wang",
      "Eric Heiden",
      "Yun-Chun Chen",
      "Miles Macklin",
      "Stavros Tsogkas",
      "Sven Dickinson",
      "Animesh Garg"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5242_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660197.pdf",
    "published": "2020-08",
    "summary": "\"The study of hand-object interaction requires generating viable grasp poses for high-dimensional multi-finger models, often relying on analytic grasp synthesis which tends to produce brittle and unnatural results. This paper presents Grasp\u2019D, an approach to grasp synthesis by differentiable contact simulation that can work with both known models and visual inputs. We use gradient-based methods as an alternative to sampling-based grasp synthesis, which fails without simplifying assumptions, such as pre-specified contact locations and eigengrasps. Such assumptions limit grasp discovery and, in particular, exclude high-contact power grasps. In contrast, our simulation-based approach allows for stable, efficient, physically realistic, high-contact grasp synthesis, even for gripper morphologies with high-degrees of freedom. We identify and address challenges in making grasp simulation amenable to gradient-based optimization, such as non-smooth object surface geometry, contact sparsity, and a rugged optimization landscape. Grasp\u2019D compares favorably to analytic grasp synthesis on human and robotic hand models, and resultant grasps achieve over 4\u00d7 denser contact, leading to significantly higher grasp stability. Video and code available at: graspd-eccv22.github.io.\""
  },
  "eccv2022_main_autoavatarautoregressiveneuralfieldsfordynamicavatarmodeling": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling",
    "authors": [
      "Ziqian Bai",
      "Timur Bagautdinov",
      "Javier Romero",
      "Michael Zollh\u00f6fer",
      "Ping Tan",
      "Shunsuke Saito"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5388_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660216.pdf",
    "published": "2020-08",
    "summary": "\"Neural fields such as implicit surfaces have recently enabled avatar modeling from raw scans without explicit temporal correspondences. In this work, we exploit autoregressive modeling to further extend this notion to capture dynamic effects, such as soft-tissue deformations. Although autoregressive models are naturally capable of handling dynamics, it is non-trivial to apply them to implicit representations, as explicit state decoding is infeasible due to prohibitive memory requirements. In this work, for the first time, we enable autoregressive modeling of implicit avatars. To reduce the memory bottleneck and efficiently model dynamic implicit surfaces, we introduce the notion of articulated observer points, which relate implicit states to the explicit surface of a parametric human body model. We demonstrate that encoding implicit surfaces as a set of height fields defined on articulated observer points leads to significantly better generalization compared to a latent representation. The experiments show that our approach outperforms the state of the art, achieving plausible dynamic deformations even for unseen motions. https://zqbai-jeremy.github.io/autoavatar\""
  },
  "eccv2022_main_deepradialembeddingforvisualsequencelearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Deep Radial Embedding for Visual Sequence Learning",
    "authors": [
      "Yuecong Min",
      "Peiqi Jiao",
      "Yanan Li",
      "Xiaotao Wang",
      "Lei Lei",
      "Xiujuan Chai",
      "Xilin Chen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5670_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660234.pdf",
    "published": "2020-08",
    "summary": "\"Connectionist Temporal Classification (CTC) is a popular objective function in sequence recognition, which provides supervision for unsegmented sequence data through aligning sequence and its corresponding labeling iteratively. The blank class of CTC plays a crucial role in the alignment process and is often considered responsible for the peaky behavior of CTC. In this study, we propose an objective function named RadialCTC that constrains sequence features on a hypersphere while retaining the iterative alignment mechanism of CTC. The learned features of each non-blank class are distributed on a radial arc from the center of the blank class, which provides a clear geometric interpretation and makes the alignment process more efficient. Besides, RadialCTC can control the peaky behavior by simply modifying the logit of the blank class. Experimental results of recognition and localization demonstrate the effectiveness of RadialCTC on two sequence recognition applications.\""
  },
  "eccv2022_main_sagastochasticwhole-bodygraspingwithcontact": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SAGA: Stochastic Whole-Body Grasping with Contact",
    "authors": [
      "Yan Wu",
      "Jiahao Wang",
      "Yan Zhang",
      "Siwei Zhang",
      "Otmar Hilliges",
      "Fisher Yu",
      "Siyu Tang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5754_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660251.pdf",
    "published": "2020-08",
    "summary": "\"The synthesis of human grasping has numerous applications including AR/VR, video games and robotics. While methods have been proposed to generate realistic hand-object interaction for object grasping and manipulation, these typically only consider interacting hand alone. Our goal is to synthesize whole-body grasping motions. Starting from an arbitrary initial pose, we aim to generate diverse and natural whole-body human motions to approach and grasp a target object in 3D space. This task is challenging as it requires modeling both whole-body dynamics and dexterous finger movements. To this end, we propose SAGA (StochAstic whole-body Grasping with contAct), a framework which consists of two key components: (a) Static whole-body grasping pose generation. Specifically, we propose a multi-task generative model, to jointly learn static whole-body grasping poses and human-object contacts. (b) Grasping motion infilling. Given an initial pose and the generated whole-body grasping pose as the start and end of the motion respectively, we design a novel contact-aware generative motion infilling module to generate a diverse set of grasp-oriented motions. We demonstrate the effectiveness of our method, which is a novel generative framework to synthesize realistic and expressive whole-body motions that approach and grasp randomly placed unseen objects. Code and models are available at https://jiahaoplus.github.io/SAGA/saga.html.\""
  },
  "eccv2022_main_neuralcaptureofanimatable3dhumanfrommonocularvideo": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Neural Capture of Animatable 3D Human from Monocular Video",
    "authors": [
      "Gusi Te",
      "Xiu Li",
      "Xiao Li",
      "Jinglu Wang",
      "Wei Hu",
      "Yan Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5908_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660269.pdf",
    "published": "2020-08",
    "summary": "\"We present a novel paradigm of building an animatable 3D human representation from a monocular video input, such that it can be rendered in any unseen poses and views. Our method is based on a dynamic Neural Radiance Field (NeRF) rigged by a mesh-based parametric 3D human model serving as a geometry proxy. Previous methods usually rely on multi-view videos or accurate 3D geometry information as additional inputs; besides, most methods suffer from degraded quality when generalized to unseen poses. We identify that the key to generalization is a good input embedding for querying dynamic NeRF: A good input embedding should define an injective mapping in the full volumetric space, guided by surface mesh deformation under pose variation. Based on this observation, we propose to embed the input query with its relationship to local surface regions spanned by a set of geodesic nearest neighbors on mesh vertices. By including both position and relative distance information, our embedding defines a distance-preserved deformation mapping and generalizes well to unseen poses. To reduce the dependency on additional inputs, we first initialize per-frame 3D meshes using off-the-shelf tools and then propose a pipeline to jointly optimize NeRF and refine the initial mesh. Extensive experiments show our method can synthesize plausible human rendering results under unseen poses and views.\""
  },
  "eccv2022_main_generalobjectposetransformationnetworkfromunpaireddata": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "General Object Pose Transformation Network from Unpaired Data",
    "authors": [
      "Yukun Su",
      "Guosheng Lin",
      "Ruizhou Sun",
      "Qingyao Wu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5972_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660286.pdf",
    "published": "2020-08",
    "summary": "\"Object pose transformation is a challenging task. Yet, most existing pose transformation networks only focus on synthesizing humans. These methods either rely on the keypoints information or rely on the manual annotations of the paired target pose images for training. However, collecting such paired data is laboring and the cue of keypoints is inapplicable to general objects. In this paper, we address a problem of novel general object pose transformation from unpaired data. Given a source image of an object that provides appearance information and the desired pose image as a reference in the absence of paired examples, we produce a depiction of that object in that pose, retaining the appearance of both the object and background. Specifically, to preserve the source information, we propose an adversarial network with $\\textbf{S}$patial-$\\textbf{S}$tructural (SS) block and $\\textbf{T}$exture-$\\textbf{S}$tyle-$\\textbf{C}$olor (TSC) block after the correlation matching module that facilitates the output to be semantically corresponding to the target pose image while contextually related to the source image. In addition, we can extend our network to complete multi-object and cross-category pose transformation. Extensive experiments demonstrate the effectiveness of our method which can create more realistic images when compared to those of recent approaches in terms of image quality. Moreover, we show the practicality of our method for several applications.\""
  },
  "eccv2022_main_compositionalhuman-sceneinteractionsynthesiswithsemanticcontrol": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Compositional Human-Scene Interaction Synthesis with Semantic Control",
    "authors": [
      "Kaifeng Zhao",
      "Shaofei Wang",
      "Yan Zhang",
      "Thabo Beeler",
      "Siyu Tang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6216_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660305.pdf",
    "published": "2020-08",
    "summary": "\"Synthesizing natural interactions between virtual humans and their 3D environments is critical for numerous applications, such as computer games and AR/VR experiences. Recent methods mainly focus on modeling geometric relations between 3D environments and humans, where the high-level semantics of the human-scene interaction has frequently been ignored. Our goal is to synthesize humans interacting with a given 3D scene controlled by high-level semantic specifications as pairs of action categories and object instances, e.g., \u201csit on the chair\u201d. The key challenge of incorporating interaction semantics into the generation framework is to learn a joint representation that effectively captures heterogeneous information, including human body articulation, 3D object geometry, and the intent of the interaction. To address this challenge, we design a novel transformer-based generative model, in which the articulated 3D human body surface points and 3D objects are jointly encoded in a unified latent space, and the semantics of the interaction between the human and objects are embedded via positional encoding. Furthermore, inspired by the compositional nature of interactions that humans can simultaneously interact with multiple objects, we define interaction semantics as the composition of varying numbers of atomic action-object pairs. Our proposed generative model can naturally incorporate varying numbers of atomic interactions, which enables synthesizing compositional human-scene interactions without requiring composite interaction data. We extend the PROX dataset with interaction semantic labels and scene instance segmentation to evaluate our method and demonstrate that our method can generate realistic human-scene interactions with semantic control. Our perceptual study shows that our synthesized virtual humans can naturally interact with 3D scenes, considerably outperforming existing methods. We name our method COINS, for COmpositional INteraction Synthesis with Semantic Control. Code and data are available at https://github.com/zkf1997/COINS.\""
  },
  "eccv2022_main_pressurevisionestimatinghandpressurefromasinglergbimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PressureVision: Estimating Hand Pressure from a Single RGB Image",
    "authors": [
      "Patrick Grady",
      "Chengcheng Tang",
      "Samarth Brahmbhatt",
      "Christopher D. Twigg",
      "Chengde Wan",
      "James Hays",
      "Charles C. Kemp"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6515_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660322.pdf",
    "published": "2020-08",
    "summary": "\"People often interact with their surroundings by applying pressure with their hands. While hand pressure can be measured by placing pressure sensors between the hand and the environment, doing so can alter contact mechanics, interfere with human tactile perception, require costly sensors, and scale poorly to large environments. We explore the possibility of using a conventional RGB camera to infer hand pressure, enabling machine perception of hand pressure from uninstrumented hands and surfaces. The central insight is that the application of pressure by a hand results in informative appearance changes. Hands share biomechanical properties that result in similar observable phenomena, such as soft-tissue deformation, blood distribution, hand pose, and cast shadows. We collected videos of 36 participants with diverse skin tone applying pressure to an instrumented planar surface. We then trained a deep model (PressureVisionNet) to infer a pressure image from a single RGB image. Our model infers pressure for participants outside of the training data and outperforms baselines. We also show that the output of our model depends on the appearance of the hand and cast shadows near contact regions. Overall, our results suggest the appearance of a previously unobserved human hand can be used to accurately infer applied pressure. Data, code, and models are available online.\""
  },
  "eccv2022_main_posescript3dhumanposesfromnaturallanguage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PoseScript: 3D Human Poses from Natural Language",
    "authors": [
      "Ginger Delmas",
      "Philippe Weinzaepfel",
      "Thomas Lucas",
      "Francesc Moreno-Noguer",
      "Gr\u00e9gory Rogez"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6526_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660340.pdf",
    "published": "2020-08",
    "summary": "\"Natural language is leveraged in many computer vision tasks such as image captioning, cross-modal retrieval or visual question answering, to provide fine-grained semantic information. While human pose is key to human understanding, current 3D human pose datasets lack detailed language descriptions. In this work, we introduce the PoseScript dataset, which pairs a few thousand 3D human poses from AMASS with rich human-annotated descriptions of the body parts and their spatial relationships. To increase the size of this dataset to a scale compatible with typical data hungry learning algorithms, we propose an elaborate captioning process that generates automatic synthetic descriptions in natural language from given 3D keypoints. This process extracts low-level pose information -- the posecodes -- using a set of simple but generic rules on the 3D keypoints. The posecodes are then combined into higher level textual descriptions using syntactic rules. Automatic annotations substantially increase the amount of available data, and make it possible to effectively pretrain deep models for finetuning on human captions. To demonstrate the potential of annotated poses, we show applications of the PoseScript dataset to retrieval of relevant poses from large-scale datasets and to synthetic pose generation, both based on a textual pose description. Code and dataset are available at https://europe.naverlabs.com/research/computer-vision/posescript/.\""
  },
  "eccv2022_main_dprostdynamicprojectivespatialtransformernetworkfor6dposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation",
    "authors": [
      "Jaewoo Park",
      "Nam Ik Cho"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6565_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660357.pdf",
    "published": "2020-08",
    "summary": "\"Predicting the object\u2019s 6D pose from a single RGB image is a fundamental computer vision task. Generally, the distance between transformed object vertices is employed as an objective function for pose estimation methods. However, projective geometry in the camera space is not considered in those methods and causes performance degradation. In this regard, we propose a new pose estimation system based on a projective grid instead of object vertices. Our pose estimation method, dynamic projective spatial transformer network (DProST), localizes the region of interest grid on the rays in camera space and transforms the grid to object space by estimated pose. The transformed grid is used as both a sampling grid and a new criterion of the estimated pose. Additionally, because DProST does not require object vertices, our method can be used in a mesh-less setting by replacing the mesh with a reconstructed feature. Experimental results show that mesh-less DProST outperforms the state-of-the-art mesh-based methods on the LINEMOD and LINEMOD-OCCLUSION dataset, and shows competitive performance on the YCBV dataset with mesh data. The source code is available at https://github.com/parkjaewoo0611/DProST.\""
  },
  "eccv2022_main_3dinteractinghandposeestimationbyhandde-occlusionandremoval": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Interacting Hand Pose Estimation by Hand De-Occlusion and Removal",
    "authors": [
      "Hao Meng",
      "Sheng Jin",
      "Wentao Liu",
      "Chen Qian",
      "Mengxiang Lin",
      "Wanli Ouyang",
      "Ping Luo"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6671_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660374.pdf",
    "published": "2020-08",
    "summary": "\"Estimating 3D interacting hand pose from a single RGB image is essential for understanding human actions. Unlike most previous works that directly predict the 3D poses of two interacting hands simultaneously, we propose to decompose the challenging interacting hand pose estimation task and estimate the pose of each hand separately. In this way, it is straightforward to take advantage of the latest research progress on the single-hand pose estimation system. However, hand pose estimation in interacting scenarios is very challenging, due to (1) severe hand-hand occlusion and (2) ambiguity caused by the homogeneous appearance of hands. To tackle these two challenges, we propose a novel Hand De-occlusion and Removal (HDR) framework to perform hand de-occlusion and distractor removal. We also propose the first large-scale synthetic amodal hand dataset, termed Amodal InterHand Dataset (AIH), to facilitate model training and promote the development of the related research. Experiments show that the proposed method significantly outperforms previous state-of-the-art interacting hand pose estimation approaches. Codes and data are available at https://github.com/MengHao666/HDR.\""
  },
  "eccv2022_main_poseforeverythingtowardscategory-agnosticposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Pose for Everything: Towards Category-Agnostic Pose Estimation",
    "authors": [
      "Lumin Xu",
      "Sheng Jin",
      "Wang Zeng",
      "Wentao Liu",
      "Chen Qian",
      "Wanli Ouyang",
      "Ping Luo",
      "Xiaogang Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6672_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660391.pdf",
    "published": "2020-08",
    "summary": "\"Existing works on 2D pose estimation mainly focus on a certain category, e.g. human, animal, and vehicle. However, there are lots of application scenarios that require detecting the poses/keypoints of the unseen class of objects. In this paper, we introduce the task of Category-Agnostic Pose Estimation (CAPE), which aims to create a pose estimation model capable of detecting the pose of any class of object given only a few samples with keypoint definition. To achieve this goal, we formulate the pose estimation problem as a keypoint matching problem and design a novel CAPE framework, termed POse Matching Network (POMNet). A transformer-based Keypoint Interaction Module (KIM) is proposed to capture both the interactions among different keypoints and the relationship between the support and query images. We also introduce Multi-category Pose (MP-100) dataset, which is a 2D pose dataset of 100 object categories containing over 20K instances and is well-designed for developing CAPE algorithms. Experiments show that our method outperforms other baseline approaches by a large margin. Codes and data are available at https://github.com/luminxu/Pose-for-Everything.\""
  },
  "eccv2022_main_posegptquantization-based3dhumanmotiongenerationandforecasting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PoseGPT: Quantization-Based 3D Human Motion Generation and Forecasting",
    "authors": [
      "Thomas Lucas",
      "Fabien Baradel",
      "Philippe Weinzaepfel",
      "Gr\u00e9gory Rogez"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6693_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660409.pdf",
    "published": "2020-08",
    "summary": "\"We address the problem of action-conditioned generation of human motion sequences. Existing work falls into two categories: forecast models conditioned on observed past motions, or generative models conditioned action labels and duration only. In contrast, we generate motion conditioned on observations of arbitrary length, including none. To solve this generalized problem, we propose PoseGPT, an auto-regressive transformer-based approach which internally compresses human motion into quantized latent sequences. An auto-encoder first maps human motion to latent index sequences in a discrete space, and vice-versa. Inspired by the Generative Pretrained Transformer (GPT), we propose to train a GPT-like model for next-index prediction in that space; this allows PoseGPT to output distributions on possible futures, with or without conditioning on past motion. The discrete and compressed nature of the latent space allows the GPT- like model to focus on long-range signal, as it removes low-level redundancy in the input signal. Predicting discrete indices also alleviates the common pitfall of predicting averaged poses, a typical failure case when regressing continuous values, as the average of discrete targets is not a target itself. Our experimental results show that our proposed approach achieves state-of-the-art results on Hu- manAct12 - a standard but small scale dataset, on BABEL - a recent large scale MoCap dataset and on GRAB - a human-object interactions dataset.\""
  },
  "eccv2022_main_dh-augdhforwardkinematicsmodeldrivenaugmentationfor3dhumanposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DH-AUG: DH Forward Kinematics Model Driven Augmentation for 3D Human Pose Estimation",
    "authors": [
      "Linzhi Huang",
      "Jiahao Liang",
      "Weihong Deng"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7764_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660427.pdf",
    "published": "2020-08",
    "summary": "\"Due to the lack of diversity of datasets, the generalization ability of the pose estimator is poor. To solve this problem, we propose a pose augmentation solution via DH forward kinematics model, which we call DH-AUG. We observe that the previous work is all based on single-frame pose augmentation, if it is directly applied to video pose estimator, there will be several previously ignored problems: (i) angle ambiguity in bone rotation (multiple solutions); (ii) the generated skeleton video lacks movement continuity. To solve these problems, we propose a special generator based on DH forward kinematics model, which is called DH-generator. Extensive experiments demonstrate that DH-AUG can greatly increase the generalization ability of the video pose estimator. In addition, when applied to a single-frame 3D pose estimator, our method outperforms the previous best pose augmentation method. The source code has been released at https://github.com/hlz0606/DH-AUG-DH-Forward-Kinematics-Model-Driven-Augmentation-for-3D-Human-Pose-Estimation.\""
  },
  "eccv2022_main_estimatingspatially-varyinglightinginurbansceneswithdisentangledrepresentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Estimating Spatially-Varying Lighting in Urban Scenes with Disentangled Representation",
    "authors": [
      "Jiajun Tang",
      "Yongjie Zhu",
      "Haoyu Wang",
      "Jun Hoong Chan",
      "Si Li",
      "Boxin Shi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/162_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660445.pdf",
    "published": "2020-08",
    "summary": "\"We present an end-to-end network for spatially-varying outdoor lighting estimation in urban scenes given a single limited field-of-view LDR image and any assigned 2D pixel position. We use three disentangled latent spaces learned by our network to represent sky light, sun light, and lighting-independent local contents respectively. At inference time, our lighting estimation network can run efficiently in an end-to-end manner by merging the global lighting and the local appearance rendered by the local appearance renderer with the predicted local silhouette. We enhance an existing synthetic dataset with more realistic material models and diverse lighting conditions for more effective training. We also capture the first real dataset with HDR labels for evaluating spatially-varying outdoor lighting estimation. Experiments on both synthetic and real datasets show that our method achieves state-of-the-art performance with more flexible editability.\""
  },
  "eccv2022_main_boostingeventstreamsuper-resolutionwitharecurrentneuralnetwork": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Boosting Event Stream Super-Resolution with a Recurrent Neural Network",
    "authors": [
      "Wenming Weng",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/248_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660461.pdf",
    "published": "2020-08",
    "summary": "\"Existing methods for event stream super-resolution (SR) either require high-quality and high-resolution frames or underperform for large factor SR. To address these problems, we propose a recurrent neural network for event SR without frames. First, we design a temporal propagation net for incorporating neighboring and long-range event-aware contexts that facilitates event SR. Second, we build a spatiotemporal fusion net for reliably aggregating the spatiotemporal clues of event stream. These two elaborate components are tightly synergized for achieving satisfying event SR results even for 16X SR. Synthetic and real-world experimental results demonstrate the clear superiority of our method. Furthermore, we evaluate our method on two downstream event-driven applications, i.e., object recognition and video reconstruction, achieving remarkable performance boost over existing methods.\""
  },
  "eccv2022_main_projectiveparallelsingle-pixelimagingtoovercomeglobalilluminationin3dstructurelightscanning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Projective Parallel Single-Pixel Imaging to Overcome Global Illumination in 3D Structure Light Scanning",
    "authors": [
      "Yuxi Li",
      "Huijie Zhao",
      "Hongzhi Jiang",
      "Xudong Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/702_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660479.pdf",
    "published": "2020-08",
    "summary": "\"We consider robust and efficient 3D structure light scanning method in situations dominated by global illumination. One typical way of solving this problem is via the analysis of 4D light transport coefficients (LTCs), which contains complete information for a projector-camera pair, and is a 4D data set. However, the process of capturing LTCs generally takes long time. We present projective parallel single-pixel imaging (pPSI), wherein the 4D LTCs are reduced to multiple projection functions to facilitate a highly efficient data capture process. We introduce local maximum constraint, which provides necessary condition for the location of correspondence matching points when projection functions are captured. Local slice extension method is introduced to further accelerate the capture of projection functions. We study the influence of scan ratio in local slice extension method on the accuracy of the correspondence matching points, and conclude that partial scanning is enough for satisfactory results. Our discussions and experiments include three typical kinds of global illuminations: inter-reflections, subsurface scattering, and step edge fringe aliasing. The proposed method is validated in several challenging scenarios.\""
  },
  "eccv2022_main_semantic-sparsecolorizationnetworkfordeepexemplar-basedcolorization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Semantic-Sparse Colorization Network for Deep Exemplar-Based Colorization",
    "authors": [
      "Yunpeng Bai",
      "Chao Dong",
      "Zenghao Chai",
      "Andong Wang",
      "Zhengzhuo Xu",
      "Chun Yuan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/820_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660495.pdf",
    "published": "2020-08",
    "summary": "\"Exemplar-based colorization approaches rely on reference image to provide plausible colors for target gray-scale image. The key and difficulty of exemplar-based colorization is to establish an accurate correspondence between these two images. Previous approaches have attempted to construct such a correspondence but are faced with two obstacles. First, using luminance channels for the calculation of correspondence is inaccurate. Second, the dense correspondence they built introduces wrong matching results and increases the computation burden. To address these two problems, we propose Semantic-Sparse Colorization Network (SSCN) to transfer both the global image style and detailed semantic-related colors to the gray-scale image in a coarse-to-fine manner. Our network can perfectly balance the global and local colors while alleviating the ambiguous matching problem. Experiments show that our method outperforms existing methods in both quantitative and qualitative evaluation and achieves state-of-the-art performance.\""
  },
  "eccv2022_main_practicalandscalabledesktop-basedhigh-qualityfacialcapture": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Practical and Scalable Desktop-Based High-Quality Facial Capture",
    "authors": [
      "Alexandros Lattas",
      "Yiming Lin",
      "Jayanth Kannan",
      "Ekin Ozturk",
      "Luca Filipi",
      "Giuseppe Claudio Guarnera",
      "Gaurav Chawla",
      "Abhijeet Ghosh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1083_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660512.pdf",
    "published": "2020-08",
    "summary": "\"We present a novel desktop-based system for high-quality facial capture including geometry and facial appearance. The proposed acquisition system is highly practical and scalable, consisting purely of commodity components. The setup consists of a set of displays for controlled illumination for reflectance capture, in conjunction with multiview acquisition of facial geometry. We additionally present a novel set of binary illumination patterns for efficient acquisition of reflectance and photometric normals using our setup, with diffuse-specular separation. We demonstrate high-quality results with two different variants of the capture setup - one entirely consisting of portable mobile devices targeting static facial capture, and the other consisting of desktop LCD displays targeting both static and dynamic facial capture.\""
  },
  "eccv2022_main_fast-vqaefficientend-to-endvideoqualityassessmentwithfragmentsampling": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "FAST-VQA: Efficient End-to-End Video Quality Assessment with Fragment Sampling",
    "authors": [
      "Haoning Wu",
      "Chaofeng Chen",
      "Jingwen Hou",
      "Liang Liao",
      "Annan Wang",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1225_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660528.pdf",
    "published": "2020-08",
    "summary": "\"Current deep video quality assessment (VQA) methods are usually with high computational costs when evaluating high-resolution videos. This cost hinders them from learning better video-quality-related representations via end-to-end training. Existing approaches typically consider naive sampling to reduce the computational cost, such as resizing and cropping. However, they obviously corrupt quality-related information in videos and are thus not optimal to learn good representations for VQA. Therefore, there is an eager need to design a new quality-retained sampling scheme for VQA. In this paper, we propose Grid Mini-patch Sampling (GMS), which allows consideration of local quality by sampling patches at their raw resolution and covers global quality with contextual relations via mini-patches sampled in uniform grids. These mini-patches are spliced and aligned temporally, named as fragments. We further build the Fragment Attention Network (FANet) specially designed to accommodate fragments as inputs. Consisting of fragments and FANet, the proposed FrAgment Sample Transformer for VQA (FAST-VQA) enables efficient end-to-end deep VQA and learns effective video-quality-related representations. It improves state-of-the-art accuracy by around 10% while reducing 99.5% FLOPs on 1080P high-resolution videos. The newly learned video-quality-related representations can also be transferred into smaller VQA datasets, boosting the performance on these scenarios. Extensive experiments show that FAST-VQA has good performance on inputs of various resolutions while retaining high efficiency. We publish our code at https://github.com/timothyhtimothy/FAST-VQA.\""
  },
  "eccv2022_main_physically-basededitingofindoorscenelightingfromasingleimage": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Physically-Based Editing of Indoor Scene Lighting from a Single Image",
    "authors": [
      "Zhengqin Li",
      "Jia Shi",
      "Sai Bi",
      "Rui Zhu",
      "Kalyan Sunkavalli",
      "Milo\u0161 Ha\u0161an",
      "Zexiang Xu",
      "Ravi Ramamoorthi",
      "Manmohan Chandraker"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1276_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660545.pdf",
    "published": "2020-08",
    "summary": "\"We present a method to edit complex indoor lighting from a single image with its predicted depth and light source segmentation masks. This is an extremely challenging problem that requires modeling complex light transport, and disentangling HDR lighting from material and geometry with only a partial LDR observation of the scene. We tackle this problem using two novel components: 1) a holistic scene reconstruction method that estimates scene reflectance and parametric 3D lighting, and 2) a neural rendering framework that re-renders the scene from our predictions. We use physically-based indoor light representations that allow for intuitive editing, and infer both visible and invisible light sources. Our neural rendering framework combines physically-based direct illumination and shadow rendering with deep networks to approximate global illumination. It can capture challenging lighting effects, such as soft shadows, directional lighting, specular materials, and interreflections. Previous single image inverse rendering methods usually entangle scene lighting and geometry and only support applications like object insertion. Instead, by combining parametric 3D lighting estimation with neural scene rendering, we demonstrate the first automatic method to achieve full scene relighting, including light source insertion, removal, and replacement, from a single image. All source code and data will be publicly released.\""
  },
  "eccv2022_main_lednetjointlow-lightenhancementanddeblurringinthedark": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "LEDNet: Joint Low-Light Enhancement and Deblurring in the Dark",
    "authors": [
      "Shangchen Zhou",
      "Chongyi Li",
      "Chen Change Loy"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1331_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660562.pdf",
    "published": "2020-08",
    "summary": "\"Night photography typically suffers from both low light and blurring issues due to the dim environment and the common use of long exposure. While existing light enhancement and deblurring methods could deal with each problem individually, a cascade of such methods cannot work harmoniously to cope well with joint degradation of visibility and sharpness. Training an end-to-end network is also infeasible as no paired data is available to characterize the coexistence of low light and blurs. We address the problem by introducing a novel data synthesis pipeline that models realistic low-light blurring degradations, especially for blurs in saturated regions, e.g., light streaks, that often appear in the night images. With the pipeline, we present the first large-scale dataset for joint low-light enhancement and deblurring. The dataset, LOL-Blur, contains 12,000 low-blur/normal-sharp pairs with diverse darkness and blurs in different scenarios. We further present an effective network, named LEDNet, to perform joint low-light enhancement and deblurring. Our network is unique as it is specially designed to consider the synergy between the two inter-connected tasks. Both the proposed dataset and network provide a foundation for this challenging joint task. Extensive experiments demonstrate the effectiveness of our method on both synthetic and real-world datasets.\""
  },
  "eccv2022_main_mpibanmpi-basedbokehrenderingframeworkforrealisticpartialocclusioneffects": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MPIB: An MPI-Based Bokeh Rendering Framework for Realistic Partial Occlusion Effects",
    "authors": [
      "Juewen Peng",
      "Jianming Zhang",
      "Xianrui Luo",
      "Hao Lu",
      "Ke Xian",
      "Zhiguo Cao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1759_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660579.pdf",
    "published": "2020-08",
    "summary": "\"Partial occlusion effects are a phenomenon that blurry objects near a camera are semi-transparent, resulting in partial appearance of occluded background. However, it is challenging for existing bokeh rendering methods to simulate realistic partial occlusion effects due to the missing information of the occluded area in an all-in-focus image. Inspired by the learnable 3D scene representation, Multiplane Image (MPI), we attempt to address the partial occlusion by introducing a novel MPI-based high-resolution bokeh rendering framework, termed MPIB. To this end, we first present an analysis on how to apply the MPI representation to bokeh rendering. Based on this analysis, we propose an MPI representation module combined with a background inpainting module to implement high-resolution scene representation. This representation can then be reused to render various bokeh effects according to the controlling parameters. To train and test our model, we also design a ray-tracing-based bokeh generator for data generation. Extensive experiments on synthesized and real-world images validate the effectiveness and flexibility of this framework.\""
  },
  "eccv2022_main_real-rawvsrreal-worldrawvideosuper-resolutionwithabenchmarkdataset": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset",
    "authors": [
      "Huanjing Yue",
      "Zhiming Zhang",
      "Jingyu Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1804_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660597.pdf",
    "published": "2020-08",
    "summary": "\"In recent years, real image super-resolution (SR) has achieved promising results due to the development of SR datasets and corresponding real SR methods. In contrast, the field of real video SR is lagging behind, especially for real raw videos. Considering the superiority of raw image SR over sRGB image SR, we construct a real-world raw video SR (Real-RawVSR) dataset and propose a corresponding SR method. We utilize two DSLR cameras and a beam-splitter to simultaneously capture low-resolution (LR) and high-resolution (HR) raw videos with 2x, 3x, and 4x magnifications. There are 450 video pairs in our dataset, with scenes varying from indoor to outdoor, and motions including camera and object movements. To our knowledge, this is the first real-world raw VSR dataset. Since the raw video is characterized by the Bayer pattern, we propose a two-branch network, which deals with both the packed RGGB sequence and the original Bayer pattern sequence, and the two branches are complementary to each other. After going through the proposed co-alignment, interaction, fusion, and reconstruction modules, we generate the corresponding HR sRGB sequence. Experimental results demonstrate that the proposed method outperforms benchmark real and synthetic video SR methods with either raw or sRGB inputs. Our code and dataset are available at https://github.com/zmzhang1998/Real-RawVSR.\""
  },
  "eccv2022_main_transformyoursmartphoneintoadslrcameralearningtheispinthewild": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Transform Your Smartphone into a DSLR Camera: Learning the ISP in the Wild",
    "authors": [
      "Ardhendu Shekhar Tripathi",
      "Martin Danelljan",
      "Samarth Shukla",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2134_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660614.pdf",
    "published": "2020-08",
    "summary": "\"We propose a trainable Image Signal Processing (ISP) framework that produces DSLR quality images given RAW images captured by a smartphone. To address the color misalignments between training image pairs, we employ a color-conditional ISP network and optimize a novel parametric color mapping between each input RAW and reference DSLR image. During inference, we predict the target color image by designing a color prediction network with efficient Global Context Transformer modules. The latter effectively leverage global information to learn consistent color and tone mappings. We further propose a robust masked aligned loss to identify and discard regions with inaccurate motion estimation during training. Lastly, we introduce the ISP in the Wild (ISPW) dataset, consisting of weakly paired phone RAW and DSLR sRGB images. We extensively evaluate our method, setting a new state-of-the-art on two datasets. The code is available at https://github.com/4rdhendu/TransformPhone2DSLR.\""
  },
  "eccv2022_main_learningdeepnon-blindimagedeconvolutionwithoutgroundtruths": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Deep Non-Blind Image Deconvolution without Ground Truths",
    "authors": [
      "Yuhui Quan",
      "Zhuojie Chen",
      "Huan Zheng",
      "Hui Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2222_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660631.pdf",
    "published": "2020-08",
    "summary": "\"Non-blind image deconvolution (NBID) is about restoring a latent sharp image from a blurred one, given an associated blur kernel. Most existing deep neural networks for NBID are trained over many ground truth (GT) images, which limits their applicability in practical applications such as microscopic imaging and medical imaging. This paper proposes an unsupervised deep learning approach for NBID which avoids accessing GT images. The challenge raised from the absence of GT images is tackled by a self-supervised reconstruction loss that approximates its supervised counterpart well. The possible errors of blur kernels are addressed by a self-supervised prediction loss based on intermediate samples as well as an ensemble inference scheme based on kernel perturbation. The experiments show that the proposed approach provides very competitive performance to existing supervised learning-based methods, no matter under accurate kernels or erroneous kernels.\""
  },
  "eccv2022_main_nestneuraleventstackforevent-basedimageenhancement": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "NEST: Neural Event Stack for Event-Based Image Enhancement",
    "authors": [
      "Minggui Teng",
      "Chu Zhou",
      "Hanyue Lou",
      "Boxin Shi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2730_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660649.pdf",
    "published": "2020-08",
    "summary": "\"Event cameras demonstrate unique characteristics such as high temporal resolution, low latency, and high dynamic range to improve performance for various image enhancement tasks. However, event streams cannot be applied to neural networks directly due to their sparse nature. To integrate events into traditional computer vision algorithms, an appropriate event representation is desirable, while existing voxel grid and event stack representations are less effective in encoding motion and temporal information. This paper presents a novel event representation named Neural Event STack (NEST), which satisfies physical constraints and encodes comprehensive motion and temporal information sufficient for image enhancement. We apply our representation on multiple tasks, which achieves superior performance on image deblurring and image super-resolution than state-of-the-art methods on both synthetic and real datasets. And we further demonstrate the possibility to generate high frame rate videos with our novel event representation.\""
  },
  "eccv2022_main_editableindoorlightingestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Editable Indoor Lighting Estimation",
    "authors": [
      "Henrique Weber",
      "Mathieu Garon",
      "Jean-Fran\u00e7ois Lalonde"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2772_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660666.pdf",
    "published": "2020-08",
    "summary": "\"We present a method for estimating lighting from a single perspective image of an indoor scene. Previous methods for predicting indoor illumination usually focus on either simple, parametric lighting that lack realism, or on richer representations that are difficult or even impossible to understand or modify after prediction. We propose a pipeline that estimates a parametric light that is easy to edit and allows renderings with strong shadows, alongside with a non-parametric texture with high-frequency information necessary for realistic rendering of specular objects. Once estimated, the predictions obtained with our model are interpretable and can easily be modified by an artist/user with a few mouse clicks. Quantitative and qualitative results show that our approach makes indoor lighting estimation easier to handle by a casual user, while still producing competitive results.\""
  },
  "eccv2022_main_fasttwo-stepblindopticalaberrationcorrection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Fast Two-Step Blind Optical Aberration Correction",
    "authors": [
      "Thomas Eboli",
      "Jean-Michel Morel",
      "Gabriele Facciolo"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2877_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660682.pdf",
    "published": "2020-08",
    "summary": "\"The optics of any camera degrades the sharpness of photographs, which is a key visual quality criterion. This degradation is characterized by the point-spread function (PSF), which depends on the wavelengths of light and is variable across the imaging field. In this paper, we propose a two-step scheme to correct optical aberrations in a single raw or JPEG image, i.e., without any prior information on the camera or lens. First, we estimate local Gaussian blur kernels for overlapping patches and sharpen them with a non-blind deblurring technique. Based on the measurements of the PSFs of dozens of lenses, these blur kernels are modeled as RGB Gaussians defined by seven parameters. Second, we remove the remaining lateral chromatic aberrations (not contemplated in the first step) with a convolutional neural network, trained to minimize the red/green and blue/green residual images. Experiments on both synthetic and real images show that the combination of these two stages yields a fast state-of-the-art blind optical aberration compensation technique that competes with commercial non-blind algorithms.\""
  },
  "eccv2022_main_seeingfarinthedarkwithpatternedflash": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Seeing Far in the Dark with Patterned Flash",
    "authors": [
      "Zhanghao Sun",
      "Jian Wang",
      "Yicheng Wu",
      "Shree Nayar"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2891_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660698.pdf",
    "published": "2020-08",
    "summary": "\"Flash illumination is widely used in imaging under low-light environments. However, illumination intensity falls off with propagation distance quadratically, which poses significant challenges for flash imaging at a long distance. We propose a new flash technique, named \u201cpatterned flash\u201d, for flash imaging at a long distance. Patterned flash concentrates optical power into a dot array. Compared with the conventional uniform flash where the signal is overwhelmed by the noise everywhere, patterned flash provides stronger signals at sparsely distributed points across the field of view to ensure the signals at those points stand out from the sensor noise. This enables post-processing to resolve important objects and details. Additionally, the patterned flash projects texture onto the scene, which can be treated as a structured light system for depth perception. Given the novel system, we develop a joint image reconstruction and depth estimation algorithm with a convolutional neural network. We build a hardware prototype and test the proposed flash technique on various scenes. The experimental results demonstrate that our patterned flash has significantly better performance at long distances in low-light environments. Our code and data are publicly available.\""
  },
  "eccv2022_main_pseudoclickinteractiveimagesegmentationwithclickimitation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PseudoClick: Interactive Image Segmentation with Click Imitation",
    "authors": [
      "Qin Liu",
      "Meng Zheng",
      "Benjamin Planche",
      "Srikrishna Karanam",
      "Terrence Chen",
      "Marc Niethammer",
      "Ziyan Wu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2903_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660717.pdf",
    "published": "2020-08",
    "summary": "\"The goal of click-based interactive image segmentation is to obtain precise object segmentation masks with limited user interaction, i.e., by a minimal number of user clicks. Existing methods require users to provide all the clicks: by first inspecting the segmentation mask and then providing points on mislabeled regions, iteratively. We ask the question: can our model directly predict where to click, so as to further reduce the user interaction cost? To this end, we propose PseudoClick, a generic framework that enables existing segmentation networks to propose candidate next clicks. These automatically generated clicks, termed pseudo clicks in this work, serve as an imitation of human clicks to refine the segmentation mask. We build PseudoClick on existing segmentation backbones and show how our click prediction mechanism leads to improved performance. We evaluate PseudoClick on 10 public datasets from different domains and modalities, showing that our model not only outperforms existing approaches but also demonstrates strong generalization capability in cross-domain evaluation. We obtain new state-of-the-art results on several popular benchmarks, e.g., on the PASCAL dataset, our model significantly outperforms existing state-of-the-art by reducing 12.4% and 11.4% number of clicks to achieve 85% and 90% IoU, respectively.\""
  },
  "eccv2022_main_ct$2$colorizationtransformerviacolortokens": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CT$^2$: Colorization Transformer via Color Tokens",
    "authors": [
      "Shuchen Weng",
      "Jimeng Sun",
      "Yu Li",
      "Si Li",
      "Boxin Shi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3007_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670001.pdf",
    "published": "2020-08",
    "summary": "\"Automatic image colorization is an ill-posed problem with multi-modal uncertainty, and there remains two main challenges with previous methods: incorrect semantic colors and under-saturation. In this paper, we propose an end-to-end transformer-based model to overcome these challenges. Benefited from the long-range context extraction of transformer and our holistic architecture, our method could colorize images with more diverse colors. Besides, we introduce color tokens into our approach and treat the colorization task as a classification problem, which increases the saturation of results. We also propose a series of modules to make image features interact with color tokens, and restrict the range of possible color candidates, which makes our results visually pleasing and reasonable. In addition, our method does not require any additional external priors, which ensures its well generalization capability. Extensive experiments and user studies demonstrate that our method achieves superior performance than previous works.\""
  },
  "eccv2022_main_simplebaselinesforimagerestoration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Simple Baselines for Image Restoration",
    "authors": [
      "Liangyu Chen",
      "Xiaojie Chu",
      "Xiangyu Zhang",
      "Jian Sun"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3043_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670017.pdf",
    "published": "2020-08",
    "summary": "\"Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid, ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by multiplication or removed. Thus, we derive a Nonlinear Activation Free Network, namely NAFNet, from the baseline. SOTA results are achieved on various challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring), exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs; 40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28 dB with less than half of its computational costs. The code and the pre-trained models are released at https://github.com/megvii-research/NAFNet.\""
  },
  "eccv2022_main_spiketransformermonoculardepthestimationforspikingcamera": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Spike Transformer: Monocular Depth Estimation for Spiking Camera",
    "authors": [
      "Jiyuan Zhang",
      "Lulu Tang",
      "Zhaofei Yu",
      "Jiwen Lu",
      "Tiejun Huang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3125_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670034.pdf",
    "published": "2020-08",
    "summary": "\"Spiking camera is a bio-inspired vision sensor that mimics the sampling mechanism of the primate fovea, which has shown great potential for capturing high-speed dynamic scenes with a sampling rate of 40,000 Hz. Unlike conventional digital cameras, the spiking camera continuously captures photons and outputs asynchronous binary spikes that encode time, location, and light intensity. Because of the different sampling mechanisms, the off-the-shelf image-based algorithms for digital cameras are unsuitable for spike streams generated by the spiking camera. Therefore, it is of particular interest to develop novel, spike-aware algorithms for common computer vision tasks. In this paper, we focus on the depth estimation task, which is challenging due to the natural properties of spike streams, such as irregularity, continuity, and spatial-temporal correlation, and has not been explored for the spiking camera. We present Spike Transformer (Spike-T), a novel paradigm for learning spike data and estimating monocular depth from continuous spike streams. To fit spike data to Transformer, we present an input spike embedding equipped with a spatio-temporal patch partition module to maintain features from both spatial and temporal domains. Furthermore, we build two spike-based depth datasets. One is synthetic, and the other is captured by a real spiking camera. Experimental results demonstrate that the proposed Spike-T can favorably predict the scene\u2019s depth and consistently outperform its direct competitors. More importantly, the representation learned by Spike-T transfers well to the unseen real data, indicating the generalization of Spike-T to real-world scenarios. To our best knowledge, this is the first time that directly depth estimation from spike streams becomes possible. Code and Datasets are available at https://github.com/Leozhangjiyuan/MDE-SpikingCamera.\""
  },
  "eccv2022_main_improvingimagerestorationbyrevisitingglobalinformationaggregation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Improving Image Restoration by Revisiting Global Information Aggregation",
    "authors": [
      "Xiaojie Chu",
      "Liangyu Chen",
      "Chengpeng Chen",
      "Xin Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3129_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670053.pdf",
    "published": "2020-08",
    "summary": "\"Global operations, such as global average pooling, are widely used in top-performance image restorers. They aggregate global information from input features along entire spatial dimensions but behave differently during training and inference in image restoration tasks: they are based on different regions, namely the cropped patches (from images) and the full-resolution images. This paper revisits global information aggregation and finds that the image-based features during inference have a different distribution than the patch-based features during training. This train-test inconsistency negatively impacts the performance of models, which is severely overlooked by previous works. To reduce the inconsistency and improve test-time performance, we propose a simple method called Test-time Local Converter (TLC). Our TLC converts global operations to local ones only during inference so that they aggregate features within local spatial regions rather than the entire large images. The proposed method can be applied to various global modules (e.g., normalization, channel and spatial attention) with negligible costs. Without the need for any fine-tuning, TLC improves state-of-the-art results on several image restoration tasks, including single-image motion deblurring, video deblurring, defocus deblurring, and image denoising. In particular, with TLC, our Restormer-Local improves the state-of-the-art result in single image deblurring from 32.92 dB to 33.57 dB on GoPro dataset. The code is available at https://github.com/megvii-research/tlc.\""
  },
  "eccv2022_main_dataassociationbetweeneventstreamsandintensityframesunderdiversebaselines": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Data Association between Event Streams and Intensity Frames under Diverse Baselines",
    "authors": [
      "Dehao Zhang",
      "Qiankun Ding",
      "Peiqi Duan",
      "Chu Zhou",
      "Boxin Shi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3265_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670071.pdf",
    "published": "2020-08",
    "summary": "\"This paper proposes a learning-based framework to associate event streams and intensity frames under diverse camera baselines, to simultaneously benefit to camera pose estimation under large baseline and depth estimation under small baseline. Based on the observation that event streams are globally sparse (a small percentage of pixels in global frames are triggered with events) and locally dense (a large percentage of pixels in local patches are triggered with events) in the spatial domain, we put forward a two-stage architecture for matching feature maps. LSparse-Net uses a large receptive field to find sparse matches while SDense-Net uses a small receptive field to find dense matches. Both two stages apply transformer modules with self-attention layers and cross-attention layers to effectively process multi-resolution features from the feature pyramid network backbone. Experimental results on public datasets show systematic performance improvement for both tasks compared to state-of-the-art methods.\""
  },
  "eccv2022_main_d2hnetjointdenoisinganddeblurringwithhierarchicalnetworkforrobustnightimagerestoration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "D2HNet: Joint Denoising and Deblurring with Hierarchical Network for Robust Night Image Restoration",
    "authors": [
      "Yuzhi Zhao",
      "Yongzhe Xu",
      "Qiong Yan",
      "Dingdong Yang",
      "Xuehui Wang",
      "Lai-Man Po"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3512_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670089.pdf",
    "published": "2020-08",
    "summary": "\"Night imaging with modern smartphone cameras is troublesome due to low photon count and unavoidable noise in the imaging system. Directly adjusting exposure time and ISO ratings cannot obtain sharp and noise-free images at the same time in low-light conditions. Though many methods have been proposed to enhance noisy or blurry night images, their performances on real-world night photos are still unsatisfactory due to two main reasons: 1) Limited information in a single image and 2) Domain gap between synthetic training images and real-world photos (e.g., differences in blur area and resolution). To exploit the information from successive long- and short-exposure images, we propose a learning-based pipeline to fuse them. A D2HNet framework is developed to recover a high-quality image by deblurring and enhancing a long-exposure image under the guidance of a short-exposure image. To shrink the domain gap, we leverage a two-phase DeblurNet-EnhanceNet architecture, which performs accurate blur removal on a fixed low resolution so that it is able to handle large ranges of blur in different resolution inputs. In addition, we synthesize a D2-Dataset from HD videos and experiment on it. The results on the validation set and real photos demonstrate our methods achieve better visual quality and state-of-the-art quantitative scores. The D2HNet codes and D2-Dataset can be found at https://github.com/zhaoyuzhi/D2HNet.\""
  },
  "eccv2022_main_learninggraphneuralnetworksforimagestyletransfer": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Graph Neural Networks for Image Style Transfer",
    "authors": [
      "Yongcheng Jing",
      "Yining Mao",
      "Yiding Yang",
      "Yibing Zhan",
      "Mingli Song",
      "Xinchao Wang",
      "Dacheng Tao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3643_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670108.pdf",
    "published": "2020-08",
    "summary": "\"State-of-the-art parametric and non-parametric style transfer approaches are prone to either distorted local style patterns due to global statistics alignment, or unpleasing artifacts resulting from patch mismatching. In this paper, we study a novel semi-parametric neural style transfer framework that alleviates the deficiency of both parametric and non-parametric stylization. The core idea of our approach is to establish accurate and fine-grained content-style correspondences using graph neural networks (GNNs). To this end, we develop an elaborated GNN model with content and style local patches as the graph vertices. The style transfer procedure is then modeled as the attention-based heterogeneous message passing between the style and content nodes in a learnable manner, leading to adaptive many-to-one style-content correlations at the local patch level. In addition, an elaborated deformable graph convolutional operation is introduced for cross-scale style-content matching. Experimental results demonstrate that the proposed semi-parametric image stylization approach yields encouraging results on the challenging style patterns, preserving both global appearance and exquisite details. Furthermore, by controlling the number of edges at the inference stage, the proposed method also triggers novel functionalities like diversified patch-based stylization with a single model.\""
  },
  "eccv2022_main_deepps2revisitingphotometricstereousingtwodifferentlyilluminatedimages": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DeepPS2: Revisiting Photometric Stereo Using Two Differently Illuminated Images",
    "authors": [
      "Ashish Tiwari",
      "Shanmuganathan Raman"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3966_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670125.pdf",
    "published": "2020-08",
    "summary": "\"Estimating 3D surface normals through photometric stereo has been of great interest in computer vision research. Despite the success of existing traditional and deep learning-based methods, it is still challenging due to: (i) the requirement of three or more differently illuminated images, (ii) the inability to model unknown general reflectance, and (iii) the requirement of accurate 3D ground truth surface normals and known lighting information for training. In this work, we attempt to address an under-explored problem of photometric stereo using just two differently illuminated images, referred to as the PS2 problem. It is an intermediate case between a single image-based reconstruction method like Shape from Shading (SfS) and the traditional Photometric Stereo (PS), which requires three or more images. We propose an inverse rendering-based deep learning framework, called DeepPS2, that jointly performs surface normal, albedo, lighting estimation, and image relighting in a completely self-supervised manner with no requirement of ground truth data. We demonstrate how image relighting in conjunction with image reconstruction enhances the lighting estimation in a self-supervised setting.\""
  },
  "eccv2022_main_instancecontouradjustmentviastructure-drivencnn": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Instance Contour Adjustment via Structure-Driven CNN",
    "authors": [
      "Shuchen Weng",
      "Yi Wei",
      "Ming-Ching Chang",
      "Boxin Shi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4084_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670142.pdf",
    "published": "2020-08",
    "summary": "\"Instance contour adjustment is desirable in image editing, which allows the contour of an instance in a photo to be either dilated or eroded via user sketching. This imposes several requirements for a favorable method in order to generate meaningful textures while preserving clear user-desired contours. Due to the ignorance of these requirements, the off-the-shelf image editing methods herein are unsuited. Therefore, we propose a specialized two-stage method. The first stage extracts the structural cues from the input image, and completes the missing structural cues for the adjusted area. The second stage is a structure-driven CNN which generates image textures following the guidance of the completed structural cues. In the structure-driven CNN, we redesign the context sampling strategy of the convolution operation and attention mechanism such that they can estimate and rank the relevance of the contexts based on the structural cues, and sample the top-ranked contexts regardless of their distribution on the image plane. Thus, the meaningfulness of image textures with clear and user-desired contours are guaranteed by the structure-driven CNN. In addition, our method does not require any semantic label as input, which thus ensures its well generalization capability. We evaluate our method against several baselines adapted from the related tasks, and the experimental results demonstrate its effectiveness.\""
  },
  "eccv2022_main_synthesizinglightfieldvideofrommonocularvideo": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Synthesizing Light Field Video from Monocular Video",
    "authors": [
      "Shrisudhan Govindarajan",
      "Prasan Shedligeri",
      "Sarah",
      "Kaushik Mitra"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4122_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670158.pdf",
    "published": "2020-08",
    "summary": "\"The hardware challenges associated with light-field(LF) imaging has made it difficult for consumers to access its benefits like applications in post-capture focus and aperture control. Learning-based techniques which solve the ill-posed problem of LF reconstruction from sparse (1, 2 or 4) views have significantly reduced the requirement for complex hardware. LF video reconstruction from sparse views poses a special challenge as acquiring ground-truth for training these models is hard. Hence, we propose a self-supervised learning-based algorithm for LF video reconstruction from monocular videos. We use self-supervised geometric, photometric and temporal consistency constraints inspired from a recent self-supervised technique for LF video reconstruction from stereo video. Additionally, we propose three key techniques that are relevant to our monocular video input. We propose an explicit disocclusion handling technique that encourages the network to inpaint disoccluded regions in a LF frame, using information from adjacent input temporal frames. This is crucial for a self-supervised technique as a single input frame does not contain any information about the disoccluded regions. We also propose an adaptive low-rank representation that provides a significant boost in performance by tailoring the representation to each input scene. Finally, we also propose a novel refinement block that is able to exploit the available LF image data using supervised learning to further refine the reconstruction quality. Our qualitative and quantitative analysis demonstrates the significance of each of the proposed building blocks and also the superior results compared to previous state-of-the-art monocular LF reconstruction techniques. We further validate our algorithm by reconstructing LF videos from monocular videos acquired using a commercial GoPro camera.\""
  },
  "eccv2022_main_human-centricimagecroppingwithpartition-awareandcontent-preservingfeatures": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Human-Centric Image Cropping with Partition-Aware and Content-Preserving Features",
    "authors": [
      "Bo Zhang",
      "Li Niu",
      "Xing Zhao",
      "Liqing Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4249_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670176.pdf",
    "published": "2020-08",
    "summary": "\"Image cropping aims to find visually appealing crops in an image, which is an important yet challenging task. In this paper, we consider a specific and practical application: human-centric image cropping, which focuses on the depiction of a person. To this end, we propose a human-centric image cropping method with two novel feature designs for the candidate crop: partition-aware feature and content-preserving feature. For partition-aware feature, we divide the whole image into nine partitions based on the human bounding box and treat different partitions in a candidate crop differently conditioned on the human information. For content-preserving feature, we predict a heatmap indicating the important content to be included in a good crop, and extract the geometric relation between the heatmap and a candidate crop. Extensive experiments demonstrate that our method can perform favorably against state-of-the-art image cropping methods on human-centric image cropping task. Code is available at https://github.com/bcmi/Human-Centric-Image-Cropping.\""
  },
  "eccv2022_main_demfideepjointdeblurringandmulti-frameinterpolationwithflow-guidedattentivecorrelationandrecursiveboosting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DeMFI: Deep Joint Deblurring and Multi-Frame Interpolation with Flow-Guided Attentive Correlation and Recursive Boosting",
    "authors": [
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4406_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670193.pdf",
    "published": "2020-08",
    "summary": "\"We propose a novel joint deblurring and multi-frame interpolation (DeMFI) framework in a two-stage manner, called DeMFINet, which converts blurry videos of lower-frame-rate to sharp videos at higher-frame-rate based on flow-guided attentive-correlation-based feature bolstering (FAC-FB) module and recursive boosting (RB), in terms of multi-frame interpolation (MFI). Its baseline version performs featureflow-based warping with FAC-FB module to obtain a sharp-interpolated frame as well to deblur two center-input frames. Its extended version further improves the joint performance based on pixel-flow-based warping with GRU-based RB. Our FAC-FB module effectively gathers the distributed blurry pixel information over blurry input frames in featuredomain to improve the joint performances. RB trained with recursive boosting loss enables DeMFI-Net to adequately select smaller RB iterations for a faster runtime during inference, even after the training is finished. As a result, our DeMFI-Net achieves state-of-the-art (SOTA) performances for diverse datasets with significant margins compared to recent joint methods. All source codes, including pretrained DeMFI-Net, are publicly available at https://github.com/JihyongOh/DeMFI.\""
  },
  "eccv2022_main_neuralimagerepresentationsformulti-imagefusionandlayerseparation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Neural Image Representations for Multi-Image Fusion and Layer Separation",
    "authors": [
      "Seonghyeon Nam",
      "Marcus A. Brubaker",
      "Michael S. Brown"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4471_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670210.pdf",
    "published": "2020-08",
    "summary": "\"We propose a framework for aligning and fusing multiple images into a single view using neural image representations (NIRs), also known as implicit or coordinate-based neural representations. Our framework targets burst images that exhibit camera ego motion and potential changes in the scene. We describe different strategies for alignment depending on the nature of the scene motion---namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. With the neural image representation, our framework effectively combines multiple inputs into a single canonical view without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.\""
  },
  "eccv2022_main_bringingrollingshutterimagesalivewithdualreverseddistortion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Bringing Rolling Shutter Images Alive with Dual Reversed Distortion",
    "authors": [
      "Zhihang Zhong",
      "Mingdeng Cao",
      "Xiao Sun",
      "Zhirong Wu",
      "Zhongyi Zhou",
      "Yinqiang Zheng",
      "Stephen Lin",
      "Imari Sato"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4547_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670227.pdf",
    "published": "2020-08",
    "summary": "\"Rolling shutter (RS) distortion can be interpreted as the result of picking a row of pixels from instant global shutter (GS) frames over time during the exposure of the RS camera. This means that the information of each instant GS frame is partially, yet sequentially, embedded into the row-dependent distortion. Inspired by this fact, we address the challenging task of reversing this process, i.e., extracting undistorted GS frames from images suffering from RS distortion. However, since RS distortion is coupled with other factors such as readout settings and the relative velocity of scene elements to the camera, models that only exploit the geometric correlation between temporally adjacent images suffer from poor generality in processing data with different readout settings and dynamic scenes with both camera motion and object motion. In this paper, instead of two consecutive frames, we propose to exploit a pair of images captured by dual RS cameras with reversed RS directions for this highly challenging task. Grounded on the symmetric and complementary nature of dual reversed distortion, we develop a novel end-to-end model, IFED, to generate dual optical flow sequence through iterative learning of the velocity field during the RS time. Extensive experimental results demonstrate that IFED is superior to naive cascade schemes, as well as the state-of-the-art which utilizes adjacent RS images. Most importantly, although it is trained on a synthetic dataset, IFED is shown to be effective at retrieving GS frame sequences from real-world RS distorted images of dynamic scenes. Code is available at https://github.com/zzh-tech/Dual-Reversed-RS.\""
  },
  "eccv2022_main_filmframeinterpolationforlargemotion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "FILM: Frame Interpolation for Large Motion",
    "authors": [
      "Fitsum Reda",
      "Janne Kontkanen",
      "Eric Tabellion",
      "Deqing Sun",
      "Caroline Pantofaru",
      "Brian Curless"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4614_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670244.pdf",
    "published": "2020-08",
    "summary": "\"We present a frame interpolation algorithm that synthesizes an engaging slow-motion video from near-duplicate photos which often exhibit large scene motion. Near-duplicates interpolation is an interesting new application, but large motion poses challenges to existing methods. To address this issue, we adapt a feature extractor that shares weights across the scales, and present a \"\"scale-agnostic\"\" motion estimator. It relies on the intuition that large motion at finer scales should be similar to small motion at coarser scales, which boosts the number of available pixels for large motion supervision. To inpaint wide disocclusions caused by large motion and synthesize crisp frames, we propose to optimize our network with the Gram matrix loss that measures the correlation difference between features. To simplify the training process, we further propose a unified single-network approach that removes the reliance on additional optical-flow or depth network and is trainable from frame triplets alone. Our approach outperforms state-of-the-art methods on the Xiph large motion benchmark while performing favorably on Vimeo-90K, Middlebury and UCF101. Codes and pre-trained models are available at https://film-net.github.io.\""
  },
  "eccv2022_main_videointerpolationbyevent-drivenanisotropicadjustmentofopticalflow": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Video Interpolation by Event-Driven Anisotropic Adjustment of Optical Flow",
    "authors": [
      "Song Wu",
      "Kaichao You",
      "Weihua He",
      "Chen Yang",
      "Yang Tian",
      "Yaoyuan Wang",
      "Ziyang Zhang",
      "Jianxing Liao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4754_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670261.pdf",
    "published": "2020-08",
    "summary": "\"Video frame interpolation is a challenging task due to the ever-changing real-world scene. Previous methods often calculate the bi-directional optical flows and then predict the intermediate optical flows under the linear motion assumptions, leading to isotropic intermediate flow generation. Follow-up research obtained anisotropic adjustment through estimated higher-order motion information with extra frames. Based on the motion assumptions, their methods are hard to model the complicated motion in real scenes. In this paper, we propose an end-to-end training method A^2OF for video frame interpolation with event-driven Anisotropic Adjustment of Optical Flows. Specifically, we use events to generate optical flow distribution masks for the intermediate optical flow, which can model the complicated motion between two frames. Our proposed method outperforms the previous methods in video frame interpolation, taking supervised event-based video interpolation to a higher stage.\""
  },
  "eccv2022_main_evac3dfromevent-basedapparentcontoursto3dmodelsviacontinuousvisualhulls": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "EvAC3D: From Event-Based Apparent Contours to 3D Models via Continuous Visual Hulls",
    "authors": [
      "Ziyun Wang",
      "Kenneth Chaney",
      "Kostas Daniilidis"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5100_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670278.pdf",
    "published": "2020-08",
    "summary": "\"3D reconstruction from multiple views is a successful computer vision field with multiple deployments in applications. State of the art is based on traditional RGB frames that enable optimization of photo-consistency cross views. In this paper, we study the problem of 3D reconstruction from event-cameras, motivated by the advantages of event-based cameras in terms of low power and latency as well as by the biological evidence that eyes in nature capture the same data and still perceive well 3D shape. The foundation of our hypothesis that 3D-reconstruction is feasible using events lies in the information contained in the occluding contours and in the continuous scene acquisition with events. We propose Apparent Contour Events (ACE), a novel event-based representation that defines the geometry of the apparent contour of an object. We represent ACE by a spatially and temporally continuous implicit function defined in the event x-y-t space. Furthermore, we design a novel continuous Voxel Carving algorithm enabled by the high temporal resolution of the Apparent Contour Events. To evaluate the performance of the method, we collect MOEC-3D, a 3D event dataset of a set of common real-world objects. We demonstrate EvAC3D\u2019s ability to reconstruct high-fidelity mesh surfaces from real event sequences while allowing the refinement of the 3D reconstruction for each individual event.\""
  },
  "eccv2022_main_dccfdeepcomprehensiblecolorfilterlearningframeworkforhigh-resolutionimageharmonization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DCCF: Deep Comprehensible Color Filter Learning Framework for High-Resolution Image Harmonization",
    "authors": [
      "Ben Xue",
      "Shenghui Ran",
      "Quan Chen",
      "Rongfei Jia",
      "Binqiang Zhao",
      "Xing Tang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5142_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670294.pdf",
    "published": "2020-08",
    "summary": "\"Image color harmonization algorithm aims to automatically match the color distribution of foreground and background images captured in different conditions. Previous deep learning based models neglect two issues that are critical for practical applications, namely high resolution (HR) image processing and model comprehensibility. In this paper, we propose a novel Deep Comprehensible Color Filter (DCCF) learning framework for high-resolution image harmonization. Specifically, DCCF first downsamples the original input image to its low-resolution (LR) counter-part, then learns four human comprehensible neural filters (i.e. hue, saturation, value and attentive rendering filters) in an end-to-end manner, finally applies these filters to the original input image to get the harmonized result. Benefiting from the comprehensible neural filters, we could provide a simple yet efficient handler for users to cooperate with deep model to get the desired results with very little effort when necessary. Extensive experiments demonstrate the effectiveness of DCCF learning framework and it outperforms state-of-the-art post-processing method on iHarmony4 dataset on images\u2019 full-resolutions by achieving 7.63% and 1.69% relative improvements on MSE and PSNR respectively. Our code is available at https://github.com/rockeyben/DCCF.\""
  },
  "eccv2022_main_selectionconvconvolutionalneuralnetworksfornon-rectilinearimagedata": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SelectionConv: Convolutional Neural Networks for Non-Rectilinear Image Data",
    "authors": [
      "David Hart",
      "Michael Whitney",
      "Bryan Morse"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5224_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670310.pdf",
    "published": "2020-08",
    "summary": "\"Convolutional Neural Networks have revolutionized vision applications. There are image domains and representations, however, that cannot be handled by standard CNNs (e.g., spherical images, superpixels). Such data are usually processed using networks and algorithms specialized for each type. In this work, we show that it may not always be necessary to use specialized neural networks to operate on such spaces. Instead, we introduce a new structured graph convolution operator that can copy 2D convolution weights, transferring the capabilities of already trained traditional CNNs to our new graph network. This network can then operate on any data that can be represented as a positional graph. By converting non-rectilinear data to a graph, we can apply these convolutions on these irregular image domains without requiring training on large domain-specific datasets.\""
  },
  "eccv2022_main_spatial-separatedcurverenderingnetworkforefficientandhigh-resolutionimageharmonization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Spatial-Separated Curve Rendering Network for Efficient and High-Resolution Image Harmonization",
    "authors": [
      "Jingtang Liang",
      "Xiaodong Cun",
      "Chi-Man Pun",
      "Jue Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5481_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670327.pdf",
    "published": "2020-08",
    "summary": "\"Image harmonization aims to modify the color of the composited region according to the specific background. Previous works model this task as a pixel-wise image translation using UNet family structures. However, the model size and computational cost limit the ability of their models on edge devices and higher-resolution images. In this paper, we propose a spatial-separated curve rendering network (S2CRNet), a novel framework to prove that the simple global editing can effectively address this task as well as the challenge of high-resolution image harmonization for the first time. In S2CRNet, we design a curve rendering module (CRM) using spatial-specific knowledge to generate the parameters of the piece-wise curve mapping in the foreground region and we can directly render the original high-resolution images using the learned color curve. Besides, we also make two extensions of the proposed framework via cascaded refinement and semantic guidance. Experiments show that the proposed method reduces more than 90% of parameters compared with previous methods but still achieves the state-of-the-art performance on 3 benchmark datasets. Moreover, our method can work smoothly on higher resolution images with much lower GPU computational resources. The source codes are available at: \\url{http://github.com/stefanLeong/S2CRNet}.\""
  },
  "eccv2022_main_bigcolorcolorizationusingagenerativecolorpriorfornaturalimages": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "BigColor: Colorization Using a Generative Color Prior for Natural Images",
    "authors": [
      "Geonung Kim",
      "Kyoungkook Kang",
      "Seongtae Kim",
      "Hwayoon Lee",
      "Sehoon Kim",
      "Jonghyun Kim",
      "Seung-Hwan Baek",
      "Sunghyun Cho"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5898_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670343.pdf",
    "published": "2020-08",
    "summary": "\"For realistic and vivid colorization, generative priors have recently been exploited. However, such generative priors often fail for in-the-wild complex images due to their limited representation space. In this paper, we propose BigColor, a novel colorization approach that provides vivid colorization for diverse in-the-wild images with complex structures. While previous generative priors are trained to synthesize both image structures and colors, we learn a generative color prior to focus on color synthesis given the spatial structure of an image. In this way, we reduce the burden of synthesizing image structures from the generative prior and expand its representation space to cover diverse images. To this end, we propose a BigGAN-inspired encoder-generator network that uses a spatial feature map instead of a spatially-flattened BigGAN latent code, resulting in an enlarged representation space. Our method enables robust colorization for diverse inputs in a single forward pass, supports arbitrary input resolutions, and provides multi-modal colorization results. We demonstrate that BigColor significantly outperforms existing methods especially on in-the-wild images with complex structures.\""
  },
  "eccv2022_main_cadyqcontent-awaredynamicquantizationforimagesuper-resolution": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution",
    "authors": [
      "Cheeun Hong",
      "Sungyong Baik",
      "Heewon Kim",
      "Seungjun Nah",
      "Kyoung Mu Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5937_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670360.pdf",
    "published": "2020-08",
    "summary": "\"Despite breakthrough advances in image super-resolution (SR) with convolutional neural networks (CNNs), SR has yet to enjoy ubiquitous applications due to the high computational complexity of SR networks. Quantization is one of the promising approaches to solve this problem. However, existing methods fail to quantize SR models with a bit-width lower than 8 bits, suffering from severe accuracy loss due to fixed bit-width quantization applied everywhere. In this work, to achieve high average bit-reduction with less accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ) method for SR networks that allocates optimal bits to local regions and layers adaptively based on the local contents of an input image. To this end, a trainable bit selector module is introduced to determine the proper bit-width and quantization level for each layer and a given local image patch. This module is governed by the quantization sensitivity that is estimated by using both the average magnitude of image gradient of the patch and the standard deviation of the input feature of the layer. The proposed quantization pipeline has been tested on various SR networks and evaluated on several standard benchmarks extensively. Significant reduction in computational complexity and the elevated restoration accuracy clearly demonstrate the effectiveness of the proposed CADyQ framework for SR. Codes are available at https://github.com/Cheeun/CADyQ.\""
  },
  "eccv2022_main_deepsemanticstatisticsmatching(d2sm)denoisingnetwork": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Deep Semantic Statistics Matching (D2SM) Denoising Network",
    "authors": [
      "Kangfu Mei",
      "Vishal M. Patel",
      "Rui Huang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6145_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670377.pdf",
    "published": "2020-08",
    "summary": "\"The ultimate aim of image restoration like denoising is to find an exact correlation between the noisy and clear image domains. But the optimization of end-to-end denoising learning like pixel-wise losses is performed in a sample-to-sample manner, which ignores the intrinsic correlation of images, especially semantics. In this paper, we introduce the Deep Semantic Statistics Matching (D2SM) Denoising Network. It exploits semantic features of pretrained classification networks, then it implicitly matches the probabilistic distribution of clear images at the semantic feature space. By learning to preserve the semantic distribution of denoised images, we empirically find our method significantly improves the denoising capabilities of networks, and the denoised results can be better understood by high-level vision tasks. Comprehensive experiments conducted on the noisy Cityscapes dataset demonstrate the superiority of our method on both the denoising performance and semantic segmentation accuracy. Moreover, the performance improvement observed on our extended tasks including super-resolution and dehazing experiments shows its potentiality as a new general plug-and-play component.\""
  },
  "eccv2022_main_3dsceneinferencefromtransienthistograms": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Scene Inference from Transient Histograms",
    "authors": [
      "Sacha Jungerman",
      "Atul Ingle",
      "Yin Li",
      "Mohit Gupta"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6177_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670394.pdf",
    "published": "2020-08",
    "summary": "\"Time-resolved image sensors that capture light at pico-to-nanosecond timescales were once limited to niche applications but are now rapidly becoming mainstream in consumer devices. We propose low-cost and low-power imaging modalities that capture scene information from minimal time-resolved image sensors with as few as one pixel. The key idea is to flood illuminate large scene patches (or the entire scene) with a pulsed light source and measure the time-resolved reflected light by integrating over the entire illuminated area. The one-dimensional measured temporal waveform, called transient, encodes both distances and albedoes at all visible scene points and as such is an aggregate proxy for the scene\u2019s 3D geometry. We explore the viability and limitations of the transient waveforms for recovering scene information by itself, and also when combined with traditional RGB cameras. We show that plane estimation can be performed from a single transient and that using only a few more it is possible to recover a depth map of the whole scene. We also show two proof-of-concept hardware prototypes that demonstrate the feasibility of our approach for compact, mobile, and budget-limited applications.\""
  },
  "eccv2022_main_neuralspace-fillingcurves": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Neural Space-Filling Curves",
    "authors": [
      "Hanyu Wang",
      "Kamal Gupta",
      "Larry Davis",
      "Abhinav Shrivastava"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6187_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670412.pdf",
    "published": "2020-08",
    "summary": "\"We present Neural Space-filling Curves (SFCs), a data-driven approach to infer a context-based scan order for a set of images. Linear ordering of pixels forms the basis for many applications such as video scrambling, compression, and auto-regressive models that are used in generative modeling for images. Existing algorithms resort to a fixed scanning algorithm such as Raster scan or Hilbert scan. Instead, our work learns a spatially coherent linear ordering of pixels from the dataset of images using a graph-based neural network. The resulting Neural SFC is optimized for an objective suitable for the downstream task when the image is traversed along with the scan line order. We show the advantage of using Neural SFCs in downstream applications such as image compression. Code available in the supplementary material.\""
  },
  "eccv2022_main_exposure-awaredynamicweightedlearningforsingle-shothdrimaging": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exposure-Aware Dynamic Weighted Learning for Single-Shot HDR Imaging",
    "authors": [
      "An Gia Vien",
      "Chul Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6250_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670429.pdf",
    "published": "2020-08",
    "summary": "\"We propose a novel single-shot high dynamic range (HDR) imaging algorithm based on exposure-aware dynamic weighted learning, which reconstructs an HDR image from a spatially varying exposure (SVE) raw image. First, we recover poorly exposed pixels by developing a network that learns local dynamic filters to exploit local neighboring pixels across color channels. Second, we develop another network that combines only valid features in well-exposed regions by learning exposure-aware feature fusion. Third, we synthesize the raw radiance map by adaptively combining the outputs of the two networks that have different characteristics with complementary information. Finally, a full-color HDR image is obtained by interpolating missing color information. Experimental results show that the proposed algorithm outperforms conventional algorithms significantly on various datasets. The source codes and pretrained models are available at https://github.com/viengiaan/EDWL.\""
  },
  "eccv2022_main_seeingthroughablackboxtowardhigh-qualityterahertzimagingviasubspace-and-attentionguidedrestoration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Seeing through a Black Box: Toward High-Quality Terahertz Imaging via Subspace-and-Attention Guided Restoration",
    "authors": [
      "Weng-Tai Su",
      "Yi-Chun Hung",
      "Po-Jen Yu",
      "Shang-Hua Yang",
      "Chia-Wen Lin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6259_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670447.pdf",
    "published": "2020-08",
    "summary": "\"Terahertz (THz) imaging has recently attracted significant attention thanks to its non-invasive, non-destructive, non-ionizing, material-classification, and ultra-fast nature for object exploration and inspection. However, its strong water absorption nature and low noise tolerance lead to undesired blurs and distortions of reconstructed THz images. The performances of existing restoration methods are highly constrained by the diffraction-limited THz signals. To address the problem, we propose a novel Subspace-and-Attention-guided Restoration Network (SARNet) that fuses multi-spectral features of a THz image for effective restoration. To this end, SARNet uses multi-scale branches to extract spatio-spectral features of amplitude and phase which are then fused via shared subspace projection and attention guidance. Here, we experimentally construct a THz time-domain spectroscopy system covering a broad frequency range from 0.1 THz to 4 THz for building up temporal/spectral/spatial/phase/material THz database of hidden 3D objects. Complementary to a quantitative evaluation, we demonstrate the effectiveness of SARNet on 3D THz tomographic reconstruction applications.\""
  },
  "eccv2022_main_tomographyofturbulencestrengthbasedonscintillationimaging": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Tomography of Turbulence Strength Based on Scintillation Imaging",
    "authors": [
      "Nir Shaul",
      "Yoav Y. Schechner"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6324_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670464.pdf",
    "published": "2020-08",
    "summary": "\"Developed areas have plenty of artificial light sources. As the stars, they appear to twinkle, i.e., scintillate. This effect is caused by random turbulence. We leverage this phenomenon in order to reconstruct the spatial distribution the turbulence strength (TS). Sensing is passive, using a multi-view camera setup in a city scale. The cameras sense the scintillation of light sources in the scene. The scintillation signal has a linear model of a line integral over the field of TS. Thus, the TS is recovered by linear tomography analysis. Scintillation offers measurements and TS recovery, which are more informative than tomography based on angle-of-arrival (projection distortion) statistics. We present the background and theory of the method. Then, we describe a large field experiment to demonstrate this idea, using distributed imagers. As far as we know, this work is the first to propose reconstruction of a TS horizontal field, using passive optical scintillation measurements.\""
  },
  "eccv2022_main_realisticblursynthesisforlearningimagedeblurring": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Realistic Blur Synthesis for Learning Image Deblurring",
    "authors": [
      "Jaesung Rim",
      "Geonung Kim",
      "Jungeon Kim",
      "Junyong Lee",
      "Seungyong Lee",
      "Sunghyun Cho"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6325_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670481.pdf",
    "published": "2020-08",
    "summary": "\"Training learning-based deblurring methods demands a tremendous amount of blurred and sharp image pairs. Unfortunately, existing synthetic datasets are not realistic enough, and deblurring models trained on them cannot handle real blurred images effectively. While real datasets have recently been proposed, they provide limited diversity of scenes and camera settings, and capturing real datasets for diverse settings is still challenging. To resolve this, this paper analyzes various factors that introduce differences between real and synthetic blurred images. To this end, we present RSBlur, a novel dataset with real blurred images and the corresponding sharp image sequences to enable a detailed analysis of the difference between real and synthetic blur. With the dataset, we reveal the effects of different factors in the blur generation process. Based on the analysis, we also present a novel blur synthesis pipeline to synthesize more realistic blur. We show that our synthesis pipeline can improve the deblurring performance on real blurred images.\""
  },
  "eccv2022_main_learningphasemaskforprivacy-preservingpassivedepthestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Phase Mask for Privacy-Preserving Passive Depth Estimation",
    "authors": [
      "Zaid Tasneem",
      "Giovanni Milione",
      "Yi-Hsuan Tsai",
      "Xiang Yu",
      "Ashok Veeraraghavan",
      "Manmohan Chandraker",
      "Francesco Pittaluga"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7139_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670497.pdf",
    "published": "2020-08",
    "summary": "\"With over a billion sold each year, cameras are not only becoming ubiquitous, but are driving progress in a wide range of domains such as mixed reality, robotics, and more. However, severe concerns regarding the privacy implications of camera-based solutions currently limit the range of environments where cameras can be deployed. The key question we address is: Can cameras be enhanced with a scalable solution to preserve users\u2019 privacy without degrading their machine intelligence capabilities? Our solution is a novel end-to-end adversarial learning pipeline in which a phase mask placed at the aperture plane of a camera is jointly optimized with respect to privacy and utility objectives. We conduct an extensive design space analysis to determine operating points with desirable privacy-utility tradeoffs that are also amenable to sensor fabrication and real-world constraints. We demonstrate the first working prototype that enables passive depth estimation while inhibiting face identification.\""
  },
  "eccv2022_main_lwgnet\u2013learnedwirtingergradientsforfourierptychographicphaseretrieval": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "LWGNet \u2013 Learned Wirtinger Gradients for Fourier Ptychographic Phase Retrieval",
    "authors": [
      "Atreyee Saha",
      "Salman S. Khan",
      "Sagar Sehrawat",
      "Sanjana S. Prabhu",
      "Shanti Bhattacharya",
      "Kaushik Mitra"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7691_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670515.pdf",
    "published": "2020-08",
    "summary": "\"Fourier Ptychographic Microscopy (FPM) is an imaging procedure that overcomes the traditional limit on Space-Bandwidth Product (SBP) of conventional microscopes through computational means. It utilizes multiple images captured using a low numerical aperture (NA) objective and enables high-resolution phase imaging through frequency domain stitching. Existing FPM reconstruction methods can be broadly categorized into two approaches: iterative optimization based methods, which are based on the physics of the forward imaging model, and data-driven methods which commonly employ a feed-forward deep learning framework. We propose a hybrid model-driven residual network that combines the knowledge of the forward imaging system with a deep data-driven network. Our proposed architecture, LWGNet, unrolls traditional Wirtinger flow optimization algorithm into a novel neural network design that enhances the gradient images through complex convolutional blocks. Unlike other conventional unrolling techniques, LWGNet uses fewer stages while performing at par or even better than existing traditional and deep learning techniques, particularly, for low-cost and low dynamic range CMOS sensors. This improvement in performance for low-bit depth and low-cost sensors has the potential to bring down the cost of FPM imaging setup significantly. Finally, we show consistently improved performance on our collected real data.\""
  },
  "eccv2022_main_pandorapolarization-aidedneuraldecompositionofradiance": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PANDORA: Polarization-Aided Neural Decomposition of Radiance",
    "authors": [
      "Akshat Dave",
      "Yongyi Zhao",
      "Ashok Veeraraghavan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/8029_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670531.pdf",
    "published": "2020-08",
    "summary": "\"Reconstructing an object\u2019s geometry and appearance from multiple images, also known as inverse rendering, is a fundamental problem in computer graphics and vision. Inverse rendering is inherently ill-posed because the captured image is an intricate function of unknown lighting, material properties and scene geometry. Recent progress in representing scene through coordinate-based neural networks has facilitated inverse rendering resulting in impressive geometry reconstruction and novel-view synthesis. Our key insight is that polarization is a useful cue for neural inverse rendering as polarization strongly depends on surface normals and is distinct for diffuse and specular reflectance. With the advent of commodity on-chip polarization sensors, capturing polarization has become practical. We propose PANDORA, a polarimetric inverse rendering approach based on implicit neural representations. From multi-view polarization images of an object, PANDORA jointly extracts the object\u2019s 3D geometry, separates the outgoing radiance into diffuse and specular and estimates the incident illumination. We show that PANDORA outperforms state-of-the-art radiance decomposition techniques. PANDORA outputs clean surface reconstructions free from texture artefacts, models strong specularities accurately and estimates illumination under practical unstructured scenarios.\""
  },
  "eccv2022_main_hummanmulti-modal4dhumandatasetforversatilesensingandmodeling": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "HuMMan: Multi-modal 4D Human Dataset for Versatile Sensing and Modeling",
    "authors": [
      "Zhongang Cai",
      "Daxuan Ren",
      "Ailing Zeng",
      "Zhengyu Lin",
      "Tao Yu",
      "Wenjia Wang",
      "Xiangyu Fan",
      "Yang Gao",
      "Yifan Yu",
      "Liang Pan",
      "Fangzhou Hong",
      "Mingyuan Zhang",
      "Chen Change Loy",
      "Lei Yang",
      "Ziwei Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/185_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670549.pdf",
    "published": "2020-08",
    "summary": "\"4D human sensing and modeling are fundamental tasks in vision and graphics with numerous applications. With the advances of new sensors and algorithms, there is an increasing demand for more versatile datasets. In this work, we contribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human subjects, 400k sequences and 60M frames. HuMMan has several appealing properties: 1) multi-modal data and annotations including color images, point clouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile device is included in the sensor suite; 3) a set of 500 actions, designed to cover fundamental movements; 4) multiple tasks such as action recognition, pose estimation, parametric human recovery, and textured mesh reconstruction are supported and evaluated. Extensive experiments on HuMMan voice the need for further study on challenges such as fine-grained action recognition, dynamic human mesh reconstruction, point cloud-based parametric human recovery, and cross-device domain gaps.\""
  },
  "eccv2022_main_dvs-voltmeterstochasticprocess-basedeventsimulatorfordynamicvisionsensors": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DVS-Voltmeter: Stochastic Process-Based Event Simulator for Dynamic Vision Sensors",
    "authors": [
      "Songnan Lin",
      "Ye Ma",
      "Zhenhua Guo",
      "Bihan Wen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/267_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670571.pdf",
    "published": "2020-08",
    "summary": "\"Recent advances in deep learning for event-driven applications with dynamic vision sensors (DVS) primarily rely on training over simulated data. However, most simulators ignore various physics-based characteristics of real DVS, such as the fidelity of event timestamps and comprehensive noise effects. We propose an event simulator, dubbed DVS-Voltmeter, to enable high-performance deep networks for DVS applications. DVS-Voltmeter incorporates the fundamental principle of physics - (1) voltage variations in a DVS circuit, (2) randomness caused by photon reception, and (3) noise effects caused by temperature and parasitic photocurrent - into a stochastic process. With the novel insight into the sensor design and physics, DVS-Voltmeter generates more realistic events, given high frame-rate videos. Qualitative and quantitative experiments show that the simulated events resemble real data. The evaluation on two tasks, i.e., semantic segmentation and intensity-image reconstruction, indicates that neural networks trained with DVS-Voltmeter generalize favorably on real events against state-of- the-art simulators.\""
  },
  "eccv2022_main_benchmarkingomni-visionrepresentationthroughthelensofvisualrealms": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Benchmarking Omni-Vision Representation through the Lens of Visual Realms",
    "authors": [
      "Yuanhan Zhang",
      "Zhenfei Yin",
      "Jing Shao",
      "Ziwei Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/287_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670587.pdf",
    "published": "2020-08",
    "summary": "\"Though impressive performance has been achieved in specific visual realms (\\eg faces, dogs, and places), an omni-vision representation that can generalize to many natural visual domains is highly desirable. Nonetheless, the existing benchmark for evaluating visual representations, such as ImageNet, VTAB-natural, and CLIP benchmark suite, is either limited in the spectrum of realms or built by arbitrarily integrating the current datasets. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark) that enables systematically measuring the generalization ability across a wide range of visual realms. OmniBenchmark firstly integrates the concepts from Wikidata to enlarge the storage of concepts of each sub-tree of WordNet. Then, it leverages expert knowledge from WordNet to define a comprehensive spectrum of 21 semantic realms in the natural domain, which is twice of ImageNet\u2019s. Finally, we manually annotate all 7,372 valid concepts, forming a 21-realm dataset with 1,074,346 images. With OmniBenchmark, we propose a hierarchical instance contrastive learning framework for learning better omni-vision representation, \\ie Relational Contrastive learning (ReCo), boosting the performance of representation learning across omni-realms. As the hierarchical semantic relation naturally emerges in the label system of visual datasets, ReCo attracts the representations within the same semantic realm during pre-training, facilitating the model converges faster than conventional contrastive learning when ReCo is further fine-tuned to the specific realm. Extensive experiments demonstrate the superior performance of ReCo over state-of-the-art contrastive learning methods on both ImageNet and OmniBenchmark. Beyond that, We conduct a systematic investigation of recent advances in both architectures (from CNNs to transformers) and learning paradigms (from supervised learning to self-supervised learning) on our benchmark. Multiple practical observations are revealed to facilitate future research.\""
  },
  "eccv2022_main_beatalarge-scalesemanticandemotionalmulti-modaldatasetforconversationalgesturessynthesis": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "BEAT: A Large-Scale Semantic and Emotional Multi-modal Dataset for Conversational Gestures Synthesis",
    "authors": [
      "Haiyang Liu",
      "Zihao Zhu",
      "Naoya Iwamoto",
      "Yichen Peng",
      "Zhengqing Li",
      "You Zhou",
      "Elif Bozkurt",
      "Bo Zheng"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/296_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670605.pdf",
    "published": "2020-08",
    "summary": "\"Achieving realistic, vivid, and human-like synthesized conversational gestures conditioned on multi-modal data is still an unsolved problem due to the lack of available datasets, models and standard evaluation metrics. To address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations. Our statistical analysis on BEAT demonstrates the correlation of conversational gestures with facial expressions, emotions, and semantics, in addition to the known correlation with audio, text, and speaker identity. Based on this observation, we propose a baseline model, Cascaded Motion Network (CaMN), which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the semantic relevancy, we introduce a metric, Semantic Relevance Gesture Recall (SRGR). Qualitative and quantitative experiments demonstrate metrics\u2019 validness, ground truth data quality, and baseline\u2019s state-of-the-art performance. To the best of our knowledge, BEAT is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields, including controllable gesture synthesis, cross-modality analysis, and emotional gesture recognition. The data, code and model are available on https://pantomatrix.github.io/BEAT/\""
  },
  "eccv2022_main_neuromorphicdataaugmentationfortrainingspikingneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Neuromorphic Data Augmentation for Training Spiking Neural Networks",
    "authors": [
      "Yuhang Li",
      "Youngeun Kim",
      "Hyoungseob Park",
      "Tamar Geller",
      "Priyadarshini Panda"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/601_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670623.pdf",
    "published": "2020-08",
    "summary": "\"Developing neuromorphic intelligence on event-based datasets with Spiking Neural Networks (SNNs) has recently attracted much research attention. However, the limited size of event-based datasets makes SNNs prone to overfitting and unstable convergence. This issue remains unexplored by previous academic works. In an effort to minimize this generalization gap, we propose Neuromorphic Data Augmentation (NDA), a family of geometric augmentations specifically designed for event-based datasets with the goal of significantly stabilizing the SNN training and reducing the generalization gap between training and test performance. The proposed method is simple and compatible with existing SNN training pipelines. Using the proposed augmentation, for the first time, we demonstrate the feasibility of unsupervised contrastive learning for SNNs. We conduct comprehensive experiments on prevailing neuromorphic vision benchmarks and show that NDA yields substantial improvements over previous state-of-the-art results. For example, the NDA-based SNN achieves accuracy gain on CIFAR10-DVS and N-Caltech 101 by 10.1% and 13.7%, respectively. Code is available on GitHub (URL).\""
  },
  "eccv2022_main_celebv-hqalarge-scalevideofacialattributesdataset": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset",
    "authors": [
      "Hao Zhu",
      "Wayne Wu",
      "Wentao Zhu",
      "Liming Jiang",
      "Siwei Tang",
      "Li Zhang",
      "Ziwei Liu",
      "Chen Change Loy"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/709_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670641.pdf",
    "published": "2020-08",
    "summary": "\"Large-scale datasets played an indispensable role in the recent success of face generation/editing and significantly facilitate the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for face-related video research. In this paper, we propose a large-scale, high-quality, and diverse video dataset, named the High-Quality Celebrity Video Dataset (CelebV-HQ), with rich facial attribute annotations. CelebV-HQ contains 35,666 video clips involving 15,653 identities and 83 manually labeled facial attributes covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of ethnicity, age, brightness, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on unconditional video generation and video facial attribute editing tasks. Furthermore, we envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions.\""
  },
  "eccv2022_main_moviecutsanewdatasetandbenchmarkforcuttyperecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MovieCuts: A New Dataset and Benchmark for Cut Type Recognition",
    "authors": [
      "Alejandro Pardo",
      "Fabian Caba",
      "Juan Le\u00f3n Alc\u00e1zar",
      "Ali Thabet",
      "Bernard Ghanem"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/859_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670659.pdf",
    "published": "2020-08",
    "summary": "\"Understanding movies and their structural patterns is a crucial task in decoding the craft of video editing. While previous works have developed tools for general analysis, such as detecting characters or recognizing cinematography properties at the shot level, less effort has been devoted to understanding the most basic video edit, the Cut. This paper introduces the Cut type recognition task, which requires modeling multi-modal information. To ignite research in this new task, we construct a large-scale dataset called MovieCuts, which contains 173,967 video clips labeled with ten cut types defined by professionals in the movie industry. We benchmark a set of audio-visual approaches, including some dealing with the problem\u2019s multi-modal nature. Our best model achieves 47.7% mAP, which suggests that the task is challenging and that attaining highly accurate Cut type recognition is an open research problem. Advances in automatic Cut-type recognition can unleash new experiences in the video editing industry, such as movie analysis for education, video re-editing, virtual cinematography, machine-assisted trailer generation, and machine-assisted video editing, among others. Our data and code are publicly available: https://github.com/PardoAlejo/MovieCuts.\""
  },
  "eccv2022_main_lamarbenchmarkinglocalizationandmappingforaugmentedreality": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "LaMAR: Benchmarking Localization and Mapping for Augmented Reality",
    "authors": [
      "Paul-Edouard Sarlin",
      "Mihai Dusmanu",
      "Johannes L. Sch\u00f6nberger",
      "Pablo Speciale",
      "Lukas Gruber",
      "Viktor Larsson",
      "Ondrej Miksik",
      "Marc Pollefeys"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/930_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670677.pdf",
    "published": "2020-08",
    "summary": "\"Localization and mapping is the foundational technology for augmented reality (AR) that enables sharing and persistence of digital content in the real world. While significant progress has been made, researchers are still mostly driven by unrealistic benchmarks not representative of real-world AR scenarios. In particular, benchmarks are often based on small-scale datasets with low scene diversity, captured from stationary cameras, and lacking other sensor inputs like inertial, radio, or depth data. Furthermore, ground-truth (GT) accuracy is mostly insufficient to satisfy AR requirements. To close this gap, we introduce a new benchmark with a comprehensive capture and GT pipeline, which allow us to co-register realistic AR trajectories in diverse scenes and from heterogeneous devices at scale. To establish accurate GT, our pipeline robustly aligns the captured trajectories against laser scans in a fully automatic manner. Based on this pipeline, we publish a benchmark dataset of diverse and large-scale scenes recorded with head-mounted and hand-held AR devices. We extend several state-of-the-art methods to take advantage of the AR specific setup and evaluate them on our benchmark. Based on the results, we present novel insights on current research gaps to provide avenues for future work in the community.\""
  },
  "eccv2022_main_unitaildetecting,reading,andmatchinginretailscene": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"Unitail: Detecting, Reading, and Matching in Retail Scene\"",
    "authors": [
      "Fangyi Chen",
      "Han Zhang",
      "Zaiwang Li",
      "Jiachen Dou",
      "Shentong Mo",
      "Hao Chen",
      "Yongxin Zhang",
      "Uzair Ahmed",
      "Chenchen Zhu",
      "Marios Savvides"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1259_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670695.pdf",
    "published": "2020-08",
    "summary": "\"To make full use of computer vision technology in stores, it is required to consider the actual needs that fit the characteristics of the retail scene. Pursuing this goal, we introduce the United Retail Datasets (Unitail), a large-scale benchmark of basic visual tasks on products that challenges algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped instances annotated, the Unitail offers a detection dataset to align product appearance better. Furthermore, it provides a gallery-style OCR dataset containing 1454 product categories, 30k text regions, and 21k transcriptions to enable robust reading on products and motivate enhanced product matching. Besides benchmarking the datasets using various start-of-the-arts, we customize a new detector for product detection and provide a simple OCR-based matching solution that verifies its effectiveness. The Unitail and its evaluation server are publicly available at https://unitedretail.github.io/\""
  },
  "eccv2022_main_notjuststreakstowardsgroundtruthforsingleimagederaining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Not Just Streaks: Towards Ground Truth for Single Image Deraining",
    "authors": [
      "Yunhao Ba",
      "Howard Zhang",
      "Ethan Yang",
      "Akira Suzuki",
      "Arnold Pfahnl",
      "Chethan Chinder Chandrappa",
      "Celso M. de Melo",
      "Suya You",
      "Stefano Soatto",
      "Alex Wong",
      "Achuta Kadambi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1506_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670713.pdf",
    "published": "2020-08",
    "summary": "\"We propose a large-scale dataset of real-world rainy and clean image pairs and a method to remove degradations, induced by rain streaks and rain accumulation, from the image. As there exists no real-world dataset for deraining, current state-of-the-art methods rely on synthetic data and thus are limited by the sim2real domain gap; moreover, rigorous evaluation remains a challenge due to the absence of a real paired dataset. We fill this gap by collecting a real paired deraining dataset through meticulous control of non-rain variations. Our dataset enables paired training and quantitative evaluation for diverse real-world rain phenomena (e.g. rain streaks and rain accumulation). To learn a representation robust to rain phenomena, we propose a deep neural network that reconstructs the underlying scene by minimizing a rain-robust loss between rainy and clean images. Extensive experiments demonstrate that our model outperforms the state-of-the-art deraining methods on real rainy images under various conditions. Project website: https://visual.ee.ucla.edu/gt_rain.htm/.\""
  },
  "eccv2022_main_eccvcaptioncorrectingfalsenegativesbycollectingmachine-and-human-verifiedimage-captionassociationsforms-coco": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-Verified Image-Caption Associations for MS-COCO",
    "authors": [
      "Sanghyuk Chun",
      "Wonjae Kim",
      "Song Park",
      "Minsuk Chang",
      "Seong Joon Oh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1625_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680001.pdf",
    "published": "2020-08",
    "summary": "\"Image-Text matching (ITM) is a common task for evaluating the quality of Vision and Language (VL) models. However, existing ITM benchmarks have a significant limitation. They have many missing correspondences, originating from the data construction process itself. For example, a caption is only matched with one image although the caption can be matched with other similar images and vice versa. To correct the massive false negatives, we construct the Extended COCO Validation (ECCV) Caption dataset by supplying the missing associations with machine and human annotators. We employ five state-of-the-art ITM models with diverse properties for our annotation process. Our dataset provides x3.6 positive image-to-caption associations and x8.5 caption-to-image associations compared to the original MS-COCO. We also propose to use an informative ranking-based metric mAP@R, rather than the popular Recall@K (R@K). We re-evaluate the existing 25 VL models on existing and proposed benchmarks. Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K, CxC R@1 are highly correlated with each other, while the rankings change when we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias introduced by the choice of machine annotator. Source code and dataset are available at https://github.com/naver-ai/eccv-caption\""
  },
  "eccv2022_main_motcomthemulti-objecttrackingdatasetcomplexitymetric": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MOTCOM: The Multi-Object Tracking Dataset Complexity Metric",
    "authors": [
      "Malte Pedersen",
      "Joakim Bruslund Haurum",
      "Patrick Dendorfer",
      "Thomas B. Moeslund"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1959_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680019.pdf",
    "published": "2020-08",
    "summary": "\"There exists no comprehensive metric for describing the complexity of Multi-Object Tracking (MOT) sequences. This lack of metrics decreases explainability, complicates comparison of datasets, and reduces the conversation on tracker performance to a matter of leader board position. As a remedy, we present the novel MOT dataset complexity metric (MOTCOM), which is a combination of three sub-metrics inspired by key problems in MOT: occlusion, erratic motion, and visual similarity. The insights of MOTCOM can open nuanced discussions on tracker performance and may lead to a wider acknowledgement of novel contributions developed for either less known datasets or those aimed at solving sub-problems. We evaluate MOTCOM on the comprehensive MOT17, MOT20, and MOTSynth datasets and show that MOTCOM is far better at describing the complexity of MOT sequences compared to the conventional density and number of tracks. Project page at https://vap.aau.dk/motcom.\""
  },
  "eccv2022_main_howtosynthesizealarge-scaleandtrainablemicro-expressiondataset?": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "How to Synthesize a Large-Scale and Trainable Micro-Expression Dataset?",
    "authors": [
      "Yuchi Liu",
      "Zhongdao Wang",
      "Tom Gedeon",
      "Liang Zheng"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2280_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680037.pdf",
    "published": "2020-08",
    "summary": "\"This paper does not contain technical novelty but introduces our key discoveries in a data generation protocol, a database and insights. We aim to address the lack of large-scale datasets in micro-expression (MiE) recognition due to the prohibitive cost of data collection, which renders large-scale training less feasible. To this end, we develop a protocol to automatically synthesize large scale MiE training data that allow us to train improved recognition models for real-world test data. Specifically, we discover three types of Action Units (AUs) that can constitute trainable MiEs. These AUs come from real-world MiEs, early frames of macro-expression videos, and the relationship between AUs and expression categories defined by human expert knowledge. With these AUs, our protocol then employs large numbers of face images of various identities and an off-the-shelf face generator for MiE synthesis, yielding the MiE-X dataset. MiE recognition models are trained or pre-trained on MiE-X and evaluated on real-world test sets, where very competitive accuracy is obtained. Experimental results not only validate the effectiveness of the discovered AUs and MiE-X dataset but also reveal some interesting properties of MiEs: they generalize across faces, are close to early-stage macro-expressions, and can be manually defined.\""
  },
  "eccv2022_main_arealworlddatasetformulti-view3dreconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Real World Dataset for Multi-View 3D Reconstruction",
    "authors": [
      "Rakesh Shrestha",
      "Siqi Hu",
      "Minghao Gou",
      "Ziyuan Liu",
      "Ping Tan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2610_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680054.pdf",
    "published": "2020-08",
    "summary": "\"We present a dataset of 371 3D models of everyday tabletop objects along with their 320,000 real world RGB and depth images. Accurate annotations of camera poses and object poses for each image are performed in a semi-automated fashion to facilitate the use of the dataset for myriad 3D applications like shape reconstruction, object pose estimation, shape retrieval etc. We primarily focus on learned multi-view 3D reconstruction due to the lack of appropriate real world benchmark for the task and demonstrate that our dataset can fill that gap. The entire annotated dataset along with the source code for the annotation tools and evaluation baselines will be made publicly available. Keywords: Dataset, Multi-view 3D reconstruction\""
  },
  "eccv2022_main_realyrethinkingtheevaluationof3dfacereconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "REALY: Rethinking the Evaluation of 3D Face Reconstruction",
    "authors": [
      "Zenghao Chai",
      "Haoxian Zhang",
      "Jing Ren",
      "Di Kang",
      "Zhengzhuo Xu",
      "Xuefei Zhe",
      "Chun Yuan",
      "Linchao Bao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2804_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680072.pdf",
    "published": "2020-08",
    "summary": "\"The evaluation of 3D face reconstruction results typically relies on a rigid shape alignment between the estimated 3D model and the ground-truth scan. We observe that aligning two shapes with different reference points can largely affect the evaluation results. This poses difficulties for precisely diagnosing and improving a 3D face reconstruction method. In this paper, we propose a novel evaluation approach with a new benchmark REALY, consists of 100 globally aligned face scans with accurate facial keypoints, high-quality region masks, and topology-consistent meshes. Our approach performs region-wise shape alignment and leads to more accurate, bidirectional correspondences during computing the shape errors. The fine-grained, region-wise evaluation results provide us detailed understandings about the performance of state-of-the-art 3D face reconstruction methods. For example, our experiments on single-image based reconstruction methods reveal that DECA performs the best on nose regions, while GANFit performs better on cheek regions. Besides, a new and high-quality 3DMM basis, HIFI3D++, is further derived using the same procedure as we construct REALY to align and retopologize several 3D face datasets. We will release REALY, HIFI3D++, and our new evaluation pipeline at https://realy3dface.com.\""
  },
  "eccv2022_main_capturing,reconstructing,andsimulatingtheurbanscene3ddataset": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"Capturing, Reconstructing, and Simulating: The UrbanScene3D Dataset\"",
    "authors": [
      "Liqiang Lin",
      "Yilin Liu",
      "Yue Hu",
      "Xingguang Yan",
      "Ke Xie",
      "Hui Huang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3218_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680090.pdf",
    "published": "2020-08",
    "summary": "\"We present UrbanScene3D, a large-scale data platform for research of urban scene perception and reconstruction. UrbanScene3D contains over 128k high-resolution images covering 16 scenes including large-scale real urban regions and synthetic cities with 136 km2 area in total. The dataset also contains high-precision LiDAR scans and hundreds of image sets with different observation patterns, which provide a comprehensive benchmark to design and evaluate aerial path planning and 3D reconstruction algorithms. In addition, the dataset, which is built on Unreal Engine and Airsim simulator together with the manually annotated unique instance label for each building in the dataset, enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with physical engine and lighting system not only produce variety of data but also enable users to simulate cars or drones in the proposed urban environment for future research. The dataset with aerial path planning and 3D reconstruction benchmark is available at: https://vcc.tech/UrbanScene3\""
  },
  "eccv2022_main_3dcompatcompositionofmaterialsonpartsof3dthings": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D CoMPaT: Composition of Materials on Parts of 3D Things",
    "authors": [
      "Yuchen Li",
      "Ujjwal Upadhyay",
      "Habib Slim",
      "Tezuesh Varshney",
      "Ahmed Abdelreheem",
      "Arpit Prajapati",
      "Suhail Pothigara",
      "Peter Wonka",
      "Mohamed Elhoseiny"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3631_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680107.pdf",
    "published": "2020-08",
    "summary": "\"We present 3D CoMPaT, a richly annotated large-scale dataset of more than 7.19 million rendered compositions of Materials on Parts of 7262 unique 3D Models; 990 compositions per model on average. 3D CoMPaT covers 43 shape categories, 235 unique part names, and 167 unique material classes that can be applied to parts of 3D objects. Each object with the applied part-material compositions is rendered from four equally spaced views as well as four randomized views, leading to a total of 58 million renderings (7.19 million compositions \u00d78 views). This dataset primarily focuses on stylizing 3D shapes at part-level with compatible materials. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. We present two variations of this task and adapt state-of-art 2D/3D deep learning methods to solve the problem as baselines for future research. We hope our work will help ease future research on compositional 3D Vision. The dataset and code are publicly available at https://www.3dcompat-dataset.org/\""
  },
  "eccv2022_main_partimagenetalarge,high-qualitydatasetofparts": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"PartImageNet: A Large, High-Quality Dataset of Parts\"",
    "authors": [
      "Ju He",
      "Shuo Yang",
      "Shaokang Yang",
      "Adam Kortylewski",
      "Xiaoding Yuan",
      "Jie-Neng Chen",
      "Shuai Liu",
      "Cheng Yang",
      "Qihang Yu",
      "Alan Yuille"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3719_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680124.pdf",
    "published": "2020-08",
    "summary": "\"It is natural to represent objects in terms of their parts. This has the potential to improve the performance of algorithms for object recognition and segmentation but can also help for downstream tasks like activity recognition. Research on part-based models, however, is hindered by the lack of datasets with per-pixel part annotations. This is partly due to the difficulty and high cost of annotating object parts so it has rarely been done except for humans (where there exists a big literature on part-based models). To help address this problem, we propose PartImageNet, a large, high-quality dataset with part segmentation annotations. It consists of $158$ classes from ImageNet with approximately 24,000 images. PartImageNet is unique because it offers part-level annotations on a general set of classes including non-rigid, articulated objects, while having an order of magnitude larger size compared to existing part datasets (excluding datasets of humans). It can be utilized for many vision tasks including Object Segmentation, Semantic Part Segmentation, Few-shot Learning and Part Discovery. We conduct comprehensive experiments which study these tasks and set up a set of baselines.\""
  },
  "eccv2022_main_a-okvqaabenchmarkforvisualquestionansweringusingworldknowledge": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge",
    "authors": [
      "Dustin Schwenk",
      "Apoorv Khandelwal",
      "Christopher Clark",
      "Kenneth Marino",
      "Roozbeh Mottaghi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4117_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680141.pdf",
    "published": "2020-08",
    "summary": "\"The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions cannot be answered by simply querying a knowledge base, and instead primarily require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision-language models.\""
  },
  "eccv2022_main_ood-cvabenchmarkforrobustnesstoout-of-distributionshiftsofindividualnuisancesinnaturalimages": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images",
    "authors": [
      "Bingchen Zhao",
      "Shaozuo Yu",
      "Wufei Ma",
      "Mingxin Yu",
      "Shenxiao Mei",
      "Angtian Wang",
      "Ju He",
      "Alan Yuille",
      "Adam Kortylewski"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4514_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680158.pdf",
    "published": "2020-08",
    "summary": "\"Enhancing the robustness of vision algorithms in real-world scenarios is challenging. One reason is that existing robustness benchmarks are limited, as they either rely on synthetic data or ignore the effects of individual nuisance factors. We introduce ROBIN, a benchmark dataset that includes out-of-distribution examples of 10 object categories in terms of pose, shape, texture, context and the weather conditions, and enables benchmarking models for image classification, object detection, and 3D pose estimation. Our experiments using popular baseline methods reveal that: 1) Some nuisance factors have a much stronger negative effect on the performance compared to others, also depending on the vision task. 2) Current approaches to enhance robustness have only marginal effects, and can even reduce robustness. 3) We do not observe significant differences between convolutional and transformer architectures. We believe our dataset provides a rich testbed to study robustness and will help push forward research in this area.\""
  },
  "eccv2022_main_facialdepthandnormalestimationusingsingledual-pixelcamera": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Facial Depth and Normal Estimation Using Single Dual-Pixel Camera",
    "authors": [
      "Minjun Kang",
      "Jaesung Choe",
      "Hyowon Ha",
      "Hae-Gon Jeon",
      "Sunghoon Im",
      "In So Kweon",
      "Kuk-Jin Yoon"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4619_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680176.pdf",
    "published": "2020-08",
    "summary": "\"Recently, Dual-Pixel (DP) sensors have been adopted in many imaging devices. However, despite their various advantages, DP sensors are used just for faster auto-focus and aesthetic image captures, and research on their usage for 3D facial understanding has been limited due to the lack of datasets and algorithmic designs that exploit parallax in DP images. It is also because the baseline of sub-aperture images is extremely narrow, and parallax exists in the defocus blur region. In this paper, we introduce a DP-oriented Depth/Normal estimation network that reconstructs the 3D facial geometry. In addition, to train the network, we collect DP facial data with more than 135K images for 101 persons captured with our multi-camera structured light systems. It contains ground-truth 3D facial models including depth map and surface normal in metric scale. Our dataset allows the proposed network to be generalized for 3D facial depth/normal estimation. The proposed network consists of two novel modules: Adaptive Sampling Module (ASM) and Adaptive Normal Module (ANM), which are specialized in handling the defocus blur in DP images. Finally, we demonstrate that the proposed method achieves state-of-the-art performances over recent DP-based depth/normal estimation methods.\""
  },
  "eccv2022_main_theanatomyofvideoeditingadatasetandbenchmarksuiteforai-assistedvideoediting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing",
    "authors": [
      "Dawit Mureja Argaw",
      "Fabian Caba",
      "Joon-Young Lee",
      "Markus Woodson",
      "In So Kweon"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4736_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680195.pdf",
    "published": "2020-08",
    "summary": "\"Machine learning is transforming the video editing industry. Recent advances in computer vision have leveled-up video editing tasks such as intelligent reframing, rotoscoping, color grading, or applying digital makeups. However, most of the solutions have focused on video manipulation and VFX. This work introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster research in AI-assisted video editing. Our benchmark suite focuses on video editing tasks, beyond visual effects, such as automatic footage organization and assisted video assembling. To enable research on these fronts, we annotate more than 1.5M tags, with relevant concepts to cinematography, from 196176 shots sampled from movie scenes. We establish competitive baseline methods and detailed analyses for each of the tasks. We hope our work sparks innovative research towards underexplored areas of AI-assisted video editing.\""
  },
  "eccv2022_main_stylebabelartisticstyletaggingandcaptioning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "StyleBabel: Artistic Style Tagging and Captioning",
    "authors": [
      "Dan Ruta",
      "Andrew Gilbert",
      "Pranav Aggarwal",
      "Naveen Marri",
      "Ajinkya Kale",
      "Jo Briggs",
      "Chris Speed",
      "Hailin Jin",
      "Baldo Faieta",
      "Alex Filipkowski",
      "Zhe Lin",
      "John Collomosse"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4976_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680212.pdf",
    "published": "2020-08",
    "summary": "\"We present StyleBabel, a unique open access dataset of natural language captions and free-form tags describing the artistic style of over 135K digital artworks, collected via a novel participatory method from experts studying at specialist art and design schools. StyleBabel was collected via an iterative method, inspired by \u2018Grounded Theory\u2019: a qualitative approach that enables annotation while co-evolving a shared language for fine-grained artistic style attribute description. We demonstrate several downstream tasks for StyleBabel, adapting the recent ALADIN architecture for fine-grained style similarity, to train cross-modal embeddings for: 1) free-form tag generation; 2) natural language description of artistic style; 3) fine-grained text search of style. To do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and cross-modal representation learning, achieving a state of the art accuracy in fine-grained style retrieval.\""
  },
  "eccv2022_main_pandoraapanoramicdetectiondatasetforobjectwithorientation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PANDORA: A Panoramic Detection Dataset for Object with Orientation",
    "authors": [
      "Hang Xu",
      "Qiang Zhao",
      "Yike Ma",
      "Xiaodong Li",
      "Peng Yuan",
      "Bailan Feng",
      "Chenggang Yan",
      "Feng Dai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5026_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680229.pdf",
    "published": "2020-08",
    "summary": "\"Panoramic images have become increasingly popular as omnidirectional panoramic technology has advanced. Many datasets and works resort to object detection to better understand the content of the panoramic image. These datasets and detectors use a Bounding Field of View (BFoV) as a bounding box in panoramic images. However, we observe that the object instances in panoramic images often appear with arbitrary orientations. It indicates that BFoV as a bounding box is inappropriate, limiting the performance of detectors. This paper proposes a new bounding box representation, Rotated Bounding Field of View (RBFoV), for the panoramic image object detection task. Then, based on the RBFoV, we present a PANoramic Detection dataset for Object with oRientAtion (PANDORA). Finally, based on PANDORA, we evaluate the current state-of-the-art panoramic image object detection methods and design an anchor-free object detector called R-CenterNet for panoramic images. Compared with these baselines, our R-CenterNet shows its advantages in terms of detection performance. Our PANDORA dataset and source code are available at https://github.com/tdsuper/SphericalObjectDetection.\""
  },
  "eccv2022_main_fs-cocotowardsunderstandingoffreehandsketchesofcommonobjectsincontext": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context",
    "authors": [
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Ayan Kumar Bhunia",
      "Tao Xiang",
      "Yulia Gryaditskaya",
      "Yi-Zhe Song"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5251_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680245.pdf",
    "published": "2020-08",
    "summary": "\"We advance sketch research to scenes with the first dataset of freehand scene sketches, FSCOCO. With practical applications in mind, we collect sketches that convey well scene content but can be sketched within a few minutes by a person with any sketching skills. Our dataset comprises 10,000 freehand scene vector sketches with per point space-time information by 100 non-expert individuals, offering both object- and scene-level abstraction. Each sketch is augmented with its text description. Using our dataset, we study for the first time the problem of fine-grained image retrieval from freehand scene sketches and sketch captions. We draw insights on: (i) Scene salience encoded in sketches using the strokes temporal order; (ii) Performance comparison of image retrieval from a scene sketch and an image caption; (iii) Complementarity of information in sketches and image captions, as well as the potential benefit of combining the two modalities. In addition, we extend a popular vector sketch LSTM-based encoder to handle sketches with larger complexity than was supported by previous work. Namely, we propose a hierarchical sketch decoder, which we leverage at a sketch-specific \u201cpretext\u201d task. Our dataset enables for the first time research on freehand scene sketch understanding and its practical applications. We will release the dataset upon acceptance.\""
  },
  "eccv2022_main_exploringfine-grainedaudiovisualcategorizationwiththessw60dataset": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset",
    "authors": [
      "Grant Van Horn",
      "Rui Qian",
      "Kimberly Wilber",
      "Hartwig Adam",
      "Oisin Mac Aodha",
      "Serge Belongie"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5265_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680262.pdf",
    "published": "2020-08",
    "summary": "\"We present a new benchmark dataset, Sapsucker Woods 60 (SSW60), for advancing research on audiovisual fine-grained categorization. While our community has made great strides in fine-grained visual categorization on images, the counterparts in audio and video fine-grained categorization are relatively unexplored. To encourage advancements in this space, we have carefully constructed the SSW60 dataset to enable researchers to experiment with classifying the same set of categories in three different modalities: images, audio, and video. The dataset covers 60 species of birds and is comprised of images from existing datasets, and brand new, expert curated audio and video datasets. We thoroughly benchmark audiovisual classification performance and modality fusion experiments through the use of state-of-the-art transformer methods. Our findings show that performance of audiovisual fusion methods is better than using exclusively image or audio based methods for the task of video classification. We also present interesting modality transfer experiments, enabled by the unique construction of SSW60 to encompass three different modalities. We hope the SSW60 dataset and accompanying baselines spur research in this fascinating area.\""
  },
  "eccv2022_main_thecaltechfishcountingdatasetabenchmarkformultiple-objecttrackingandcounting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "The Caltech Fish Counting Dataset: A Benchmark for Multiple-Object Tracking and Counting",
    "authors": [
      "Justin Kay",
      "Peter Kulits",
      "Suzanne Stathatos",
      "Siqi Deng",
      "Erik Young",
      "Sara Beery",
      "Grant Van Horn",
      "Pietro Perona"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5272_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680281.pdf",
    "published": "2020-08",
    "summary": "\"We present the Caltech Fish Counting Dataset (CFC), a large-scale dataset for detecting, tracking, and counting fish in sonar videos. We identify sonar videos as a rich source of data for advancing low signal-to-noise computer vision applications and tackling domain generalization in multiple-object tracking (MOT) and counting. In comparison to existing MOT and counting datasets, which are largely restricted to videos of people and vehicles in cities, CFC is sourced from a natural-world domain where targets are not easily resolvable and appearance features cannot be easily leveraged for target re-identification. With over half a million annotations in over 1,500 videos sourced from seven different sonar cameras, CFC allows researchers to train MOT and counting algorithms and evaluate generalization performance at unseen test locations. We perform extensive baseline experiments and identify key challenges and opportunities for advancing the state of the art in generalization in MOT and counting.\""
  },
  "eccv2022_main_adatasetforinteractivevision-languagenavigationwithunknowncommandfeasibility": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility",
    "authors": [
      "Andrea Burns",
      "Deniz Arsan",
      "Sanjna Agrawal",
      "Ranjitha Kumar",
      "Kate Saenko",
      "Bryan A. Plummer"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5327_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680304.pdf",
    "published": "2020-08",
    "summary": "\"Vision-language navigation (VLN), in which an agent follows language instruction in a visual environment, has been studied under the premise that the input command is fully feasible in the environment. Yet in practice, a request may not be possible due to language ambiguity or environment changes. To study VLN with unknown command feasibility, we introduce a new dataset Mobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete a natural language command in a mobile app. Mobile apps provide a scalable domain to study real downstream uses of VLN methods. Moreover, mobile app commands provide instruction for interactive navigation, as they result in action sequences with state changes via clicking, typing, or swiping. MoTIF is the first to include feasibility annotations, containing both binary feasibility labels and fine-grained labels for why tasks are unsatisfiable. We further collect follow-up questions for ambiguous queries to enable research on task uncertainty resolution. Equipped with our dataset, we propose the new problem of feasibility prediction, in which a natural language instruction and multimodal app environment are used to predict instruction feasibility. MoTIF provides a more realistic app dataset as it contains many diverse environments, high-level goals, and longer action sequences than prior work. We evaluate interactive VLN methods using MoTIF, quantify the generalization ability of current approaches to new app environments, and measure the effect of task feasibility on navigation performance.\""
  },
  "eccv2022_main_bracethebreakdancingcompetitiondatasetfordancemotionsynthesis": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis",
    "authors": [
      "Davide Moltisanti",
      "Jinyi Wu",
      "Bo Dai",
      "Chen Change Loy"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5564_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680321.pdf",
    "published": "2020-08",
    "summary": "\"Generative models for audio-conditioned dance motion synthesis map music features to dance movements. Models are trained to associate motion patterns to audio patterns, usually without an explicit knowledge of the human body. This approach relies on a few assumptions: strong music-dance correlation, controlled motion data and relatively simple poses and movements. These characteristics are found in all existing datasets for dance motion synthesis, and indeed recent methods can achieve good results. We introduce a new dataset aiming to challenge these common assumptions, compiling a set of dynamic dance sequences displaying complex human poses. We focus on breakdancing which features acrobatic moves and tangled postures. We source our data from the Red Bull BC One competition videos. Estimating human keypoints from these videos is difficult due to the complexity of the dance, as well as the multiple moving cameras recording setup. We adopt a hybrid labelling pipeline leveraging deep estimation models as well as manual annotations to obtain good quality keypoint sequences at a reduced cost. Our efforts produced the BRACE dataset, which contains over 3 hours and 30 minutes of densely annotated poses. We test state-of-the-art methods on BRACE, showing their limitations when evaluated on complex sequences. Our dataset can readily foster advance in dance motion synthesis. With intricate poses and swift movements, models are forced to go beyond learning a mapping between modalities and reason more effectively about body structure and movements.\""
  },
  "eccv2022_main_dresscodehigh-resolutionmulti-categoryvirtualtry-on": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Dress Code: High-Resolution Multi-Category Virtual Try-On",
    "authors": [
      "Davide Morelli",
      "Matteo Fincato",
      "Marcella Cornia",
      "Federico Landi",
      "Fabio Cesari",
      "Rita Cucchiara"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5660_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680337.pdf",
    "published": "2020-08",
    "summary": "\"Image-based virtual try-on strives to transfer the appearance of a clothing item onto the image of a target person. Prior work focuses mainly on upper-body clothes (e.g. t-shirts, shirts, and tops) and neglects full-body or lower-body items. This shortcoming arises from a main factor: current publicly available datasets for image-based virtual try-on do not account for this variety, thus limiting progress in the field. To address this deficiency, we introduce Dress Code, which contains images of multi-category clothes. Dress Code is more than 3x larger than publicly available datasets for image-based virtual try-on and features high-resolution paired images (1024x768) with front-view, full-body reference models. To generate HD try-on images with high visual quality and rich in details, we propose to learn fine-grained discriminating features. Specifically, we leverage a semantic-aware discriminator that makes predictions at pixel-level instead of image- or patch-level. Extensive experimental evaluation demonstrates that the proposed approach surpasses the baselines and state-of-the-art competitors in terms of visual quality and quantitative results. The Dress Code dataset is publicly available at https://github.com/aimagelab/dress-code.\""
  },
  "eccv2022_main_adata-centricapproachforimprovingambiguouslabelswithcombinedsemi-supervisedclassificationandclustering": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Data-Centric Approach for Improving Ambiguous Labels with Combined Semi-Supervised Classification and Clustering",
    "authors": [
      "Lars Schmarje",
      "Monty Santarossa",
      "Simon-Martin Schr\u00f6der",
      "Claudius Zelenka",
      "Rainer Kiko",
      "Jenny Stracke",
      "Nina Volkmann",
      "Reinhard Koch"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5740_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680354.pdf",
    "published": "2020-08",
    "summary": "\"Consistently high data quality is essential for the development of novel loss functions and architectures in the field of deep learning. The existence of such data and labels is usually presumed, while acquiring high-quality datasets is still a major issue in many cases. Subjective annotations by annotators often lead to ambiguous labels in real-world datasets. We propose a data-centric approach to relabel such ambiguous labels instead of implementing the handling of this issue in a neural network. A hard classification is by definition not enough to capture the real-world ambiguity of the data. Therefore, we propose our method \"\"Data-Centric Classification & Clustering (DC3)\"\" which combines semi-supervised classification and clustering. It automatically estimates the ambiguity of an image and performs a classification or clustering depending on that ambiguity. DC3 is general in nature so that it can be used in addition to many Semi-Supervised Learning (SSL) algorithms. On average, our approach yields a 7.6% better F1-Score for classifications and a 7.9% lower inner distance of clusters across multiple evaluated SSL algorithms and datasets. Most importantly, we give a proof-of-concept that the classifications and clusterings from DC3 are beneficial as proposals for the manual refinement of such ambiguous labels. Overall, a combination of SSL with our method DC3 can lead to better handling of ambiguous labels during the annotation process.\""
  },
  "eccv2022_main_clearposelarge-scaletransparentobjectdatasetandbenchmark": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "ClearPose: Large-Scale Transparent Object Dataset and Benchmark",
    "authors": [
      "Xiaotong Chen",
      "Huijie Zhang",
      "Zeren Yu",
      "Anthony Opipari",
      "Odest Chadwicke Jenkins"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5905_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680372.pdf",
    "published": "2020-08",
    "summary": "\"Transparent objects are ubiquitous in household settings and pose distinct challenges for visual sensing and perception systems. The optical properties of transparent objects leaves conventional 3D sensors alone unreliable for object depth and pose estimation. These challenges are highlighted by the shortage of large-scale RGB-Depth datasets focusing on transparent objects in real-world settings. In this work, we contribute a large-scale real-world RGB-Depth transparent object dataset named ClearPose to serve as a benchmark dataset for segmentation, scene-level depth completion, and object-centric pose estimation tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth frames and 4M instance annotations covering 63 household objects. The dataset includes object categories commonly used in daily life under various lighting and occluding conditions as well as challenging test scenarios such as cases of occlusion by opaque or translucent objects, non-planar orientations, presence of liquids, etc. We benchmark several state-of-the-art depth completion and object pose estimation deep neural networks on ClearPose.\""
  },
  "eccv2022_main_whendeepclassifiersagreeanalyzingcorrelationsbetweenlearningorderandimagestatistics": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics",
    "authors": [
      "Iuliia Pliushch",
      "Martin Mundt",
      "Nicolas Lupp",
      "Visvanathan Ramesh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6000_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680388.pdf",
    "published": "2020-08",
    "summary": "\"Although a plethora of architectural variants for deep classification has been introduced over time, recent works have found empirical evidence towards similarities in their training process. It has been hypothesized that neural networks converge not only to similar representations, but also exhibit a notion of empirical agreement on which data instances are learned first. Following in the latter works\u2019 footsteps, we define a metric to quantify the relationship between such classification agreement over time, and posit that the agreement phenomenon can be mapped to core statistics of the investigated dataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal, ImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to be independent of specific architectures, training hyper-parameters or labels, albeit follows an ordering according to image statistics.\""
  },
  "eccv2022_main_animeceleblarge-scaleanimationcelebheadsdatasetforheadreenactment": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head Reenactment",
    "authors": [
      "Kangyeol Kim",
      "Sunghyun Park",
      "Jaeseong Lee",
      "Sunghyo Chung",
      "Junsoo Lee",
      "Jaegul Choo"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6068_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680405.pdf",
    "published": "2020-08",
    "summary": "\"We present a novel Animation CelebHeads dataset (AnimeCeleb) to address an animation head reenactment. Different from previous animation head datasets, we utilize a 3D animation models as the controllable image samplers, which can provide a large amount of head images with their corresponding detailed pose annotations. To facilitate a data creation process, we build a semi-automatic pipeline leveraging an open 3D computer graphics software with a developed annotation system. After training with the AnimeCeleb, recent head reenactment models produce high-quality animation head reenactment results, which are not achievable with existing datasets. Furthermore, motivated by metaverse application, we propose a novel pose mapping method and architecture to tackle a cross-domain head reenactment task. During inference, a user can easily transfer one\u2019s motion to an arbitrary animation head. Experiments demonstrate an usefulness of the AnimeCeleb to train animation head reenactment models, and the superiority of our cross-domain head reenactment model compared to state-of-the-art methods. Our dataset and code are available at \\href{https://github.com/kangyeolk/AnimeCeleb}{\\textit{this url}}.\""
  },
  "eccv2022_main_mugenaplaygroundforvideo-audio-textmultimodalunderstandingandgeneration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration",
    "authors": [
      "Thomas Hayes",
      "Songyang Zhang",
      "Xi Yin",
      "Guan Pang",
      "Sasha Sheng",
      "Harry Yang",
      "Songwei Ge",
      "Qiyuan Hu",
      "Devi Parikh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6199_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680421.pdf",
    "published": "2020-08",
    "summary": "\"Multimodal video-audio-text understanding and generation can benefit from datasets that are narrow but rich. The narrowness allows bite-sized challenges that the research community can make progress on. The richness ensures we are making progress along the core challenges. To this end, we present a large-scale video-audio-text dataset MUGEN, collected using the open-sourced platform game CoinRun. We made substantial modifications to make the game richer by introducing audio and enabling new interactions. We trained RL agents with different objectives to navigate the game and interact with 13 objects and characters. This allows us to automatically extract a large collection of diverse videos and associated audio. We sample 375K video clips (3.2s each) and collect text descriptions from human annotators. Each video has additional annotations that are extracted automatically from the game engine, such as accurate semantic maps for each frame and templated textual descriptions. Altogether, MUGEN can help progress research in many tasks in multimodal understanding and generation. We benchmark representative approaches on tasks involving video-audio-text retrieval and generation. Our dataset and code are released at: https://mugen-org.github.io/.\""
  },
  "eccv2022_main_adensematerialsegmentationdatasetforindoorandoutdoorsceneparsing": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing",
    "authors": [
      "Paul Upchurch",
      "Ransen Niu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6243_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680440.pdf",
    "published": "2020-08",
    "summary": "\"A key algorithm for understanding the world is material segmentation, which assigns a label (metal, glass, etc.) to each pixel. We find that a model trained on existing data underperforms in some settings and propose to address this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor and outdoor images, which is 23x more segments than existing data. Our data covers a more diverse set of scenes, objects, viewpoints and materials, and contains a more fair distribution of skin types. We show that a model trained on our data outperforms a state-of-the-art model across datasets and viewpoints. We propose a large-scale scene parsing benchmark and baseline of 0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across 46 materials.\""
  },
  "eccv2022_main_mimicmealargescalediverse4ddatabaseforfacialexpressionanalysis": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MimicME: A Large Scale Diverse 4D Database for Facial Expression Analysis",
    "authors": [
      "Athanasios Papaioannou",
      "Baris Gecer",
      "Shiyang Cheng",
      "Grigorios G. Chrysos",
      "Jiankang Deng",
      "Eftychia Fotiadou",
      "Christos Kampouris",
      "Dimitrios Kollias",
      "Stylianos Moschoglou",
      "Kritaphat Songsri-In",
      "Stylianos Ploumpis",
      "George Trigeorgis",
      "Panagiotis Tzirakis",
      "Evangelos Ververas",
      "Yuxiang Zhou",
      "Allan Ponniah",
      "Anastasios Roussos",
      "Stefanos Zafeiriou"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6439_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680457.pdf",
    "published": "2020-08",
    "summary": "\"Recently, Deep Neural Networks (DNNs) have been shown to outperform traditional methods in many disciplines such as computer vision, speech recognition and natural language processing. A prerequisite for the successful application of DNNs is the big number of data. Even though various facial datasets exist for the case of 2D images, there is a remarkable absence of datasets when we have to deal with 3D faces. The available facial datasets are limited either in terms of expressions or in the number of subjects. This lack of large datasets hinders the exploitation of the great advances that DNNs can provide. In this paper, we overcome these limitations by introducing MimicMe, a novel large-scale database of dynamic high-resolution 3D faces. MimicMe contains recordings of 4,700 subjects with a great diversity on age, gender and ethnicity. The recordings are in the form of 4D videos of subjects displaying a multitude of facial behaviours, resulting to over 280,000 3D meshes in total. We have also manually annotated a big portion of these meshes with 3D facial landmarks and they have been categorized in the corresponding expressions. We have also built very powerful blendshapes for parametrising facial behaviour. MimicMe will be made publicly available upon publication and we envision that it will be extremely valuable to researchers working in many problems of face modelling and analysis, including 3D/4D face and facial expression recognition. We conduct several experiments and demonstrate the usefulness of the database for various applications.\""
  },
  "eccv2022_main_delvingintouniversallesionsegmentationmethod,dataset,andbenchmark": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"Delving into Universal Lesion Segmentation: Method, Dataset, and Benchmark\"",
    "authors": [
      "Yu Qiu",
      "Jing Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6472_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680475.pdf",
    "published": "2020-08",
    "summary": "\"Most efforts on lesion segmentation from CT slices focus on one specific lesion type. However, universal and multi-category lesion segmentation is more important because the diagnoses of different body parts are usually correlated and carried out simultaneously. The existing universal lesion segmentation methods are weakly-supervised due to the lack of pixel-level annotation data. To bring this field into the fully-supervised era, we establish a large-scale universal lesion segmentation dataset, SegLesion. We also propose a baseline method for this task. Considering that it is easy to encode CT slices owing to the limited CT scenarios, we propose a Knowledge Embedding Module (KEM) to adapt the concept of dictionary learning for this task. Specifically, KEM first learns the knowledge encoding of CT slices and then embeds the learned knowledge encoding into the deep features of a CT slice to increase the distinguishability. With KEM incorporated, a Knowledge Embedding Network (KEN) is designed for universal lesion segmentation. To extensively compare KEN to previous segmentation methods, we build a large benchmark for SegLesion. KEN achieves state-of-the-art performance and can thus serve as a strong baseline for future research. Data and code will be released.\""
  },
  "eccv2022_main_largescalereal-worldmulti-persontracking": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Large Scale Real-World Multi-person Tracking",
    "authors": [
      "Bing Shuai",
      "Alessandro Bergamo",
      "Uta B\u00fcchler",
      "Andrew Berneshawi",
      "Alyssa Boden",
      "Joseph Tighe"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6479_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680493.pdf",
    "published": "2020-08",
    "summary": "\"This paper presents a new large scale multi-person tracking dataset. Our dataset is over an order of magnitude larger than currently available high quality multi-object tracking datasets such as MOT17, HiEve, and MOT20 datasets. The lack of large scale training and test data for this task has limited the community\u2019s ability to understand the performance of their tracking systems on a wide range of scenarios and conditions such as variations in person density, actions being performed, weather, and time of day. Our dataset was specifically sourced to provide a wide variety of these conditions and our annotations include rich meta-data such that the performance of a tracker can be evaluated along these different dimensions. The lack of training data has also limited the ability to perform end-to-end training of tracking systems. As such, the highest performing tracking systems all rely on strong detectors trained on external image datasets. We hope that the release of this dataset will enable new lines of research that take advantage of large scale video based training data.\""
  },
  "eccv2022_main_d2-tpreddiscontinuousdependencyfortrajectorypredictionundertrafficlights": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights",
    "authors": [
      "Yuzhen Zhang",
      "Wentong Wang",
      "Weizhi Guo",
      "Pei Lv",
      "Mingliang Xu",
      "Wei Chen",
      "Dinesh Manocha"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6862_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680512.pdf",
    "published": "2020-08",
    "summary": "\"A profound understanding of inter-agent relationships and motion behaviors is important to achieve high-quality planning when navigating in complex scenarios, especially at urban traffic intersections. We present a trajectory prediction approach with respect to traffic lights, D2-TPred, which uses a spatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG) to handle the problem of discontinuous dependency in the spatial-temporal space. Specifically, the SDG is used to capture spatial interactions by reconstructing sub-graphs for different agents with dynamic and changeable characteristics during each frame. The BDG is used to infer motion tendency by modeling the implicit dependency of the current state on priors behaviors, especially the discontinuous motions corresponding to acceleration, deceleration, or turning direction. Moreover, we present a new dataset for vehicle trajectory prediction under traffic lights called VTP-TL. Our experimental results show that our model achieves more than {20.45\\% and 20.78\\% }improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to other trajectory prediction algorithms. We will release all of the source code, dataset, and the trained model at the time of publication.\""
  },
  "eccv2022_main_themissinglinkfindinglabelrelationsacrossdatasets": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "The Missing Link: Finding Label Relations across Datasets",
    "authors": [
      "Jasper Uijlings",
      "Thomas Mensink",
      "Vittorio Ferrari"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7043_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680530.pdf",
    "published": "2020-08",
    "summary": "\"Computer Vision is driven by the many datasets which can be used for training or evaluating novel methods. Each of these dataset, however, has its own design principles resulting in a different set of labels,different appearance domains and different annotation instructions. In this paper we explore the automatic discovery of visual-semantic relations between labels across datasets. We want to understand how the instances with label a in dataset A relate to the instances with label b in dataset B,are they in an identity, parent/child, or overlap relation? Or is there no visual link between these two? To find relations between labels across datasets,we propose methods based on language, on vision, and on a combination of both. In order to evaluate these we establish ground-truth relations between three datasets: COCO, ADE20k, and Berkeley Deep Drive. Our methods can effectively discover label relations across datasets and the type of the relations. We use these results for a deeper inspection on why instances relate, find missing aspects, and use our relations to create finer-grained annotations. We conclude that label relations cannot be established by looking at the label-name semantics alone, the relations depend highly on how each of the individual datasets was constructed.\""
  },
  "eccv2022_main_learningomnidirectionalflowin360\u00b0videoviasiameserepresentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning Omnidirectional Flow in 360\u00b0 Video via Siamese Representation",
    "authors": [
      "Keshav Bhandari",
      "Bin Duan",
      "Gaowen Liu",
      "Hugo Latapie",
      "Ziliang Zong",
      "Yan Yan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7090_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680546.pdf",
    "published": "2020-08",
    "summary": "\"Optical flow estimation in omnidirectional videos faces two significant issues: the lack of benchmark datasets and the challenge of adapting perspective video-based methods to accommodate the omnidirectional nature. This paper proposes the first perceptually natural-synthetic omnidirectional benchmark dataset with a 360\u00c2\u00b0 field of view, FLOW360, with 40 different videos and 4,000 video frames. We conduct comprehensive characteristic analysis and comparisons between our dataset and existing optical flow datasets, which manifest perceptual realism, uniqueness, and diversity. To accommodate the omnidirectional nature, we present a novel Siamese representation Learning framework for Omnidirectional Flow (SLOF). We train our network in a contrastive manner with a hybrid loss function that combines contrastive loss and optical flow loss. Extensive experiments verify the proposed framework\u2019s effectiveness and show up to 40% performance improvement over the state-of-the-art approaches. Our FLOW360 dataset and code are available at https://siamlof.github.io/.\""
  },
  "eccv2022_main_vizwiz-fewshotlocatingobjectsinimagestakenbypeoplewithvisualimpairments": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "VizWiz-FewShot: Locating Objects in Images Taken by People with Visual Impairments",
    "authors": [
      "Yu-Yun Tseng",
      "Alexander Bell",
      "Danna Gurari"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7450_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680563.pdf",
    "published": "2020-08",
    "summary": "\"We introduce a few-shot localization dataset originating from photographers who authentically were trying to learn about the visual content in the images they took. It includes over 8,000 segmentations of 100 categories in over 4,000 images that were taken by people with visual impairments. Compared to existing few-shot object detection and instance segmentation datasets, our dataset is the first to locate holes in objects (e.g., found in 12.4% of our segmentations), it shows objects that occupy a much larger range of sizes relative to the images, and text is over five times more common in our objects (e.g., found in 24.7% of our segmentations). Analysis of two modern few-shot localization algorithms demonstrates that they generalize poorly to our new dataset. The algorithms commonly struggle to locate objects with holes, very small and very large objects, and objects lacking text. To encourage a larger community to work on these unsolved challenges, we publicly share our annotated few-shot dataset at http://anonymous.\""
  },
  "eccv2022_main_trovetransformingroadscenedatasetsintophotorealisticvirtualenvironments": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "TRoVE: Transforming Road Scene Datasets into Photorealistic Virtual Environments",
    "authors": [
      "Shubham Dokania",
      "Anbumani Subramanian",
      "Manmohan Chandraker",
      "C.V. Jawahar"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7561_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680579.pdf",
    "published": "2020-08",
    "summary": "\"High-quality structured data with rich annotations are critical components in intelligent vehicle systems dealing with road scenes. However, data curation and annotation require intensive investments and yield low-diversity scenarios. The recently growing interest in synthetic data raises questions about the scope of improvement in such systems and the amount of manual work still required to produce high volumes and variations of simulated data. This work proposes a synthetic data generation pipeline that utilizes existing datasets, like nuScenes, to address the difficulties and domain-gaps present in simulated datasets. We show that using annotations and visual cues from existing datasets, we can facilitate automated multi-modal data generation, mimicking real scene properties with high-fidelity, along with mechanisms to diversify samples in a physically meaningful way. We demonstrate improvements in mIoU metrics by presenting qualitative and quantitative experiments with real and synthetic data for semantic segmentation on the Cityscapes and KITTI-STEP datasets. All relevant code and data is released on github.\""
  },
  "eccv2022_main_trappedintexturebias?alargescalecomparisonofdeepinstancesegmentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Trapped in Texture Bias? A Large Scale Comparison of Deep Instance Segmentation",
    "authors": [
      "Johannes Theodoridis",
      "Jessica Hofmann",
      "Johannes Maucher",
      "Andreas Schilling"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7815_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680597.pdf",
    "published": "2020-08",
    "summary": "\"Do deep learning models for instance segmentation generalize to novel objects in a systematic way? For classification, such behavior has been questioned. In this study, we aim to understand if certain design decisions such as framework, architecture or pre-training contribute to the semantic understanding of instance segmentation. To answer this question, we consider a special case of robustness and compare pre-trained models on a challenging benchmark for object-centric out-of-distribution texture. We do not introduce another method in this work. Instead, we take a step back and evaluate a broad range of existing literature. This includes Cascade and Mask R-CNN, Swin Transformer, BMask, YOLACT(++), DETR, BCNet, SOTR and SOLOv2. We find that YOLACT++, SOTR and SOLOv2 are significantly more robust to out-of-distribution texture than other frameworks. In addition, we show that deeper and dynamic architectures improve robustness whereas training schedules, data augmentation and pre-training have only a minor impact. In summary we evaluate 68 models on 61 versions of MS COCO for a total of 4148 evaluations.\""
  },
  "eccv2022_main_deformablefeatureaggregationfordynamicmulti-modal3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Deformable Feature Aggregation for Dynamic Multi-modal 3D Object Detection",
    "authors": [
      "Zehui Chen",
      "Zhenyu Li",
      "Shiquan Zhang",
      "Liangji Fang",
      "Qinhong Jiang",
      "Feng Zhao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/132_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680616.pdf",
    "published": "2020-08",
    "summary": "\"Point clouds and RGB images are two general perceptional sources in autonomous driving. The former can provide accurate localization of objects, and the latter is denser and richer in semantic information. Recently, AutoAlign presents a learnable paradigm in combining these two modalities for 3D object detection. However, it suffers from high computational cost introduced by the global-wise attention. To solve the problem, we propose Cross-Domain DeformCAFA module in this work. It attends to sparse learnable sampling points for cross-modal relational modeling, which enhances the tolerance to calibration error and greatly speeds up the feature aggregation across different modalities. To overcome the complex GT-AUG under multi-modal settings, we design a simple yet effective cross-modal augmentation strategy on convex combination of image patches given their depth information. Moreover, by carrying out a novel image-level dropout training scheme, our model is able to infer in a dynamic manner. To this end, we propose AutoAlignV2, a faster and stronger multi-modal 3D detection framework, built on top of AutoAlign. Extensive experiments on nuScenes benchmark demonstrate the effectiveness and efficiency of AutoAlignV2. Notably, our best model reaches 72.4 NDS on nuScenes test leaderboard, achieving new state-of-the-art results among all published multi-modal 3D object detectors.\""
  },
  "eccv2022_main_welsalearningtopredict6dposefromweaklylabeleddatausingshapealignment": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "WeLSA: Learning to Predict 6D Pose from Weakly Labeled Data Using Shape Alignment",
    "authors": [
      "Shishir Reddy Vutukur",
      "Ivan Shugurov",
      "Benjamin Busam",
      "Andreas Hutter",
      "Slobodan Ilic"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/184_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680633.pdf",
    "published": "2020-08",
    "summary": "\"Object pose estimation is a crucial task in computer vision and augmented reality. One of its key challenges is the difficulty of annotation of real training data and the lack of textured CAD models. Therefore, pipelines which do not require CAD models and which can be trained with few labeled images are desirable. We propose a weakly-supervised approach for object pose estimation from RGB-D data using training sets composed of very few labeled images with pose annotations along with weakly-labeled images with ground truth segmentation masks without pose labels. We achieve this by learning to annotate weakly-labeled training data through shape alignment while simultaneously training a pose prediction network. Point cloud alignment is performed using structure and rotation-invariant feature-based losses. We further learn an implicit shape representation, which allows the method to work without the known CAD model and also contributes to pose alignment and pose refinement during training on weakly labeled images. The experimental evaluation shows that our method achieves state-of-the-art results on LineMOD, Occlusion-LineMOD and TLess despite being trained using relative poses and on only a fraction of labeled data used by the other methods. We also achieve comparable results to state-of-the-art RGB-D based pose estimation approaches even when further reducing the amount of unlabeled training data. In addition, our method works even if relative camera poses are given instead of object pose annotations which are typically easier to obtain.\""
  },
  "eccv2022_main_graphr-cnntowardsaccurate3dobjectdetectionwithsemantic-decoratedlocalgraph": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Graph R-CNN: Towards Accurate 3D Object Detection with Semantic-Decorated Local Graph",
    "authors": [
      "Honghui Yang",
      "Zili Liu",
      "Xiaopei Wu",
      "Wenxiao Wang",
      "Wei Qian",
      "Xiaofei He",
      "Deng Cai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/193_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680650.pdf",
    "published": "2020-08",
    "summary": "\"Two-stage detectors have gained much popularity in 3D object detection. Most two-stage 3D detectors utilize grid points, voxel grids, or sampled keypoints for RoI feature extraction in the second stage. Such methods, however, are inefficient in handling unevenly distributed and sparse outdoor points. This paper solves this problem in three aspects. 1) Dynamic Point Aggregation. We propose the patch search to quickly search points in a local region for each 3D proposal. The dynamic farthest voxel sampling is then applied to evenly sample the points. Especially, the voxel size varies along the distance to accommodate the uneven distribution of points. 2) RoI-graph Pooling. We build local graphs on the sampled points to better model contextual information and mine point relations through iterative message passing. 3) Visual Features Augmentation. We introduce a simple yet effective fusion strategy to compensate for sparse LiDAR points with limited semantic cues. Based on these modules, we construct our Graph R-CNN as the second stage, which can be applied to existing one-stage detectors to consistently improve the detection performance. Extensive experiments show that Graph R-CNN outperforms the state-of-the-art 3D detection models by a large margin on both the KITTI and Waymo Open Dataset. And we rank first place on the KITTI BEV car detection leaderboard.\""
  },
  "eccv2022_main_mppnetmulti-framefeatureintertwiningwithproxypointsfor3dtemporalobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MPPNet: Multi-Frame Feature Intertwining with Proxy Points for 3D Temporal Object Detection",
    "authors": [
      "Xuesong Chen",
      "Shaoshuai Shi",
      "Benjin Zhu",
      "Ka Chun Cheung",
      "Hang Xu",
      "Hongsheng Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/368_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680667.pdf",
    "published": "2020-08",
    "summary": "\"Accurate and reliable 3D detection is vital for many applications including autonomous driving vehicles and service robots. In this paper, we present a flexible and high-performance 3D detection frame-work, named MPPNet, for 3D temporal object detection with point cloud sequences. We propose a novel three-hierarchy framework with proxy points for multi-frame feature encoding and interactions to achieve better detection. The three hierarchies conduct per-frame feature encoding, short-clip feature fusion, and whole-sequence feature aggregation, respectively. To enable processing long-sequence point clouds with reasonable computational resources, intra-group feature mixing and inter-group feature attention are proposed to form the second and third feature encoding hierarchies, which are recurrently applied for aggregating multi-frame trajectory features. The proxy points not only act as consistent object representations for each frame, but also serves as the courier to facilitate feature interaction between frames. The experiments on large Waymo Open dataset show that our approach outperforms state-of-the-art methods with large margins when applied to both short (e.g., 4-frame) and long (e.g., 16-frame) point cloud sequences. Code will be publicly available at https://github.com/open-mmlab/OpenPCDet.\""
  },
  "eccv2022_main_long-taildetectionwitheffectiveclass-margins": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Long-Tail Detection with Effective Class-Margins",
    "authors": [
      "Jang Hyun Cho",
      "Philipp Kr\u00e4henb\u00fchl"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/640_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680684.pdf",
    "published": "2020-08",
    "summary": "\"Large-scale object detection and instance segmentation faces a severe data imbalance. The finer-grained object classes become, the less frequent they appear in our datasets. However at test-time, we expect a detector that performs well for all classes and not just the most frequent ones. In this paper, we provide a theoretical understanding of the long-trail detection problem. We show how the commonly used mean average precision evaluation metric on an unknown test-set is bound by a margin-based binary classification error on a long-tailed object-detection training set. We optimize margin-based binary classification error with a novel surrogate objective called Effective Class-Margin Loss (ECM). The ECM loss is simple, theoretically well-motivated, and outperforms other heuristic counterparts on LVIS v1 benchmark over a wide range of architecture and detectors. Code is available at https://github.com/janghyuncho/ECM-Loss.\""
  },
  "eccv2022_main_semi-supervisedmonocular3dobjectdetectionbymulti-viewconsistency": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Semi-Supervised Monocular 3D Object Detection by Multi-View Consistency",
    "authors": [
      "Qing Lian",
      "Yanbo Xu",
      "Weilong Yao",
      "Yingcong Chen",
      "Tong Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/654_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680702.pdf",
    "published": "2020-08",
    "summary": "\"The success of monocular 3D object detection highly relies on considerable labeled data, which is costly to obtain. To alleviate the annotation effort, we propose MVC-MonoDet, the first semi-supervised training framework that improves Monocular 3D object detection by enforcing multi-view consistency. In particular, a box-level regularization and an object-level regularization are designed to enforce the consistency of 3D bounding box predictions of the detection model across unlabeled multi-view data (stereo or video). The box-level regularizer requires the model to consistently estimate 3D boxes in different views so that the model can learn cross-view invariant features for 3D detection. The object-level regularizer employs an object-wise photometric consistency loss that mitigates 3D box estimation error through structure from motion (SFM). A key innovation in our approach to effectively utilize these consistency losses from multi-view data is a novel relative depth module that replaces the standard depth module in vanilla SFM. This technique allows the depth estimation to be coupled with the estimated 3D bounding boxes, so that the derivative of consistency regularization can be used to directly optimize the estimated 3D bounding boxes using unlabeled data. We show that the proposed semi-supervised learning techniques effectively improve the performance of 3D detection on the KITTI and nuScenes datasets. We also demonstrate that the framework is flexible and can be adapted to both stereo and video data.\""
  },
  "eccv2022_main_ptseformerprogressivetemporal-spatialenhancedtransformertowardsvideoobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer towards Video Object Detection",
    "authors": [
      "Han Wang",
      "Jun Tang",
      "Xiaodong Liu",
      "Shanyan Guan",
      "Rong Xie",
      "Li Song"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/690_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680719.pdf",
    "published": "2020-08",
    "summary": "\"Recent years have witnessed a trend of applying context frames to boost the performance of object detection as video object detection. Existing methods usually aggregate features at one stroke to enhance the feature. These methods, however, usually lack spatial information from neighboring frames and suffer from insufficient feature aggregation. To address the issues, we perform a progressive way to introduce both temporal information and spatial information for an integrated enhancement. The temporal information is introduced by the temporal feature aggregation model (TFAM), by conducting an attention mechanism between the context frames and the target frame (i.e., the frame to be detected). Meanwhile, we employ a Spatial Transition Awareness Model (STAM) to convey the location transition information between each context frame and target frame. Built upon a transformer-based detector DETR, our PTSEFormer also follows an end-to-end fashion to avoid heavy post-processing procedures while achieving 88.1% mAP on the ImageNet VID dataset. Codes are available at https://github.com/Hon-Wong/PTSEFormer.\""
  },
  "eccv2022_main_bevformerlearningbirds-eye-viewrepresentationfrommulti-cameraimagesviaspatiotemporaltransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers",
    "authors": [
      "Zhiqi Li",
      "Wenhai Wang",
      "Hongyang Li",
      "Enze Xie",
      "Chonghao Sima",
      "Tong Lu",
      "Yu Qiao",
      "Jifeng Dai"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/694_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690001.pdf",
    "published": "2020-08",
    "summary": "\"3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9% in terms of NDS metric on the nuScenes test set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. We further show that BEVFormer remarkably improves the accuracy of velocity estimation and recall of objects under low visibility conditions. The code will be released at https://github.com/zhiqi-li/BEVFormer.\""
  },
  "eccv2022_main_category-level6dobjectposeandsizeestimationusingself-superviseddeeppriordeformationnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Category-Level 6D Object Pose and Size Estimation Using Self-Supervised Deep Prior Deformation Networks",
    "authors": [
      "Jiehong Lin",
      "Zewei Wei",
      "Changxing Ding",
      "Kui Jia"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/919_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690019.pdf",
    "published": "2020-08",
    "summary": "\"It is difficult to precisely annotate object instances and their semantics in 3D space, and as such, synthetic data are extensively used for these tasks, e.g., category-level 6D object pose and size estimation. However, the easy annotations in synthetic domains bring the downside effect of synthetic-to-real (Sim2Real) domain gap. In this work, we aim to address this issue in the task setting of Sim2Real, unsupervised domain adaptation for category-level 6D object pose and size estimation. We propose a method that is built upon a novel Deep Prior Deformation Network, shortened as DPDN. DPDN learns to deform features of categorical shape priors to match those of object observations, and is thus able to establish deep correspondence in the feature space for direct regression of object poses and sizes. To reduce the Sim2Real domain gap, we formulate a novel self-supervised objective upon DPDN via consistency learning; more specifically, we apply two rigid transformations to each object observation in parallel, and feed them into DPDN respectively to yield dual sets of predictions; on top of the parallel learning, an inter-consistency term is employed to keep cross consistency between dual predictions for improving the sensitivity of DPDN to pose changes, while individual intra-consistency ones are used to enforce self-adaptation within each learning itself. We train DPDN on both training sets of the synthetic CAMERA25 and real-world REAL275 datasets; our results outperform the existing methods on REAL275 test set under both the unsupervised and supervised settings. Ablation studies also verify the efficacy of our designs. Our code is released publicly at https://github.com/JiehongLin/Self-DPDN.\""
  },
  "eccv2022_main_denseteacherdensepseudo-labelsforsemi-supervisedobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Dense Teacher: Dense Pseudo-Labels for Semi-Supervised Object Detection",
    "authors": [
      "Hongyu Zhou",
      "Zheng Ge",
      "Songtao Liu",
      "Weixin Mao",
      "Zeming Li",
      "Haiyan Yu",
      "Jian Sun"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/961_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690036.pdf",
    "published": "2020-08",
    "summary": "\"To date, the most powerful semi-supervised object detectors (SS-OD) are based on pseudo-boxes, which need a sequence of post-processing with fine-tuned hyper-parameters. In this work, we propose replacing the sparse pseudo-boxes with the dense prediction as a united and straightforward form of pseudo-label. Compared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any post-processing method, thus retaining richer information. We also introduce a region selection technique to highlight the key information while suppressing the noise carried by dense labels. We name our proposed SS-OD algorithm that leverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows superior performance under various settings compared with the pseudo-box-based methods. Code will be available.\""
  },
  "eccv2022_main_point-to-boxnetworkforaccurateobjectdetectionviasinglepointsupervision": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Point-to-Box Network for Accurate Object Detection via Single Point Supervision",
    "authors": [
      "Pengfei Chen",
      "Xuehui Yu",
      "Xumeng Han",
      "Najmul Hassan",
      "Kai Wang",
      "Jiachen Li",
      "Jian Zhao",
      "Humphrey Shi",
      "Zhenjun Han",
      "Qixiang Ye"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/986_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690053.pdf",
    "published": "2020-08",
    "summary": "\"Object detection using single point supervision has received increasing attention over the years. However, the performance gap between point supervised object detection (PSOD) and bounding box supervised detection remains large. In this paper, we attribute such a large performance gap to the failure of generating high-quality proposal bags which are crucial for multiple instance learning (MIL). To address this problem, we introduce a lightweight alternative to the off-the-shelf proposal (OTSP) method and thereby create the Point-to-Box Network (P2BNet), which can construct a inter-objects balanced proposal bag by generating proposals in an anchor-like way. By fully investigating the accurate position information, P2BNet further constructs an instance-level bag, avoiding the mixture of multiple objects. Finally, a coarse-to-fine policy in a cascade fashion is utilized to improve the IoU between proposals and ground-truth (GT). Benefiting from these strategies, P2BNet is able to produce high-quality instance-level bags for object detection. P2BNet improves the mean average precision (AP) by more than 50% relative to the previous best PSOD method on the MS COCO dataset. It also demonstrates the great potential to bridge the performance gap between point supervised and bounding-box supervised detectors. The code will be released at github.com/ucas-vg/P2BNet.\""
  },
  "eccv2022_main_domainadaptivehandkeypointandpixellocalizationinthewild": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Domain Adaptive Hand Keypoint and Pixel Localization in the Wild",
    "authors": [
      "Takehiko Ohkawa",
      "Yu-Jhe Li",
      "Qichen Fu",
      "Ryosuke Furuta",
      "Kris M. Kitani",
      "Yoichi Sato"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1100_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690070.pdf",
    "published": "2020-08",
    "summary": "\"We aim to improve the performance of regressing hand keypoints and segmenting pixel-level hand masks under new imaging conditions (e.g., outdoors) when we only have labeled images taken under very different conditions (e.g., indoors). In the real world, it is important that the model trained for both tasks works under various imaging conditions. However, their variation covered by existing labeled hand datasets is limited. Thus, it is necessary to adapt the model trained on the labeled images (source) to unlabeled images (target) with unseen imaging conditions. While self-training domain adaptation methods (i.e., learning from the unlabeled target images in a self-supervised manner) have been developed for both tasks, their training may degrade performance when the predictions on the target images are noisy. To avoid this, it is crucial to assign a low importance (confidence) weight to the noisy predictions during self-training. In this paper, we propose to utilize the divergence of two predictions to estimate the confidence of the target image for both tasks. These predictions are given from two separate networks, and their divergence helps identify the noisy predictions. To integrate our proposed confidence estimation into self-training, we propose a teacher-student framework where the two networks (teachers) provide supervision to a network (student) for self-training, and the teachers are learned from the student by knowledge distillation. Our experiments show its superiority over state-of-the-art methods in adaptation settings with different lighting, grasping objects, backgrounds, and camera viewpoints. Our method improves by 4% the multi-task score on HO3D compared to the latest adversarial adaptation method. We also validate our method on Ego4D, egocentric videos with rapid changes in imaging conditions outdoors.\""
  },
  "eccv2022_main_towardsdata-efficientdetectiontransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards Data-Efficient Detection Transformers",
    "authors": [
      "Wen Wang",
      "Jing Zhang",
      "Yang Cao",
      "Yongliang Shen",
      "Dacheng Tao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1174_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690090.pdf",
    "published": "2020-08",
    "summary": "\"Detection transformers have achieved competitive performance on the sample-rich COCO dataset. However, we show most of them suffer from significant performance drops on small-size datasets, like Cityscapes. In other words, the detection transformers are generally data-hungry. To tackle this problem, we empirically analyze the factors that affect data efficiency, through a step-by-step transition from a data-efficient RCNN variant to the representative DETR. The empirical results suggest that sparse feature sampling from local image areas holds the key. Based on this observation, we alleviate the data-hungry issue of existing detection transformers by simply alternating how key and value sequences are constructed in the cross-attention layer, with minimum modifications to the original models. Besides, we introduce a simple yet effective label augmentation method to provide richer supervision and improve data efficiency. Experiments show that our method can be readily applied to different detection transformers and improve their performance on both small-size and sample-rich datasets.\""
  },
  "eccv2022_main_open-vocabularydetrwithconditionalmatching": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Open-Vocabulary DETR with Conditional Matching",
    "authors": [
      "Yuhang Zang",
      "Wei Li",
      "Kaiyang Zhou",
      "Chen Huang",
      "Chen Change Loy"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1248_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690107.pdf",
    "published": "2020-08",
    "summary": "\"Open-vocabulary object detection, which is concerned with the problem of detecting novel objects guided by natural language, has gained increasing attention from the community. Ideally, we would like to extend an open-vocabulary detector such that it can produce bounding box predictions based on user inputs in form of either natural language or exemplar image. This offers great flexibility and user experience for human-computer interaction. To this end, we propose a novel open-vocabulary detector based on DETR---hence the name OV-DETR---which, once trained, can detect any object given its class name or an exemplar image. The biggest challenge of turning DETR into an open-vocabulary detector is that it is impossible to calculate the classification cost matrix of novel classes without access to their labeled images. To overcome this challenge, we formulate the learning objective as a binary matching one between input queries (class name or exemplar image) and the corresponding objects, which learns useful correspondence to generalize to unseen queries during testing. For training, we choose to condition the Transformer decoder on the input embeddings obtained from a pre-trained vision-language model like CLIP, in order to enable matching for both text and image queries. With extensive experiments on LVIS and COCO datasets, we demonstrate that our OV-DETR---the first end-to-end Transformer-based open-vocabulary detector---achieves non-trivial improvements over current state of the arts.\""
  },
  "eccv2022_main_prediction-guideddistillationfordenseobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Prediction-Guided Distillation for Dense Object Detection",
    "authors": [
      "Chenhongyi Yang",
      "Mateusz Ochal",
      "Amos Storkey",
      "Elliot J. Crowley"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1356_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690123.pdf",
    "published": "2020-08",
    "summary": "\"Real-world object detection models should be cheap and accurate. Knowledge distillation (KD) can boost the accuracy of a small, light detection model by leveraging useful information from a larger teacher model. However, a key challenge is identifying the most informative features produced by the teacher for distillation. In this work, we show that only a very small fraction of features within a ground-truth bounding box are responsible for a teacher\u2019s high detection performance. Based on this, we propose Prediction-Guided Distillation (PGD), which focuses distillation on these key predictive regions of the teacher and yields considerable gains in performance over many existing KD baselines. In addition, we propose an adaptive weighting scheme over the key regions to smooth out their influence and achieve even better performance. Our proposed approach outperforms current state-of-the-art KD baselines on a variety of advanced one-stage detection architectures. Specifically, on the COCO dataset, our method achieves between +3.1% and +4.6% AP improvement using ResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On the CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP, also using these backbones. Our code is available at https://github.com/ChenhongyiYang/PGD.\""
  },
  "eccv2022_main_multimodalobjectdetectionviaprobabilisticensembling": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Multimodal Object Detection via Probabilistic Ensembling",
    "authors": [
      "Yi-Ting Chen",
      "Jinghao Shi",
      "Zelin Ye",
      "Christoph Mertz",
      "Deva Ramanan",
      "Shu Kong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1448_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690139.pdf",
    "published": "2020-08",
    "summary": "\"Object detection with multimodal inputs can improve many safety-critical systems such as autonomous vehicles (AVs). Motivated by AVs that operate in both day and night, we study multimodal object detection with RGB and thermal cameras, since the latter provides much stronger object signatures under poor illumination. We explore strategies for fusing information from different modalities. Our key contribution is a probabilistic ensembling technique, ProbEn, a simple non-learned method that fuses together detections from multi-modalities. We derive ProbEn from Bayes\u2019 rule and first principles that assume conditional independence across modalities. Through probabilistic marginalization, ProbEn elegantly handles missing modalities when detectors do not fire on the same object. Importantly, ProbEn also notably improves multimodal detection even when the conditional independence assumption does not hold, e.g., fusing outputs from other fusion methods (both off-the-shelf and trained in-house). We validate ProbEn on two benchmarks containing both aligned (KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms prior work by more than {\\bf 13\\%} in relative performance!\""
  },
  "eccv2022_main_exploitingunlabeleddatawithvisionandlanguagemodelsforobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exploiting Unlabeled Data with Vision and Language Models for Object Detection",
    "authors": [
      "Shiyu Zhao",
      "Zhixing Zhang",
      "Samuel Schulter",
      "Long Zhao",
      "Vijay Kumar B G",
      "Anastasis Stathopoulos",
      "Manmohan Chandraker",
      "Dimitris N. Metaxas"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1474_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690156.pdf",
    "published": "2020-08",
    "summary": "\"Building robust and generic object detection frameworks requires scaling to larger label spaces and bigger training datasets. However, it is prohibitively costly to acquire annotations for thousands of categories at a large scale. We propose a novel method that leverages the rich semantics available in recent vision and language models to localize and classify objects in unlabeled images, effectively generating pseudo labels for object detection. Starting with a generic and class-agnostic region proposal mechanism, we use vision and language models to categorize each region of an image into any object category that is required for downstream tasks. We demonstrate the value of the generated pseudo labels in two specific tasks, open-vocabulary detection, where a model needs to generalize to unseen object categories, and semi-supervised object detection, where additional unlabeled images can be used to improve the model. Our empirical evaluation shows the effectiveness of the pseudo labels in both tasks, where we outperform competitive baselines and achieve a novel state-of-the-art for open-vocabulary object detection. Our code is available at https://github.com/xiaofeng94/VL-PLM.\""
  },
  "eccv2022_main_cpochangerobustpanoramatopointcloudlocalization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "CPO: Change Robust Panorama to Point Cloud Localization",
    "authors": [
      "Junho Kim",
      "Hojun Jang",
      "Changwoon Choi",
      "Young Min Kim"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1567_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690173.pdf",
    "published": "2020-08",
    "summary": "\"We present CPO, a fast and robust algorithm that localizes a 2D panorama with respect to a 3D point cloud of a scene possibly containing changes. To robustly handle scene changes, our approach deviates from conventional feature point matching, and focuses on the spatial context provided from panorama images. Specifically, we propose efficient color histogram generation and subsequent robust localization using score maps. By utilizing the unique equivariance of spherical projections, we propose very fast color histogram generation for a large number of camera poses without explicitly rendering images for all candidate poses. We accumulate the regional consistency of the panorama and point cloud as 2D/3D score maps, and use them to weigh the input color values to further increase robustness. The weighted color distribution quickly finds good initial poses and achieves stable convergence for gradient-based optimization. CPO is lightweight and achieves effective localization in all tested scenarios, showing stable performance despite scene changes, repetitive structures, or featureless regions, which are typical challenges for visual localization with perspective cameras.\""
  },
  "eccv2022_main_inttowardsinfinite-frames3ddetectionwithanefficientframework": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "INT: Towards Infinite-Frames 3D Detection with an Efficient Framework",
    "authors": [
      "Jianyun Xu",
      "Zhenwei Miao",
      "Da Zhang",
      "Hongyu Pan",
      "Kaixuan Liu",
      "Peihan Hao",
      "Jun Zhu",
      "Zhengyang Sun",
      "Hongmin Li",
      "Xin Zhan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1751_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690190.pdf",
    "published": "2020-08",
    "summary": "\"It is natural to construct a multi-frame instead of a single-frame 3D detector for a continuous-time stream. Although increasing the number of frames might improve performance, previous multi-frame studies only used very limited frames to build their systems due to the dramatically increased computational and memory cost. To address these issues, we propose a novel on-stream training and prediction framework that, in theory, can employ an infinite number of frames while keeping the same amount of computation as a single-frame detector. This infinite framework (INT), which can be used with most existing detectors, is utilized, for example, on the popular CenterPoint, with significant latency reductions and performance improvements. We\u2019ve also conducted extensive experiments on two large-scale datasets, nuScenes and Waymo Open Dataset, to demonstrate the scheme\u2019s effectiveness and efficiency. By employing INT on CenterPoint, we can get around 7% (Waymo) and 15% (nuScenes) performance boost with only 2 4ms latency overhead, and currently SOTA on the Waymo 3D Detection leaderboard.\""
  },
  "eccv2022_main_end-to-endweaklysupervisedobjectdetectionwithsparseproposalevolution": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "End-to-End Weakly Supervised Object Detection with Sparse Proposal Evolution",
    "authors": [
      "Mingxiang Liao",
      "Fang Wan",
      "Yuan Yao",
      "Zhenjun Han",
      "Jialing Zou",
      "Yuze Wang",
      "Bailan Feng",
      "Peng Yuan",
      "Qixiang Ye"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1852_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690207.pdf",
    "published": "2020-08",
    "summary": "\"Conventional methods for weakly supervised object detection (WSOD) typically enumerate dense proposals and select the discriminative proposals as objects. However, these two-stage \u201cenumerate-and-select\u201d methods suffer object feature ambiguity brought by dense proposals and low detection efficiency caused by the proposal enumeration procedure. In this study, we propose a sparse proposal evolution (SPE) approach, which advances WSOD from the two-stage pipeline with dense proposals to an end-to-end framework with sparse proposals. SPE is built upon a visual transformer equipped with a seed proposal generation (SPG) branch and a sparse proposal refinement (SPR) branch. SPG generates high-quality seed proposals by taking advantage of the cascaded self-attention mechanism of the visual transformer, and SPR trains the detector to predict sparse proposals which are supervised by the seed proposals in a one-to-one matching fashion. SPG and SPR are iteratively performed so that seed proposals update to accurate supervision signals and sparse proposals evolve to precise object regions. Experiments on VOC and COCO object detection datasets show that SPE outperforms the state-of-the-art end-to-end methods by 7.0% mAP and 8.1% AP50. It is an order of magnitude faster than the two-stage methods, setting the first solid baseline for end-to-end WSOD with sparse proposals.\""
  },
  "eccv2022_main_calibration-freemulti-viewcrowdcounting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Calibration-Free Multi-View Crowd Counting",
    "authors": [
      "Qi Zhang",
      "Antoni B. Chan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1929_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690224.pdf",
    "published": "2020-08",
    "summary": "\"Deep learning-based multi-view crowd counting (MVCC) has been proposed to handle scenes with large size, in irregular shape, or with severe occlusions. The current MVCC methods require camera calibrations in both training and testing, limiting the real application scenarios of MVCC. To extend and apply MVCC to more practical situations, in this paper we propose calibration-free multi-view crowd counting (CF-MVCC), which obtains the scene-level count directly from the density map predictions for each camera view without needing the camera calibrations in the test. Specifically, the proposed CF-MVCC method first estimates the homography matrix to align each pair of camera views, and then estimates a matching probability map for each camera-view pair. Based on the matching maps of all camera-view pairs, a weight map for each camera view is predicted, which represents how many cameras can reliably see a given pixel in the camera view. Finally, using the weight maps, the total scene-level count is obtained as a simple weighted sum of the density maps for the camera views. Experiments are conducted on several multi-view counting datasets, and promising performance is achieved compared to calibrated MVCC methods that require camera calibrations as input and use scene-level density maps as supervision.\""
  },
  "eccv2022_main_unsuperviseddomainadaptationformonocular3dobjectdetectionviaself-training": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Unsupervised Domain Adaptation for Monocular 3D Object Detection via Self-Training",
    "authors": [
      "Zhenyu Li",
      "Zehui Chen",
      "Ang Li",
      "Liangji Fang",
      "Qinhong Jiang",
      "Xianming Liu",
      "Junjun Jiang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1930_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690242.pdf",
    "published": "2020-08",
    "summary": "\"Monocular 3D object detection (Mono3D) has achieved unprecedented success with the advent of deep learning techniques and emerging large-scale autonomous driving datasets. However, drastic performance degradation remains an unwell-studied challenge for practical cross-domain deployment as the lack of labels on the target domain. In this paper, we first comprehensively investigate the significant underlying factor of the domain gap in Mono3D, where the critical observation is a depth-shift issue caused by the geometric misalignment of domains. Then, we propose STMono3D, a new self-teaching framework for unsupervised domain adaptation on Mono3D. To mitigate the depth-shift, we introduce the geometry-aligned multi-scale training strategy to disentangle the camera parameters and guarantee the geometry consistency of domains. Based on this, we develop a teacher-student paradigm to generate adaptive pseudo labels on the target domain. Benefiting from the end-to-end framework that provides richer information of the pseudo labels, we propose the quality-aware supervision strategy to take instance-level pseudo confidences into account and improve the effectiveness of the target-domain training process. Moreover, the positive focusing training strategy and dynamic threshold are proposed to handle tremendous FN and FP pseudo samples. STMono3D achieves remarkable performance on all evaluated datasets and even surpasses fully supervised results on the KITTI 3D object detection dataset. To the best of our knowledge, this is the first study to explore effective UDA methods for Mono3D.\""
  },
  "eccv2022_main_superline3dself-supervisedlinesegmentationanddescriptionforlidarpointcloud": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SuperLine3D: Self-Supervised Line Segmentation and Description for LiDAR Point Cloud",
    "authors": [
      "Xiangrui Zhao",
      "Sheng Yang",
      "Tianxin Huang",
      "Jun Chen",
      "Teng Ma",
      "Mingyang Li",
      "Yong Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2025_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690259.pdf",
    "published": "2020-08",
    "summary": "\"Poles and building edges are frequently observable objects on urban roads, conveying reliable hints for various computer vision tasks. To repetitively extract them as features and perform association between discrete LiDAR frames for registration, we propose the first learning-based feature segmentation and description model for 3D lines in LiDAR point cloud. To train our model without the time consuming and tedious data labeling process, we first generate synthetic primitives for the basic appearance of target lines, and build an iterative line auto-labeling process to gradually refine line labels on real LiDAR scans. Our segmentation model can extract lines under arbitrary scale perturbations, and we use shared EdgeConv encoder layers to train the two segmentation and descriptor heads jointly. Base on the model, we can build a highly-available global registration module for point cloud registration, in conditions without initial transformation hints. Experiments have demonstrated that our line-based registration method is highly competitive to state-of-the-art point-based approaches. Our code is available at https://github.com/zxrzju/SuperLine3D.git.\""
  },
  "eccv2022_main_exploringplainvisiontransformerbackbonesforobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
    "authors": [
      "Yanghao Li",
      "Hanzi Mao",
      "Ross Girshick",
      "Kaiming He"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2151_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690276.pdf",
    "published": "2020-08",
    "summary": "\"We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.\""
  },
  "eccv2022_main_adversarially-awarerobustobjectdetector": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Adversarially-Aware Robust Object Detector",
    "authors": [
      "Ziyi Dong",
      "Pengxu Wei",
      "Liang Lin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2179_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690293.pdf",
    "published": "2020-08",
    "summary": "\"Object detection, as a fundamental computer vision task, has achieved a remarkable progress with the emergence of deep neural networks. Nevertheless, few works explore the adversarial robustness of object detectors to resist adversarial attacks for practical applications in various real-world scenarios. Detectors have been greatly challenged by unnoticeable perturbation, with sharp performance drop on clean images and extremely poor performance on adversarial images. In this work, we empirically explore the model training for adversarial robustness in object detection, which greatly attributes to the conflict between learning clean images and adversarial images. To mitigate this issue, we propose a Robust Detector (RobustDet) based on adversarially-aware convolution to disentangle gradients for model learning on clean and adversarial images. RobustDet also employs the Adversarial Image Discriminator (AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that our model effectively disentangles gradients and significantly enhances the detection robustness with maintaining the detection ability on clean images.\""
  },
  "eccv2022_main_headhetero-assistsdistillationforheterogeneousobjectdetectors": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "HEAD: HEtero-Assists Distillation for Heterogeneous Object Detectors",
    "authors": [
      "Luting Wang",
      "Xiaojie Li",
      "Yue Liao",
      "Zeren Jiang",
      "Jianlong Wu",
      "Fei Wang",
      "Chen Qian",
      "Si Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2285_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690310.pdf",
    "published": "2020-08",
    "summary": "\"Conventional knowledge distillation (KD) methods for object detection mainly concentrate on homogeneous teacher-student detectors. However, the design of a lightweight detector for deployment is often significantly different from a high-capacity detector. Thus, we investigate KD among heterogeneous teacher-student pairs for a wide application. We observe that the core difficulty for heterogeneous KD (hetero-KD) is the significant semantic gap between the backbone features of heterogeneous detectors due to the different optimization manners. Conventional homogeneous KD (homo-KD) methods suffer from such a gap and are hard to directly obtain satisfactory performance for hetero-KD. In this paper, we propose the HEtero-Assists Distillation (HEAD) framework, leveraging heterogeneous detection heads as assistants to guide the optimization of the student detector to reduce this gap. In HEAD, the assistant is an additional detection head with the architecture homogeneous to the teacher head attached to the student backbone. Thus, a hetero-KD is transformed into a homo-KD, allowing efficient knowledge transfer from the teacher to the student. Moreover, we extend HEAD into a Teacher-Free HEAD (TF-HEAD) framework when a well-trained teacher detector is unavailable. Our method has achieved significant improvement compared to current detection KD methods. For example, on the MS-COCO dataset, TF-HEAD helps R18 RetinaNet achieve 33.9 mAP (+2.2), while HEAD further pushes the limit to 36.2 mAP (+4.5).\""
  },
  "eccv2022_main_youshouldlookatallobjects": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "You Should Look at All Objects",
    "authors": [
      "Zhenchao Jin",
      "Dongdong Yu",
      "Luchuan Song",
      "Zehuan Yuan",
      "Lequan Yu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2379_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690327.pdf",
    "published": "2020-08",
    "summary": "\"Feature pyramid network (FPN) is one of the key components for object detectors. However, there is a long-standing puzzle for researchers that the detection performance of large-scale objects are usually suppressed after introducing FPN. To this end, this paper first revisits FPN in the detection framework and reveals the nature of the success of FPN from the perspective of optimization. Then, we point out that the degraded performance of large-scale objects is due to the arising of improper back-propagation paths after integrating FPN. It makes each level of the backbone network only has the ability to look at the objects within a certain scale range. Based on these analysis, two feasible strategies are proposed to enable each level of the backbone to look at all objects in the FPN-based detection frameworks. Specifically, one is to introduce auxiliary objective functions to make each backbone level directly receive the back-propagation signals of various-scale objects during training. The other is to construct the feature pyramid in a more reasonable way to avoid the irrational back-propagation paths. Extensive experiments on the COCO benchmark validate the soundness of our analysis and the effectiveness of our methods. Without bells and whistles, we demonstrate that our method achieves solid improvements (more than 2%) on various detection frameworks: one-stage, two-stage, anchor-based, anchor-free and transformer-based detectors.\""
  },
  "eccv2022_main_detectingtwenty-thousandclassesusingimage-levelsupervision": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Detecting Twenty-Thousand Classes Using Image-Level Supervision",
    "authors": [
      "Xingyi Zhou",
      "Rohit Girdhar",
      "Armand Joulin",
      "Philipp Kr\u00e4henb\u00fchl",
      "Ishan Misra"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2557_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690344.pdf",
    "published": "2020-08",
    "summary": "\"Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is provided in the supplementary.\""
  },
  "eccv2022_main_dcl-netdeepcorrespondencelearningnetworkfor6dposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation",
    "authors": [
      "Hongyang Li",
      "Jiehong Lin",
      "Kui Jia"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2558_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690362.pdf",
    "published": "2020-08",
    "summary": "\"Establishment of point correspondence between camera and object coordinate systems is a promising way to solve 6D object poses. However, surrogate objectives of correspondence learning in 3D space are a step away from the true ones of object pose estimation, making the learning suboptimal for the end task. In this paper, we address this shortcoming by introducing a new method of Deep Correspondence Learning Network for direct 6D object pose estimation, shortened as DCL-Net. Specifically, DCL-Net employs dual newly proposed Feature Disengagement and Alignment (FDA) modules to establish, in the feature space, partial-to-partial correspondence and complete-to-complete one for partial object observation and its complete CAD model, respectively, which result in aggregated pose and match feature pairs from two coordinate systems; these two FDA modules thus bring complementary advantages. The match feature pairs are used to learn confidence scores for measuring the qualities of deep correspondence, while the pose ones are weighted by confidence scores for direct object pose regression. A confidence-based pose refinement network is also proposed to further improve pose precision in an iterative manner. Extensive experiments show that DCL-Net outperforms existing methods on three benchmarking datasets, including YCB-Video, LineMOD, and Oclussion-LineMOD; ablation studies also confirm the efficacy of our novel designs. Our code is released publicly at https://github.com/Gorilla-Lab-SCUT/DCL-Net.\""
  },
  "eccv2022_main_monocular3dobjectdetectionwithdepthfrommotion": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Monocular 3D Object Detection with Depth from Motion",
    "authors": [
      "Tai Wang",
      "Jiangmiao Pang",
      "Dahua Lin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2691_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690380.pdf",
    "published": "2020-08",
    "summary": "\"Perceiving 3D objects from monocular inputs is crucial for robotic systems, given its economy compared to multi-sensor settings. It is notably difficult as a single image can not provide any clues for predicting absolute depth values. Motivated by binocular methods for 3D object detection, we take advantage of the strong geometry structure provided by camera ego-motion for accurate object depth estimation and detection. We first make a theoretical analysis on this general two-view case and notice two challenges: 1) Cumulative errors from multiple estimations that make the direct prediction intractable; 2) Inherent dilemmas caused by static cameras and matching ambiguity. Accordingly, we establish the stereo correspondence with a geometry-aware cost volume as the alternative for depth estimation and further compensate it with monocular understanding to address the second problem. Our framework, named Depth from Motion (DfM), then uses the established geometry to lift 2D image features to the 3D space and detects 3D objects thereon. We also present a pose-free DfM to make it usable when the camera pose is unavailable. Our framework outperforms state-of-the-art methods by a large margin on the KITTI benchmark. Detailed quantitative and qualitative analyses also validate our theoretical conclusions. The code is released at https://github.com/Tai-Wang/Depth-from-Motion.\""
  },
  "eccv2022_main_disp6ddisentangledimplicitshapeandposelearningforscalable6dposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DISP6D: Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation",
    "authors": [
      "Yilin Wen",
      "Xiangyu Li",
      "Hao Pan",
      "Lei Yang",
      "Zheng Wang",
      "Taku Komura",
      "Wenping Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2707_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690397.pdf",
    "published": "2020-08",
    "summary": "\"Scalable 6D pose estimation for rigid objects from RGB images aims at handling multiple objects and generalizing to novel objects. Building on a well-known auto-encoding framework to cope with object symmetry and the lack of labeled training data, we achieve scalability by disentangling the latent representation of auto-encoder into shape and pose sub-spaces. The latent shape space models the similarity of different objects through contrastive metric learning, and the latent pose code is compared with canonical rotations for rotation retrieval. Because different object symmetries induce inconsistent latent pose spaces, we re-entangle the shape representation with canonical rotations to generate shape-dependent pose codebooks for rotation retrieval. We show state-of-the-art performance on two benchmarks containing textureless CAD objects without category and daily objects with categories respectively, and further demonstrate improved scalability by extending to a more challenging setting of daily objects across categories.\""
  },
  "eccv2022_main_distillingobjectdetectorswithglobalknowledge": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Distilling Object Detectors with Global Knowledge",
    "authors": [
      "Sanli Tang",
      "Zhongyu Zhang",
      "Zhanzhan Cheng",
      "Jing Lu",
      "Yunlu Xu",
      "Yi Niu",
      "Fan He"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2717_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690415.pdf",
    "published": "2020-08",
    "summary": "\"Knowledge distillation learns a lightweight student model that mimics a cumbersome teacher. Existing methods regard the knowledge as the feature of each instance or their relations, which is the instance-level knowledge only from the teacher model, i.e., the local knowledge. However, the empirical studies show that the local knowledge is much noisy in object detection tasks, especially on the blurred, occluded, or small instances. Thus, a more intrinsic approach is to measure the representations of instances w.r.t. a group of common basis vectors in the two feature spaces of the teacher and the student detectors, i.e., global knowledge. Then, the distilling algorithm can be applied as space alignment. To this end, a novel prototype generation module (PGM) is proposed to find the common basis vectors, dubbed prototypes, in the two feature spaces. Then, a robust distilling module (RDM) is applied to construct the global knowledge based on the prototypes and filtrate noisy global and local knowledge by measuring the discrepancy of the representations in two feature spaces. Experiments with Faster-RCNN and RetinaNet on PASCAL and COCO datasets show that our method achieves the best performance for distilling object detectors with various backbones, which even surpasses the performance of the teacher model. We also show that the existing methods can be easily combined with global knowledge and obtain further improvement. Code is available: https://github.com/hikvision-research/DAVAR-Lab-ML.\""
  },
  "eccv2022_main_unifyingvisualperceptionbydispersiblepointslearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Unifying Visual Perception by Dispersible Points Learning",
    "authors": [
      "Jianming Liang",
      "Guanglu Song",
      "Biao Leng",
      "Yu Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2746_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690432.pdf",
    "published": "2020-08",
    "summary": "\"We present a conceptually simple, flexible, and universal visual perception head for variant visual tasks, e.g., classification, object detection, instance segmentation and pose estimation, and different frameworks, such as one-stage or two-stage pipelines. Our approach effectively identifies an object in an image while simultaneously generating a high-quality bounding box or contour-based segmentation mask or set of keypoints. The method, called UniHead, views different visual perception tasks as the dispersible points learning via the transformer encoder architecture. Given a fixed spatial coordinate, UniHead adaptively scatters it to different spatial points and reasons about their relations by transformer encoder. It directly outputs the final set of predictions in the form of multiple points, allowing us to perform different visual tasks in different frameworks with the same head design. We show extensive evaluations on ImageNet classification and all three tracks of the COCO suite of challenges, including object detection, instance segmentation and pose estimation. Without bells and whistles, UniHead can unify these visual tasks via a single visual head design and achieve comparable performance compared to expert models developed for each task. We hope our simple and universal UniHead will serve as a solid baseline and help promote universal visual perception research. Code and models are available at https://github.com/Sense-X/UniHead.\""
  },
  "eccv2022_main_psecopseudolabelingandconsistencytrainingforsemi-supervisedobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection",
    "authors": [
      "Gang Li",
      "Xiang Li",
      "Yujie Wang",
      "Yichao Wu",
      "Ding Liang",
      "Shanshan Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2783_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690449.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we delve into two key techniques in Semi-Supervised Object Detection (SSOD), namely pseudo labeling and consistency training. We observe that these two techniques currently neglect some important properties of object detection, hindering efficient learning on unlabeled data. Specifically, for pseudo labeling, existing works only focus on the classification score yet fail to guarantee the localization precision of pseudo boxes; For consistency training, the widely adopted random-resize training only considers the label-level consistency but misses the feature-level one, which also plays an important role in ensuring the scale invariance. To address the problems incurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that includes Prediction-guided Label Assignment (PLA) and Positive-proposal Consistency Voting (PCV). PLA relies on model predictions to assign labels and makes it robust to even coarse pseudo boxes; while PCV leverages the regression consistency of positive proposals to reflect the localization quality of pseudo boxes. Furthermore, in consistency training, we propose Multi-view Scale-invariant Learning (MSL) that includes mechanisms of both label- and feature-level consistency, where feature consistency is achieved by aligning shifted feature pyramids between two images with identical content but varied scales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency training (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points under 1%, 5%, and 10% labelling ratios, respectively. It also significantly improves the learning efficiency for SSOD, e.g., PseCo halves the training time of the SOTA approach but achieves even better performance. Code is available at https://github.com/ligang-cs/PseCo.\""
  },
  "eccv2022_main_exploringresolutionanddegradationcluesasself-supervisedsignalforlowqualityobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exploring Resolution and Degradation Clues As Self-Supervised Signal for Low Quality Object Detection",
    "authors": [
      "Ziteng Cui",
      "Yingying Zhu",
      "Lin Gu",
      "Guo-Jun Qi",
      "Xiaoxiao Li",
      "Renrui Zhang",
      "Zenghui Zhang",
      "Tatsuya Harada"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2829_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690465.pdf",
    "published": "2020-08",
    "summary": "\"Image restoration algorithms such as super resolution (SR) are indispensable pre-processing modules for object detection in low qual-ity images. Most of these algorithms assume the degradation is fixed andknown a priori. However, in pratical, either the real degrdation or optimalup-sampling ratio rate is unknown or differs from assumption, leading toa deteriorating performance for both the pre-processing module and theconsequent high-level task such as object detection. Here, we propose anovel self-supervised framework to detect objects in degraded low res-olution images. We utilizes the downsampling degradation as a kind oftransformation for self-supervised signals to explore the equivariant representation against various resolutions and other degradation conditions.The Auto Encoding Resolution in Self-supervision (AERIS) frameworkcould further take the advantage of advanced SR architectures with anarbitrary resolution restoring decoder to reconstruct the original corre-spondence from the degraded input image. Both the representation learn-ing and object detection are optimized jointly in an end-to-end trainingfashion. The generic AERIS frameworkcould be implemented on variousmainstream object detection architectures from CNN to Transformer.The extensive experiments show that our methods has achieved supe-rior performance compared with existing methods when facing variantdegradation situations.We will release the open source code.\""
  },
  "eccv2022_main_robustcategory-level6dposeestimationwithcoarse-to-finerenderingofneuralfeatures": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features",
    "authors": [
      "Wufei Ma",
      "Angtian Wang",
      "Alan Yuille",
      "Adam Kortylewski"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2856_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690484.pdf",
    "published": "2020-08",
    "summary": "\"We consider the problem of category-level 6D pose estimation from a single RGB image. Our approach represents an object category as a cuboid mesh and learns a generative model of the neural feature activations at each mesh vertex to perform pose estimation through differentiable rendering. A common problem of rendering-based approaches is that they rely on bounding box proposals, which do not convey information about the 3D rotation of the object and are not reliable when objects are partially occluded. Instead, we introduce a coarse-to-fine optimization strategy that utilizes the rendering process to estimate a sparse set of 6D object proposals, which are subsequently refined with gradient-based optimization. The key to enabling the convergence of our approach is a neural feature representation that is trained to be scale- and rotation-invariant using contrastive learning. Our experiments demonstrate an enhanced category-level 6D pose estimation performance compared to prior work, particularly under strong partial occlusion.\""
  },
  "eccv2022_main_translation,scaleandrotationcross-modalalignmentmeetsrgb-infraredvehicledetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"Translation, Scale and Rotation: Cross-Modal Alignment Meets RGB-Infrared Vehicle Detection\"",
    "authors": [
      "Maoxun Yuan",
      "Yinyan Wang",
      "Xingxing Wei"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3041_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690501.pdf",
    "published": "2020-08",
    "summary": "\"Integrating multispectral data in object detection, especially visible and infrared images, has received great attention in recent years. Since visible (RGB) and infrared (IR) images can provide complementary information to handle light variations, the paired images are used in many fields, such as multispectral pedestrian detection, RGB-IR crowd counting and RGB-IR salient object detection. Compared with natural RGB-IR images, we find detection in aerial RGB-IR images suffers from cross-modal weakly misalignment problems, which are manifested in the position, size and angle deviations of the same object. In this paper, we mainly address the challenge of cross-modal weakly misalignment in aerial RGB-IR images. Specifically, we firstly explain and analyze the cause of the weakly misalignment problem. Then, we propose a Translation-Scale-Rotation Alignment (TSRA) module to address the problem by calibrating the feature maps from these two modalities. The module predicts the deviation between two modality objects through an alignment process and utilizes Modality-Selection (MS) strategy to improve the performance of alignment. Finally, a two-stream feature alignment detector (TSFADet) based on the TSRA module is constructed for RGB-IR object detection in aerial images. With comprehensive experiments on the public DroneVehicle datasets, we verify that our method reduces the effect of the cross-modal misalignment and achieve robust detection results.\""
  },
  "eccv2022_main_rflagaussianreceptivefieldbasedlabelassignmentfortinyobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "RFLA: Gaussian Receptive Field Based Label Assignment for Tiny Object Detection",
    "authors": [
      "Chang Xu",
      "Jinwang Wang",
      "Wen Yang",
      "Huai Yu",
      "Lei Yu",
      "Gui-Song Xia"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3138_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690518.pdf",
    "published": "2020-08",
    "summary": "\"Detecting tiny objects is one of the main obstacles hindering the development of object detection. The performance of generic object detectors tends to drastically deteriorate on tiny object detection tasks. In this paper, we point out that either box prior in the anchor-based detector or point prior in the anchor-free detector is sub-optimal for tiny objects. Our key observation is that the current anchor-based or anchor-free label assignment paradigms will incur many outlier tiny-sized ground truth samples, leading to detectors imposing less focus on the tiny objects. To this end, we propose a Gaussian Receptive Field based Label Assignment (RFLA) strategy for tiny object detection. Specifically, RFLA first utilizes the prior information that the feature receptive field follows Gaussian distribution. Then, instead of assigning samples with IoU or center sampling strategy, a new Receptive Field Distance (RFD) is proposed to directly measure the similarity between the Gaussian receptive field and ground truth. Considering that the IoU-threshold based and center sampling strategy are skewed to large objects, we further design a Hierarchical Label Assignment (HLA) module based on RFD to achieve balanced learning for tiny objects. Extensive experiments on four datasets demonstrate the effectiveness of the proposed methods. Especially, our approach outperforms the state-of-the-art competitors with 4.0 AP points on the AI-TOD dataset. Codes are available at https://github.com/Chasel-Tsui/mmdet-rfla.\""
  },
  "eccv2022_main_rethinkingiou-basedoptimizationforsingle-stage3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Rethinking IoU-Based Optimization for Single-Stage 3D Object Detection",
    "authors": [
      "Hualian Sheng",
      "Sijia Cai",
      "Na Zhao",
      "Bing Deng",
      "Jianqiang Huang",
      "Xian-Sheng Hua",
      "Min-Jian Zhao",
      "Gim Hee Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3295_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690536.pdf",
    "published": "2020-08",
    "summary": "\"Since Intersection-over-Union (IoU) based optimization maintains the consistency of the final IoU prediction metric and losses, it has been widely used in both regression and classification branches of single-stage 2D object detectors. Recently, several 3D object detection methods adopt IoU-based optimization and directly replace the 2D IoU with 3D IoU. However, such a direct computation in 3D is very costly due to the complex implementation and inefficient backward operations. Moreover, 3D IoU-based optimization is sub-optimal as it is sensitive to rotation and thus can cause training instability and detection performance deterioration. In this paper, we propose a novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the rotation-sensitivity issue, and produce more efficient optimization objectives compared with 3D IoU during the training stage. Specifically, our RDIoU simplifies the complex interactions of regression parameters by decoupling the rotation variable as an independent term, yet preserving the geometry of 3D IoU. By incorporating RDIoU into both the regression and classification branches, the network is encouraged to learn more precise bounding boxes and concurrently overcome the misalignment issue between classification and regression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset validate that our RDIoU method can bring substantial improvement for the single-stage 3D object detection. Our code will be available upon paper acceptance.\""
  },
  "eccv2022_main_td-roadtop-downroadnetworkextractionwithholisticgraphconstruction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "TD-Road: Top-Down Road Network Extraction with Holistic Graph Construction",
    "authors": [
      "Yang He",
      "Ravi Garg",
      "Amber Roy Chowdhury"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3359_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690553.pdf",
    "published": "2020-08",
    "summary": "\"Graph-based approaches have been becoming increasingly popular in road network extraction, in addition to segmentation-based methods. Road networks are represented as graph structures, being able to explicitly define the topology structures and avoid the ambiguity of segmentation masks, such as between a real junction area and multiple separate roads in different heights. In contrast to the bottom-up graph-based approaches, which rely on orientation information, we propose a novel top-down approach to generate road network graphs with a holistic model, namely TD-Road. We decompose road extraction as two subtasks: key point prediction and connectedness prediction. We directly apply graph structures (i.e., locations of node and connections between them) as training supervisions for neural networks and generate road graph outputs in inference, instead of learning some intermediate properties of a graph structure (e.g., orientations or distances for the next move). Our network integrates a relation inference module with key point prediction, to capture connections between neighboring points and outputs the final road graphs with no post-processing steps required. Extensive experiments are conducted on challenging datasets, including City-Scale and SpaceNet to show the effectiveness and simplicity of our method, that the proposed method achieves remarkable results compared with previous state-of-the-art methods.\""
  },
  "eccv2022_main_multi-faceteddistillationofbase-novelcommonalityforfew-shotobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Multi-faceted Distillation of Base-Novel Commonality for Few-Shot Object Detection",
    "authors": [
      "Shuang Wu",
      "Wenjie Pei",
      "Dianwen Mei",
      "Fanglin Chen",
      "Jiandong Tian",
      "Guangming Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3523_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690569.pdf",
    "published": "2020-08",
    "summary": "\"Most of existing methods for few-shot object detection follow the fine-tuning paradigm, which potentially assumes that the class-agnostic generalizable knowledge can be learned and transferred implicitly from base classes with abundant samples to novel classes with limited samples via such a two-stage training strategy. However, it is not necessarily true since the object detector can hardly distinguish between class-agnostic knowledge and class-specific knowledge automatically without explicit modeling. In this work we propose to learn three types of class-agnostic commonalities between base and novel classes explicitly: recognition-related semantic commonalities, localization-related semantic commonalities and distribution commonalities. We design a unified distillation framework based on a memory bank, which is able to perform distillation of all three types of commonalities jointly and efficiently. Extensive experiments demonstrate that our method can be readily integrated into most of existing fine-tuning based methods and consistently improve the performance by a large margin.\""
  },
  "eccv2022_main_pointclmacontrastivelearning-basedframeworkformulti-instancepointcloudregistration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PointCLM: A Contrastive Learning-Based Framework for Multi-Instance Point Cloud Registration",
    "authors": [
      "Mingzhi Yuan",
      "Zhihao Li",
      "Qiuye Jin",
      "Xinrong Chen",
      "Manning Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3580_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690586.pdf",
    "published": "2020-08",
    "summary": "\"Multi-instance point cloud registration is the problem of estimating multiple poses of source point cloud instances within a target point cloud. Solving this problem is challenging since inlier correspondences of one instance constitute outliers of all the other instances. Existing methods often rely on time-consuming hypothesis sampling or features leveraging spatial consistency, resulting in limited performance. In this paper, we propose PointCLM, a contrastive learning-based framework for mutli-instance point cloud registration. We first utilize contrastive learning to learn well-distributed deep representations for the input putative correspondences. Then based on these representations, we propose a outlier pruning strategy and a clustering strategy to efficiently remove outliers and assign the remaining correspondences to correct instances. Our method outperforms the state-of-the-art methods on both synthetic and real datasets by a large margin.\""
  },
  "eccv2022_main_weaklysupervisedobjectlocalizationviatransformerwithimplicitspatialcalibration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration",
    "authors": [
      "Haotian Bai",
      "Ruimao Zhang",
      "Jiong Wang",
      "Xiang Wan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3632_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690603.pdf",
    "published": "2020-08",
    "summary": "\"Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Recent studies leverage the advantage of self-attention in visual Transformer for long-range dependency to re-active semantic regions, aiming to avoid partial activation in traditional class activation mapping (CAM). However, the long-range modeling in Transformer neglects the inherent spatial coherence of the object, and it usually diffuses the semantic-aware regions far from the object boundary, making localization results significantly larger or far smaller. To address such an issue, we introduce a simple yet effective Spatial Calibration Module (SCM) for accurate WSOL, incorporating semantic similarities of patch tokens and their spatial relationships into a unified diffusion model. Specifically, we introduce a learnable parameter to dynamically adjust the semantic correlations and spatial context intensities for effective information propagation. In practice, SCM is designed as an external module of Transformer, and can be removed during inference to reduce the computation cost. The object-sensitive localization ability is implicitly embedded into the Transformer encoder through optimization in the training phase. It enables the generated attention maps to capture the sharper object boundaries and filter the object-irrelevant background area. Extensive experimental results demonstrate the effectiveness of the proposed method, which significantly outperforms its counterpart TS-CAM on both CUB-200 and ImageNet-1K benchmarks. The code is available at https://github.com/164140757/SCM.\""
  },
  "eccv2022_main_mttranscross-domainobjectdetectionwithmeanteachertransformer": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "MTTrans: Cross-Domain Object Detection with Mean Teacher Transformer",
    "authors": [
      "Jinze Yu",
      "Jiaming Liu",
      "Xiaobao Wei",
      "Haoyi Zhou",
      "Yohei Nakata",
      "Denis Gudovskiy",
      "Tomoyuki Okuno",
      "Jianxin Li",
      "Kurt Keutzer",
      "Shanghang Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3723_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690620.pdf",
    "published": "2020-08",
    "summary": "\"Recently, DEtection TRansformer (DETR), an end-to-end object detection pipeline, has achieved promising performance. However, it requires large-scale labeled data and suffers from domain shift, especially when no labeled data is available in the target domain. To solve this problem, we propose an end-to-end cross-domain detection Transformer based on the mean teacher framework, MTTrans, which can fully exploit unlabeled target domain data in object detection training and transfer knowledge between domains via pseudo labels. We further propose the comprehensive multi-level feature alignment to improve the pseudo labels generated by the mean teacher framework taking advantage of the cross-scale self-attention mechanism in Deformable DETR. Image and object features are aligned at the local, global, and instance levels with domain query-based feature alignment (DQFA), bi-level graph-based prototype alignment (BGPA), and token-wise image feature alignment (TIFA). On the other hand, the unlabeled target domain data pseudo-labeled and available for the object detection training by the mean teacher framework can lead to better feature extraction and alignment. Thus, the mean teacher framework and the comprehensive multi-level feature alignment can be optimized iteratively and mutually based on the architecture of Transformers. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in three domain adaptation scenarios, especially the result of Sim10k to Cityscapes scenario is remarkably improved from 52.6 mAP to 57.9 mAP. Code will be released.\""
  },
  "eccv2022_main_multi-domainmulti-definitionlandmarklocalizationforsmalldatasets": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Multi-Domain Multi-Definition Landmark Localization for Small Datasets",
    "authors": [
      "David Ferman",
      "Gaurav Bharaj"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3780_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690637.pdf",
    "published": "2020-08",
    "summary": "\"We present a novel method for multi image domain and multi-landmark definition learning for small dataset facial localization. Training a small dataset alongside a large(r) dataset helps with robust learning for the former, and provides a universal mechanism for facial landmark localization for new and/or smaller standard datasets. To this end, we propose a Vision Transformer encoder with a novel decoder with a definition agnostic shared landmark semantic group structured prior, that is learnt, as we train on more than one dataset concurrently. Due to our novel definition agnostic group prior the datasets may vary in landmark definitions and domains. During the decoder stage we use cross- and self-attention, whose output is later fed into domain/definition specific heads that minimize a Laplacian-log-likelihood loss. We achieve state-of-the-art performance on standard landmark localization datasets such as COFW and WFLW, when trained with a bigger dataset. We also show state-of-the-art performance on several varied image domain small datasets for animals, caricatures, and facial portrait paintings. Further, we contribute a small dataset (150 images) of pareidolias to show efficacy of our method. Finally, we provide several analysis and ablation studies to justify our claims.\""
  },
  "eccv2022_main_deviantdepthequivariantnetworkformonocular3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection",
    "authors": [
      "Abhinav Kumar",
      "Garrick Brazil",
      "Enrique Corona",
      "Armin Parchami",
      "Xiaoming Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3798_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690655.pdf",
    "published": "2020-08",
    "summary": "\"Modern neural networks use building blocks such as convolutions that are equivariant to arbitrary 2D translations. However, these vanilla blocks are not equivariant to arbitrary 3D translations in the projective manifold. Even then, all monocular 3D detectors use vanilla blocks to obtain the 3D coordinates, a task for which the vanilla blocks are not designed for. This paper takes the first step towards convolutions equivariant to arbitrary 3D translations in the projective manifold. Since the depth is the hardest to estimate for monocular detection, this paper proposes Depth EquiVarIAnt NeTwork (DEVIANT) built with existing scale equivariant steerable blocks. As a result, DEVIANT is equivariant to the depth translations in the projective manifold whereas vanilla networks are not. The additional depth equivariance forces the DEVIANT to learn consistent depth estimates, and therefore, DEVIANT achieves state-of-the-art monocular 3D detection results on KITTI and Waymo datasets in the image-only category and performs competitively to methods using extra information. Moreover, DEVIANT works better than vanilla networks in cross-dataset evaluation.\""
  },
  "eccv2022_main_label-guidedauxiliarytrainingimproves3dobjectdetector": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Label-Guided Auxiliary Training Improves 3D Object Detector",
    "authors": [
      "Yaomin Huang",
      "Xinmei Liu",
      "Yichen Zhu",
      "Zhiyuan Xu",
      "Chaomin Shen",
      "Zhengping Che",
      "Guixu Zhang",
      "Yaxin Peng",
      "Feifei Feng",
      "Jian Tang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3921_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690674.pdf",
    "published": "2020-08",
    "summary": "\"Detecting 3D objects from point clouds is a practical yet challenging task that has attracted increasing attention recently. In this paper, we propose a Label-Guided auxiliary training method for 3D object detection (LG3D), which serves as an auxiliary network to enhance the feature learning of existing 3D object detectors. Specifically, we propose two novel modules: a Label-Annotation-Inducer that maps annotations and point clouds in bounding boxes to task-specific representations and a Label-Knowledge-Mapper that assists the original features to obtain detection-critical representations. The proposed auxiliary network is discarded in inference and thus has no extra computational cost at test time. We conduct extensive experiments on both indoor and outdoor datasets to verify the effectiveness of our approach. For example, our proposed LG3D improves VoteNet by 2.5\\% and 3.1\\% mAP on the SUN RGB-D and ScanNetV2 datasets, respectively. The code is available at https://github.com/FabienCode/LG3D.\""
  },
  "eccv2022_main_promptdettowardsopen-vocabularydetectionusinguncuratedimages": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PromptDet: Towards Open-Vocabulary Detection Using Uncurated Images",
    "authors": [
      "Chengjian Feng",
      "Yujie Zhong",
      "Zequn Jie",
      "Xiangxiang Chu",
      "Haibing Ren",
      "Xiaolin Wei",
      "Weidi Xie",
      "Lin Ma"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4001_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690691.pdf",
    "published": "2020-08",
    "summary": "\"The goal of this work is to establish a scalable pipeline for expanding an object detector towards novel/unseen categories, using zero manual annotations. To achieve that, we make the following four contributions: (i) in pursuit of generalisation, we propose a two-stage open-vocabulary object detector, where the class-agnostic object proposals are classified with a text encoder from pre-trained visual-language model; (ii) To pair the visual latent space (of RPN box proposals) with that of the pre-trained text encoder, we propose the idea of regional prompt learning to align the textual embedding space with regional visual object features; (iii) To scale up the learning procedure towards detecting a wider spectrum of objects, we exploit the available online resource via a novel self-training framework, which allows to train the proposed detector on a large corpus of noisy uncurated web images. Lastly, (iv) to evaluate our proposed detector, termed as PromptDet, we conduct extensive experiments on the challenging LVIS and MS-COCO dataset. PromptDet shows superior performance over existing approaches with fewer additional training images and zero manual annotations whatsoever. Project page with code: https://fcjian.github.io/promptdet.\""
  },
  "eccv2022_main_denselyconstraineddepthestimatorformonocular3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Densely Constrained Depth Estimator for Monocular 3D Object Detection",
    "authors": [
      "Yingyan Li",
      "Yuntao Chen",
      "Jiawei He",
      "Zhaoxiang Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4005_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690708.pdf",
    "published": "2020-08",
    "summary": "\"Estimating accurate 3D locations of objects from monocular images is a challenging problem because of lacking depth. Previous work shows that utilizing the object\u2019s keypoint projection constraints to estimate multiple depth candidates boosts the detection performance. However, the existing methods can only utilize vertical edges as projection constraints for depth estimation. So these methods only use a small number of projection constraints and produce insufficient depth candidates, leading to inaccurate depth estimation. In this paper, we propose a method that utilizes dense projection constraints from edges of any direction. In this way, we employ much more projection constraints and produce considerable depth candidates. Besides, we present a graph matching weighting module to merge the depth candidates. The proposed method DCD (Densely Constrained Detector) achieves state-of-the-art performance on the KITTI and WOD benchmarks. Code is released at https://github.com/BraveGroup/DCD.\""
  },
  "eccv2022_main_polarimetricposeprediction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Polarimetric Pose Prediction",
    "authors": [
      "Daoyi Gao",
      "Yitong Li",
      "Patrick Ruhkamp",
      "Iuliia Skobleva",
      "Magdalena Wysocki",
      "HyunJun Jung",
      "Pengyuan Wang",
      "Arturo Guridi",
      "Benjamin Busam"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4105_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690726.pdf",
    "published": "2020-08",
    "summary": "\"Light has many properties that vision sensors can passively measure. Colour-band separated wavelength and intensity are arguably the most commonly used for monocular 6D object pose estimation. This paper explores how complementary polarisation information, i.e. the orientation of light wave oscillations, influences the accuracy of pose predictions. A hybrid model that leverages physical priors jointly with a data-driven learning strategy is designed and carefully tested on objects with different levels of photometric complexity. Our design significantly improves the pose accuracy compared to state-of-the-art photometric approaches and enables object pose estimation for highly reflective and transparent objects. A new multi-modal instance-level 6D object pose dataset with highly accurate pose annotations for multiple objects with varying photometric complexity is introduced as a benchmark.\""
  },
  "eccv2022_main_dfnetenhanceabsoluteposeregressionwithdirectfeaturematching": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DFNet: Enhance Absolute Pose Regression with Direct Feature Matching",
    "authors": [
      "Shuai Chen",
      "Xinghui Li",
      "Zirui Wang",
      "Victor Adrian Prisacariu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4115_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700001.pdf",
    "published": "2020-08",
    "summary": "\"We introduce a camera relocalization pipeline that combines absolute pose regression (APR) and direct feature matching. By incorporating exposure-adaptive novel view synthesis, our method successfully addresses photometric distortions in outdoor environments that existing photometric-based methods fail to handle. With domain-invariant feature matching, our solution improves pose regression accuracy using semi-supervised learning on unlabeled data. In particular, the pipeline consists of two components: Novel View Synthesizer and DFNet. The former synthesizes novel views compensating for changes in exposure and the latter regresses camera poses and extracts robust features that close the domain gap between real images and synthetic ones. Furthermore, we introduce an online synthetic data generation scheme. We show that these approaches effectively enhance camera pose estimation both in indoor and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by outperforming existing single-image APR methods by as much as 56%, comparable to 3D structure-based methods.\""
  },
  "eccv2022_main_cornerformerpurifyinginstancesforcorner-baseddetectors": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Cornerformer: Purifying Instances for Corner-Based Detectors",
    "authors": [
      "Haoran Wei",
      "Xin Chen",
      "Lingxi Xie",
      "Qi Tian"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4286_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700017.pdf",
    "published": "2020-08",
    "summary": "\"Corner-based object detectors enjoy the potential of detecting arbitrarily-sized instances, yet the performance is mainly harmed by the accuracy of instance construction. Specifically, there are three factors, namely, 1) the corner keypoints are prone to false-positives; 2) incorrect matches emerge upon corner keypoint pull-push embeddings; and 3) the heuristic NMS cannot adjust the corners pull-push mechanism. Accordingly, this paper presents an elegant framework named Cornerformer that is composed of two factors. First, we build a Corner Transformer Encoder (CTE, a self-attention module) in a 2D-form to enhance the information aggregated by corner keypoints, offering stronger features for the pull-push loss to distinguish instances from each other. Second, we design an Attenuation-Auto-Adjusted NMS (A3-NMS) to maximally leverage the semantic outputs and avoid true objects from being removed. Experiments on object detection and human pose estimation show the superior performance of Cornerformer in terms of accuracy and inference speed.\""
  },
  "eccv2022_main_pillarnetreal-timeandhigh-performancepillar-based3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PillarNet: Real-Time and High-Performance Pillar-Based 3D Object Detection",
    "authors": [
      "Guangsheng Shi",
      "Ruifeng Li",
      "Chao Ma"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4346_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700034.pdf",
    "published": "2020-08",
    "summary": "\"Real-time and high-performance 3D object detection is of critical importance for autonomous driving. Recent top-performing 3D object detectors mainly rely on point-based or 3D voxel-based convolutions, which are both computationally inefficient for onboard deployment. In contrast, pillar-based methods use solely 2D convolutions, which consume less computation resources, but they lag far behind their voxel-based counterparts in detection accuracy. In this paper, by examining the primary performance gap between pillar- and voxel-based detectors, we develop a real-time and high-performance pillar-based detector, dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network for effective pillar feature learning, a neck network for spatial-semantic feature fusion and the commonly used detect head. Using only 2D convolutions, PillarNet is flexible to an optional pillar size and compatible with classical 2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits from our designed orientation-decoupled IoU regression loss along with the IoU-aware prediction branch. Extensive experimental results on large-scale nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet performs well over the state-of-the-art 3D detectors in terms of effectiveness and efficiency.\""
  },
  "eccv2022_main_robustobjectdetectionwithinaccurateboundingboxes": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Robust Object Detection with Inaccurate Bounding Boxes",
    "authors": [
      "Chengxin Liu",
      "Kewei Wang",
      "Hao Lu",
      "Zhiguo Cao",
      "Ziming Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4348_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700052.pdf",
    "published": "2020-08",
    "summary": "\"Learning accurate object detectors often requires large-scale training data with precise object bounding boxes. However, labeling such data is expensive and time-consuming. As the crowd-sourcing labeling process and the ambiguities of the objects may raise noisy bounding box annotations, the object detectors will suffer from the degenerated training data. In this work, we aim to address the challenge of learning robust object detectors with inaccurate bounding boxes. Inspired by the fact that localization precision suffers significantly from inaccurate bounding boxes while classification accuracy is less affected, we propose leveraging classification as a guidance signal for refining localization results. Specifically, by treating an object as a bag of instances, we introduce an Object-Aware Multiple Instance Learning approach (OA-MIL), featured with object-aware instance selection and object-aware instance extension. The former aims to select accurate instances for training, instead of directly using inaccurate box annotations. The latter focuses on generating high-quality instances for selection. Extensive experiments on synthetic noisy datasets (i.e., noisy PASCAL VOC and MS-COCO) and a real noisy wheat head dataset demonstrate the effectiveness of our OA-MIL. Code is available at https://github.com/cxliu0/OA-MIL.\""
  },
  "eccv2022_main_efficientdecoder-freeobjectdetectionwithtransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Efficient Decoder-Free Object Detection with Transformers",
    "authors": [
      "Peixian Chen",
      "Mengdan Zhang",
      "Yunhang Shen",
      "Kekai Sheng",
      "Yuting Gao",
      "Xing Sun",
      "Ke Li",
      "Chunhua Shen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4433_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700069.pdf",
    "published": "2020-08",
    "summary": "\"Vision transformers (ViTs) are changing the landscape of object detection tasks. A natural usage of ViTs in detection is to replace the CNN-based backbone with a transformer-based backbone, which is simple yet brings an enormous computation burden during inference. More subtle usage is the DETR family, which eliminates the need for many hand-designed components in object detection but introduces a decoder demanding an extra-long time to converge. As a result, transformer-based object detection could not prevail in large-scale applications. To overcome these issues, we propose a novel decoder-free fully transformer-based (DFFT) object detector, achieving high efficiency in both training and inference stages for the first time. We simplify objection detection to an encoder-only single-level anchor-based dense prediction problem by centering around two entry points: 1) Eliminate the training-inefficient decoder and leverage two strong encoders to preserve the accuracy of single-level feature map prediction; 2) Explore low-level semantic features for the detection task with limited computational resources. In particular, we design a novel lightweight detection-oriented transformer backbone that efficiently captures low-level features with rich semantics based on a well-conceived ablation study. Extensive experiments on the MS COCO benchmark demonstrate that DFFT{SMALL} outperforms DETR by 2.5% AP with 28% computation cost reduction and more than 10X fewer training epochs. Compared with the cutting-edge anchor-based detector RetinaNet, DFFT{SMALL} obtains over 5.5% AP gain while cutting down 70% computation cost.\""
  },
  "eccv2022_main_cross-modalityknowledgedistillationnetworkformonocular3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Cross-Modality Knowledge Distillation Network for Monocular 3D Object Detection",
    "authors": [
      "Yu Hong",
      "Hang Dai",
      "Yong Ding"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4878_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700085.pdf",
    "published": "2020-08",
    "summary": "\"Leveraging LiDAR-based detectors or real LiDAR point data to guide monocular 3D detection has brought significant improvement, e.g., Pseudo-LiDAR methods. However, the existing methods usually apply non-end-to-end training strategies and insufficiently leverage the LiDAR information, where the rich potential of the LiDAR data has not been well exploited. In this paper, we propose the Cross-Modality Knowledge Distillation (CMKD) network for monocular 3D detection to efficiently and directly transfer the knowledge from LiDAR modality to image modality on both features and responses. Moreover, we further extend CMKD as a semi-supervised training framework by distilling knowledge from large-scale unlabeled data and significantly boost the performance. Until submission, CMKD ranks 1st among the monocular 3D detectors with publications on both KITTI test set and Waymo val set with significant performance gains compared to previous state-of-the-art methods. Our code will be released at https://github.com/Cc-Hy/CMKD.\""
  },
  "eccv2022_main_reacttemporalactiondetectionwithrelationalqueries": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "ReAct: Temporal Action Detection with Relational Queries",
    "authors": [
      "Dingfeng Shi",
      "Yujie Zhong",
      "Qiong Cao",
      "Jing Zhang",
      "Lin Ma",
      "Jia Li",
      "Dacheng Tao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4975_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700102.pdf",
    "published": "2020-08",
    "summary": "\"This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component.\""
  },
  "eccv2022_main_towardsaccurateactivecameralocalization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards Accurate Active Camera Localization",
    "authors": [
      "Qihang Fang",
      "Yingda Yin",
      "Qingnan Fan",
      "Fei Xia",
      "Siyan Dong",
      "Sheng Wang",
      "Jue Wang",
      "Leonidas J. Guibas",
      "Baoquan Chen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5136_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700119.pdf",
    "published": "2020-08",
    "summary": "\"In this work, we tackle the problem of active camera localization, which controls the camera movements actively to achieve an accurate camera pose. The past solutions are mostly based on Markov Localization, which reduces the position-wise camera uncertainty for localization. These approaches localize the camera in the discrete pose space and are agnostic to the localization-driven scene property, which restricts the camera pose accuracy in the coarse scale. We propose to overcome these limitations via a novel active camera localization algorithm, composed of a passive and an active localization module. The former optimizes the camera pose in the continuous pose space by establishing point-wise camera-world correspondences. The latter explicitly models the scene and camera uncertainty components to plan the right path for accurate camera pose estimation. We validate our algorithm on the challenging localization scenarios from both synthetic and scanned real-world indoor scenes. Experimental results demonstrate that our algorithm outperforms both the state-of-the-art Markov Localization based approach and other compared approaches on the fine-scale camera pose accuracy. Code and data are released at https://github.com/qhFang/AccurateACL.\""
  },
  "eccv2022_main_cameraposeauto-encodersforimprovingposeregression": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Camera Pose Auto-Encoders for Improving Pose Regression",
    "authors": [
      "Yoli Shavit",
      "Yosi Keller"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5158_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700137.pdf",
    "published": "2020-08",
    "summary": "\"Absolute pose regressor (APR) networks are trained to estimate the pose of the camera given a captured image. They compute latent image representations from which the camera position and orientation are regressed. APRs provide a different tradeoff between localization accuracy, runtime, and memory, compared to structure-based localization schemes that provide state-of-the-art accuracy. In this work, we introduce Camera Pose Auto-Encoders (PAEs), multilayer perceptrons that are trained via a Teacher-Student approach to encode camera poses using APRs as their teachers. We show that the resulting latent pose representations can closely reproduce APR performance and demonstrate their effectiveness for related tasks. Specifically, we propose a light-weight test-time optimization in which the closest train poses are encoded and used to refine camera position estimation. This procedure achieves a new state-of-the-art position accuracy for APRs, on both the CambridgeLandmarks and 7Scenes benchmarks. We also show that train images can be reconstructed from the learned pose encoding, paving the way for integrating visual information from the train set at a low memory cost. Our code and pre-trained models are available at https://github.com/yolish/camera-pose-auto-encoders.\""
  },
  "eccv2022_main_improvingtheintra-classlong-tailin3ddetectionviarareexamplemining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Improving the Intra-Class Long-Tail in 3D Detection via Rare Example Mining",
    "authors": [
      "Chiyu Max Jiang",
      "Mahyar Najibi",
      "Charles R. Qi",
      "Yin Zhou",
      "Dragomir Anguelov"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5325_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700155.pdf",
    "published": "2020-08",
    "summary": "\"Continued improvements in deep learning architectures have steadily advanced the overall performance of 3D object detectors to levels on par with humans for certain tasks and datasets, where the overall performance is mostly driven by common examples. However, even the best performing models suffer from the most naive mistakes when it comes to rare examples that do not appear frequently in the training data, such as vehicles with irregular geometries. Most studies in the long-tail literature focus on class-imbalanced classification problems with known imbalanced label counts per class, but they are not directly applicable to the intra-class long-tail examples in problems with large intra-class variations such as 3D object detection, where instances with the same class label can have drastically varied properties such as shapes and sizes. Other works propose to mitigate this problem using active learning based on the criteria of uncertainty, difficulty, or diversity. In this study, we identify a new conceptual dimension - rareness - to mine new data for improving the long-tail performance of models. We show that rareness, as opposed to difficulty, is the key to data-centric improvements for 3D detectors, since rareness is the result of a lack in data support while difficulty is related to the fundamental ambiguity in the problem. We propose a general and effective method to identify the rareness of objects based on density estimation in the feature space using flow models, and propose a principled cost-aware formulation for mining rare object tracks, which improves overall model performance, but more importantly - significantly improves the performance for rare objects (by 30.97%).\""
  },
  "eccv2022_main_baggingregionalclassificationactivationmapsforweaklysupervisedobjectlocalization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Bagging Regional Classification Activation Maps for Weakly Supervised Object Localization",
    "authors": [
      "Lei Zhu",
      "Qian Chen",
      "Lujia Jin",
      "Yunfei You",
      "Yanye Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5508_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700174.pdf",
    "published": "2020-08",
    "summary": "\"Classification activation map (CAM), utilizing the classification structure to generate pixel-wise localization maps, is a crucial mechanism for weakly supervised object localization (WSOL). However, CAM directly uses the classifier trained on image-level features to locate objects, making it prefers to discern global discriminative factors rather than regional object cues. Thus only the discriminative locations are activated when feeding pixel-level features into this classifier. To solve this issue, this paper elaborates a plug-and-play mechanism called BagCAMs to better project a well-trained classifier for the localization task without refining or re-training the baseline structure. Our BagCAMs adopts a proposed regional localizer generation (RLG) strategy to define a set of regional localizers and then derive them from a well-trained classifier. These regional localizers can be viewed as the base learner that only discerns region-wise object factors for localization tasks, and their results can be effectively weighted by our BagCAMs to form the final localization map. Experiments indicate that adopting our proposed BagCAMs can improve the performance of baseline WSOL methods to a great extent and obtains state-of-the-art performance on three WSOL benchmarks. Code are released at https://github.com/zh460045050/BagCAMs.\""
  },
  "eccv2022_main_uc-owodunknown-classifiedopenworldobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "UC-OWOD: Unknown-Classified Open World Object Detection",
    "authors": [
      "Zhiheng Wu",
      "Yue Lu",
      "Xingyu Chen",
      "Zhengxing Wu",
      "Liwen Kang",
      "Junzhi Yu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5661_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700191.pdf",
    "published": "2020-08",
    "summary": "\"Open World Object Detection (OWOD) is a challenging computer vision problem that requires detecting unknown objects and gradually learning the identified unknown classes. However, it cannot distinguish unknown instances as multiple unknown classes. In this work, we propose a novel OWOD problem called Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to detect unknown instances and classify them into different unknown classes. Besides, we formulate the problem and devise a two-stage object detector to solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative classification head are used to detect known and unknown objects. Then, similarity-based unknown classification and unknown clustering refinement modules are constructed to distinguish multiple unknown classes. Moreover, two novel evaluation protocols are designed to evaluate unknown-class detection. Abundant experiments and visualizations prove the effectiveness of the proposed method. Code is available at https://github.com/JohnWuzh/UC-OWOD.\""
  },
  "eccv2022_main_raytran3dposeestimationandshapereconstructionofmultipleobjectsfromvideoswithray-tracedtransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "RayTran: 3D Pose Estimation and Shape Reconstruction of Multiple Objects from Videos with Ray-Traced Transformers",
    "authors": [
      "Micha\u0142 J. Tyszkiewicz",
      "Kevis-Kokitsi Maninis",
      "Stefan Popov",
      "Vittorio Ferrari"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5669_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700209.pdf",
    "published": "2020-08",
    "summary": "\"We propose a transformer-based neural network architecture for multi-object 3D reconstruction from RGB videos. It relies on two alternative ways to represent its knowledge: as a global 3D grid of features and an array of view-specific 2D grids. We progressively exchange information between the two with a dedicated bidirectional attention mechanism. We exploit knowledge about the image formation process to significantly sparsify the attention weight matrix, making our architecture feasible on current hardware, both in terms of memory and computation. We attach a DETR-style head on top of the 3D feature grid in order to detect the objects in the scene and to predict their 3D pose and 3D shape. Compared to previous methods, our architecture is single stage, end-to-end trainable, and it can reason holistically about a scene from multiple video frames without needing a brittle tracking step. We evaluate our method on the challenging Scan2CAD dataset, where we outperform (1) recent state-of-the-art methods for 3D object pose estimation from RGB videos; and (2) a strong alternative method combining Multi-view Stereo with RGB-D CAD alignment. We plan to release our source code.\""
  },
  "eccv2022_main_gtcargraphtransformerforcamerare-localization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "GTCaR: Graph Transformer for Camera Re-Localization",
    "authors": [
      "Xinyi Li",
      "Haibin Ling"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5761_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700227.pdf",
    "published": "2020-08",
    "summary": "\"Camera re-localization or absolute pose regression is the centerpiece in numerous computer vision tasks such as visual odometry, structure from motion (SfM) and SLAM. In this paper we propose a neural network approach with a graph Transformer backbone, namely GTCaR (Graph Transformer for Camera Re-localization), to address the multi-view camera re-localization problem. In contrast with prior work where the pose regression is mainly guided by photometric consistency, GTCaR effectively fuses the image features, camera pose information and inter-frame relative camera motions into encoded graph attributes and is trained towards the graph consistency and pose accuracy combined instead, yielding significantly higher computational efficiency. By leveraging graph Transformer layers with edge features and enabling the adjacency tensor, GTCaR dynamically captures the global attention and thus endows the pose graph with evolving structures to achieve improved robustness and accuracy. In addition, optional temporal Transformer layers actively enhance the spatiotemporal inter-frame relation for sequential inputs. Evaluation of the proposed network on various public benchmarks demonstrates that GTCaR outperforms state-of-the-art approaches.\""
  },
  "eccv2022_main_3dobjectdetectionwithaself-supervisedlidarsceneflowbackbone": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Object Detection with a Self-Supervised Lidar Scene Flow Backbone",
    "authors": [
      "Eme\u00e7 Er\u00e7elik",
      "Ekim Yurtsever",
      "Mingyu Liu",
      "Zhijie Yang",
      "Hanzhen Zhang",
      "P\u0131nar Top\u00e7am",
      "Maximilian Listl",
      "Y\u0131lmaz Kaan \u00c7ayl\u0131",
      "Alois Knoll"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5763_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700244.pdf",
    "published": "2020-08",
    "summary": "\"State-of-the-art lidar-based 3D object detection methods rely on supervised learning and large labeled datasets. However, annotating lidar data is resource-consuming, and depending only on supervised learning limits the applicability of trained models. Self-supervised training strategies can alleviate these issues by learning a general point cloud backbone model for downstream 3D vision tasks. Against this backdrop, we show the relationship between self-supervised multi-frame flow representations and single-frame 3D detection hypotheses. Our main contribution leverages learned flow and motion representations and combines a self-supervised backbone with a supervised 3D detection head. First, a self-supervised scene flow estimation model is trained with cycle consistency. Then, the point cloud encoder of this model is used as the backbone of a single-frame 3D object detection head model. This second 3D object detection model learns to utilize motion representations to distinguish dynamic objects exhibiting different movement patterns. Experiments on KITTI and nuScenes benchmarks show that the proposed self-supervised pre-training increases 3D detection performance significantly. https://github.com/emecercelik/ssl-3d-detection.git\""
  },
  "eccv2022_main_openvocabularyobjectdetectionwithpseudobounding-boxlabels": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Open Vocabulary Object Detection with Pseudo Bounding-Box Labels",
    "authors": [
      "Mingfei Gao",
      "Chen Xing",
      "Juan Carlos Niebles",
      "Junnan Li",
      "Ran Xu",
      "Wenhao Liu",
      "Caiming Xiong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5913_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700263.pdf",
    "published": "2020-08",
    "summary": "\"Despite great progress in object detection, most existing methods work only on a limited set of object categories, due to the tremendous human effort needed for bounding-box annotations of training data. To alleviate the problem, recent open vocabulary and zero-shot detection methods attempt to detect novel object categories beyond those seen during training. They achieve this goal by training on a pre-defined base categories to induce generalization to novel objects. However, their potential is still constrained by the small set of base categories available for training. To enlarge the set of base classes, we propose a method to automatically generate pseudo bounding-box annotations of diverse objects from large-scale image-caption pairs. Our method leverages the localization ability of pre-trained vision-language models to generate pseudo bounding-box labels and then directly uses them for training object detectors. Experimental results show that our method outperforms the state-of-the-art open vocabulary detector by 8% AP on COCO novel categories, by 6.3% AP on PASCAL VOC, by 2.3% AP on Objects365 and by 2.8% AP on LVIS. Code is available at https://github.com/salesforce/PB-OVD.\""
  },
  "eccv2022_main_few-shotobjectdetectionbyknowledgedistillationusingbag-of-visual-wordsrepresentations": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Few-Shot Object Detection by Knowledge Distillation Using Bag-of-Visual-Words Representations",
    "authors": [
      "Wenjie Pei",
      "Shuang Wu",
      "Dianwen Mei",
      "Fanglin Chen",
      "Jiandong Tian",
      "Guangming Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6004_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700279.pdf",
    "published": "2020-08",
    "summary": "\"While fine-tuning based methods for few-shot object detection have achieved remarkable progress, a crucial challenge that has not been addressed well is the potential class-specific overfitting on base classes and sample-specific overfitting on novel classes. In this work we design a novel knowledge distillation framework to guide the learning of the object detector and thereby restrain the overfitting in both the pre-training stage on base classes and fine-tuning stage on novel classes. To be specific, we first present a novel Position-Aware Bag-of-Visual-Words model for learning a representative bag of visual words (BoVW) from a limited size of image set, which is used to encode general images based on the similarities between the learned visual words and an image. Then we perform knowledge distillation based on the fact that an image should have consistent BoVW representations in two different feature spaces. To this end, we pre-learn a feature space independently from the object detection, and encode images using BoVW in this space. The obtained BoVW representation for an image can be considered as distilled knowledge to guide the learning of object detector: the extracted features by the object detector for the same image are expected to derive the consistent BoVW representations with the distilled knowledge. Extensive experiments validate the effectiveness of our method and demonstrate the superiority over other state-of-the-art methods.\""
  },
  "eccv2022_main_salisasaliency-basedinputsamplingforefficientvideoobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SALISA: Saliency-Based Input Sampling for Efficient Video Object Detection",
    "authors": [
      "Babak Ehteshami Bejnordi",
      "Amirhossein Habibian",
      "Fatih Porikli",
      "Amir Ghodrati"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6037_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700296.pdf",
    "published": "2020-08",
    "summary": "\"High-resolution images are widely adopted for high-performance object detection in videos. However, processing high-resolution inputs comes with high computation costs, and naive down-sampling of the input to reduce the computation costs quickly degrades the detection performance. In this paper, we propose SALISA, a novel non-uniform SALiency-based Input SAmpling technique for video object detection that allows for heavy down-sampling of unimportant background regions while preserving the fine-grained details of a high-resolution image. The resulting image is spatially smaller, leading to reduced computational costs while enabling a performance comparable to a high-resolution input. To achieve this, we propose a differentiable resampling module based on a thin plate spline spatial transformer network (TPS-STN). This module is regularized by a novel loss to provide an explicit supervision signal to learn to \u201cmagnify\u201d salient regions. We report state-of-the-art results in the low compute regime on the ImageNet-VID and UA-DETRAC video object detection datasets. We demonstrate that on both datasets, the mAP of an EfficientDet-D1 (EfficientDet-D2) gets on par with EfficientDet-D2 (EfficientDet-D3) at a much lower computational cost. We also show that SALISA significantly improves the detection of small objects. In particular, SALISA with an EfficientDet-D1 detector improves the detection of small objects by 77%, and remarkably also outperforms EfficientDet-D3 baseline.\""
  },
  "eccv2022_main_eco-trefficientcorrespondencesfindingviacoarse-to-finerefinement": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "ECO-TR: Efficient Correspondences Finding via Coarse-to-Fine Refinement",
    "authors": [
      "Dongli Tan",
      "Jiang-Jiang Liu",
      "Xingyu Chen",
      "Chao Chen",
      "Ruixin Zhang",
      "Yunhang Shen",
      "Shouhong Ding",
      "Rongrong Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6054_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700313.pdf",
    "published": "2020-08",
    "summary": "\"Abstract. Modeling sparse and dense image matching within a unified functional model has recently attracted increasing research interest. However, existing efforts mainly focus on improving matching accuracy while ignoring its efficiency, which is crucial for real-world applications. In this paper, we propose an efficient structure named Efficient Correspondence Transformer (ECO-TR) by finding correspondences in a coarse-to-fine manner, which significantly improves the efficiency of functional model. To achieve this, multiple transformer blocks are stage-wisely connected to gradually refine the predicted coordinates upon a shared multi-scale feature extraction network. Given a pair of images and for arbitrary query coordinates, all the correspondences are predicted within a single feed-forward pass. We further propose an adaptive query-clustering strategy and an uncertainty-based outlier detection module to cooperate with the proposed framework for faster and better predictions. Experiments on various sparse and dense matching tasks demonstrate the superiority of our method in both efficiency and effectiveness against existing state-of-the-arts. Project page: https://dltan7.github.io/ecotr/.\""
  },
  "eccv2022_main_votefromthecenter6dofposeestimationinrgb-dimagesbyradialkeypointvoting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting",
    "authors": [
      "Yangzheng Wu",
      "Mohsen Zand",
      "Ali Etemad",
      "Michael Greenspan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6108_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700331.pdf",
    "published": "2020-08",
    "summary": "\"We propose a novel keypoint voting scheme based on intersecting spheres, that is more accurate than existing schemes and allows for fewer, more disperse keypoints. The scheme is based upon the distance between points, which as a 1D quantity can be regressed more accurately than the 2D and 3D vector and offset quantities regressed in previous work, yielding more accurate keypoint localization. The scheme forms the basis of the proposed RCVPose method for 6 DoF pose estimation of 3D objects in RGB-D data, which is particularly effective at handling occlusions. A CNN is trained to estimate the distance between the 3D point corresponding to the depth mode of each RGB pixel, and a set of 3 disperse keypoints defined in the object frame. At inference, a sphere centered at each 3D point is generated, of radius equal to this estimated distance. The surfaces of these spheres vote to increment a 3D accumulator space, the peaks of which indicate keypoint locations. The proposed radial voting scheme is more accurate than previous vector or offset schemes, and is robust to disperse keypoints. Experiments demonstrate RCVPose to be highly accurate and competitive, achieving state-of-theart results on the LINEMOD (99.7%) and YCB-Video (97.2%) datasets, notably scoring +4.9% higher (71.1%) than previous methods on the challenging Occlusion LINEMOD dataset, and on average outperforming all other published results from the BOP benchmark for these 3 datasets. Our code is available at http://www.github.com/aaronwool/rcvpose.\""
  },
  "eccv2022_main_long-tailedinstancesegmentationusinggumbeloptimizedloss": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Long-Tailed Instance Segmentation Using Gumbel Optimized Loss",
    "authors": [
      "Konstantinos Panagiotis Alexandridis",
      "Jiankang Deng",
      "Anh Nguyen",
      "Shan Luo"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6148_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700349.pdf",
    "published": "2020-08",
    "summary": "\"Major advancements have been made in the field of object detection and segmentation recently. However, when it comes to rare categories, the state-of-the-art methods fail to detect them, resulting in a significant performance gap between rare and frequent categories. In this paper, we identify that Sigmoid or Softmax functions used in deep detectors are a major reason for low performance and are suboptimal for long-tailed detection and segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for long-tailed detection and segmentation. It aligns with the Gumbel distribution of rare classes in imbalanced datasets, considering the fact that most classes in long-tailed detection have low expected probability. The proposed GOL significantly outperforms the best state-of-the-art method by 1.1% on AP, and boosts the overall segmentation by 9.0% and detection by 8.0%, particularly improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS dataset. Code available at: https://github.com/kostas1515/ GOL.\""
  },
  "eccv2022_main_detmatchtwoteachersarebetterthanoneforjoint2dand3dsemi-supervisedobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "DetMatch: Two Teachers Are Better than One for Joint 2D and 3D Semi-Supervised Object Detection",
    "authors": [
      "Jinhyung Park",
      "Chenfeng Xu",
      "Yiyang Zhou",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6162_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700366.pdf",
    "published": "2020-08",
    "summary": "\"While numerous 3D detection works leverage the complementary relationship between RGB images and point clouds, developments in the broader framework of semi-supervised object recognition remain uninfluenced by multi-modal fusion. Current methods develop independent pipelines for 2D and 3D semi-supervised learning despite the availability of paired image and point cloud frames. Observing that the distinct characteristics of each sensor cause them to be biased towards detecting different objects, we propose DetMatch, a flexible framework for joint semi-supervised learning on 2D and 3D modalities. By identifying objects detected in both sensors, our pipeline generates a cleaner, more robust set of pseudo-labels that both demonstrates stronger performance and stymies single-modality error propagation. Further, we leverage the richer semantics of RGB images to rectify incorrect 3D class predictions and improve localization of 3D boxes. Evaluating our method on the challenging KITTI and Waymo datasets, we improve upon strong semi-supervised learning methods and observe higher quality pseudo-labels. Code will be released here: https://github.com/Divadi/DetMatch.\""
  },
  "eccv2022_main_objectboxfromcenterstoboxesforanchor-freeobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "ObjectBox: From Centers to Boxes for Anchor-Free Object Detection",
    "authors": [
      "Mohsen Zand",
      "Ali Etemad",
      "Michael Greenspan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6191_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700385.pdf",
    "published": "2020-08",
    "summary": "\"We present ObjectBox, a novel single-stage anchor-free and highly generalizable object detection approach. As opposed to both existing anchor-based and anchor-free detectors, which are more biased toward specific object scales in their label assignments, we use only object center locations as positive samples and treat all objects equally in different feature levels regardless of the objects\u2019 sizes or shapes. Specifically, our label assignment strategy considers the object center locations as shape- and size-agnostic anchors in an anchor-free fashion, and allows learning to occur at all scales for every object. To support this, we define new regression targets as the distances from two corners of the center cell location to the four sides of the bounding box. Moreover, to handle scale-variant objects, we propose a tailored IoU loss to deal with boxes with different sizes. As a result, our proposed object detector does not need any dataset-dependent hyperparameters to be tuned across datasets. We evaluate our method on MS-COCO 2017 and PASCAL VOC 2012 datasets, and compare our results to state-of-the-art methods. We observe that ObjectBox performs favorably in comparison to prior works. Furthermore, we perform rigorous ablation experiments to evaluate different components of our method. Our code is available at: https://github.com/MohsenZand/ObjectBox.\""
  },
  "eccv2022_main_isgeometryenoughformatchinginvisuallocalization?": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Is Geometry Enough for Matching in Visual Localization?",
    "authors": [
      "Qunjie Zhou",
      "S\u00e9rgio Agostinho",
      "Aljo\u0161a O\u0161ep",
      "Laura Leal-Taix\u00e9"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6212_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700402.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we propose to go beyond the well-established approach to vision-based localization that relies on visual descriptor matching between a query image and a 3D point cloud. While matching keypoints via visual descriptors makes localization highly accurate, it has significant storage demands, raises privacy concerns and requires update to the descriptors in the long-term. To elegantly address those practical challenges for large-scale localization, we present GoMatch, an alternative to visual-based matching that solely relies on geometric information for matching image keypoints to maps, represented as sets of bearing vectors. Our novel bearing vectors representation of 3D points, significantly relieves the cross-modal challenge in geometric-based matching that prevented prior work to tackle localization in a realistic environment. With additional careful architecture design, GoMatch improves over prior geometric-based matching work with a reduction of (10.67m,95.7deg) and (1.43m, 34.7deg) in average median pose errors on Cambridge Landmarks and 7-Scenes, while requiring as little as 1.5/1.7% of storage capacity in comparison to the best visual-based matching methods. This confirms its potential and feasibility for real-world localization and opens the door to future efforts in advancing city-scale visual localization methods that do not require storing visual descriptors.\""
  },
  "eccv2022_main_swformersparsewindowtransformerfor3dobjectdetectioninpointclouds": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds",
    "authors": [
      "Pei Sun",
      "Mingxing Tan",
      "Weiyue Wang",
      "Chenxi Liu",
      "Fei Xia",
      "Zhaoqi Leng",
      "Dragomir Anguelov"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6236_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700422.pdf",
    "published": "2020-08",
    "summary": "\"3D object detection in point clouds is a core component for modern robotics and autonomous driving systems. A key challenge in 3D object detection comes from the inherent sparse nature of point occupancy within the 3D scene. In this paper, we propose Sparse Window Transformer (SWFormer ), a scalable and accurate model for 3D object detection, which can take full advantage of the sparsity of point clouds. Built upon the idea of Swin Transformers, SWFormer operates on the sparse voxels and processes variable-length sparse windows efficiently using a bucketing scheme. In addition to self-attention within each spatial window, our SWFormer also captures cross-window correlation with multi-scale feature fusion and window shifting operations. To further address the unique challenge of detecting 3D objects accurately from sparse features, we propose a new voxel diffusion technique. Experimental results on the Waymo Open Dataset show our SWFormer achieves state-of-the-art 73.36 L2 mAPH on vehicle and pedestrian for 3D object detection on the official test set, outperforming all previous single-stage and two-stage models.\""
  },
  "eccv2022_main_pcr-cgpointcloudregistrationviadeepexplicitcolorandgeometry": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry",
    "authors": [
      "Yu Zhang",
      "Junle Yu",
      "Xiaolin Huang",
      "Wenhui Zhou",
      "Ji Hou"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6316_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700439.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we introduce PCR-CG: a novel 3D point cloud registration module explicitly embedding the color signals into geometry representation. Different from the previous methods that used only geometry representation, our module is specifically designed to effectively correlate color and geometry for the point cloud registration task. Our key contribution is a 2D-3D cross-modality learning algorithm that embeds the features learned from color signals to the geometry representation. With our designed 2D-3D projection module, the pixel features in a square region centered at correspondences perceived from images are effectively correlated with point cloud representations. In this way, the overlap regions can be inferred not only from point cloud but also from the texture appearances. Adding color is non-trivial. We compare against a variety of baselines designed for adding color to 3D, such as exhaustively adding per-pixel features or RGB values in an implicit manner. We leverage Predator as our baseline method and incorporate our module into it. Our experimental results indicate a significant improvement on the 3DLoMatch benchmark. With the help of our module, we achieve a significant improvement of 6.5% registration recall over our baseline method. To validate the effectiveness of 2D features on 3D, we ablate different 2D pre-trained networks and show a positive correlation between the pre-trained weights and task performance. Our study reveals a significant advantage of correlating explicit deep color features to the point cloud in the registration task.\""
  },
  "eccv2022_main_glamdglobalandlocalattentionmaskdistillationforobjectdetectors": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "GLAMD: Global and Local Attention Mask Distillation for Object Detectors",
    "authors": [
      "Younho Jang",
      "Wheemyung Shin",
      "Jinbeom Kim",
      "Simon Woo",
      "Sung-Ho Bae"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6328_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700456.pdf",
    "published": "2020-08",
    "summary": "\"Knowledge distillation (KD) is a well-known model compression strategy to improve models\u2019 performance with fewer parameters. However, recent KD approaches for object detection have faced two limitations. First, they distill nearby foreground regions, ignoring potentially useful background information. Second, they only consider global contexts, thereby the student model can hardly learn local details from the teacher model. To overcome such challenging issues, we propose a novel knowledge distillation method, GLAMD, distilling both global and local knowledge from the teacher. We divide the feature maps into several patches and apply an attention mechanism for both the entire feature area and each patch to extract the global context as well as local details simultaneously. Our method outperforms the state-of-the-art methods with 40.8 AP on COCO2017 dataset, which is 3.4 AP higher than the student model (ResNet50 based Faster R-CNN) and 0.7 AP higher than the previous global attention-based distillation method.\""
  },
  "eccv2022_main_fcaf3dfullyconvolutionalanchor-free3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection",
    "authors": [
      "Danila Rukhovich",
      "Anna Vorontsova",
      "Anton Konushin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6356_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700473.pdf",
    "published": "2020-08",
    "summary": "\"Recently, promising applications in robotics and augmented reality have attracted considerable attention to 3D object detection from point clouds. In this paper, we present FCAF3D -- a first-in-class fully convolutional anchor-free indoor 3D object detection method. It is a simple yet effective method that uses a voxel representation of a point cloud and processes voxels with sparse convolutions. FCAF3D can handle large-scale scenes with minimal runtime through a single fully convolutional feed-forward pass. Existing 3D object detection methods make prior assumptions on the geometry of objects, and we argue that it limits their generalization ability. To eliminate prior assumptions, we propose a novel parametrization of oriented bounding boxes that allows obtaining better results in a purely data-driven way. The proposed method achieves state-of-the-art 3D object detection results in terms of mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets.The code and models are available at https://github.com/samsunglabs/fcaf3d\""
  },
  "eccv2022_main_videoanomalydetectionbysolvingdecoupledspatio-temporaljigsawpuzzles": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles",
    "authors": [
      "Guodong Wang",
      "Yunhong Wang",
      "Jie Qin",
      "Dongming Zhang",
      "Xiuguo Bao",
      "Di Huang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6451_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700490.pdf",
    "published": "2020-08",
    "summary": "\"Video Anomaly Detection (VAD) is an important topic in computer vision. Motivated by the recent advances in self-supervised learning, this paper addresses VAD by solving an intuitive yet challenging pretext task, i.e., spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained classification problem. Our method exhibits several advantages over existing works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial and temporal dimensions, responsible for capturing highly discriminative appearance and motion features, respectively; 2) full permutations are used to provide abundant jigsaw puzzles covering various difficulty levels, allowing the network to distinguish subtle spatio-temporal differences between normal and abnormal events; and 3) the pretext task is tackled in an end-to-end manner without relying on any pre-trained models. Our method outperforms state-of-the-art counterparts on three public benchmarks. Especially on ShanghaiTech Campus, the result is superior to reconstruction and prediction-based methods by a large margin.\""
  },
  "eccv2022_main_class-agnosticobjectdetectionwithmulti-modaltransformer": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Class-Agnostic Object Detection with Multi-modal Transformer",
    "authors": [
      "Muhammad Maaz",
      "Hanoona Rasheed",
      "Salman Khan",
      "Fahad Shahbaz Khan",
      "Rao Muhammad Anwer",
      "Ming-Hsuan Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6491_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700507.pdf",
    "published": "2020-08",
    "summary": "\"What constitutes an object? This has been a long-standing question in computer vision. Towards this goal, numerous learning-free and learning-based approaches have been developed to score objectness. However, they generally do not scale well across new domains and for unseen objects. In this paper, we advocate that existing methods lack a top-down supervision signal governed by human-understandable semantics. For the first time in literature, we demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned image-text pairs can effectively bridge this gap. Our extensive experiments across various domains and novel objects show the state-of-the-art performance of MViTs to localize generic objects in images. Based on the observation that existing MViTs do not include multi-scale feature processing and usually require longer training schedules, we develop an efficient and flexible MViT architecture using multi-scale and deformable self-attention. We show the significance of MViT proposals in a diverse range of applications including open-world object detection, salient and camouflage object detection, supervised and self-supervised detection tasks. Further, MViTs can adaptively generate proposals given a specific language query and thus offer enhanced interactability. Code: https://git.io/J1HPY.\""
  },
  "eccv2022_main_enhancingmulti-modalfeaturesusinglocalself-attentionfor3dobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Enhancing Multi-modal Features Using Local Self-Attention for 3D Object Detection",
    "authors": [
      "Hao Li",
      "Zehan Zhang",
      "Xian Zhao",
      "Yulong Wang",
      "Yuxi Shen",
      "Shiliang Pu",
      "Hui Mao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6955_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700527.pdf",
    "published": "2020-08",
    "summary": "\"LiDAR and Camera sensors have complementary properties: LiDAR senses accurate positioning, while camera provides rich texture and color information. Fusing these two modalities can intuitively improve the performance of 3D detection. Most multi-modal fusion methods use networks to extract features of LiDAR and camera modality respectively, then simply add or concancate them together. We argue that these two kinds of signals are completely different, so it is not proper to combine these two heterogeneous features directly. In this paper, we propose EMMF-Det to do multi-modal fusion leveraging range and camera images. EMMF-Det uses self-attention mechanism to do feature re-weighting on these two modalities interactively, which can enchance the features with color, texture and localiztion information provided by LiDAR and camera signals. On the Waymo Open Dataset, EMMF-Det acheives the state-of-the-art performance. Besides this, evaluation on self-built dataset further proves the effectiveness of our method.\""
  },
  "eccv2022_main_objectdetectionasprobabilisticsetprediction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Object Detection As Probabilistic Set Prediction",
    "authors": [
      "Georg Hess",
      "Christoffer Petersson",
      "Lennart Svensson"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6973_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700545.pdf",
    "published": "2020-08",
    "summary": "\"Accurate uncertainty estimates are essential for deploying deep object detectors in safety-critical systems. The development and evaluation of probabilistic object detectors have been hindered by shortcomings in existing performance measures, which tend to involve arbitrary thresholds or limit the detector\u2019s choice of distributions. In this work, we propose to view object detection as a set prediction task where detectors predict the distribution over the set of objects. Using the negative log-likelihood for random finite sets, we present a proper scoring rule for evaluating and training probabilistic object detectors. The proposed method can be applied to existing probabilistic detectors, is free from thresholds, and enables fair comparison between architectures. Three different types of detectors are evaluated on the COCO dataset. Our results indicate that the training of existing detectors is optimized toward non-probabilistic metrics. We hope to encourage the development of new object detectors that can accurately estimate their own uncertainty.\""
  },
  "eccv2022_main_weakly-supervisedtemporalactiondetectionforfine-grainedvideoswithhierarchicalatomicactions": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions",
    "authors": [
      "Zhi Li",
      "Lu He",
      "Huijuan Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6982_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700562.pdf",
    "published": "2020-08",
    "summary": "\"Action understanding has evolved into the era of fine granularity, as most human behaviors in real life have only minor differences. To detect these fine-grained actions accurately in a label-efficient way, we tackle the problem of weakly-supervised fine-grained temporal action detection in videos for the first time. Without the careful design to capture subtle differences between fine-grained actions, previous weakly-supervised models for general action detection cannot perform well in fine-grained settings. We propose to model actions as the combinations of reusable atomic actions which are automatically discovered from data through self-supervised clustering, in order to capture the commonality and individuality of fine-grained actions. The learnt atomic actions, represented by visual concepts, are further mapped to fine and coarse action labels leveraging the semantic label hierarchy. Our approach constructs a visual representation hierarchy of four levels: clip level, atomic action level, fine action class level and coarse action class level, with supervision at each level. Extensive experiments on two large-scale fine-grained video datasets, FineAction and FineGym, show the benefit of our proposed weakly-supervised model for fine-grained action detection, and it achieves state-of-the-art results.\""
  },
  "eccv2022_main_neuralcorrespondencefieldforobjectposeestimation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Neural Correspondence Field for Object Pose Estimation",
    "authors": [
      "Lin Huang",
      "Tomas Hodan",
      "Lingni Ma",
      "Linguang Zhang",
      "Luan Tran",
      "Christopher D. Twigg",
      "Po-Chen Wu",
      "Junsong Yuan",
      "Cem Keskin",
      "Robert Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7039_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700580.pdf",
    "published": "2020-08",
    "summary": "\"We propose a method for estimating the 6DoF pose of a rigid object with an available 3D model from a single RGB image. Unlike classical correspondence-based methods which predict 3D object coordinates at pixels of the input image, the proposed method predicts 3D object coordinates at 3D query points sampled in the camera frustum. The move from pixels to 3D points, which is inspired by recent PIFu-style methods for 3D reconstruction, enables reasoning about the whole object, including its (self-)occluded parts. For a 3D query point associated with a pixel-aligned image feature, we train a fully-connected neural network to predict: (i) the corresponding 3D object coordinates, and (ii) the signed distance to the object surface, with the first defined only for query points in the surface vicinity. We call the mapping realized by this network as Neural Correspondence Field. The object pose is then robustly estimated from the predicted 3D-3D correspondences by the Kabsch-RANSAC algorithm. The proposed method achieves state-of-the-art results on three BOP datasets and is shown superior especially in challenging cases with occlusion. The project website is at: linhuang17.github.io/NCF.\""
  },
  "eccv2022_main_onlabelgranularityandobjectlocalization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "On Label Granularity and Object Localization",
    "authors": [
      "Elijah Cole",
      "Kimberly Wilber",
      "Grant Van Horn",
      "Xuan Yang",
      "Marco Fornoni",
      "Pietro Perona",
      "Serge Belongie",
      "Andrew Howard",
      "Oisin Mac Aodha"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7044_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700598.pdf",
    "published": "2020-08",
    "summary": "\"Weakly supervised object localization (WSOL) aims to learn representations that encode object location using only image-level category labels. However, many objects can be labeled at different levels of granularity. Is it an animal, a bird, or a great horned owl? Which image-level labels should we use? In this paper we study the role of label granularity in WSOL. To facilitate this investigation we introduce iNatLoc500, a new large-scale fine-grained benchmark dataset for WSOL. Surprisingly, we find that choosing the right training label granularity provides a much larger performance boost than choosing the best WSOL algorithm. We also show that changing the label granularity can significantly improve data efficiency.\""
  },
  "eccv2022_main_oimnet++prototypicalnormalizationandlocalization-awarelearningforpersonsearch": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "OIMNet++: Prototypical Normalization and Localization-Aware Learning for Person Search",
    "authors": [
      "Sanghoon Lee",
      "Youngmin Oh",
      "Donghyeon Baek",
      "Junghyup Lee",
      "Bumsub Ham"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7048_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700615.pdf",
    "published": "2020-08",
    "summary": "\"We address the task of person search, that is, localizing and re-identifying query persons from a set of raw scene images. Recent approaches are typically built upon OIMNet, a pioneer work on person search, that learns joint person representations for performing both detection and person re-identification (reID) tasks. To obtain the representations, they extract features from pedestrian proposals, and then project them on a unit hypersphere with L2 normalization. These methods also incorporate all positive proposals, that sufficiently overlap with the ground truth, equally to learn person representations for reID. We have found that 1) the L2 normalization without considering feature distributions degenerates the discriminative power of person representations, and 2) positive proposals often also depict background clutter and person overlaps, which could encode noisy features to person representations. In this paper, we introduce OIMNet++ that addresses the aforementioned limitations. To this end, we introduce a novel normalization layer, dubbed ProtoNorm, that calibrates features from pedestrian proposals, while considering a long-tail distribution of person IDs, enabling L2 normalized person representations to be discriminative. We also propose a localization-aware feature learning scheme that encourages better-aligned proposals to contribute more in learning discriminative representations. Experimental results and analysis on standard person search benchmarks demonstrate the effectiveness of OIMNet++.\""
  },
  "eccv2022_main_out-of-distributionidentificationletdetectortellwhichiamnotsure": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Out-of-Distribution Identification: Let Detector Tell Which I Am Not Sure",
    "authors": [
      "Ruoqi Li",
      "Chongyang Zhang",
      "Hao Zhou",
      "Chao Shi",
      "Yan Luo"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7158_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700631.pdf",
    "published": "2020-08",
    "summary": "\"The superior performance of object detectors is often established under the condition that the test samples are in the same distribution as the training data. However, in most practical applications, out-of-distribution (OOD) instances are inevitable and usually lead to detection uncertainty. In this work, the Feature structured OOD-IDentification (FOOD-ID) model is proposed to reduce the uncertainty of detection results by identifying the OOD instances. Instead of outputting each detection result directly, FOOD-ID uses a likelihood-based measuring mechanism to identify whether the feature satisfies the corresponding class distribution and outputs the OOD results separately. Specifically, the clustering-oriented feature structuration is firstly developed using class-specified prototypes and Attractive-Repulsive loss for more discriminative feature representation and more compact distribution. With the structured features space, the density distribution of all training categories is estimated based on a class-conditional normalizing flow, which is then used for the OOD identification in the test stage. The proposed FOOD-ID can be easily applied to various object detectors including anchor-based frameworks and anchor-free frameworks. Extensive experiments on the PASCAL VOC dataset and an industrial defect dataset demonstrate that FOOD-ID achieves satisfactory OOD identification performance, with which the certainty of detection results is improved significantly.\""
  },
  "eccv2022_main_learningwithfreeobjectsegmentsforlong-tailedinstancesegmentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning with Free Object Segments for Long-Tailed Instance Segmentation",
    "authors": [
      "Cheng Zhang",
      "Tai-Yu Pan",
      "Tianle Chen",
      "Jike Zhong",
      "Wenjin Fu",
      "Wei-Lun Chao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7311_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700648.pdf",
    "published": "2020-08",
    "summary": "\"One fundamental challenge in building an instance segmentation model for a large number of classes in complex scenes is the lack of training examples, especially for rare objects. In this paper, we explore the possibility to increase the training examples without laborious data collection and annotation. We find that an abundance of instance segments can potentially be obtained freely from object-centric images, according to two insights: (i) an object-centric image usually contains one salient object in a simple background; (ii) objects from the same class often share similar appearances or similar contrasts to the background. Motivated by these insights, we propose a simple and scalable framework FreeSeg for extracting and leveraging these \"\"free\"\" object foreground segments to facilitate model training in long-tailed instance segmentation. Concretely, we investigate the similarity among object-centric images of the same class to propose candidate segments of foreground instances, followed by a novel ranking of segment quality. The resulting high-quality object segments can then be used to augment the existing long-tailed datasets, e.g., by copying and pasting the segments onto the original training images. Extensive experiments show that FreeSeg yields substantial improvements on top of strong baselines and achieves state-of-the-art accuracy for segmenting rare object categories. Our code is publicly available at https://github.com/czhang0528/FreeSeg.\""
  },
  "eccv2022_main_autoregressiveuncertaintymodelingfor3dboundingboxprediction": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Autoregressive Uncertainty Modeling for 3D Bounding Box Prediction",
    "authors": [
      "YuXuan Liu",
      "Nikhil Mishra",
      "Maximilian Sieb",
      "Yide Shentu",
      "Pieter Abbeel",
      "Xi Chen"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7331_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700665.pdf",
    "published": "2020-08",
    "summary": "\"3D bounding boxes are a widespread intermediate representation in many computer vision applications. However, predicting them is a challenging task, largely due to partial observability, which motivates the need for a strong sense of uncertainty. While many recent methods have explored better architectures for consuming sparse and unstructured point cloud data, we hypothesize that there is room for improvement in the modeling of the output distribution and explore how this can be achieved using an autoregressive prediction head. Additionally, we release a simulated dataset, COB-3D, which highlights new types of ambiguity that arise in real-world robotics applications, where 3D bounding box prediction has largely been underexplored. We propose methods for leveraging our autoregressive model to make high confidence predictions and meaningful uncertainty measures, achieving strong results on SUN-RGBD, Scannet, KITTI, and our new dataset. Code and dataset are available at https://bbox.yuxuanliu.com.\""
  },
  "eccv2022_main_3drandomocclusionandmulti-layerprojectionfordeepmulti-camerapedestrianlocalization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "3D Random Occlusion and Multi-layer Projection for Deep Multi-Camera Pedestrian Localization",
    "authors": [
      "Rui Qiu",
      "Ming Xu",
      "Yuyao Yan",
      "Jeremy S. Smith",
      "Xi Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7417_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700681.pdf",
    "published": "2020-08",
    "summary": "\"Although deep-learning based methods for monocular pedestrian detection have made a great progress, they are still vulnerable to heavy occlusions. Using multi-view information fusion is a potential solution but has limited applications, due to the lack of annotated training samples in existing multi-view datasets, which increases the risk of overfitting. To address this problem, a data augmentation method is proposed to randomly generate 3D cylinder occlusions, on the ground plane, which are of the average size of pedestrians and projected to multiple views, to relieve the impact of overfitting in the training. Moreover, the feature map of each view is projected to multiple parallel planes at different heights, by using homographies, which allows the CNNs to fully utilize the features across the height of each pedestrian to infer the locations of pedestrians on the ground plane. The proposed 3DROM method has a greatly improved performance in comparison with the state-of-the-art deep-learning based methods for multi-view pedestrian detection.\""
  },
  "eccv2022_main_asimplesingle-scalevisiontransformerforobjectdetectionandinstancesegmentation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Simple Single-Scale Vision Transformer for Object Detection and Instance Segmentation",
    "authors": [
      "Wuyang Chen",
      "Xianzhi Du",
      "Fan Yang",
      "Lucas Beyer",
      "Xiaohua Zhai",
      "Tsung-Yi Lin",
      "Huizhong Chen",
      "Jing Li",
      "Xiaodan Song",
      "Zhangyang Wang",
      "Denny Zhou"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7441_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700697.pdf",
    "published": "2020-08",
    "summary": "\"This work presents a simple vision transformer design as a strong baseline for object localization and instance segmentation tasks. Transformers recently demonstrate competitive performance in image classification tasks. To adopt ViT to object detection and dense prediction tasks, many works inherit the multistage design from convolutional networks and highly customized ViT architectures. Behind this design, the goal is to pursue a better trade-off between computational cost and effective aggregation of multiscale global contexts. However, existing works adopt multistage architectural design as a black-box solution without a clear understanding of its true benefits. In this paper, we comprehensively study three architecture design choices on ViT -- spatial reduction, doubled channels, and multiscale features -- and demonstrate that a vanilla ViT architecture can fulfill this goal without handcrafting multiscale features, maintaining the original ViT design philosophy. We further complete a scaling rule to optimize our model\u2019s trade-off on accuracy and computation cost / model size. By leveraging a constant feature resolution and hidden size throughout the encoder blocks, we propose a simple and compact ViT architecture called Universal Vision Transformer (UViT) that achieves strong performance on COCO object detection and instance segmentation benchmark. Our code will be released upon acceptance.\""
  },
  "eccv2022_main_simpleopen-vocabularyobjectdetectionwithvisiontransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Simple Open-Vocabulary Object Detection with Vision Transformers",
    "authors": [
      "Matthias Minderer",
      "Alexey Gritsenko",
      "Austin Stone",
      "Maxim Neumann",
      "Dirk Weissenborn",
      "Alexey Dosovitskiy",
      "Aravindh Mahendran",
      "Anurag Arnab",
      "Mostafa Dehghani",
      "Zhuoran Shen",
      "Xiao Wang",
      "Xiaohua Zhai",
      "Thomas Kipf",
      "Neil Houlsby"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7529_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700714.pdf",
    "published": "2020-08",
    "summary": "\"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models will be made available on GitHub.\""
  },
  "eccv2022_main_asimpleapproachandbenchmarkfor21,000-categoryobjectdetection": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"A Simple Approach and Benchmark for 21,000-Category Object Detection\"",
    "authors": [
      "Yutong Lin",
      "Chen Li",
      "Yue Cao",
      "Zheng Zhang",
      "Jianfeng Wang",
      "Lijuan Wang",
      "Zicheng Liu",
      "Han Hu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/8094_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710001.pdf",
    "published": "2020-08",
    "summary": "\"Current object detection systems and benchmarks typically handle a limited number of categories, up to about a thousand categories. This paper scales the number of categories for object detection systems and benchmarks up to 21,000, by leveraging existing object detection and image classification data. Unlike previous efforts that usually transfer knowledge from base detectors to image classification data, we propose to rely more on a reverse information flow from a base image classifier to object detection data. In this framework, the large-vocabulary classification capability is first learnt thoroughly using only the image classification data. In this step, the image classification problem is reformulated as a special configuration of object detection that treats the entire image as a special RoI. Then, a simple multi-task learning approach is used to join the image classification and object detection data, with the backbone and the RoI classification branch shared between two tasks. This two-stage approach, though very simple without a sophisticated process such as multi-instance learning (MIL) to generate pseudo labels for object proposals on the image classification data, performs rather strongly that it surpasses previous large-vocabulary object detection systems on a standard evaluation protocol of tailored LVIS. Considering that the tailored LVIS evaluation only accounts for a few hundred novel object categories, we present a new evaluation benchmark that assesses the detection of all 21,841 object classes in the ImageNet-21K dataset. The baseline approach and evaluation benchmark will be publicly available at https://github.com/SwinTransformer/Simple-21K-Detection. We hope these would ease future research on large-vocabulary object detection.\""
  },
  "eccv2022_main_knowledgecondensationdistillation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Knowledge Condensation Distillation",
    "authors": [
      "Chenxin Li",
      "Mingbao Lin",
      "Zhiyuan Ding",
      "Nie Lin",
      "Yihong Zhuang",
      "Yue Huang",
      "Xinghao Ding",
      "Liujuan Cao"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/75_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710019.pdf",
    "published": "2020-08",
    "summary": "\"Knowledge Distillation (KD) transfers the knowledge from a high-capacity teacher network to strengthen a smaller student. Existing methods focus on excavating the knowledge hints and transferring the whole knowledge to the student. However, the knowledge redundancy arises since the knowledge shows different values to the student at different learning stages. In this paper, we propose Knowledge Condensation Distillation (KCD). Specifically, the knowledge value on each sample is dynamically estimated, based on which an Expectation-Maximization (EM) framework is forged to iteratively condense a compact knowledge set from the teacher to guide the student learning. Our approach is easy to build on top of the off-the-shelf KD methods, with no extra training parameters and negligible computation overhead. Thus, it presents one new perspective for KD, in which the student that actively identifies teacher\u2019s knowledge in line with its aptitude can learn to learn more effectively and efficiently. Experiments on standard benchmarks manifest that the proposed KCD can well boost the performance of student model with even higher distillation efficiency. Code is available at https://github.com/dzy3/KCD.\""
  },
  "eccv2022_main_reducinginformationlossforspikingneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Reducing Information Loss for Spiking Neural Networks",
    "authors": [
      "Yufei Guo",
      "Yuanpei Chen",
      "Liwen Zhang",
      "YingLei Wang",
      "Xiaode Liu",
      "Xinyi Tong",
      "Yuanyuan Ou",
      "Xuhui Huang",
      "Zhe Ma"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/88_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710036.pdf",
    "published": "2020-08",
    "summary": "\"The Spiking Neural Network (SNN) has attracted more and more attention recently. It adopts binary spike signals to transmit information. Benefitting from the information passing paradigm of SNNs, the multiplications of activations and weights can be replaced by additions, which are more energy-efficient. However, its \u201cHard Reset\"\" mechanism for the firing activity would ignore the difference among membrane potentials when the membrane potential is above the firing threshold, causing information loss. Meanwhile, quantifying the membrane potential to 0/1 spikes at the firing instants will inevitably introduce the quantization error thus bringing about information loss too. To address these problems, we propose a \u201cSoft Reset\"\" mechanism for the supervised training-based SNNs, which will drive the membrane potential to a dynamic reset potential according to its magnitude, and Membrane Potential Rectifier (MPR) to reduce the quantization error via redistributing the membrane potential to a range close to the spikes. Experimental results show that the SNNs with the proposed \u201cSoft Reset\"\" mechanism and MPR outperform their vanilla counterparts on both static and dynamic datasets.\""
  },
  "eccv2022_main_maskedgenerativedistillation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Masked Generative Distillation",
    "authors": [
      "Zhendong Yang",
      "Zhe Li",
      "Mingqi Shao",
      "Dachuan Shi",
      "Zehuan Yuan",
      "Chun Yuan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/140_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710053.pdf",
    "published": "2020-08",
    "summary": "\"Knowledge distillation has been applied to various tasks successfully. The current distillation algorithm usually improves students\u2019 performance by imitating the output of the teacher. This paper shows that teachers can also improve students\u2019 representation power by guiding students\u2019 feature recovery. From this point of view, we propose Masked Generative Distillation (MGD), which is simple: we mask random pixels of the student\u2019s feature and force it to generate the teacher\u2019s full feature through a simple block. MGD is a truly general feature-based distillation method, which can be utilized on various tasks, including image classification, object detection, semantic segmentation and instance segmentation. We experiment on different models with extensive datasets and the results show that all the students achieve excellent improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1 accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP, SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at https://github.com/yzd-v/MGD.\""
  },
  "eccv2022_main_fine-graineddatadistributionalignmentforpost-trainingquantization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Fine-Grained Data Distribution Alignment for Post-Training Quantization",
    "authors": [
      "Yunshan Zhong",
      "Mingbao Lin",
      "Mengzhao Chen",
      "Ke Li",
      "Yunhang Shen",
      "Fei Chao",
      "Yongjian Wu",
      "Rongrong Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/190_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710070.pdf",
    "published": "2020-08",
    "summary": "\"While post-training quantization receives popularity mostly due to its evasion in accessing the original complete training dataset, its poor performance also stems from scarce images. To alleviate this limitation, in this paper, we leverage the synthetic data introduced by zero-shot quantization with calibration dataset and propose a fine-grained data distribution alignment (FDDA) method to boost the performance of post-training quantization. The method is based on two important properties of batch normalization statistics (BNS) we observed in deep layers of the trained network, i.e., inter-class separation and intra-class incohesion. To preserve this fine-grained distribution information: 1) We calculate the per-class BNS of the calibration dataset as the BNS centers of each class and propose a BNS-centralized loss to force the synthetic data distributions of different classes to be close to their own centers. 2) We add Gaussian noise into the centers to imitate the incohesion and propose a BNS-distorted loss to force the synthetic data distribution of the same class to be close to the distorted centers. By utilizing these two fine-grained losses, our method manifests the state-of-the-art performance on ImageNet, especially when both the first and last layers are quantized to the low-bit.\""
  },
  "eccv2022_main_learningwithrecoverableforgetting": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning with Recoverable Forgetting",
    "authors": [
      "Jingwen Ye",
      "Yifang Fu",
      "Jie Song",
      "Xingyi Yang",
      "Songhua Liu",
      "Xin Jin",
      "Mingli Song",
      "Xinchao Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/798_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710087.pdf",
    "published": "2020-08",
    "summary": "\"Life-long learning aims at learning a sequence of tasks without forgetting the previously acquired knowledge. However, the involved training data may not be life-long legitimate due to privacy or copyright reasons. In practical scenarios, for instance, the model owner may wish to enable or disable the knowledge of specific tasks or specific samples from time to time. Such flexible control over knowledge transfer, unfortunately, has been largely overlooked in previous incremental or decremental learning methods, even at a problem-setup level. In this paper, we explore a novel learning scheme, termed as \\textbf{L}earning w\\textbf{I}th \\textbf{R}ecoverable \\textbf{F}orgetting (LIRF), that explicitly handles the task- or sample-specific knowledge removal and recovery. Specifically, LIRF brings in two innovative schemes, namely knowledge \\emph{deposit} and \\emph{withdrawal}, which allow for isolating user-designated knowledge from a pre-trained network and injecting it back when necessary. During the knowledge deposit process, the specified knowledge is extracted from the target network and stored in a deposit module, while the insensitive or general knowledge of the target network is preserved and further augmented. During knowledge withdrawal, the taken-off knowledge is added back to the target network. The deposit and withdraw processes only demand for a few epochs of finetuning on the removal data, ensuring both data and time efficiency. We conduct experiments on several datasets, and demonstrate that the proposed LIRF strategy yields encouraging results with gratifying generalization capability.\""
  },
  "eccv2022_main_efficientonepassself-distillationwithzipfslabelsmoothing": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Efficient One Pass Self-Distillation with Zipf's Label Smoothing",
    "authors": [
      "Jiajun Liang",
      "Linze Li",
      "Zhaodong Bing",
      "Borui Zhao",
      "Yao Tang",
      "Bo Lin",
      "Haoqiang Fan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/807_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710104.pdf",
    "published": "2020-08",
    "summary": "\"Self-distillation exploits non-uniform soft supervision from itself during training and improves performance without any runtime cost. However, the overhead during training is often overlooked, and yet reducing time and memory overhead during training is increasingly important in the giant models\u2019 era. This paper proposes an efficient self-distillation method named Zipf\u2019s Label Smoothing (Zipf\u2019s LS), which uses the on-the-fly prediction of a network to generate soft supervision that conforms to Zipf distribution without using any contrastive samples or auxiliary parameters. Our idea comes from an empirical observation that when the network is duly trained the output values of a network\u2019s final softmax layer, after sorting by the magnitude and averaged across samples, should follow a distribution reminiscent to Zipf\u2019s Law in the word frequency statistics of natural languages. By enforcing this property on the sample level and throughout the whole training period, we find that the prediction accuracy can be greatly improved. Using ResNet50 on the INAT21 fine-grained classification dataset, our technique achieves +3.61% accuracy gain compared to the vanilla baseline, and 0.88% more gain against the previous label smoothing or self-distillation strategies. The implementation is publicly available at https://github.com/megvii-research/zipfls.\""
  },
  "eccv2022_main_pruneyourmodelbeforedistillit": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Prune Your Model before Distill It",
    "authors": [
      "Jinhyuk Park",
      "Albert No"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1136_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710120.pdf",
    "published": "2020-08",
    "summary": "\"Knowledge distillation transfers the knowledge from a cumbersome teacher to a small student. Recent results suggest that the student-friendly teacher is more appropriate to distill since it provides more transferrable knowledge. In this work, we propose the novel framework, \u201cprune, then distill,\u201d that prunes the model first to make it more transferrable and then distill it to the student. We provide several exploratory examples where the pruned teacher teaches better than the original unpruned networks. We further show theoretically that the pruned teacher plays the role of regularizer in distillation, which reduces the generalization error. Based on this result, we propose a novel neural network compression scheme where the student network is formed based on the pruned teacher and then apply the \u201cprune, then distill\u201d strategy. The code is available at https://github.com/ososos888/prune-then-distill.\""
  },
  "eccv2022_main_deeppartialupdatingtowardscommunicationefficientupdatingforon-deviceinference": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Deep Partial Updating: Towards Communication Efficient Updating for On-Device Inference",
    "authors": [
      "Zhongnan Qu",
      "Cong Liu",
      "Lothar Thiele"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1447_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710137.pdf",
    "published": "2020-08",
    "summary": "\"Emerging edge intelligence applications require the server to retrain and update deep neural networks deployed on remote edge nodes to leverage newly collected data samples. Unfortunately, it may be impossible in practice to continuously send fully updated weights to these edge nodes due to the highly constrained communication resource. In this paper, we propose the weight-wise deep partial updating paradigm, which smartly selects a small subset of weights to update in each server-to-edge communication round, while achieving a similar performance compared to full updating. Our method is established through analytically upper-bounding the loss difference between partial updating and full updating, and only updates the weights which make the largest contributions to the upper bound. Extensive experimental results demonstrate the efficacy of our partial updating methodology which achieves a high inference accuracy while updating a rather small number of weights.\""
  },
  "eccv2022_main_patchsimilarityawaredata-freequantizationforvisiontransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Patch Similarity Aware Data-Free Quantization for Vision Transformers",
    "authors": [
      "Zhikai Li",
      "Liping Ma",
      "Mengjuan Chen",
      "Junrui Xiao",
      "Qingyi Gu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1580_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710154.pdf",
    "published": "2020-08",
    "summary": "\"Vision transformers have recently gained great success on various computer vision tasks; nevertheless, their high model complexity makes it challenging to deploy on resource-constrained devices. Quantization is an effective approach to reduce model complexity, and data-free quantization, which can address data privacy and security concerns during model deployment, has received widespread interest. Unfortunately, all existing methods, such as BN regularization, were designed for convolutional neural networks and cannot be applied to vision transformers with significantly different model architectures. In this paper, we propose PSAQ-ViT, a Patch Similarity Aware data-free Quantization framework for Vision Transformers, to enable the generation of \"\"realistic\"\" samples based on the vision transformer\u2019s unique properties for calibrating the quantization parameters. Specifically, we analyze the self-attention module\u2019s properties and reveal a general difference (patch similarity) in its processing of Gaussian noise and real images. The above insights guide us to design a relative value metric to optimize the Gaussian noise to approximate the real images, which are then utilized to calibrate the quantization parameters. Extensive experiments and ablation studies are conducted on various benchmarks to validate the effectiveness of PSAQ-ViT, which can even outperform the real-data-driven methods.\""
  },
  "eccv2022_main_l3accelerator-friendlylosslessimageformatforhigh-resolution,high-throughputdnntraining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"L3: Accelerator-Friendly Lossless Image Format for High-Resolution, High-Throughput DNN Training\"",
    "authors": [
      "Jonghyun Bae",
      "Woohyeon Baek",
      "Tae Jun Ham",
      "Jae W. Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1766_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710171.pdf",
    "published": "2020-08",
    "summary": "\"The training process of deep neural networks (DNNs) is usually pipelined with stages for data preparation on CPUs followed by gradient computation on accelerators like GPUs. In an ideal pipeline, the end-to-end training throughput is eventually limited by the throughput of the accelerator, not by that of data preparation. In the past, the DNN training pipeline achieved a near-optimal throughput by utilizing datasets encoded with a lightweight, lossy image format like JPEG. However, as high-resolution, losslessly-encoded datasets become more popular for applications requiring high accuracy, a performance problem arises in the data preparation stage due to low-throughput image decoding on the CPU. Thus, we propose L3, a custom lightweight, lossless image format for high-resolution, high-throughput DNN training. The decoding process of L3 is effectively parallelized on the accelerator, thus minimizing CPU intervention for data preparation during DNN training. L3 achieves a 9.29x higher data preparation throughput than PNG, the most popular lossless image format, for the Cityscapes dataset on NVIDIA A100 GPU, which leads to 1.71x higher end-to-end training throughput. Compared to JPEG and WebP, two popular lossy image formats, L3 provides up to 1.77x and 2.87x higher end-to-end training throughput for ImageNet, respectively, at equivalent metric performance.\""
  },
  "eccv2022_main_streamingmultiscaledeepequilibriummodels": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Streaming Multiscale Deep Equilibrium Models",
    "authors": [
      "Can Ufuk Ertenli",
      "Emre Akbas",
      "Ramazan Gokberk Cinbis"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2054_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710189.pdf",
    "published": "2020-08",
    "summary": "\"We present StreamDEQ, a method that infers frame-wise representations on videos with minimal per-frame computation. In contrast to conventional methods where compute time grows at least linearly with the network depth, we aim to update the representations in a continuous manner. For this purpose, we leverage the recently emerging implicit layer model which infers the representation of an image by solving a fixed-point problem. Our main insight is to leverage the slowly changing nature of videos and use the previous frame representation as an initial condition on each frame. This scheme effectively recycles the recent inference computations and greatly reduces the needed processing time. Through extensive experimental analysis, we show that StreamDEQ is able to recover near-optimal representations in a few frames time, and maintain an up-to-date representation throughout the video duration. Our experiments on video semantic segmentation and video object detection show that StreamDEQ achieves on par accuracy with the baseline (standard MDEQ) while being more than 3x faster.\""
  },
  "eccv2022_main_symmetryregularizationandsaturatingnonlinearityforrobustquantization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Symmetry Regularization and Saturating Nonlinearity for Robust Quantization",
    "authors": [
      "Sein Park",
      "Yeongsang Jang",
      "Eunhyeok Park"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2337_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710207.pdf",
    "published": "2020-08",
    "summary": "\"Robust quantization improves the tolerance of networks for various implementations, allowing the maintenance of accuracy in a different bit-width or quantization policy. It offers appealing candidates, especially when the target objective (i.e., energy consumption and performance) is not static and implementations are fragmented. In this work, we perform extensive analyses to identify the sources of quantization error and present three insights to robustify the network against quantization: reduction of error propagation, range clamping for error minimization, and inherited robustness against quantization. Based on these insights, we propose two novel methods called symmetry regularization (SymReg) and saturating nonlinearity (SatNL). Applying the proposed methods during training can enhance the robustness of arbitrary neural networks against quantization on existing post-training quantization (PTQ) and quantization-aware training (QAT) algorithms, obtaining a single weight flexible enough to maintain the output quality at various conditions. We conduct extensive studies and validate the effectiveness of the proposed methods.\""
  },
  "eccv2022_main_sp-netslowlyprogressingdynamicinferencenetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SP-Net: Slowly Progressing Dynamic Inference Networks",
    "authors": [
      "Huanyu Wang",
      "Wenhu Zhang",
      "Shihao Su",
      "Hui Wang",
      "Zhenwei Miao",
      "Xin Zhan",
      "Xi Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2761_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710225.pdf",
    "published": "2020-08",
    "summary": "\"Dynamic inference networks improve computational efficiency by executing a subset of network components, i.e., executing path, conditioned on input sample. Prevalent methods typically assign routers to computational blocks so that a computational block can be skipped or executed. However, such inference mechanisms are prone to suffer instability in the optimization of dynamic inference networks. First, a dynamic inference network is more sensitive to its routers than its computational blocks. Second, the components executed by the network vary with samples, resulting in unstable feature evolution throughout the network. To alleviate the problems above, we propose a slowly progressing dynamic inference network to stabilize the optimization. First, we design a dynamic auxiliary module to slow down the progress in routers. Moreover, we regularize the feature evolution directions across the network to smoothen the feature extraction. As a result, we conduct extensive experiments on three widely used benchmarks and show that our proposed SP-Nets achieve state-of-the-art performance in terms of efficiency and accuracy.\""
  },
  "eccv2022_main_equivarianceandinvarianceinductivebiasforlearningfrominsufficientdata": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Equivariance and Invariance Inductive Bias for Learning from Insufficient Data",
    "authors": [
      "Tan Wang",
      "Qianru Sun",
      "Sugiri Pranata",
      "Karlekar Jayashree",
      "Hanwang Zhang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3535_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710242.pdf",
    "published": "2020-08",
    "summary": "\"We are interested in learning robust models from insufficient data, without the need for any externally pre-trained checkpoints. First, compared to sufficient data, we show why insufficient data renders the model more easily biased to the limited training environments that are usually different from testing. For example, if all the training swan samples are \u201cwhite\u201d, the model may wrongly use the \u201cwhite\u201d environment to represent the intrinsic class swan. Then, we justify that equivariance inductive bias can retain the class feature while invariance inductive bias can remove the environmental feature, leaving the class feature that generalizes to any environmental changes in testing. To impose them on learning, for equivariance, we demonstrate that any off-the-shelf contrastive-based self-supervised feature learning method can be deployed; for invariance, we propose a class-wise invariant risk minimization (IRM) that efficiently tackles the challenge of missing environmental annotation in conventional IRM. State-of-the-art experimental results on real-world benchmarks (VIPriors, ImageNet100 and NICO) validate the great potential of equivariance and invariance in data-efficient learning. The code is available at https://github.com/Wangt-CN/EqInv.\""
  },
  "eccv2022_main_mixed-precisionneuralnetworkquantizationvialearnedlayer-wiseimportance": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Mixed-Precision Neural Network Quantization via Learned Layer-Wise Importance",
    "authors": [
      "Chen Tang",
      "Kai Ouyang",
      "Zhi Wang",
      "Yifei Zhu",
      "Wen Ji",
      "Yaowei Wang",
      "Wenwu Zhu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3598_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710260.pdf",
    "published": "2020-08",
    "summary": "\"The exponentially large discrete search space in mixed-precision quantization (MPQ) makes it hard to determine the optimal bit-width for each layer. Previous works usually resort to iterative search methods on the training set, which consume hundreds or even thousands of GPU-hours. In this study, we reveal that some unique learnable parameters in quantization, namely the scale factors in the quantizer, can serve as importance indicators of a layer, reflecting the contribution of that layer to the final accuracy at certain bit-widths. These importance indicators naturally perceive the numerical transformation during quantization-aware training, which can precisely provide quantization sensitivity metrics of layers. However, a deep network always contains hundreds of such indicators, and training them one by one would lead to an excessive time cost. To overcome this issue, we propose a joint training scheme that can obtain all indicators at once. It considerably speeds up the indicators training process by parallelizing the original sequential training processes. With these learned importance indicators, we formulate the MPQ search problem as a one-time integer linear programming (ILP) problem. That avoids the iterative search and significantly reduces search time without limiting the bit-width search space. For example, MPQ search on ResNet18 with our indicators takes only 0.06 seconds, which improves time efficiency exponentially compared to iterative search methods. Also, extensive experiments show our approach can achieve SOTA accuracy on ImageNet for far-ranging models with various constraints (e.g., BitOps, compress rate).\""
  },
  "eccv2022_main_eventneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Event Neural Networks",
    "authors": [
      "Matthew Dutson",
      "Yin Li",
      "Mohit Gupta"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3751_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710276.pdf",
    "published": "2020-08",
    "summary": "\"Video data is often repetitive; for example, the contents of adjacent frames are usually strongly correlated. Such redundancy occurs at multiple levels of complexity, from low-level pixel values to textures and high-level semantics. We propose Event Neural Networks (EvNets), which leverage this redundancy to achieve considerable computation savings during video inference. A defining characteristic of EvNets is that each neuron has state variables that provide it with long-term memory, which allows low-cost, high-accuracy inference even in the presence of significant camera motion. We show that it is possible to transform a wide range of neural networks into EvNets without re-training. We demonstrate our method on state-of-the-art architectures for both high- and low-level visual processing, including pose recognition, object detection, optical flow, and image enhancement. We observe roughly an order-of-magnitude reduction in computational costs compared to conventional networks, with minimal reductions in model accuracy.\""
  },
  "eccv2022_main_edgevitscompetinglight-weightcnnsonmobiledeviceswithvisiontransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "EdgeViTs: Competing Light-Weight CNNs on Mobile Devices with Vision Transformers",
    "authors": [
      "Junting Pan",
      "Adrian Bulat",
      "Fuwen Tan",
      "Xiatian Zhu",
      "Lukasz Dudziak",
      "Hongsheng Li",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3769_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710294.pdf",
    "published": "2020-08",
    "summary": "\"Self-attention based models such as vision transformers (ViTs) have emerged as a very competitive architecture alternative to convolutional neural networks (CNNs) in computer vision. Despite increasingly stronger variants with ever-higher recognition accuracies, due to the quadratic complexity of self-attention, existing ViTs are typically demanding in computation and model size. Although several successful design choices (e.g., the convolutions and hierarchical multi-stage structure) of prior CNNs have been reintroduced into recent ViTs, they are still not sufficient to meet the limited resource requirements of mobile devices. This motivates a very recent attempt to develop light ViTs based on the state-of-the-art MobileNet-v2, but still leaves a performance gap behind. In this work, pushing further along this under-studied direction we introduce EdgeViTs, a new family of light-weight ViTs that, for the first time, enable attention-based vision models to compete with the best light-weight CNNs in the tradeoff between accuracy and on-device efficiency. This is realized by introducing a highly cost-effective local-global-local (LGL) information exchange bottleneck based on optimal integration of self-attention and convolutions. For device-dedicated evaluation, rather than relying on inaccurate proxies like the number of FLOPs or parameters, we adopt a practical approach of focusing directly on on-device latency and, for the first time, energy efficiency. Extensive experiments on image classification, object detection, and semantic segmentation validate the high efficiency of our EdgeViTs when compared to the state-of-the-art efficient CNNs and ViTs in terms of accuracy-efficiency tradeoff on mobile hardware. Specifically, we show that our models are Pareto-optimal when both accuracy-latency and accuracy-energy tradeoffs are considered, achieving strict dominance over other ViTs in almost all cases and competing with the most efficient CNNs.\""
  },
  "eccv2022_main_palquantacceleratinghigh-precisionnetworksonlow-precisionaccelerators": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PalQuant: Accelerating High-Precision Networks on Low-Precision Accelerators",
    "authors": [
      "Qinghao Hu",
      "Gang Li",
      "Qiman Wu",
      "Jian Cheng"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4047_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710312.pdf",
    "published": "2020-08",
    "summary": "\"Recently low-precision deep learning accelerators (DLAs) have become popular due to their advantages in chip area and energy consumption, yet the low-precision quantized models on these DLAs bring in severe accuracy degradation. One way to achieve both high accuracy and efficient inference is to deploy high-precision neural networks on low-precision DLAs, which is rarely studied. In this paper, we propose the PArallel Low-precision Quantization (PalQuant) method that approximates high-precision computations via learning parallel low-precision representations from scratch. In addition, we present a novel cyclic shuffle module to boost the cross-group information communication between parallel low-precision groups. Extensive experiments demonstrate that PalQuant has superior performance to state-of-the-art quantization methods in both accuracy and inference speed, e.g., for ResNet-18 network quantization, PalQuant can obtain 0.52 % higher accuracy and 1.78 times speedup simultaneously over their 4-bit counter-part on a state-of-the-art 2-bit accelerator. Code is available at https://github.com/huqinghao/PalQuant.\""
  },
  "eccv2022_main_disentangleddifferentiablenetworkpruning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Disentangled Differentiable Network Pruning",
    "authors": [
      "Shangqian Gao",
      "Feihu Huang",
      "Yanfu Zhang",
      "Heng Huang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4273_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710329.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we propose a novel channel pruning method for compression and acceleration of Convolutional Neural Networks (CNNs). Many existing channel pruning works try to discover compact sub-networks by optimizing a regularized loss function through differentiable operations. Usually, a learnable parameter is used to characterize each channel, which entangles the width and channel importance. In this setting, the FLOPs or parameter constraints implicitly restrict the search space of the pruned model. To solve the aforementioned problems, we propose optimizing each layer\u2019s width by relaxing the hard equality constraint used in previous works. The relaxation is inspired by the definition of the top-$k$ operation. By doing so, we partially disentangle the learning of width and channel importance, which enables independent parametrization for width and importance and makes pruning more flexible. We also introduce soft top-$k$ to improve the learning of width. Moreover, to make pruning more efficient, we use two neural networks to parameterize the importance and width. The importance generation network considers both inter-channel and inter-layer relationships. The width generation network has similar functions. In addition, our method can be easily optimized by popular SGD methods, which enjoys the benefits of differentiable pruning. Extensive experiments on CIFAR-10 and ImageNet show that our method is competitive with state-of-the-art methods.\""
  },
  "eccv2022_main_ida-detaninformationdiscrepancy-awaredistillationfor1-bitdetectors": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "IDa-Det: An Information Discrepancy-Aware Distillation for 1-Bit Detectors",
    "authors": [
      "Sheng Xu",
      "Yanjing Li",
      "Bohan Zeng",
      "Teli Ma",
      "Baochang Zhang",
      "Xianbin Cao",
      "Peng Gao",
      "Jinhu L\u00fc"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4277_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710347.pdf",
    "published": "2020-08",
    "summary": "\"Knowledge distillation (KD) has been proven to be useful for training compact object detection models. However, we observe that KD is often effective when the teacher model and student counterpart share similar proposal information. This explains why existing KD methods are less effective for 1-bit detectors, caused by a significant information discrepancy between the real-valued teacher and the 1-bit student. This paper presents an Information Discrepancy-aware strategy (IDa-Det) to distill 1-bit detectors that can effectively eliminate information discrepancies and significantly reduce the performance gap between a 1-bit detector and its real-valued counterpart. We formulate the distillation process as a bi-level optimization formulation. At the inner level, we select the representative proposals with maximum information discrepancy. We then introduce a novel entropy distillation loss to reduce the disparity based on the selected proposals. Extensive experiments demonstrate IDa-Det\u2019s superiority over state-of-the-art 1-bit detectors and KD methods on both PASCAL VOC and COCO datasets. IDa-Det achieves a 76.9\\% mAP for a 1-bit Faster-RCNN with ResNet-18 backbone. Our code is open-sourced on https://github.com/SteveTsui/IDa-Det.\""
  },
  "eccv2022_main_learningtoweightsamplesfordynamicearly-exitingnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Learning to Weight Samples for Dynamic Early-Exiting Networks",
    "authors": [
      "Yizeng Han",
      "Yifan Pu",
      "Zihang Lai",
      "Chaofei Wang",
      "Shiji Song",
      "Junfeng Cao",
      "Wenhui Huang",
      "Chao Deng",
      "Gao Huang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4295_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710363.pdf",
    "published": "2020-08",
    "summary": "\"Early exiting is an effective paradigm for improving the inference efficiency of deep networks. By constructing classifiers with varying resource demands (the exits), such networks allow easy samples to be output at early exits, removing the need for executing deeper layers. While existing works mainly focus on the architectural design of multi-exit networks, the training strategies for such models are largely left unexplored. The current state-of-the-art models treat all samples the same during training. However, the early-exiting behavior during testing has been ignored, leading to a gap between training and testing. In this paper, we propose to bridge this gap by sample weighting. Intuitively, easy samples, which generally exit early in the network during inference, should contribute more to training early classifiers. The training of hard samples (mostly exit from deeper layers), however, should be emphasized by the late classifiers. Our work proposes to adopt a weight prediction network to weight the loss of different training samples at each exit. This weight prediction network and the backbone model are jointly optimized under a \\emph{meta-learning} framework with a novel optimization objective. By bringing the adaptive behavior during inference into the training phase, we show that the proposed weighting mechanism consistently improves the trade-off between classification accuracy and inference efficiency. Code is available at https://github.com/LeapLabTHU/L2W-DEN.\""
  },
  "eccv2022_main_adabinimprovingbinaryneuralnetworkswithadaptivebinarysets": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "AdaBin: Improving Binary Neural Networks with Adaptive Binary Sets",
    "authors": [
      "Zhijun Tu",
      "Xinghao Chen",
      "Pengju Ren",
      "Yunhe Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4762_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710380.pdf",
    "published": "2020-08",
    "summary": "\"This paper studies the Binary Neural Networks (BNNs) in which weights and activations are both binarized into 1-bit values, thus greatly reducing the memory usage and computational complexity. Since the modern deep neural networks are of sophisticated design with complex architecture for the accuracy reason, the diversity on distributions of weights and activations is very high. Therefore, the conventional sign function cannot be well used for effectively binarizing full precision values in BNNs. To this end, we present a simple yet effective approach called AdaBin to adaptively obtain the optimal binary sets {b_1, b_2} (b_1, b_2 belong to R) of weights and activations for each layer instead of a fixed set (i.e., {-1, +1}). In this way, the proposed method can better fit different distributions and increase the representation ability of binarized features. In practice, we use the center position and distance of 1-bit values to define a new binary quantization function. For the weights, we propose an equalization method to align the symmetrical center of binary distribution to real-valued distribution, and minimize the Kullback-Leibler divergence of them. Meanwhile, we introduce a gradient-based optimization method to get these two parameters for activations, which are jointly trained in an end-to-end manner. Experimental results on benchmark models and datasets demonstrate that the proposed AdaBin is able to achieve state-of-the-art performance. For instance, we obtain a 66.4% Top-1 accuracy on the ImageNet using ResNet-18 architecture, and a 69.4 mAP on PASCAL VOC using SSD300.\""
  },
  "eccv2022_main_adaptivetokensamplingforefficientvisiontransformers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Adaptive Token Sampling for Efficient Vision Transformers",
    "authors": [
      "Mohsen Fayyaz",
      "Soroush Abbasi Koohpayegani",
      "Farnoush Rezaei Jafari",
      "Sunando Sengupta",
      "Hamid Reza Vaezi Joze",
      "Eric Sommerlade",
      "Hamed Pirsiavash",
      "J\u00fcrgen Gall"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4901_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710397.pdf",
    "published": "2020-08",
    "summary": "\"While state-of-the-art vision transformer models achieve promising results in image classification, they are computationally expensive and require many GFLOPs. Although the GFLOPs of a vision transformer can be decreased by reducing the number of tokens in the network, there is no setting that is optimal for all input images. In this work, we therefore introduce a differentiable parameter-free Adaptive Token Sampler (ATS) module, which can be plugged into any existing vision transformer architecture. ATS empowers vision transformers by scoring and adaptively sampling significant tokens. As a result, the number of tokens is not constant anymore and varies for each input image. By integrating ATS as an additional layer within the current transformer blocks, we can convert them into much more efficient vision transformers with an adaptive number of tokens. Since ATS is a parameter-free module, it can be added to the off-the-shelf pre-trained vision transformers as a plug and play module, thus reducing their GFLOPs without any additional training. Moreover, due to its differentiable design, one can also train a vision transformer equipped with ATS. We evaluate the efficiency of our module in both image and video classification tasks by adding it to multiple SOTA vision transformers. Our proposed module improves the SOTA by reducing their computational costs (GFLOPs) by 2$\\times$, while preserving their accuracy on the ImageNet, Kinetics-400, and Kinetics-600 datasets.\""
  },
  "eccv2022_main_weightfixingnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Weight Fixing Networks",
    "authors": [
      "Christopher Subia-Waud",
      "Srinandan Dasmahapatra"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5083_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710416.pdf",
    "published": "2020-08",
    "summary": "\"Modern iterations of deep learning models contain millions (billions) of unique parameters--each represented by a $b$-bit number. Popular attempts at compressing neural networks (such as pruning and quantisation) have shown that many of the parameters are superfluous, which we can remove (pruning) or express with $b\u2019 < b$ bits (quantisation) without hindering performance. Here we look to go much further in minimising the information content of networks. Rather than a channel or layer-wise encoding, we look to lossless whole-network quantisation to minimise the entropy and number of unique parameters in a network. We propose a new method, which we call Weight Fixing Networks (WFN) that we design to realise four model outcome objectives: i) very few unique weights, ii) low-entropy weight encodings, iii) unique weight values which are amenable to energy-saving versions of hardware multiplication, and iv) lossless task-performance. Some of these goals are conflicting. To best balance these conflicts, we combine a few novel (and some well-trodden) tricks; a novel regularisation term, (i, ii) a view of clustering cost as relative distance change (i, ii, iv), and a focus on whole-network re-use of weights (i, iii). Our Imagenet experiments demonstrate lossless compression using 56x fewer unique weights and a 1.9x lower weight-space entropy than SOTA quantisation approaches.\""
  },
  "eccv2022_main_self-slimmedvisiontransformer": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Self-Slimmed Vision Transformer",
    "authors": [
      "Zhuofan Zong",
      "Kunchang Li",
      "Guanglu Song",
      "Yali Wang",
      "Yu Qiao",
      "Biao Leng",
      "Yu Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5408_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710433.pdf",
    "published": "2020-08",
    "summary": "\"Vision transformers (ViTs) have become the popular structures and outperformed convolutional neural networks (CNNs) on various vision tasks. However, such powerful transformers bring a huge computation burden, because of the exhausting token-to-token comparison. The previous works focus on dropping insignificant tokens to reduce the computational cost of ViTs. But when the dropping ratio increases, this hard manner will inevitably discard the vital tokens, which limits its efficiency. To solve the issue, we propose a generic self-slimmed learning approach for vanilla ViTs, namely SiT. Specifically, we first design a novel Token Slimming Module (TSM), which can boost the inference efficiency of ViTs by dynamic token aggregation. As a general method of token hard dropping, our TSM softly integrates redundant tokens into fewer informative ones. It can dynamically zoom visual attention without cutting off discriminative token relations in the images, even with a high slimming ratio. Furthermore, we introduce a concise Feature Recalibration Distillation (FRD) framework, wherein we design a reverse version of TSM (RTSM) to recalibrate the unstructured token in a flexible auto-encoder manner. Due to the similar structure between teacher and student, our FRD can effectively leverage structure knowledge for better convergence. Finally, we conduct extensive experiments to evaluate our SiT. It demonstrates that our method can speed up ViTs by 1.7x with negligible accuracy drop, and even speed up ViTs by 3.6x while maintaining 97% of their performance. Surprisingly, by simply arming LV-ViT with our SiT, we achieve new state-of-the-art performance on ImageNet. Code is available at https://github.com/Sense-X/SiT.\""
  },
  "eccv2022_main_switchableonlineknowledgedistillation": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Switchable Online Knowledge Distillation",
    "authors": [
      "Biao Qian",
      "Yang Wang",
      "Hongzhi Yin",
      "Richang Hong",
      "Meng Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5410_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710450.pdf",
    "published": "2020-08",
    "summary": "\"Online Knowledge Distillation (OKD) improves the involved models by reciprocally exploiting the difference between teacher and student. Several crucial bottlenecks over the gap between them -- e.g., Why and when does a large gap harm the performance, especially for student? How to quantify the gap between teacher and student? -- have received limited formal study. In this paper, we propose Switchable Online Knowledge Distillation (SwitOKD), to answer these questions.Instead of focusing on the accuracy gap at test phase by the existing arts, the core idea of SwitOKD is to adaptively calibrate the gap at training phase, namely distillation gap, via a switching strategy be-tween two modes -- expert mode (pause the teacher while keep the student learning) and learning mode (restart the teacher). To possess an appropriate distillation gap, we further devise an adaptive switching threshold, which provides a formal criterion as to when to switch to learning mode or expert mode, and thus improves the student\u2019s performance. Meanwhile, the teacher benefits from our adaptive switching threshold and keeps basically on a par with other online arts. We further extend SwitOKD to multiple networks with two basis topologies. Finally, extensive experiments and analysis validate the merits of SwitOKD for classification over the state-of-the-arts. Our code is available at https://github.com/hfutqian/SwitOKD.\""
  },
  "eccv2022_main_$\\ell_\\infty$-robustnessandbeyondunleashingefficientadversarialtraining": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "$\\ell_\\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training",
    "authors": [
      "Hadi M. Dolatabadi",
      "Sarah Erfani",
      "Christopher Leckie"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5478_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710466.pdf",
    "published": "2020-08",
    "summary": "\"Neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches in training robust models against such attacks. However, it is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration, hampering its effectiveness. Recently, Fast Adversarial Training (FAT) was proposed that can obtain robust models efficiently. However, the reasons behind its success are not fully understood, and more importantly, it can only train robust models for $\\ell_\\infty$-bounded attacks as it uses FGSM during training. In this paper, by leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a general, more principled approach toward reducing the time complexity of robust training. Unlike existing methods, our approach can be adapted to a wide variety of training objectives, including TRADES, $\\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental results indicate that our approach speeds up adversarial training by 2-3 times while experiencing a slight reduction in the clean and robust accuracy.\""
  },
  "eccv2022_main_multi-granularitypruningformodelaccelerationonmobiledevices": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Multi-Granularity Pruning for Model Acceleration on Mobile Devices",
    "authors": [
      "Tianli Zhao",
      "Xi Sheryl Zhang",
      "Wentao Zhu",
      "Jiaxing Wang",
      "Sen Yang",
      "Ji Liu",
      "Jian Cheng"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5496_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710483.pdf",
    "published": "2020-08",
    "summary": "\"For practical deep neural network design on mobile devices, it is essential to consider the constraints incurred by the computational resources and the inference latency in various applications. Among deep network acceleration approaches, pruning is a widely adopted practice to balance the computational resource consumption and the accuracy, where unimportant connections can be removed either channel-wisely or randomly with a minimal impact on model accuracy. The coarse-grained channel pruning instantly results in a significant latency reduction, while the fine-grained weight pruning is more flexible to retain accuracy. In this paper, we present a unified framework for the Joint Channel pruning and Weight pruning, named JCW, which achieves an optimal pruning proportion between channel and weight pruning. To fully optimize the trade-off between latency and accuracy, we further develop a tailored multi-objective evolutionary algorithm in the JCW framework, which enables one single round search to obtain the optimal candidate architectures for various deployment requirements. Extensive experiments demonstrate that the JCW achieves a better trade-off between the latency and accuracy against previous state-of-the-art pruning methods on the ImageNet classification dataset.\""
  },
  "eccv2022_main_deepensemblelearningbydiverseknowledgedistillationforfine-grainedobjectclassification": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Deep Ensemble Learning by Diverse Knowledge Distillation for Fine-Grained Object Classification",
    "authors": [
      "Naoki Okamoto",
      "Tsubasa Hirakawa",
      "Takayoshi Yamashita",
      "Hironobu Fujiyoshi"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5616_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710501.pdf",
    "published": "2020-08",
    "summary": "\"Ensemble of networks with bidirectional knowledge distillation does not significantly improve on the performance of ensemble of networks without bidirectional knowledge distillation. We think that this is because there is a relationship between the knowledge in knowledge distillation and the individuality of networks in the ensemble. In this paper, we propose a knowledge distillation for ensemble by optimizing the elements of knowledge distillation as hyperparameters. The proposed method uses graphs to represent diverse knowledge distillations. It automatically designs the knowledge distillation for the optimal ensemble by optimizing the graph structure to maximize the ensemble accuracy. Graph optimization and evaluation experiments using Stanford Dogs, Stanford Cars, CUB-200-2011, CIFAR-10, and CIFAR-100 show that the proposed method achieves higher ensemble accuracy than conventional ensembles.\""
  },
  "eccv2022_main_helpfulorharmfulinter-taskassociationincontinuallearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Helpful or Harmful: Inter-Task Association in Continual Learning",
    "authors": [
      "Hyundong Jin",
      "Eunwoo Kim"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5666_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710518.pdf",
    "published": "2020-08",
    "summary": "\"When optimizing sequentially incoming tasks, deep neural networks generally suffer from catastrophic forgetting due to their lack of ability to maintain knowledge from old tasks. This may lead to a significant performance drop of the previously learned tasks. To alleviate this problem, studies on continual learning have been conducted as a countermeasure. Nevertheless, it suffers from an increase in computational cost due to the expansion of the network size or a change in knowledge that is favorably linked to previous tasks. In this work, we propose a novel approach to differentiate helpful and harmful information for old tasks using a model search to learn a current task effectively. Given a new task, the proposed method discovers an underlying association knowledge from old tasks, which can provide additional support in acquiring the new task knowledge. In addition, by introducing a sensitivity measure to the loss of the current task from the associated tasks, we find cooperative relations between tasks while alleviating harmful interference. We apply the proposed approach to both task- and class-incremental scenarios in continual learning, using a wide range of datasets from small to large scales. Experimental results show that the proposed method outperforms a large variety of continual learning approaches for the experiments while effectively alleviating catastrophic forgetting.\""
  },
  "eccv2022_main_towardsaccuratebinaryneuralnetworksviamodelingcontextualdependencies": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards Accurate Binary Neural Networks via Modeling Contextual Dependencies",
    "authors": [
      "Xingrun Xing",
      "Yangguang Li",
      "Wei Li",
      "Wenrui Ding",
      "Yalong Jiang",
      "Yufeng Wang",
      "Jing Shao",
      "Chunlei Liu",
      "Xianglong Liu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5704_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710535.pdf",
    "published": "2020-08",
    "summary": "\"Existing Binary Neural Networks (BNNs) mainly operate on local convolutions with binarization function. However, such simple bit operations lack the ability of modeling contextual dependencies, which is critical for learning discriminative deep representations in vision models. In this work, we tackle this issue by presenting new designs of binary neural modules, which enables BNNs to learn effective contextual dependencies. First, we propose a binary multi-layer perceptron (MLP) block as an alternative to binary convolution blocks to directly model contextual dependencies. Both short-range and long-range feature dependencies are modeled by binary MLPs, where the former provides local inductive bias and the latter breaks limited receptive field in binary convolutions. A sparse contextual interaction is achieved with the long-short range binary MLP block. Second, to improve the robustness of binary models with contextual dependencies, we compute the contextual dynamic embeddings to determine the binarization thresholds in general binary convolutional blocks. Armed with our binary MLP blocks and improved binary convolution, we build the BNNs with explicit Contextual Dependency modeling, termed as BCDNet. On the standard ImageNet-1K classification benchmark, the BCDNet achieves 72.3% Top-1 accuracy and outperforms leading binary methods by a large margin. In particular, the proposed BCDNet exceeds the state-of-the-art ReActNet-A by 2.9% Top-1 accuracy with similar operations. Our codes is available at https://github.com/Sense-GVT/BCDNet.\""
  },
  "eccv2022_main_spinanempiricalevaluationonsharingparametersofisotropicnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks",
    "authors": [
      "Chien-Yu Lin",
      "Anish Prabhu",
      "Thomas Merth",
      "Sachin Mehta",
      "Anurag Ranjan",
      "Maxwell Horton",
      "Mohammad Rastegari"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5750_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710552.pdf",
    "published": "2020-08",
    "summary": "\"Recent isotropic networks, such as ConvMixer and Vision Transformers, have found significant success across visual recognition tasks, matching or outperforming non-isotropic Convolutional Neural Networks. Isotropic architectures are particularly well-suited to cross-layer weight sharing, an effective neural network compression technique. In this paper, we perform an empirical evaluation on methods for sharing parameters in isotropic networks (SPIN). We present a framework to formalize major weight sharing design decisions and perform a comprehensive empirical evaluation of this design space. Guided by our experimental results, we propose a weight sharing strategy to generate a family of models with better overall efficiency, in terms of FLOPs and parameters versus accuracy, compared to traditional scaling methods alone, for example compressing ConvMixer by 1.9x while improving accuracy on ImageNet. Finally, we perform a qualitative study to further understand the behavior of weight sharing in isotropic architectures. The code is available at https://github.com/apple/ml-spin.\""
  },
  "eccv2022_main_ensembleknowledgeguidedsub-networksearchandfine-tuningforfilterpruning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Ensemble Knowledge Guided Sub-network Search and Fine-Tuning for Filter Pruning",
    "authors": [
      "Seunghyun Lee",
      "Byung Cheol Song"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6024_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710568.pdf",
    "published": "2020-08",
    "summary": "\"Conventional NAS-based pruning algorithms aim to find the sub-network with the best validation performance. However, validation performance does not successfully represent test performance, i.e., potential performance. Also, although fine-tuning the pruned network to restore the performance drop is an inevitable process, few studies have handled this issue. This paper proposes a novel sub-network search and fine-tuning method that is named Ensemble Knowledge Guidance (EKG). First, we experimentally prove that the fluctuation of the loss landscape is an effective metric to evaluate the potential performance. In order to search a sub-network with the smoothest loss landscape at a low cost, we propose a pseudo-supernet built by an ensemble sub-network knowledge distillation. Next, we propose a novel fine-tuning that re-uses the information of the search phase. We store the interim sub-networks, that is, the by-products of the search phase, and transfer their knowledge into the pruned network. Note that EKG is easy to be plugged-in and computationally efficient. For example, in the case of ResNet-50, about 45% of FLOPS is removed without any performance drop in only 315 GPU hours.\""
  },
  "eccv2022_main_networkbinarizationviacontrastivelearning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Network Binarization via Contrastive Learning",
    "authors": [
      "Yuzhang Shang",
      "Dan Xu",
      "Ziliang Zong",
      "Liqiang Nie",
      "Yan Yan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6207_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710585.pdf",
    "published": "2020-08",
    "summary": "\"Neural network binarization accelerates deep models by quantizing their weights and activations into 1-bit. However, there is still a huge performance gap between Binary Neural Networks (BNNs) and their full-precision (FP) counterparts. As the quantization error caused by weights binarization has been reduced in earlier works, the activations binarization becomes the major obstacle for further improvement of the accuracy. BNN characterises a unique and interesting structure, where the binary and latent FP activations exist in the same forward pass (i.e. Binarize(a_F) = a_B). To mitigate the information degradation caused by the binarization operation from FP to binary activations, we establish a novel contrastive learning framework while training BNNs through the lens of Mutual Information (MI) maximization. MI is introduced as the metric to measure the information shared between binary and the FP activations, which assists binarization with contrastive learning. Specifically, the representation ability of the BNNs is greatly strengthened via pulling the positive pairs with binary and full-precision activations from the same input samples, as well as pushing negative pairs from different samples (the number of negative pairs can be exponentially large). This benefits the downstream tasks, not only classification but also segmentation and depth estimation, etc. The experimental results show that our method can be implemented as a pile-up module on existing state-of-the-art binarization methods and can remarkably improve the performance over them on CIFAR-10/100 and ImageNet, in addition to the great generalization ability on NYUD-v2.\""
  },
  "eccv2022_main_lipschitzcontinuityretainedbinaryneuralnetwork": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Lipschitz Continuity Retained Binary Neural Network",
    "authors": [
      "Yuzhang Shang",
      "Dan Xu",
      "Bin Duan",
      "Ziliang Zong",
      "Liqiang Nie",
      "Yan Yan"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6210_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710601.pdf",
    "published": "2020-08",
    "summary": "\"Relying on the premise that the performance of a binary neural network can be largely restored with eliminated quantization error between full-precision weight vectors and their corresponding binary vectors, existing works of network binarization frequently adopt the idea of model robustness to reach the aforementioned objective. However, robustness remains to be an ill-defined concept without solid theoretical support. In this work, we introduce the Lipschitz continuity, a well-defined functional property, as the rigorous criteria to define the model robustness for BNN. We then propose to retain the Lipschitz continuity as a regularization term to improve the model robustness. Particularly, while the popular Lipschitz-involved regularization methods often collapse in BNN due to its extreme sparsity, we design the Retention Matrices to approximate spectral norms of the targeted weight matrices, which can be deployed as the approximation for the Lipschitz constant of BNNs without the exact Lipschitz constant computation (NP-hard). Our experiments prove that our BNN-specific regularization method can effectively strengthen the robustness of BNN (testified on ImageNet-C), achieving state-of-the-art performance on CIFAR and ImageNet.\""
  },
  "eccv2022_main_spvitenablingfastervisiontransformersvialatency-awaresofttokenpruning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning",
    "authors": [
      "Zhenglun Kong",
      "Peiyan Dong",
      "Xiaolong Ma",
      "Xin Meng",
      "Wei Niu",
      "Mengshu Sun",
      "Xuan Shen",
      "Geng Yuan",
      "Bin Ren",
      "Hao Tang",
      "Minghai Qin",
      "Yanzhi Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6265_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710618.pdf",
    "published": "2020-08",
    "summary": "\"Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Considering the computation complexity, the internal data pattern of ViTs, and the edge device deployment, we propose a latency-aware soft token pruning framework, SPViT, which can be set up on vanilla Transformers of both flatten and hierarchical structures, such as DeiTs and Swin-Transformers (Swin). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique, which integrates the less informative tokens chosen by the selector module into a package token rather than discarding them completely. SPViT is bound to the trade-off between accuracy and latency requirements of specific edge devices through our proposed latency-aware training strategy. Experiment results show that SPViT significantly reduces the computation cost of ViTs with comparable performance on image classification. Moreover, SPViT can guarantee the identified model meets the latency specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile devices. For example, SPViT reduces the latency of DeiT-T to 26 ms (26% 41% superior to existing works) on the mobile device with 0.25% 4% higher top-1 accuracy on ImageNet. Our code is released at https://github.com/PeiyanFlying/SPViT\""
  },
  "eccv2022_main_softmaskingforcost-constrainedchannelpruning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Soft Masking for Cost-Constrained Channel Pruning",
    "authors": [
      "Ryan Humble",
      "Maying Shen",
      "Jorge Albericio Latorre",
      "Eric Darve",
      "Jose Alvarez"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6269_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710640.pdf",
    "published": "2020-08",
    "summary": "\"Structured channel pruning has been shown to significantly accelerate inference time for convolution neural networks (CNNs) on modern hardware, with a relatively minor loss of network accuracy. Recent works permanently zero these channels during training, which we observe to significantly hamper final accuracy, particularly as the fraction of the network being pruned increases. We propose Soft Masking for cost-constrained Channel Pruning (SMCP) to allow pruned channels to adaptively return to the network while simultaneously pruning towards a target cost constraint. By adding a soft mask re-parameterization of the weights and channel pruning from the perspective of removing input channels, we allow gradient updates to previously pruned channels and the opportunity for the channels to later return to the network. We then formulate input channel pruning as a global resource allocation problem. Our method outperforms prior works on both the ImageNet classification and PASCAL VOC detection datasets.\""
  },
  "eccv2022_main_non-uniformstepsizequantizationforaccuratepost-trainingquantization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Non-uniform Step Size Quantization for Accurate Post-Training Quantization",
    "authors": [
      "Sangyun Oh",
      "Hyeonuk Sim",
      "Jounghyun Kim",
      "Jongeun Lee"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6287_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710657.pdf",
    "published": "2020-08",
    "summary": "\"Quantization is a very effective optimization technique to reduce hardware cost and memory footprint of deep neural network (DNN) accelerators. In particular, post-training quantization (PTQ) is often preferred as it does not require a full dataset or costly retraining. However, performance of PTQ lags significantly behind that of quantization-aware training especially for low-precision networks (<= 4-bit). In this paper we propose a novel PTQ scheme to bridge the gap, with minimal impact on hardware cost. The main idea of our scheme is to increase arithmetic precision while retaining the same representational precision. The excess arithmetic precision enables us to better match the input data distribution while also presenting a new optimization problem, to which we propose a novel search-based solution. Our scheme is based on logarithmic-scale quantization, which can help reduce hardware cost through the use of shifters instead of multipliers. Our evaluation results using various DNN models on challenging computer vision tasks (image classification, object detection, semantic segmentation) show superior accuracy compared with the state-of-the-art PTQ methods at various low-bit precisions.\""
  },
  "eccv2022_main_superticketsdrawingtask-agnosticlotteryticketsfromsupernetsviajointlyarchitecturesearchingandparameterpruning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning",
    "authors": [
      "Haoran You",
      "Baopu Li",
      "Zhanyi Sun",
      "Xu Ouyang",
      "Yingyan Lin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6312_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710673.pdf",
    "published": "2020-08",
    "summary": "\"Neural architecture search (NAS) has demonstrated amazing success in searching for efficient deep neural networks (DNNs) from a given supernet. In parallel, the lottery ticket hypothesis has shown that DNNs contain small subnetworks that can be trained from scratch to achieve a comparable or higher accuracy than original DNNs. As such, it is currently a common practice to develop efficient DNNs via a pipeline of first search and then prune. Nevertheless, doing so often requires a search-train-prune-retrain process and thus prohibitive computational cost. In this paper, we discover for the first time that both efficient DNNs and their lottery subnetworks (i.e., lottery tickets) can be directly identified from a supernet, which we term as SuperTickets, via a two-in-one training scheme with jointly architecture searching and parameter pruning. Moreover, we develop a progressive and unified SuperTickets identification strategy that allows the connectivity of subnetworks to change during supernet training, achieving better accuracy and efficiency trade-offs than conventional sparse training. Finally, we evaluate whether such identified SuperTickets drawn from one task can transfer well to other tasks, validating their potential of handling multiple tasks simultaneously. Extensive experiments and ablation studies on three tasks and four benchmark datasets validate that our proposed SuperTickets achieve boosted accuracy and efficiency trade-offs than both typical NAS and pruning pipelines, regardless of having retraining or not. Codes and pretrained models are available at https://github.com/RICE-EIC/SuperTickets.\""
  },
  "eccv2022_main_meta-gftrainingdynamic-depthneuralnetworksharmoniously": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Meta-GF: Training Dynamic-Depth Neural Networks Harmoniously",
    "authors": [
      "Yi Sun",
      "Jian Li",
      "Xin Xu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6337_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710691.pdf",
    "published": "2020-08",
    "summary": "\"Most state-of-the-art deep neural networks use static inference graphs, which makes it impossible for such networks to dynamically adjust the depth or width of the network according to the complexity of the input data. Different from these static models, depth-adaptive neural networks, e.g. the multi-exit networks, aim at improving the computation efficiency by conducting adaptive inference conditioned on the input. To achieve adaptive inference, multiple output exits are attached at different depths of the multi-exit networks. Unfortunately, these exits usually interfere with each other in the training stage. The interference would reduce performance of the models and cause negative influences on the convergence speed. To address this problem, we investigate the gradient conflict of these multi-exit networks, and propose a novel meta-learning based training paradigm namely Meta-GF(meta gradient fusion) to harmoniously train these exits. Different from existing approaches, Meta-GF takes account of the importances of the shared parameters to each exit, and fuses the gradients of each exit by the meta-learned weights. Experimental results on CIFAR and ImageNet verify the effectiveness of the proposed method. Furthermore, the proposed Meta-GF requires no modification on the network structures and can be directly combined with previous training techniques. The code is available at https://github.com/SYVAE/MetaGF.\""
  },
  "eccv2022_main_towardsultralowlatencyspikingneuralnetworksforvisionandsequentialtasksusingtemporalpruning": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards Ultra Low Latency Spiking Neural Networks for Vision and Sequential Tasks Using Temporal Pruning",
    "authors": [
      "Sayeed Shafayet Chowdhury",
      "Nitin Rathi",
      "Kaushik Roy"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6421_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710709.pdf",
    "published": "2020-08",
    "summary": "\"Spiking Neural Networks (SNNs) can be energy efficient alternatives to commonly used deep neural networks (DNNs). However, computation over multiple timesteps increases latency and energy and incurs memory access overhead of membrane potentials. Hence, latency reduction is pivotal to obtain SNNs with high energy efficiency. But, reducing latency can have an adverse effect on accuracy. To optimize the accuracy-energy-latency trade-off, we propose a temporal pruning method which starts with an SNN of T timesteps, and reduces T every iteration of training, with threshold and leak as trainable parameters. This results in a continuum of SNNs from T timesteps, all the way up to unit timestep. Training SNNs directly with 1 timestep results in convergence failure due to layerwise spike vanishing and difficulty in finding optimum thresholds. The proposed temporal pruning overcomes this by enabling the learning of suitable layerwise thresholds with backpropagation by maintaining sufficient spiking activity. Using the proposed algorithm, we achieve top-1 accuracy of 93.05%, 70.15% and 69.00% on CIFAR-10, CIFAR-100 and ImageNet, respectively with VGG16, in just 1 timestep. Note, SNNs with leaky-integrate-and-fire (LIF) neurons behave as Recurrent Neural Networks (RNNs), with the membrane potential retaining information of previous inputs. The proposed SNNs also enable performing sequential tasks such as reinforcement learning on Cartpole and Atari pong environments using only 1 to 5 timesteps.\""
  },
  "eccv2022_main_towardsaccuratenetworkquantizationwithequivalentsmoothregularizer": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Towards Accurate Network Quantization with Equivalent Smooth Regularizer",
    "authors": [
      "Kirill Solodskikh",
      "Vladimir Chikin",
      "Ruslan Aydarkhanov",
      "Dehua Song",
      "Irina Zhelavskaya",
      "Jiansheng Wei"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6454_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710726.pdf",
    "published": "2020-08",
    "summary": "\"Neural network quantization techniques have been a prevailing way to reduce the inference time and storage cost of full-precision models for mobile devices. However, they still suffer from accuracy degradation due to inappropriate gradients in the optimization phase, especially for low-bit precision network and low-level vision tasks. To alleviate this issue, this paper defines a family of equivalent smooth regularizers for neural network quantization, named as SQR, which represents the equivalent of actual quantization error. Based on the definition, we propose a novel QSin regularizer as an instance to evaluate the performance of SQR, and also build up an algorithm to train the network for integer weight and activation. Extensive experimental results on classification and SR tasks reveal that the proposed method achieves higher accuracy than other prominent quantization approaches. Especially for SR task, our method alleviates the plaid artifacts effectively for quantized networks in terms of visual quality.\""
  },
  "eccv2022_main_explicitmodelsizecontrolandrelaxationviasmoothregularizationformixed-precisionquantization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Explicit Model Size Control and Relaxation via Smooth Regularization for Mixed-Precision Quantization",
    "authors": [
      "Vladimir Chikin",
      "Kirill Solodskikh",
      "Irina Zhelavskaya"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6475_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720001.pdf",
    "published": "2020-08",
    "summary": "\"While Deep Neural Networks (DNNs) quantization leads to a significant reduction in computational and storage costs, it reduces model capacity and therefore, usually leads to an accuracy drop. One of the possible ways to overcome this issue is to use different quantization bit-widths for different layers. The main challenge of the mixed-precision approach is to define the bit-widths for each layer, while staying under memory and latency requirements. Motivated by this challenge, we introduce a novel technique for explicit complexity control of DNNs quantized to mixed-precision, which uses smooth optimization on the surface containing neural networks of constant size. Furthermore, we introduce a family of smooth quantization regularizers, which can be used jointly with our complexity control method for both post-training mixed-precision quantization and quantization-aware training. Our approach can be applied to any neural network architecture. Experiments show that the proposed techniques reach state-of-the-art results.\""
  },
  "eccv2022_main_basqbranch-wiseactivation-clippingsearchquantizationforsub-4-bitneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "BASQ: Branch-Wise Activation-Clipping Search Quantization for Sub-4-Bit Neural Networks",
    "authors": [
      "Han-Byul Kim",
      "Eunhyeok Park",
      "Sungjoo Yoo"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6512_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720017.pdf",
    "published": "2020-08",
    "summary": "\"In this paper, we propose Branch-wise Activation-clipping Search Quantization (BASQ), which is a novel quantization method for low-bit activation. BASQ optimizes clip value in continuous search space while simultaneously searching L2 decay weight factor for updating clip value in discrete search space. We also propose a novel block structure for low precision that works properly on both MobileNet and ResNet structures with branch-wise searching. We evaluate the proposed methods by quantizing both weights and activations to 4-bit or lower. Contrary to the existing methods which are effective only for redundant networks, e.g., ResNet-18, or highly optimized networks, e.g., MobileNet-v2, our proposed method offers constant competitiveness on both types of networks across low precisions from 2 to 4-bits. Specifically, our 2-bit MobileNet-v2 offers top-1 accuracy of 64.71% on ImageNet, outperforming the existing method by a large margin (2.8%), and our 4-bit MobileNet-v2 gives 71.98% which is comparable to the full-precision accuracy 71.88% while our uniform quantization method offers comparable accuracy of 2-bit ResNet-18 to the state-of-the-art non-uniform quantization method.\""
  },
  "eccv2022_main_youalreadyhaveitagenerator-freelow-precisiondnntrainingframeworkusingstochasticrounding": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "You Already Have It: A Generator-Free Low-Precision DNN Training Framework Using Stochastic Rounding",
    "authors": [
      "Geng Yuan",
      "Sung-En Chang",
      "Qing Jin",
      "Alec Lu",
      "Yanyu Li",
      "Yushu Wu",
      "Zhenglun Kong",
      "Yanyue Xie",
      "Peiyan Dong",
      "Minghai Qin",
      "Xiaolong Ma",
      "Xulong Tang",
      "Zhenman Fang",
      "Yanzhi Wang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6538_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720034.pdf",
    "published": "2020-08",
    "summary": "\"Stochastic rounding is a critical technique used in low-precision deep neural networks (DNNs) training to ensure good model accuracy. However, it requires a large number of random numbers generated on the fly. This is not a trivial task on the hardware platforms such as FPGA and ASIC. The widely used solution is to introduce random number generators with extra hardware costs. In this paper, we innovatively propose to employ the stochastic property of DNN training process itself and directly extract random numbers from DNNs in a self-sufficient manner. We propose different methods to obtain random numbers from different sources in neural networks and a generator-free framework is proposed for low-precision DNN training on a variety of deep learning tasks. Moreover, we evaluate the quality of the extracted random numbers and find that high-quality random numbers widely exist in DNNs, while their quality can even pass the NIST test suite.\""
  },
  "eccv2022_main_realspikelearningreal-valuedspikesforspikingneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Real Spike: Learning Real-Valued Spikes for Spiking Neural Networks",
    "authors": [
      "Yufei Guo",
      "Liwen Zhang",
      "Yuanpei Chen",
      "Xinyi Tong",
      "Xiaode Liu",
      "YingLei Wang",
      "Xuhui Huang",
      "Zhe Ma"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6623_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720052.pdf",
    "published": "2020-08",
    "summary": "\"Brain-inspired spiking neural networks (SNNs) have recently drawn more and more attention due to their event-driven and energy efficient characteristics. The integration of storage and computation paradigm on neuromorphic hardwares makes SNNs much different from Deep Neural Networks (DNNs). In this paper, we argue that SNNs may not benefit from the weight-sharing mechanism, which can effectively reduce parameters and improve inference efficiency in DNNs, in some hardwares, and assume that an SNN with unshared convolution kernels could perform better. Motivated by this assumption, a training-inference decoupling method for SNNs named as Real Spike is proposed, which not only enjoys both unshared convolution kernels and binary spikes in inference time but also aintains both shared convolution kernels and Real-valued Spikes during training. This decoupling mechanism of SNN is realized by a re-parameterization technique. Furthermore, based on the training-inference-decoupled idea, a series of other ways for constructing Real Spike on different levels are presented, which also enjoy shared convolutions in the inference and are friendly to both neuromorphic and non-neuromorphic hardware platforms. A theoretical proof is given to clarify that the Real Spike-based SNN network is superior to its vanilla counterpart. Experimental results show that all different Real Spike versions can consistently improve the SNN performance. Moreover, the proposed method outperforms the state-of-the-art models on both non-spiking static and neuromorphic datasets.\""
  },
  "eccv2022_main_fedltnfederatedlearningforsparseandpersonalizedlotteryticketnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "FedLTN: Federated Learning for Sparse and Personalized Lottery Ticket Networks",
    "authors": [
      "Vaikkunth Mugunthan",
      "Eric Lin",
      "Vignesh Gokul",
      "Christian Lau",
      "Lalana Kagal",
      "Steve Pieper"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6634_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720069.pdf",
    "published": "2020-08",
    "summary": "\"Federated learning (FL) enables clients to collaboratively train a model, while keeping their local training data decentralized. However, high communication costs, data heterogeneity across clients, and lack of personalization techniques hinder the development of FL. In this paper, we propose FedLTN, a novel approach motivated by the well-known Lottery Ticket Hypothesis to learn sparse and personalized lottery ticket networks (LTNs) for communication-efficient and personalized FL under non-identically and independently distributed (non-IID) data settings. Preserving batch-norm statistics of local clients, postpruning without rewinding, and aggregation of LTNs using server momentum ensures that our approach significantly outperforms existing state-of-the-art solutions. Experiments on CIFAR-10 and TinyImageNet datasets show the efficacy of our approach in learning personalized models while significantly reducing communication costs.\""
  },
  "eccv2022_main_theoreticalunderstandingoftheinformationflowoncontinuallearningperformance": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Theoretical Understanding of the Information Flow on Continual Learning Performance",
    "authors": [
      "Joshua Andle",
      "Salimeh Yasaei Sekeh"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7281_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720085.pdf",
    "published": "2020-08",
    "summary": "\"Continual learning (CL) requires a model to continually learn new tasks with incremental available information while retaining previous knowledge. Despite the numerous previous approaches to CL, most of them still suffer forgetting, expensive memory cost, or lack sufficient theoretical understanding. While different CL training regimes have been extensively studied empirically, insufficient attention has been paid to the underlying theory. In this paper, we establish a probabilistic framework to analyze information flow through layers in networks for sequential tasks and its impact on learning performance. Our objective is to optimize the information preservation between layers while learning new tasks. This manages task-specific knowledge passing throughout the layers while maintaining model performance on previous tasks. Our analysis provides novel insights into information adaptation within the layers during incremental task learning. We provide empirical evidence and practically highlight the performance improvement across multiple tasks. Code is available at https://github.com/Sekeh-Lab/InformationFlow-CL.\""
  },
  "eccv2022_main_exploringlotterytickethypothesisinspikingneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Exploring Lottery Ticket Hypothesis in Spiking Neural Networks",
    "authors": [
      "Youngeun Kim",
      "Yuhang Li",
      "Hyoungseob Park",
      "Yeshwanth Venkatesha",
      "Ruokai Yin",
      "Priyadarshini Panda"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7345_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720101.pdf",
    "published": "2020-08",
    "summary": "\"Spiking Neural Networks (SNNs) have recently emerged as a new generation of low-power deep neural networks, which is suitable to be implemented on low-power mobile/edge devices. As such devices have limited memory storage, neural pruning on SNNs has been widely explored in recent years. Most existing SNN pruning works focus on shallow SNNs (2 6 layers), however, deeper SNNs (>16 layers) are proposed by state-of-the-art SNN works, which is difficult to be compatible with the current SNN pruning work. To scale up a pruning technique towards deep SNNs, we investigate Lottery Ticket Hypothesis (LTH) which states that dense networks contain smaller subnetworks (i.e., winning tickets) that achieve comparable performance to the dense networks. Our studies on LTH reveal that the winning tickets consistently exist in deep SNNs across various datasets and architectures, providing up to 97% sparsity without huge performance degradation. However, the iterative searching process of LTH brings a huge training computational cost when combined with the multiple timesteps of SNNs. To alleviate such heavy searching cost, we propose Early-Time (ET) ticket where we find the important weight connectivity from a smaller number of timesteps. The proposed ET ticket can be seamlessly combined with a common pruning techniques for finding winning tickets, such as Iterative Magnitude Pruning (IMP) and Early-Bird (EB) tickets. Our experiment results show that the proposed ET ticket reduces search time by up to 38% compared to IMP or EB methods. Code is available at Github.\""
  },
  "eccv2022_main_ontheangularupdateandhyperparametertuningofascale-invariantnetwork": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "On the Angular Update and Hyperparameter Tuning of a Scale-Invariant Network",
    "authors": [
      "Juseung Yun",
      "Janghyeon Lee",
      "Hyounguk Shon",
      "Eojindl Yi",
      "Seung Hwan Kim",
      "Junmo Kim"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7375_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720120.pdf",
    "published": "2020-08",
    "summary": "\"Modern deep neural networks are equipped with normalization layers such as batch normalization or layer normalization to enhance and stabilize training dynamics. If a network contains such normalization layers, the optimization objective is invariant to the scale of the neural network parameters. The scale-invariance induces the neural network\u2019s output to be only affected by the weights\u2019 direction and not the weights\u2019 scale. We first find a common feature of good hyperparameter combinations on such a scale-invariant network, including learning rate, weight decay, number of data samples, and batch size. Then we observe that hyperparameter setups that lead to good performance show similar degrees of angular update during one epoch. Using a stochastic differential equation, we analyze the angular update and show how each hyperparameter affects it. With this relationship, we can derive a simple hyperparameter tuning method and apply it to the efficient hyperparameter search.\""
  },
  "eccv2022_main_lanalatencyawarenetworkacceleration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "LANA: Latency Aware Network Acceleration",
    "authors": [
      "Pavlo Molchanov",
      "Jimmy Hall",
      "Hongxu Yin",
      "Jan Kautz",
      "Nicolo Fusi",
      "Arash Vahdat"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7385_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720136.pdf",
    "published": "2020-08",
    "summary": "\"We introduce latency-aware network acceleration (LANA)-an approach that builds on neural architecture search technique to accelerate neural networks. LANA consists of two phases: in the first phase, it trains many alternative operations for every layer of a target network using layer-wise feature map distillation. In the second phase, it solves the combinatorial selection of efficient operations using a novel constrained integer linear optimization (ILP) approach. ILP brings unique properties as it (i) performs NAS within a few seconds to minutes, (ii) easily satisfies budget constraints, (iii) works on the layer-granularity, (iv) supports a huge search space O(10^100), surpassing prior search approaches in efficacy and efficiency. In extensive experiments, we show that LANA yields efficient and accurate models constrained by a target latency budget, while being significantly faster than other techniques. We analyze three popular network architectures: EfficientNetV1, EfficientNetV2 and ResNeST, and achieve accuracy improvement (up to 3.0%) for all models when compressing larger models. LANA achieves significant speed-ups (up to 5x) with minor to no accuracy drop on GPU and CPU.\""
  },
  "eccv2022_main_rdo-qextremelyfine-grainedchannel-wisequantizationviarate-distortionoptimization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "RDO-Q: Extremely Fine-Grained Channel-Wise Quantization via Rate-Distortion Optimization",
    "authors": [
      "Zhe Wang",
      "Jie Lin",
      "Xue Geng",
      "Mohamed M. Sabry Aly",
      "Vijay Chandrasekhar"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7592_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720156.pdf",
    "published": "2020-08",
    "summary": "\"Allocating different bit widths to different channels and quantizing them independently bring higher quantization precision and accuracy. Most of prior works use equal bit width to quantize all layers or channels, which is sub-optimal. On the other hand, it is very challenging to explore the hyperparameter space of channel bit widths, as the search space increases exponentially with the number of channels, which could be tens of thousand in a deep neural network. In this paper, we address the problem of efficiently exploring the hyperparameter space of channel bit widths. We formulate the quantization of deep neural networks as a rate-distortion optimization problem, and present an ultra-fast algorithm to search the bit allocation of channels. Our approach has only linear time complexity and can find the optimal bit allocation within a few minutes on CPU. In addition, we provide an effective way to improve the performance on target hardware platforms. We restrict the bit rate (size) of each layer to allow as many weights and activations as possible to be stored on-chip, and incorporate hardware-aware constraints into our objective function. The hardware-aware constraints do not cause additional overhead to optimization, and have very positive impact on hardware performance. Experimental results show that our approach achieves state-of-the-art results on four deep neural networks, ResNet-18, ResNet-34, ResNet-50, and MobileNet-v2, on ImageNet. Hardware simulation results demonstrate that our approach is able to bring up to 3.5x and 3.0x speedups on two deep-learning accelerators, TPU and Eyeriss, respectively.\""
  },
  "eccv2022_main_u-boostnasutilization-boosteddifferentiableneuralarchitecturesearch": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "U-Boost NAS: Utilization-Boosted Differentiable Neural Architecture Search",
    "authors": [
      "Ahmet Caner Y\u00fcz\u00fcg\u00fcler",
      "Nikolaos Dimitriadis",
      "Pascal Frossard"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7802_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720172.pdf",
    "published": "2020-08",
    "summary": "\"Optimizing resource utilization in target platforms is key to achieving high performance during DNN inference. While optimizations have been proposed for inference latency, memory footprint, and energy consumption, prior hardware-aware neural architecture search (NAS) methods have omitted resource utilization, preventing DNNs to take full advantage of the target inference platforms. Modeling resource utilization efficiently and accurately is challenging, especially for widely-used array-based inference accelerators such as Google TPU. In this work, we propose a novel hardware-aware NAS framework that does not only optimize for task accuracy and inference latency, but also for resource utilization. We also propose and validate a new computational model for resource utilization in inference accelerators. By using the proposed NAS framework and the proposed resource utilization model, we achieve 2.8 - 4x speedup for DNN inference compared to prior hardware-aware NAS methods while attaining similar or improved accuracy in image classification on CIFAR-10 and Imagenet-100 datasets.\""
  },
  "eccv2022_main_ptq4vitpost-trainingquantizationforvisiontransformerswithtwinuniformquantization": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "PTQ4ViT: Post-Training Quantization for Vision Transformers with Twin Uniform Quantization",
    "authors": [
      "Zhihang Yuan",
      "Chenhao Xue",
      "Yiqi Chen",
      "Qiang Wu",
      "Guangyu Sun"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7856_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720190.pdf",
    "published": "2020-08",
    "summary": "\"Quantization is one of the most effective methods to compress neural networks, which has achieved great success on convolutional neural networks (CNNs). Recently, vision transformers have demonstrated great potential in computer vision. However, previous post-training quantization methods performed not well on vision transformer, resulting in more than 1% accuracy drop even in 8-bit quantization. Therefore, we analyze the problems of quantization on vision transformers. We observe the distributions of activation values after softmax and GELU functions are quite different from the Gaussian distribution. We also observe that common quantization metrics, such as MSE and cosine distance, are inaccurate to determine the optimal scaling factor. In this paper, we propose the twin uniform quantization method to reduce the quantization error on these activation values. And we propose to use a Hessian guided metric to evaluate different scaling factors, which improves the accuracy of calibration with a small cost. To enable the fast quantization of vision transformers, we develop an efficient framework, PTQ4ViT. Experiments show the quantized vision transformers achieve near-lossless prediction accuracy (less than 0.5% drop at 8-bit quantization) on the ImageNet classification task.\""
  },
  "eccv2022_main_bitwidth-adaptivequantization-awareneuralnetworktrainingameta-learningapproach": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach",
    "authors": [
      "Jiseok Youn",
      "Jaehun Song",
      "Hyung-Sin Kim",
      "Saewoong Bahk"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/8106_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720207.pdf",
    "published": "2020-08",
    "summary": "\"Deep neural network quantization with adaptive bitwidths has gained increasing attention due to the ease of model deployment on various platforms with different resource budgets. In this paper, we propose a meta-learning approach to achieve this goal. Specifically, we propose MEBQAT, a simple yet effective way of bitwidth-adaptive quantization aware training (QAT) where meta-learning is effectively combined with QAT by redefining meta-learning tasks to incorporate bitwidths. After being deployed on a platform, MEBQAT allows the (meta-)trained model to be quantized to any candidate bitwidth then helps to conduct inference without much accuracy drop from quantization. Moreover, with a few-shot learning scenario, MEBQAT can also adapt a model to any bitwidth as well as any unseen target classes by adding conventional optimization or metric-based meta-learning. We design variants of MEBQAT to support both (1) a bitwidth-adaptive quantization scenario and (2) a new few-shot learning scenario where both quantization bitwidths and target classes are jointly adapted. We experimentally demonstrate their validity in multiple QAT schemes. By comparing their performance to (bitwidth-dedicated) QAT, existing bitwidth adaptive QAT and vanilla meta-learning, we find that merging bitwidths into meta-learning tasks achieves a higher level of robustness.\""
  },
  "eccv2022_main_understandingthedynamicsofdnnsusinggraphmodularity": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Understanding the Dynamics of DNNs Using Graph Modularity",
    "authors": [
      "Yao Lu",
      "Wen Yang",
      "Yunzhe Zhang",
      "Zuohui Chen",
      "Jinyin Chen",
      "Qi Xuan",
      "Zhen Wang",
      "Xiaoniu Yang"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/52_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720224.pdf",
    "published": "2020-08",
    "summary": "\"There are good arguments to support the claim that deep neural networks (DNNs) capture better feature representations than the previous hand-crafted feature engineering, which leads to a significant performance improvement. In this paper, we move a tiny step towards understanding the dynamics of feature representations over layers. Specifically, we model the process of class separation of intermediate representations in pre-trained DNNs as the evolution of communities in dynamic graphs. Then, we introduce modularity, a generic metric in graph theory, to quantify the evolution of communities. In the preliminary experiment, we find that modularity roughly tends to increase as the layer goes deeper and the degradation and plateau arise when the model complexity is great relative to the dataset. Through an asymptotic analysis, we prove that modularity can be broadly used for different applications. For example, modularity provides new insights to quantify the difference between feature representations. More crucially, we demonstrate that the degradation and plateau in modularity curves represent redundant layers in DNNs and can be pruned with minimal impact on performance, which provides theoretical guidance for layer pruning. Our code is available at https://github.com/yaolu-zjut/Dynamic-Graphs-Construction.\""
  },
  "eccv2022_main_latentdiscriminantdeterministicuncertainty": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Latent Discriminant Deterministic Uncertainty",
    "authors": [
      "Gianni Franchi",
      "Xuanlong Yu",
      "Andrei Bursuc",
      "Emanuel Aldea",
      "Severine Dubuisson",
      "David Filliat"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1119_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720242.pdf",
    "published": "2020-08",
    "summary": "\"Predictive uncertainty estimation is essential for deploying Deep Neural Networks in real-world autonomous systems. However, most successful approaches are computationally intensive. In this work, we attempt to address these challenges in the context of autonomous driving perception tasks. Recently proposed Deterministic Uncertainty Methods (DUM) can only partially meet such requirements as their scalability to complex computer vision tasks is not obvious. In this work we advance a scalable and effective DUM for high-resolution semantic segmentation, that relaxes the Lipschitz constraint typically hindering practicality of such architectures. We learn a discriminant latent space by leveraging a distinction maximization layer over an arbitrarily-sized set of trainable prototypes. Our approach achieves competitive results over Deep Ensembles, the state-of-the-art for uncertainty prediction, on image classification, segmentation and monocular depth estimation tasks.\""
  },
  "eccv2022_main_makingheadsortailstowardssemanticallyconsistentvisualcounterfactuals": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals",
    "authors": [
      "Simon Vandenhende",
      "Dhruv Mahajan",
      "Filip Radenovic",
      "Deepti Ghadiyaram"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1330_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720260.pdf",
    "published": "2020-08",
    "summary": "\"A visual counterfactual explanation replaces image regions in a query image with regions from a distractor image such that the system\u2019s decision on the transformed image changes to the distractor class. In this work, we present a novel framework for computing visual counterfactual explanations based on two key ideas. First, we enforce that the replaced and replacer regions contain the same semantic part, resulting in more semantically consistent explanations. Second, we use multiple distractor images in a computationally efficient way and obtain more discriminative explanations with fewer region replacements. Our approach is 27 % more semantically consistent and an order of magnitude faster than a competing method on three fine-grained image recognition datasets. We highlight the utility of our counterfactuals over existing works through machine teaching experiments where we teach humans to classify different bird species. We also complement our explanations with the vocabulary of parts and attributes that contributed the most to the system\u2019s decision. In this task as well, we obtain state-of-the-art results when using our counterfactual explanations relative to existing works, reinforcing the importance of semantically consistent explanations. Source code is available at https://github.com/facebookresearch/visual-counterfactuals.\""
  },
  "eccv2022_main_hiveevaluatingthehumaninterpretabilityofvisualexplanations": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "HIVE: Evaluating the Human Interpretability of Visual Explanations",
    "authors": [
      "Sunnie S. Y. Kim",
      "Nicole Meister",
      "Vikram V. Ramaswamy",
      "Ruth Fong",
      "Olga Russakovsky"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1511_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720277.pdf",
    "published": "2020-08",
    "summary": "\"As AI technology is increasingly applied to high-impact, high-risk domains, there have been a number of new methods aimed at making AI models more human interpretable. Despite the recent growth of interpretability work, there is a lack of systematic evaluation of proposed techniques. In this work, we introduce HIVE (Human Interpretability of Visual Explanations), a novel human evaluation framework that assesses the utility of explanations to human users in AI-assisted decision making scenarios, and enables falsifiable hypothesis testing, cross-method comparison, and human-centered evaluation of visual interpretability methods. To the best of our knowledge, this is the first work of its kind. Using HIVE, we conduct IRB-approved human studies with nearly 1000 participants and evaluate four methods that represent the diversity of computer vision interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our results suggest that explanations engender human trust, even for incorrect predictions, yet are not distinct enough for users to distinguish between correct and incorrect predictions. We open-source HIVE to enable future studies and encourage more human-centered approaches to interpretability research.\""
  },
  "eccv2022_main_bayescapbayesianidentitycapforcalibrateduncertaintyinfrozenneuralnetworks": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen Neural Networks",
    "authors": [
      "Uddeshya Upadhyay",
      "Shyamgopal Karthik",
      "Yanbei Chen",
      "Massimiliano Mancini",
      "Zeynep Akata"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1824_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720295.pdf",
    "published": "2020-08",
    "summary": "\"High-quality calibrated uncertainty estimates are crucial for numerous real-world applications, especially for deep learning-based deployed ML systems. While Bayesian deep learning techniques allow uncertainty estimation, training them with large-scale datasets is an expensive process that does not always yield models competitive with non-Bayesian counterparts. Moreover, many of the high-performing deep learning models that are already trained and deployed are non-Bayesian in nature, and do not provide uncertainty estimates. To address these issues, we propose BayesCap that learns a Bayesian identity mapping for the frozen model, allowing uncertainty estimation. BayesCap is a memory-efficient method that can be trained on a small fraction of the original dataset, enhancing pretrained non-Bayesian computer vision models by providing calibrated uncertainty estimates for the predictions without (i) hampering the performance of the model and (ii) the need for expensive retraining the model from scratch. The proposed method is agnostic to various architectures and tasks. We show the efficacy of our method on a wide variety of tasks with a diverse set of architectures, including image super-resolution, deblurring, inpainting, and crucial application such as medical image translation. Moreover, we apply the derived uncertainty estimates to detect out-of-distribution samples in critical scenarios like depth estimation in autonomous driving.\""
  },
  "eccv2022_main_sesssaliencyenhancingwithscalingandsliding": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "SESS: Saliency Enhancing with Scaling and Sliding",
    "authors": [
      "Osman Tursun",
      "Simon Denman",
      "Sridha Sridharan",
      "Clinton Fookes"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2311_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720313.pdf",
    "published": "2020-08",
    "summary": "\"High-quality saliency maps are essential in several machine learning application areas including explainable AI and weakly supervised object detection and segmentation. Many techniques have been developed to generate better saliency using neural networks. However, they are often limited to specific saliency visualisation methods or saliency issues. We propose a novel saliency enhancing approach called \\textbf{SESS} (\\textbf{S}aliency \\textbf{E}nhancing with \\textbf{S}caling and \\textbf{S}liding). It is a method and model agnostic extension to existing saliency map generation methods. With SESS, existing saliency approaches become robust to scale variance, multiple occurrences of target objects, presence of distractors and generate less noisy and more discriminative saliency maps. SESS improves saliency by fusing saliency maps extracted from multiple patches at different scales from different areas, and combines these individual maps using a novel fusion scheme that incorporates channel-wise weights and spatial weighted average. To improve efficiency, we introduce a pre-filtering step that can exclude uninformative saliency maps to improve efficiency while still enhancing overall results. We evaluate SESS on object recognition and detection benchmarks where it achieves significant improvement. The code is released publicly to enable researchers to verify performance and further development. Code is available at https://github.com/neouyghur/SESS.\""
  },
  "eccv2022_main_notokenleftbehindexplainability-aidedimageclassificationandgeneration": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "No Token Left Behind: Explainability-Aided Image Classification and Generation",
    "authors": [
      "Roni Paiss",
      "Hila Chefer",
      "Lior Wolf"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/2764_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720329.pdf",
    "published": "2020-08",
    "summary": "\"The application of zero-shot learning in computer vision has been revolutionized by the use of image-text matching models. The most notable example, CLIP, has been widely used for both zero-shot classification and guiding generative models with a text prompt. However, the zero-shot use of CLIP is unstable with respect to the phrasing of the input text, making it necessary to carefully engineer the prompts used. We find that this instability stems from a selective similarity score, which is based only on a subset of the semantically meaningful input tokens. To mitigate it, we present a novel explainability-based approach, which adds a loss term to ensure that CLIP focuses on all relevant semantic parts of the input, in addition to employing the CLIP similarity loss used in previous works. When applied to one-shot classification through prompt engineering, our method yields an improvement in the recognition rate, without additional training or fine-tuning. Additionally, we show that CLIP guidance of generative models using our method significantly improves the generated images. Finally, we demonstrate a novel use of CLIP guidance for text-based image generation with spatial conditioning on object location, by requiring the image explainability heatmap for each object to be confined to a pre-determined bounding box.\""
  },
  "eccv2022_main_interpretableimageclassificationwithdifferentiableprototypesassignment": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Interpretable Image Classification with Differentiable Prototypes Assignment",
    "authors": [
      "Dawid Rymarczyk",
      "\u0141ukasz Struski",
      "Micha\u0142 G\u00f3rszczak",
      "Koryna Lewandowska",
      "Jacek Tabor",
      "Bartosz Zieli\u0144ski"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3240_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720346.pdf",
    "published": "2020-08",
    "summary": "\"Existing prototypical-based models address the black-box nature of deep learning. However, they are sub-optimal as they often assume separate prototypes for each class, require multi-step optimization, make decisions based on prototype absence (so-called negative reasoning process), and derive vague prototypes. To address those shortcomings, we introduce ProtoPool, an interpretable prototype-based model with positive reasoning and three main novelties. Firstly, we reuse prototypes in classes, which significantly decreases their number. Secondly, we allow automatic, fully differentiable assignment of prototypes to classes, which substantially simplifies the training process. Finally, we propose a new focal similarity function that contrasts the prototype from the background and consequently concentrates on more salient visual features. We show that ProtoPool obtains state-of-the-art accuracy on the CUB-200-2011 and the Stanford Cars datasets, substantially reducing the number of prototypes. We provide a theoretical analysis of the method and a user study to show that our prototypes capture more salient features than those obtained with competitive methods. We made the code available at https://github.com/gmum/ProtoPool.\""
  },
  "eccv2022_main_contributionsofshape,texture,andcolorinvisualrecognition": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "\"Contributions of Shape, Texture, and Color in Visual Recognition\"",
    "authors": [
      "Yunhao Ge",
      "Yao Xiao",
      "Zhi Xu",
      "Xingrui Wang",
      "Laurent Itti"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4287_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720364.pdf",
    "published": "2020-08",
    "summary": "\"We investigate the contributions of three important features of the human visual system (HVS)---shape, texture, and color ---to object classification. We build a humanoid vision engine (HVE) that explicitly and separately computes shape, texture, and color features from images. The resulting feature vectors are then concatenated to support the final classification. We show that HVE can summarize and rank-order the contributions of the three features to object recognition. We use human experiments to confirm that both HVE and humans predominantly use some specific features to support the classification of specific classes (e.g., texture is the dominant feature to distinguish a zebra from other quadrupeds, both for humans and HVE). With the help of HVE, given any environment (dataset), we can summarize the most important features for the whole task (global; e.g., color is the most important feature overall for classification with the CUB dataset), and for each class (local; e.g., shape is the most important feature to recognize boats in the iLab-20M dataset). To demonstrate more usefulness of HVE, we use it to simulate the open-world zero-shot learning ability of humans with no attribute labeling. Finally, we show that HVE can also simulate human imagination ability with the combination of different features.\""
  },
  "eccv2022_main_steexsteeringcounterfactualexplanationswithsemantics": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "STEEX: Steering Counterfactual Explanations with Semantics",
    "authors": [
      "Paul Jacob",
      "\u00c9loi Zablocki",
      "H\u00e9di Ben-Younes",
      "Micka\u00ebl Chen",
      "Patrick P\u00e9rez",
      "Matthieu Cord"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4953_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720382.pdf",
    "published": "2020-08",
    "summary": "\"As deep learning models are increasingly used in safety-critical applications, explainability and trustworthiness become major concerns. For simple images, such as low-resolution face portraits, synthesizing visual counterfactual explanations has recently been proposed as a way to uncover the decision mechanisms of a trained classification model. In this work, we address the problem of producing counterfactual explanations for high-quality images and complex scenes. Leveraging recent semantic-to-image models, we propose a new generative counterfactual explanation framework that produces plausible and sparse modifications which preserve the overall scene structure. Furthermore, we introduce the concept of \"\"region-targeted counterfactual explanations\"\", and a corresponding framework, where users can guide the generation of counterfactuals by specifying a set of semantic regions of the query image the explanation must be about. Extensive experiments are conducted on challenging datasets including high-quality portraits (CelebAMask-HQ) and driving scenes (BDD100k).\""
  },
  "eccv2022_main_arevisiontransformersrobusttopatchperturbations?": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Are Vision Transformers Robust to Patch Perturbations?",
    "authors": [
      "Jindong Gu",
      "Volker Tresp",
      "Yao Qin"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5424_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720399.pdf",
    "published": "2020-08",
    "summary": "\"Recent advances in Vision Transformer (ViT) have demonstrated its impressive performance in image classification, which makes it a promising alternative to Convolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image as a sequence of image patches. The patch-based input image representation makes the following question interesting: How does ViT perform when individual input image patches are perturbed with natural corruptions or adversarial perturbations, compared to CNNs? In this work, we study the robustness of ViT to patch-wise perturbations. Surprisingly, we {find} that ViTs are more robust to naturally corrupted patches than CNNs, whereas they are more vulnerable to adversarial patches. Furthermore, we discover that the attention mechanism greatly affects the robustness of vision transformers. Specifically, the attention module can help improve the robustness of ViT by effectively ignoring natural corrupted patches. However, when ViTs are attacked by an adversary, the attention mechanism can be easily fooled to focus more on the adversarially perturbed patches and cause a mistake. Based on our analysis, we propose a simple temperature-scaling based method to {improve} the robustness of ViT against adversarial patches. Extensive qualitative and quantitative experiments are performed to support our findings, understanding, and improvement of ViT robustness to patch-wise perturbations across a set of transformer-based architectures.\""
  },
  "eccv2022_main_adatasetgenerationframeworkforevaluatingmegapixelimageclassifiers\\&theirexplanations": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "A Dataset Generation Framework for Evaluating Megapixel Image Classifiers \\& Their Explanations",
    "authors": [
      "Gautam Machiraju",
      "Sylvia Plevritis",
      "Parag Mallick"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7350_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720416.pdf",
    "published": "2020-08",
    "summary": "\"Deep learning-based megapixel image classifiers have exceptional prediction performance in a number of domains, including clinical pathology. However, extracting reliable, human-interpretable model explanations has remained challenging. Because real-world megapixel images often contain latent image features highly correlated with image labels, it is difficult to distinguish correct explanations from incorrect ones. Furthering this issue are the flawed assumptions and designs of today\u2019s classifiers. To investigate classification and explanation performance, we introduce a framework to (a) generate synthetic control images that reflect common properties of megapixel images and (b) evaluate average test-set correctness. By benchmarking two commonplace Convolutional Neural Networks (CNNs), we demonstrate how this interpretability evaluation framework can inform architecture selection beyond classification performance -- in particular, we show that a simple Attention-based architecture identifies salient objects in all seven scenarios, while a standard CNN fails to do so in six scenarios. This work carries widespread applicability to any megapixel imaging domain.\""
  },
  "eccv2022_main_cartoonexplanationsofimageclassifiers": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Cartoon Explanations of Image Classifiers",
    "authors": [
      "Stefan Kolek",
      "Duc Anh Nguyen",
      "Ron Levie",
      "Joan Bruna",
      "Gitta Kutyniok"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7464_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720439.pdf",
    "published": "2020-08",
    "summary": "\"We present CartoonX (Cartoon Explanation), a novel model-agnostic explanation method tailored towards image classifiers and based on the rate-distortion explanation (RDE) framework. Natural images are roughly piece-wise smooth signals---also called cartoon-like images---and tend to be sparse in the wavelet domain. CartoonX is the first explanation method to exploit this by requiring its explanations to be sparse in the wavelet domain, thus extracting the relevant piece-wise smooth part of an image instead of relevant pixel-sparse regions. We demonstrate that CartoonX can reveal novel valuable explanatory information, particularly for misclassifications. Moreover, we show that CartoonX achieves a lower distortion with fewer coefficients than state-of-the-art methods.\""
  },
  "eccv2022_main_shap-camvisualexplanationsforconvolutionalneuralnetworksbasedonshapleyvalue": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Shap-CAM: Visual Explanations for Convolutional Neural Networks Based on Shapley Value",
    "authors": [
      "Quan Zheng",
      "Ziwei Wang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7522_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720455.pdf",
    "published": "2020-08",
    "summary": "\"Explaining deep convolutional neural networks has been recently drawing increasing attention since it helps to understand the networks\u2019 internal operations and why they make certain decisions. Saliency maps, which emphasize salient regions largely connected to the network\u2019s decision-making, are one of the most common ways for visualizing and analyzing deep networks in the computer vision community. However, saliency maps generated by existing methods cannot represent authentic information in images due to the unproven proposals about the weights of activation maps which lack solid theoretical foundation and fail to consider the relations between each pixels. In this paper, we develop a novel post-hoc visual explanation method called Shap-CAM based on class activation mapping. Unlike previous gradient-based approaches, Shap-CAM gets rid of the dependence on gradients by obtaining the importance of each pixels through Shapley value. We demonstrate that Shap-CAM achieves better visual performance and fairness for interpreting the decision making process. Our approach outperforms previous methods on both recognition and localization tasks.\""
  },
  "eccv2022_main_privacy-preservingfacerecognitionwithlearnableprivacybudgetsinfrequencydomain": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain",
    "authors": [
      "Jiazhen Ji",
      "Huan Wang",
      "Yuge Huang",
      "Jiaxiang Wu",
      "Xingkun Xu",
      "Shouhong Ding",
      "ShengChuan Zhang",
      "Liujuan Cao",
      "Rongrong Ji"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/123_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720471.pdf",
    "published": "2020-08",
    "summary": "\"Face recognition technology has been used in many fields due to its high recognition accuracy, including the face unlocking of mobile devices, community access control systems, and city surveillance. As the current high accuracy is guaranteed by very deep network structures, facial images often need to be transmitted to third-party servers with high computational power for inference. However, facial images visually reveal the user\u2019s identity information. In this process, both untrusted service providers and malicious users can significantly increase the risk of a personal privacy breach. Current privacy-preserving approaches to face recognition are often accompanied by many side effects, such as a significant increase in inference time or a noticeable decrease in recognition accuracy. This paper proposes a privacy-preserving face recognition method using differential privacy in the frequency domain. Due to the utilization of differential privacy, it offers a guarantee of privacy in theory. Meanwhile, the loss of accuracy is very slight. This method first converts the original image to the frequency domain and removes the direct component termed DC. Then a privacy budget allocation method can be learned based on the loss of the back-end face recognition network within the differential privacy framework. Finally, it adds the corresponding noise to the frequency domain features. Our method performs very well with several classical face recognition test sets according to the extensive experiments.\""
  },
  "eccv2022_main_contrast-physunsupervisedvideo-basedremotephysiologicalmeasurementviaspatiotemporalcontrast": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Contrast-Phys: Unsupervised Video-Based Remote Physiological Measurement via Spatiotemporal Contrast",
    "authors": [
      "Zhaodong Sun",
      "Xiaobai Li"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/205_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720488.pdf",
    "published": "2020-08",
    "summary": "\"Video-based remote physiological measurement utilizes face videos to measure the blood volume change signal, which is also called remote photoplethysmography (rPPG). Supervised methods for rPPG measurements achieve state-of-the-art performance. However, supervised rPPG methods require face videos and ground truth physiological signals for model training. In this paper, we propose an unsupervised rPPG measurement method that does not require ground truth signals for training. We use a 3DCNN model to generate multiple rPPG signals from each video in different spatiotemporal locations and train the model with a contrastive loss where rPPG signals from the same video are pulled together while those from different videos are pushed away. We test on five public datasets, including RGB videos and NIR videos. The results show that our method outperforms the previous unsupervised baseline and achieves accuracies very close to the current best supervised rPPG methods on all five datasets. Furthermore, we also demonstrate that our approach can run at a much faster speed and is more robust to noises than the previous unsupervised baseline. Our code is available at https://github.com/zhaodongsun/contrast-phys.\""
  },
  "eccv2022_main_source-freedomainadaptationwithcontrastivedomainalignmentandself-supervisedexplorationforfaceanti-spoofing": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "Source-Free Domain Adaptation with Contrastive Domain Alignment and Self-Supervised Exploration for Face Anti-Spoofing",
    "authors": [
      "Yuchen Liu",
      "Yabo Chen",
      "Wenrui Dai",
      "Mengran Gou",
      "Chun-Ting Huang",
      "Hongkai Xiong"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/362_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720506.pdf",
    "published": "2020-08",
    "summary": "\"Despite promising success in intra-dataset tests, existing face anti-spoofing (FAS) methods suffer from poor generalization ability under domain shift. This problem can be solved by aligning source and target data. However, due to privacy and security concerns of human faces, source data are usually inaccessible during adaptation for practical deployment, where only a pre-trained source model and unlabeled target data are available. In this paper, we propose a novel Source-free Domain Adaptation framework for Face Anti-Spoofing, namely SDA-FAS, that addresses the problems of source knowledge adaptation and target data exploration under the source-free setting. For source knowledge adaptation, we present novel strategies to realize self-training and domain alignment. We develop a contrastive domain alignment module to align conditional distribution across different domains by aggregating the features of fake and real faces separately. We demonstrate in theory that the pre-trained source model is equivalent to the source data as source prototypes for supervised contrastive learning in domain alignment. The source-oriented regularization is also introduced into self-training to alleviate the self-biasing problem. For target data exploration, self-supervised learning is employed with specified patch shuffle data augmentation to explore intrinsic spoofing features for unseen attack types. To our best knowledge, SDA-FAS is the first attempt that jointly optimizes the source-adapted knowledge and target self-supervised exploration for FAS. Extensive experiments on thirteen cross-dataset testing scenarios show that the proposed framework outperforms the state-of-the-art methods by a large margin.\""
  },
  "eccv2022_main_onmitigatinghardclustersforfaceclustering": {
    "conf_id": "ECCV2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ECCV2022",
    "title": "On Mitigating Hard Clusters for Face Clustering",
    "authors": [
      "Yingjie Chen",
      "Huasong Zhong",
      "Chong Chen",
      "Chen Shen",
      "Jianqiang Huang",
      "Tao Wang",
      "Yun Liang",
      "Qianru Sun"
    ],
    "page_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/413_ECCV_2022_paper.php",
    "pdf_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720523.pdf",
    "published": "2020-08",
    "summary": "\"Face clustering is a promising way to scale up face recognition systems using large-scale unlabeled face images. It remains challenging to identify small or sparse face image clusters that we call hard clusters, which is caused by the heterogeneity, i.e., high variations in size and sparsity, of the clusters. Consequently, the conventional way of using a uniform threshold (to identify clusters) often leads to a terrible misclassification for the samples that should belong to hard clusters. We tackle this problem by leveraging the neighborhood information of samples and inferring the cluster memberships (of samples) in a probabilistic way. We introduce two novel modules, Neighborhood-Diffusion-based Density (NDDe) and Transition-Probability-based Distance (TPDi), based on which we can simply apply the standard Density Peak Clustering algorithm with a uniform threshold. Our experiments on multiple benchmarks show that each module contributes to the final performance of our method, and by incorporating them into other advanced face clustering methods, these two modules can boost the performance of these methods to a new state-of-the-art.\""
  }
}